URL: http://www.cs.ubc.ca/spider/cebly/Papers/multirl2.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: fcclaus,ceblyg@cs.ubc.ca  
Title: The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems  
Author: Caroline Claus and Craig Boutilier 
Address: Vancouver, B.C., Canada V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia  
Abstract: Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (opti-mal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Boutilier. </author> <title> Learning conventions in multiagent stochastic domains using likelihood estimates. </title> <booktitle> Proc. 12th Intl. Conf. Uncertainty in AI, </booktitle> <address> pp.106-114, Portland, OR, </address> <year> 1996. </year>
Reference-contexts: This simple adaptive strategy will converge to an equilibrium in our simple cooperative games assuming that agents randomize when multiple best responses exist [12], and can be made to converge to an optimal equilibrium if appropriate mechanisms are adopted <ref> [1] </ref>; that is, the probability of coordinated equilibrium after k interactions can be made arbitrarily high by increasing k sufficiently. <p> We note that most game theoretic models assume that each agent can observe the actions executed by its counterparts with certainty. As pointed out and addressed in <ref> [1, 7] </ref>, this assumption is often unrealistic. A more general model allows each agent to obtain an observation which is related stochastically to the actual joint action selected, where Pr a (o) denotes the probability of observation o being obtained by all agents when joint action a is performed.
Reference: [2] <author> C. Boutilier. </author> <title> Planning, learning and coordination in multia gent decision processes. </title> <booktitle> Proc. 6th Conf. Theor. Aspects of Rationality and Knowledge, </booktitle> <address> pp.195-210, Amsterdam, </address> <year> 1996. </year>
Reference-contexts: Sequential optimality will not be of primary interest, though we will discuss this issue in Sections 5 and 6). 1 We can view the prob 1 Many of our conclusions hold mutatis mutandis for sequential, multiagent Markov decision processes <ref> [2] </ref> with multiple states; but lem at hand, then, as a distributed bandit problem. More formally, we assume a collection ff of n (heteroge-neous) agents, each agent i 2 ff having available to it a finite set of individual actions A i .
Reference: [3] <author> G. W. Brown. </author> <title> Iterative solution of games by fictitious play. </title> <editor> In T. C. Koopmans, editor, </editor> <title> Activity Analysis of Production and Allocation. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1951. </year>
Reference-contexts: One especially simple, yet often effective, learning model for achieving coordination is fictitious play <ref> [3, 5] </ref>. Each agent i keeps a count C j a j , for each j 2 ff and a j 2 A j , of the number of times agent j has used action a j in the past.
Reference: [4] <author> C. Claus and C. Boutilier. </author> <title> The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems. AAAI-97 Work. Multiagent Learning, </title> <address> pp.13-18, Providence, </address> <year> 1997. </year>
Reference-contexts: A preliminary version of this paper <ref> [4] </ref> studies the methods below within this model. 3 Comparing Independent and Joint-Action Learners We first compare the relative performance of independent and joint-action learners on a simple coordination game of the form described above: a0 a1 b1 0 10 The first thing to note is that ILs using nonexploitive exploration
Reference: [5] <author> D. Fudenberg and D. M. Kreps. </author> <title> Lectures on Learning and Equilibrium in Strategic Form Games. CORE Foundation, </title> <address> Louvain-La-Neuve, Belgium, </address> <year> 1992. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 5, 6, 9] </ref>. We then briefly consider the importance of exploitive exploration strategies and examine, through a series of examples, how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> For instance, communication between agents might be admitted [22] or one could impose conventions or rules that restrict behavior so as to ensure coordination [18]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 9, 11] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [17, 10, 24].) we will see that interesting issues emerge. <p> One especially simple, yet often effective, learning model for achieving coordination is fictitious play <ref> [3, 5] </ref>. Each agent i keeps a count C j a j , for each j 2 ff and a j 2 A j , of the number of times agent j has used action a j in the past. <p> Furthermore, it ensures that agents cannot adopt deterministic exploration strategies and become strictly correlated. Finally, the last condition ensures that agents exploit their knowledge. In the context of ficticious play and its variants, this exploration strategy would be asymptotically myopic <ref> [5] </ref>. This is necessary to ensure that an equilibrium will be reached. Under these conditions we have: Theorem 1 Let E t be a random variable denoting the probability of a (deterministic) equilibrium strategy profile being played at time t.
Reference: [6] <author> D. Fudenberg and D. K. Levine. </author> <title> Steady state learning and Nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(3) </volume> <pages> 547-573, </pages> <year> 1993. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 5, 6, 9] </ref>. We then briefly consider the importance of exploitive exploration strategies and examine, through a series of examples, how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> For instance, communication between agents might be admitted [22] or one could impose conventions or rules that restrict behavior so as to ensure coordination [18]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 9, 11] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [17, 10, 24].) we will see that interesting issues emerge.
Reference: [7] <author> J. Hu and M. P. Wellman. </author> <title> Self-fulfilling bias in multiagent learning. </title> <booktitle> Proc. </booktitle> <address> ICMAS-96, pp.118-125, Kyoto, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 20, 16, 11, 7, 15] </ref>. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> We note that most game theoretic models assume that each agent can observe the actions executed by its counterparts with certainty. As pointed out and addressed in <ref> [1, 7] </ref>, this assumption is often unrealistic. A more general model allows each agent to obtain an observation which is related stochastically to the actual joint action selected, where Pr a (o) denotes the probability of observation o being obtained by all agents when joint action a is performed.
Reference: [8] <author> L. P. Kaelbling, M. L. Littman, A. W. Moore. </author> <title> Reinforcement learning: A survey. J. Art. Intel. </title> <journal> Res., </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: In such a case, reinforcement learning can be used by the agents to estimate, based on past experience, the expected reward associated with individual or joint actions. We refer to <ref> [8] </ref> for a survey of RL techniques. A simple, well-understood algorithm for single agent learning is Q-learning [21]. The formulation of Q-learning for general sequential decision processes is more sophisticated than we need here.
Reference: [9] <author> E. Kalai and E. Lehrer. </author> <title> Rational learning leads to Nash equi librium. </title> <journal> Econometrica, </journal> <volume> 61(5) </volume> <pages> 1019-1045, </pages> <year> 1993. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 5, 6, 9] </ref>. We then briefly consider the importance of exploitive exploration strategies and examine, through a series of examples, how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> For instance, communication between agents might be admitted [22] or one could impose conventions or rules that restrict behavior so as to ensure coordination [18]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 9, 11] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [17, 10, 24].) we will see that interesting issues emerge.
Reference: [10] <author> M. Kandori, G. Mailath, R. Rob. </author> <title> Learning, mutation and long run equilibria in games. </title> <journal> Econometrica, </journal> <volume> 61 </volume> <pages> 29-56, </pages> <year> 1993. </year>
Reference-contexts: Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 9, 11]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [17, 10, 24] </ref>.) we will see that interesting issues emerge. One especially simple, yet often effective, learning model for achieving coordination is fictitious play [3, 5].
Reference: [11] <author> M. L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proc. 11th Intl. Conf. on Machine Learning, </booktitle> <address> pp.157-163, New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 20, 16, 11, 7, 15] </ref>. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> For instance, communication between agents might be admitted [22] or one could impose conventions or rules that restrict behavior so as to ensure coordination [18]. Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents <ref> [5, 6, 9, 11] </ref>. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study [17, 10, 24].) we will see that interesting issues emerge.
Reference: [12] <author> D. Monderer, L. S. Shapley. </author> <title> Fictitious play property for games with identical interests. </title> <journal> J. Econ. Th., </journal> <volume> 68 </volume> <pages> 258-265, </pages> <year> 1996. </year>
Reference-contexts: This simple adaptive strategy will converge to an equilibrium in our simple cooperative games assuming that agents randomize when multiple best responses exist <ref> [12] </ref>, and can be made to converge to an optimal equilibrium if appropriate mechanisms are adopted [1]; that is, the probability of coordinated equilibrium after k interactions can be made arbitrarily high by increasing k sufficiently.
Reference: [13] <author> R. B. Myerson. </author> <title> Game Theory: Analysis of Conflict. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: The decision problem is cooperative since each agent's reward is drawn from the same distribution, reflecting the utility assessment of all agents. The agents wish to choose actions that maximize (expected) reward. We adopt some standard game theoretic terminology <ref> [13] </ref>. A randomized strategy for agent i is a distribution 2 (A i ) (where (A i ) is the set of distributions over the agent's action set A i ). Intuitively, (a i ) denotes the probability of agent i selecting the individual action a i . <p> If they choose them randomly, or in some way reflecting personal biases, then they risk choosing a suboptimal, or uncoordinated joint action. The general problem of equilibrium selection <ref> [13] </ref> can be addressed in several ways. For instance, communication between agents might be admitted [22] or one could impose conventions or rules that restrict behavior so as to ensure coordination [18].
Reference: [14] <author> H. Robbins and S. Munro. </author> <title> A stochastic approximation method. </title> <journal> Annals Math. Stat., </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: This implies that each agent can observe the forming action a in state s, and incorporates consideration of the values of possible states s 0 to which action a leads. This learning method is, in fact, a basic stochastic approximation technique <ref> [14] </ref>. We use (perhaps, misuse) the Q notation and terminology to emphasize the connection with action selection. actions of other agents.
Reference: [15] <author> T. Sandholm and R. Crites. </author> <title> Learning in the iterated prisoner's dilemma. </title> <journal> Biosystems, </journal> <volume> 37 </volume> <pages> 147-166, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 20, 16, 11, 7, 15] </ref>. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> Naive application of Q-learning to MASs can be successful if we can ensure that each agent's strategy will eventually settle. This is one of the questions we explore below. Application of Q-learning and other RL methods have met with some success in the past <ref> [22, 16, 17, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [16] <author> S. Sen, M. Sekaran, J. Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> AAAI-94, </booktitle> <address> pp.426-431, Seattle, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 20, 16, 11, 7, 15] </ref>. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention [22, 20, 16, 11, 7, 15]. As noted in <ref> [16] </ref>, using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> Naive application of Q-learning to MASs can be successful if we can ensure that each agent's strategy will eventually settle. This is one of the questions we explore below. Application of Q-learning and other RL methods have met with some success in the past <ref> [22, 16, 17, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1). <p> Convergence guarantees are not especially practical for complex games, but new exploration heuristics may help in this regard. Several proposals have been put forth that are closely related to ours. Tan [20] and Sen, Sekaran and Hale <ref> [16] </ref> apply RL to independent agents and demonstrate empirical convergence. These results are consistent with ours, but properties of the convergence points (whether they are optimal or even in equilibrium are not considered). Wheeler and Naren-dra [23] develop a learning automata (LA) model for fully cooperative games. <p> However, the connections between the two models deserve further exploration. A number of important directions remain to be pursued. The most obvious is the generalization of these ideas to general, multistate, sequential problems for which Q-learning is designed (for instance, as addressed in <ref> [20, 16] </ref>. An interesting issue that emerges when one tries to directly apply fictitious play models to such a setting is estimating the value of actions using the Q-values of future states when the actual future value obtained can hinge on coordination (or lack thereof) at these future states.
Reference: [17] <author> Y. Shoham and M. Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: Initial experimental results and observations. </title> <address> KR-92, pp.225-231, Cambridge, </address> <year> 1992. </year>
Reference-contexts: Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 9, 11]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [17, 10, 24] </ref>.) we will see that interesting issues emerge. One especially simple, yet often effective, learning model for achieving coordination is fictitious play [3, 5]. <p> Naive application of Q-learning to MASs can be successful if we can ensure that each agent's strategy will eventually settle. This is one of the questions we explore below. Application of Q-learning and other RL methods have met with some success in the past <ref> [22, 16, 17, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [18] <author> Y. Shoham and M. Tennenholtz. </author> <title> On the synthesis of useful so cial laws for artificial agent societies. </title> <booktitle> Proc. AAAI-92, </booktitle> <address> pp.276-281, San Jose, </address> <year> 1992. </year>
Reference-contexts: The general problem of equilibrium selection [13] can be addressed in several ways. For instance, communication between agents might be admitted [22] or one could impose conventions or rules that restrict behavior so as to ensure coordination <ref> [18] </ref>.
Reference: [19] <author> S. Singh, T. Jaakkola, M. L. Littman, and C. Szepesvari. </author> <title> Con vergence results for single-step on-policy reinforcement learning algorithms. </title> <booktitle> Machine Learning, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: popular biased strategy is Boltzmann exploration: action a is chosen with probability e Q (a)=T a 0 e Q (a 0 )=T (2) The temperature parameter T can be decreased over time so that the exploitation probability increases (and can be done in such a way that convergence is assured <ref> [19] </ref>). The existence of multiple agents, each simultaneously learning, is a potential impediment to the successful employment of Q-learning (or RL generally) in multiagent settings. When agent i is learning the value of its actions in the presence of other agents, it is learning in a nonstationary environment.
Reference: [20] <author> M. Tan. </author> <title> Multi-agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents. Proc. 10th Intl. Conf. on Machine Learning, </booktitle> <address> pp.330-337, Amherst, MA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 20, 16, 11, 7, 15] </ref>. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> Convergence guarantees are not especially practical for complex games, but new exploration heuristics may help in this regard. Several proposals have been put forth that are closely related to ours. Tan <ref> [20] </ref> and Sen, Sekaran and Hale [16] apply RL to independent agents and demonstrate empirical convergence. These results are consistent with ours, but properties of the convergence points (whether they are optimal or even in equilibrium are not considered). <p> However, the connections between the two models deserve further exploration. A number of important directions remain to be pursued. The most obvious is the generalization of these ideas to general, multistate, sequential problems for which Q-learning is designed (for instance, as addressed in <ref> [20, 16] </ref>. An interesting issue that emerges when one tries to directly apply fictitious play models to such a setting is estimating the value of actions using the Q-values of future states when the actual future value obtained can hinge on coordination (or lack thereof) at these future states.
Reference: [21] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learn ing, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: The use of reinforcement learning (RL), in particular, has attracted recent attention [22, 20, 16, 11, 7, 15]. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning <ref> [21] </ref>, have been applied directly to MASs with some success. However, a general understanding of the conditions under which RL can be usefully applied, and exactly what form RL might take in MASs, are problems that have not yet been tackled in depth. <p> In such a case, reinforcement learning can be used by the agents to estimate, based on past experience, the expected reward associated with individual or joint actions. We refer to [8] for a survey of RL techniques. A simple, well-understood algorithm for single agent learning is Q-learning <ref> [21] </ref>. The formulation of Q-learning for general sequential decision processes is more sophisticated than we need here. In our stateless setting, we assume a Q-value, Q (a), that provides an estimate of the value of performing (individual or joint) action a. <p> Here is the learning rate (0 1), governing to what extent the new sample replaces the current estimate. If is decreased slowly during learning and all actions are sampled infinitely, Q-learning will converge to true Q-values for all actions in the single agent setting <ref> [21] </ref>. 2 2 Generally, Q (a; s) is taken to denote the long-term value of per Convergence of Q-learning does not depend on the exploration strategy used. An agent can try its actions at any timethere is no requirement to perform actions that are currently estimated to be best.
Reference: [22] <author> G. Wei. </author> <title> Learning to coordinate actions in multi-agent sys tems. </title> <booktitle> Proc. IJCAI-93, </booktitle> <address> pp.311-316, Chambery, FR, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction The application of learning to the problem of coordination in multiagent systems (MASs) has become increasingly popular in AI and game theory. The use of reinforcement learning (RL), in particular, has attracted recent attention <ref> [22, 20, 16, 11, 7, 15] </ref>. As noted in [16], using RL as a means of achieving coordinated behavior is attractive because of its generality and robustness. Standard techniques for RL, for example, Q-learning [21], have been applied directly to MASs with some success. <p> If they choose them randomly, or in some way reflecting personal biases, then they risk choosing a suboptimal, or uncoordinated joint action. The general problem of equilibrium selection [13] can be addressed in several ways. For instance, communication between agents might be admitted <ref> [22] </ref> or one could impose conventions or rules that restrict behavior so as to ensure coordination [18]. <p> Naive application of Q-learning to MASs can be successful if we can ensure that each agent's strategy will eventually settle. This is one of the questions we explore below. Application of Q-learning and other RL methods have met with some success in the past <ref> [22, 16, 17, 15] </ref>. There are two distinct ways in which Q-learning could be applied to a multiagent system. We say a MARL algorithm is an independent learner (IL) algorithm if the agents learn Q-values for their individual actions based on Equation (1).
Reference: [23] <author> R. M. Wheeler and K. S. Narendra. </author> <title> Decentralized learning in Markov chains. </title> <journal> IEEE Trans. Aut. Control, </journal> <volume> 31 </volume> <pages> 519-526, </pages> <year> 1951. </year>
Reference-contexts: Tan [20] and Sen, Sekaran and Hale [16] apply RL to independent agents and demonstrate empirical convergence. These results are consistent with ours, but properties of the convergence points (whether they are optimal or even in equilibrium are not considered). Wheeler and Naren-dra <ref> [23] </ref> develop a learning automata (LA) model for fully cooperative games. They show that using this model agents will converge to equilibrium if there is a unique pure strategy equilibrium; thus the coordination problem that interests us here is not addressed directly.
Reference: [24] <author> H. Peyton Young. </author> <title> The evolution of conventions. </title> <journal> Economet rica, </journal> <volume> 61(1) </volume> <pages> 57-84, </pages> <year> 1993. </year>
Reference-contexts: Independent learners (ILs) apply Q-learning in the classic sense, ignoring the existence of other agents. Joint action learners (JALs), in contrast, learn the value of their own actions in conjunction with those of other agents via integration of RL with equilibrium (or coordination) learning methods <ref> [24, 5, 6, 9] </ref>. We then briefly consider the importance of exploitive exploration strategies and examine, through a series of examples, how game structure and exploration strategies influence the dynamics of the learning process and the convergence to equilibrium. <p> Here we entertain the suggestion that coordinated action choice might be learned through repeated play of the game with the same agents [5, 6, 9, 11]. (Repeated play with a random selection of similar agents from a large population has also been the object of considerable study <ref> [17, 10, 24] </ref>.) we will see that interesting issues emerge. One especially simple, yet often effective, learning model for achieving coordination is fictitious play [3, 5]. <p> Note that this type of action selection runs counter to the requirement that a best response be cho 7 Fictitious play based on histories of an appropriately chosen length is shown to converge in <ref> [24] </ref>. 8 One could imagine that an IL might bias its action selection toward those whose Q-values have high variance, or adhere to a multimodal distribution, perhaps indicative of another agent acting simultaneously; but this seems to run contrary to the spirit of ILs. sen except for random exploration.
References-found: 24

