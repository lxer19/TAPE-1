URL: http://now.cs.berkeley.edu/clumps/kaili.ps
Refering-URL: http://now.cs.berkeley.edu/clumps/index.html
Root-URL: 
Title: Thread Scheduling for Cache Locality  
Author: James Philbin and Jan Edler Otto J. Anshus Craig C. Douglas Kai Li 
Address: 4 Independence Way, Princeton, NJ 08540  N-9037 Tromso, Norway  P.O. Box 218, Yorktown Heights, NY 10598-0218; and  P.O. Box 208285, New Haven, CT 06520-8285  Princeton, NJ 08540  
Affiliation: NEC Research Institute,  Department of Computer Science, Institute of Mathematical and Physical Sciences, University of Tromso,  IBM T.J. Watson Research Center,  Department of Computer Science, Yale University,  Department of Computer Science, Princeton University,  
Abstract: This paper describes a method to improve the cache locality of sequential programs by scheduling fine-grained threads. The algorithm relies upon hints provided at the time of thread creation to determine a thread execution order likely to reduce cache misses. This technique may be particularly valuable when compiler-directed tiling is not feasible. Experiments with several application programs, on two systems with different cache structures, show that our thread scheduling method can improve program performance by reducing second-level cache misses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah, D.J. Kuck, and D.H. Lawrie. </author> <title> Automatic Program Transformations for Virtual Memory Computers. </title> <booktitle> In Proceedings of the 1979 National Computer Conference, </booktitle> <pages> pages 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: One attractive way to ameliorate the processor/memory performance gap is to improve the data locality of applications. Tiling (also called blocking), a well-known software technique <ref> [1, 12, 18, 29] </ref>, achieves this goal by restructuring a program to re-use certain blocks of data that fit in the cache. Tiling can reduce cache misses and can be applied to any level of the memory hierarchy, including virtual memory, caches, and registers. <p> We call this version transposed. The threaded version is produced by substituting the inner-most loop with a thread: for i = 1 to n th fork (DotP roduct; i; j; A <ref> [1; i] </ref>; B [1; j]); th run (0); DotP roduct (i; j) : for k = 1 to n We call this version threaded. We have looked at several compiler tiled matrix multiplication programs. <p> We call this version transposed. The threaded version is produced by substituting the inner-most loop with a thread: for i = 1 to n th fork (DotP roduct; i; j; A [1; i]; B <ref> [1; j] </ref>); th run (0); DotP roduct (i; j) : for k = 1 to n We call this version threaded. We have looked at several compiler tiled matrix multiplication programs. <p> Early research efforts have proposed ways of rearranging data structures and altering algorithms to reduce page faulting in virtual memory <ref> [16, 1] </ref>. Tiling has become a well-known software technique for using the memory hierarchy effectively [18, 29]. Tiling can be applied to any levels of memory hierarchy, including virtual memory, caches, and registers.
Reference: [2] <author> A. Agarwal, B. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month>
Reference-contexts: Scheduling on multiprocessors based on cache affinity is a technique that maintains processor/thread locality on multiprocessors [38]. Affinity scheduling avoids cache fills and invalidations on cache coherent multiprocessors when threads are allowed to migrate among processors. Multi-threaded architectures have been proposed and implemented that support multi-threaded parallel programs <ref> [2, 3, 26, 28, 39] </ref>. They typically have architectural support for scheduling threads in one or a few CPU cycles in order to hide various latencies in pipelines, memory references, and network transactions. The success of this approach relies on creating threads efficiently at either compile time or run time.
References-found: 2

