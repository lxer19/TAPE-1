URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/roc/public/ICSE97-Final.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/roc/public/www/Lackwit.html
Root-URL: http://www.cs.cmu.edu
Phone: +1 412 268 5728 +1 412 268 5143  
Title: Lackwit: A Program Understanding Tool Based on Type Inference  
Author: Robert OCallahan Daniel Jackson 
Keyword: restructuring, abstraction, C, representation  
Address: 5000 Forbes Avenue 5000 Forbes Avenue Pittsburgh, PA 15213 USA Pittsburgh, PA 15213 USA  
Affiliation: School of Computer Science School of Computer Science Carnegie Mellon University Carnegie Mellon University  
Abstract: By determining, statically, where the structure of a program requires sets of variables to share a common representation, we can identify abstract data types, detect abstraction violations, find unused variables, functions, and fields of data structures, detect simple errors in operations on abstract datatypes, and locate sites of possible references to a value. We compute representation sharing with type inference, using types to encode representations. The method is efficient, fully automatic, and smoothly integrates pointer aliasing and higher-order functions. We show how we used a prototype tool to answer a users questions about a 17,000 line program written in C. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Robert W. Bowdidge and William G. Griswold. </author> <title> Automated support for encapsulating abstract data types. </title> <booktitle> Proc. ACM SIGSOFT Conf. On Foundations of Software Engineering, </booktitle> <address> New Orleans, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Our extension with vectors of attributes is a simple case of their unions of prime types. Unlike us, they do not try to distinguish different occurrences of the same type constructor. Bowdidge and Griswolds star diagram tool aids in encapsulating abstract data types <ref> [1] </ref>. They assume that there is a single global variable to be abstracted, but they discuss extending their method to operate on data structures with multiple instances.
Reference: 2. <author> F. Cardone and M. Coppo. </author> <title> Type inference with recursive types: syntax and semantics. </title> <journal> Information and Computation, 1992, </journal> <volume> number 1, </volume> <pages> pp. 48-80. </pages>
Reference-contexts: Ignoring the b tags (described below), this is a completely standard polymorphic type system. We use the standard inference algorithm W <ref> [9, 2] </ref> to compute the types of all variables of a source program (with no initial type declarations). Roughly speaking, W works by assigning variables to types and unifying type expressions as the program structure requires, in the manner described informally above. <p> Similarly, comparing the tags on function types with the tags on declared functions gives us an analysis of higher-order control flow. 7 Actually in the polymorphic type system we must use a more complicated relation than just tag equality; see below. regular trees (see Cardone and Coppo <ref> [2] </ref> for details). We do not use polymorphic recursion; that is, let and letrec bindings are the only places where we perform polymorphic generalization. We use a value restriction on polymorphic lets to make side-effects safe [16]. Our representation tags correspond closely to the region variables of region inference [15].
Reference: 3. <author> L. Damas and R. Milner. </author> <title> Principal type schemes for functional programs. </title> <booktitle> Proceedings of the Ninth Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1982, </year> <pages> pp. 207-212. </pages>
Reference: 4. <author> Michael D. Ernst. </author> <title> Practical fine-grained static slicing of optimized code. </title> <type> Technical report MSR-TR-94-14, </type> <institution> Microsoft Research, Microsoft Corporation, Redmond, </institution> <month> July </month> <year> 1994. </year> <month> &lt;ftp://ftp.research.microsoft.com/pub/tech-reports/Summer94/TR-94-14.ps&gt; </month>
Reference-contexts: Dataflow-based tools such as the VDG slicer <ref> [4] </ref> and Chopshop [8] do not scale to handle very large programs.
Reference: 5. <author> D. Evans, J. Guttag, J. Horning, and Y. Tan. LCLint: </author> <title> a tool for using specifications to check code. </title> <booktitle> Proc. ACM SIGSOFT Conf. On Foundations of Software Engineering, </booktitle> <address> New Orleans, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Dataflow-based tools such as the VDG slicer [4] and Chopshop [8] do not scale to handle very large programs. Code checking tools such as LCLint <ref> [5] </ref> do not try to present high-level views of a large system. (Furthermore, although LCLint does compute some semantic information, it does not have a framework for accurate global analysis.) EXAMPLE Consider the trivial program in Figure 4. <p> Our analysis is more powerful than that incorporated in their Rigi tool, but we would certainly benefit greatly from such visualization and manipulation techniques. LCLint <ref> [5] </ref> is a tool that finds inconsistencies between C programs and simple specifications.
Reference: 6. <author> E. Gansner, S. North and K. Vo. </author> <title> DAG a graph drawing program. </title> <journal> Software Practice and Experience, </journal> <volume> volume 18, number 11, </volume> <month> November </month> <year> 1988, </year> <pages> pp. 1055-1063. </pages>
Reference-contexts: The query engine outputs relational tables in a text format. A Perl script converts these tables to a graph, in the process performing the postprocessing analyses described above. We use the dot graph-drawing tool <ref> [6] </ref> to produce the graph. Performance is currently good, but could be greatly improved. In particular, the recursive-descent parser should be rewritten as an LALR parser for speed.
Reference: 7. <author> A. Hume. </author> <title> A tale of two greps. </title> <journal> Software Practice and Experience, </journal> <volume> volume 18, number 11, </volume> <month> November </month> <year> 1988, </year> <pages> pp. 1063-1072. </pages>
Reference: 8. <author> Daniel Jackson and Eugene Rollins. </author> <title> Abstractions of program dependencies for reverse engineering. </title> <booktitle> Proc. ACM SIGSOFT Conf. On Foundations of Software Engineering, </booktitle> <address> New Orleans, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Dataflow-based tools such as the VDG slicer [4] and Chopshop <ref> [8] </ref> do not scale to handle very large programs. <p> See our technical report <ref> [8] </ref> for details. e 1 :(t, t) fi t e 2 :t e 3 :t (Binary Function Call) e 1 (e 2 , e 3 ):t THE TAGS Note that the type expressions given in the productions above include superscripted tags marked b.
Reference: 9. <author> Robin Milner. </author> <title> A theory of type polymorphism in programming. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 17 </volume> <pages> 348-375, </pages> <year> 1978. </year>
Reference-contexts: Type inference is an attractive basis for an analysis for many reasons. It is fully automatic. It elegantly handles complex features of rich source languages like C, such as recursive pointer-based data structures and function pointers. Because our type system is a straightforward elaboration of a standard type system <ref> [9] </ref>, we can employ the standard inference algorithm with only minor adaptation, and can be confident in the soundness of our scheme. Although the algorithm is known to have doubly-exponential time complexity in the worst case, in practice its performance is excellent. <p> Viewing the representations themselves as types, this is the problem of type inference <ref> [9] </ref>. In the program above, type inference would proceed as follows. <p> Ignoring the b tags (described below), this is a completely standard polymorphic type system. We use the standard inference algorithm W <ref> [9, 2] </ref> to compute the types of all variables of a source program (with no initial type declarations). Roughly speaking, W works by assigning variables to types and unifying type expressions as the program structure requires, in the manner described informally above.
Reference: 10. <author> Gail Murphy and David Notkin. </author> <title> Lightweight source model extraction. </title> <booktitle> Proc. ACM SIGSOFT Conf. On Foundations of Software Engineering, </booktitle> <year> 1995. </year>
Reference: 11. <author> H. Mller, S. Tilley, M. Orgun, B. Corrie and N. Madhavji. </author> <title> A reverse engineering environment based on spatial and visual software interconnection models. </title> <booktitle> In SIGSOFT '92: Proceedings of the Fifth ACM SIGSOFT Symposium on Software Development Environments (Tyson's Corner, </booktitle> <address> Virginia; December 9-11, </address> <year> 1992), </year> <pages> pages 88-98, </pages> <month> December </month> <year> 1992. </year> <booktitle> In ACM Software Engineering Notes, </booktitle> <pages> 17(5). </pages>
Reference-contexts: Our system has advantages over all other tools we know of for analyzing source code for program understanding; see Reflection Model Tool <ref> [11] </ref> lack semantic depth in that they fail to capture effects such as aliasing that are vital for understanding the manipulation of data in large programs. Dataflow-based tools such as the VDG slicer [4] and Chopshop [8] do not scale to handle very large programs. <p> Our method provides an answer to this problem. Muller et al. <ref> [11] </ref> have proposed a reverse engineering technique in which first a static analysis is performed, and then the graphical output is visualized and manipulated by the user with the help of various automatic tools, to reveal and impose structure.
Reference: 12. <author> R. OCallahan and D. Jackson. </author> <title> Practical Program Understanding with Type Inference. </title> <type> Technical Report CMU-CS-96-130, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <month> May </month> <year> 1996. &lt;ftp://reports.adm.cs.cmu.edu/usr/anon/1996/CMU-CS-96-130.ps&gt; </year>
Reference-contexts: W is defined for a simple functional In fact, we use a number type that includes all integer and floating-point values, but int simplifies the presentation. language; we provide a translation from C into such a language (see our technical report <ref> [12] </ref> for details). W computes a type for each language construct, proceeding by structural induction over the abstract syntax tree. (Constructs such as statements that have no associated value are assigned a special type.) A type is assigned to a construct by an inference rule.
Reference: 13. <author> T. Parr, H. Dietz, and W. Cohen. </author> <title> PCCTS Reference Manual (version 1.00). </title> <journal> ACM SIGPLAN Notices, </journal> <month> February </month> <year> 1992, </year> <pages> pp. 88-165. </pages>
Reference-contexts: These are a genuine problem, because these pointers are updated in a frequently-executed loop. CURRENT STATUS AND FUTURE WORK The Lackwit front end is currently written in C and C++, and is based on the PCCTS toolkit <ref> [13] </ref>. The database is simply a sequential binary file, implemented by hand in C. The solver and query engine are also written in C; currently the query language is very simple and a bit unwieldy. The query engine outputs relational tables in a text format.
Reference: 14. <author> Bjarne Steensgaard. </author> <title> Points-to analysis in almost linear time. </title> <booktitle> Proceedings of the 23rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: The reader familiar with type theory may be interested in the following details. Recursive types are treated as infinite See Steensgaards work on points-to analysis <ref> [14] </ref>. <p> In particular, the recursive-descent parser should be rewritten as an LALR parser for speed. We would gain a lot of performance at the cost of flexibility and simplicity by eliminating our intermediate language and generating constraints directly from the C abstract syntax (as Steensgaard does <ref> [14] </ref>). Most importantly, ref ref (read) ref (written) ref (read and written) (arrows point from supertypes to subtypes) simplifying constraints in the front end would produce at least an order of magnitude saving in the size of the database and the processing time of the solver. <p> To our knowledge, we are the first to use these techniques for program understanding. Other researchers have been investigating type inference methods for inferring properties of C programs. In <ref> [14] </ref> Steensgaard presents a method based on type inference that yields an almost-linear time points-to analysis. That algorithm is monomorphic (context-insensitive) and does not distinguish elements of compound structures, but variants have been constructed that overcome these limitations. Wright and Cartwright [17] use polymorphic type inference to analyze Scheme programs.
Reference: 15. <author> Mads Tofte and Jean-Pierre Taplin. </author> <title> Implementation of the typed call-by-value l-calculus using a stack of regions. </title> <booktitle> Proceedings of the 21st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1994, </year> <pages> pp. 188-201. </pages>
Reference-contexts: We do not use polymorphic recursion; that is, let and letrec bindings are the only places where we perform polymorphic generalization. We use a value restriction on polymorphic lets to make side-effects safe [16]. Our representation tags correspond closely to the region variables of region inference <ref> [15] </ref>. OBTAINING AND DISPLAYING GLOBAL INFORMATION The result of the type inference phase is a mapping from source variables and functions to type signatures in our extended type system. <p> Another interesting problem is to enrich the type system to handle a wider class of source languages, for example by adding subtyping for object-oriented languages. RELATED WORK Our basic analysis technique is similar to region inference, used by Tofte and Taplin <ref> [15] </ref> to improve the space efficiency of implementations of functional languages.
Reference: 16. <author> Andrew Wright. </author> <title> Simple imperative polymorphism. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> volume 8, number 4, </volume> <month> December </month> <year> 1995, </year> <pages> pp. 343-356. </pages>
Reference-contexts: We do not use polymorphic recursion; that is, let and letrec bindings are the only places where we perform polymorphic generalization. We use a value restriction on polymorphic lets to make side-effects safe <ref> [16] </ref>. Our representation tags correspond closely to the region variables of region inference [15]. OBTAINING AND DISPLAYING GLOBAL INFORMATION The result of the type inference phase is a mapping from source variables and functions to type signatures in our extended type system.

References-found: 16

