URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/CDRS96.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Title: A comparison of feature selection algorithms in the context of rough classifiers  
Author: Suresh K. Choubey Jitender S. Deogun Vijay V. Raghavan Hayri Sever 
Abstract: In this paper, we study the feature selection problem and develop and analyze four algorithms for feature selection in the context of rough set methodology. The initial state and the feasibility criterion of all these algorithms are the same, that is, they start from a given feature set and progressively remove features, while controlling the amount of degradation in classification quality, but differ in the heuristic used for pruning the search space of features. Our experimental results confirm the analytical results on the complexity of algorithms as well as on controlled degradation of upper classification. The algorithms presented can be used with any methods of deriving a classifier where the quality of classification is monotonically decreasing function while feature set is reduced, though we have adopted the upper classifier in our study. The upper classifier has some important features that makes it suitable for database mining applications. In particular, we have shown that the upper classifier can be summarized at a desired level of abstraction by using extended decision tables. We also point out that an inconsistent decision algorithm can be interpreted as if it were a consistent decision algorithm. Keywords- Rough sets, feature selection, upper classifier, database mining. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Kira and L. Rendell, </author> <title> "The feature selection problem: Tradational methods and a new algorithm," </title> <booktitle> in Proceedings of AAAI-92, </booktitle> <pages> pp. 129-134, </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: PERSON raghavan@cacs.usl.edu, The Center for Advanced Computer Studies, University of SW Louisiana,Lafayette, LA 70504, USA x sever@eti.cc.hun.edu.tr, The Department of Computer Science, Hacettepe University, 06532 Beytepe, Ankara, TR ing and classifying objects, reducing the cost of classification (e.g., eliminating redundant tests in medical diagnosis), and improving the quality of classification <ref> [1] </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J ); where: l is the number of features, and J is the computational effort required to evaluate each subset. <p> When the description of a given object does not match to known concepts we use 5NNR classification scheme with Euclidean distance function to determine closest known concept. The difference between two values of an attribute are computed as suggested in Relief algorithm <ref> [1] </ref>; that is, the difference between two non-quantitative values is one if they are different and zero otherwise, and the difference between two quantitative values is normalized into the interval [0,1]. We first consider results from Table 2.
Reference: [2] <author> M. James, </author> <title> Classification Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: This type of exhaustive search would be appropriate only if l is small and J is com-putationally inexpensive. Greedy approaches like Sequential backward/forward techniques <ref> [2, 3] </ref>, and dynamic programming [4] are some of the efficient search techniques applied with some feature selection criterion.
Reference: [3] <author> M. Modrzejewski, </author> <title> "Feature selection using rough sets theory," </title> <booktitle> in Machine Learning: Proceedings of ECML-93 (P. </booktitle> <editor> B. Brazdil, </editor> <publisher> ed.), </publisher> <pages> pp. 213-226, </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: This type of exhaustive search would be appropriate only if l is small and J is com-putationally inexpensive. Greedy approaches like Sequential backward/forward techniques <ref> [2, 3] </ref>, and dynamic programming [4] are some of the efficient search techniques applied with some feature selection criterion.
Reference: [4] <author> C. Y. Chang, </author> <title> "Dynamic programming as applied to feature subset selection in a pattern recognition system," </title> <journal> IEEE Trans. Syst., Man, Cybern., </journal> <volume> vol. SMC-3, </volume> <pages> pp. 166-171, </pages> <year> 1973. </year>
Reference-contexts: This type of exhaustive search would be appropriate only if l is small and J is com-putationally inexpensive. Greedy approaches like Sequential backward/forward techniques [2, 3], and dynamic programming <ref> [4] </ref> are some of the efficient search techniques applied with some feature selection criterion.
Reference: [5] <author> R. A. Devijver and J. Kittler, </author> <title> Pattern Recogna-tion: A statistical approach. </title> <publisher> London: Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: For near-optimal solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics <ref> [5, 6] </ref>; entropy or classification accuracy in pattern recognition and machine learning [7, 8, 9]; classification quality based on variations of MZ metric in information retrieval systems [10]. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We present four feature selection algorithms of polynomial time complexity.
Reference: [6] <author> A. J. Miller, </author> <title> Subset Selection in Regression. </title> <publisher> Chap-man and Hall, </publisher> <year> 1990. </year>
Reference-contexts: For near-optimal solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics <ref> [5, 6] </ref>; entropy or classification accuracy in pattern recognition and machine learning [7, 8, 9]; classification quality based on variations of MZ metric in information retrieval systems [10]. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We present four feature selection algorithms of polynomial time complexity.
Reference: [7] <author> S. K. Pal and B. Chakraborty, </author> <title> "Fuzzy set theoretic measure for automatic feature evaluation," </title> <journal> IEEE Trans. Syst., Man, Cybern., </journal> <volume> vol. SMC-16, no. 5, </volume> <pages> pp. 754-760, </pages> <year> 1986. </year>
Reference-contexts: solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [5, 6]; entropy or classification accuracy in pattern recognition and machine learning <ref> [7, 8, 9] </ref>; classification quality based on variations of MZ metric in information retrieval systems [10]. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We present four feature selection algorithms of polynomial time complexity.
Reference: [8] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [5, 6]; entropy or classification accuracy in pattern recognition and machine learning <ref> [7, 8, 9] </ref>; classification quality based on variations of MZ metric in information retrieval systems [10]. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We present four feature selection algorithms of polynomial time complexity.
Reference: [9] <author> U. M. Fayyad and K. B. Irani, </author> <title> "The attribute selection problem in decision tree generation," </title> <booktitle> in Proceedings of AAAI-92, </booktitle> <pages> pp. 104-110, </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [5, 6]; entropy or classification accuracy in pattern recognition and machine learning <ref> [7, 8, 9] </ref>; classification quality based on variations of MZ metric in information retrieval systems [10]. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We present four feature selection algorithms of polynomial time complexity.
Reference: [10] <author> P. Bollmann and V. S. Cherniavsky, </author> <title> "Measurement theoretical investigation of the mz-metric," </title> <note> in Information Retrieval Research (R. </note> <editor> N. Oddy, S. E. Robertson, C. J. van Rijsbergen, and R. W. Williams, </editor> <booktitle> eds.), </booktitle> <pages> pp. 256-267, </pages> <address> Boston: Butter-worths, </address> <year> 1981. </year>
Reference-contexts: features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [5, 6]; entropy or classification accuracy in pattern recognition and machine learning [7, 8, 9]; classification quality based on variations of MZ metric in information retrieval systems <ref> [10] </ref>. In such procedures, irrelevant features are either eliminated or assigned small coefficients. We present four feature selection algorithms of polynomial time complexity. The objective is to find a small subset of features that are sufficient and necessary to define target concepts with respect to a given threshold.
Reference: [11] <author> S. Salzberg, </author> <title> "Improving classification methods via feature selection," </title> <type> Tech. Rep. </type> <institution> JHU-92/12, Johns Hopkins University, Department of Computer Science, </institution> <month> Jun </month> <year> 1992. </year> <month> (revised April </month> <year> 1993). </year>
Reference-contexts: The threshold value indicates how much degradation one is willing to allow in the quality of classification. Controlled threshold value is inspired by Salzberg's CSS algorithm <ref> [11] </ref>. Even though our feature selection algorithms are developed as a preprocessing stage for rough classifiers, they can certainly be integrated to any other data analysis technique.
Reference: [12] <editor> R. Slowinski, ed., </editor> <booktitle> Intelligent Decision Support-Handbook of Advances and Applications of the Rough Set Theory. </booktitle> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The theory of rough sets in either algebraic or probabilistic approximation spaces has been used for a number of real life applications; namely, in medicine, pharmacology, industry, engineering, control systems, social sciences, switching circuits, image processing, etc., <ref> [12, 13] </ref>. In this article we consider classification meth 1 ods only in algebraic approximation spaces, which do not require any preliminary or additional information about data, as opposed to rough sets in probabilistic approximation spaces.
Reference: [13] <author> V. V. Raghavan and H. </author> <title> Sever, "The state of rough sets for database mining applications," </title> <booktitle> in Proceedings of 23rd Computer Science Conference Workshop on Rough Sets and Database Mining (T. </booktitle> <editor> Y. Lin, ed.), </editor> <address> (San Jose State University, San Jose, CA), </address> <pages> pp. 1-11, </pages> <month> mar </month> <year> 1995. </year>
Reference-contexts: The theory of rough sets in either algebraic or probabilistic approximation spaces has been used for a number of real life applications; namely, in medicine, pharmacology, industry, engineering, control systems, social sciences, switching circuits, image processing, etc., <ref> [12, 13] </ref>. In this article we consider classification meth 1 ods only in algebraic approximation spaces, which do not require any preliminary or additional information about data, as opposed to rough sets in probabilistic approximation spaces.
Reference: [14] <author> C. J. Matheus, P. K. Chan, and G. Piatetsky-Shapiro, </author> <title> "Systems for knowledge discovery in databases," </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 903-912, </pages> <year> 1993. </year>
Reference-contexts: We use upper classifiers and decision tables to address some aspects of very large data that can be listed as redundant, incomplete, noisy, and dynamic data <ref> [14] </ref>. In the rough set literature, the terms `inconsistent' and `nondeterministic' decision algorithms (or rules) are used interchangeably [15, 16], though they are different concepts. As shown in Deogun et al. [17], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterminis-tically.
Reference: [15] <author> R. Slowinski and J. Stefanowiski, </author> <title> "Rough classification with valued closeness relation," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (San Jose, CA), </address> <year> 1995. </year>
Reference-contexts: We use upper classifiers and decision tables to address some aspects of very large data that can be listed as redundant, incomplete, noisy, and dynamic data [14]. In the rough set literature, the terms `inconsistent' and `nondeterministic' decision algorithms (or rules) are used interchangeably <ref> [15, 16] </ref>, though they are different concepts. As shown in Deogun et al. [17], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterminis-tically. This is an important result, particularly when the background knowledge is incomplete and dynamic.
Reference: [16] <author> Z. Pawlak, </author> <title> "On learning- a rough set approach," </title> <booktitle> in Lecture Notes, </booktitle> <volume> vol. 208, </volume> <pages> pp. 197-227, </pages> <publisher> Springer Verlag, </publisher> <year> 1986. </year>
Reference-contexts: We use upper classifiers and decision tables to address some aspects of very large data that can be listed as redundant, incomplete, noisy, and dynamic data [14]. In the rough set literature, the terms `inconsistent' and `nondeterministic' decision algorithms (or rules) are used interchangeably <ref> [15, 16] </ref>, though they are different concepts. As shown in Deogun et al. [17], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterminis-tically. This is an important result, particularly when the background knowledge is incomplete and dynamic.
Reference: [17] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever, "Rough set based classification methods and extended decision tables," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, California), </address> <pages> pp. 302-309, </pages> <year> 1994. </year>
Reference-contexts: In the rough set literature, the terms `inconsistent' and `nondeterministic' decision algorithms (or rules) are used interchangeably [15, 16], though they are different concepts. As shown in Deogun et al. <ref> [17] </ref>, inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterminis-tically. This is an important result, particularly when the background knowledge is incomplete and dynamic. <p> We use EDT to represent a decision algorithm that is induced such that the ante-cent part of each rule corresponds to only one elementary set in the base decision table. Details of EDT are omitted for lack of space and they can be found in <ref> [17] </ref>. The important thing we would like to point out is that EDT can be useful in three places. First, EDT enables us compute the accuracy measure of a decision rule.
Reference: [18] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever, "Exploiting upper approximations in the rough set methodology," </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> (Montreal, Quebec, Canada), </address> <pages> pp. 1-10, </pages> <month> aug </month> <year> 1995. </year>
Reference-contexts: As shown in Deogun et al. [17], inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeterminis-tically. This is an important result, particularly when the background knowledge is incomplete and dynamic. We have proposed in Deogun et al. <ref> [18] </ref>, ways to improve upper classifiers one of the classification methods in rough set theory. The enhancement is achieved by a sequential backward feature selection algorithm to preprocess a given set of features. This is important because rough classification methods are incapable of removing superfluous features. <p> We also proved that the sequential backward selection algorithms find a small subset of relevant features that are ideally sufficient and necessary to define target concepts with respect to a given threshold. This threshold value indicates acceptable degradation of classification quality. In <ref> [18] </ref>, only one heuristic was reported. <p> To justify that BF S algorithm finds a -reduct of CON in S; we recall that the quality of upper classifier decreases as the feature set is pruned down <ref> [18, 19] </ref>. In BFS algorithm, we initialize the current node to the root node in the beginning. Here the state of root node consists of the given set of features and quality of classification using these features.
Reference: [19] <author> H. </author> <title> Sever, Knowledge Structuring for Database Mining and Text Retrieval Using Past Optimal Queries. </title> <type> PhD thesis, </type> <institution> The University of Southwestern Louisiana, </institution> <year> 1995. </year>
Reference-contexts: To justify that BF S algorithm finds a -reduct of CON in S; we recall that the quality of upper classifier decreases as the feature set is pruned down <ref> [18, 19] </ref>. In BFS algorithm, we initialize the current node to the root node in the beginning. Here the state of root node consists of the given set of features and quality of classification using these features. <p> Let F be the computational effort to compute theta-superfluous in given S, then, BFS was shown to be of order O (l 2 ); complexity <ref> [19] </ref>, where l = jCON j. For the HHS algorithm, in the beginning, the current node is initialized to root node. The current node is expanded to its successors which have one less condition attributes than its predecessors.

References-found: 19

