URL: http://www.isi.edu/~pedro/PUBLICATIONS/pldi97.ps.Z
Refering-URL: http://www.isi.edu/~pedro/PUBLICATIONS/pldi97.html
Root-URL: http://www.isi.edu
Email: fpedro,marting@cs.ucsb.edu  
Title: Dynamic Feedback: An Effective Technique for Adaptive Computing  
Author: Pedro Diniz and Martin Rinard 
Address: Engineering I Building  Santa Barbara, CA 93106-5110  
Affiliation: Department of Computer Science  University of California, Santa Barbara  
Abstract: This paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment. We have implemented dynamic feedback in the context of a par-allelizing compiler for object-based programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For example, the best consistency protocol in a software distributed shared memory system often depends on the access pattern of the parallel program [12]. The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices <ref> [1, 2, 18, 21] </ref>. The best concrete data structure to implement a given abstract data type often depends on how it is used [14, 22].
Reference: [2] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For example, the best consistency protocol in a software distributed shared memory system often depends on the access pattern of the parallel program [12]. The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices <ref> [1, 2, 18, 21] </ref>. The best concrete data structure to implement a given abstract data type often depends on how it is used [14, 22].
Reference: [3] <author> J. Auslander, M. Philipose, C. Chambers, S. Eggers, , and B. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Application Version Size (bytes) Serial 25; 248 Barnes-Hut Original 31; 152 Dynamic 33; 648 Serial 36; 832 Water Original 46; 960 Dynamic 50; 784 Serial 36; 064 String Original 43; 616 Dynamic 45; 664 Table 1: Executable Code Sizes (bytes) We also considered using dynamic compilation <ref> [3, 11, 24] </ref> to produce the different versions of the parallel sections as they were required. Although this approach would reduce the amount of code present at any given point in time, it would significantly increase the amount of time required to switch policies in the sampling phases. <p> It must therefore postpone the decision to apply the optimization until run-time, when the information is available. 7.5 Dynamic Compilation Dynamic compilation systems enable the generation of code at run time <ref> [3, 11, 24] </ref>. Because delaying the compilation until run time provides the compiler with information about the concrete values of input parameters, the compiler may be able to generate more efficient code. Existing research has focused on providing efficient mechanisms for dynamic compilation.
Reference: [4] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: The applications are Barnes-Hut <ref> [4] </ref>, a hierarchical N-body solver, Water [38], which simulates water molecules in the liquid state, and String [19], which builds a velocity model of the geology between two oil wells. Each application is a serial C++ program that performs a computation of interest to the scientific computing community.
Reference: [5] <author> E. Brewer. </author> <title> High-level optimization via automated statistical modeling. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Sloan Research Fellowship. To appear in Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI'97) to be held in Las Vegas, NV, June 1997. given problem often depends on the combination of input and hardware platform used to execute the algorithm <ref> [5] </ref>. In all of these cases, it is impossible to statically choose the best implementation the best implementation depends on information (such as the input data, dynamic program characteristics or hardware features) that is either difficult to extract or unavailable at compile time. <p> Brewer <ref> [5] </ref> describes a system that uses statistical modeling to automatically predict which algorithm will work best for a given combination of input and hardware platform. The different algorithms are implemented by hand, not automatically generated from a single specification.
Reference: [6] <author> C. Chambers and D. Ungar. </author> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Researchers have proposed several adaptive optimizations for improving the efficiency of dynamic dispatch. The standard mechanism is to collect data that indicates which methods tend to be invoked from which call sites, then to insert a type test that checks for common types first <ref> [6] </ref>. Dynamic type feedback is designed to direct the compiler's attention to parts of the program that would benefit from optimization [20]. Once a method has been optimized, the generated code continues to collect data that can be used to drive further optimizations and reverse poor implementation choices.
Reference: [7] <author> P. Chang, S. Mahlke, W. Chen, and W. Hwu. </author> <title> Profile-guided automatic inline expansion for C programs. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 22(5):349369, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: This information is then used to drive optimizations that inline methods based on predictions about the class of the receiver. Profiling has also been used to guide decisions to inline procedures in C programs <ref> [7] </ref>, to drive instruction scheduling algorithms [8], to help place code so as to minimize the impact on the memory hierarchy [29], to aid in register allocation [28, 39], and to direct the compiler to frequently executed parts of the program so that the compiler can apply further optimizations [13].
Reference: [8] <author> W. Chen, S. Mahlke, N. Warter, S. Anik, and W. Hwu. </author> <title> Profile-assisted instruction scheduling. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(2):151181, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: This information is then used to drive optimizations that inline methods based on predictions about the class of the receiver. Profiling has also been used to guide decisions to inline procedures in C programs [7], to drive instruction scheduling algorithms <ref> [8] </ref>, to help place code so as to minimize the impact on the memory hierarchy [29], to aid in register allocation [28, 39], and to direct the compiler to frequently executed parts of the program so that the compiler can apply further optimizations [13].
Reference: [9] <author> A. Cox and R. Fowler. </author> <title> Adaptive cache coherency for detecting migratory shared data. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Profile-based approaches collect a single aggregate set of measurements for the entire execution, and can therefore miss environment changes that take place within a single execution. 7.2 Adaptive Execution Techniques Other researchers have recognized the need to use dynamic performance data to optimize the execution <ref> [9, 35, 36] </ref>. These approaches are based on a set of control variables that parameterize a given algorithm in the implementation. An example of a control variable is the prefetch distance in an algorithm that prefetches data accessed by a loop [36].
Reference: [10] <author> P. Diniz and M. Rinard. </author> <title> Synchronization transformations for parallel computing. </title> <booktitle> In Proceedings of the Twenty-fourth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Paris, France, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: This paper describes the use of dynamic feedback in the context of a parallelizing compiler for object-based languages. The compiler generates parallel code that uses synchronization constructs to make operations execute atomically [33]. Our experimental results show that the resulting synchronization overhead can significantly degrade the performance <ref> [10] </ref>. We have developed a set of synchronization transformations and a set of synchronization optimization policies that use the transformations to reduce the synchronization overhead [10]. Unfortunately, the best policy is different for different programs, and may even vary dynamically for different parts of the same program. <p> Our experimental results show that the resulting synchronization overhead can significantly degrade the performance <ref> [10] </ref>. We have developed a set of synchronization transformations and a set of synchronization optimization policies that use the transformations to reduce the synchronization overhead [10]. Unfortunately, the best policy is different for different programs, and may even vary dynamically for different parts of the same program. <p> The synchronization constructs ensure that the operation executes atomically with respect to all other operations that access the object. 3 Synchronization Optimizations We found that, in practice, the overhead generated by the synchronization constructs often reduced the performance. We therefore developed several synchronization optimization algorithms <ref> [10] </ref>. These algorithms are designed for parallel programs, such as those generated by our compiler, that use mutual exclusion locks to implement critical regions. Each critical region acquires its mutual exclusion lock, performs its computation, then releases the lock. <p> Our synchronization optimization algorithms statically detect computations that repeatedly release and reacquire the same lock. They then apply lock elimination transformations to eliminate the intermediate release and acquire constructs <ref> [10] </ref>. The result is a computation that acquires and releases the lock only once. In effect, the optimization coalesces multiple critical regions that acquire and release the same lock multiple times into a single larger critical region that includes all of the original critical regions. <p> Our previous research produced analyses and transformations for reducing the synchronization overhead and the different synchronization optimization policies <ref> [10] </ref>. Plevyak, Zhang and Chien have developed a similar synchronization optimization technique, access region expansion, for concurrent object-oriented programs [31]. Because access region expansion is designed to reduce the overhead in sequential executions of such programs, it does not address the trade off between lock overhead and waiting overhead.
Reference: [11] <author> D. Engler. </author> <title> VCODE: A retargetable, extensible, very fast dynamic code generation system. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadel-phia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Application Version Size (bytes) Serial 25; 248 Barnes-Hut Original 31; 152 Dynamic 33; 648 Serial 36; 832 Water Original 46; 960 Dynamic 50; 784 Serial 36; 064 String Original 43; 616 Dynamic 45; 664 Table 1: Executable Code Sizes (bytes) We also considered using dynamic compilation <ref> [3, 11, 24] </ref> to produce the different versions of the parallel sections as they were required. Although this approach would reduce the amount of code present at any given point in time, it would significantly increase the amount of time required to switch policies in the sampling phases. <p> It must therefore postpone the decision to apply the optimization until run-time, when the information is available. 7.5 Dynamic Compilation Dynamic compilation systems enable the generation of code at run time <ref> [3, 11, 24] </ref>. Because delaying the compilation until run time provides the compiler with information about the concrete values of input parameters, the compiler may be able to generate more efficient code. Existing research has focused on providing efficient mechanisms for dynamic compilation.
Reference: [12] <author> B. Falsafi, A. Lebeck, S. Reinhardt, I. Schoinas, M. Hill, J. Larus, A. Rogers, and D. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The most efficient implementation of a given abstraction often depends on the environment in which it is used. For example, the best consistency protocol in a software distributed shared memory system often depends on the access pattern of the parallel program <ref> [12] </ref>. The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices [1, 2, 18, 21]. The best concrete data structure to implement a given abstract data type often depends on how it is used [14, 22].
Reference: [13] <author> M. Fernandez. </author> <title> Simple and effective link-time optimization of Modula-3 programs. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Program Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: programs [7], to drive instruction scheduling algorithms [8], to help place code so as to minimize the impact on the memory hierarchy [29], to aid in register allocation [28, 39], and to direct the compiler to frequently executed parts of the program so that the compiler can apply further optimizations <ref> [13] </ref>. Brewer [5] describes a system that uses statistical modeling to automatically predict which algorithm will work best for a given combination of input and hardware platform. The different algorithms are implemented by hand, not automatically generated from a single specification.
Reference: [14] <author> S. Freudenberger, J. Schwartz, and M. Sharir. </author> <title> Experience with the SETL optimizer. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(1):2645, </volume> <month> January </month> <year> 1983. </year>
Reference-contexts: The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices [1, 2, 18, 21]. The best concrete data structure to implement a given abstract data type often depends on how it is used <ref> [14, 22] </ref>. The best algorithm to solve a fl Pedro Diniz is sponsored by the PRAXIS XXI program administrated by JNICT Junta Nacional de Investigac ao Cient ifica e Tecnologica from Portugal, and holds a Fulbright travel grant. Martin Rinard is supported in part by an Alfred P.
Reference: [15] <author> S. C. Goldstein, K. E. Schauser, and D. E. Culler. </author> <title> Lazy Threads: Implementing a fast parallel call. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1):520, </volume> <month> August </month> <year> 1996. </year>
Reference-contexts: The goal is simply to minimize the lock overhead. 7.7 Parallel Function Calls Several researchers have developed efficient implementations for parallel function calls <ref> [15, 27, 30] </ref>. These implementations dynamically match the amount of exploited parallelism to the amount of parallelism available on the parallel hardware platform by selecting between an efficient sequential call and a full parallel call.
Reference: [16] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> gprof: a call graph execution profiler. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1982. </year>
Reference-contexts: Figure 7 presents the waiting proportion, which is the proportion of time spent in waiting overhead. 3 These data were collected using program-counter sampling to profile the execution <ref> [16, 23] </ref>. This figure clearly shows that waiting overhead is the primary cause of performance loss for this application, and that the Aggressive synchronization optimization policy generates enough false exclusion to severely degrade the performance. Water has two computationally intensive parallel sections: the INTERF section and the POTENG section.
Reference: [17] <author> D. Grove, J. Dean, C. Garret, and C. Chambers. </author> <title> Profile-guided receiver class prediction. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <address> Austin, TX, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: The program can then be recompiled, with the profiling data used to guide policy decisions in the compiler. Profiling has been used in the context of object-oriented languages to predict the most frequently occurring class of the receiver object at a given call site <ref> [17] </ref>. This information is then used to drive optimizations that inline methods based on predictions about the class of the receiver.
Reference: [18] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2):179 193, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: For example, the best consistency protocol in a software distributed shared memory system often depends on the access pattern of the parallel program [12]. The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices <ref> [1, 2, 18, 21] </ref>. The best concrete data structure to implement a given abstract data type often depends on how it is used [14, 22].
Reference: [19] <author> J. Harris, S. Lazaratos, and R. Michelena. </author> <title> Tomographic string inversion. </title> <booktitle> In 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, </booktitle> <pages> pages 8285, </pages> <year> 1990. </year>
Reference-contexts: The applications are Barnes-Hut [4], a hierarchical N-body solver, Water [38], which simulates water molecules in the liquid state, and String <ref> [19] </ref>, which builds a velocity model of the geology between two oil wells. Each application is a serial C++ program that performs a computation of interest to the scientific computing community.
Reference: [20] <author> U. Holzle and D. Ungar. </author> <title> Optimizing dynamically-dispatched calls with run-time type feedback. </title> <booktitle> In SIGPLAN '94 Conference on Programming Language Design and Implementation, SIGPLAN Notices 29(6), </booktitle> <pages> pages 326336, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: Dynamic type feedback is designed to direct the compiler's attention to parts of the program that would benefit from optimization <ref> [20] </ref>. Once a method has been optimized, the generated code continues to collect data that can be used to drive further optimizations and reverse poor implementation choices.
Reference: [21] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: For example, the best consistency protocol in a software distributed shared memory system often depends on the access pattern of the parallel program [12]. The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices <ref> [1, 2, 18, 21] </ref>. The best concrete data structure to implement a given abstract data type often depends on how it is used [14, 22].
Reference: [22] <author> G. Kiczales. </author> <title> Beyond the black box: open implementation. </title> <journal> IEEE Software, </journal> <volume> 13(1), </volume> <month> January </month> <year> 1986. </year>
Reference-contexts: The best data distribution of dense matrices in distributed memory machines depends on how the different parts of the program access the matrices [1, 2, 18, 21]. The best concrete data structure to implement a given abstract data type often depends on how it is used <ref> [14, 22] </ref>. The best algorithm to solve a fl Pedro Diniz is sponsored by the PRAXIS XXI program administrated by JNICT Junta Nacional de Investigac ao Cient ifica e Tecnologica from Portugal, and holds a Fulbright travel grant. Martin Rinard is supported in part by an Alfred P.
Reference: [23] <author> D. Knuth. </author> <title> An empirical study of FORTRAN programs. </title> <journal> Software Practice and Experience, </journal> <volume> 1:105133, </volume> <year> 1971. </year>
Reference-contexts: Figure 7 presents the waiting proportion, which is the proportion of time spent in waiting overhead. 3 These data were collected using program-counter sampling to profile the execution <ref> [16, 23] </ref>. This figure clearly shows that waiting overhead is the primary cause of performance loss for this application, and that the Aggressive synchronization optimization policy generates enough false exclusion to severely degrade the performance. Water has two computationally intensive parallel sections: the INTERF section and the POTENG section.
Reference: [24] <author> P. Lee and M. Leone. </author> <title> Optimizing ML with run-time code generation. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Program Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Application Version Size (bytes) Serial 25; 248 Barnes-Hut Original 31; 152 Dynamic 33; 648 Serial 36; 832 Water Original 46; 960 Dynamic 50; 784 Serial 36; 064 String Original 43; 616 Dynamic 45; 664 Table 1: Executable Code Sizes (bytes) We also considered using dynamic compilation <ref> [3, 11, 24] </ref> to produce the different versions of the parallel sections as they were required. Although this approach would reduce the amount of code present at any given point in time, it would significantly increase the amount of time required to switch policies in the sampling phases. <p> It must therefore postpone the decision to apply the optimization until run-time, when the information is available. 7.5 Dynamic Compilation Dynamic compilation systems enable the generation of code at run time <ref> [3, 11, 24] </ref>. Because delaying the compilation until run time provides the compiler with information about the concrete values of input parameters, the compiler may be able to generate more efficient code. Existing research has focused on providing efficient mechanisms for dynamic compilation.
Reference: [25] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: One version uses the Original policy, another uses the Bounded policy, another uses the Aggressive policy, and the final version uses dynamic feedback. We report results for the applications running on a 16 processor Stanford DASH machine <ref> [25] </ref> running a modified version of the IRIX 5.2 operating system. The programs were compiled using the IRIX 5.3 CC compiler at the -O2 optimization level. 6.1 Barnes-Hut Table 2 presents the execution times for the different versions of Barnes-Hut. Figure 4 presents the corresponding speedup curves.
Reference: [26] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime par-allelization. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 8391, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Several systems address this problem by parallelizing programs dynamically using information that is available only as the program runs. The inspector/executor approach dynamically analyzes the values in index arrays to automatically parallelize computations that access irregular meshes <ref> [26, 37] </ref>. The Jade implementation dynamically analyzes how tasks access data to exploit the concurrency in coarse-grain parallel programs [34]. Speculative approaches optimistically execute loops in parallel, rolling back the computation if the parallel execution violates the data dependences [32].
Reference: [27] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The goal is simply to minimize the lock overhead. 7.7 Parallel Function Calls Several researchers have developed efficient implementations for parallel function calls <ref> [15, 27, 30] </ref>. These implementations dynamically match the amount of exploited parallelism to the amount of parallelism available on the parallel hardware platform by selecting between an efficient sequential call and a full parallel call.
Reference: [28] <author> W. Morris. Ccg: </author> <title> a prototype coagulating code generator. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Profiling has also been used to guide decisions to inline procedures in C programs [7], to drive instruction scheduling algorithms [8], to help place code so as to minimize the impact on the memory hierarchy [29], to aid in register allocation <ref> [28, 39] </ref>, and to direct the compiler to frequently executed parts of the program so that the compiler can apply further optimizations [13]. Brewer [5] describes a system that uses statistical modeling to automatically predict which algorithm will work best for a given combination of input and hardware platform.
Reference: [29] <author> K. Pettis and D. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Profiling has also been used to guide decisions to inline procedures in C programs [7], to drive instruction scheduling algorithms [8], to help place code so as to minimize the impact on the memory hierarchy <ref> [29] </ref>, to aid in register allocation [28, 39], and to direct the compiler to frequently executed parts of the program so that the compiler can apply further optimizations [13].
Reference: [30] <author> J. Plevyak, V. Karamcheti, X. Zhang, and A. Chien. </author> <title> A hybrid execution model for fine-grained languages on distributed memory mul-ticomputers. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: The goal is simply to minimize the lock overhead. 7.7 Parallel Function Calls Several researchers have developed efficient implementations for parallel function calls <ref> [15, 27, 30] </ref>. These implementations dynamically match the amount of exploited parallelism to the amount of parallelism available on the parallel hardware platform by selecting between an efficient sequential call and a full parallel call.
Reference: [31] <author> J. Plevyak, X. Zhang, and A. Chien. </author> <title> Obtaining sequential efficiency for concurrent object-oriented languages. </title> <booktitle> In Proceedings of the Twenty-second Annual ACM Symposium on the Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Our previous research produced analyses and transformations for reducing the synchronization overhead and the different synchronization optimization policies [10]. Plevyak, Zhang and Chien have developed a similar synchronization optimization technique, access region expansion, for concurrent object-oriented programs <ref> [31] </ref>. Because access region expansion is designed to reduce the overhead in sequential executions of such programs, it does not address the trade off between lock overhead and waiting overhead.
Reference: [32] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD test: speculative run-time parallelization of loop with privatization and reduction parallelization. </title> <booktitle> In SIGPLAN '95 Conference on Programming Language Design and Implementation, SIGPLAN Notices 30(6), </booktitle> <address> San Diego, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The Jade implementation dynamically analyzes how tasks access data to exploit the concurrency in coarse-grain parallel programs [34]. Speculative approaches optimistically execute loops in parallel, rolling back the computation if the parallel execution violates the data dependences <ref> [32] </ref>. A major difference between dynamic feedback and these run-time techniques is that dynamic feedback is designed to automatically choose between several implementations that deliver the same functionality. Each implementation is equally valid, and may very well perform the best in the current environment.
Reference: [33] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A new framework for parallelizing compilers. </title> <booktitle> In SIGPLAN '96 Conference on Programming Language Design and Implementation, SIGPLAN Notices 31(6), </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <note> (An extended version of this document can be found via the author's web address http://www.cs.ucsb.edu/~fmartin,pedrog). </note>
Reference-contexts: This paper describes the use of dynamic feedback in the context of a parallelizing compiler for object-based languages. The compiler generates parallel code that uses synchronization constructs to make operations execute atomically <ref> [33] </ref>. Our experimental results show that the resulting synchronization overhead can significantly degrade the performance [10]. We have developed a set of synchronization transformations and a set of synchronization optimization policies that use the transformations to reduce the synchronization overhead [10]. <p> This code executes all of the operations in the computation in parallel. Our experimental results indicate that this approach can effectively parallelize irregular computations that manipulate dynamic, linked data structures such as trees and graphs <ref> [33] </ref>. To ensure that operations execute atomically, the compiler augments each object with a mutual exclusion lock. It then automatically inserts synchronization constructs into operations that update objects. These operations first acquire the object's lock, perform the update, then release the lock. <p> This indicates that the synchronization optimizations introduced no significant false exclusion. The reason that this application does not exhibit perfect speedup is that the compiler is unable to parallelize one section of the computation. At large numbers of processors the serial execution of this section becomes a bottleneck <ref> [33] </ref>. To investigate how the overheads of the different policies change over time, we produced a version of the application with small target sampling and production intervals. We instrumented this version to print out the measured overhead at the end of each sampling interval.
Reference: [34] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Heterogeneous Parallel Programming in Jade. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 245256, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The inspector/executor approach dynamically analyzes the values in index arrays to automatically parallelize computations that access irregular meshes [26, 37]. The Jade implementation dynamically analyzes how tasks access data to exploit the concurrency in coarse-grain parallel programs <ref> [34] </ref>. Speculative approaches optimistically execute loops in parallel, rolling back the computation if the parallel execution violates the data dependences [32]. A major difference between dynamic feedback and these run-time techniques is that dynamic feedback is designed to automatically choose between several implementations that deliver the same functionality.
Reference: [35] <author> T. Romer, D. Lee, B.Bershad, and J. Chen. </author> <title> Dynamic page mapping policies for cache conflict resolution on standard hardware. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 255 266, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Profile-based approaches collect a single aggregate set of measurements for the entire execution, and can therefore miss environment changes that take place within a single execution. 7.2 Adaptive Execution Techniques Other researchers have recognized the need to use dynamic performance data to optimize the execution <ref> [9, 35, 36] </ref>. These approaches are based on a set of control variables that parameterize a given algorithm in the implementation. An example of a control variable is the prefetch distance in an algorithm that prefetches data accessed by a loop [36].
Reference: [36] <author> R. Saavedra and D. Park. </author> <title> Improving the effectiveness of software prefetching with adaptive execution. </title> <booktitle> In Proceedings of the Parallel Architectures and Compilation Techniques (PACT'96), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Profile-based approaches collect a single aggregate set of measurements for the entire execution, and can therefore miss environment changes that take place within a single execution. 7.2 Adaptive Execution Techniques Other researchers have recognized the need to use dynamic performance data to optimize the execution <ref> [9, 35, 36] </ref>. These approaches are based on a set of control variables that parameterize a given algorithm in the implementation. An example of a control variable is the prefetch distance in an algorithm that prefetches data accessed by a loop [36]. <p> These approaches are based on a set of control variables that parameterize a given algorithm in the implementation. An example of a control variable is the prefetch distance in an algorithm that prefetches data accessed by a loop <ref> [36] </ref>. Typically, the programmer defines the set of observable variables and a feedback function that uses the observable variables to produce values for the control variables.
Reference: [37] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 3(6):573592, </volume> <month> De-cember </month> <year> 1991. </year>
Reference-contexts: Several systems address this problem by parallelizing programs dynamically using information that is available only as the program runs. The inspector/executor approach dynamically analyzes the values in index arrays to automatically parallelize computations that access irregular meshes <ref> [26, 37] </ref>. The Jade implementation dynamically analyzes how tasks access data to exploit the concurrency in coarse-grain parallel programs [34]. Speculative approaches optimistically execute loops in parallel, rolling back the computation if the parallel execution violates the data dependences [32].
Reference: [38] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The applications are Barnes-Hut [4], a hierarchical N-body solver, Water <ref> [38] </ref>, which simulates water molecules in the liquid state, and String [19], which builds a velocity model of the geology between two oil wells. Each application is a serial C++ program that performs a computation of interest to the scientific computing community.
Reference: [39] <author> D. Wall. </author> <title> Global register allocation at link time. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, volume 21. ACM, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Profiling has also been used to guide decisions to inline procedures in C programs [7], to drive instruction scheduling algorithms [8], to help place code so as to minimize the impact on the memory hierarchy [29], to aid in register allocation <ref> [28, 39] </ref>, and to direct the compiler to frequently executed parts of the program so that the compiler can apply further optimizations [13]. Brewer [5] describes a system that uses statistical modeling to automatically predict which algorithm will work best for a given combination of input and hardware platform.
References-found: 39

