URL: ftp://ftp.cs.huji.ac.il/users/clag/aaai97S/symp.ps
Refering-URL: http://www.cs.huji.ac.il/~clag/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: clag@cs.huji.ac.il, jeff@cs.huji.ac.il  
Title: Multiagent Learning Systems and Expert Agents  
Author: Claudia V. Goldman Jeffrey S. Rosenschein 
Address: Givat Ram, Jerusalem, Israel  
Affiliation: Institute of Computer Science The Hebrew University  
Abstract: This paper focuses on two main research topics we are investigating. First, we investigate how agents can learn strategic behavior in a teacher-learner model. The notion of the teacher here should be understood as a "trainer". We present the general teacher-learner model together with results from experiments performed in the traffic lights domain. Second, we investigate how agents can learn to become experts, and eventually organize themselves appropriately for a range of tasks. The model is based on evolutionary processes that lead to organizations of experts. In our case, the organization emerges as a step prior to the execution of a task, and as a general process related to a range of problems in a domain. To explore these ideas, we designed and implemented a testbed based on the idea of the game of Life. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Asada, M.; Uchibe, E.; and Hosoda, K. </author> <year> 1995. </year> <title> Agents that learn from other competitive agents. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth international conference,Workshop on Agents that Learn from Other Agents. </booktitle>
Reference: <author> Gardner, M. </author> <year> 1983. </year> <title> Wheels, Life and other mathematical amusements. W.H. </title> <publisher> Freeman and Company. </publisher>
Reference-contexts: The social expertise of the agents is shaped by their environment. We have developed a model and an algorithm for evolving a population of agents in a given domain (Goldman & Rosenschein 1997; ). The implementation of this testbed is along the lines of the Game of Life <ref> (Gardner 1983) </ref>. We consider finite and structured domains (e.g., a collection of documents (i.e., html files, mail files,...), or a collection of pre-computed plans for achieving goals in a domain.
Reference: <author> Goldman, C. V., and Rosenschein, J. S. </author> <title> Patterns of organization in multiagent systems. </title> <note> Submitted to the International Journal of Applied Intelligence, a special issue on Intelligent Adaptive Agents. </note>
Reference: <author> Goldman, C. V., and Rosenschein, J. S. </author> <year> 1996a. </year> <title> Incremental and mutual adaptation in multiagent systems. </title> <type> Technical Report 96-15, </type> <institution> The Hebrew University. </institution>
Reference-contexts: We first studied (Goldman & Rosen-schein 1996b) how agents could learn from other agents by receiving training examples, and then generalizing the knowledge they have acquired. Then, we defined the teacher-learner model <ref> (Goldman & Rosen-schein 1996a) </ref>, in which each agent plays both roles, the teacher and the learner. The agents learn to adapt to each other dynamically while they actually interact and perform their actions.
Reference: <author> Goldman, C. V., and Rosenschein, J. S. </author> <year> 1996b. </year> <title> Mutually supervised learning in multiagent systems. </title> <booktitle> Adaptation and Learning in Multi-Agent Systems LNAI 1042 </booktitle> <pages> 85-96. </pages>
Reference-contexts: In the first case, the agent learns directly from the other agents it interacts with, in the environment in which it acts. We first studied <ref> (Goldman & Rosen-schein 1996b) </ref> how agents could learn from other agents by receiving training examples, and then generalizing the knowledge they have acquired. Then, we defined the teacher-learner model (Goldman & Rosen-schein 1996a), in which each agent plays both roles, the teacher and the learner.
Reference: <author> Goldman, C. V., and Rosenschein, J. S. </author> <year> 1997. </year> <title> Evolving organizations of agents. Technical Report WS-97-03, </title> <booktitle> American Association for Artificial Intelligence. Multiagent Learning Workshop at AAAI97. </booktitle>
Reference-contexts: We have implemented this system, called Musag, on the Internet <ref> (Goldman, Langer, & Rosenschein 1997) </ref>. A discussion of this system is beyond the scope of this paper. <p> of agents that can reproduce, die, or just add documents to their collection after they compared the number of documents they are holding and the current population density of agents on documents that are neighbors of the documents in their collection (more details about this algorithm can be found in <ref> (Goldman & Rosenschein 1997) </ref>). There were three known behaviors that could emerge in different configurations with these rules: stable populations, that did not change their structure once they got to it, configurations that faded away, and periodic or oscillating configurations.
Reference: <author> Goldman, C. V.; Langer, A.; and Rosenschein, J. S. </author> <year> 1997. </year> <title> Musag: an agent that learns what you mean. </title> <journal> Journal of Applied AI, a special issue on Practical Applications of Intelligent Agents and Multiagent Technology 11(5,6). </journal>
Reference-contexts: We have implemented this system, called Musag, on the Internet <ref> (Goldman, Langer, & Rosenschein 1997) </ref>. A discussion of this system is beyond the scope of this paper. <p> of agents that can reproduce, die, or just add documents to their collection after they compared the number of documents they are holding and the current population density of agents on documents that are neighbors of the documents in their collection (more details about this algorithm can be found in <ref> (Goldman & Rosenschein 1997) </ref>). There were three known behaviors that could emerge in different configurations with these rules: stable populations, that did not change their structure once they got to it, configurations that faded away, and periodic or oscillating configurations.
Reference: <author> Littman, M. L. </author> <year> 1994. </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international conference, </booktitle> <pages> 157|163. </pages>
Reference: <author> Mataric, M. J. </author> <year> 1994a. </year> <title> Learning to behave socially. </title> <booktitle> In Proceedings From Animals to Animats 3, Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> 157|163. </pages>
Reference: <author> Mataric, M. J. </author> <year> 1994b. </year> <title> Reward functions for accelerated learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international conference, </booktitle> <pages> 157|163. </pages>
Reference: <author> Sen, S.; Sekaran, M.; and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 426-431. </pages>
References-found: 11

