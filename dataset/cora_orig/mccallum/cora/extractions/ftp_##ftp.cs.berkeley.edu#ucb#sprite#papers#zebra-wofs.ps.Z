URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/zebra-wofs.ps.Z
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: 
Title: Zebra: A Striped Network File System  
Author: John H. Hartman John K. Ousterhout 
Note: This work was supported in part by the National Science Foundation under grant CCR-8900029, the National Aeronautics and Space Administration and the Defense Advanced Research Projects Agency under contract NAG2-591.  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California  
Abstract: This paper presents the design of Zebra, a striped network file system. Zebra applies ideas from log-structured file system (LFS) and RAID research to network file systems, resulting in a network file system that has scalable performance, uses its servers efficiently even when its applications are using small files, and provides high availability. Zebra stripes file data across multiple servers, so that the file transfer rate is not limited by the performance of a single server. High availability is achieved by maintaining parity information for the file system. If a server fails its contents can be reconstructed using the contents of the remaining servers and the parity information. Zebra differs from existing striped file systems in the way it stripes file data: Zebra does not stripe on a per-file basis; instead it stripes the stream of bytes written by each client. Clients write to the servers in units called stripe fragments, which are analogous to segments in an LFS. Stripe fragments contain file blocks that were written recently, without regard to which file they belong. This method of striping has numerous advantages over per-file striping, including increased server efficiency, efficient parity computation, and elimination of parity update. This paper will appear in the proceedings of the USENIX Workshop on File Systems, May 1992. 
Abstract-found: 1
Intro-found: 1
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff and John K. Ousterhout, </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 198-212. </pages>
Reference-contexts: Large transfers are relatively simple to achieve for large files, but small files pose a problem. Client file caches are effective at reducing server accesses for small file reads, but they arent as effective at filtering out small file writes <ref> [Baker91] </ref>. Zebra clients use the storage servers efficiently by writing to them in large transfers, even if their applications are writing small files. High availability 1 . Zebra can tolerate the loss of any single machine in the system, including a storage server.
Reference: [Cabrera91] <author> Luis-Felipe Cabrera and Darrell D. E. Long, Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <booktitle> Computing Systems 4, 4 (Fall 1991), </booktitle> <pages> 405-436. </pages>
Reference-contexts: In these file systems the blocks of each file are striped across multiple storage devices. These storage devices may be disks, as in HPFS [Poston88], I/O nodes in a parallel computer, as in CFS [Pierce89] and Bridge [Dibble90], or they may be network file servers as in Swift <ref> [Cabrera91] </ref>. It is important to note that these systems stripe on a per-file basis, therefore they work best with large files.
Reference: [Dibble90] <author> Peter C. Dibble, </author> <title> A Parallel Interleaved File System, </title> <type> Ph.D. Thesis, </type> <institution> University of Rochester, </institution> <year> 1990. </year>
Reference-contexts: In these file systems the blocks of each file are striped across multiple storage devices. These storage devices may be disks, as in HPFS [Poston88], I/O nodes in a parallel computer, as in CFS [Pierce89] and Bridge <ref> [Dibble90] </ref>, or they may be network file servers as in Swift [Cabrera91]. It is important to note that these systems stripe on a per-file basis, therefore they work best with large files.
Reference: [Johnson84] <author> O. G. Johnson, </author> <title> Three-dimensional wave equation computations on vector computers, </title> <booktitle> Proceedings of the IEEE 72 (January 1984). </booktitle>
Reference-contexts: The idea of using striping to improve data transfer bandwidth is not a new one. Its often used to improve the performance of disk subsystems by striping data across multiple disks attached to the same computer. Mainframes and supercomputers have used striped disks for quite a while <ref> [Johnson84] </ref>. The term disk striping was first defined by Salem and Garcia-Molina in 1986 [Salem86]. More recently there has been lots of interest in arrays of many small disks, originating with the paper by Patterson et al. in 1988 [Patterson88].
Reference: [Lee92] <author> Edward K. Lee, Peter M. Chen, John H. Hartman, Ann L. Chervenak Drapeau, Ethan L. Miller, Randy H. Katz, Garth A. Gibson and David A. Patterson, </author> <title> RAID-II: A Scalable Storage Architecture for High Bandwidth Network File Service, </title> <type> Technical Report UCB/CSD 92/ 672, </type> <institution> Computer Science Division, Electrical Engineering and Computer Sciences, University of California, Berkeley, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year> <note> Zebra April 28, 1992 9 </note>
Reference-contexts: The simple functionality of the storage servers is well-suited to machines that emphasize I/O capabilities rather than CPU speed. For example, the RAID-II project at Berkeley <ref> [Lee92] </ref> is building a storage system that has a high-bandwidth connection between its disk array and the network. Unfortunately, it is relatively expensive for the host CPU to access the data that passes over that connection.
Reference: [Ousterhout88] <author> John K. Ousterhout, Andrew R. Cherenson, Frederick Douglis, Michael N. Nelson, and Brent B. Welch, </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: Zebra is currently only a paper design, although a prototype is being implemented in the Sprite operating system <ref> [Ousterhout88] </ref>. This paper describes the design of Zebra, not the prototype implementation. The rest of this paper is organized as follows.
Reference: [Patterson88] <author> David A. Patterson, Garth Gibson and Randy H. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: 1 Introduction Zebra is a network file system architecture designed to provide both high performance and high availability. This is accomplished by incorporating ideas from log-structured file systems, such as Sprite LFS [Rosenblum91], and redundant arrays of inexpensive disks (RAID) <ref> [Patterson88] </ref> into a network file system. From log-structured file systems Zebra borrows the idea that small, independent writes to the storage subsystem can be batched together into large sequential writes, thus improving the storage subsystems write performance. <p> The term disk striping was first defined by Salem and Garcia-Molina in 1986 [Salem86]. More recently there has been lots of interest in arrays of many small disks, originating with the paper by Patterson et al. in 1988 <ref> [Patterson88] </ref>. All of this work focuses on aggregating several relatively slow disks to create a single logical disk with a much higher data rate. In recent years striping has been applied to file systems as a whole.
Reference: [Pierce89] <author> Paul Pierce, </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem, </title> <booktitle> Proceedings of the Fourth Conference on Hyper-cubes, </booktitle> <address> Monterey CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: In recent years striping has been applied to file systems as a whole. In these file systems the blocks of each file are striped across multiple storage devices. These storage devices may be disks, as in HPFS [Poston88], I/O nodes in a parallel computer, as in CFS <ref> [Pierce89] </ref> and Bridge [Dibble90], or they may be network file servers as in Swift [Cabrera91]. It is important to note that these systems stripe on a per-file basis, therefore they work best with large files.
Reference: [Poston88] <institution> Alan Poston A High Performance File System for UNIX, NASA NAS document, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: In recent years striping has been applied to file systems as a whole. In these file systems the blocks of each file are striped across multiple storage devices. These storage devices may be disks, as in HPFS <ref> [Poston88] </ref>, I/O nodes in a parallel computer, as in CFS [Pierce89] and Bridge [Dibble90], or they may be network file servers as in Swift [Cabrera91]. It is important to note that these systems stripe on a per-file basis, therefore they work best with large files.
Reference: [Rosenblum91] <author> Mendel Rosenblum and John K. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 1-15. </pages>
Reference-contexts: 1 Introduction Zebra is a network file system architecture designed to provide both high performance and high availability. This is accomplished by incorporating ideas from log-structured file systems, such as Sprite LFS <ref> [Rosenblum91] </ref>, and redundant arrays of inexpensive disks (RAID) [Patterson88] into a network file system. From log-structured file systems Zebra borrows the idea that small, independent writes to the storage subsystem can be batched together into large sequential writes, thus improving the storage subsystems write performance.
Reference: [Salem86] <author> Kenneth Salem and Hector Garcia-Molina, </author> <title> Disk Striping, </title> <booktitle> Proceedings of the 2nd International Conference on Data Engineering, Febru-ary 1986, </booktitle> <pages> 336-342. </pages>
Reference-contexts: Its often used to improve the performance of disk subsystems by striping data across multiple disks attached to the same computer. Mainframes and supercomputers have used striped disks for quite a while [Johnson84]. The term disk striping was first defined by Salem and Garcia-Molina in 1986 <ref> [Salem86] </ref>. More recently there has been lots of interest in arrays of many small disks, originating with the paper by Patterson et al. in 1988 [Patterson88]. All of this work focuses on aggregating several relatively slow disks to create a single logical disk with a much higher data rate.
Reference: [Shirriff92] <author> Ken Shirriff and John Ousterhout, </author> <title> A Trace-driven Analysis of Name and Attribute Caching in a Distributed File System, </title> <booktitle> Proceedings of the Winter 1992 USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1992, </year> <note> 315- 331. </note>
Reference-contexts: One technique for eliminating this bottleneck is to have the clients cache both name and mapping information. Client name caching has been shown to be effective at reducing the load on the name server in a network file system <ref> [Shirriff92] </ref>. By caching whole directories of file names and their mapping information the Zebra clients can eliminate the need for contacting the file manager each time they modify the name space or access a file.
References-found: 12

