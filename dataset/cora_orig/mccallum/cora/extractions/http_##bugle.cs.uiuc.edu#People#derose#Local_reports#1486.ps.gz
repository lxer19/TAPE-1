URL: http://bugle.cs.uiuc.edu/People/derose/Local_reports/1486.ps.gz
Refering-URL: http://bugle.cs.uiuc.edu/People/derose/other_publications.html
Root-URL: http://www.cs.uiuc.edu
Title: 3-D Land Avoidance and Load Balancing in Regional Ocean Simulation  
Author: L. De Rose, K. Gallivan, and E. Gallopoulos 
Address: 1308 West Main Street Urbana, Illinois 61801 USA  Rio, 265 00 Patra Greece  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  Laboratory for High-Performance Computer Architectures Department of Computer Engineering and Informatics University of Patras  
Note: To appear in the Proceedings of the 1996 International Conference on Parallel Processing. Bloomingdale, IL,  
Pubnum: CSRD Report No. 1486  
Email: E-mail: fderose,gallivang@csrd.uiuc.edu, stratis@daidalos.ceid.upatras.gr  
Date: August 1996  April 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Andrich and G. Madec. </author> <title> Performance evaluation for an ocean general circulation model: vectorization and multitasking. </title> <booktitle> In 1988 International Conference on Supercomputing, </booktitle> <pages> pages 295-302, </pages> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL.
Reference: [2] <author> M. Ashworth. </author> <title> Parallel processing for environmental modeling. </title> <editor> In G.-R. Hoffmann and T. Kauranne, editors, </editor> <booktitle> Parallel Supercomputing in Atmospheric Science, </booktitle> <pages> pages 1-25. </pages> <publisher> World Scientific, </publisher> <year> 1993. </year> <title> Paper from Fifth ECMWF Workshop on Use of Multiprocessors in Meteorology, European Centre for Medium Range Weather Forecasts, </title> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL.
Reference: [3] <author> R. Bleck, S. Dean, M. O'Keefe, and A. Sawdey. </author> <title> A comparison of data-parallel and message-passing versions of the Miami isopycnic coordinate ocean model (MICOM). </title> <journal> Parallel Comput., </journal> <volume> 21(10) </volume> <pages> 1695-1720, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: All remaining grid-points will be termed "non-participating". The location and number of these points depends on the simulated basin. We will be denoting by s (DOM) the total number of non-participating grid-points in domain DOM, 1 In <ref> [3, 15] </ref> layers correspond to surfaces of constant pressure, whereas in [6] they correspond to a fixed wavenumber. 2 We are referring to the sea, not to enclosed lakes, etc. and by fl (DOM) := s (DOM)=I fi J fi K the fraction of non-participating points. <p> Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL. <p> The central topics of this paper, land avoidance and load balancing, have received only scant attention by researchers. Some reasons were listed in Section 2. There were only a few exceptions <ref> [3, 6] </ref>. To the best of our knowledge, [6] is the most comprehensive treatment of the issue of land avoidance for OCM to date. The authors provide an insightful discussion of the problems related to the design of an OCM code for the Cray X-MP. <p> Their work is closely related to the considerations that result in the optimizations performed within a single cluster in our Cedar code. The authors of <ref> [3] </ref> use land avoidance in their implementation of an isopycnic OCM code on a message-passing environment (Cray T3D) and deal with the load balancing question. They consider two partitioning schemes, one that works best when the number of processors is small, and another for many processors.
Reference: [4] <author> R. M. Chervin and A. J. Semtner Jr. </author> <title> An ocean modeling system for supercomputer architectures of the 1990s. </title> <editor> In M. E. Schlesinger, editor, </editor> <booktitle> Proceedings of the NATO Advanced Research Workshop on Climate-Ocean Interaction, </booktitle> <pages> pages 87-95. </pages> <publisher> Kluwer Academic Publishers., </publisher> <year> 1988. </year>
Reference: [5] <author> M. D. Cox. </author> <title> A primitive equation, 3-dimensional model of the ocean. </title> <type> Technical Report 1, </type> <institution> Geophysical Fluid Dynamics Laboratory/NOAA, Princeton University, </institution> <address> Prince-ton, NJ 08542, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: Any reduction in the time needed to simulate a single timestep translates into significant performance gains over the long-running climate simulations. The ocean circulation model (OCM) that we use is based on a model of the Geophysical Fluid Dynamics Laboratory (GFDL) <ref> [5] </ref>, as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Am-bientali (IMGA-CNR) to the Mediterranean basin geometry. This model, from here on referred to as GFDL-IMGA, simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. <p> Temperature, salinity, and the prediction of currents are the main physical phenomena of interest. As most of the major OCM to date, this model is based on the primitive equations model designed by Bryan and Cox <ref> [5] </ref>. A complete mathematical formulation can be found in [5][8].
Reference: [6] <author> A.M. Davies, R.B. Grzonka, and C.V. Stephens. </author> <title> Implementation of a three-dimensional hydrodynamic numerical sea model using parallel processing on a Cray X-MP series computer. In D.J. </title> <editor> Evans, editor, </editor> <booktitle> Advances in Parallel Computing, </booktitle> <pages> pages 145-185. </pages> <publisher> JAI Press Ltd., </publisher> <year> 1992. </year>
Reference-contexts: Data and computations are needed for only the first two classes of points, collectively called "participating points" in <ref> [6] </ref>. Even though all points are represented in memory, it is only necessary to compute over "participating" points. All remaining grid-points will be termed "non-participating". The location and number of these points depends on the simulated basin. <p> All remaining grid-points will be termed "non-participating". The location and number of these points depends on the simulated basin. We will be denoting by s (DOM) the total number of non-participating grid-points in domain DOM, 1 In [3, 15] layers correspond to surfaces of constant pressure, whereas in <ref> [6] </ref> they correspond to a fixed wavenumber. 2 We are referring to the sea, not to enclosed lakes, etc. and by fl (DOM) := s (DOM)=I fi J fi K the fraction of non-participating points. Hence, variable fl characterizes the "density" of non-participating points in the domain. <p> Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL. <p> The central topics of this paper, land avoidance and load balancing, have received only scant attention by researchers. Some reasons were listed in Section 2. There were only a few exceptions <ref> [3, 6] </ref>. To the best of our knowledge, [6] is the most comprehensive treatment of the issue of land avoidance for OCM to date. The authors provide an insightful discussion of the problems related to the design of an OCM code for the Cray X-MP. <p> The central topics of this paper, land avoidance and load balancing, have received only scant attention by researchers. Some reasons were listed in Section 2. There were only a few exceptions [3, 6]. To the best of our knowledge, <ref> [6] </ref> is the most comprehensive treatment of the issue of land avoidance for OCM to date. The authors provide an insightful discussion of the problems related to the design of an OCM code for the Cray X-MP.
Reference: [7] <author> L. De Rose, K. Gallivan, and E. Gallopoulos. </author> <title> Trace analysis of the GFDL ocean circulation model: A preliminary study. </title> <type> Technical Report 863, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1989. </year>
Reference-contexts: Not surprisingly therefore, the problem of excessive redundant computation manifested itself in the course of our tuning the GFDL-IMGA code. 2.2 Parallelization Overview In previous work we progressed from mapping the code on a shared memory multiprocessor (Alliant FX/8; <ref> [7] </ref>) to restructuring it for Cedar, a prototype architecture with multiple clusters of multiprocessors and shared memory [9].
Reference: [8] <author> Luiz Antonio De Rose. </author> <title> Parallel ocean circulation modeling on Cedar. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> De-cember </month> <year> 1991. </year>
Reference-contexts: In our previous work we applied several loop and data partitioning strategies on the 3-D iteration space <ref> [9, 10, 11, 8] </ref>. Partitioning and Data Placement Each Cedar cluster contains its own private (cluster) memory while each cluster contains a cache shared by all processors in the cluster. <p> Therefore, in order to minimize communication and increase data locality, it becomes critical to select good partitioning and data placement strategies. We briefly review our work on this subject <ref> [10, 8] </ref>. We considered two 1-D, and one 2-D schemes for partitioning the data.
Reference: [9] <author> L. DeRose, K. Gallivan, and E. Gallopoulos. </author> <title> Status report: Parallel ocean circulation modeling on CEDAR. </title> <editor> In G.-R. Hoffmann and T. Kauranne, editors, </editor> <booktitle> Parallel Supercomputing in Atmospheric Science, </booktitle> <pages> pages 157-172. </pages> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1993. </year>
Reference-contexts: excessive redundant computation manifested itself in the course of our tuning the GFDL-IMGA code. 2.2 Parallelization Overview In previous work we progressed from mapping the code on a shared memory multiprocessor (Alliant FX/8; [7]) to restructuring it for Cedar, a prototype architecture with multiple clusters of multiprocessors and shared memory <ref> [9] </ref>. The code was parameterized to offer several choices for data decomposition of the computational domain, as well as for functional decomposition and for control and data 3 mapping strategies across the computational and memory hierarchies. <p> In our previous work we applied several loop and data partitioning strategies on the 3-D iteration space <ref> [9, 10, 11, 8] </ref>. Partitioning and Data Placement Each Cedar cluster contains its own private (cluster) memory while each cluster contains a cache shared by all processors in the cluster.
Reference: [10] <author> L. DeRose, K. Gallivan, and E. Gallopoulos. </author> <title> Experiments with an ocean circulation model on CEDAR. </title> <booktitle> In Proc. 1992 ACM Int'l. Conference on Supercomputing, </booktitle> <pages> pages 397-408, </pages> <address> Washington D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In our previous work we applied several loop and data partitioning strategies on the 3-D iteration space <ref> [9, 10, 11, 8] </ref>. Partitioning and Data Placement Each Cedar cluster contains its own private (cluster) memory while each cluster contains a cache shared by all processors in the cluster. <p> Therefore, in order to minimize communication and increase data locality, it becomes critical to select good partitioning and data placement strategies. We briefly review our work on this subject <ref> [10, 8] </ref>. We considered two 1-D, and one 2-D schemes for partitioning the data. <p> Hence, whenever a new slab is read by one cluster, the neighboring data is brought together with it from global memory. The experiments described in this paper used this latter approach; hence, in our analysis, the communication overhead between clusters is very small. Our experiments in <ref> [10] </ref> have shown that the best approach, even for configurations with slower than Cedar access times to global memory, is to reserve the cluster memories for the 2-D workspace and use global memory to store the values of the variables over the 3-D domain. <p> This approach resembles the use of the solid state disk system on some Cray systems. Data slabs are transferred from global to cluster memories as needed. Prefetching significantly improves the performance of such transfers <ref> [10] </ref>. 3 3-D LAND AVOIDANCE AND INTRA-CLUSTER LOAD BALANCING As mentioned earlier, computations over land areas are unnecessary and may represent a significant fraction of the total work. On the other hand, the regular computation pattern resulting from the inclusion of land areas reduces loop control and data access overhead.
Reference: [11] <author> L. DeRose, K. Gallivan, E. Gallopoulos, and A. Navarra. </author> <title> Parallel ocean circulation modeling on Cedar. </title> <editor> In J. J. Don-garra, K. Kennedy, P. Messina, D. C. Sorensen, , and R. G. Voigt, editors, </editor> <booktitle> Proc. Fifth SIAM Conf. Parallel Processing for Scientific Computing, </booktitle> <pages> pages 401-405, </pages> <address> Philadelphia, </address> <month> March </month> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: Given the restrictions of the slab organization, this assignment is also the one that a restructuring compiler would follow in order to exploit the computer resources available on a vector multiprocessor <ref> [11] </ref>. On some architectures, however, this organization does not provide sufficient granularity to exploit effectively the available processing and memory resources. Cedar is a case in point. <p> In our previous work we applied several loop and data partitioning strategies on the 3-D iteration space <ref> [9, 10, 11, 8] </ref>. Partitioning and Data Placement Each Cedar cluster contains its own private (cluster) memory while each cluster contains a cache shared by all processors in the cluster.
Reference: [12] <author> J. Konicek, T. Tilton, A. Veidenbaum, C. Zhu, E. David-son, R. Downing, M. Haney, M. Sharma, P. Yew, P. Farmwald, D. Kuck, D. Lavery, R. Lindsey, D. Pointer, J. Andrews, T. Beck, T. Murphy, S. Turner, and N. Warter. </author> <title> The organization of the Cedar system. </title> <booktitle> In Proc. 1991 Int'l Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 49-56, </pages> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The code was parameterized to offer several choices for data decomposition of the computational domain, as well as for functional decomposition and for control and data 3 mapping strategies across the computational and memory hierarchies. The Computational Environment The Cedar system <ref> [12] </ref>, developed at the Center for Supercomputing Research and Development of the University of Illinois, has as its main characteristic the hierarchical organization of its computational capabilities and memory system.
Reference: [13] <author> D. Kuck, E. Davidson, D. Lawrie, A. Sameh, C.-Q Zhu, A. Veidenbaum, J. Konicek, P. Yew, K. Gallivan, W. Jalby, H. Wijshoff, U.M. Yang R. Bramley, P. Emrath, D. Padua, J. Hoeflinger R. Eigenmann, G. Jaxon, Z. Li, T. Murphy, and S. Turner J. Andrews. </author> <title> The Cedar System and an Initial Performance Study. Comput. Arch. News. </title> <booktitle> Proc. 20th Int'l. Symp., </booktitle> <volume> 21(2), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: The unit allows, with some restrictions, each CE to have up to 512 outstanding 64-bit word requests to global memory. The simplest use of prefetching allows a reasonably effective block move of data from global memory to cluster memory; for extensive comments on the performance of Cedar see <ref> [13] </ref>. Code Characteristics A high-level view of the loop structure of the baroclinic phase of the GFDL-IMGA code is presented in Figure 4. In the original GFDL-IMGA code, the iteration space is swept one I fi K tile at a time.
Reference: [14] <author> R.C. Pacanowski, K. Dixon, and A. Rosati. </author> <title> The GFDL Modular Ocean Model, </title> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL.
Reference: [15] <author> A. Sawdey, M. O'Keefe, R. Bleck, and R.W. </author> <title> Numrich. The design, implementation and performance of a parallel ocean circulation model. </title> <editor> In G-R. Hoffman and N. Kreitz, editors, </editor> <booktitle> Coming of Age: Proc. 6th ECMWF Workshop on the Use of Parallel Processors in Meteorology, </booktitle> <address> Reading, England, </address> <month> November </month> <year> 1994. </year> <title> World Scientific Publishers (Singapore), </title> <note> to appear. </note>
Reference-contexts: All remaining grid-points will be termed "non-participating". The location and number of these points depends on the simulated basin. We will be denoting by s (DOM) the total number of non-participating grid-points in domain DOM, 1 In <ref> [3, 15] </ref> layers correspond to surfaces of constant pressure, whereas in [6] they correspond to a fixed wavenumber. 2 We are referring to the sea, not to enclosed lakes, etc. and by fl (DOM) := s (DOM)=I fi J fi K the fraction of non-participating points. <p> Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL.
Reference: [16] <author> A.J. Semnter, Jr. and R. M. Chervin. </author> <title> Breakthroughs in ocean and climate modeling made possible by supercomputers of today and tomorrow. </title> <editor> In J.L. Martig and S.F. Lundstrom, editors, </editor> <booktitle> Proc. Supercomputing 1989, </booktitle> <pages> pages 230-239, </pages> <address> Washington, DC, 1989. </address> <publisher> IEEE Computer Soc. Press. </publisher>
Reference-contexts: on 32 CE's via a land avoidance/load balancing strategy is a significant fraction of a more realistic expectation. 5 RELATED WORK During the last 30 years the original code of Bryan and Cox was modified several times as researchers have been closely following new developments in high-performance computer systems technology <ref> [16] </ref>. Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]).
Reference: [17] <author> A. J. Semtner Jr. </author> <title> An oceanic general circulation model with bottom topography. </title> <type> Technical Report 9, </type> <institution> UCLA Dept. of Metheorology, </institution> <year> 1974. </year>
Reference: [18] <author> J. P. Singh and J. L. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experience, results and implications. </title> <journal> J. Paral. Distr. Comput., </journal> <pages> pages 27-48, </pages> <year> 1992. </year>
Reference-contexts: Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL.
Reference: [19] <author> R. D. Smith, J. K. Dukowicz, and R. C. Malone. </author> <title> Parallel ocean general circulation modeling. </title> <editor> In J. M. Hyman, editor, </editor> <booktitle> Experimental Mathematics: Computational Issues in Nonlinear Science, </booktitle> <pages> pages 38-61. </pages> <publisher> North-Holland, </publisher> <year> 1992. </year> <booktitle> Proc. 11th Annual Int'l. Conf. of the Centre for Nonlinear Studies, </booktitle> <address> Los Alamos, NM, </address> <month> May </month> <year> 1991. </year> <month> 9 </month>
Reference-contexts: Initial mapping efforts concentrated on vector architectures, including (memory-to-memory) vector machines with long startups ([5]), register-to-register vector architectures ([17]), and vector multiprocessors ([4, 7]). More recently, interest shifted to more aggressive exploitation of parallelism and to the design of codes that represent better models <ref> [14, 2, 1, 19, 3, 15, 6, 18] </ref>). These reports present significant performance improvements for specific architectures or classes of architectures; equally important are the software reengineering efforts that have been undertaken in the context of the new and much improved OCM, the Modular Ocean Model from GFDL.
References-found: 19

