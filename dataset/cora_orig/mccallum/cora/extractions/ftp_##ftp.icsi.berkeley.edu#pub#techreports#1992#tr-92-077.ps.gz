URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-077.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Email: Email addresses: bertoni@hermes.mc.dsi.unimi.it, campadelli@hermes.mc.dsi.unimi.it, mor-purgo@imiucca.csi.unimi.it, panizza@hermes.mc.dsi.unimi.it.  
Title: Polynomial Uniform Convergence and Polynomial-Sample Learnability  
Author: Alberto Bertoni Paola Campadelli Anna Morpurgo yz and Sandra Panizza 
Note: Part of this research was performed while the author was visitor at the International  
Address: Milano, Via Comelico 39, 20135 Milano, Italy.  1947 Center St., Suite 600, Berkeley, CA 94704.  
Affiliation: Dipartimento di Scienze dell'Informazione, Universita degli Studi di  Computer Science Institute,  
Date: November 1992  
Pubnum: TR-92-077  
Abstract: In the PAC model, polynomial-sample learnability in the distribution dependent framework has been characterized in terms of minimun cardinality of *-covers. In this paper we propose another approach to the problem by investigating the relationship between polynomial-sample learnability and uniform convergence, in analogy to what was done for the distribution free setting. First of all, we introduce the notion of polynomial uniform convergence, giving a characterization for it in terms of an entropic measure, then we study its relationship with polynomial-sample learn-ability. We show that, contrarily to what happens in the distribution independent setting, polynomial uniform convergence is a sufficient but not necessary condition for polynomial-sample learnability. fl This research was partly supported by CNR, grant 92.01568.PF.69, project Sistemi Informatici e Cal-colo Parallelo. An extended abstract of this paper appeared in Proc. 5th Annual ACM Workshop on Computational Learning Theory (1992). 
Abstract-found: 1
Intro-found: 1
Reference: [BeIt88] <author> G. Benedek, A. Itai. </author> <title> "Learnability by Fixed Distributions". Proc. 1988 Workshop on Computational Learning Theory (1988) 80-90. Polynomial Uniform Convergence and Polynomial-Sample Learnability 15 </title>
Reference-contexts: Valiant's original PAC learning model requires that the algorithm work for any probability distribution on the domain. An interesting restriction of the basic model of Valiant is that of fixing the probability distribution. Benedek and Itai <ref> [BeIt88] </ref> study the problem with a geometrical approach based on the fact that the probability distribution induces a metric on the error, that is on the probability that the hypothesis disagrees with the target concept, and obtain bounds on the sample complexity in this model in terms of *-covers. <p> fruitfully applied also in this context? And what is the exact relationship between uniform convergence and learnability in the two frameworks, the distribution-free and the distribution-dependent one? In this paper we explore the problem of distribution-dependent learnability studying its relationship with uniform convergence, instead of following the approach proposed in <ref> [BeIt88] </ref> (Natarajan [Na88] [Na92] also studied the problem with an approach similar to ours); in particular we investigate polynomial-sample learnability, i.e. learnability with a polynomial bound on the sample size, in terms of a parameter tightly related to the one introduced in [VaCh71] for studying the problem of uniform convergence in <p> convergent iff there exists a polynomial p (n; 1=*; 1=ffi) such that, for all n and all probability distributions P n on X n , n fx j F n (x) &gt; *g &lt; ffi): The problem of learnability in the distribution-dependent framework has been studied by Benedek and Itai <ref> [BeIt88] </ref> with a different approach using the notion of coverability. Let F n be a class of functions on X n and P n a probability distribution on X n . An *-cover of F n w.r.t. <p> This notion plays a role analogous to the Vapnik-Chervonenkis dimension, in the sense that it characterizes learnability in the distribution-dependent framework. In fact from the results in <ref> [BeIt88] </ref> readily follows that a family fhX n ; P n ; F n ig n1 is polynomial-sample learnable iff there is a polynomial p (n; 1=*) such that N P n (F n ; *) 2 p (n;1=*) ; where N P n (F n ; *) is the cardinality <p> Bounds on the sample size sufficient or necessary to learn a family fhX n ; P n ; F n ig n1 at approximation * and confidence 1 ffi have been given in terms of the minimum cardinality of *-covers <ref> [BeIt88] </ref>; the following result [Pa91] gives instead an upper bound in terms of the average information per example, H F n (t)=t. We recall that a learning algorithm is said consistent if it always outputs a hypothesis that agrees with the target on every example in the t-sample.
Reference: [BeBeMa89] <author> S. Ben-David, G. Benedek, Y. Mansour. </author> <title> "A Parametrization Scheme for Classifying Models of Learnability". </title> <booktitle> Proc. 1989 Workshop on Computational Learning Theory (1989) 285-302. </booktitle>
Reference-contexts: Thus, if for a class F the relative frequencies of the events in F converge to their probabilities uniformly over F , then by (1), (2), and (3) the class is learnable by consistent hypotheses (solidly learnable <ref> [BeBeMa89] </ref>).
Reference: [BeCaMoPa92] <author> A. Bertoni, P. Campadelli, A. Morpurgo, S. Panizza. </author> <title> "Polynomial Uniform Convergence of Relative Frequencies to Probabilities". </title> <booktitle> Advances in Neural Information Processing Systems 4 , San Mateo, </booktitle> <address> CA: </address> <publisher> Morgan Kaufmann Publishers (1992) 904-911. </publisher>
Reference-contexts: 2 p (n; 8=") maxf16=" 2 ; (n; 1=")g, we can conclude that 8" 8n (P a (n; "; p (n; " 4 ): (13) From assertions (12) and (13) follows 1 &gt; 5 , a contradiction. 2 We are now ready to prove the main result of this section <ref> [BeCaMoPa92] </ref>. Theorem 4.3 Given fhX n ; P n ; F n ig n1 , the following conditions are equivalent: C1. The relative frequencies of events in F n converge uniformly polynomially to their prob abilities. C2.
Reference: [BlEhHaWa89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, K. Warmuth. </author> <title> "Learnability and the Vapnik-Chervonenkis Dimension". </title> <journal> J. </journal> <note> ACM 36 (1989) 929-965. </note>
Reference-contexts: For the distribution-dependent framework they give sufficient and necessary conditions for such convergence and their results are based on 2 Bertoni, Campadelli, Morpurgo, and Panizza a parameter they call entropy, which is a measure of the average information provided by a sample of fixed size. Blumer et al. <ref> [BlEhHaWa89] </ref> apply the results obtained by Vapnik and Chervonenkis in the distribution-free setting to the field of computational learning theory and show that the Vapnik-Chervonenkis dimension is a useful notion to characterize the distribution-free learnability of a class of concepts. <p> Thus, if for a class F the relative frequencies of the events in F converge to their probabilities uniformly over F , then by (1), (2), and (3) the class is learnable by consistent hypotheses (solidly learnable [BeBeMa89]). As shown in <ref> [BlEhHaWa89] </ref>, a class is solidly learnable iff it has finite VC dimension, which completes the proof. 2 In the distribution-dependent context, where the probability measure P is fixed and known, the problem of uniform convergence has been characterized in terms of the expectation E [log 2 F (x)], called entropy H <p> An important characterization of polynomial-sample learnability has been given in terms of the Vapnik-Chervonenkis dimension; in fact it has been shown <ref> [BlEhHaWa89] </ref> that a family fhX n ; F n ig n1 is polynomial-sample learnable iff the Vapnik-Chervonenkis dimension of F n is bounded by a polynomial in n. <p> C3. d n = VCdim (F n ) is bounded by a polynomial in n. Proof. * C2 , C3 is proved in <ref> [BlEhHaWa89] </ref>. * C3 ) C1 is an immediate consequence of the results in [VaCh71]. <p> 2 n 2 n 14 Bertoni, Campadelli, Morpurgo, and Panizza If * &lt; 2 n 2 , since A is a consistent algorithm, it needs ( 4 ffi + 8VCdim (F n ) * log 13 * ) examples to learn F n at approximation * and confidence 1 ffi <ref> [BlEhHaWa89] </ref>. Observing that VCdim (F n ) = 2 n=2 &lt; 1=*, then ( 4 ffi + 8 * ) examples are sufficient. 2. fhX n ; P n ; F n ig n1 is not polynomially uniformly convergent. <p> Even though the results are stated for the boolean domain f0; 1g n , they also hold for infinite domains such as E n , provided that certain measurability constraints on the concept class (see <ref> [BlEhHaWa89] </ref>) are satisfied. In the distribution-dependent context we have characterized the property of polynomial uniform convergence of fhX n ; P n ; F n ig n1 by means of the parameter M (n; t).
Reference: [Na88] <author> B. Natarajan. </author> <title> "Learning over Classes of Distributions". </title> <booktitle> Proc. 1988 Workshop on Computational Learning Theory (1988) 408-409. </booktitle>
Reference-contexts: also in this context? And what is the exact relationship between uniform convergence and learnability in the two frameworks, the distribution-free and the distribution-dependent one? In this paper we explore the problem of distribution-dependent learnability studying its relationship with uniform convergence, instead of following the approach proposed in [BeIt88] (Natarajan <ref> [Na88] </ref> [Na92] also studied the problem with an approach similar to ours); in particular we investigate polynomial-sample learnability, i.e. learnability with a polynomial bound on the sample size, in terms of a parameter tightly related to the one introduced in [VaCh71] for studying the problem of uniform convergence in the distribution-dependent
Reference: [Na92] <author> B. Natarajan. </author> <title> "Probably Approximate Learning Over Classes of Distributions". </title> <journal> Siam J. Comput. </journal> <volume> 21 (3) (1992). </volume>
Reference-contexts: in this context? And what is the exact relationship between uniform convergence and learnability in the two frameworks, the distribution-free and the distribution-dependent one? In this paper we explore the problem of distribution-dependent learnability studying its relationship with uniform convergence, instead of following the approach proposed in [BeIt88] (Natarajan [Na88] <ref> [Na92] </ref> also studied the problem with an approach similar to ours); in particular we investigate polynomial-sample learnability, i.e. learnability with a polynomial bound on the sample size, in terms of a parameter tightly related to the one introduced in [VaCh71] for studying the problem of uniform convergence in the distribution-dependent framework. <p> In order to prove Theorem 4.3 we need the following results. The following lemma is obtained by minor modifications from [VaCh71, Lemma 2, The orem 4, and Lemma 4]. (A similar result was independently proved in <ref> [Na92] </ref>.) Lemma 4.2 Given fhX n ; P n ; F n ig n1 , if lim t!1 H F n (t)=t = 0 then 8* 8ffi 8n (t * 2 ffi n fx j F n (x) &gt; *g &lt; ffi); where t 0 is such that H F n
Reference: [Pa91] <author> S. </author> <month> Panizza. </month> <institution> "Apprendimento PAC con distribuzione di probabilita fissata". Tesi di Laurea, Universita degli studi di Milano, Dipartimento di Scienze dell'Informazione, </institution> <address> A.A. </address> <year> 1990-91 (1991). </year>
Reference-contexts: The random variable C (t) index function F n are related to one another; in fact, the following result holds <ref> [Pa91] </ref>. Lemma 4.1 C (t) n (x) + 1). Proof. The left inequality holds trivially. For the right inequality the case C (t) n = t is also trivial. <p> Bounds on the sample size sufficient or necessary to learn a family fhX n ; P n ; F n ig n1 at approximation * and confidence 1 ffi have been given in terms of the minimum cardinality of *-covers [BeIt88]; the following result <ref> [Pa91] </ref> gives instead an upper bound in terms of the average information per example, H F n (t)=t. We recall that a learning algorithm is said consistent if it always outputs a hypothesis that agrees with the target on every example in the t-sample.
Reference: [Sa72] <author> N. Sauer. </author> <title> "On the Density of Families of Sets". </title> <journal> J. </journal> <note> Combinatorial Theory (A) 13 (1972) 145-147. </note>
Reference-contexts: If C (t) n &lt; t, then no subset of fx 1 ; : : : ; x t g of cardinality C (t) n + 1 is shattered by F n . As shown in <ref> [Sa72] </ref>, F n (x) (C t where (n; t) = &gt; &lt; P n k if t &gt; n From the known bound (n; t) t n + 1 F n (x) t C t n (x) + 1 which proves the right inequality of the lemma. 2 For t 2
Reference: [Va84] <author> L.G. Valiant. </author> <title> "A Theory of the Learnable". </title> <booktitle> Communications of the ACM 27 (1984) 1134-1142. </booktitle>
Reference-contexts: 1 Introduction The probably approximately correct (PAC) learning model proposed by Valiant <ref> [Va84] </ref> provides a complexity theoretical basis to study the problem of learning from examples produced by an arbitrary distribution. The model can be informally described as follows. Given a domain X, a concept class C is a set of subsets of X. <p> ; : : :; x t ) P f j : The relative frequencies of the events are said to converge uniformly over F to the probabilities if, for every * &gt; 0, lim t!1 P (t) fx j (t) We now introduce the PAC learning model proposed by Valiant <ref> [Va84] </ref> and its distribution-dependent restriction. For our purposes it is convenient to consider parametrized domains and classes of concepts.
Reference: [VaCh71] <author> V.N. Vapnik, A.Ya. Chervonenkis. </author> <title> "On the uniform convergence of relative frequencies of events to their probabilities". </title> <journal> Theory of Prob. and its Appl. </journal> <note> 16 (2) (1971) 265-280. </note>
Reference-contexts: They propose a learning algorithm, the best-agreement learning algorithm, which returns as hypothesis one of the elements of the *-cover that has the smallest number of inconsistencies with the sample. They give a nice characterization of distribution dependent learnability in terms of *-covers. Vapnik and Chervonenkis <ref> [VaCh71] </ref> studied the convergence of empirical probability estimates, they considered the following problem, that arises for example in pattern recognition: given a class of events, do their relative frequencies in a sequence of independent trials converge to the probabilities uniformly over the entire class of events? More precisely, under what conditions <p> convergence, instead of following the approach proposed in [BeIt88] (Natarajan [Na88] [Na92] also studied the problem with an approach similar to ours); in particular we investigate polynomial-sample learnability, i.e. learnability with a polynomial bound on the sample size, in terms of a parameter tightly related to the one introduced in <ref> [VaCh71] </ref> for studying the problem of uniform convergence in the distribution-dependent framework. We will restrict our discussion to the Boolean domain, although our results also hold for infinite domains, such as E n , provided certain measurability constraints on the concept class are satisfied. <p> In section 3 we discuss the relationship between polynomial uniform convergence and polynomial-sample learnability in the distribution-free framework, and give a general characterization of both in terms of the Vapnik-Chervonenkis dimension. In section 4 we introduce an entropic measure related to the one introduced in <ref> [VaCh71] </ref>, consider the concept of polynomial uniform convergence in the distribution-dependent framework and characterize it in terms of this entropic measure. The relationship between polynomial uniform convergence and polynomial-sample learnability in the distribution dependent framework is then discussed in section 5. Notation. <p> and T , S T [ T S, will be denoted by S4T , the symbol ^ will denote the logic operator AND, the base 2 logarithm will be denoted by log. 2 Preliminary Definitions and Results In this section we recall the PAC learning model, present the results in <ref> [VaCh71] </ref> on uniform convergence, proving a stronger version of one of them, and introduce the notion of polynomial uniform convergence. Let us first give the basic notation. <p> We will now recall the notion of Vapnik-Chervonenkis dimension and prove that the sufficient condition for uniform convergence given in <ref> [VaCh71] </ref> for the distribution-free case is also a necessary one. We will then present Vapnik and Chervonenkis' result for the distribution-dependent case. <p> We will then present Vapnik and Chervonenkis' result for the distribution-dependent case. In order to study the problem of uniform convergence of the relative frequencies to the probabilities the notion of index F (x) of the class F with respect to a t-sample x has been introduced <ref> [VaCh71] </ref>. <p> In particular the following result holds. Theorem 2.1 For all probability distributions on X, the relative frequencies of the events converge (in probability) to their probabilities uniformly over F iff VCdim (F ) &lt; 1. Proof . The if part is proved in <ref> [VaCh71] </ref>. Assume now that the relative frequencies of the events converge (in probability) to their probabilities uniformly over F , that is for every * &gt; 0, lim t!1 P (t) fx j F (x) &gt; *g = 0. <p> In fact the following result <ref> [VaCh71] </ref> holds. Polynomial Uniform Convergence and Polynomial-Sample Learnability 5 Theorem 2.2 A necessary and sufficient condition for the relative frequencies of the events in F to converge uniformly over F (in probability) to their corresponding probabilities is that lim H F (t) = 0: Vapnik and Chervonenkis [VaCh71] studied the problem <p> the following result <ref> [VaCh71] </ref> holds. Polynomial Uniform Convergence and Polynomial-Sample Learnability 5 Theorem 2.2 A necessary and sufficient condition for the relative frequencies of the events in F to converge uniformly over F (in probability) to their corresponding probabilities is that lim H F (t) = 0: Vapnik and Chervonenkis [VaCh71] studied the problem of uniform convergence of the frequencies to the probabilities in the limit. <p> C3. d n = VCdim (F n ) is bounded by a polynomial in n. Proof. * C2 , C3 is proved in [BlEhHaWa89]. * C3 ) C1 is an immediate consequence of the results in <ref> [VaCh71] </ref>. <p> In order to prove Theorem 4.3 we need the following results. The following lemma is obtained by minor modifications from <ref> [VaCh71, Lemma 2, The orem 4, and Lemma 4] </ref>. (A similar result was independently proved in [Na92].) Lemma 4.2 Given fhX n ; P n ; F n ig n1 , if lim t!1 H F n (t)=t = 0 then 8* 8ffi 8n (t * 2 ffi n fx j <p> Proof. Let x 2 X (t) (2t) n and let x 0 and x 00 denote the first and the second half, respectively, of y. In <ref> [VaCh71, Lemma 2] </ref> it is proved that, if t &gt; 2=* 2 , then P (t) (t) n fy j sup j-f (x 0 ) -f (x 00 )j 2 Let us denote the event fsup f2F n j-f (x 0 ) -f (x 00 )j * 2 g by C <p> As proved in <ref> [VaCh71] </ref>, P (2t) * 2 t * 2 ): Let ff = * 2 =16. Let t 0 be such that H F n (t 0 )=t 0 ff=4 (such a t 0 must exist because by hypothesis lim t!1 H F n (t)=t = 0). Following [VaCh71], let us first <p> As proved in <ref> [VaCh71] </ref>, P (2t) * 2 t * 2 ): Let ff = * 2 =16. Let t 0 be such that H F n (t 0 )=t 0 ff=4 (such a t 0 must exist because by hypothesis lim t!1 H F n (t)=t = 0). Following [VaCh71], let us first estimate P + (t; ff=2) with t = mt 0 (m integer) and obtain P + mt 0 ; 2 (t 0 ) mff 2 : (7) From (7) let us now obtain a bound on P + (2t; ff), where ff = * 2 =16 and <p> ); 10 Bertoni, Campadelli, Morpurgo, and Panizza then P (t) (t) The following lemma, which relates the problem of polynomial uniform convergence of a family of events to the parameter P a (n; *; t), will only be stated since it is proved by minor modifications of Theorem 4 in <ref> [VaCH71] </ref>. For the sake of simplicity it is convenient to introduce the following notations: a (t) n =t P a (n; *; t) = P (t) n (x) *g: Lemma 4.3 If t 16=* 2 then P (t) (t) 4 (1 P a (n; 8*; 2t)).
References-found: 10

