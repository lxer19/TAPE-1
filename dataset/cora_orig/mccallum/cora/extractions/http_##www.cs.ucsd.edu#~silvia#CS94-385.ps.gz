URL: http://www.cs.ucsd.edu/~silvia/CS94-385.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~silvia/
Root-URL: http://www.cs.ucsd.edu
Title: Performance Analysis of Parallel Strategies for Localized N-body Solvers  
Author: Silvia M. Figueira Scott B. Baden 
Abstract: Although there exist several approaches to rapidly solving the N-body problem, and a diversity of implementation strategies, the performance tradeoffs of the various strategies with respect to problem-specific data distributions is poorly understood on a parallel computer. We present a synthetic workload model and a simulator that enables us to evaluate the performance tradeoffs encountered in implementing particle methods on MIMD computers. These results can be used to evaluate designs early on in the implementation process.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baden, </author> <title> Programming Abstractions for Dynamically Partitioning and Coordinating Localized Scientific Calculations Running on Multiprocessors, </title> <journal> SIAM J. on Sci. and Stat. Comput., </journal> <volume> 12 (1991), </volume> <pages> pp. 145-157. </pages>
Reference-contexts: We varied c and MAX . All simulations were conducted with 5000 particles and run for 50 timesteps, unless otherwise specified. The maximum number of flops per interaction was 19 and the number of bytes of storage per particle was 72. Dynamic recursive bisection was used for load balancing <ref> [1] </ref>. 4 Figueira and Baden Fig. 3. Execution time of strategies 1, 2 and 3. The short displacement of the particles favors strategies 2 and 3. Fig. 4. Memory required (in total number of neighbors in a timestep) for the lists, in strategies 2 and 3.
Reference: [2] <author> S. Baden, </author> <title> Very Large Vortex Calculations in Two Dimensions, </title> <booktitle> Proceedings of the UCLA Workshop on Vortex Methods, </booktitle> <month> May, </month> <year> 1987. </year>
Reference-contexts: Strategies 1 and 2 have been used to calculate the non-bonded forces in Molecular Dynamics systems, like EulerGROMOS [4], and have been discussed by Tamayo, Mesirov and Boghosian [11], and by Plimpton and Heffelfinger [10]. Strategy 1 has also been employed in vortex dynamics calculations <ref> [2] </ref>. 4 Results Our simulator was written in C++ (gcc compiler version 2.5.7) and runs under LPARX (version 1.1) [7]. It has been used in experiments simulating three dimensional N-body systems on 16 processors of the Intel Paragon running the operating system Paragon OSF/1 R1.2.
Reference: [3] <author> J. Barnes and P. Hut, </author> <title> A Hierarchical O(NlogN) Force Calculation Algorithm, </title> <booktitle> Nature, 324 (1986), </booktitle> <pages> pp. 446-449. </pages>
Reference-contexts: Often the short range interactions predominate. We will consider only local interactions, i.e., each particle interacts over a small neighborhood only. Our results apply to single-level particle methods in which the particle density variation is not too great; thus we exclude hierarchical methods <ref> [3, 5] </ref>. A synthetic workload model was developed that characterizes an N-body system in terms of its properties.
Reference: [4] <author> T. Clark, R. Hanxleden, J. McCammon and L. Scott, </author> <title> Parallelizing Molecular Dynamics using Spatial Decomposition, </title> <booktitle> Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May, </month> <year> 1994, </year> <pages> pp. 95-102. </pages>
Reference-contexts: As compared with methods that partition on the basis of particles or interactions, spatial decomposition strategies avoid global communication, achieving better scalability [10]. Strategies 1 and 2 have been used to calculate the non-bonded forces in Molecular Dynamics systems, like EulerGROMOS <ref> [4] </ref>, and have been discussed by Tamayo, Mesirov and Boghosian [11], and by Plimpton and Heffelfinger [10]. Strategy 1 has also been employed in vortex dynamics calculations [2]. 4 Results Our simulator was written in C++ (gcc compiler version 2.5.7) and runs under LPARX (version 1.1) [7].
Reference: [5] <author> L. Greengard and V. Rokhlin, </author> <title> A Fast Algorithm for Particle Simulations, </title> <journal> Journal of Computational Physics, </journal> <volume> 73 (1987), </volume> <pages> pp. 325-348. </pages>
Reference-contexts: Often the short range interactions predominate. We will consider only local interactions, i.e., each particle interacts over a small neighborhood only. Our results apply to single-level particle methods in which the particle density variation is not too great; thus we exclude hierarchical methods <ref> [3, 5] </ref>. A synthetic workload model was developed that characterizes an N-body system in terms of its properties.
Reference: [6] <author> R. Hockney and J. Eastwood, </author> <title> Computer Simulation using Particles, </title> <publisher> McGraw-Hill Inc., </publisher> <year> 1981. </year>
Reference-contexts: The time evolution of the particles is simulated using a random walk, determined by MAX . 3 Implementation Strategies The force computation is organized around the procedure for enumerating interactions. Fundamentally, two approaches have been taken: the link-cell method (also called "chaining mesh" <ref> [6] </ref>), and the Verlet Neighbor List method [12]. The link-cell method uses a mesh to organize the computation.
Reference: [7] <author> S. Kohn and S. Baden, </author> <title> A Robust Parallel Programming Model for Dynamic Non-Uniform Scientific Computations, </title> <booktitle> Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May, </month> <year> 1994, </year> <pages> pp. 509-517. </pages>
Reference-contexts: Strategy 1 has also been employed in vortex dynamics calculations [2]. 4 Results Our simulator was written in C++ (gcc compiler version 2.5.7) and runs under LPARX (version 1.1) <ref> [7] </ref>. It has been used in experiments simulating three dimensional N-body systems on 16 processors of the Intel Paragon running the operating system Paragon OSF/1 R1.2. The peak bandwidth of the Paragon is around 70 MB/sec, and the average message startup cost is 100 microseconds [9]. <p> While this approach is generally slower than the traditional (fine-grain) Verlet list, it offers reduced storage overheads. To validate our model, we have compared the performance of the strategies in a computation of smoothed particle hydrodynamics in 3 dimensions <ref> [8, 7] </ref> on the Intel Paragon. The computation was written in mixture of C++ and Fortran 77 and ran under LPARX. The initial data consists of 12190 particles distributed in a disk with a hole in the center and declining density; they are assigned a circular velocity.
Reference: [8] <author> J. Monaghan, </author> <title> Smoothed Particle Hydrodynamics, </title> <booktitle> Annual Review of Astronomy and Astrophysics, 30 (1992), </booktitle> <pages> pp. 543-574. </pages>
Reference-contexts: While this approach is generally slower than the traditional (fine-grain) Verlet list, it offers reduced storage overheads. To validate our model, we have compared the performance of the strategies in a computation of smoothed particle hydrodynamics in 3 dimensions <ref> [8, 7] </ref> on the Intel Paragon. The computation was written in mixture of C++ and Fortran 77 and ran under LPARX. The initial data consists of 12190 particles distributed in a disk with a hole in the center and declining density; they are assigned a circular velocity.
Reference: [9] <author> P. Pierce and G. </author> <title> Regnier The Paragon Implementation of the NX Message Passing Interface, </title> <booktitle> Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May, </month> <year> 1994, </year> <pages> pp. 184-190. </pages>
Reference-contexts: It has been used in experiments simulating three dimensional N-body systems on 16 processors of the Intel Paragon running the operating system Paragon OSF/1 R1.2. The peak bandwidth of the Paragon is around 70 MB/sec, and the average message startup cost is 100 microseconds <ref> [9] </ref>. We explore the effect that workload variations have on the performance of the different implementation strategies. We varied c and MAX . All simulations were conducted with 5000 particles and run for 50 timesteps, unless otherwise specified.
Reference: [10] <author> S. Plimpton and G. Heffelfinger, </author> <title> Scalable Parallel Molecular Dynamics on MIMD Supercomputers, </title> <booktitle> Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> April, </month> <year> 1992, </year> <pages> pp. 246-251. </pages>
Reference-contexts: To parallelize these computations, we employ spatial decomposition to partition the link-cell mesh into contiguous simply-connected regions. As compared with methods that partition on the basis of particles or interactions, spatial decomposition strategies avoid global communication, achieving better scalability <ref> [10] </ref>. Strategies 1 and 2 have been used to calculate the non-bonded forces in Molecular Dynamics systems, like EulerGROMOS [4], and have been discussed by Tamayo, Mesirov and Boghosian [11], and by Plimpton and Heffelfinger [10]. <p> the basis of particles or interactions, spatial decomposition strategies avoid global communication, achieving better scalability <ref> [10] </ref>. Strategies 1 and 2 have been used to calculate the non-bonded forces in Molecular Dynamics systems, like EulerGROMOS [4], and have been discussed by Tamayo, Mesirov and Boghosian [11], and by Plimpton and Heffelfinger [10]. Strategy 1 has also been employed in vortex dynamics calculations [2]. 4 Results Our simulator was written in C++ (gcc compiler version 2.5.7) and runs under LPARX (version 1.1) [7].
Reference: [11] <author> P. Tamayo, J. Mesirov and B. Boghosian, </author> <title> Parallel Approach to Short Range Molecular Dynamics Simulations, </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <month> November, </month> <year> 1991, </year> <pages> pp. 462-470. </pages>
Reference-contexts: Strategies 1 and 2 have been used to calculate the non-bonded forces in Molecular Dynamics systems, like EulerGROMOS [4], and have been discussed by Tamayo, Mesirov and Boghosian <ref> [11] </ref>, and by Plimpton and Heffelfinger [10]. Strategy 1 has also been employed in vortex dynamics calculations [2]. 4 Results Our simulator was written in C++ (gcc compiler version 2.5.7) and runs under LPARX (version 1.1) [7].
Reference: [12] <author> L. Verlet, </author> <title> Computer "Experiments" on Classical Fluids, </title> <journal> Physical Review, </journal> <volume> 159 (1967), </volume> <pages> pp. 98-103. </pages>
Reference-contexts: Fundamentally, two approaches have been taken: the link-cell method (also called "chaining mesh" [6]), and the Verlet Neighbor List method <ref> [12] </ref>. The link-cell method uses a mesh to organize the computation. Each box of the mesh contains a list with the particles found in its corresponding subregion of the domain, and the interacting neighbors lie in neighbor boxes, avoiding a costly O (N 2 ) search for neighbors.
References-found: 12

