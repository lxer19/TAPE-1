URL: http://www.cs.cmu.edu/~jab/pubs/phd-proposal.ps
Refering-URL: http://www.cs.cmu.edu/~jab/pubs/propo/propo.html
Root-URL: 
Title: Learning Evaluation Functions  
Author: Justin A. Boyan Andrew W. Moore, co-chair Scott E. Fahlman, co-chair Tom Mitchell Thomas G. Dietterich, 
Degree: THESIS PROPOSAL  Thesis Committee  
Affiliation: Oregon Graduate Institute  
Date: June 17, 1996  
Abstract: Evaluation functions are an essential component of practical search algorithms for optimization, planning and control. Examples of such algorithms include hillclimb-ing, simulated annealing, best-first search, A*, and alpha-beta. In all of these, the evaluation functions are typically built manually by domain experts, and may require considerable tweaking to work well. I will investigate the thesis that statistical machine learning can be used to automatically generate high-quality evaluation functions for practical combinatorial problems. The data for such learning is gathered by running trajectories through the search space. The learned evaluation function may be applied either to guide further exploration of the same space, or to improve performance in new problem spaces which share similar features. Two general families of learning algorithms apply here: reinforcement learning and meta-optimization. The reinforcement learning approach, dating back to Samuel's checkers player [ 1959 ] but with more recent successes on backgammon [ Tesauro, 1992 ] and job-shop scheduling [ Zhang and Dietterich, 1995 ] , is based on asynchronous dynamic programming with value function approximation. The currently-popular algorithms, Q-learning and TD() with neural networks, run slowly and may even be unstable. I will evaluate several original value-function-approximation algorithms tailored for combinatorial optimization domains. The meta-optimization approach, which has also been applied to game-playing [ Pol-lack et al., 1996 ] and combinatorial optimization [ Ochotta, 1994 ] , is conceptually simpler: we assume a fixed parametric form for the evaluation function and optimize it directly. These methods, lacking the theoretical advantages of dynamic programming, have been ignored by the reinforcement-learning community; however, recent advances 
Abstract-found: 1
Intro-found: 1
Reference: [ Barto et al., 1989 ] <author> A. Barto, R. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report COINS 89-95, </type> <institution> University of Massachusetts, </institution> <year> 1989. </year>
Reference-contexts: Lee and Mahajan [ 1988 ] trained a nonlinear evaluation function on expertly-played games, and it played high-quality Othello. But the biggest advance in the state of the art came more recently, when the reinforcement-learning community elaborated the connection between AI search and the Bellman equations <ref> [ Barto et al., 1989, Watkins, 1989, Sutton, 1990 ] </ref> .
Reference: [ Barto et al., 1995 ] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <journal> AI Journal, </journal> <year> 1995. </year>
Reference-contexts: Tesauro modified Sut-ton's TD () algorithm [ Sutton, 1988 ] , which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. An implementation is given in Appendix A.1. When = 0, this algorithm reduces to the RTDP algorithm <ref> [ Barto et al., 1995 ] </ref> , which is closely related to VI; the key difference is that its backups are done along sample trajectories through the MDP, rather than along sweeps of the entire state space.
Reference: [ Bellman, 1957 ] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: for control : 8x; V fl (x) = max 2 X P (yjx; a)V fl (y) 5 (4) From the value function V fl , it is easy to infer the optimal policy: at any state x, any action which instantiates the max in Equation 4 is an optimal choice <ref> [ Bellman, 1957 ] </ref> . This formalizes the notion that V fl is an ideal evaluation function. <p> For small problems of up to 10 7 or so discrete states, the value function can be stored in a lookup table and computed by any of a variety of algorithms, including linear programming 5 [ Denardo, 1982 ] , policy iteration [ Howard, 1960 ] , and value iteration <ref> [ Bellman, 1957 ] </ref> . Value iteration (VI) has been the basis for the most important simulation-based algorithms for learning V fl , so I focus on it here.
Reference: [ Bellman, 1978 ] <author> R. Bellman. </author> <title> An Introduction to Artificial Intelligence: Can Computers Think? Boyd & Fraser Publishing Company, </title> <year> 1978. </year>
Reference-contexts: This connection had been unexplored despite the publication of an AI textbook by Richard Bellman himself <ref> [ Bellman, 1978 ] </ref> ! Reinforcement learning's most celebrated success has also been in a game domain, the game of backgammon [ Tesauro, 1992, Tesauro, 1994, Boyan, 1992 ] .
Reference: [ Berry and Fristedt, 1985 ] <author> D. A. Berry and B. Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <year> 1985. </year>
Reference-contexts: By contrast, ROUT completed successfully in under 1 million evaluations, and performed at the significantly higher level of -0.09. ROUT's adaptively-generated training set contained only 133 states. Task 3: Multi-armed Bandit Problem Our third test problem is to compute the optimal policy for a finite-horizon k-armed bandit <ref> [ Berry and Fristedt, 1985 ] </ref> . <p> For these comparative experiments, however, we used linear or neural net fits for both algorithms. 28 marked by a single diamond at the top left of the graph. found efficiently using Gittins indices, solving the finite-horizon problem is equivalent to solving a large acyclic, stochastic MDP in belief space <ref> [ Berry and Fristedt, 1985 ] </ref> . We show results for k = 3 arms and a horizon of n = 25 pulls, where the resulting MDP has 736,281 states. Solving this MDP by DAG-SP produces the optimal exploration policy, which has an expected reward of 0.6821 per pull.
Reference: [ Bertsekas and Tsitsiklis, 1996 ] <author> D. Bertsekas and J. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: This technique goes by the names Neuro-Dynamic Programming <ref> [ Bertsekas and Tsitsiklis, 1996 ] </ref> and Value Function Approximation (VFA) [ Boyan et al., 1995 ] . 2.2 Literature Review Any review of the literature on reinforcement learning and evaluation functions must begin with the pioneering work of Arthur Samuel on the game of checkers [ Samuel, 1959, Samuel, 1967
Reference: [ Bertsekas, 1995 ] <author> D. Bertsekas. </author> <title> A counterexample to temporal differences learning. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 270-9, </pages> <year> 1995. </year>
Reference-contexts: In the case of uncontrolled Markov chains and linear function approximators, online TD () does converge [ Sutton, 1988, Dayan, 1992, Tsitsiklis 7 and Roy, 1996 ] |but even then, if 6= 1, convergence is not necessarily to a good approxi-mation of V fl <ref> [ Bertsekas, 1995 ] </ref> .
Reference: [ Boyan and Moore, 1995 ] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances In Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Learning the value function in combinatorially large domains, however, is quite challenging. The currently-popular approximation algorithms, Q-learning and TD () with neural networks, run very slowly and may even be unstable <ref> [ Boyan and Moore, 1995 ] </ref> . Section 2 of this proposal reviews the literature on value function approximation and describes an original algorithm for more efficient learning. Section 3 focuses specifically on the application of value function approximation to combinatorial optimization problems, and presents another new algorithm. <p> I have therefore investigated generalizations of the Dijkstra and DAG-SP algorithms specifically modified to accommodate huge state spaces and value function approximation. My variant of Dijkstra's algorithm, called Grow-Support, was presented in <ref> [ Boyan and Moore, 1995 ] </ref> and will not be explored further in this thesis. My variant of DAG-SP is an algorithm called ROUT [ Boyan and Moore, 1996 ] , which this thesis will develop further.
Reference: [ Boyan and Moore, 1996 ] <author> J. A. Boyan and A. W. Moore. </author> <title> Learning evaluation functions for large acyclic domains. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year> <note> (To appear). </note>
Reference-contexts: My variant of Dijkstra's algorithm, called Grow-Support, was presented in [ Boyan and Moore, 1995 ] and will not be explored further in this thesis. My variant of DAG-SP is an algorithm called ROUT <ref> [ Boyan and Moore, 1996 ] </ref> , which this thesis will develop further. In the huge domains for which ROUT is designed, DAG-SP's key preprocessing step| topologically sorting the entire state space|is no longer tractable. Instead, ROUT must expend some extra effort to identify states on the current frontier.
Reference: [ Boyan et al., 1995 ] <author> J. A. Boyan, A. W. Moore, and R. S. Sutton, </author> <title> editors. Proceedings of the Workshop on Value Function Approximation, </title> <booktitle> Machine Learning Conference, </booktitle> <month> July </month> <year> 1995. </year> <note> CMU-CS-95-206. Web: http://www.cs.cmu.edu/~reinf/ml95/. </note>
Reference-contexts: This technique goes by the names Neuro-Dynamic Programming [ Bertsekas and Tsitsiklis, 1996 ] and Value Function Approximation (VFA) <ref> [ Boyan et al., 1995 ] </ref> . 2.2 Literature Review Any review of the literature on reinforcement learning and evaluation functions must begin with the pioneering work of Arthur Samuel on the game of checkers [ Samuel, 1959, Samuel, 1967 ] .
Reference: [ Boyan, 1992 ] <author> J. A. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> Engineering Department, Cambridge University, </institution> <year> 1992. </year>
Reference-contexts: This connection had been unexplored despite the publication of an AI textbook by Richard Bellman himself [ Bellman, 1978 ] ! Reinforcement learning's most celebrated success has also been in a game domain, the game of backgammon <ref> [ Tesauro, 1992, Tesauro, 1994, Boyan, 1992 ] </ref> . Tesauro modified Sut-ton's TD () algorithm [ Sutton, 1988 ] , which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. An implementation is given in Appendix A.1. <p> which includes a similar component for optimizing the design of a penetrator (a needle-nosed projectile which is sent crashing into a planet to analyze soil). 5.5 Backgammon Move Selection 22 The domain of backgammon fits naturally in this research for several reasons: it was the subject of my Master's thesis <ref> [ Boyan, 1992 ] </ref> , it is the most celebrated and most successful application of value function approximation [ Tesauro, 1992 ] , and it also appears to be suitable for direct meta-optimization approaches [ Pollack et al., 1996 ] .
Reference: [ Censor et al., 1988 ] <author> Y. Censor, M. D. Altschuler, and W. D. Powlis. </author> <title> A computational solution of the inverse problem in radiation-therapy treatment planning. </title> <journal> Applied Mathematics and Computation, </journal> <volume> 25 </volume> <pages> 57-87, </pages> <year> 1988. </year>
Reference-contexts: Appendix B.2 presents our preliminary results with STAGE on this domain. 19 20 5.2 Radiotherapy Treatment Planning Radiation therapy is a widely-used method of treating tumors <ref> [ Censor et al., 1988 ] </ref> . A linear accelerator which produces a radioactive beam is mounted on a rotating gantry, and the patient is placed so that the tumor is at the center of the beam's rotation.
Reference: [ Chao and Harper, 1996 ] <author> Heng-Yi Chao and Mary P. Harper. </author> <title> An efficient lower bound algorithm for channel routing. Integration: </title> <journal> The VLSI Journal, </journal> <note> 1996. (to appear). </note>
Reference-contexts: This was exactly the tradeoff found to be profitable in [ Zhang, 1996 ] , as discussed in Section 3.2. To investigate how problem-dependent learned evaluation functions are in channel routing, we re-ran experiment (G) on a suite of eight problems from the literature <ref> [ Chao and Harper, 1996 ] </ref> . Table 3 summarizes the results and gives the coefficients of the linear evaluation function learned (independently) for each problem. Problem lower best best learned coefficients instance bound hillcl. <p> 141:7 &gt; HYC2 9 9 9 &lt; 0:05; 2:78; 0:34; 2:66 &gt; HYC4 20 49 30 &lt; 3:96; 49:62; 0:33; 49:14 &gt; HYC6 50 75 59 &lt; 1:38; 9:59; 0:72; 9:48 &gt; HYC8 21 55 33 &lt; 0:79; 32:96; 0:11; 32:78 &gt; Table 3: STAGE results on eight problems from <ref> [ Chao and Harper, 1996 ] </ref> . 31 The similarities among the learned evaluation functions are striking.
Reference: [ Christensen, 1986 ] <author> J. Christensen. </author> <title> Learning static evaluation functions by linear regression. </title> <editor> In T. Mitchell, J. Carbonell, and R. Michalski, editors, </editor> <booktitle> Machine learning: A guide to current research, </booktitle> <pages> pages 39-42. </pages> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1986. </year>
Reference: [ Cohn, 1992 ] <author> J. M. Cohn. </author> <title> Automatic Device Placement for Analog Cells in KOAN. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University Department of Electrical and Computer Engineering, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: and set p = 0:5; U = 10. (I will later show a machine-learning procedure which achieved much better performance on this task by assigning, counterintuitively, a negative value to U .) The simulated annealing literature is full of similar examples of evaluation functions being engineered for good performance (e.g. <ref> [ Cohn, 1992, Szykman and Cagan, 1995 ] </ref> ). 1.2 Proposal This thesis will investigate new machine learning algorithms for generating evaluation functions automatically.
Reference: [ Cormen et al., 1990 ] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In fact, for small discrete problems there are classical algorithms for both problem classes which compute V fl more efficiently by explicitly working backwards: for the deterministic class, Dijkstra's shortest-path algorithm; and for the acyclic class, Directed-Acyclic-Graph-Shortest-Paths (DAG-SP) <ref> [ Cormen et al., 1990 ] </ref> . 1 DAG-SP first topologically sorts the MDP, producing a linear ordering of the states in which every state x precedes all states reachable from x. Then, it runs through that list in reverse, performing one backup per state. <p> For the quantized high-dimensional state spaces characteristic of real-world control tasks, the curse of dimensionality makes such enumeration intractable. Computing V fl requires generalization: one natural technique is to encode the states as real-valued feature 1 Although <ref> [ Cormen et al., 1990 ] </ref> presents DAG-SP only for deterministic acyclic problems, it applies straightforwardly to the stochastic case. 6 vectors and to use a function approximator to fit V fl over this feature space.
Reference: [ Crites and Barto, 1996 ] <author> R. Crites and A. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselno, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: Boyan [ 1992 ] replicated Tesauro's results and demonstrated improvements using modular neural networks. Tesauro's combination of TD () and neural networks has been applied to other domains, including elevator control <ref> [ Crites and Barto, 1996 ] </ref> and job-shop scheduling [ Zhang and Dietterich, 1995 ] . (I will discuss the scheduling application in detail in Section 3.2.) Nevertheless, it is important to note that when function approximators are used, TD () provides no guarantees of optimality.
Reference: [ Dayan, 1992 ] <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <type> 8(3/4), </type> <month> May </month> <year> 1992. </year>
Reference-contexts: In the case of uncontrolled Markov chains and linear function approximators, online TD () does converge <ref> [ Sutton, 1988, Dayan, 1992, Tsitsiklis 7 and Roy, 1996 ] </ref> |but even then, if 6= 1, convergence is not necessarily to a good approxi-mation of V fl [ Bertsekas, 1995 ] .
Reference: [ Denardo, 1982 ] <author> E. Denardo. </author> <title> Dynamic Programming: Models and Applications. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1982. </year>
Reference-contexts: This formalizes the notion that V fl is an ideal evaluation function. For small problems of up to 10 7 or so discrete states, the value function can be stored in a lookup table and computed by any of a variety of algorithms, including linear programming 5 <ref> [ Denardo, 1982 ] </ref> , policy iteration [ Howard, 1960 ] , and value iteration [ Bellman, 1957 ] . Value iteration (VI) has been the basis for the most important simulation-based algorithms for learning V fl , so I focus on it here.
Reference: [ Fahlman and Lebiere, 1990 ] <author> S. Fahlman and C. Lebiere. </author> <title> The Cascade-Correlation learning architecture. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The approaches based on locally-weighted regression are best-suited for optimizing evaluation functions with a relatively small number of parameters. For learning more complicated evaluation functions, one possibility is to use a cascaded neural network architecture <ref> [ Fahlman and Lebiere, 1990 ] </ref> , which can build a ~ V function of p parameters without ever having to optimize more than O ( p p) of them at a time.
Reference: [ Gordon, 1995 ] <author> G. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference: [ Harmon et al., 1995 ] <author> M. Harmon, L. Baird, and A. H. Klopf. </author> <title> Advantage updating applied to a differential game. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances In Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The model is flexible enough to represent AI planning problems, stochastic games (e.g. backgammon) against a fixed opponent, and combinatorial optimization search spaces. With natural extensions, it can also represent continuous stochastic control domains, two-player games, and many other problem formulations <ref> [ Littman and Szepesvari, 1996, Harmon et al., 1995 ] </ref> . Specifying a fixed deterministic policy : X ! A reduces an MDP to a Markov chain.
Reference: [ Howard, 1960 ] <author> R. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press and John Wiley & Sons, </publisher> <year> 1960. </year>
Reference-contexts: For small problems of up to 10 7 or so discrete states, the value function can be stored in a lookup table and computed by any of a variety of algorithms, including linear programming 5 [ Denardo, 1982 ] , policy iteration <ref> [ Howard, 1960 ] </ref> , and value iteration [ Bellman, 1957 ] . Value iteration (VI) has been the basis for the most important simulation-based algorithms for learning V fl , so I focus on it here.
Reference: [ Kaelbling, 1990 ] <author> L. P. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: The specific algorithms I will explore are PMAX and IEMAX [ Moore and Schneider, 1996 ] . PMAX's approach is simple: it always recommends sampling the current optimum of the model. IEMAX, patterned after Kaelbling's IE algorithm <ref> [ Kaelbling, 1990 ] </ref> , recommends sampling the parameter setting which optimizes the "optimistic model," i.e. the 95th-percentile values of the model's predicted confidence intervals.
Reference: [ Lee and Mahajan, 1988 ] <author> K.-F. Lee and S. Mahajan. </author> <title> A pattern classification approach to evaluation function learning. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <year> 1988. </year>
Reference: [ Lin and Kernighan, 1973 ] <author> S. Lin and B. W. Kernighan. </author> <title> An effective heuristic algorithm for the traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 498-516, </pages> <year> 1973. </year>
Reference-contexts: Evaluation functions have generally been designed by human domain experts. The weights f1,3,5,9g in the chess evaluation function given above summarize the judgment of generations of chess players. In combinatorial optimization domains, such as the Traveling Salesperson Problem <ref> [ Lin and Kernighan, 1973 ] </ref> , the state space consists of legal candidate solutions, and the domain's objective function (the function which evaluates the quality of a final solution) can itself serve as a natural evaluation function to guide search.
Reference: [ Lin, 1993 ] <author> L.-J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: A state's V fl value simply predicts the objective function value of the best state that can be reached from x in the amount of time remaining. Greedily following the V fl values (depicted) always leads to the globally optimal f (x fl ) = 22. 14 replay <ref> [ Lin, 1993 ] </ref> , Gaussian output representation [ Pomerleau, 1991 ] , exploration, and loop--avoidance techniques all had to be carefully calibrated. They trained the value function on a set of small problem instances. Training continued until performance stopped improving on a validation set of other problem instances.
Reference: [ Littman and Szepesvari, 1996 ] <author> M. L. Littman and C. Szepesvari. </author> <title> A generalized reinforcement-learning model: Convergence and applications. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1996. </year> <note> (To Appear). </note>
Reference-contexts: The model is flexible enough to represent AI planning problems, stochastic games (e.g. backgammon) against a fixed opponent, and combinatorial optimization search spaces. With natural extensions, it can also represent continuous stochastic control domains, two-player games, and many other problem formulations <ref> [ Littman and Szepesvari, 1996, Harmon et al., 1995 ] </ref> . Specifying a fixed deterministic policy : X ! A reduces an MDP to a Markov chain. <p> With this change, the optimization process is no longer deterministic nor strictly an MDP, but is still a generalized-MDP <ref> [ Littman and Szepesvari, 1996 ] </ref> with associated Bellman equations: 8x 2 X; V fl (x) = y2X x 0 2fx;yg This equation has the same form as the Bellman equation used in Tesauro's backgammon experiments, where a position's value is defined as an expectation (over dice rolls) of a max <p> Pig belongs to the class of symmetric, alternating, Markov games. This means that the minimax-optimal value function can be formulated as the unique solution to a system of generalized Bellman equations <ref> [ Littman and Szepesvari, 1996 ] </ref> similar to Equation 4. The state space, with two-player symmetry factored out, has 515,000 positions|large enough to be interesting, but small enough that computing the exact V fl is tractable.
Reference: [ Moore and Schneider, 1996 ] <author> A. W. Moore and J. Schneider. </author> <title> Memory-based stochastic optimization. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This approach was also used by Samuel [ 1959 ] and has been applied in the simulated annealing community [ Ochotta, 1994 ] . Meta-optimization methods, lacking the theoretical advantages of dynamic programming, have been largely ignored by the reinforcement-learning community; however, recent advances in local optimization <ref> [ Moore and Schneider, 1996 ] </ref> may help make them superior to reinforcement learning in practice. My research will lay out a framework for understanding both methods more formally; investigate improvements in both; and evaluate empirically which approach works better. <p> four years of CPU time (!), Powell's method produced an evaluation function which performed well and generalized robustly to larger instances. 4.2 Meta-Optimization Algorithms I believe that the computational requirements of the meta-optimization approach can be significantly reduced by the use of stochastic optimization techniques being developed in our group <ref> [ Moore and Schneider, 1996 ] </ref> . These techniques are designed to optimize functions for which samples are both expensive to gather and potentially very noisy. <p> The specific algorithms I will explore are PMAX and IEMAX <ref> [ Moore and Schneider, 1996 ] </ref> . PMAX's approach is simple: it always recommends sampling the current optimum of the model.
Reference: [ Moriarty and Miikkulainen, 1995 ] <author> D. Moriarty and R. Miikkulainen. </author> <title> Discovering complex othello strategies through evolutionary neural networks. </title> <booktitle> Connection Science, </booktitle> <address> 7(3-4):195-209, </address> <year> 1995. </year>
Reference-contexts: My focus will instead be on designing well-motivated algorithms and validat ing them empirically. 23 * Direct policy optimization. A reasonable approach for some domains is to bypass the evaluation function altogether, and instead learn a direct mapping from states to actions <ref> [ Moriarty and Miikkulainen, 1995 ] </ref> . I will not consider this approach. 6.2 Timeline I hope to complete this thesis in a little over a year.
Reference: [ Nilsson, 1980 ] <author> N.J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1980. </year>
Reference-contexts: The function values provide a basis for decision-making in search. The choice of evaluation function "critically determines search results" <ref> [ Nilsson, 1980, p.74 ] </ref> in popular algorithms for planning and control (A fl ), game-playing (alpha-beta), and combinatorial optimization (hillclimbing, simulated annealing). Evaluation functions have generally been designed by human domain experts.
Reference: [ Ochotta, 1994 ] <author> E. Ochotta. </author> <title> Synthesis of High-Performance Analog Cells in ASTRX/OBLX. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University Department of Electrical and Computer Engineering, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The second approach, meta-optimization, is conceptually simpler: we assume a fixed parametric form for the evaluation function and optimize it directly. This approach was also used by Samuel [ 1959 ] and has been applied in the simulated annealing community <ref> [ Ochotta, 1994 ] </ref> . Meta-optimization methods, lacking the theoretical advantages of dynamic programming, have been largely ignored by the reinforcement-learning community; however, recent advances in local optimization [ Moore and Schneider, 1996 ] may help make them superior to reinforcement learning in practice. <p> Surprisingly, this procedure developed an excellent backgammon player! Meta-optimization has also been applied successfully to aid combinatorial optimization. Ochotta's simulated annealing system for synthesizing analog circuit cells made use of a sophisticated cost function parametrized by 46 real numbers <ref> [ Ochotta, 1994 ] </ref> . These and 10 other parameters of the annealer were optimized using Powell's method as described in [ Press et al., 1992 ] .
Reference: [ Pollack et al., 1996 ] <author> J. Pollack, A. Blair, and M. Land. </author> <title> Coevolution of a backgammon player. In C.G. </title> <editor> Langton, editor, </editor> <booktitle> Proceedings of Artificial Life 5. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year> <note> (to appear). </note>
Reference-contexts: Genetic-algorithm approaches to game learning generally fall into this category (e.g. [ Tunstall-Pedoe, 1991 ] ). Most recently, Pollack et. al. attacked backgammon with a simple hillclimbing approach <ref> [ Pollack et al., 1996 ] </ref> . Like Samuels, they used Alpha (challenger) and Beta (champion) evaluation functions. <p> fits naturally in this research for several reasons: it was the subject of my Master's thesis [ Boyan, 1992 ] , it is the most celebrated and most successful application of value function approximation [ Tesauro, 1992 ] , and it also appears to be suitable for direct meta-optimization approaches <ref> [ Pollack et al., 1996 ] </ref> . Backgammon provides a perfect setting for developing both approaches and putting them in head-to-head competition. 6 Summary 6.1 Contributions This thesis will contribute to the state-of-the-art in machine learning, reinforcement learning, heuristic search and combinatorial optimization. Its specific contributions will include: 1.
Reference: [ Pomerleau, 1991 ] <author> D. Pomerleau. </author> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Greedily following the V fl values (depicted) always leads to the globally optimal f (x fl ) = 22. 14 replay [ Lin, 1993 ] , Gaussian output representation <ref> [ Pomerleau, 1991 ] </ref> , exploration, and loop--avoidance techniques all had to be carefully calibrated. They trained the value function on a set of small problem instances. Training continued until performance stopped improving on a validation set of other problem instances.
Reference: [ Press et al., 1992 ] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> second edition, </address> <year> 1992. </year> <month> 34 </month>
Reference-contexts: Ochotta's simulated annealing system for synthesizing analog circuit cells made use of a sophisticated cost function parametrized by 46 real numbers [ Ochotta, 1994 ] . These and 10 other parameters of the annealer were optimized using Powell's method as described in <ref> [ Press et al., 1992 ] </ref> . Each parameter setting was evaluated by summing the mean, median and minimum final results of 200 annealing runs on a small representative problem instance.
Reference: [ Prieditis, 1993 ] <author> A. </author> <title> Prieditis. Machine discovery of effective admissible heuristics. </title> <journal> Machine Learning, </journal> <volume> 12 </volume> <pages> 117-141, </pages> <year> 1993. </year>
Reference: [ Samuel, 1959 ] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 211-229, </pages> <year> 1959. </year>
Reference-contexts: Two general families of learning algorithms apply here: reinforcement learning and meta-optimization. The reinforcement learning approach, dating back to Samuel's checkers player <ref> [ Samuel, 1959 ] </ref> but with more recent successes on backgammon [ Tesauro, 1992 ] and job-shop scheduling [ Zhang and Dietterich, 1995 ] , is based on approximating a special evaluation function known as the value function. <p> the names Neuro-Dynamic Programming [ Bertsekas and Tsitsiklis, 1996 ] and Value Function Approximation (VFA) [ Boyan et al., 1995 ] . 2.2 Literature Review Any review of the literature on reinforcement learning and evaluation functions must begin with the pioneering work of Arthur Samuel on the game of checkers <ref> [ Samuel, 1959, Samuel, 1967 ] </ref> . <p> Of course, if one could develop a perfect system of this sort it would be the equivalent of always looking ahead to the end of the game. <ref> [ Samuel, 1959, p. 219 ] </ref> Samuel's program incrementally changed the coefficients of an evaluation polynomial so as to make each visited state's value closer to the value obtained from lookahead search. After Samuel, work in evaluation function learning was sporadic until the 1980s.
Reference: [ Samuel, 1967 ] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers II|Recent progress. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11(6) </volume> <pages> 601-617, </pages> <year> 1967. </year>
Reference-contexts: the names Neuro-Dynamic Programming [ Bertsekas and Tsitsiklis, 1996 ] and Value Function Approximation (VFA) [ Boyan et al., 1995 ] . 2.2 Literature Review Any review of the literature on reinforcement learning and evaluation functions must begin with the pioneering work of Arthur Samuel on the game of checkers <ref> [ Samuel, 1959, Samuel, 1967 ] </ref> .
Reference: [ Sutton, 1987 ] <author> R. S. Sutton. </author> <title> Implementation details of the TD() procedure for the case of vector predictions and backpropagation. </title> <type> Technical Note TN87-509.1, </type> <institution> GTE Laboratories, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: A Algorithms A.1 TD () for Control This implementation of TD () is trajectory-based. For a version of TD () that performs updates after each move, refer to <ref> [ Sutton, 1987 ] </ref> . 24 TD (, start states ^ X, fitter F ): /* Assumes known world model MDP; F is parametrized by weight vector w. */ repeat steps 1 and 2 forever: 1.
Reference: [ Sutton, 1988 ] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: Tesauro modified Sut-ton's TD () algorithm <ref> [ Sutton, 1988 ] </ref> , which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. An implementation is given in Appendix A.1. <p> In the case of uncontrolled Markov chains and linear function approximators, online TD () does converge <ref> [ Sutton, 1988, Dayan, 1992, Tsitsiklis 7 and Roy, 1996 ] </ref> |but even then, if 6= 1, convergence is not necessarily to a good approxi-mation of V fl [ Bertsekas, 1995 ] .
Reference: [ Sutton, 1990 ] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Lee and Mahajan [ 1988 ] trained a nonlinear evaluation function on expertly-played games, and it played high-quality Othello. But the biggest advance in the state of the art came more recently, when the reinforcement-learning community elaborated the connection between AI search and the Bellman equations <ref> [ Barto et al., 1989, Watkins, 1989, Sutton, 1990 ] </ref> .
Reference: [ Szykman and Cagan, 1995 ] <author> S. Szykman and J. Cagan. </author> <title> A simulated annealing-based approach to three-dimensional component packing. </title> <journal> ASME Journal of Mechanical Design, </journal> <volume> 117, </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: and set p = 0:5; U = 10. (I will later show a machine-learning procedure which achieved much better performance on this task by assigning, counterintuitively, a negative value to U .) The simulated annealing literature is full of similar examples of evaluation functions being engineered for good performance (e.g. <ref> [ Cohn, 1992, Szykman and Cagan, 1995 ] </ref> ). 1.2 Proposal This thesis will investigate new machine learning algorithms for generating evaluation functions automatically.
Reference: [ Tesauro and Sejnowski, 1989 ] <author> G. Tesauro and T. J. Sejnowski. </author> <title> A parallel network that learns to play backgammon. </title> <journal> Artificial Intelligence, </journal> <volume> 39, </volume> <year> 1989. </year>
Reference-contexts: Rather, all our algorithms assume that the dynamics of the world are fully known in advance. * Learning approaches based on analyzing the problem-space operators, such as [ Priedi-tis, 1993 ] , or based on human-labelled training data, such as <ref> [ Tesauro and Sejnowski, 1989 ] </ref> . All our algorithms learn from simulations and function approximation. * Theoretical results and analysis. The new algorithms presented here make no assumptions about the function-approximation scheme used, so strong theoretical results are unlikely.
Reference: [ Tesauro, 1992 ] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Two general families of learning algorithms apply here: reinforcement learning and meta-optimization. The reinforcement learning approach, dating back to Samuel's checkers player [ Samuel, 1959 ] but with more recent successes on backgammon <ref> [ Tesauro, 1992 ] </ref> and job-shop scheduling [ Zhang and Dietterich, 1995 ] , is based on approximating a special evaluation function known as the value function. <p> This connection had been unexplored despite the publication of an AI textbook by Richard Bellman himself [ Bellman, 1978 ] ! Reinforcement learning's most celebrated success has also been in a game domain, the game of backgammon <ref> [ Tesauro, 1992, Tesauro, 1994, Boyan, 1992 ] </ref> . Tesauro modified Sut-ton's TD () algorithm [ Sutton, 1988 ] , which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. An implementation is given in Appendix A.1. <p> crashing into a planet to analyze soil). 5.5 Backgammon Move Selection 22 The domain of backgammon fits naturally in this research for several reasons: it was the subject of my Master's thesis [ Boyan, 1992 ] , it is the most celebrated and most successful application of value function approximation <ref> [ Tesauro, 1992 ] </ref> , and it also appears to be suitable for direct meta-optimization approaches [ Pollack et al., 1996 ] .
Reference: [ Tesauro, 1994 ] <author> G. Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: This connection had been unexplored despite the publication of an AI textbook by Richard Bellman himself [ Bellman, 1978 ] ! Reinforcement learning's most celebrated success has also been in a game domain, the game of backgammon <ref> [ Tesauro, 1992, Tesauro, 1994, Boyan, 1992 ] </ref> . Tesauro modified Sut-ton's TD () algorithm [ Sutton, 1988 ] , which is normally thought of as a model-free algorithm for learning to predict, into a model-based algorithm for learning to control. An implementation is given in Appendix A.1.
Reference: [ Thrun and Schwartz, 1993 ] <author> S. Thrun and A. Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School, </booktitle> <year> 1993. </year>
Reference: [ Tsitsiklis and Roy, 1996 ] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference: [ Tunstall-Pedoe, 1991 ] <author> W. Tunstall-Pedoe. </author> <title> Genetic algorithms optimizing evaluation functions. </title> <journal> ICCA Journal, </journal> <volume> 14(3) </volume> <pages> 119-128, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, if several consecutive Alpha functions lost, then the biggest coefficient was simply reset to zero|in effect, diversifying the meta-optimization search at the expense of accurate value function approximation. Genetic-algorithm approaches to game learning generally fall into this category (e.g. <ref> [ Tunstall-Pedoe, 1991 ] </ref> ). Most recently, Pollack et. al. attacked backgammon with a simple hillclimbing approach [ Pollack et al., 1996 ] . Like Samuels, they used Alpha (challenger) and Beta (champion) evaluation functions.
Reference: [ Utgoff and Clouse, 1991 ] <author> P. Utgoff and J. Clouse. </author> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceedings of AAAI, </booktitle> <year> 1991. </year>
Reference-contexts: For ex ample, even if a domain's correct V fl is very jagged and discontinuous, meta-optimization may discover a quite different, smooth ~ V that performs well. (This point was also made in <ref> [ Utgoff and Clouse, 1991 ] </ref> .) * Although this approach promises to require a heavy computational burden, it must be admitted that the temporal-difference approaches have not set a high standard in this regard. 17 * Finally, new stochastic optimization algorithms designed to be data-efficient may make this approach more
Reference: [ van Laarhoven, 1987 ] <author> P. van Laarhoven. </author> <title> Simulated Annealing: Theory and Applications. </title> <publisher> Kluwer Academic, </publisher> <year> 1987. </year> <month> 35 </month>
Reference-contexts: Simulated annealing, too, is Markovian in the expanded state space X fi ftemperatureg <ref> [ van Laarhoven, 1987 ] </ref> . In the Markov-chain cases, V A is simply the value function of the chain: it predicts the eventual expected outcome from every state.
Reference: [ Watkins, 1989 ] <author> C. Watkins. </author> <title> Learning From Delayed Rewards. </title> <type> PhD thesis, </type> <address> Cambridge Uni--versity, </address> <year> 1989. </year>
Reference-contexts: Lee and Mahajan [ 1988 ] trained a nonlinear evaluation function on expertly-played games, and it played high-quality Othello. But the biggest advance in the state of the art came more recently, when the reinforcement-learning community elaborated the connection between AI search and the Bellman equations <ref> [ Barto et al., 1989, Watkins, 1989, Sutton, 1990 ] </ref> .
Reference: [ Wong et al., 1988 ] <author> D. F. Wong, H.W. Leong, and C.L. Liu. </author> <title> Simulated Annealing for VLSI Design. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: However, to get good optimization results, engineers often spend considerable effort tweaking the coefficients of penalty terms and other additions to their objective function. This excerpt, from a book on VLSI layout by simulated annealing <ref> [ Wong et al., 1988 ] </ref> , is typical: Clearly, the objective function to be minimized is the channel width w. However, w is too crude a measure of the quality of intermediate solutions. <p> This section briefly describes the major domains. I will also report results on testbed domains such as the the children's game of Pig (see Appendix B.1) and small Traveling Salesperson problems. 5.1 VLSI Channel Routing The problem of "Manhattan channel routing" is an important subtask of VLSI circuit design <ref> [ Wong et al., 1988 ] </ref> . Given two rows of labelled pins across a rectangular channel, we must connect like-labelled pins to one another by placing wire segments into vertical and horizontal tracks (see Figure 5). Segments may cross but not otherwise overlap. <p> Channel routing is known to be NP-hard, though good heuristics now make optimal solutions obtainable for commercial-grade problems. Nevertheless, it makes a good testbed for optimization algorithms. We follow the development of <ref> [ Wong et al., 1988, Chapter 4 ] </ref> , which attacks the problem with simulated annealing. The operator set is sophisticated, involving manipulations to a partitioning of vertices in an acyclic constraint graph. <p> U = P w i , where u i is the fraction of track i that is unoccupied. <ref> [ Wong et al., 1988 ] </ref> They hand-tuned the coefficients and set p = 0:5; U = 10. To apply STAGE to this problem, we began with not the contrived function C but the natural objective function f (x) = w. <p> Our results provide some interesting insights. Experiments (A) and (B) show that hill-climbing easily gets stuck in very poor local optima, even when the "patience" parameter is set high (allowing it to evaluate more random moves before terminating). Experiment (C) shows that simulated annealing, as used in <ref> [ Wong et al., 1988 ] </ref> , does considerably better. Very surprisingly, when given plenty of computing time, the annealer of experiment (D) does better still. <p> Like the hand-tuned cost function C of <ref> [ Wong et al., 1988 ] </ref> (Equation 7), all the STAGE-learned cost functions assigned a relatively small positive weight to feature p, except on instance HYC7 (where STAGE's learning helped least). Unlike function C, they all assigned a large negative weight to feature U .
Reference: [ Zhang and Dietterich, 1995 ] <author> W. Zhang and T. G. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <pages> pages 1114-1120, </pages> <year> 1995. </year>
Reference-contexts: Two general families of learning algorithms apply here: reinforcement learning and meta-optimization. The reinforcement learning approach, dating back to Samuel's checkers player [ Samuel, 1959 ] but with more recent successes on backgammon [ Tesauro, 1992 ] and job-shop scheduling <ref> [ Zhang and Dietterich, 1995 ] </ref> , is based on approximating a special evaluation function known as the value function. The value function is defined to predict the best possible long-term outcome expected from each state; a greedy search guided by it will produce an optimal result. <p> Boyan [ 1992 ] replicated Tesauro's results and demonstrated improvements using modular neural networks. Tesauro's combination of TD () and neural networks has been applied to other domains, including elevator control [ Crites and Barto, 1996 ] and job-shop scheduling <ref> [ Zhang and Dietterich, 1995 ] </ref> . (I will discuss the scheduling application in detail in Section 3.2.) Nevertheless, it is important to note that when function approximators are used, TD () provides no guarantees of optimality. <p> This part of my thesis considers the possibility of automating this task using value function approximation. I consider two methods: Section 3.2 extends the approach of <ref> [ Zhang and Dietterich, 1995 ] </ref> , and Section 3.3 introduces a simpler approach based on predicting search outcomes. 3.2 Optimization as an MDP To my knowledge, the work by Zhang and Dietterich [ Zhang and Dietterich, 1995, Zhang, 1996 ] represents the only application of value function approximation to a <p> I consider two methods: Section 3.2 extends the approach of [ Zhang and Dietterich, 1995 ] , and Section 3.3 introduces a simpler approach based on predicting search outcomes. 3.2 Optimization as an MDP To my knowledge, the work by Zhang and Dietterich <ref> [ Zhang and Dietterich, 1995, Zhang, 1996 ] </ref> represents the only application of value function approximation to a combinatorial optimization problem.
Reference: [ Zhang, 1996 ] <author> W. Zhang. </author> <title> Reinforcement Learning for Job-Shop Scheduling. </title> <type> PhD thesis, </type> <institution> Oregon State University, </institution> <year> 1996. </year>
Reference-contexts: I consider two methods: Section 3.2 extends the approach of [ Zhang and Dietterich, 1995 ] , and Section 3.3 introduces a simpler approach based on predicting search outcomes. 3.2 Optimization as an MDP To my knowledge, the work by Zhang and Dietterich <ref> [ Zhang and Dietterich, 1995, Zhang, 1996 ] </ref> represents the only application of value function approximation to a combinatorial optimization problem. <p> A source reference for researchers interested in automatic learning and tuning of eval uation functions. 2. A new algorithm for learning approximate value functions in large, acyclic Markov decision problems. 3. Formulations for posing general combinatorial optimization problems as Value Function Approximation tasks, extending <ref> [ Zhang, 1996 ] </ref> . 4. A new algorithm for using prediction learning to bootstrap search algorithms. 5. Development of new techniques, based on memory-based stochastic optimization, for direct meta-optimization of evaluation functions. 6. An empirical comparison, on several large-scale applications, of the VFA and direct optimization approaches. 7. <p> However, if the input features can be made suitably instance-independent, then the cost of training an evaluation function can be amortized over the whole family of applicable instances. This was exactly the tradeoff found to be profitable in <ref> [ Zhang, 1996 ] </ref> , as discussed in Section 3.2. To investigate how problem-dependent learned evaluation functions are in channel routing, we re-ran experiment (G) on a suite of eight problems from the literature [ Chao and Harper, 1996 ] .
Reference: [ Zweben and Fox, 1994 ] <author> M. Zweben and M.S. Fox. </author> <title> Scheduling and Rescheduling with Iterative Repair. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <month> 36 </month>
Reference-contexts: Following the "repair-based scheduling" paradigm of <ref> [ Zweben and Fox, 1994 ] </ref> , they defined a search over the space of fully-specified schedules which meet the ordering constraints but not necessarily the resource constraints. <p> They trained the value function on a set of small problem instances. Training continued until performance stopped improving on a validation set of other problem instances. The N best-performing networks were saved and used for the comparisons against Zweben's iterative-repair system <ref> [ Zweben and Fox, 1994 ] </ref> , the previously best scheduler. The results showed that searches with the learned evaluation functions produced schedules as good as Zweben's in about half the CPU time.
References-found: 55

