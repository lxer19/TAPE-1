URL: http://krusty.eecs.umich.edu/people/charles/thesis.ps
Refering-URL: http://ai.eecs.umich.edu/people/charles/resume_ds.html
Root-URL: http://www.cs.umich.edu
Title: Dynamical System Representation, Generation, and Recognition of Basic Oscillatory Motion Gestures, and Applications for the
Author: by Charles Jacob Cohen 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Doctoral Committee: Professor Lynn Conway, Chair Associate Professor Daniel Koditschek Professor Pramod Khargonekar Assistant Professor Yili Liu  
Note: Associate Research Scientist Terry Weymouth  
Date: 1996  
Affiliation: (Electrical Engineering: Systems) in The University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 131 BIBLIOGRAPHY </institution>
Reference: [1] <author> P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman. </author> <title> Automated tracking and grasping of a moving object with a robotic hand-eye system. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 9(2) </volume> <pages> 152-165, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The Buhgler Juggler [55] can also be considered a local gesture control device. The system identifies and visually tracks a falling ball "gesture" and uses the dynamics of the gesture to command a robot arm to juggle via a "mirror law" calculation at field rate. Allen's system <ref> [1] </ref> tracks a moving toy train, which can be considered a gesture, to be grasped by a robot arm. Using optical flow, updated centroids of the train are obtained every 0.25 seconds, with a probabilistic method used to obtain a model of the motion.
Reference: [2] <author> V. I. Arnold. </author> <title> Ordinary Differential Equations. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: Four hundred years ago, Newton developed a parsimonious representation of physical motions based on their dynamic properties, _ x (t) = f (x): (3.3) A dynamical system is a mathematical model describing the evolution of all possible states in some state space as a function of time [30] <ref> [2] </ref>. The set of all possible states is a state space. Given an initial state, the set of all subsequent states as it evolves over time is a "trajectory" or "motion".
Reference: [3] <author> Thomas Baudel and Michel Beaudouin-Lafon. CHARADE: </author> <title> Remote control of objects using free-hand gestures. </title> <journal> Communications of the ACM, </journal> <volume> 36(7) </volume> <pages> 28-35, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Computation time is based on the number of view-based features used (20 in their example) and the number of view models stored (10). Through the use of specialized hardware and a parallel architecture, processing time is less than 100 ms. Baudel's system <ref> [3] </ref> uses symbolic gestures to control a Macintosh hypertext program. Gestures are tracked through the use of a data-glove and converted into hypertext program commands. Because Baudel's system identifies natural hand gestures unobtrusively with a data-glove, it is more intuitive to use than a standard mouse or stylus control system.
Reference: [4] <author> B. B. Beck. </author> <title> Animal Tool Behavior: The Use and Manufacture of Tools by Animals. </title> <publisher> Garland STPM Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: In this case, an examination of simple "observation-imitation" mimicry attempts could provide for interesting experimental trials. Just as certain creatures learn patterns and behaviors by mimicking the actions of others [63] [7] <ref> [4] </ref> [45], the interaction of two of our gesture recognition and control systems could be tested and evaluated by having the responder attempt to observe and then imitate or mimic the actions of the controller. 121 Successful mimicry occurs when the motion of one device converges onto and closely matches the
Reference: [5] <author> Bir Bhanu. </author> <title> Cad-based robot vision. </title> <booktitle> IEEE Vision, </booktitle> <pages> pages 13-16, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: The recognition of the VCR as the device "pointed to" can be done by including in the VI methods for orienting and scaling camera views with CAD models of the environment <ref> [5] </ref>. The oriented CAD models enable the calculation and maintenance of camera-view locations of known objects in the environment. The user's iconic gesturing within the returned images can then be interpreted with reference to the locations of these known objects located within those images.
Reference: [6] <author> R. L. Birdwhistell. </author> <title> Kinesics and Context; essays on body motion communication. </title> <address> Philadelphia, PA, </address> <year> 1970. </year>
Reference-contexts: Transparency is often associated with universality, a belief which states that some gestures have standard cross-cultural meanings. In reality, gesture meanings are very culturally dependent. Within a society, gestures have standard meanings, but no known body motion or gesture has the same meaning in all societies <ref> [6] </ref>. Even in ASL, few signs are so clearly transparent that a non-signer can guess their meaning without additional clues [38].
Reference: [7] <author> J. T. Bonner. </author> <title> The Evolution of Culture in Animals. </title> <publisher> Princeton University Press, </publisher> <address> Princeton N.J., </address> <year> 1980. </year>
Reference-contexts: In this case, an examination of simple "observation-imitation" mimicry attempts could provide for interesting experimental trials. Just as certain creatures learn patterns and behaviors by mimicking the actions of others [63] <ref> [7] </ref> [4] [45], the interaction of two of our gesture recognition and control systems could be tested and evaluated by having the responder attempt to observe and then imitate or mimic the actions of the controller. 121 Successful mimicry occurs when the motion of one device converges onto and closely matches
Reference: [8] <author> J. Borenstein and Y. Koren. </author> <title> Histogramic in-motion mapping for mobile robotic obstacle avoidance. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <pages> pages 278-288, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: A compete panoramic scan from all 24 sen 87 sors takes 160 msec. A PC-compatible single-board computer controls the sensors, running a special sonar-firing algorithm which reduces crosstalk. Another full PC runs an integrated obstacle-avoidance/map building algorithm, HIMM (Historgramic In-Motion Mapping), developed by Borenstein and Koren <ref> [8] </ref>. Because of the built-in obstacle avoidance code, the user controlling the mobile robot can give general gesture commands, without having to worry about the robot crashing into walls or other objects.
Reference: [9] <author> Johann Borenstein, H. R. Everett, and Liquiang Feng. </author> <title> Navigating Mobile Robots: Systems and Techniques. </title> <editor> A. K. Peters, </editor> <publisher> Ltd, </publisher> <address> Wellesley, MA, </address> <year> 1996. </year>
Reference-contexts: Such a mobile robot can be given a position or velocity command. A variety of internal and external sensors allow a robot to navigate through its environment <ref> [9] </ref>. External vision sensors, in addition to their navigational use, can also be used to perceive gesture commands. From the tour guide's perspective, it would be most efficient if the robotic system followed his gestural commands the way a normal human tour would.
Reference: [10] <author> S. Brown, H. Hefter, M. Mertens, and H. Freund. </author> <title> Disturbances in human arm movement trajectory due to mild cerebellar dysfunction. </title> <journal> Journal of neurosurgery psychiatry., </journal> <volume> 53(4) </volume> <pages> 306-313, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Therefore, the gesture velocities for "slow" and "fast" need to separated by a distinct amount. This amount can be determined empirically by watching people make circles and determining when the velocities can be consistently distinguished. Brown <ref> [10] </ref> has determined that humans can stay within 5% of the velocity in a desired line motion. Her experiments show that people are very adept at choosing distinct motion speeds when the human pretends there is a visual target velocity to track.
Reference: [11] <author> Stuart K. Card, Jock D. Mackinlay, and George G. Robertson. </author> <title> The design space of input devices. </title> <booktitle> In Proceedings of the CHI '90 Conference on Human Factors in Computing Systems, </booktitle> <pages> pages 117-124, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The technology used to recognize gestures, and the response derived from gestures, further complicates this issue. Gestures are interpreted to control computer memory and displays or to control actuated mechanisms. Human-computer interaction (HCI) studies usually focus on the computer input/output interface <ref> [11] </ref>. Many telerobotic studies analyze the performance of remotely controlled actuated mechanisms. The study of experimental systems which span both HCI and telerobotics will illuminate criteria for the design of a gestural input device for controlling actuated mechanisms located in remote environments.
Reference: [12] <author> Charles J. Cohen. </author> <title> The M-ROVER remote laboratory project. </title> <note> Report submitted to the Office of the Vice President for Research, </note> <institution> University of Michigan, </institution> <year> 1995. </year>
Reference-contexts: This new system is far less expensive than our prototype, and so will be much more available to users. The control of the M-ROVER is designed for use at one of the UMTV station sites. This project, called the Remote Laboratory Project (see <ref> [12] </ref> for more details), uses iconic gestures to control both the VC-C1 camera and the M-ROVER.
Reference: [13] <author> Charles J. Cohen and Lynn Conway. </author> <title> A method and system architecture for remote control of actuated mechanisms exploiting visual feedback augmented with gestural control information. </title> <journal> Submitted to IEEE Transactions on Systems, Man, and Cybernetics., </journal> <year> 1996. </year> <month> 132 </month>
Reference-contexts: The Trajectory and Task Planner, also located on a SUN workstation, converts the gesture command to a reference trajectory for the neck to track (see <ref> [13] </ref>). This module is equivalent to the Transformation module detailed in section 4.6. As before, this reference trajectory is sent to a Transputer computer network, which runs a inverse dynamics control loop, yielding torque commands for the pan and tilt motors [55]. 7.4.2 Example of Use. <p> We now detail a remote gesture for commanding the neck to center the camera at the icon's position . The details of implementing this and other basic commands can be found in <ref> [13] </ref>. While simple, this example shows how control icons can be placed in the visual stream by the local user using a basic visual signaler consisting of a character generator, a character positioner and a video mixer. <p> The VS and VI can be modified to control different devices using more complex icons and motions. The use of additional icons for the control of other actuated mechanisms is detailed in <ref> [13] </ref>. In this section, we first discuss the control of a more complex robot and the implementation of the VS/VI architecture on an inexpensive personal computer. Next we discuss using background visual information as an aid to gesture interpretation and control. <p> Such control could also be expanded to allow multiple users to control the same device, or allow them to control both their view and a remote view during a video-conferencing session (for further details, see <ref> [13] </ref>). 8.2.1 Design of a Gestural Control Language. A gestural control language could be developed for use with our gesture recognition and control system. Such a language's lexicon would be composed of a large number of gesture "words".
Reference: [14] <author> Link-Belt Construction Equipment Company. </author> <title> Operating safety: </title> <editor> Cranes and excava-tors, </editor> <year> 1987. </year>
Reference-contexts: Arbitrary gestures are those whose interpretation must be learned due to their opacity. Although they are not common in a cultural setting, once learned they can be used and understood without any complimentary verbal information. An example is the set of gestures used for crane operation <ref> [14] </ref>, discussed in section 2.4.1. Arbitrary gestures are useful to us, because they can be specifically created for use in device control. We plan to exploit gestures which are already arbitrarily defined and understood without any additional verbal information. 2.2 Voice and Handwriting Recognition: Parallel Issues for Gesture Recognition. <p> Two areas in which such gesture lexicons are used are crane and excavator control and runway traffic control. A sample set of crane and excavator control gestures are shown in figure 2.5 <ref> [14] </ref> 2 . These gestures are composed of oscillating planar motions, that is, circles or back-and-forth lines made in two dimensions in real world three dimensional space. Some of the gestures used to signal aircraft on a runway are also planar oscillators, as shown in figure 2.6 [67].
Reference: [15] <author> L. Conway. </author> <title> System and method for teleinteraction. </title> <type> U.S. Patent 5,444,476, </type> <month> August </month> <year> 1995. </year>
Reference-contexts: This concept is called "two-way pointing." Using two-way pointing as a form of tele-interaction, a person can coach another on using an object in their environment by pointing at appropriate locations and giving verbal information <ref> [15] </ref>. In this two-way environment, it is desirable to have control of the camera at both the local and distant site. Such control allows both users to be more actively involved in the interaction.
Reference: [16] <author> L. Conway and C. Cohen. </author> <title> Remote control through the visual information stream. </title> <type> U.S. Patent Pending, </type> <year> 1995. </year>
Reference-contexts: The following section describes the prototype iconic gestural control system developed for use in our visual communications environment. 7.4 Iconic Gestural Control System. We now showcase the above ideas in an Iconic Gestural Control System, as detailed in figure 7.9 <ref> [16] </ref> (patent pending). Such a system functions as follows: the visual information stream from the distant site is intercepted, visual command information is added to the video stream by using a Visual Signaler that encodes the desired control actions, and the merged stream is sent back to the original site.
Reference: [17] <author> L. Conway, R. Volz, and M. Walker. </author> <title> Teleautonomous systems: methods and architectures for intermingling autonomous and telerobotic technology. </title> <journal> RA, </journal> <volume> 6(2) </volume> <pages> 146-158, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Research to mitigate the effects of time delays has taken the form of studying system modeling and sensor display concepts [42]. Conway uses a simulator to aid in mechanism device control, exploiting a time-desynchronized planning model that projects the future trajectory of the telerobotic mechanism (see <ref> [17] </ref> and the patent [18]). Herzinger adds additional sensors to aid user mechanism control [31]. The time delay problems can also be alleviated if a symbol, representing a recognized gesture command, is sent to the remote site.
Reference: [18] <author> L. Conway, R. Volz, and M. Walker. </author> <title> Tele-autonomous system and method employing time/position synchrony/desynchrony. </title> <type> U.S. Patent 5,046,022, </type> <month> September </month> <year> 1991. </year>
Reference-contexts: Conway uses a simulator to aid in mechanism device control, exploiting a time-desynchronized planning model that projects the future trajectory of the telerobotic mechanism (see [17] and the patent <ref> [18] </ref>). Herzinger adds additional sensors to aid user mechanism control [31]. The time delay problems can also be alleviated if a symbol, representing a recognized gesture command, is sent to the remote site. However, this creates an additional "time delay": the time required to recognize the gesture.
Reference: [19] <author> Lynn Conway. </author> <title> The architecture, design and use of visual communication stations in engineering education. </title> <booktitle> In Technology Based Engineering Education Consortium Conference, </booktitle> <address> Sante Fe, NM, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: It is also easily expandable to include other devices. For example, video can be output from a PC and presented through the VCS by using standard video cards, a "Presenter Plus" module, etc <ref> [19] </ref>. 7.3.2 UMTV Network. The University of Michigan has a campus-wide broadband cable-TV network (composed of coaxial and fiber-optic lines). Modulated RF transmissions can be viewed using any cable ready television or VCR attached to the UMTV network, as is done with any cable television network.
Reference: [20] <author> Lynn Conway. </author> <title> The UMTV demonstration project: Experiences in system architecture and educational applications of interactive hybrid communications. </title> <booktitle> In World Conference on Engineering Education, </booktitle> <address> Minneapolis, MN, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: Signals transmitted on in-bound cables from any modulator are received at 100 a central hub, and then retransmitted on the out-bound cables. In addition, the UMTV "stations" have hybrid communication capabilities enabled by cross-linking computers and phone networks with the video network <ref> [20] </ref>. 7.3.3 Two-Way Tele-Interactions. Sites that have VCS stations with a video modulator can do more than just transmit information to the University. As seen in figure 7.6, by using two VCS stations modulated to different channels, two-way visual and audio communication between sites is possible.
Reference: [21] <author> John J. Craig. </author> <title> Introduction to Robotics: Mechanics and Control. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: A simple method is to postpone CARMEL's response until the gesture command is completed. For instance, after completing a gesture command, the human signals CARMEL to respond by turning on the flashlight. As CARMEL moves, the sensor camera neck would continue to track the flashlight via visual servoing <ref> [21] </ref>, which aims to keep the flashlight image centered in the camera's field of view. CARMEL would stop moving when the human turns the flashlight off or when the sensor module looses track of the flashlight.
Reference: [22] <author> Jill Crisman and Charles E. Thorpe. SCARF: </author> <title> A color vision system that tracks roads and intersections. </title> <journal> RA, </journal> <volume> 9(1) </volume> <pages> 49-58, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: A representative system architecture is shown in figure 2.3. Mechanisms. Road-following systems used by some mobile robots can be conceptualized as examples of local control through gestures. Two representative systems are Crisman's <ref> [22] </ref> and Dickmanns' [26] work. Crisman's system uses road features detected through monocular vision as navigation cues for a mobile robot platform. On a reduced color image, classification is performed by determining the probability of each pixel representing a road surface.
Reference: [23] <author> Yunato Cui and John J. Weng. SHOSLIF-M: </author> <title> SHOSLIF for motion understanding (phase i for hand sign recognition). </title> <type> Technical Report CPS 94-68, </type> <month> December </month> <year> 1994. </year>
Reference-contexts: The system picks the model which has the highest probability of generating the data stream. States consist of a probability distribution of the hand features (as opposed to the pose itself). Weng's SHOSLIF identifies the most discriminating features of an image through a multi-class, multivariate discriminant analysis <ref> [23] </ref>. These features are categorized in a space partition tree. SHOSLIF can recognize 28 ASL signs. The calculations are multiplication intensive, based on the size of the image, the number of images in a gesture sequence, and the number of signs in the lexicon. <p> We showed how to exploit a two-way visual environment to enable humans to gestu-rally control remote devices. 8.2 Further System Extensions and Future Research Areas. Our system recognizes oscillatory motion gestures. If our system were attached as a front end to an ASL recognition system (such as SHOSHLIF <ref> [23] </ref> or Starner's system [64]), then a wider range of gestures than those identified by the individual systems could be recognized.
Reference: [24] <author> Cybermation. </author> <title> K2A mobile platform. Commercial Offer, </title> <year> 1987. </year>
Reference-contexts: CARMEL (C omputer-Aided Robitics for M aintenance, E mergency, and Life support) is a Cybermation mobile platform, containing a three-wheel synchro-drive which permits omnidirectional steering <ref> [24] </ref> (see figure 6.3. CARMEL is different from many other mobile robots because its base has a fixed orientation. Therefore, only its final Cartesian floor position can be controlled. CARMEL has a maximum travel speed of 0.78 m/sec and a maximum steering rate of 120 deg/sec.
Reference: [25] <author> Trevor J. Darrell and Alex P. Pentland. </author> <title> Space-time gestures. </title> <booktitle> In IEEE Conference on Vision and Pattern Recognition, </booktitle> <address> NY, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Several hours are required to train the neural network using a SUN/4 workstation. Darrell's monocular vision processing system accurately identifies a wide variance of yes/no hand gestures <ref> [25] </ref> . Gestures are represented by a view-based approach, and stored patterns are matched to perceived gestures using dynamic time warping. View-based vision approaches also permit a wide range of gestural inputs when compared to the mouse and stylus input devices. <p> Unfortunately, if a gesture is generated at a local site and interpreted at a remote site, time delays cause further gesture recognition difficulties: the recognition must now take into account blurred and warped gestures. With additional processing, Darrell's system (see section 2.3.1) <ref> [25] </ref> could help identify "warped" 13 gestures. A special communication environment could be developed to take advantage of gestural inputs for the control of devices. Since gestures are visual, a video communication system could transmit a gesture from the local site to the remote site.
Reference: [26] <author> E. D. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. </title> <booktitle> Machine Vision and Applications, </booktitle> <pages> pages 223-240, </pages> <year> 1988. </year>
Reference-contexts: A representative system architecture is shown in figure 2.3. Mechanisms. Road-following systems used by some mobile robots can be conceptualized as examples of local control through gestures. Two representative systems are Crisman's [22] and Dickmanns' <ref> [26] </ref> work. Crisman's system uses road features detected through monocular vision as navigation cues for a mobile robot platform. On a reduced color image, classification is performed by determining the probability of each pixel representing a road surface.
Reference: [27] <author> Gregory D. Hager, Wen-Chung Chang, and A. S. Morse. </author> <title> Robot feedback control based on stereo vision: Towards calibration-free hand-eye coordination. </title> <booktitle> In IEEE Int. Conf. </booktitle> <address> Robt. </address> <note> and Aut., page (to appear), </note> <institution> San Diego California, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Because it is difficult to determine the three dimensional position of the flashlight using monocular vision, we simplify the system by adapting Greg Hager's method of using image plane coordinates <ref> [27] </ref>. Positions and velocities are represented as image plane coordinates (see figure 4.7). For both x and y motions, velocity is computed by averaging the differences between the previous four positions. <p> Using the Cyclops vision system, the current sensor module (see section 4.4) tracks only one feature. However, Cyclops was designed to simultaneously track multiple features [55]. Other systems, which simultaneously track multiple complex features, such as Hager's vision system <ref> [27] </ref>, are also available. These systems would be used to identify the different points of attention or "feature points". The motion of such points would be tracked to enable the recognition of multiple simultaneous gestures.
Reference: [28] <author> W. S. Harwin and R. D. Jackson. </author> <title> Analysis of intentional head gestures to assist computer access by physically disabled people. </title> <journal> Journal of Biomedical Engineering, </journal> <volume> 12 </volume> <pages> 193-198, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: People are also very adept at learning new arbitrary gestures. Gesturing is natural for humans, and only a short amount of training is required before people can consistently use new gestures to communicate information or control devices (see [69], [29], and <ref> [28] </ref>). Wolf also discovered that, without prompting, test subjects used very similar gestures for the same operations, and even used the same screen drawn gestures days later. Hauptmann also found a high degree of similarity in the gesture types used by different people to perform the same manipulations.
Reference: [29] <author> A. G. Hauptmann. </author> <title> Speed and gestures for graphic image manipulation. </title> <booktitle> In Computer Human Interaction 1989 Proceedings, </booktitle> <pages> pages 241-245, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Hauptmann attempted to answer similar questions by examining three dimensional spatial gestures which were used to rotate, translate, and scale three dimensional graphic objects on a computer screen <ref> [29] </ref>. Although the uses were drastically different (text versus graphics), the concepts and conclusions were strikingly similar. People consistently used the same gestures for specific commands. <p> People are also very adept at learning new arbitrary gestures. Gesturing is natural for humans, and only a short amount of training is required before people can consistently use new gestures to communicate information or control devices (see [69], <ref> [29] </ref>, and [28]). Wolf also discovered that, without prompting, test subjects used very similar gestures for the same operations, and even used the same screen drawn gestures days later. Hauptmann also found a high degree of similarity in the gesture types used by different people to perform the same manipulations.
Reference: [30] <author> Morris W. Hirsch and Stephen Smale. </author> <title> Differential Equations, Dynamical Systems, and Linear Algebra. </title> <publisher> Academic Press, Inc., </publisher> <address> Orlando, Fla., </address> <year> 1974. </year> <month> 133 </month>
Reference-contexts: Four hundred years ago, Newton developed a parsimonious representation of physical motions based on their dynamic properties, _ x (t) = f (x): (3.3) A dynamical system is a mathematical model describing the evolution of all possible states in some state space as a function of time <ref> [30] </ref> [2]. The set of all possible states is a state space. Given an initial state, the set of all subsequent states as it evolves over time is a "trajectory" or "motion".
Reference: [31] <author> G. Hirzinger, J. Heindl, and K. Landzettel. </author> <title> Predictor and knowledge based telerobotic control concepts. </title> <booktitle> In Proc. 1989 IEEE Int. Conf. Robotics and Automat., </booktitle> <pages> pages 1768-1777, </pages> <address> Scottsdale, AZ, </address> <month> May 14-19 </month> <year> 1989. </year>
Reference-contexts: Conway uses a simulator to aid in mechanism device control, exploiting a time-desynchronized planning model that projects the future trajectory of the telerobotic mechanism (see [17] and the patent [18]). Herzinger adds additional sensors to aid user mechanism control <ref> [31] </ref>. The time delay problems can also be alleviated if a symbol, representing a recognized gesture command, is sent to the remote site. However, this creates an additional "time delay": the time required to recognize the gesture. The Robogest architecture uses such a gestural control method.
Reference: [32] <author> Paul Horowitz and Winfield Hill. </author> <title> The Art of Electronics. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: Therefore, the VC-C1 is free to immediately respond to gesture commands because the second camera is used to gesture tracking. The RS-232 communication system is ideal for communication between the transputer network and both CARMEL and the VC-C1. RS-232 is both a hardware connection and a communication software protocol <ref> [32] </ref>. As a hardware connection, an RS-232 cable has a transmit line, a receive line, and a ground. The cable also has two additional lines (CTS and RTS) which are used for a communication handshaking protocol. CARMEL and the VC-C1 are each designed with a pre-installed RS-232 interface.
Reference: [33] <author> Canon Inc. </author> <title> VC-C1 Communications Camera. Instruction Manual, </title> <year> 1994. </year>
Reference-contexts: The PC interface receives commands for these controllable features from the transputer network through an on-board RS-232 communication card [34]. 6.2.2 VC-C1 Description. The VC-C1 is a video camera encased in a controllable pan-tilt neck platform <ref> [33] </ref>. The pan axis and tilt axis are driven by stepper motors. The pan axis' range of motion is 50 degrees with a maximum velocity of 38 deg/sec. The tilt axis' range of motion is 20 degrees with a maximum velocity of 35 deg/sec.
Reference: [34] <author> DigiBoard Inc. </author> <title> Digiboard digichannel PC/8 installation guide reference manual, </title> <year> 1989. </year>
Reference-contexts: The PC interface receives commands for these controllable features from the transputer network through an on-board RS-232 communication card <ref> [34] </ref>. 6.2.2 VC-C1 Description. The VC-C1 is a video camera encased in a controllable pan-tilt neck platform [33]. The pan axis and tilt axis are driven by stepper motors. The pan axis' range of motion is 50 degrees with a maximum velocity of 38 deg/sec.
Reference: [35] <author> Videonics Inc. </author> <title> MX-1 digital video mixer instruction manual, </title> <year> 1994. </year>
Reference-contexts: To recognize planar oscillators, a vision system which can recognize icons at frame rate is required. More advanced vision systems could allow the recognition of planar oscillating motions created by human hands. The image of a hand would be locally superimposed onto the video stream using available Chromakey technology <ref> [35] </ref>, thus allowing 115 the hand's motion to reach the remote site for gesture identification.
Reference: [36] <author> J. A. Scott Kelso. </author> <title> Dynamic Patterns: The Self-Organization of Brain and Behavior. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: Specifically, we have shown how one type of human behavior gesturing- can be parameterized and classified in a computationally efficient manner using dynamical systems theory methods. Kelso, in his book entitled Dynamic Patterns <ref> [36] </ref>, proposes that many motions and behaviors from humans, animals, and devices are best represented, parsimoniously, as dynamical systems.
Reference: [37] <author> Adam Kendon. </author> <title> Conducting Interaction: Patterns of behavior in focused encounters. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: People frequently use gestures to communicate. Gestures are used for everything from pointing at a person to get their attention to conveying information about space and temporal characteristics <ref> [37] </ref>. Evidence indicates that gesturing does not simply embellish spoken language, but is part of the language generation process [47].
Reference: [38] <author> E. S. Klima and U. Bellugi. </author> <title> Language in another mode. Language and brain: Developmental aspects, </title> <journal> Neurosciences research program bulletin, </journal> <volume> 12(4) </volume> <pages> 539-550, </pages> <year> 1974. </year>
Reference-contexts: In reality, gesture meanings are very culturally dependent. Within a society, gestures have standard meanings, but no known body motion or gesture has the same meaning in all societies [6]. Even in ASL, few signs are so clearly transparent that a non-signer can guess their meaning without additional clues <ref> [38] </ref>. Fortunately, this means that gestures used for device control can be freely chosen. 1 Semiotic refers to a general philosophical theory of signs and system that deals with their function in both artificially constructed and natural languages. 4 The centrifugal-centripetal dichotomy refers to the intentionality of a gesture.
Reference: [39] <author> David Kortenkamp, Eric Huber, and R. Peter Bonasso. </author> <title> Recognizing and interpreting gestures on a mobile robot. </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI '96), page (to appear), </booktitle> <year> 1996. </year>
Reference-contexts: Using optical flow, updated centroids of the train are obtained every 0.25 seconds, with a probabilistic method used to obtain a model of the motion. Kortenkamp uses static gestures, in the form of six human arm poses, <ref> [39] </ref>, to control a mobile robot. Small regions in 3-D visual space, proximity spaces, are used to concentrate the system's visual attention on various parts of a human agent. Vision updates occur at frame rate, but the system can only track gestures which move less than 36deg=sec.
Reference: [40] <author> P. R. Kumar and P. Varaiya. </author> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1986. </year>
Reference-contexts: The separate graphs show that each pair of (x i ; y i ) data points results in a different best fitting line. cursive Linear Least Squares. Best Fitting Line. Mathematical Overview. A method of incrementally updating the parameter is described below. For full details, see <ref> [40] </ref>. The concept is illustrated in figure 3.14. After the first two data points determine the best fit line, each additional data point slightly adjusts the line to a new best fit. <p> As this method requires knowledge of all the data points a-priori, this type of regression is called a "batch" method. We now describe the regression batch method, used to determine the parameters for a generic higher order LIP dynamical system given full state information <ref> [40] </ref>. 124 Figure A.1: The Regression Batch Method represented by finding the equi-librium point for a parameter line attached to energy function "springs". <p> The recursive LLS method uses a tuning rule for updating the parameter vector without resorting to a matrix inversion. See <ref> [40] </ref> for full details.
Reference: [41] <author> Y. Lebrun. </author> <booktitle> Neurolinguistic models of language and speech, </booktitle> <pages> pages 1-30. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Gestures which are elements of an autonomous semiotic system are those used in a gesture language, such as ASL. On the other hand, gestures which are created as partial elements of multisemiotic activity are gestures which accompany other languages, such as oral ones <ref> [41] </ref>. We are primarily interested in gestures which are created as their own independent, semiotic language. 2.1.2 Gesture Typologies. Another standard gesture classification scheme uses three categories: arbitrary, mimetic, and deictic. [51]. In mimetic gestures, motions form an object's main shape or representative feature [71].
Reference: [42] <author> A. Liu, G. Tharp, L. French, S. Lai, and L. Stark. </author> <title> Some of what one needs to know about using head-mounted displays to improve teleoperator performance. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 9(5) </volume> <pages> 638-649, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The range of possible controls is limited because of added instabilities arising from time delays in feedback loops. Research to mitigate the effects of time delays has taken the form of studying system modeling and sensor display concepts <ref> [42] </ref>. Conway uses a simulator to aid in mechanism device control, exploiting a time-desynchronized planning model that projects the future trajectory of the telerobotic mechanism (see [17] and the patent [18]). Herzinger adds additional sensors to aid user mechanism control [31].
Reference: [43] <author> Andrej Ljolje and Stephen E. Levinson. </author> <title> Development of an acoustic-phonetic hidden markov model for continuous speech recognition. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39(1) </volume> <pages> 29-39, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: An HMM is created for each word (string of phonemes) in a given lexicon. One of the tasks in isolated speech recognition is to measure an observed sequence of phonetic units and determine which HMM was most likely to generate such a sequence. <ref> [43] </ref> [54]. From some points of view, handwriting can be considered a type of gesture. On-line (also called "real time" or "dynamic") recognition machines identify handwriting as a user writes.
Reference: [44] <author> Pattie Maes, Trevor Darrell, Bruce Blumberg, and Alex Pentland. </author> <title> The alive system: Full-body interaction with autonomous agents. </title> <booktitle> In Computer Animation '95 Conference, </booktitle> <publisher> IEEE Press, </publisher> <address> Geneva, Switzerland, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: This system had difficulty identifying gestures which differed in their dynamic 10 phase, for instance, when one gesture was twice the speed of another. The ALIVE II system <ref> [44] </ref> identifies full body gestures, as opposed to hand gestures, through basic image processing techniques. Body gestures are used to control simulated mechanisms ("virtual creatures") located in computer memory ("virtual environment").
Reference: [45] <author> Valerie Manusov. </author> <title> Mimicry or synchrony: The effects of intentionality attributions for nonverbal mirroring behavior. </title> <journal> Communication Quarterly, </journal> <volume> 40(1) </volume> <pages> 69-83, </pages> <month> Winter </month> <year> 1992. </year>
Reference-contexts: In this case, an examination of simple "observation-imitation" mimicry attempts could provide for interesting experimental trials. Just as certain creatures learn patterns and behaviors by mimicking the actions of others [63] [7] [4] <ref> [45] </ref>, the interaction of two of our gesture recognition and control systems could be tested and evaluated by having the responder attempt to observe and then imitate or mimic the actions of the controller. 121 Successful mimicry occurs when the motion of one device converges onto and closely matches the motion
Reference: [46] <author> K. V. Mardia, N. M. Ghali, T. J. Hainsworth, M. Howes, and N. Sheehy. </author> <title> Techniques for online gesture recognition on workstations. </title> <journal> Image and Vision Computing, </journal> <volume> 11(5) </volume> <pages> 283-294, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The representative architecture for these systems is show in figure 2.2. and Display. A basic gesture input device is the word processing tablet. Through the use of mouse drawn gestures, Mardia's system allows the editing of display graphics <ref> [46] </ref>. Similarly, 9 Rubine's system uses gestures (stylus markings) on a graphics tablet to represent word processing commands [56]. In these representative systems, two dimensional hand gestures are sent via an input device to the computer's memory and appear on the computer monitor.
Reference: [47] <author> D. McNeill and E. Levy. </author> <booktitle> Conceptual Representations in Language Activity and Gesture, </booktitle> <pages> pages 271-295. </pages> <publisher> John Wiley and Sons Ltd, </publisher> <year> 1982. </year> <month> 134 </month>
Reference-contexts: People frequently use gestures to communicate. Gestures are used for everything from pointing at a person to get their attention to conveying information about space and temporal characteristics [37]. Evidence indicates that gesturing does not simply embellish spoken language, but is part of the language generation process <ref> [47] </ref>. Biologists define "gesture" broadly, stating, "the notion of gesture is to embrace all kinds of instances where an individual engages in movements whose communicative intent is paramount, manifest, and openly acknowledged" [51]. Gestures associated with speech are referred to as gesticulation. <p> However, these studies were among the first steps toward designing such a system. A further complication in gesture recognition is determining which features of the gesture generator are used. In a study of representing language through gestures, McNeill and Levy <ref> [47] </ref> have identified gestures according to physical properties. These properties include hand configuration, orientation of the palm, and direction of movement. McNeill and Levy note that gestures have a preparatory phase, an actual gesture phase, and a retraction phase 18 (see figure 2.8).
Reference: [48] <author> Kouichi Murakami and Hitomi Taguchi. </author> <title> Gesture recognition using recurrent neural networks. </title> <journal> Journal of the ACM, </journal> <volume> 1(1) </volume> <pages> 237-242, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Rubine's system requires the computation of 13 different features, with five features requiring multiplications and additions based on the total number of data points used to create the gesture. Again, a linear evaluation function is used to discriminate between gestures. Murakami's system inputs the gesture data via a data-glove <ref> [48] </ref>. Instead of geometric modeling, a neural network identifies gestures based on a finger alphabet containing 42 symbols. The data-glove, which provides 13 features, allows a wider range of motions than the mouse and stylus input devices, which enables a richer vocabulary of gestural inputs and responses.
Reference: [49] <author> Kumpati S. Narendra and Anuradha M. Annaswamy. </author> <title> Stable Adaptive Systems. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: An adaptive estimator identifies the unknown parameters of a LIP/LIS system while recovering unmeasured states at the same time. We examine a specific adaptive estimator, the Series-Parallel Observer, developed by Luders and Narendra <ref> [49] </ref>. Reasons for Use. Our vision system produces only position measurements. However, we intended to use second order models which require both position and velocity measurements. <p> The Series-Parallel observer provides an estimate for the unknown state (see figure 3.8), allowing us to use it for our error calculations (see section 3.6.1). It is important to note that this observer only is defined for LIS/LIP systems. 29 tification versus Adaptive Estimation. Mathematical Overview. As developed in <ref> [49] </ref>, any controllable and observable n-th order single-input single output linear time-invariant (LTI) system (presented a standard format) _ x = Ax + bu; (3.6) which is input-output equivalent to the LTI series-parallel format system: _x 1 = x 1 + T w (3.7) with and w defined as: , c
Reference: [50] <author> Kumpati S. Narendra and Jeyendran Balakrishnan. </author> <title> Improving transient response of adaptive control systems using multiple models and switching. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39 </volume> <pages> 1861-1866, </pages> <month> Sept </month> <year> 1994. </year>
Reference-contexts: In this work, they create a bank of indirect controllers which are tuned on line but whose identification models have different initial estimates of the plant parameters. When the plant is identified, the bin that best matches that identification supplies a required control strategy for the system <ref> [50] </ref>. Each bin's model, which has parameters that tune it to a specific gesture, is used to predict the future position and velocity of the motion. This prediction is made by feeding the current state of the motion into the gesture model.
Reference: [51] <author> Jean-Luc Nespoulous, Paul Perron, and Andre Roch Lecours. </author> <title> The Biological Foundations of Gestures: Motor and Semiotic Aspects. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hills-dale, NJ, </address> <year> 1986. </year>
Reference-contexts: Biologists define "gesture" broadly, stating, "the notion of gesture is to embrace all kinds of instances where an individual engages in movements whose communicative intent is paramount, manifest, and openly acknowledged" <ref> [51] </ref>. Gestures associated with speech are referred to as gesticulation. Gestures which function independently of speech are referred to as autonomous. Autonomous gestures can be organized into their own communicative language, such as American Sign Language (ASL). Autonomous gestures can also represent motion commands. <p> In the following subsections, we examine some various ways in which biologists and sociologists define gestures to discover if there are gestures ideal for use in device control. 2.1.1 Gesture Dichotomies. One classification method categorizes gestures using four dichotomies: act-symbol, opacity-transparency, autonomous semiotic 1 -multisemiotic, and centrifugal-centripetal (intentional) <ref> [51] </ref>. The act-symbol dichotomy refers to the notion that some gestures are pure actions, while others are intended as symbols. <p> We are primarily interested in gestures which are created as their own independent, semiotic language. 2.1.2 Gesture Typologies. Another standard gesture classification scheme uses three categories: arbitrary, mimetic, and deictic. <ref> [51] </ref>. In mimetic gestures, motions form an object's main shape or representative feature [71]. For instance, a chin sweeping gesture can be used to represent a goat by alluding to its beard. These gestures are intended to be transparent.
Reference: [52] <author> D. Pisoni, H. Nusbaum, and B. Greene. </author> <title> Perception of synthetic speech generated by rule. </title> <booktitle> Proceedings of the IEEE 73, </booktitle> <pages> pages 1665-1676, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Although this rate seems very high, these results occur in laboratory environments. Also, studies have shown that humans have an individual word recognition rate of 99:2% <ref> [52] </ref>. State of the art speech recognition systems, which have the capability to understand a large vocabulary, use HMMs. HMMs are also used by a number of gesture recognition systems (see section 2.3.1). In some speech recognition systems, the states of an HMM represent phonetic units.
Reference: [53] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press Syndicate, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Again, x 1 is the position state, and x 2 is the velocity state. To use the models described here on digital a computer, a fourth-order Runge-Kutta integration method is used <ref> [53] </ref>. Simulations showed that a sampling rate of 60 Hz is sufficiently small to allow the use of this method. 3.5.1 Linear with Offset Component. The linear with offset component model is the most basic second order linear system. <p> The basic idea of fitting by regression is simply described by examining how to fit n two dimensional data points (x i ; y i ); i = 1; :::; n, to a polynomial model that has m adjustable parameters j ; j = 1; :::; m. See <ref> [53] </ref> for a general discussion. Fitting two tunable parameters (m = 2, where 1 and 2 represent the line's slope and y-axis intercept) to the data is equivalent to determining a line that best represents the data. Fitting three tunable parameters (m = 3) is equivalent to determining a parabola.
Reference: [54] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2), </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: An HMM is created for each word (string of phonemes) in a given lexicon. One of the tasks in isolated speech recognition is to measure an observed sequence of phonetic units and determine which HMM was most likely to generate such a sequence. [43] <ref> [54] </ref>. From some points of view, handwriting can be considered a type of gesture. On-line (also called "real time" or "dynamic") recognition machines identify handwriting as a user writes.
Reference: [55] <author> Alfred A. Rizzi, Louis L. Whitcomb, and D. E. Koditschek. </author> <title> Distributed real-time control of a spatial robot juggler. </title> <journal> IEEE Computer, </journal> <volume> 25(5), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: Each feature's trajectory is a gesture used to control the attached mobile platform. A Linear Least Squares calculation (with additional computations to handle noise) is used to recognize gestures at video field rate. The Buhgler Juggler <ref> [55] </ref> can also be considered a local gesture control device. The system identifies and visually tracks a falling ball "gesture" and uses the dynamics of the gesture to command a robot arm to juggle via a "mirror law" calculation at field rate. <p> The gesture recognition system is implemented on a transputer based distributed control system. What follows is based directly on the transputer implementation found in <ref> [55] </ref> for use with the Buhgler Juggler. A transputer is an Inmos T800 (or T805) microprocessor placed on an XP/DCS board. The standard Inmos T800 processor is a 32-bit scalar processor capable of 10 MIPS and 1.5 Mflops. <p> One aspect of using image plane coordinates and working with two dimensional data is that the gesture must be created parallel to the image plane. A non-parallel gesture will 55 yield a warped projection onto the image plane, resulting in a possible misclassification. The Cyclops vision system <ref> [55] </ref> can track one or more image plane "ellipse-like blobs", computing each blob's attributes: its center in image plane coordinates, its size in pixels, and its skewness. That is, it tracks the blob's zero, first, and second moments. Our system tracks only one gesture, which corresponds to tracking one flashlight. <p> Using the Cyclops vision system, the current sensor module (see section 4.4) tracks only one feature. However, Cyclops was designed to simultaneously track multiple features <ref> [55] </ref>. Other systems, which simultaneously track multiple complex features, such as Hager's vision system [27], are also available. These systems would be used to identify the different points of attention or "feature points". The motion of such points would be tracked to enable the recognition of multiple simultaneous gestures. <p> This module is equivalent to the Transformation module detailed in section 4.6. As before, this reference trajectory is sent to a Transputer computer network, which runs a inverse dynamics control loop, yielding torque commands for the pan and tilt motors <ref> [55] </ref>. 7.4.2 Example of Use. We now detail a remote gesture for commanding the neck to center the camera at the icon's position . The details of implementing this and other basic commands can be found in [13]. <p> With a future state prediction, a camera attached to an actuated platform could reposition itself to keep the motion within the camera's field of view by anticipating the joint's positions, thus allowing it to track a quick motion. This is similar to the action performed by the Buhgler Juggler <ref> [55] </ref>, except while their system updates the camera's focus of attention based on a falling ball "gesture", this new anticipatory system could update a camera's focus of attention based on any known gesture in the system's lexicon.
Reference: [56] <author> Dean Rubine. </author> <title> Specifying gestures by example. </title> <journal> Computer Graphics, </journal> <volume> 25(4) </volume> <pages> 329-337, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: A basic gesture input device is the word processing tablet. Through the use of mouse drawn gestures, Mardia's system allows the editing of display graphics [46]. Similarly, 9 Rubine's system uses gestures (stylus markings) on a graphics tablet to represent word processing commands <ref> [56] </ref>. In these representative systems, two dimensional hand gestures are sent via an input device to the computer's memory and appear on the computer monitor. These symbolic gestures are identified as editing commands through geometric modeling techniques. The commands are then executed, modifying the document stored in computer memory.
Reference: [57] <author> Alexander I. Rudnicky, Alexander G. Hauptmann, and Kai-Fu Lee. </author> <title> Survey of current speech technology. </title> <journal> Communications of the ACM, </journal> <volume> 37(3) </volume> <pages> 52-57, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Typical speech recognition systems match transformed speech against a stored representation. Most systems use some form of spectral representation, such as spectral templates or hidden Markov models (HMM). Speech recognition systems are classified along the following dimensions <ref> [57] </ref>: * Speaker dependent versus Independent: Can the system recognize the speech of many different individuals without training or does it have to be trained for a specific voice? Currently, speaker dependent systems are more accurate, because they do not need to account for large variations in words. * Discrete or
Reference: [58] <author> Shankar Sastry and Marc Bodson. </author> <title> Adaptive Control: Stability, Convergence, and Robustness. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: The following mathematical overview illustrates that the convergence properties of the on-line gradient descent method are inadequate for our use. Mathematical Overview. For a general review of gradient descent methods, see <ref> [58] </ref>. However, a detailed descrip tion for our problem is now presented.
Reference: [59] <author> J. Schlenzig, E. Hunter, and R. Jain. </author> <title> Recursive identification of gesture inputs using hidden markov models. </title> <booktitle> Proceedings of the the Second Annual Conference on Applications of Computer Vision, </booktitle> <month> December </month> <year> 1994. </year>
Reference-contexts: However, this creates an additional "time delay": the time required to recognize the gesture. The Robogest architecture uses such a gestural control method. The Robogest system controls a robot with static hand gestures, which are recognized, converted into a command, and sent to a remote site <ref> [59] </ref>. A hidden Markov model describes the six hand gestures, and recognition occurs at 0.5 Hz. Each gesture is paired with various robot behaviors, such as turn, accelerate, and stop. Other telerobotic systems use a joystick to control the remote mechanism (see figure 2.4) [61].
Reference: [60] <author> R. N. Shepard and J. Metzler. </author> <title> Mental rotation of three-dimensional objects. </title> <journal> Science, </journal> <volume> 171 </volume> <pages> 701-703, </pages> <year> 1971. </year>
Reference-contexts: Therefore, this research illustrates that accurate recognition of gestures from an intuitively created arbitrary lexicon (as defined in 2.1.2) is possible. In both studies, humans were used to interpret the gestures, and humans are very adept at handling noisy data and recognizing shifted data <ref> [60] </ref>. Unfortunately, the problems of computers recognizing specific gestures were not addressed. However, these studies were among the first steps toward designing such a system. A further complication in gesture recognition is determining which features of the gesture generator are used.
Reference: [61] <author> T. B. Sheridan. </author> <title> Space teleoperation through time delay: </title> <journal> Review and prognosis. IEEE Transactions on Robotics and Automation, </journal> <volume> 9(5) </volume> <pages> 592-606, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Certain challenges arise when controlling mechanisms located a very large distance, due to the effect of communication time delays <ref> [61] </ref>. For example, when relaying a control command from the earth to synchronous earth orbit, the round trip time delay for visual feedback is several tenths of a second. The range of possible controls is limited because of added instabilities arising from time delays in feedback loops. <p> A hidden Markov model describes the six hand gestures, and recognition occurs at 0.5 Hz. Each gesture is paired with various robot behaviors, such as turn, accelerate, and stop. Other telerobotic systems use a joystick to control the remote mechanism (see figure 2.4) <ref> [61] </ref>. Consistent with our previous observations, a joystick is used to input "gestures" which control the remote actuated mechanism.
Reference: [62] <author> J. F. Sousa-Poza and R. Rohrberg. </author> <title> Body movement in relation to type of information (person- and non-person oriented) and cognitive style (file dependence). </title> <journal> Human Communication Research, </journal> <volume> 4(1), </volume> <year> 1977. </year>
Reference-contexts: Centrifu--gal gestures are directed toward a specific object, while centripetal gestures are not <ref> [62] </ref>. We are concerned with gestures which are directed toward the control of a specific object. Gestures which are elements of an autonomous semiotic system are those used in a gesture language, such as ASL.
Reference: [63] <author> Michael P. </author> <title> Speed. </title> <journal> Muellerian mimicry and the psychology of predation. Animal Be-haviour, </journal> <volume> 45(3) </volume> <pages> 571-580, </pages> <month> March </month> <year> 1993. </year> <month> 135 </month>
Reference-contexts: In this case, an examination of simple "observation-imitation" mimicry attempts could provide for interesting experimental trials. Just as certain creatures learn patterns and behaviors by mimicking the actions of others <ref> [63] </ref> [7] [4] [45], the interaction of two of our gesture recognition and control systems could be tested and evaluated by having the responder attempt to observe and then imitate or mimic the actions of the controller. 121 Successful mimicry occurs when the motion of one device converges onto and closely
Reference: [64] <author> Thad Starner and Alex Pentland. </author> <title> Visual recognition of american sign language using hidden markov models. </title> <booktitle> IEEE International Symposium on Computer Vision, </booktitle> <month> Novem-ber </month> <year> 1995. </year>
Reference-contexts: Then, high resolution image processing is used to determine the finger's exact position. Tracking occurs at 7 frames per second. Starner and Pentland's system uses an HMM method to recognize forty American Sign Language gestures <ref> [64] </ref>. Acquired feature vectors are run through all possible sets of five-word sentences. The probability of the data stream being generated by each HMM model is then determined. The system picks the model which has the highest probability of generating the data stream. <p> Our system recognizes oscillatory motion gestures. If our system were attached as a front end to an ASL recognition system (such as SHOSHLIF [23] or Starner's system <ref> [64] </ref>), then a wider range of gestures than those identified by the individual systems could be recognized.
Reference: [65] <author> S. Sull and N. Ahuja. </author> <title> Integrated matching and segmentation of multiple features in two views. </title> <booktitle> In Proc. of Asian Conference on Computer Vision, </booktitle> <pages> pages 213-216, </pages> <address> Osaka, Japan, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In order for the visual interpreter to make precise interpretations of locations in the return image, the incoming video signal may need to be re-scaled and re-registered with the outgoing image, using feature matching methods, such as those found in <ref> [65] </ref>. When integrated in the VI, a feature matching module compares the outgoing and incoming images, and adjusts the interpreted locations of icons according (see figures 7.16 and 7.17).
Reference: [66] <author> C. Tappert, C. Suen, and T. Wakahara. </author> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(8) </volume> <pages> 787-808, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: They reach reported rates of 95% due only to very careful writing. They are best used for filling out forms which have predefined prototypes and set areas for characters. For a more detailed overview of handwriting tablets, consult <ref> [66] </ref>. 2.3 Presentation, Recognition, and Exploitation of Gestures in Experimental Systems. In this section, we examine experimental systems which use "gestural" input. The researchers who create these systems use their own definitions of gesture, which are as diverse as the biological and sociological definitions.
Reference: [67] <institution> Federal Aviation Administration U.S. Department of Transportation. </institution> <note> Aeronautical information manual official guide to basic flight information and ATC procedures, </note> <year> 1995. </year>
Reference-contexts: These gestures are composed of oscillating planar motions, that is, circles or back-and-forth lines made in two dimensions in real world three dimensional space. Some of the gestures used to signal aircraft on a runway are also planar oscillators, as shown in figure 2.6 <ref> [67] </ref>. Again, these gestures are created by forming oscillating hand motions. Although all of these gestures can be generated using only one hand, humans recognize some of them more easily because they are generated using two hands. The "travel ahead" 2 I wish to thank Dr.
Reference: [68] <author> Pierre Wellner. </author> <title> The DigitalDesk Calculator: Tangible manipulation on a desk top display. </title> <booktitle> In Proceedings of the ACM Symposium on User Interface Software and Technology 91, </booktitle> <pages> pages 27-33, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: A gesture interacts with a virtual environment, and the system can use background environment information to aid in interpreting the gesture command. Therefore, the user can use gestures to point at or grab objects in the virtual environment. The Digital Desk Calculator <ref> [68] </ref> is another kinematic gesture tracking system which controls a display. Computer graphics are displayed onto a real world desk top. The system tracks a human finger which points at real desk top items.
Reference: [69] <author> C. G. Wolf and P. Morrel-Samuals. </author> <title> The use of hand-drawn gestures for text editing. </title> <journal> In International Journal of Man-Machine Studies, </journal> <volume> volume 27, </volume> <pages> pages 91-102, </pages> <year> 1987. </year>
Reference-contexts: Such a gesture set allows a user to get "the feeling of directly manipulating the controlled object, as if by holding invisible strings" <ref> [69] </ref>. As was described in section 2.3, many gesture recognition systems use gestures that point or represent some symbol. That is, the gestures are static. The gestures recognized by other systems for use in controlling devices are moving gestures. <p> People are also very adept at learning new arbitrary gestures. Gesturing is natural for humans, and only a short amount of training is required before people can consistently use new gestures to communicate information or control devices (see <ref> [69] </ref>, [29], and [28]). Wolf also discovered that, without prompting, test subjects used very similar gestures for the same operations, and even used the same screen drawn gestures days later.
Reference: [70] <author> C. G. Wolf and J. R. Rhyne. </author> <title> A taxonomic approach to understanding direct manipulation. </title> <booktitle> In Journal of the Human Factors Society 31th Annual Meeting, </booktitle> <pages> pages 576-780, </pages> <year> 1987. </year>
Reference-contexts: Combinations of symbolic and trajectory commands are possible in a single gesture. Given that gestures are used to communicate information, the question arises: is it possible to consistently capture that information in a usable form? Based on the work of 8 Wolf <ref> [70] </ref> in studying hand drawn gestures, we analyze gestural control devices by examining the following generation, representation, recognition, and transformation questions: * What is the gesture lexicon? * How are the gestures generated? * How does the system recognize gestures? * What are the memory size and computational time requirements for <p> That is, the gestures are static. The gestures recognized by other systems for use in controlling devices are moving gestures. Wolf states that these gestures are defined from both the resulting visual form and the temporal characteristics of the motion as it was created <ref> [70] </ref>. Moving gestures convey more information than static ones. We define the dynamics of a gesture as the structure of its positions in space and time. This definition of a gesture's dynamical aspect includes Wolf's notion of temporal characteristics. <p> The next section addresses this issue. 2.4.3 Issues Concerning the Recognition and Generation of Human Gen erated Gestures. In addition to human-to-human communication research, the use of gestures as computer input has been studied. Wolf 's research in this area examines the following concerns <ref> [70] </ref>: 1. How consistent are people in their use of gestures? 17 2. What are the most common gestures used in a given domain, and how easily are they recalled? 3. <p> What kind of variability exists in a gesture, and are the deviations predictable or random? Wolf explored these questions by studying hand drawn two dimensional gestures which were used to edit text documents <ref> [70] </ref>. Hauptmann attempted to answer similar questions by examining three dimensional spatial gestures which were used to rotate, translate, and scale three dimensional graphic objects on a computer screen [29]. Although the uses were drastically different (text versus graphics), the concepts and conclusions were strikingly similar.
Reference: [71] <author> W. Wundt. </author> <title> The language of gestures. The Hague, </title> <publisher> Mouton, </publisher> <year> 1973. </year> <month> 136 </month>
Reference-contexts: We are primarily interested in gestures which are created as their own independent, semiotic language. 2.1.2 Gesture Typologies. Another standard gesture classification scheme uses three categories: arbitrary, mimetic, and deictic. [51]. In mimetic gestures, motions form an object's main shape or representative feature <ref> [71] </ref>. For instance, a chin sweeping gesture can be used to represent a goat by alluding to its beard. These gestures are intended to be transparent. Mimetic gestures are not useful in our research, because we interested in control, not representation.
References-found: 72

