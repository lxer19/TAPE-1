URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/mitalewife.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/mitalewife.html
Root-URL: 
Title: The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor  
Author: Anant Agarwal, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, and Dan Nussbaum 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: The Alewife multiprocessor project focuses on the architecture and design of a large-scale parallel machine. The machine uses a low dimension direct interconnection network to provide scalable communication bandwidth, while allowing the exploitation of locality. Despite its distributed memory architecture, Alewife allows efficient shared memory programming through a multilayered approach to locality management. A new scalable cache coherence scheme called LimitLESS directories allows the use of caches for reducing communication latency and network bandwidth requirements. Alewife also employs run-time and compile-time methods for partitioning and placement of data and processes to enhance communication locality. While the above methods attempt to minimize communication latency, remote communication with distant processors cannot be completely avoided. Alewife's processor, Sparcle, is designed to tolerate these latencies by rapidly switching between threads of computation. This paper describes the Alewife architecture and concentrates on the novel hardware features of the machine including LimitLESS directories and the rapid context switching processor.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The implementation of such an architecture requires multiple register sets or some other mechanism to allow fast context switches, additional network bandwidth, support logic in the cache controller, and extra complexity in the thread scheduling mechanism. Other methods, such as weak ordering <ref> [12, 1, 26] </ref>, incur similar implementation complexities in the cache controller to allow multiple outstanding requests.
Reference: [2] <author> Anant Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1991. To appear. </note>
Reference-contexts: The Alewife experiment explores methods for automatic enhancement of locality in a scalable parallel machine. The Alewife multiprocessor uses a distributed shared-memory architecture with a low-dimension direct network. Such networks are cost-effective, modular, and encourage the exploitation of locality <ref> [34, 19, 2] </ref>. Unfortunately, non-uniform communication latencies usually make such machines hard to program because the onus of managing locality invariably falls on the programmer. The goal of the Alewife project is to discover and to evaluate techniques for automatic locality management in scalable multiprocessors.
Reference: [3] <author> Anant Agarwal. </author> <title> Performance Tradeoffs in Multithreaded Processors. </title> <month> September </month> <year> 1989. </year> <note> MIT VLSI Memo 89-566. Submitted for publication. </note>
Reference-contexts: Inter-processor interrupts are implemented via asynchronous traps. 5.2 Simulation Results and Analysis We compare the behavior of a multithreaded architecture to a standard configuration, and analyze how synchronization, local memory access latency, and remote memory access latency contribute to the run time of each application. See <ref> [3] </ref> for additional analyses. A thorough evaluation of multithreading will require a large parallel machine and a scheduler optimized for multithreaded multiprocessors. On the largest machines we can reasonably simulate (around 64 processors) and with our current scheduler, the scheduling cost of threads generally outweighs the benefits of latency tolerance.
Reference: [4] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Section 4 discusses the LimitLESS directory scheme, and Section 5 outlines our approach to latency tolerance. We also discuss the performance of the machine on a few applications. Other details of the machine are presented elsewhere <ref> [4, 8, 28] </ref>. Section 6 discusses related work, and Section 7 offers some perspective and 2 summarizes the paper. 2 System Overview The Alewife compiler, runtime system, and hardware try to reduce the communication latency where possible, and attempt to tolerate the latency otherwise. <p> Each node has and an additional 4M bytes of 4 local memory, a portion of which is used for the coherence directory. Alewife's cache and floating-point units are SPARC compatible. Sparcle uses a block multithreaded architecture <ref> [4] </ref>. Initially, our software system will be based on Mul-T [23]. A parallel C-like language is also under development. Mul-T's basic mechanism for generating concurrent threads is the future construct. <p> The current register window is altered via SPARC instructions (SAVE and RESTORE). To effect a context switch, the trap routine saves the Program Counter (PC) and Processor Status Register (PSR), flushes the pipeline, and sets the Frame Pointer (FP) to a new register window. <ref> [4] </ref> shows that even with a low-cost implementation, a context switch can be done in about 11 cycles. By maintaining a separate PC and PSR for each context, a custom processor could switch contexts even faster.
Reference: [5] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proc. 15th International Symposium on Computer Architecture, IEEE, </booktitle> <address> New York, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Caches reduce the volume of traffic imposed on the network by providing demand-driven data replication where needed. However, replicating blocks of data in multiple caches introduces the cache coherence problem [15, 38]. A number of cache coherence protocols have been proposed to solve the coherence problem in network-based multiprocessors <ref> [6, 37, 5, 20] </ref>. These message-based protocols allocate a section of the system's memory, called a directory, to store the locations and state of the cached copies of each data block. <p> To store pointers to blocks potentially cached by all the processors in the system, the size of the directory memory in early full-map protocols grows as fi (N 2 ), where N is the number of processors in the system. As observed in <ref> [5] </ref>, the first two concerns are easily dispelled: The directory can 6 be distributed along with main memory among the processing nodes to match the aggregate bandwidth of distributed main memory. <p> Thus, the challenge lies in alleviating the severe memory requirements of the distributed full-map directory schemes. Scalable coherence protocols differ in the size and the structure of the directory memory. Limited directory protocols <ref> [5] </ref>, for example, avoid the severe memory overhead of full-map directories by allowing only a limited number of simultaneously cached copies of any individual block of data. Unlike a full-map directory, the size of a limited directory grows as fi (N log N ) with the number of processors. <p> In this case, the pointer set is modified to contain i (if necessary) and the memory controller issues a message containing the data of the block to be written (Write Data). Following the notation in <ref> [5] </ref>, both full-map and LimitLESS are members of the Dir N N B class of cache coherence protocols. Therefore, from the point of view of the protocol specification, the LimitLESS scheme does not differ substantially from the full-map protocol.
Reference: [6] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: Caches reduce the volume of traffic imposed on the network by providing demand-driven data replication where needed. However, replicating blocks of data in multiple caches introduces the cache coherence problem [15, 38]. A number of cache coherence protocols have been proposed to solve the coherence problem in network-based multiprocessors <ref> [6, 37, 5, 20] </ref>. These message-based protocols allocate a section of the system's memory, called a directory, to store the locations and state of the cached copies of each data block. <p> The protocols send messages with data requests or invalidation signals, and record the acknowledgment of each of these messages to ensure global consistency of memory. Although directory protocols have been around since the late 1970's, the usefulness of the early protocols (e.g., <ref> [6] </ref>) was in doubt for several reasons: First, the directory itself was a centralized monolithic resource which serialized all requests. Second, directory accesses were expected to consume a disproportionately large fraction of the available network bandwidth. Third, the directory became prohibitively large as the number of processors increased.
Reference: [7] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: In such systems, widely shared data locations degrade system performance by causing constant eviction and reassignment, or thrashing, of directory pointers. However, previous studies have shown that a small set of pointers is sufficient to capture the worker-set of processors that concurrently read many types of data <ref> [7, 39, 30] </ref>. <p> Multigrid is such an application. All of the protocols require approximately the same time to complete the computation phase. This confirms the assumption that for applications with small worker-sets, such as multigrid, the limited (and therefore the LimitLESS) directory protocols perform almost as well as the full-map protocol. See <ref> [7] </ref> for more evidence of the general success of limited directory protocols. To measure the performance of LimitLESS under extreme conditions, we simulated a version of SIMPLE with barrier synchronization implemented using a single lock (rather than a software combining tree).
Reference: [8] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <note> LimitLESS Directories: </note>
Reference-contexts: Section 4 discusses the LimitLESS directory scheme, and Section 5 outlines our approach to latency tolerance. We also discuss the performance of the machine on a few applications. Other details of the machine are presented elsewhere <ref> [4, 8, 28] </ref>. Section 6 discusses related work, and Section 7 offers some perspective and 2 summarizes the paper. 2 System Overview The Alewife compiler, runtime system, and hardware try to reduce the communication latency where possible, and attempt to tolerate the latency otherwise. <p> In fact, the LimitLESS protocol is also specified in 8 the state is handled in software when the size of the pointer set (n) is greater than the size of the limited directory (p). (See <ref> [8] </ref> for details). In this situation, the transitions with the square labels (1, 2, and 3) are executed by the interrupt handler on the processor that is local to the overflowing directory.
References-found: 8

