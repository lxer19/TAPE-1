URL: http://www.cs.nyu.edu/cs/projects/proteus/domain/repjun96.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/domain/
Root-URL: http://www.cs.nyu.edu
Email: sekine,grishman@cs.nyu.edu  
Title: Domain project intermediate report  
Author: Satoshi Sekine and Ralph Grishman 
Date: June 10, 1996  
Address: New York University 715 Broadway, 7th floor New York, NY 10003, USA  
Affiliation: Computer Science Department  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W.N.Francis, </author> <title> H.Kucera "Manual of information to accompany A Standard Corpus of Present-Day Edited American English" Brown University, </title> <institution> Department of Linguistics (1964/1979) </institution>
Reference-contexts: First, we want to verify our intuition that there exist some structural differences across domains in the real world. We analyzes syntactic structures of sentences in different domains using statistical methods. 2 Data and tools The data we used in the experiment is the `Brown Corpus' <ref> [1] </ref> in Penn Tree Bank CD-ROM version 1 [2]. In order to measure the differences, we use `cross entropy' which measured the similarity of model and test sample in which data are probabilistically distributed.
Reference: [2] <author> M.Marcus, B.Santorini, </author> <title> M.Marcinkiewicz "Building a large annotated corpus of English: the Penn Tree bank" in the distributed Penn Tree Bank Project CD-ROM, Linguistic Data Consortium, </title> <institution> University of Pennsylva-nia. </institution>
Reference-contexts: We analyzes syntactic structures of sentences in different domains using statistical methods. 2 Data and tools The data we used in the experiment is the `Brown Corpus' [1] in Penn Tree Bank CD-ROM version 1 <ref> [2] </ref>. In order to measure the differences, we use `cross entropy' which measured the similarity of model and test sample in which data are probabilistically distributed. Also, we use a clustering technique in order to see what kind of grouping we can make based on the similarity among domains.
Reference: [3] <author> E.Charnik: </author> <title> "Statistical Language Learning" The MIT Press (1993) 12 </title>
Reference-contexts: CE (T; M ) = i The smaller the value, the more accurately the model represents the test set. If the model is identical to the test set, the value is equal to the entropy of the set itself. <ref> [3] </ref> 2.3 Clustering Given a set of items and a distance between each pair of items (not necessarily Euclidean distance), then we can group similar items based on the distance.
Reference: [4] <author> R.Gnanadesikan: </author> <title> "Methods for statistical Data Analysis of Multivariate Observations" (1977) </title>
Reference-contexts: Also, there are several methods to compute distance between two clusters (three major methods are shortest distance, average distance and longest distance). Here, for simplicity, we choose non-overlapping clusters and average-distance clustering metric. <ref> [4] </ref> 4 3 Experiment 3.1 Extract Subtrees First, we decide to use the top level sections of Brown corpus as our domains. It remains a question if the sections are sufficiently uniform and the best for our purpose.
Reference: [5] <author> Satoshi Sekine: </author> <title> "A New Direction for Sublanguage NLP" International Conference on New Methods in Language Processing (1994) </title>
Reference: [6] <author> Satoshi Sekine, </author> <booktitle> Ralph Grishman "A Corpus-based Probabilistic Grammar with Only Two Non-terminals" Proceedings of the Fourth International Workshop on Parsing Technologies (1995) 13 </booktitle>
References-found: 6

