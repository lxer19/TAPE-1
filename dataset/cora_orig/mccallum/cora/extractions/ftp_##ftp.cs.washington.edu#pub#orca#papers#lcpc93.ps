URL: ftp://ftp.cs.washington.edu/pub/orca/papers/lcpc93.ps
Refering-URL: http://www.cs.washington.edu/research/zpl/papers/abstracts/lcpc93.html
Root-URL: 
Title: ZPL: An Array Sublanguage  
Author: Calvin Lin Lawrence Snyder 
Date: October 10, 1993  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Note: Appears in "Proceedings of the Sixth International Workshop on Languages and Compilers for Parallel Computing," pages 96-114, 1993.  
Abstract: The notion of isolating the "common case" is a well known computer science principle. This paper describes ZPL, a language that treats data parallelism as a common case of MIMD parallelism. This separation of concerns has many benefits. It allows us to define a clean and concise language for describing data parallel computations, and this in turn leads to efficient parallel execution. Our particular language also provides mechanisms for handling boundary conditions. We introduce the concepts, constructs and semantics of our new language, and give a simple example that contrasts ZPL with other data parallel languages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jeanne C. Adams, Walter S. Brainerd, Jeanne T. Martin, Brian T. Smith, and Jerrold L. Wagener. </author> <title> Fortran 90 Handbook. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction A variety of languages have been proposed that generally provide data parallel or array semantics, including C* [15], Fortran 90 <ref> [1] </ref>, NESL [4], and HPF [8]. One characteristic of these languages is that data parallelism is the only model provided. This fact introduces a certain pressure to support a wide range of general purpose facilities. <p> Like regions, directions have an associated rank, and their values are initialized once at load time and do not change during the execution of a program. Examples of direction declarations are shown below. direction north = [-1,0]; east = <ref> [0, 1] </ref>; west = [0,-1]; south = [1, 0]; se = [1, 1]; The At operator (@) is used to access neighboring array elements. <p> Like regions, directions have an associated rank, and their values are initialized once at load time and do not change during the execution of a program. Examples of direction declarations are shown below. direction north = [-1,0]; east = [0, 1]; west = [0,-1]; south = <ref> [1, 0] </ref>; se = [1, 1]; The At operator (@) is used to access neighboring array elements. <p> Examples of direction declarations are shown below. direction north = [-1,0]; east = [0, 1]; west = [0,-1]; south = [1, 0]; se = <ref> [1, 1] </ref>; The At operator (@) is used to access neighboring array elements. <p> Partial reductions and scans are expressed by using an optional parameter that specifies the dimensions across which the reduction or scan should take place. Dimensions are specified by placing numbers in brackets, with <ref> [1] </ref> indicating the first dimension and [d] indicating the d th dimension. The code fragment below, where v is a vector and s is a scalar, shows examples of partial reductions. [R] v := +"[1] A; -- reduce A along columns and assign to vector v [R] s := +"[1][2] A;
Reference: [2] <author> Gail Alverson, William Griswold, David Notkin, and Lawrence Snyder. </author> <title> A flexible communication abstraction for nonshared memory parallel computing. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: ZPL is a different kind of language. ZPL is an array sublanguage of the Orca family of parallel programming languages [11, 12]. The Orca languages provide a general MIMD programming model <ref> [2, 6, 17] </ref>, allowing programmers to write efficient, portable and scalable parallel programs. For many tasks, such as initializing an array A to all 0's, a full MIMD programming model is overly general. <p> Arrays can be declared using regions as shown below: var A: real [R]; B: real [R]; C: real [R]; 9 [R] NE2 = [-2,2] east2 = <ref> [0, 2] </ref> [SE2 of R] [east2 of R] [NE2 of R] By applying regions to entire statements, ZPL programs are syntactically clean since they are not muddled by complicated indices. Significantly, region scopes are the only way to reference arrays.
Reference: [3] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: If v i &gt; 0, the lower bound of the "Of" array is the upper bound of the i th dimension. Reductions and Scans. Reductions and scans are common operations that are known to have efficient parallel implementations <ref> [3, 9] </ref>. ZPL supplies reductions and scans as operators because this allows them to be optimized by the compiler. For example, if reductions are not provided by the hardware, the messages needed to implement consecutive reductions can be combined to reduce communication latency.
Reference: [4] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language. </title> <type> Technical Report CMU-CS92-103, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: 1 Introduction A variety of languages have been proposed that generally provide data parallel or array semantics, including C* [15], Fortran 90 [1], NESL <ref> [4] </ref>, and HPF [8]. One characteristic of these languages is that data parallelism is the only model provided. This fact introduces a certain pressure to support a wide range of general purpose facilities. <p> Jacobi in NESL. NESL <ref> [4] </ref> is an applicative language that is based on vector operations such as scan and reduce. NESL supports nested parallelism and distinguishes itself from most other data parallel languages by its functional nature. Below is NESL code that applies to both regular and irregular meshes.
Reference: [5] <author> R. E. Cypher, J. L. C. Sanz, and L. Snyder. </author> <title> Algorithms for image component labeling on simd mesh connected computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(2) </volume> <pages> 276-281, </pages> <year> 1990. </year>
Reference-contexts: The simplicity will be evident in the description presented below. The expressiveness is justified by the fact that ZPL is sufficient to program the SIMPLE benchmark [11] and other standard data parallel applications, including the Ising model, the Floyd-Steinberg dithering model, Cypher et al.'s recursive connected component labeling algorithm <ref> [5] </ref>, a median threshold filtering, and the game of Life.
Reference: [6] <author> William Griswold, Gail Harrison, David Notkin, and Lawrence Snyder. </author> <title> Scalable ab-stractions for parallel programming. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, 1990. </booktitle> <address> Charleston, South Carolina. </address>
Reference-contexts: ZPL is a different kind of language. ZPL is an array sublanguage of the Orca family of parallel programming languages [11, 12]. The Orca languages provide a general MIMD programming model <ref> [2, 6, 17] </ref>, allowing programmers to write efficient, portable and scalable parallel programs. For many tasks, such as initializing an array A to all 0's, a full MIMD programming model is overly general.
Reference: [7] <author> Philip J. Hatcher, Michael J. Quinn, Ray J. Anderson, Anthony J. Lapadula, Bradley K. Seevers, and Andrew F. Bennett. </author> <title> Architecture-independent scientific pro-gramming in Dataparallel C: Three case studies. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 208-217, </pages> <year> 1991. </year> <month> 18 </month>
Reference-contexts: Broadly speaking, these languages are more general than ZPL since ZPL is an attempt to trade off expressiveness for improved clarity and efficiency. On the other hand, these other languages do not provide the same support for boundary conditions that ZPL does. C* and Data- parallel C <ref> [7] </ref> have SIMD execution semantics and provide pointers. Dino [16] programs can specify arbitrary point-to-point communication through tagged accesses to variables. This more general communication adds power but complicates optimizations. 6 Conclusion In this paper we have introduced ZPL, a data parallel programming language whose goals are clarity and efficiency.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification. </title> <month> January </month> <year> 1993. </year>
Reference-contexts: 1 Introduction A variety of languages have been proposed that generally provide data parallel or array semantics, including C* [15], Fortran 90 [1], NESL [4], and HPF <ref> [8] </ref>. One characteristic of these languages is that data parallelism is the only model provided. This fact introduces a certain pressure to support a wide range of general purpose facilities.
Reference: [9] <author> Richard E. Ladner and Michael J. Fischer. </author> <title> Parallel prefix computation. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 27(4) </volume> <pages> 831-838, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: If v i &gt; 0, the lower bound of the "Of" array is the upper bound of the i th dimension. Reductions and Scans. Reductions and scans are common operations that are known to have efficient parallel implementations <ref> [3, 9] </ref>. ZPL supplies reductions and scans as operators because this allows them to be optimized by the compiler. For example, if reductions are not provided by the hardware, the messages needed to implement consecutive reductions can be combined to reduce communication latency.
Reference: [10] <author> Jinling Lee. </author> <title> Extending the SIMPLE program in Poker. </title> <type> Technical Report 89-11-07, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1989. </year>
Reference-contexts: We have no comparison against other data parallel implementations, but we can compare our ZPL version against those written in other paradigms. A parallel implementation written in a MIMD message passing style was about 5000 lines of C code <ref> [10] </ref>. A sequential implementation from Cornell was about 2400 lines of Fortran code. Meanwhile, our ZPL version was less than 500 lines.
Reference: [11] <author> Calvin Lin and Lawrence Snyder. </author> <title> A portable implementation of SIMPLE. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(5) </volume> <pages> 363-401, </pages> <year> 1991. </year>
Reference-contexts: Though this has resulted in rich and interesting languages, it has not always served the goals of simplicity, cleanliness or efficiency. ZPL is a different kind of language. ZPL is an array sublanguage of the Orca family of parallel programming languages <ref> [11, 12] </ref>. The Orca languages provide a general MIMD programming model [2, 6, 17], allowing programmers to write efficient, portable and scalable parallel programs. For many tasks, such as initializing an array A to all 0's, a full MIMD programming model is overly general. <p> The simplicity will be evident in the description presented below. The expressiveness is justified by the fact that ZPL is sufficient to program the SIMPLE benchmark <ref> [11] </ref> and other standard data parallel applications, including the Ising model, the Floyd-Steinberg dithering model, Cypher et al.'s recursive connected component labeling algorithm [5], a median threshold filtering, and the game of Life.
Reference: [12] <author> Calvin Lin and Lawrence Snyder. </author> <title> Data ensembles in Orca C. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Though this has resulted in rich and interesting languages, it has not always served the goals of simplicity, cleanliness or efficiency. ZPL is a different kind of language. ZPL is an array sublanguage of the Orca family of parallel programming languages <ref> [11, 12] </ref>. The Orca languages provide a general MIMD programming model [2, 6, 17], allowing programmers to write efficient, portable and scalable parallel programs. For many tasks, such as initializing an array A to all 0's, a full MIMD programming model is overly general. <p> ZPL program is shown below, and it's immediately apparent which parts of the computation apply to which portions of the data space. 14 [border] begin ... [R] begin ... 4.2 Relationship to Orca C As a first approximation, a program written using the full capabilities of the Orca C language <ref> [12] </ref> will utilize one of four types of statements: [R] A := sqrt (A); -- data parallel assignment err := max" Delta; -- "standard" parallel abstraction if err &lt; 10**-6 then. . . -- control construct else . . .
Reference: [13] <author> Constantine Polychronopolous, Milind Girkar, Mohammad Reza Haghighat, Chia Ling Lee, Bruce Leung, and Dale Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 39-48, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Recursion is allowed. In addition, ZPL uses two-level scoping, i.e. procedures are not allowed to be defined inside the scope of other procedures. This detail stems from the fact that our implementation is based on the Parafrase-2 <ref> [13] </ref> source to source translator for the C language. <p> The implementation of a ZPL compiler began in April, 1993 and is expected to be completed in the summer of 1993. We are modifying the Parafrase-2 source to source translator <ref> [13, 14] </ref> to compile ZPL code down to C for a variety of parallel computers. We believe that this system will provide an ideal testbed to experiment with novel compiler optimizations for parallel computers. Acknowledgments.
Reference: [14] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten. </author> <title> The structure of parafrase-2: an advanced parallelizing compiler for c and fortran. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 423-453. </pages>
Reference-contexts: The implementation of a ZPL compiler began in April, 1993 and is expected to be completed in the summer of 1993. We are modifying the Parafrase-2 source to source translator <ref> [13, 14] </ref> to compile ZPL code down to C for a variety of parallel computers. We believe that this system will provide an ideal testbed to experiment with novel compiler optimizations for parallel computers. Acknowledgments.
Reference: [15] <author> J.R. Rose and Guy L. Steele Jr. </author> <title> C*: An extended C language for data parallel pro-gramming. </title> <type> Technical Report PL 87-5, </type> <institution> Thinking Machines Corporation, </institution> <year> 1987. </year>
Reference-contexts: 1 Introduction A variety of languages have been proposed that generally provide data parallel or array semantics, including C* <ref> [15] </ref>, Fortran 90 [1], NESL [4], and HPF [8]. One characteristic of these languages is that data parallelism is the only model provided. This fact introduces a certain pressure to support a wide range of general purpose facilities.
Reference: [16] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The Dino parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: On the other hand, these other languages do not provide the same support for boundary conditions that ZPL does. C* and Data- parallel C [7] have SIMD execution semantics and provide pointers. Dino <ref> [16] </ref> programs can specify arbitrary point-to-point communication through tagged accesses to variables. This more general communication adds power but complicates optimizations. 6 Conclusion In this paper we have introduced ZPL, a data parallel programming language whose goals are clarity and efficiency.
Reference: [17] <author> Lawrence Snyder. </author> <title> The XYZ abstraction levels of Poker-like languages. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 470-489. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: ZPL is a different kind of language. ZPL is an array sublanguage of the Orca family of parallel programming languages [11, 12]. The Orca languages provide a general MIMD programming model <ref> [2, 6, 17] </ref>, allowing programmers to write efficient, portable and scalable parallel programs. For many tasks, such as initializing an array A to all 0's, a full MIMD programming model is overly general. <p> Collectively, the four statement types are said to define programming at the "problem level" of Orca C. This name emphasizes the fact that these statement types express the high level solution of the programmer's problem. For historical reasons, the problem level is also known as the Z level <ref> [17] </ref>. The problem level text describes the main logic of the problem solution. If the solution is sufficiently simple, ZPL constructs alone may suffice to express the program. But for complex algorithms or certain performance-critical situations, the full MIMD capabilities of the language may be needed.
Reference: [18] <author> Lawrence Snyder. </author> <booktitle> Foundations of practical parallel programming languages. In Proceedings of the Second International Conference of the Austrian Center for Parallel Computation. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: And the constructs provided for defining the processes, out of which phases are defined, are known as the X level of programming. Further description of the phase abstractions programming model can be found in the literature <ref> [18] </ref>. 15 Though a full introduction of the Y and X programming facilities of Orca C, and a careful introduction to the phase abstractions concepts would be beyond the scope of this paper, a brief description of the phase definition approach is appropriate.
Reference: [19] <author> David Grimes Socha. </author> <title> Supporting Fine-Grain Computation on Distributed Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1991. </year> <month> 19 </month>
Reference-contexts: This gives ZPL a greater efficiency than might be apparent if the literal interpretation of phase abstractions every line is a control structure or a phase invocation were strictly enforced. 5 Other Related Work ZPL inherits the notion of regions from Spot <ref> [19] </ref>, a language designed to support stencil- based computations on distributed memory computers. The two languages are syntactically similar, but Spot presents a point-based view which leads to a certain awkwardness. For example, history values are introduced to maintain multiple values of variables across iterations.
References-found: 19

