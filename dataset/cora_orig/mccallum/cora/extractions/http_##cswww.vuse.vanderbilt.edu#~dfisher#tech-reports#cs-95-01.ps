URL: http://cswww.vuse.vanderbilt.edu/~dfisher/tech-reports/cs-95-01.ps
Refering-URL: http://cswww.vuse.vanderbilt.edu/~dfisher/tech-reports/tr-95-01.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Iterative Optimization and Simplification of Hierarchical Clusterings  
Author: Doug Fisher 
Keyword: clustering, iterative optimization, cluster validation, resampling, pruning, objective functions.  
Address: Box 1679, Station B  Nashville, TN 37235  
Affiliation: Department of Computer Science  Vanderbilt University  
Pubnum: Technical Report CS-95-01  
Email: dfisher@vuse.vanderbilt.edu  
Phone: (615) 343-4111  
Web: http://www.vuse.vanderbilt.edu/~dfisher/dfisher.html  
Abstract: Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R., & Matessa, M. </author> <year> (1991). </year> <title> An interative Bayesian algorithm for categorization. </title>
Reference-contexts: Nonetheless, this measure is commonly used, we will take this opportunity to note its problems, and none of the techniques that we describe is tied to this measure. 2.2 The Structure of Clusters As in Cobweb, Autoclass (Cheeseman, et. al., 1988), and other systems <ref> (Anderson & Matessa, 1991) </ref>, we will assume that clusters, C k , are described probabilistically: each variable value has an associated conditional probability, P (A i = V ij jC k ), which reflects the proportion of observations in C k that exhibit the value, V ij , along variable A
Reference: <editor> In D. Fisher & M. Pazzani (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ahn, W., & Medin, D. L. </author> <year> (1989). </year> <title> A two-stage categorization model of family resemblance sorting. </title> <booktitle> Proceedings of the Eleventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 315-322). </pages> <address> Ann Arbor, MI: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Observations are at leaves and are described by three variables: Size, Color, and Shape. 2.3 Hierarchical Sorting Our strategy for initial clustering is sorting, which is a term adapted from a psychological task that requires subjects to perform roughly the same `clustering' procedure that we describe here <ref> (e.g., Ahn & Medin, 1989) </ref>.
Reference: <author> Biswas, G., Weinberg, J., & Li, C. </author> <year> (1994). </year> <title> Iterate: A conceptual clustering method for knowledge discovery in databases. </title> <editor> In B. Braunschweig and R. Day (Eds.) </editor> <booktitle> Innovative Applications of Artificial Intelligence in the Oil and Gas Industry. </booktitle> <publisher> Editions Technip. </publisher>
Reference: <author> Biswas, G., Weinberg, J. B., Yang, Q., & Koller, G. R. </author> <year> (1991). </year> <title> Conceptual clustering and exploratory data analysis. </title> <booktitle> Proceedings of the Eighth International Machine Learning Workshop (pp. </booktitle> <pages> 591-595). </pages> <address> Evanston, Il: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For each child, C k , of Root (in order) Do L k ORDER (C k ) L MERGE (fL k jlist of objects constructed by ORDER (C k )g) Return (L) sort-based algorithms (Fisher et al, 1993). The idea behind iterative redistribution <ref> (Biswas, Weinberg, Yang, & Koller, 1991) </ref> is simple: observations in a single-level clustering are `removed' from their original cluster and resorted relative to the clustering. If a cluster contains only one observation, then the cluster is `removed' and its single observation is resorted.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference: <author> Carpineto, C., & Romano, G. </author> <year> (1993). </year> <title> GALOIS: An order-theoretic approach to conceptual clustering. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 33-40), </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <pages> AutoClass: </pages>
Reference-contexts: This allows an analyst to get an early indication of the possible presence and form of structure in data, but search can continue as long as it seems worthwhile. This seems to be a primary motivation behind the design of Autoclass <ref> (Cheeseman, et. al., 1988) </ref>. This paper describes and evaluates three strategies for iterative optimization, one inspired by the iterative `seed' selection strategy of Cluster (Michalski & Stepp, 1983), one is a common form of optimization that iteratively reclassifies single observations, and a third method appears novel in the clustering literature. <p> Nonetheless, this measure is commonly used, we will take this opportunity to note its problems, and none of the techniques that we describe is tied to this measure. 2.2 The Structure of Clusters As in Cobweb, Autoclass <ref> (Cheeseman, et. al., 1988) </ref>, and other systems (Anderson & Matessa, 1991), we will assume that clusters, C k , are described probabilistically: each variable value has an associated conditional probability, P (A i = V ij jC k ), which reflects the proportion of observations in C k that exhibit the <p> For example, we might consider Bayesian variants like those found in Autoclass <ref> (Cheeseman, et al, 1988) </ref> and Anderson and Matessa's (1991) system. We do not evaluate alternative measures such as these here, but do suggest a number of other candidates.
References-found: 8

