URL: http://www.gia.ist.utl.pt/~pedrod/tai95a.ps.gz
Refering-URL: http://www.gia.ist.utl.pt/~pedrod/
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Two-Way Induction  
Author: Pedro Domingos 
Address: Irvine, California 92717, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: General-to-specific learners like ID3 and CN2 perform well when the target concept descriptions are general, but often have difficulties when they are specific or mixed. This problem can be alleviated by combining them with a specific-to-general learning component, resulting in a two-way induction system. In this paper one design for such a component is proposed, as well as two methods for combining the two components. Experiments on artificial domains show the combined learner to match or outperform "pure" versions of C4.5 and CN2 across the entire generality spectrum, with the advantage increasing for greater concept specificity. Experiments on 24 real-world domains from the UCI repository confirm the utility of two-way induction: the combined learner achieves higher accuracy than C4.5 in 17 domains (at the 5% significance level in 12), and similar results are obtained with CN2. Closer observation of the system's behavior leads to a better understanding of its ability to correct overly-general rules with specific ones, and shows that there is still room for improvement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. W. Aha. </author> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 1-10, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Following a philosophy justified elsewhere <ref> [17, 1] </ref>, classes of domains rather that individual concept definitions were considered. The study was carried out using C4.5R, RISE and TWI-2. The independent variable of interest is the specificity of the target concept description.
Reference: [2] <author> C. E. Brodley. </author> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 17-24, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The research described in this paper falls in the general area of empirical multi-strategy learning, which attempts to produce more accurate learners by combining two or more individual algorithms [9]. The MCS system combines decision trees with the nearest-neighbor algorithm and linear machines <ref> [2] </ref>. FCLS combines rules with specific examples in a best-match framework [20]. Quinlan has combined decision trees and other methods with nearest-neighbor in regression tasks [15].
Reference: [3] <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> pages 151-163, </pages> <address> Porto, Portugal, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A recent version of CN2 was used <ref> [3] </ref>. The default settings for C4.5, C4.5RULES and CN2 were kept. Note that all results for C4.5RULES (abbreviated C4.5R) and CN2 below are those obtained using their own classification procedures, not the one above for G rules.
Reference: [4] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction and Motivation Most widely-used decision-tree and rule learners perform general-to-specific induction: they start with an empty concept description, and gradually add restrictions to it until there is not enough evidence to continue, or perfect discrimination is achieved. ID3/C4.5 [14] and CN2 <ref> [4] </ref> are examples of systems that function in this way. They have some significant advantages: because only statistical measures are used to evaluate induction steps, as opposed to individual examples being considered, good noise immunity can be achieved.
Reference: [5] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: The essential idea behind VDM-type metrics is that two values should be considered similar if they make similar class predictions, and dissimilar if their predictions diverge. This has been found to give good results in several domains <ref> [5] </ref>. Notice that, in particular, SV DM (x i ; x j ) is always 0 if i = j.
Reference: [6] <author> P. Domingos. </author> <title> The RISE 2.0 system: A case study in multistrategy learning. </title> <type> Technical Report 95-2, </type> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA, </address> <year> 1995. </year>
Reference-contexts: Finally related work is discussed, and some conclusions and directions for future work are presented. 2 Specific-to-General Induction Specific-to-general induction is performed by the RISE algorithm, of which the learning and classification procedures will be considered in turn. More details can be found in <ref> [7, 6] </ref>. 2.1 Representation and Search Each example is a vector of attribute-value pairs, together with a specification of the class to which it belongs; attributes can be either nominal (symbolic) or numeric. Each rule consists of a conjunction of antecedents and a predicted class.
Reference: [7] <author> P. Domingos. </author> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally related work is discussed, and some conclusions and directions for future work are presented. 2 Specific-to-General Induction Specific-to-general induction is performed by the RISE algorithm, of which the learning and classification procedures will be considered in turn. More details can be found in <ref> [7, 6] </ref>. 2.1 Representation and Search Each example is a vector of attribute-value pairs, together with a specification of the class to which it belongs; attributes can be either nominal (symbolic) or numeric. Each rule consists of a conjunction of antecedents and a predicted class. <p> With this optimization, RISE's worst-case time complexity has been shown to be quadratic in the number of examples and the number of attributes, which is comparable to that of general-to-specific rule induction algorithms <ref> [7] </ref>. 2.2 Classification At performance time, classification of each test example is performed by finding the nearest rule to it, and assigning the example to the rule's class.
Reference: [8] <author> A. R. Golding and P. S. Rosenbloom. </author> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 22-27, </pages> <address> Menlo Park, CA, </address> <year> 1991. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Specific-to-general induction is performed by systems like NGE [16] and KBNGE [19], but neither combines it with general-to-specific learning. Golding and Rosenbloom's Anapron system for name pronunciation <ref> [8] </ref> performs search in neither direction, but is similar in spirit to TWI: a set of expert-supplied rules functions as the G component, and a set of implicit rules (the "arules") contained in a case base functions as the S component. 6 Conclusions and Future Work This paper has shown that
Reference: [9] <editor> R. Michalski and G. Tecuci, editors. </editor> <title> Machine Learning: A Multistrategy Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: Also, unlike version spaces, TWI is able to deal with disjunctive concepts, noise, and missing and continuous attributes. The research described in this paper falls in the general area of empirical multi-strategy learning, which attempts to produce more accurate learners by combining two or more individual algorithms <ref> [9] </ref>. The MCS system combines decision trees with the nearest-neighbor algorithm and linear machines [2]. FCLS combines rules with specific examples in a best-match framework [20]. Quinlan has combined decision trees and other methods with nearest-neighbor in regression tasks [15].
Reference: [10] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: since in TWI-2 the distinction between S and G rules is maintained, the G rules can be seen as an approximate and accessible model of the domain, while the S rules represent second-order refinements and exceptions that should be taken into account to achieve higher accuracy. 5 Related Work Mitchell's <ref> [10] </ref> version space approach is an early example of two-way induction. TWI is a heuristic algorithm, in contrast to Mitchell's exhaustive candidate elimination procedure, but has the advantage that its worst-case space and time complexities are respectively linear and low-order polynomial, instead of exponential.
Reference: [11] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA, </address> <year> 1995. </year>
Reference-contexts: both the concept and its negation have necessarily long rules, are for intermediate values of C. 4.2 Natural Domains Whether the expected advantages of adding a specific-to-general component to a general-to-specific learner are realized in real-world applications was investigated by carrying out experiments on 24 domains from the UCI repository <ref> [11] </ref>. Again, 20 runs were conducted for each dataset, with two-thirds of the data used for training. The default classifier (assigning examples to the most frequent class) was included as a baseline.
Reference: [12] <author> T. Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <booktitle> In Proceedings of the Second European Working Session on Learning, </booktitle> <pages> pages 67-78, </pages> <address> Bled, Yugoslavia, </address> <year> 1987. </year> <pages> Sigma. </pages>
Reference-contexts: When two or more rules are equally close to a test example, the rule that was most accurate on the training set wins. So as to not unduly favor more specific rules, the Laplace-corrected accuracy is used <ref> [12] </ref>: LAcc (R) = N corr (R) + 1 N won (R) + C where R is any rule, C is the number of classes, N won (R) is the total number or examples won by R, N corr (R) is the number of examples among those that R correctly classifies,
Reference: [13] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: FCLS combines rules with specific examples in a best-match framework [20]. Quinlan has combined decision trees and other methods with nearest-neighbor in regression tasks [15]. The post-pruning step used in many rule and decision-tree learners can be seen as a form of specific-to-general induction, and Quin-lan <ref> [13] </ref> and others have shown it to be effective. Specific-to-general induction is performed by systems like NGE [16] and KBNGE [19], but neither combines it with general-to-specific learning.
Reference: [14] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction and Motivation Most widely-used decision-tree and rule learners perform general-to-specific induction: they start with an empty concept description, and gradually add restrictions to it until there is not enough evidence to continue, or perfect discrimination is achieved. ID3/C4.5 <ref> [14] </ref> and CN2 [4] are examples of systems that function in this way. They have some significant advantages: because only statistical measures are used to evaluate induction steps, as opposed to individual examples being considered, good noise immunity can be achieved. <p> Applying the general-to-specific learner produces a set of G rules. If CN2 is used, the rules it outputs are used directly. If C4.5 is used, C4.5RULES is first applied to convert the decision tree it produces into a set of rules <ref> [14] </ref>. Two algorithms for combining the S and G rules were designed, TWI-1 and TWI-2. In TWI-1, the sets of S and G rules are merged to form a single set, deleting any duplicates.
Reference: [15] <author> J. R. Quinlan. </author> <title> Combining instance-based and model-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 236-243, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The MCS system combines decision trees with the nearest-neighbor algorithm and linear machines [2]. FCLS combines rules with specific examples in a best-match framework [20]. Quinlan has combined decision trees and other methods with nearest-neighbor in regression tasks <ref> [15] </ref>. The post-pruning step used in many rule and decision-tree learners can be seen as a form of specific-to-general induction, and Quin-lan [13] and others have shown it to be effective. Specific-to-general induction is performed by systems like NGE [16] and KBNGE [19], but neither combines it with general-to-specific learning.
Reference: [16] <author> S. Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276, </pages> <year> 1991. </year>
Reference-contexts: They have other disadvantages, however: search has to start from specific examples, making it more sensitive to noise, and early decisions, based on little evidence, can be wrong and difficult to correct later on. As a result, even though some specific-to-general learners exist (e.g., EACH <ref> [16] </ref>), they have not come into widespread use. <p> The post-pruning step used in many rule and decision-tree learners can be seen as a form of specific-to-general induction, and Quin-lan [13] and others have shown it to be effective. Specific-to-general induction is performed by systems like NGE <ref> [16] </ref> and KBNGE [19], but neither combines it with general-to-specific learning.
Reference: [17] <author> C. Schaffer. </author> <title> Analysis of artificial data sets. </title> <booktitle> In Proceedings of the Second International Symposium on Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: Following a philosophy justified elsewhere <ref> [17, 1] </ref>, classes of domains rather that individual concept definitions were considered. The study was carried out using C4.5R, RISE and TWI-2. The independent variable of interest is the specificity of the target concept description.
Reference: [18] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: The distance measure used is a combination of Euclidean distance for numeric attributes, and a simplified version of Stanfill and Waltz's value difference metric for symbolic attributes <ref> [18] </ref>. Let E = (e 1 ; e 2 ; . . . ; e A ; c E ) be an example with value e i for the ith attribute and class c E .
Reference: [19] <author> D. Wettschereck. </author> <title> A hybrid nearest-neighbor and nearest-hyperrectangle algorithm. </title> <booktitle> In Proceedings of the Ninth European Conference on Machine Learning, </booktitle> <pages> pages 323-335, </pages> <address> Catania, Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The post-pruning step used in many rule and decision-tree learners can be seen as a form of specific-to-general induction, and Quin-lan [13] and others have shown it to be effective. Specific-to-general induction is performed by systems like NGE [16] and KBNGE <ref> [19] </ref>, but neither combines it with general-to-specific learning.
Reference: [20] <author> J. Zhang. </author> <title> A method that combines inductive learning with exemplar-based learning. </title> <booktitle> In Proceedings of the Second International IEEE Conference on Tools for Artificial Intelligence, </booktitle> <pages> pages 31-37, </pages> <address> San Jose, CA, </address> <year> 1990. </year> <institution> Institute of Electrical and Electronics Engineers, Computer Society. </institution>
Reference-contexts: The MCS system combines decision trees with the nearest-neighbor algorithm and linear machines [2]. FCLS combines rules with specific examples in a best-match framework <ref> [20] </ref>. Quinlan has combined decision trees and other methods with nearest-neighbor in regression tasks [15]. The post-pruning step used in many rule and decision-tree learners can be seen as a form of specific-to-general induction, and Quin-lan [13] and others have shown it to be effective.
References-found: 20

