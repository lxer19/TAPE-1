URL: http://reality.sgi.com/ronnyk/mlcj.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Title: Data Mining using MLC A Machine Learning Library in C http://www.sgi.com/Technology/mlc  
Author: Ron Kohavi Dan Sommerfield James Dougherty 
Keyword: data mining, machine learning, classification, benchmark.  
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  
Affiliation: Data Mining and Visualization, 2 Scalable Server Division Silicon Graphics, Inc.  
Note: Appeared in International Journal on Artificial Intelligence Tools Vol. 6, No.  This is a long version of the paper with the same title that appeared in IEEE Tool with Artificial Intelligence 1996, pages 234-245 and received the best paper award.  
Email: ronnyk@engr.sgi.com  sommda@engr.sgi.com  jfd@engr.sgi.com  
Phone: 1  
Date: 4 (1997) 537-566  Received 21 April 1997 Revised 7 July 1997  
Abstract: Data mining algorithms including machine learning, statistical analysis, and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread. In this paper, we focus on classification algorithms and review the need for multiple classification algorithms. We describe a system called MLC ++ , which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest. MLC ++ not only provides a workbench for such comparisons, but also provides a library of C ++ classes to aid in the development of new algorithms, especially hybrid algorithms and multi-strategy algorithms. Such algorithms are generally hard to code from scratch. We discuss design issues, interfaces to other programs, and visualization of the resulting classifiers. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992), </year> <title> `Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms', </title> <journal> International Journal of Man-Machine Studies 36(1), </journal> <pages> 267-287. </pages>
Reference: <author> Aha, D. W. </author> <year> (1997), </year> <journal> `Special ai review issue on lazy learning', Artificial Intelligence Review 11(1-5). </journal>
Reference: <author> Ali, K. M. </author> <year> (1996), </year> <title> Learning Probabilistic Relational Concept Descriptions, </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Irvine. http://www.ics.uci.edu/~ali. </address>
Reference: <author> Auer, P., Holte, R. & Maass, W. </author> <year> (1995), </year> <title> Theory and applications of agnostic PAC-learning with small decision trees, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine 22 Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kauf--mann. </publisher>
Reference: <author> Brand, E., Edelstein, H., Gerritsen, R., Millenson, J., Schubert, G., Small, G. R. & Small, R. D. </author> <year> (1997), </year> <title> Data Mining Products and Markets: A Multi-Client Study, Two Crows Corporation. </title> <address> www.twocrows.com. </address>
Reference-contexts: We refer the reader to the URL http://www.kdnuggets.com/siftware.html 18 for pointers and description. Additionally, an excellent comparison of commercial data mining tools is available from Two Crows <ref> (Brand, Edelstein, Gerritsen, Millen-son, Schubert, Small & Small 1997) </ref>, and a slightly outdated report is available from Intelligent Software Strategies (Hall 1996). Typical features of commercial products include classification (decision trees, neural networks, instance-based methods), clustering, regression, pattern detection, and associations.
Reference: <author> Breiman, L. </author> <year> (1994), </year> <title> Heuristics of instability in model selection, </title> <institution> Technical Report Statistics Department, University of California at Berkeley. </institution>
Reference: <author> Breiman, L. </author> <year> (1996a), </year> <title> Arcing classifiers, </title> <type> Technical report, </type> <institution> Statistics Department, University of California, Berkeley. </institution> <note> http://www.stat.Berkeley.EDU/users/breiman/. </note>
Reference: <author> Breiman, L. </author> <year> (1996b), </year> <title> `Bagging predictors', </title> <booktitle> Machine Learning 24, </booktitle> <pages> 123-140. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Winnow The multiplicative algorithm described in Littlestone (1988). The following external inducers are interfaced by MLC ++ : C4.5 The C4.5 decision-tree induction by Quinlan (1993). C4.5-rules The trees to rules induction algorithm by Quinlan (1993). CART The commercial version of CART <ref> (Breiman, Friedman, Olshen & Stone 1984) </ref> from Salford system was interfaced. In the experiments we have used version 2.0 with a memory limit of 50,000,000 4-word workarea (200MB).
Reference: <author> Clark, P. & Boswell, R. </author> <year> (1991), </year> <title> Rule induction with CN2: Some recent improvements, </title> <editor> in Y. Kodratoff, ed., </editor> <booktitle> `Proceedings of the fifth European conference (EWSL-91)', </booktitle> <publisher> Springer Verlag, </publisher> <pages> pp. 151-163. </pages> <note> *http://www.cs.utexas.edu/users/pclark/papers/newcn.ps Clark, </note> <author> P. & Niblett, T. </author> <year> (1989), </year> <title> `The CN2 induction algorithm', </title> <booktitle> Machine Learning 3(4), </booktitle> <pages> 261-283. </pages>
Reference: <author> Cohen, W. W. </author> <year> (1995), </year> <title> Fast effective rule induction, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993), </year> <title> `A weighted nearest neighbor algorithm for learning with symbolic features', </title> <booktitle> Machine Learning 10(1), </booktitle> <pages> 57-78. </pages>
Reference: <author> Cover, T. M. & Hart, P. E. </author> <year> (1967), </year> <title> `Nearest neighbor pattern classification', </title> <journal> IEEE Transactions on information theory IT-13(1), </journal> <pages> 21-27. </pages>
Reference: <author> Dasarathy, B. V. </author> <year> (1990), </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: ID3 The decision tree algorithm based on Quinlan (1986). It does not do any pruning. Lazy decision trees An algorithm for building the "best" decision tree for every test instance described in Friedman, Kohavi & Yun (1996). Nearest-neighbor The classical nearest-neighbor with options for weight setting, normalizations, and editing <ref> (Dasarathy 1990, Aha 1992, Wettschereck 1994) </ref>. Naive-Bayes A simple induction algorithm that assumes a conditional independence model of attributes given the label (Domingos & Pazzani 1997, Langley, Iba & Thompson 1992, Duda & Hart 1973, Good 1965). 1R The 1R algorithm described by Holte (1993).
Reference: <author> Domingos, P. & Pazzani, M. </author> <year> (1997), </year> <title> `Beyond independence: Conditions for the optimality of the simple Bayesian classifier', </title> <booktitle> Machine Learning 29(2/3), </booktitle> <pages> 103-130. </pages>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995), </year> <title> Supervised and unsupervised dis-cretization of continuous features, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 194-202. </pages>
Reference-contexts: Discretizers replace real-valued attributes in 16 the database with discrete-valued attributes for algorithms which can only use dis-crete values. MLC ++ provides several discretization methods <ref> (Dougherty, Kohavi & Sahami 1995, Kohavi & Sahami 1996) </ref>. Programming with MLC ++ generally requires little coding. Because MLC ++ contains many well-tested modules and utility classes, the bulk of MLC ++ programming is determining how to use the existing code base to implement new functionality.
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. 23 Efron, </publisher> <editor> B. & Tibshirani, R. </editor> <year> (1995), </year> <title> Cross-validation and the bootstrap: Estimating the error rate of a prediction rule, </title> <type> Technical Report 477, </type> <institution> Stanford University, Statistics department. </institution>
Reference: <author> Fayyad, U. M., Piatetsky-Shapiro, G. & Smyth, P. </author> <year> (1996), </year> <title> From data mining to knowledge discovery: An overview, in `Advances in Knowledge Discovery and Data Mining', </title> <publisher> AAAI Press and the MIT Press, </publisher> <pages> chapter 1, pp. 1-34. </pages>
Reference-contexts: Other classifiers, such as nearest-neighbor algorithms and other lazy algorithms (see Aha (1997) for details), are usually fast to train but slow in classification. 3 Given these factors, one can define a utility function to rank different algorithms <ref> (Fayyad, Piatetsky-Shapiro & Smyth 1996) </ref>. The last step is to test drive the algorithms and note their utility for your specific domain problem.
Reference: <author> Fix, E. & Hodges, J. </author> <year> (1951), </year> <title> Discriminatory analysis|nonparametric discrimination: Consistency properties, </title> <type> Technical Report 21-49-004, report no. 04, </type> <institution> USAF School of Aviation Medicine, Randolph Field, Tex. </institution>
Reference: <author> Fix, E. & Hodges, J. </author> <year> (1952), </year> <title> Discriminatory analysis|nonparametric discrimination: Small sample performance, </title> <type> Technical Report 21-49-004, report no. 11, </type> <institution> USAF School of Aviation Medicine, Randolph Field, Tex. </institution>
Reference: <author> Friedman, J., Kohavi, R. & Yun, Y. </author> <year> (1996), </year> <title> Lazy decision trees, </title> <booktitle> in `Proceedings of the Thirteenth National Conference on Artificial Intelligence', </booktitle> <publisher> AAAI Press and the MIT Press, </publisher> <pages> pp. 717-724. </pages>
Reference: <author> Garey, M. R. & Johnson, D. S. </author> <year> (1979), </year> <title> Computers and Intractability: a Guide to the Theory of NP-completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Inductive Logic Programming models (Lavrac & Dzeroski 1994, Muggleton 1992) are more powerful than propositional learning methods, yet very few practical systems are available that are as accurate as propositional systems. Most learning algorithms attempt to solve NP-hard problems <ref> (Garey & Johnson 1979) </ref> using heuristics such as limited lookahead. It is still unclear how general the heuristics are and when they will lead to solutions that are far from the global optimum.
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-48. </pages>
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference: <author> Gordon, L. & Olshen, R. A. </author> <year> (1978), </year> <title> `Asymptotically efficient solutions for the classification problem', </title> <journal> The Annals of Statistics 6(3), </journal> <pages> 515-533. </pages>
Reference: <author> Gordon, L. & Olshen, R. A. </author> <year> (1984), </year> <title> `Almost sure consistent nonparametric regression from recursive partitioning schemes', </title> <journal> Journal of Multivariate Analysis 15, </journal> <pages> 147-163. </pages>
Reference: <author> Hall, C. </author> <year> (1996), </year> <title> Data Mining: Tools and Reviews, Cutter Information Corp. Intelligent Software Strategies. </title>
Reference-contexts: We refer the reader to the URL http://www.kdnuggets.com/siftware.html 18 for pointers and description. Additionally, an excellent comparison of commercial data mining tools is available from Two Crows (Brand, Edelstein, Gerritsen, Millen-son, Schubert, Small & Small 1997), and a slightly outdated report is available from Intelligent Software Strategies <ref> (Hall 1996) </ref>. Typical features of commercial products include classification (decision trees, neural networks, instance-based methods), clustering, regression, pattern detection, and associations. SGI's MineSet MineSet is Silicon Graphics' data mining and visualization product. From release 1.1 and on, it uses MLC ++ as a base for the induction and classification algorithms.
Reference: <author> Hertz, J., Krogh, A. & Palmer, R. G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley. </publisher>
Reference-contexts: CN2 The direct rule induction algorithm by Clark & Niblett (1989) and Clark & Boswell (1991). IB The set of Instance Based learning algorithms (Nearest-neighbors) by Aha (1992). Neural network: Aspirin Migraines The Aspirin/MIGRAINES neural network environment version 6.0 from 29 Aug 1996 uses standard backpropagation <ref> (Hertz et al. 1991, Rumelhart, Hinton & Williams 1986) </ref>.
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> `Very simple classification rules perform well on most commonly used datasets', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference-contexts: Feature selection methods run a search using the inducer itself to determine which attributes in the database are useful for learning. The wrapper approach to feature selection automatically tailors the selection to the inducer being run <ref> (John, Kohavi & Pfleger 1994, Kohavi & John 1997) </ref>. A voting wrapper runs an algorithm on different portions of the dataset and lets them vote on the predicted class (Wolpert 1992, Breiman 1996b, Perrone 1993, Ali 1996).
Reference: <author> Kearns, M. J. & Vazirani, U. V. </author> <year> (1994), </year> <title> An Introduction to Computational Learning Theory, </title> <publisher> MIT Press. 24 Kohavi, R. </publisher> <year> (1995a), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> `Proceedings of the European Conference on Machine Learning', Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 174-189. </pages> <address> http://robotics.stanford.edu/~ronnyk/mlc.html. </address>
Reference: <author> Kohavi, R. </author> <year> (1995b), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1137-1143. </pages> <address> http://robotics.stanford.edu/~ronnyk. </address>
Reference-contexts: The two most important wrappers in MLC ++ are accuracy estimators and feature selectors. Accuracy estimators use any of a range of methods, such as holdout, cross-validation, or bootstrap to estimate the performance of an inducer <ref> (Kohavi 1995b) </ref>. Feature selection methods run a search using the inducer itself to determine which attributes in the database are useful for learning. The wrapper approach to feature selection automatically tailors the selection to the inducer being run (John, Kohavi & Pfleger 1994, Kohavi & John 1997).
Reference: <author> Kohavi, R. </author> <year> (1995c), </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs, </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. STAN-CS-TR-95-1560, ftp://starry.stanford.edu/pub/ronnyk/teza.ps. </institution>
Reference: <author> Kohavi, R. </author> <year> (1996), </year> <title> Scaling up the accuracy of Naive-Bayes classifiers: a decision-tree hybrid, </title> <booktitle> in `Proceedings of the Second International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 202-207. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive-Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves <ref> (Kohavi 1996) </ref>. The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning.
Reference: <author> Kohavi, R. & John, G. </author> <year> (1995), </year> <title> Automatic parameter selection by minimizing estimated error, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 304-312. </pages>
Reference-contexts: The following inducers are created as combinations of others: IDTM Induction of Decision Tables with Majority. A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 <ref> (Kohavi & John 1995) </ref>. FSS Naive-Bayes Feature subset selection on top of Naive-Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996). The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning. <p> The following inducers are created as combinations of others: IDTM Induction of Decision Tables with Majority. A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive-Bayes <ref> (Kohavi & Sommerfield 1995) </ref>. NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996). The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning. <p> Discretizers replace real-valued attributes in 16 the database with discrete-valued attributes for algorithms which can only use dis-crete values. MLC ++ provides several discretization methods <ref> (Dougherty, Kohavi & Sahami 1995, Kohavi & Sahami 1996) </ref>. Programming with MLC ++ generally requires little coding. Because MLC ++ contains many well-tested modules and utility classes, the bulk of MLC ++ programming is determining how to use the existing code base to implement new functionality.
Reference: <author> Kohavi, R. & John, G. H. </author> <year> (1997), </year> <title> `Wrappers for feature subset selection', </title> <booktitle> Artificial Intelligence 97(1-2), </booktitle> <pages> 273-324. </pages>
Reference-contexts: OODG Oblivous read-Once Decision Graph induction algorithm described in Ko havi (1995c). 4 Option Decision Trees The trees have option nodes that allow several optional splits, which are then voted as experts during classification <ref> (Kohavi & Kunz 1997) </ref>. Perceptron The simple Perceptron algorithm described in Hertz, Krogh & Palmer (1991). Winnow The multiplicative algorithm described in Littlestone (1988). The following external inducers are interfaced by MLC ++ : C4.5 The C4.5 decision-tree induction by Quinlan (1993).
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in `Tools with Artificial Intelligence', </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: Feature selection methods run a search using the inducer itself to determine which attributes in the database are useful for learning. The wrapper approach to feature selection automatically tailors the selection to the inducer being run <ref> (John, Kohavi & Pfleger 1994, Kohavi & John 1997) </ref>. A voting wrapper runs an algorithm on different portions of the dataset and lets them vote on the predicted class (Wolpert 1992, Breiman 1996b, Perrone 1993, Ali 1996).
Reference: <author> Kohavi, R. & Kunz, C. </author> <year> (1997), </year> <title> Option decision trees with majority votes, </title> <editor> in D. Fisher, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Fourteenth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 161-169. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: OODG Oblivous read-Once Decision Graph induction algorithm described in Ko havi (1995c). 4 Option Decision Trees The trees have option nodes that allow several optional splits, which are then voted as experts during classification <ref> (Kohavi & Kunz 1997) </ref>. Perceptron The simple Perceptron algorithm described in Hertz, Krogh & Palmer (1991). Winnow The multiplicative algorithm described in Littlestone (1988). The following external inducers are interfaced by MLC ++ : C4.5 The C4.5 decision-tree induction by Quinlan (1993).
Reference: <author> Kohavi, R. & Sahami, M. </author> <year> (1996), </year> <title> Error-based and entropy-based discretization of continuous features, </title> <booktitle> in `Proceedings of the Second International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 114-119. </pages>
Reference-contexts: A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive-Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves <ref> (Kohavi 1996) </ref>. The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning.
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in `The First International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 192-197. </pages>
Reference-contexts: The following inducers are created as combinations of others: IDTM Induction of Decision Tables with Majority. A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 <ref> (Kohavi & John 1995) </ref>. FSS Naive-Bayes Feature subset selection on top of Naive-Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996). The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning. <p> The following inducers are created as combinations of others: IDTM Induction of Decision Tables with Majority. A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive-Bayes <ref> (Kohavi & Sommerfield 1995) </ref>. NBTree A decision tree hybrid with Naive-Bayes at the leaves (Kohavi 1996). The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning. <p> Discretizers replace real-valued attributes in 16 the database with discrete-valued attributes for algorithms which can only use dis-crete values. MLC ++ provides several discretization methods <ref> (Dougherty, Kohavi & Sahami 1995, Kohavi & Sahami 1996) </ref>. Programming with MLC ++ generally requires little coding. Because MLC ++ contains many well-tested modules and utility classes, the bulk of MLC ++ programming is determining how to use the existing code base to implement new functionality.
Reference: <author> Kohavi, R. & Wolpert, D. H. </author> <year> (1996), </year> <title> Bias plus variance decomposition for zero-one loss functions, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 275-283. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. 25 Kononenko, </note> <author> I. </author> <year> (1993), </year> <title> `Inductive and Bayesian learning in medical diagnosis', </title> <booktitle> Ap--plied Artificial Intelligence 7, </booktitle> <pages> 317-337. </pages>
Reference-contexts: A feature subset selection wrapper on top of decision tables (Kohavi 1995a, Kohavi 1995c). C4.5-auto Automatic parameter setting for C4.5 (Kohavi & John 1995). FSS Naive-Bayes Feature subset selection on top of Naive-Bayes (Kohavi & Sommerfield 1995). NBTree A decision tree hybrid with Naive-Bayes at the leaves <ref> (Kohavi 1996) </ref>. The ability to create hybrid algorithms and wrapped algorithms is a very important and powerful approach for multistrategy learning.
Reference: <author> Koutsofios, E. & North, S. C. </author> <year> (1994), </year> <title> Drawing graphs with dot. </title> <note> Available by anonymous ftp from research.att.com:dist/drawdag/dotdoc.ps.Z. </note>
Reference-contexts: Finally, the Conversion utility changes multi-valued nominal attributes to local or binary encodings which may be more useful for nearest-neighbor or neural-network algorithms. 3.4 Visualization Some induction algorithms support visual output of the classifiers. All graph-based algorithms support can display two-dimensional representations of their graphs using dot and dotty <ref> (Koutsofios & North 1994) </ref>. Dotty is also capable of showing extra information at each node, such as class distributions. Figure 1 shows such a graph. The decision tree algorithms, such as ID3, may generate output for Silicon Graphics' MineSet product.
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of Bayesian classifiers, </title> <booktitle> in `Proceedings of the tenth national conference on artificial intelligence', </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference: <author> Lavrac, N. & Dzeroski, S. </author> <year> (1994), </year> <title> Inductive logic programming : Techniques and Applications, </title> <editor> E. </editor> <publisher> Horwood, </publisher> <address> New York. </address>
Reference-contexts: Inductive Logic Programming models <ref> (Lavrac & Dzeroski 1994, Muggleton 1992) </ref> are more powerful than propositional learning methods, yet very few practical systems are available that are as accurate as propositional systems. Most learning algorithms attempt to solve NP-hard problems (Garey & Johnson 1979) using heuristics such as limited lookahead.
Reference: <author> LeBlank, J., Ward, M. & Wittels, N. </author> <year> (1990), </year> <title> Exploring n-dimensional databases, </title> <booktitle> in `Proceedings of Visualization', </booktitle> <pages> pp. 230-237. </pages>
Reference-contexts: GLDs were described in Michalski (1978) and later used in Wnek, Sarma, Wahab & Michalski (1990), Thrun et al. (1991), and Wnek & Michalski (1994) to compare algorithms. They are sometimes called Dimensional Stacking <ref> (LeBlank, Ward & Wittels 1990) </ref> and have been rediscovered many times. Figure 3 shows a GLD for the monk1 problem. 7 8 height of each pie represents the number of instances for that value or range.
Reference: <author> Littlestone, N. </author> <year> (1988), </year> <title> `Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm', </title> <booktitle> Machine Learning 2, </booktitle> <pages> 285-318. </pages>
Reference: <author> Merz, C. & Murphy, P. </author> <year> (1998), </year> <note> `UCI repository of machine learning databases'. *http://www.ics.uci.edu/~mlearn/MLRepository.html Meyers, </note> <author> S. </author> <year> (1996), </year> <title> More Effective C ++ : 35 New Ways to Improve Your Programs and Designs, </title> <publisher> Addison Wesley Pub Co Inc. </publisher>
Reference-contexts: In the second part of the paper we describe the MLC ++ system and its dual role as a system for end-users and algorithm developers. We show a large comparison of 22 algorithms on eight large datasets for the UC Irvine repository <ref> (Merz & Murphy 1998) </ref>. A study of this magnitude would be extremely hard to conduct without such a tool, yet with MLC ++ , it was mostly a matter of CPU cycles and a few scripts to parse the output. <p> 14 30,162 15,060 75.22 chess 36 2,130 1,066 52.22 DNA 180 2,000 1,186 51.91 letter 16 15,000 5,000 4.06 shuttle 9 43,500 14,500 78.60 satimage 36 4,435 2,000 23.82 waveform-40 40 300 4,700 33.84 In this section we present a comparison on some large datasets from the UC Irvine repository <ref> (Merz & Murphy 1998) </ref>. We also take this opportunity to correct some misunderstandings of results in Holte (1993). Table 1 shows the basic characteristics of the chosen domains. Instances with unknown values were removed from the original datasets.
Reference: <author> Michalski, R. S. </author> <year> (1978), </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions, </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference: <author> Muggleton, S. </author> <year> (1992), </year> <title> Inductive Logic Programming, </title> <publisher> Academic Press. </publisher>
Reference: <author> Murphy, P. M. & Pazzani, M. J. </author> <year> (1994), </year> <title> `Exploring the decision forest: An empirical investigation of occam's razor in decision tree induction', </title> <journal> Journal of Artificial Intelligence Research 1, </journal> <pages> 257-275. </pages>
Reference: <author> Murthy, S. K., Kasif, S. & Salzberg, S. </author> <year> (1994), </year> <title> `A system for the induction of oblique decision trees', </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 1-33. </pages>
Reference: <author> Murthy, S. & Salzberg, S. </author> <year> (1995), </year> <title> Lookahead and pathology in decision tree induction, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1025-1031. </pages>
Reference: <author> Naeher, S. </author> <year> (1996), </year> <title> LEDA: A Library of Efficient Data Types and Algorithms, </title> <institution> 3.3 edn, Max-Planck-Institut fuer Informatik, </institution> <address> IM Stadtwald, D-66123 Saar-bruecken, FRG. http://www.mpi-sb.mpg.de/LEDA/leda.html. </address>
Reference-contexts: MCore classes not only provide a wide range of functions, they provide solid tests of these functions, reducing the time it takes to build more complex operations. All general-purpose classes used within the full library are from MCore, with the exception of graph classes from the LEDA library <ref> (Naeher 1996) </ref>. Although the classes in MCore were built for use by the MLC ++ library, they are not tied to machine learning and may be used as a general purpose library. MCore is a separate library which may be linked independently of the whole library. <p> Moreover, most PC-based compilers still cannot compile MLC ++ . We hope that this will change as 32-bit operating systems mature and compilers improve. We quickly chose LEDA <ref> (Naeher 1996) </ref> for graph manipulation and algorithms, and GNU's libg++, which provided some basic data structures. Unfortunately, the GNU library was found to be deficient in many respects. It hasn't kept up with the emerging C ++ standards (e.g., constness issues, templates) and we slowly rewrote all the classes used.
Reference: <author> Parsaye, K. </author> <year> (1996), </year> <title> Rules are much more than decision trees. </title> <note> http://www.datamining.com/datamine/trees.htm. 26 Perrone, </note> <author> M. </author> <year> (1993), </year> <title> Improving regression estimation: averaging methods for vari-ance reduction with extensions to general convex measure optimization, </title> <type> PhD thesis, </type> <institution> Brown University, Physics Dept. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> `Induction of decision trees', </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference: <editor> Reprinted in Shavlik and Dietterich (eds.) </editor> <booktitle> Readings in Machine Learning. </booktitle>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1994), </year> <title> Comparing connectionist and symbolic learning methods, </title> <editor> in S. J. Hanson, G. A. Drastal & R. L. Rivest, eds, </editor> <title> `Computational Learning Theory and Natural Learning Systems', Vol. I: Constraints and Prospects, </title> <publisher> MIT Press, </publisher> <pages> chapter 15, pp. 445|456. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1995), </year> <title> Oversearching and layered search in empirical learning, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1019-1024. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E. & Williams, R. J. </author> <year> (1986), </year> <title> Learning Internal Representations by Error Propagation, </title> <publisher> MIT Press, </publisher> <address> chapter 8. </address>
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <pages> pp. 259-265. </pages>
Reference-contexts: This result, sometimes called the No Free Lunch Theorem or Conservation Law <ref> (Wolpert 1994, Schaffer 1994) </ref>, assumes that all possible targets are equally likely. A brief review of theoretical results is presented in Appendix A.
Reference: <author> Stroustroup, B. </author> <year> (1994), </year> <title> The Design and Evolution of C ++ , Addison-Wesley Publishing Company. </title>
Reference-contexts: We assumed that GNU's g++ compiler would catch up. In 1987 "Mike Tiemann gave a most animated and interesting presentation of how the GNU C ++ compiler he was building would do just about everything and put all other C ++ compiler writers out of business" <ref> (Stroustroup 1994) </ref>. Sadly, the GNU C ++ compiler is still weaker than most commercial grade compilers and (at least as of 1995) cannot handle templates well enough. Moreover, most PC-based compilers still cannot compile MLC ++ .
Reference: <author> Taylor, C., Michie, D. & Spiegalhalter, D. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Paramount Publishing International. </publisher>
Reference-contexts: Users can ask questions of the form: "what if I had an instance with certain values" and see the classifier predictions and the posterior probabilities according to the Naive-Bayes model. 3.5 A Global Comparison Statlog <ref> (Taylor, Michie & Spiegalhalter 1994) </ref> was a large project that compared about 20 different learning algorithms on 20 datasets. The project, funded by the ESPRIT program of the European Community, lasted from October 1990 to June 1993.
Reference: <author> Thrun et al. </author> <year> (1991), </year> <title> The Monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Tognazzini, B. </author> <year> (1994), </year> <title> `Quality, the road less traveled', </title> <booktitle> Advanced Systems 13. </booktitle>
Reference: <author> Valiant, L. G. </author> <year> (1984), </year> <title> `A theory of the learnable', </title> <journal> Communications of the ACM 27, </journal> <pages> 1134-1142. </pages>
Reference-contexts: Relevance to learning The fact that one structure is more general or more powerful does not imply that it is easier to learn; in fact, the opposite is true: more powerful structures are provably harder to learn in some frameworks, such as Valiant's Probably Approximately Correct framework <ref> (Valiant 1984, Kearns & Vazirani 1994) </ref>. Inductive Logic Programming models (Lavrac & Dzeroski 1994, Muggleton 1992) are more powerful than propositional learning methods, yet very few practical systems are available that are as accurate as propositional systems.
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For example, whether a customer will be able to pay a loan or whether he or she will respond to a yet another credit card offer. Using methods such as holdout, bootstrap, and cross-validation <ref> (Weiss & Kulikowski 1991, Efron & Tibshirani 1995, Kohavi 1995b) </ref>, one can estimate the future prediction accuracy on unseen data quite well in practice. Comprehensibility The ability for humans to understand the data and the classification rules induced by the learning algorithm.
Reference: <author> Wettschereck, D. </author> <year> (1994), </year> <title> A Study of Distance-Based Machine Learning Algorithms, </title> <type> PhD thesis, </type> <institution> Oregon State University. </institution>
Reference: <author> Wnek, J. & Michalski, R. S. </author> <year> (1994), </year> <title> `Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments', </title> <booktitle> Machine Learning 14(2), </booktitle> <pages> 139-168. </pages> <note> 27 Wnek, </note> <author> J., Sarma, J., Wahab, A. A. & Michalski, R. S. </author> <year> (1990), </year> <title> Comparing learning paradigms via diagrammatic visualization, </title> <booktitle> in `Methodologies for Intelligent Systems, 5. Proceedings of the Fifth International Symposium', </booktitle> <pages> pp. 428-437. </pages> <note> Also technical report MLI90-2, </note> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1992), </year> <title> `Stacked generalization', </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. </pages>
Reference-contexts: The wrapper approach to feature selection automatically tailors the selection to the inducer being run (John, Kohavi & Pfleger 1994, Kohavi & John 1997). A voting wrapper runs an algorithm on different portions of the dataset and lets them vote on the predicted class <ref> (Wolpert 1992, Breiman 1996b, Perrone 1993, Ali 1996) </ref>. A discretization wrapper pre-discretizes the data, allowing algorithms that do not support continuous features (or those that do not handle them well) to work properly.
Reference: <author> Wolpert, D. H. </author> <year> (1994), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <editor> in D. H. Wolpert, ed., </editor> <title> `The Mathematics of Generalization', </title> <publisher> Addison Wesley. </publisher> <pages> 28 </pages>
Reference-contexts: This result, sometimes called the No Free Lunch Theorem or Conservation Law <ref> (Wolpert 1994, Schaffer 1994) </ref>, assumes that all possible targets are equally likely. A brief review of theoretical results is presented in Appendix A.
References-found: 71

