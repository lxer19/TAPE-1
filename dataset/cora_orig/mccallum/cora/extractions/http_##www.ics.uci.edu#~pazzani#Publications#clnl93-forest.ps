URL: http://www.ics.uci.edu/~pazzani/Publications/clnl93-forest.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: pmurphy@ics.uci.edu pazzani@ics.uci.edu  
Title: Exploring the Decision Forest  
Author: Patrick M. Murphy Michael J. Pazzani 
Note: Presented at: Computational Learning and Natural Learning Workshop, Provincetown Massachusetts, 10- 12 Spetember 1993.  
Address: Irvine, CA 92717  
Affiliation: Department of Information Computer Science University of California,  
Abstract: We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees, and the factors that affect the error rate of individual trees. The experiments were performed on a massively parallel Maspar 1 computer. The results of the experimentation on two artificial and two real world problems indicate that for three of the four problems investigated, the smallest consistent decision trees tend to be less accurate than the average accuracy of those slightly larger. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. </author> <year> (1987). </year> <title> Occam's Razor. </title> <journal> Information Processing Letters 24, </journal> <pages> 377-380. </pages> <publisher> North-Holland. </publisher>
Reference-contexts: We have shown that for some concepts the preference of simpler decision trees does not result in an increase in predictive accuracy on unseen test data, even when the simple trees are consistent with the training data. Like Schaffer, we do not dispute the theoretical papers on Occam's razor <ref> (Blumer at al., 1987) </ref>, minimum description length (Quinlan & Rivest, 1987; Muggleton et al, 1992 1987), or minimizing the number of leaves of a decision tree (Fayyad & Irani, 1990).
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Pacific Grove,CA: </address> <publisher> Wadsworth & Brooks. </publisher>
Reference: <author> Fayyad, U. M & Irani, K. B. </author> <year> (1990). </year> <booktitle> What Should be Minimized in a Decision Tree? In Proceedings of the Eighth National Conference on Artificial Intelligence AAAI-90, 749754. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Like Schaffer, we do not dispute the theoretical papers on Occam's razor (Blumer at al., 1987), minimum description length (Quinlan & Rivest, 1987; Muggleton et al, 1992 1987), or minimizing the number of leaves of a decision tree <ref> (Fayyad & Irani, 1990) </ref>. Rather, we point out that for a variety of reasons, the assumptions behind these theoretical papers mean that the results of these papers do not apply to the experiments reported here.
Reference: <author> Fayyad, U. M & Irani, K. B. </author> <year> (1992). </year> <title> The Attribute Selection Problem in Decision Tree Generation. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <pages> 104-110. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <editor> 11 Muggleton S., Srinivasan A. and Bain M. </editor> <year> (1992). </year> <title> Compression, Significance and Accuracy. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop. </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <title> UCI Repository of machine learning databases [Machinereadable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 81-106. </pages> <publisher> Kluwer. </publisher>
Reference-contexts: The goal of this experimentation is to gain insight into the quality of the decision trees produced by an existing algorithm, ID3 <ref> (Quinlan, 1986) </ref> and to understand how factors such as the size of a consistent decision tree are related to the error rate on classifying unseen test instances. For the purpose of this paper a consistent decision tree is one that correctly classifies every training example 2 . <p> a uniform distribution of errors. 0.80.60.40.20.0 0.1 0.3 0.5 7 9 Error Proportion of Trees Number of Leaves 0.80.60.40.20.0 0.1 0.3 12 14 Error Proportion of Trees Number of Leaves with 7 to 10 leaves (upper) and 11 to 14 leaves (lower) for XYZ _ AB 9 4 ID3 ID3 <ref> (Quinlan, 1986) </ref> finds a single decision tree consistent with the training data. Here, we address the issue of how the tree found by ID3 compares to the minimum size tree, and to other consistent trees with the same number of nodes as ID3's tree. Table 2 provides the relevant data.
Reference: <author> Quinlan J. R. and Rivest R. L. </author> <year> (1989). </year> <title> Inferring Decision Trees Using the Minimum Description Length Principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> pp. 227-248. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5 Programs for Machine Learning. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rissanen J. </author> <year> (1978). </year> <title> Modeling by Shortest Data Description. </title> <journal> Automatica, </journal> <volume> 14. </volume>
Reference-contexts: This last result is somewhat surprising since one gets the impression from reading the machine learning literature (e.g., Muggleton, Srivivasan & Bain, 1992) that the smaller hypothesis (i.e., the one that provides the most compression of the data <ref> (Rissanen, 1978) </ref>) is likely to be more accurate. We will explore this issue, in further detail in Section 3. First, we'll present data showing that this result is not unique to this particular concept.
Reference: <author> Schaffer, C. </author> <year> (1992). </year> <title> Sparse Data and the Effect of Overfitting Avoidance in Decision Tree Induction. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <pages> 147-152. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting Avoidance as Bias. </title> <booktitle> Machine Learning, </booktitle> <pages> 153-178. </pages> <publisher> Kluwer. </publisher>
Reference: <author> Wallace, C. & Patrick. </author> <year> (1993). </year> <title> Coding Decision Trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 7-22. </pages> <publisher> Kluwer. </publisher> <pages> 12 </pages>
References-found: 12

