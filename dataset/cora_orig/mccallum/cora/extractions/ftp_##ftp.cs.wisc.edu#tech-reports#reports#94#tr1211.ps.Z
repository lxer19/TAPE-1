URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/94/tr1211.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Email: alvy@cs.wisc.edu  
Phone: (608) 262-6617  
Title: Fast-Cache: A New Abstraction for Memory System Simulation  
Author: Alvin R. Lebeck David A. Wood 
Date: November 4, 1993  
Address: 1210 West Dayton Street Madison, WI. 53706  
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Abstract: Trace-driven simulation has long been the dominate technique for evaluating memory system performance. However, the reference trace abstraction, upon which it is based, does not exploit the full potential of on-the-fly simulation systems, which tightly couple reference generation and simulation. In particular, an on-the-fly trace-driven system must simulate each reference even when the common case, e.g., a cache hit, requires no action. We have developed a new memory system simulation method that optimizes this common case, significantly reducing simulation time. Fast-Cache tightly integrates reference generation and simulation by providing the abstraction of tagged memory blocks: each reference invokes a user-specified function depending upon the reference type and memory block state. The simulator controls how references are processed by manipulating memory block states, specifying a special NULL function for no action cases. Fast-Cache implements this abstraction by using binary-rewriting to perform a table lookup before each memory reference. On a SPARCStation 10, Fast-Cache simulation times are two to three times faster than a conventional trace-driven simulator that calls a procedure on each memory reference; simulation times are only three to six times slower than the original, un-instrumented program. Fast-Cache also outperforms specialized hardware, the error correcting code (ECC) bits on a Thinking Machines CM-5, for all but one of our experiments. Fast-Cache slowdowns range from 3.1 to 6.7, whereas ECC slowdowns range from 1.3 to 46. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Borg, A., Kessler, R. E., and Wall, D. P., </author> <title> Generation and Analysis of Very Long Address Traces, </title> <booktitle> Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <month> 270-279 (May </month> <year> 1990). </year>
Reference-contexts: On-the-fly simula tion techniqueswhere the trace is simulated as soon as it's producedhave become popular because they - 1 - eliminate I/O overhead, context switches, and large storage requirements <ref> [1, 2, 4, 20] </ref>. However, these on-the-fly simulation systems continue to use the reference trace abstraction. Although simple, this abstraction requires the simulator to process each reference; this is unnecessary in most simulations because the common case requires no action. <p> Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space <ref> [1, 2, 4, 20, 26] </ref>. Some of these systems call a procedure before each memory reference [4, 20], incurring the overhead of saving and restoring processor state for each memory reference executed. <p> Some of these systems call a procedure before each memory reference [4, 20], incurring the overhead of saving and restoring processor state for each memory reference executed. Other systems write the trace to a shared buffer and invoke the simulator when the buffer is full <ref> [1] </ref> or at basic block boundaries [26]. These systems amortize the overhead of saving and restore processor state over several memory references. - 15 - Most of the above systems instrument assembly code. Shade [2] provides an alternative technique to instru-mentation by interpreting each instruction.
Reference: 2. <author> Cmelik, R. F. and Keppel, D., Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling, </title> <type> Technical Report UWCSE 93-06-06, </type> <address> Univeristy of Washington (June 1993). </address>
Reference-contexts: On-the-fly simula tion techniqueswhere the trace is simulated as soon as it's producedhave become popular because they - 1 - eliminate I/O overhead, context switches, and large storage requirements <ref> [1, 2, 4, 20] </ref>. However, these on-the-fly simulation systems continue to use the reference trace abstraction. Although simple, this abstraction requires the simulator to process each reference; this is unnecessary in most simulations because the common case requires no action. <p> Although SPARC register windows allow saving most processor state with a single instruction, condition codes and some global registers must also be preserved across user handler invocations. We use a sequence of instructions to save and restore the condition codes <ref> [2] </ref>, avoiding the overhead of system calls. Condition codes are seldom live [10], and we could eliminate saving and restoring them on some handler invocations; however, our current implementation does not perform the necessary analysis. We use a metric called slowdown to evaluate simulation techniques. <p> As shown, Fast-Cache significantly outperforms calling a procedure to simulate each memory reference. Fast-Cache simulation times are 3.7 to 5.5 times slower than the original program, while PROC ranges from 10 to 15 times slower. Fast-Cache also compares favorably with previously published slowdowns for uniprocessor simulations <ref> [2, 4, 16] </ref>. As the dark bars show, miss detection dominates. The overhead of invoking the simulator on every memory reference clearly dominates PROC simulation times. Fast-Cache significantly reduces the absolute overhead of miss detection, although it remains a large fraction of the simulation time. <p> Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space <ref> [1, 2, 4, 20, 26] </ref>. Some of these systems call a procedure before each memory reference [4, 20], incurring the overhead of saving and restoring processor state for each memory reference executed. <p> These systems amortize the overhead of saving and restore processor state over several memory references. - 15 - Most of the above systems instrument assembly code. Shade <ref> [2] </ref> provides an alternative technique to instru-mentation by interpreting each instruction. For tracing, Shade writes addresses to an internal buffer and calls a trace analyzer when the buffer is full.
Reference: 3. <author> Dally, W. J. and Willis, D. S., </author> <title> Universal Mechanisms for Concurrency, </title> <booktitle> In Proceedings PARLE89, </booktitle> <month> (June </month> <year> 1989). </year>
Reference-contexts: Nonetheless, for hardware valid bits to be better than Fast-Cache for even moderate-sized caches would require user-mode accessible valid bits <ref> [3] </ref> and fast user-level trap mechanisms [9]. However, it is unlikely that these features will be widely available in the near future. 5. Fast-Cache Applications and Extensions The previous sections focus on counting misses for a specific cache configuration.
Reference: 4. <author> Davis, H., Goldschmidt, S. R., and Hennessy, J., </author> <title> Multiprocessor Simulation and Tracing using Tango, </title> <booktitle> Proceedings of ICPP II pp. </booktitle> <month> 99-107 (August </month> <year> 1991). </year>
Reference-contexts: On-the-fly simula tion techniqueswhere the trace is simulated as soon as it's producedhave become popular because they - 1 - eliminate I/O overhead, context switches, and large storage requirements <ref> [1, 2, 4, 20] </ref>. However, these on-the-fly simulation systems continue to use the reference trace abstraction. Although simple, this abstraction requires the simulator to process each reference; this is unnecessary in most simulations because the common case requires no action. <p> As shown, Fast-Cache significantly outperforms calling a procedure to simulate each memory reference. Fast-Cache simulation times are 3.7 to 5.5 times slower than the original program, while PROC ranges from 10 to 15 times slower. Fast-Cache also compares favorably with previously published slowdowns for uniprocessor simulations <ref> [2, 4, 16] </ref>. As the dark bars show, miss detection dominates. The overhead of invoking the simulator on every memory reference clearly dominates PROC simulation times. Fast-Cache significantly reduces the absolute overhead of miss detection, although it remains a large fraction of the simulation time. <p> Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space <ref> [1, 2, 4, 20, 26] </ref>. Some of these systems call a procedure before each memory reference [4, 20], incurring the overhead of saving and restoring processor state for each memory reference executed. <p> Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space [1, 2, 4, 20, 26]. Some of these systems call a procedure before each memory reference <ref> [4, 20] </ref>, incurring the overhead of saving and restoring processor state for each memory reference executed. Other systems write the trace to a shared buffer and invoke the simulator when the buffer is full [1] or at basic block boundaries [26].
Reference: 5. <author> Gee, Jeffrey D., Hill, Mark D., Pnevmatikatos, Dionisios N., and Smith, Alan Jay, </author> <title> Cache Performance of the SPEC92 Benchmark Suite, </title> <journal> IEEE Micro 13(4) pp. </journal> <month> 17-27 (August </month> <year> 1993). </year>
Reference-contexts: However, current simulation techniques are discouragingly slow; simulation times are at least an order of magnitude slower than the execution time of the original program. Gee, et. al. <ref> [5] </ref>, esti mate that 17 months of processing time were used to obtain miss ratios for the SPEC92 benchmarks [19]. Most memory system studies rely on trace-driven simulation [23], which consists of two components: refer ence generation and trace simulation.
Reference: 6. <author> Grunwald, D., Zorn, B., and Henderson, R., </author> <title> Improving the Cache Locality of Memory Allocation, </title> <booktitle> Proceedings PLDI, </booktitle> <pages> pp. </pages> <month> 177-186 </month> <year> (1993). </year>
Reference-contexts: 1. Introduction Simulation is the most-widely-used method to evaluate memory system performance. Hardware designers use simulation to develop new memory system architectures, while programmers use it to improve the utilization of existing memory systems <ref> [6, 8, 17, 20] </ref>. However, current simulation techniques are discouragingly slow; simulation times are at least an order of magnitude slower than the execution time of the original program.
Reference: 7. <author> Hill, M. D. and , A. J. Smith, </author> <title> Evaluating Associativity in CPU Caches, </title> <journal> IEEE Transactions on Computers 38(12) pp. </journal> <month> 1612-1630 (December </month> <year> 1989). </year>
Reference-contexts: For example, to compute miss ratio, the most popular memory system metric, a simulator need only count misses since the total number of memory references can quickly be obtained by profiling tools [13]. In Tycho <ref> [7] </ref>, the simulator used by Gee et. al., hits in the smallest (1 Kilo-byte) direct-mapped cache require no action. Therefore, sixty to seventy percent of the references invoke the simulator unnecessarily. Increasing the smallest cache size to 16 Kilo-bytes filters nearly 90% of the references. <p> Simple simulations benefit primarily from the predefined NULL handler, whereas more complex simulations also benefit from Fast-Cache's direct invocation of simulator functions. For example, the property of inclusion [18] allows efficient simulation of multiple cache configurations in a single pass over the reference trace <ref> [7, 27] </ref>. However, with the trace abstraction, simulation time is much higher for caches that do not guarantee inclusion. The simulator must probe each cache individually to determine if the reference is a hit or a miss. <p> Puzak [21] extended this work to set-associative memories by filtering references to a direct-mapped cache. However, these techniques are of little or no use with on-the-fly simulators. Simulators that evaluate several cache configurations in a single pass over the reference trace <ref> [7, 18, 27, 28] </ref> use the property of inclusion to limit the search for caches that contain a given block. The portion of the simulator modeling larger caches only requires action when the block is not contained in all smaller caches.
Reference: 8. <author> Hill, Mark D., Larus, James R., Lebeck, Alvin R., Talluri, Madhusudhan, and Wood, David A., </author> <title> Wisconsin Architectural Research Tool Set, Computer Architecture News 21(4) pp. </title> <month> 8-10 (August </month> <year> 1993). </year>
Reference-contexts: 1. Introduction Simulation is the most-widely-used method to evaluate memory system performance. Hardware designers use simulation to develop new memory system architectures, while programmers use it to improve the utilization of existing memory systems <ref> [6, 8, 17, 20] </ref>. However, current simulation techniques are discouragingly slow; simulation times are at least an order of magnitude slower than the execution time of the original program. <p> Fast-Cache inserts 9 instructions to perform the table lookup for a byte of state, and 15 instructions if memory block tags are less than 8 bits. PROC only requires 4 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 We used a C-language version of tomcatv that was restructured to improve its cache behavior <ref> [8] </ref>. - 7 - hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh | | | | | | | | | | | | Benchmarks Slowdown All-Hits Miss-Count PROC Fast-Cache Compress Eqntott Gcc Tomcatv Xlisp Fast-Cache outperforms the procedure call implementation (PROC) by a factor of 2 to 3. <p> Cache Profiling Fast-Cache can be extended to provide the data necessary to produce a cache profile <ref> [8, 17] </ref>. A cache profile identifies code sections and data structures that exhibit poor cache behavior. A very simple profile displays the number of cache misses for each memory reference. <p> Fast-Cache can be extended to provide the program counter for each memory reference by adding an additional parameter to the user handler invocations. More sophisticated cache profiles can be created by using different handlers. For example, CPROF <ref> [8] </ref> classifies cache misses as compulsory, capacity, conflict, or anti-conflict. Fast-Cache's generality allows invocation of a separate handler for each miss type. 5.3. Instruction Fetch Fast-Cache currently does not support instruction fetch simulation.
Reference: 9. <author> Johnson, D., </author> <title> Trap Architectures for Lisp Systems, </title> <booktitle> Proceedings of ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pp. </pages> <month> 419-429 (June </month> <year> 1990). </year>
Reference-contexts: Nonetheless, for hardware valid bits to be better than Fast-Cache for even moderate-sized caches would require user-mode accessible valid bits [3] and fast user-level trap mechanisms <ref> [9] </ref>. However, it is unlikely that these features will be widely available in the near future. 5. Fast-Cache Applications and Extensions The previous sections focus on counting misses for a specific cache configuration. However, Fast-Cache can be used for more complex simulations than simple miss counting.
Reference: 10. <author> Kessler, P. B., </author> <title> Fast Breakpoints: </title> <booktitle> Design and Implementation, In Proceedings of PLDI '90, </booktitle> <pages> pp. </pages> <month> 78-84 (Jun </month> <year> 1990). </year>
Reference-contexts: We use a sequence of instructions to save and restore the condition codes [2], avoiding the overhead of system calls. Condition codes are seldom live <ref> [10] </ref>, and we could eliminate saving and restoring them on some handler invocations; however, our current implementation does not perform the necessary analysis. We use a metric called slowdown to evaluate simulation techniques. Slowdown is the simulation time divided by the execution time of the original, un-instrumented program.
Reference: 11. <author> Kessler, R. E. and Hill, Mark D., </author> <title> Page Placement Algorithms for Large Real-Index Caches, </title> <journal> ACM Trans. on Computer Systems 10(4) pp. </journal> <month> 338-359 (November </month> <year> 1992). </year>
Reference-contexts: A simulation system should minimize perturbation of the application program's memory system behavior. Interleaving simulator data and application data changes data placement, and can significantly alter memory system behavior <ref> [11] </ref>. Fast-Cache avoids this problem by providing a separate data segment for the simulator and allowing users to specify alignment. Padding all additional text and data to this alignment guarantees that application data in the instrumented program maps to the same set as in the original, un-instrumented program.
Reference: 12. <author> Larus, J. R. and Ball, T., </author> <title> Rewriting Executable Files to Measure Program Behavior, </title> <note> to appear in Software Practice & Experience, </note> (). 
Reference-contexts: Furthermore, since UNIX automatically zeros data pages on first reference, Fast-Cache need not explicitly initialize the block states and un-accessed state pages are never instantiated. Fast-Cache inserts the table lookup instructions before each data reference 1 by rewriting existing executable files. Binary rewriting, also known as executable file rewriting <ref> [12] </ref>, takes a program that is executable on an existing machine, adds instrumentation code and produces an executable that runs on the same machine. We discuss some of the advantages of binary rewriting in Section 3.3.
Reference: 13. <author> Larus, J. R., </author> <title> Efficient Program Tracing, </title> <note> IEEE Computer (May 1993). </note>
Reference-contexts: Second, the trace can be saved and reused for multiple simulations, guaranteeing reproducible results and amortizing reference generation overhead. However, software reference generation techniques have improved to the point that regenerating the trace is nearly as efficient as reading it from disk or tape <ref> [13] </ref>. On-the-fly simula tion techniqueswhere the trace is simulated as soon as it's producedhave become popular because they - 1 - eliminate I/O overhead, context switches, and large storage requirements [1, 2, 4, 20]. However, these on-the-fly simulation systems continue to use the reference trace abstraction. <p> For example, to compute miss ratio, the most popular memory system metric, a simulator need only count misses since the total number of memory references can quickly be obtained by profiling tools <ref> [13] </ref>. In Tycho [7], the simulator used by Gee et. al., hits in the smallest (1 Kilo-byte) direct-mapped cache require no action. Therefore, sixty to seventy percent of the references invoke the simulator unnecessarily. Increasing the smallest cache size to 16 Kilo-bytes filters nearly 90% of the references.
Reference: 14. <author> Larus, J. R. and Schnarr, E., EEL: </author> <title> A Library for Editing Executable Files, </title> <note> To be submitted for publication., </note> (). 
Reference-contexts: Binary rewriting avoids this problem by instrumenting machine instructions. However, binary rewriting requires complex analysis to correctly insert instrumentation because it occurs after the program is linked. Recent advances in this area have produced systems that separate the details of binary rewriting from the insertion of instrumentation <ref> [14] </ref>. In some scenarios, it is necessary to include the instrumentation time in slowdown computations.
Reference: 15. <author> Leiserson, et al., Charles E., </author> <title> The Network Architecture of the Connection Machine CM-5, </title> <booktitle> Proc. ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> (July </month> <year> 1992). </year>
Reference-contexts: This simulation uses four state bits, one for each level of the memory system. The TLB and cache hierarchy handlers are similar to the handler described in Section 2. The network interface handler is much more complicated, modeling a user-level messaging system like in the CM-5 <ref> [15] </ref>. Fast-Cache provides accurate instruction cycle counts and can be used for timing dependent simulations, such as prefetching and write buffers. User handlers access the current value of the cycle counter through a global variable. Fast-Cache can be used to simulate set-associative caches as well.
Reference: 16. <author> Martonosi, M., Gupta, A., and Anderson, T., </author> <title> Effectiveness of Trace Sampling for Performance Debugging Tools, Performance Evaluation Review 21(1) pp. </title> <month> 248-259 (May </month> <year> 1993). </year>
Reference-contexts: As shown, Fast-Cache significantly outperforms calling a procedure to simulate each memory reference. Fast-Cache simulation times are 3.7 to 5.5 times slower than the original program, while PROC ranges from 10 to 15 times slower. Fast-Cache also compares favorably with previously published slowdowns for uniprocessor simulations <ref> [2, 4, 16] </ref>. As the dark bars show, miss detection dominates. The overhead of invoking the simulator on every memory reference clearly dominates PROC simulation times. Fast-Cache significantly reduces the absolute overhead of miss detection, although it remains a large fraction of the simulation time. <p> Related Work Fast-Cache builds on the observation that in most simulations, the common case requires no action. This observation has been used in several earlier efforts to improve memory system simulation. MemSpy <ref> [16] </ref> optimizes for cache hits by saving only the registers necessary to determine if a reference is a hit or a miss; hits branch around the remaining register saves and miss processing. Although we cannot directly compare MemSpy's performance to Fast-Cache, there is an important disadvantage of this approach.
Reference: 17. <author> Martonosi, M., Gupta, A., and Anderson, T., MemSpy: </author> <title> Analyzing Memory System Bottlenecks in Programs, Performance Evaluation Review 20(1) pp. </title> <month> 1-12 (June </month> <year> 1992). </year>
Reference-contexts: 1. Introduction Simulation is the most-widely-used method to evaluate memory system performance. Hardware designers use simulation to develop new memory system architectures, while programmers use it to improve the utilization of existing memory systems <ref> [6, 8, 17, 20] </ref>. However, current simulation techniques are discouragingly slow; simulation times are at least an order of magnitude slower than the execution time of the original program. <p> Cache Profiling Fast-Cache can be extended to provide the data necessary to produce a cache profile <ref> [8, 17] </ref>. A cache profile identifies code sections and data structures that exhibit poor cache behavior. A very simple profile displays the number of cache misses for each memory reference.
Reference: 18. <author> Mattson, R. L., Gecsei, J., Schultz, D. R., and Traiger, I. L., </author> <title> Evaluation Techniques for Storage Hierarchies, </title> <journal> IBM Systems Journal 9(2) pp. </journal> <month> 78-117 </month> <year> (1970). </year>
Reference-contexts: Simple simulations benefit primarily from the predefined NULL handler, whereas more complex simulations also benefit from Fast-Cache's direct invocation of simulator functions. For example, the property of inclusion <ref> [18] </ref> allows efficient simulation of multiple cache configurations in a single pass over the reference trace [7, 27]. However, with the trace abstraction, simulation time is much higher for caches that do not guarantee inclusion. <p> References to MRU blocks would invoke the NULL handler, while all other references invoke the simulator. This is similar to Puzak's trace filtering for set-associative caches [21]; the property of inclusion <ref> [18] </ref> indicates the number of references optimized is equal to the number of cache hits in a direct-mapped cache with the same number of sets as the set-associative cache. A further optimization distinguishes misses from hits to non-MRU blocks by using more than two states per cache block. <p> Puzak [21] extended this work to set-associative memories by filtering references to a direct-mapped cache. However, these techniques are of little or no use with on-the-fly simulators. Simulators that evaluate several cache configurations in a single pass over the reference trace <ref> [7, 18, 27, 28] </ref> use the property of inclusion to limit the search for caches that contain a given block. The portion of the simulator modeling larger caches only requires action when the block is not contained in all smaller caches.
Reference: 19. <institution> Newsletter, SPEC, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: However, current simulation techniques are discouragingly slow; simulation times are at least an order of magnitude slower than the execution time of the original program. Gee, et. al. [5], esti mate that 17 months of processing time were used to obtain miss ratios for the SPEC92 benchmarks <ref> [19] </ref>. Most memory system studies rely on trace-driven simulation [23], which consists of two components: refer ence generation and trace simulation. A reference generator produces a list of memory addressesa reference tracein program order. <p> For this comparison, Fast-Cache uses 8 state bits and we use an array for the cache data structure in PROC and the Fast-Cache miss handler. Section 3.3 discusses using fewer state bits in Fast-Cache. We use 5 programs from the SPEC92 benchmark suite <ref> [19] </ref>: compress, eqntott, gcc, xlisp, and tomcatv. 2 All programs operate on the SPEC input files; for gcc we simulate only the cc1 program operating on the single SPEC input file 1stmt.i. All programs are compiled with gcc [25] version 2.4.5 at optimization level -O2.
Reference: 20. <author> Porterfield, A., </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications, </title> <type> Ph. D. Thesis, </type> <institution> Dept of Computer Science, Rice University, </institution> <year> (1989). </year>
Reference-contexts: 1. Introduction Simulation is the most-widely-used method to evaluate memory system performance. Hardware designers use simulation to develop new memory system architectures, while programmers use it to improve the utilization of existing memory systems <ref> [6, 8, 17, 20] </ref>. However, current simulation techniques are discouragingly slow; simulation times are at least an order of magnitude slower than the execution time of the original program. <p> On-the-fly simula tion techniqueswhere the trace is simulated as soon as it's producedhave become popular because they - 1 - eliminate I/O overhead, context switches, and large storage requirements <ref> [1, 2, 4, 20] </ref>. However, these on-the-fly simulation systems continue to use the reference trace abstraction. Although simple, this abstraction requires the simulator to process each reference; this is unnecessary in most simulations because the common case requires no action. <p> Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space <ref> [1, 2, 4, 20, 26] </ref>. Some of these systems call a procedure before each memory reference [4, 20], incurring the overhead of saving and restoring processor state for each memory reference executed. <p> Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space [1, 2, 4, 20, 26]. Some of these systems call a procedure before each memory reference <ref> [4, 20] </ref>, incurring the overhead of saving and restoring processor state for each memory reference executed. Other systems write the trace to a shared buffer and invoke the simulator when the buffer is full [1] or at basic block boundaries [26].
Reference: 21. <author> Puzak, T. R., </author> <title> Analysis of Cache Replacement Algorithms, </title> <type> Ph. D. Thesis, </type> <institution> Dept. of Electrical and Computer Engineering, University of Massachusetts (February 1985). </institution>
Reference-contexts: References to MRU blocks would invoke the NULL handler, while all other references invoke the simulator. This is similar to Puzak's trace filtering for set-associative caches <ref> [21] </ref>; the property of inclusion [18] indicates the number of references optimized is equal to the number of cache hits in a direct-mapped cache with the same number of sets as the set-associative cache. <p> Smith [24] proposed doing this by deleting references to the n most recently used blocks. The subsequent trace can be used to obtain correct miss counts for fully associative memories that use LRU replacement with more than n blocks. Puzak <ref> [21] </ref> extended this work to set-associative memories by filtering references to a direct-mapped cache. However, these techniques are of little or no use with on-the-fly simulators.
Reference: 22. <author> Reinhardt, S. K., Hill, M. D., Larus, J. R., Lebeck, A. R., Lewis, J. C., and Wood, D. A., </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers, </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> (May </month> <year> 1993). </year>
Reference-contexts: A special case of the tagged block abstraction has been previously implemented using special hardware. The Wisconsin Wind Tunnel (WWT) uses the error correcting code (ECC) bits of a Thinking Machines CM-5 as valid bits <ref> [22] </ref>. References to valid blocks, e.g., cache hits, execute at hardware speed; references to invalid blocks, e.g., cache misses, trap to the simulator. By executing most references without software intervention, WWT potentially achieves significant performance gains over other simulation systems. <p> Fast-Cache simulations run 3.7 to 5.5 times slower than the original program on a SPARCstation 10, while a trace-driven simulator runs 10 to 15 times slower. We also compare Fast-Cache to a hardware implementation of tagged memory blocks that uses the error correcting code (ECC) bits on a CM-5 <ref> [22] </ref>. Fast-Cache slowdowns range from 3.1 to 6.7, while ECC slowdowns range from 1.3 to over 45. For very low miss ratios, ECC outperforms Fast-Cache because of its low overhead miss detection. <p> Although ECC optimizes cache hits, it incurs significant overhead for miss processing. Fast-Cache remains stable across simulations because it provides a better balance between miss detection and miss processing. The Wisconsin Wind Tunnel uses ECC to provide hardware valid bits for simulated cache blocks <ref> [22] </ref>. Misses in the simulated cache also cause misses in the hardware cache, which in turn accesses physical memory marked with bad ECC. Only those cache blocks present in the simulated cache have valid ECC.
Reference: 23. <author> Smith, A. J., </author> <title> Cache Memories, </title> <journal> ACM Computing Surveys 14(3) pp. </journal> <month> 473-530 (Sept. </month> <year> 1982). </year>
Reference-contexts: Gee, et. al. [5], esti mate that 17 months of processing time were used to obtain miss ratios for the SPEC92 benchmarks [19]. Most memory system studies rely on trace-driven simulation <ref> [23] </ref>, which consists of two components: refer ence generation and trace simulation. A reference generator produces a list of memory addressesa reference tracein program order.
Reference: 24. <author> Smith, A. J., </author> <title> Two Methods for Efficient Analysis of Memory Address Trace Data, </title> <journal> IEEE Transactions on Software Engineering 3(12)(January 1977). </journal> - <volume> 17 </volume> - 
Reference-contexts: By presenting the tagged memory block abstraction, Fast-Cache hides these low-level details from simulator writers, allowing the entire simulator to be written in C. Other work has focussed on reducing the size of reference traces by filtering out references that would hit. Smith <ref> [24] </ref> proposed doing this by deleting references to the n most recently used blocks. The subsequent trace can be used to obtain correct miss counts for fully associative memories that use LRU replacement with more than n blocks.
Reference: 25. <author> Stallman, R., </author> <title> The GNU Project Optimizing C Compiler, Free Software Foundation INC. </title> (). 
Reference-contexts: We use 5 programs from the SPEC92 benchmark suite [19]: compress, eqntott, gcc, xlisp, and tomcatv. 2 All programs operate on the SPEC input files; for gcc we simulate only the cc1 program operating on the single SPEC input file 1stmt.i. All programs are compiled with gcc <ref> [25] </ref> version 2.4.5 at optimization level -O2. Program characteristics are shown in Table 2. 3.2. Results The slowdowns for both Fast-Cache and PROC are presented in Figure 3. As shown, Fast-Cache significantly outperforms calling a procedure to simulate each memory reference.
Reference: 26. <author> Stunkel, C. B. and Fuchs, W. K., TRAPEDS: </author> <title> Producing Traces for Multicomputers Via Execution Driven Simulation, </title> <booktitle> Proc. SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. </pages> <month> 70-78 (May </month> <year> 1989). </year>
Reference-contexts: Again, references that hit in all simulated caches would invoke the NULL handler. Several simulation systems reduce simulation time by executing the reference generator and the simulator in the same address space <ref> [1, 2, 4, 20, 26] </ref>. Some of these systems call a procedure before each memory reference [4, 20], incurring the overhead of saving and restoring processor state for each memory reference executed. <p> Other systems write the trace to a shared buffer and invoke the simulator when the buffer is full [1] or at basic block boundaries <ref> [26] </ref>. These systems amortize the overhead of saving and restore processor state over several memory references. - 15 - Most of the above systems instrument assembly code. Shade [2] provides an alternative technique to instru-mentation by interpreting each instruction.
Reference: 27. <author> Sugumar, R. A. and Abraham, S. G., </author> <title> Efficient Simulation of Multiple Cache Configurations using Binomial Trees, </title> <type> Technical Report CSE-TR-111-91, </type> <year> (1991). </year>
Reference-contexts: Simple simulations benefit primarily from the predefined NULL handler, whereas more complex simulations also benefit from Fast-Cache's direct invocation of simulator functions. For example, the property of inclusion [18] allows efficient simulation of multiple cache configurations in a single pass over the reference trace <ref> [7, 27] </ref>. However, with the trace abstraction, simulation time is much higher for caches that do not guarantee inclusion. The simulator must probe each cache individually to determine if the reference is a hit or a miss. <p> An example configuration file is shown Figure 7. The performance of simulators that evaluate multiple cache configurations can also be improved by using Fast-Cache. For example, a Fast-Cache implementation that uses a binomial tree <ref> [27] </ref> to simulate multiple cache configurations would optimize references that hit at the root of the binomial tree. The binomial tree reduces the search time when determining what action to take. <p> Puzak [21] extended this work to set-associative memories by filtering references to a direct-mapped cache. However, these techniques are of little or no use with on-the-fly simulators. Simulators that evaluate several cache configurations in a single pass over the reference trace <ref> [7, 18, 27, 28] </ref> use the property of inclusion to limit the search for caches that contain a given block. The portion of the simulator modeling larger caches only requires action when the block is not contained in all smaller caches.
Reference: 28. <author> Wang, W. and Baer, J., </author> <title> Efficient TraceDriven Simulation Methods for Cache Performance Analysis, </title> <booktitle> Proc. SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. </pages> <month> 27-36 (May </month> <year> 1990). </year>
Reference-contexts: Puzak [21] extended this work to set-associative memories by filtering references to a direct-mapped cache. However, these techniques are of little or no use with on-the-fly simulators. Simulators that evaluate several cache configurations in a single pass over the reference trace <ref> [7, 18, 27, 28] </ref> use the property of inclusion to limit the search for caches that contain a given block. The portion of the simulator modeling larger caches only requires action when the block is not contained in all smaller caches.
Reference: 29. <author> Whalley, David B., </author> <title> Fast Instruction Cache Performance Evaluation Using Compiler-Time Analysis, </title> <booktitle> Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Syst ems, </booktitle> <pages> pp. </pages> <month> 13-22 (May </month> <year> 1992). </year>
Reference-contexts: Fast-Cache's generality allows invocation of a separate handler for each miss type. 5.3. Instruction Fetch Fast-Cache currently does not support instruction fetch simulation. However, it can easily be extended to use a combination of table lookup and static analysis <ref> [29] </ref> to efficiently simulate instruction fetches. Fast-Cache knows the program counter for each instruction when it is rewriting the executable. For split instruction and data caches, at the beginning of each basic block; Fast-Cache can perform a table lookup for instructions that occupy unique cache blocks.
References-found: 29

