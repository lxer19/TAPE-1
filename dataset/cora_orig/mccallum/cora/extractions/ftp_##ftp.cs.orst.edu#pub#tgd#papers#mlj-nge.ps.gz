URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/mlj-nge.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: wettscd@cs.orst.edu  tgd@cs.orst.edu  
Title: An Experimental Comparison of the Nearest-Neighbor and Nearest-Hyperrectangle Algorithms  
Author: DIETRICH WETTSCHERECK THOMAS G. DIETTERICH Editor: Richard Sutton 
Keyword: Exemplar-based learning, instance-based learning, nested generalized exemplars, nearest neighbors, feature weights.  
Address: Corvallis, Oregon, OR 97331  
Affiliation: Computer Science Department, Oregon State University,  
Note: Machine Learning, 1-25 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Algorithms based on Nested Generalized Exemplar (NGE) theory (Salzberg, 1991) classify new data points by computing their distance to the nearest "generalized exemplar" (i.e., either a point or an axis-parallel rectangle). They combine the distance-based character of nearest neighbor (NN) classifiers with the axis-parallel rectangle representation employed in many rule-learning systems. An implementation of NGE was compared to the k-nearest neighbor (kNN) algorithm in 11 domains and found to be significantly inferior to kNN in 9 of them. Several modifications of NGE were studied to understand the cause of its poor performance. These show that its performance can be substantially improved by preventing NGE from creating overlapping rectangles, while still allowing complete nesting of rectangles. Performance can be further improved by modifying the distance metric to allow weights on each of the features (Salzberg, 1991). Best results were obtained in this study when the weights were computed using mutual information between the features and the output class. The best version of NGE developed is a batch algorithm (BNGE FW MI ) that has no user-tunable parameters. BNGE FW MI 's performance is comparable to the first-nearest neighbor algorithm (also incorporating feature weights). However, the k-nearest neighbor algorithm is still significantly superior to BNGE FW MI in 7 of the 11 domains, and inferior to it in only 2. We conclude that, even with our improvements, the NGE approach is very sensitive to the shape of the decision boundaries in classification problems. In domains where the decision boundaries are axis-parallel, the NGE approach can produce excellent generalization with interpretable hypotheses. In all domains tested, NGE algorithms require much less memory to store generalized exemplars than is required by NN algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D.W., </author> <year> (1990). </year> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks. </title> <type> Tech nical Report, </type> <institution> University of California, Irvine. </institution>
Reference-contexts: If the example is equidistant to several hyperrectangles, the smallest of these is chosen. In our implementation of NGE, we first make a pass over the training examples and normalize the values of each feature into the interval [0,1] <ref> (linear normalization, Aha 1990) </ref>. Features of values in the test set are normalized by the same scaling factors (but note that they may fall outside the [0,1] range). Aside from this scaling pass, the basic algorithm is entirely incremental. Each hyperrectangle H j is labeled with an output class. <p> C4.5 also has a rectangular bias and performs, under similar conditions, significantly better than NGE in these six domains <ref> (Aha, 1990, Section 4.3.3) </ref>. 7 This suggests that the axis-parallel bias is not the cause of NGE's poor performance. Examination of the learned hyperrectangles in several of the other domains suggests that permitting rectangles to nest and overlap is a problem.
Reference: <author> Bakiri, G., </author> <year> (1991). </year> <title> Converting English Text to Speech: A Machine Learning Approach. </title> <type> Ph.D. Thesis, </type> <institution> Oregon State University, Corvallis, Oregon. </institution>
Reference-contexts: In experiments with Salzberg's method for computing feature weights, we found that performance was almost always decreased (see below). We therefore considered another procedure for computing feature weights that has given promising results in other exemplar-based learning methods <ref> (Bakiri, 1991) </ref>. 20 D. WETTSCHERECK AND T.G. DIETTERICH 6.1.
Reference: <author> Carpenter, G.A., Grossberg, S., Markuzon, N., Reynolds, J.H., & Rosen, D.B., </author> <year> (1992). </year> <title> Fuzzy ARTMAP: A Neural Network Architecture for Incremental Supervised Learning of Analog Multidimensional Maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 698-713. </pages>
Reference: <author> Dasarathy, B.V., </author> <year> (1991). </year> <title> Nearest Neighbor(NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: WETTSCHERECK AND T.G. DIETTERICH 2.2. The Nearest Neighbor Algorithm One of the most venerable algorithms in machine learning is the nearest neighbor algorithm <ref> (NN, see Dasarathy 1991 for a survey of the literature) </ref>. The entire training set is stored in memory.
Reference: <author> Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, K., Sandhu, S., Guppy, K., Lee, S. & Froelicher, V., </author> <year> (1989). </year> <title> Rapid searches for complex patterns in biological molecules. </title> <journal> American Journal of Cardiology, </journal> <volume> 64, </volume> <pages> 304-310. </pages>
Reference-contexts: The eight Irvine data sets are summarized in Table 1. There are a few important points to note: (a) the Waveform-40 domain is identical to the Waveform-21 domain with the addition of 19 irrelevant features (having random values), (b) the Cleveland database <ref> (Detrano et al., 1989) </ref> contains some missing features, and (c) many input features in the Hungarian database (Detrano et al., 1989) and the Voting Record database are missing. NEAREST-HYPERRECTANGLE COMPARISON 7 2.4. <p> There are a few important points to note: (a) the Waveform-40 domain is identical to the Waveform-21 domain with the addition of 19 irrelevant features (having random values), (b) the Cleveland database <ref> (Detrano et al., 1989) </ref> contains some missing features, and (c) many input features in the Hungarian database (Detrano et al., 1989) and the Voting Record database are missing. NEAREST-HYPERRECTANGLE COMPARISON 7 2.4. Experimental Methods To measure the performance of the NGE and nearest neighbor algorithms, we employed the training set/test set methodology.
Reference: <author> Duda, R.O., & Hart, P.E., </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Holte, R.C., </author> <year> (1993). </year> <title> Very Simple Classification Rules Perform Well on Most Commonly Used Datasets. Machine Learning, </title> <type> 11, 63-90. NEAREST-HYPERRECTANGLE COMPARISON 25 Murphy, P.M. & Aha, </type> <institution> D.W., </institution> <year> (1994). </year> <title> UCI Repository of machine learning databases [Machine readable data repository]. </title> <type> Technical Report, </type> <institution> University of California, Irvine. </institution>
Reference: <author> Quinlan, J.R. </author> <year> (1992). </year> <title> C4.5: Programs for Empirical Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf mann Publishers, INC. </publisher>
Reference-contexts: We know that in Task B (non-axis-parallel decision boundary), the axis-parallel bias is inappropriate (see reports the performance of C4.5 <ref> (Quinlan, 1992) </ref> in six of the domains which are also used in this paper. <p> For continuous features, nIntervals was chosen to be 5. The probabilities were then estimated from the training data; missing values were ignored. The mutual information measure is also known in the machine learning literature as the "information gain" used as a splitting criterion in ID3 and C4.5 <ref> (Quinlan, 1992) </ref>. 6.2. Experiments with Feature Weights the two different procedures for computing the weights. Salzberg's method gives a statistically significant increase in performance in the Hungarian domain, a statistically significant decrease in Task C, and has no significant effect in any of the other domains.
Reference: <author> Salzberg, S., </author> <year> (1991). </year> <title> A Nearest Hyperrectangle Learning Method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 277-309. </pages>
Reference: <author> Simpson, P.K. </author> <year> (1992). </year> <title> Fuzzy min-max neural networks: 1. Classification. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 776-786. </pages>
Reference-contexts: However, in domains where such rectangles are evidently not appropriate, BNGE FW MI 's performance suffers, while kNN FW MI is robust under these situations. This shows that further research is still needed to develop an NGE algorithm that is robust in such situations. 8. Related Work Simpson <ref> (Simpson, 1992) </ref> introduced an incremental algorithm which is extremely similar to NGE called Fuzzy Min-Max Neural Networks.
Reference: <author> Weiss, </author> <title> S.M., & Kulikowski, </title> <address> C.A., </address> <year> (1991). </year> <title> Computer Systems that learn. </title> <address> San Mateo CA: </address> <publisher> Morgan Kaufmann Publishers, INC. </publisher>
Reference-contexts: On Task B, simple NN outperforms NGE. At the right end of the figure (over the label "cv"), we show the performance that is obtained if leave-one-out cross-validation <ref> (Weiss & Kulikowski, 1991) </ref> is employed to determine the optimal number of seeds. This strategy worked very well, so we adopted it in all subsequent experiments (unless otherwise noted). 3 The following number of seeds was tested during each leave-one-out cross-validation run: 3, 5, 7, 10, 15, 20, and 25.
Reference: <author> Wettschereck, D. </author> <year> (1994). </year> <title> A Hybrid Nearest-Neighbor and Nearest-Hyperrectangle Algorithm. </title> <booktitle> Proceedings of the 7th European Conference on Machine Learning. </booktitle> <publisher> In press. </publisher>
References-found: 12

