URL: http://www.icsi.berkeley.edu/~dpwe/research/waspaa97/waspaa97.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~dpwe/research/waspaa97/
Root-URL: http://www.icsi.berkeley.edu
Email: &lt;dpwe@icsi.berkeley.edu&gt;  
Title: COMPUTATIONAL AUDITORY SCENE ANALYSIS EXPLOITING SPEECH-RECOGNITION KNOWLEDGE  
Author: Dan Ellis 
Address: Berkeley CA 94704  
Affiliation: International Computer Science Institute  
Abstract: The field of computational auditory scene analysis (CASA) strives to build computer models of the human ability to interpret sound mixtures as the combination of distinct sources. A major obstacle to this enterprise is defining and incorporating the kind of high level knowledge of real-world signal structure exploited by listeners. Speech recognition, while typically ignoring the problem of nonspeech inclusions, has been very successful at deriving powerful statistical models of speech structure from training data. In this paper, we describe a scene analysis system that includes both speech and nonspeech components, addressing the problem of working backwards from speech recognizer output to estimate the speech component of a mixture. Ultimately , such hybrid approaches will require more radical adaptation of current speech recognition approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Weintraub, </author> <title> A theory and computational model of monaural auditory sound separation, </title> <type> Ph.D. dissertation, </type> <institution> Stanford Univ., </institution> <year> 1985. </year>
Reference-contexts: This paper looks at integrating the approaches and domains of computational auditory scene analysis with the data-derived knowledge and ambiguity-resolution techniques of automatic speech recognition. While much previous work in CASA has been oriented towards helping the problem of speech recognition <ref> [1, 2, 3] </ref>, this has almost always been formulated in terms of a decoupled preprocessor [4]; given that speech recognition is currently the further advanced of the two domains, we consider the converse possibility of using speech recognizers to help scene analysis systems, and integrating the two processes to benefit them <p> preprocessor [4]; given that speech recognition is currently the further advanced of the two domains, we consider the converse possibility of using speech recognizers to help scene analysis systems, and integrating the two processes to benefit them both. (This was indeed the approach which Weintraub lamented he could not take <ref> [1] </ref>). A current theme in CASA work is iterative explanation, in which an account of a scene is constructed by attending to the successive residuals left after explaining more prominent pieces [5, 3, 6].
Reference: [2] <author> G. J. Brown, </author> <title> Computational Auditory Scene Analysis: a representational approach, </title> <type> Ph.D. thesis, </type> <institution> CS dept., Shef field Univ, </institution> <year> 1992. </year>
Reference-contexts: This paper looks at integrating the approaches and domains of computational auditory scene analysis with the data-derived knowledge and ambiguity-resolution techniques of automatic speech recognition. While much previous work in CASA has been oriented towards helping the problem of speech recognition <ref> [1, 2, 3] </ref>, this has almost always been formulated in terms of a decoupled preprocessor [4]; given that speech recognition is currently the further advanced of the two domains, we consider the converse possibility of using speech recognizers to help scene analysis systems, and integrating the two processes to benefit them <p> However, the majority of work in CASA has concentrated on identifying the number and extent of the different sources present, while limiting them to simple models such as smoothly-varying periodic sounds <ref> [2] </ref>. A major barrier to the mutual integration of scene analysis and speech recognition systems is their incompatible representations.
Reference: [3] <author> H. G. Okuno, T. Nakatani, T. Kawabata, </author> <title> Interfacing sound stream segregation to speech recognition systems Preliminary results of listening to several things at the same time, </title> <booktitle> Proc. AAAI-96 (2), </booktitle> <pages> pp. 1082-9, </pages> <year> 1996. </year>
Reference-contexts: This paper looks at integrating the approaches and domains of computational auditory scene analysis with the data-derived knowledge and ambiguity-resolution techniques of automatic speech recognition. While much previous work in CASA has been oriented towards helping the problem of speech recognition <ref> [1, 2, 3] </ref>, this has almost always been formulated in terms of a decoupled preprocessor [4]; given that speech recognition is currently the further advanced of the two domains, we consider the converse possibility of using speech recognizers to help scene analysis systems, and integrating the two processes to benefit them <p> A current theme in CASA work is iterative explanation, in which an account of a scene is constructed by attending to the successive residuals left after explaining more prominent pieces <ref> [5, 3, 6] </ref>. The approach adopted in this paper is to analyze mixtures of speech and environmental sounds by hypothesizing the presence of objects of both types, then iteratively refining each component.
Reference: [4] <author> M. Cooke, </author> <title> Auditory or ganisation and speech perception: Arguments for an integrated computational theory , Proc. </title> <booktitle> ESCA workshop on the Aud. Basis of Speech Percep., Keele 1996. </booktitle>
Reference-contexts: While much previous work in CASA has been oriented towards helping the problem of speech recognition [1, 2, 3], this has almost always been formulated in terms of a decoupled preprocessor <ref> [4] </ref>; given that speech recognition is currently the further advanced of the two domains, we consider the converse possibility of using speech recognizers to help scene analysis systems, and integrating the two processes to benefit them both. (This was indeed the approach which Weintraub lamented he could not take [1]). <p> Adaptive recognizers, which shift their classification boundaries to match inferred speaker characteristics, could improve nonspeech discrimination by supporting more accurate estimates of the speech component. Cooke <ref> [4] </ref> makes a number of observations concerning desirable properties of speech recognizers to be used with scene analysis systems.
Reference: [5] <author> V. R. Lesser, S. H. Nawab, F. I. Klassner, IPUS: </author> <title> An architecture for the integrated processing and understanding of signals, </title> <journal> AI Journal 77(1), </journal> <year> 1995 </year>
Reference-contexts: A current theme in CASA work is iterative explanation, in which an account of a scene is constructed by attending to the successive residuals left after explaining more prominent pieces <ref> [5, 3, 6] </ref>. The approach adopted in this paper is to analyze mixtures of speech and environmental sounds by hypothesizing the presence of objects of both types, then iteratively refining each component. <p> Speech recognizers similarly ignore periodicity, although perceptual experiments demonstrate pitch to be an important basis for sound organization [12]. Few CASA systems have exploited much structure in their signals beyond local data features; top-down components rely on stored templates <ref> [5] </ref>. Some problems and possible improvements The iteration between speech and nonspeech presents a startup problem: one or other component has to make a preliminary ef fort to recognize the mixture.
Reference: [6] <author> D. P. W. Ellis, </author> <title> Prediction-driven Computational Auditory Scene Analysis, </title> <type> Ph.D. dissertation, </type> <institution> EECS dept., M.I.T., </institution> <year> 1996. </year>
Reference-contexts: A current theme in CASA work is iterative explanation, in which an account of a scene is constructed by attending to the successive residuals left after explaining more prominent pieces <ref> [5, 3, 6] </ref>. The approach adopted in this paper is to analyze mixtures of speech and environmental sounds by hypothesizing the presence of objects of both types, then iteratively refining each component. <p> The approach adopted in this paper is to analyze mixtures of speech and environmental sounds by hypothesizing the presence of objects of both types, then iteratively refining each component. Exploiting general source knowledge represented as the state of hypothesized models is the explicit goal of Ellis s Prediction-driven CASA <ref> [6] </ref>; it is also implicit in Moores decomposition of a signal as the combination of hidden Markov models [7]. <p> Each component model attempts to explain the partial spectrum it has been given according to its constraints: The speech recognizer searches for a matching phoneme sequence, and the nonspeech analyzer (which is a simplified version of the system described in <ref> [6] </ref>) tries to match its input with simple noise elements. In each case, a model will generate two outputs: abstract model parameters (e.g. the phoneme sequence or the noise profiles), and the spectral sur - face implied by these parameters. <p> The current system would benefit from his idea of a recognizer that penalizes absence of ener gy more strongly than excess (since excess could be caused by mixture components, whereas absence cannot). This ties into one of the key ideas of the prediction-driven approach of Ellis <ref> [6] </ref>, that the ubiquity of masking is a problem for models based on subtraction and residuals: a distinction must be made between the absence of ener gy in a given channel, and the situation when an existing element has accounted for all the input energy at a level that could be
Reference: [7] <author> R. K. Moore, </author> <title> Signal decomposition using Markov modeling techniques, </title> <journal> Royal Sig. Res. Estab. Tech. </journal> <volume> memo no. 3931, </volume> <year> 1986 </year>
Reference-contexts: Exploiting general source knowledge represented as the state of hypothesized models is the explicit goal of Ellis s Prediction-driven CASA [6]; it is also implicit in Moores decomposition of a signal as the combination of hidden Markov models <ref> [7] </ref>. However, the majority of work in CASA has concentrated on identifying the number and extent of the different sources present, while limiting them to simple models such as smoothly-varying periodic sounds [2]. <p> This approach treats the nonspeech component as stationary; by contrast, scene analysis explicitly detects and models interference and can exploit structure in the nonspeech component. HMM decomposition <ref> [7] </ref> is also able to exploit prior knowledge of dynamic nonspeech additions by finding combinations of hidden Markov models for both speech and interference that fit the mixture.
Reference: [8] <author> S. Pfieffer, S. Fischer, W. Effelsberg, </author> <title> Automatic audio content analysis, </title> <booktitle> ACM Multimedia'96, </booktitle> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: Other applications could include multimedia indexing interested more in extracting the nonspeech sound ef fects as content indicators, but which must handle speech appropriately <ref> [8] </ref>. Another scenario is a portable aid for the deaf, providing a textual description of the sound environment in near-real time [9]. Ultimately this same information will be required by humanlike robots of the future. 2.
Reference: [9] <author> R. S. Goldhor, Audiofile Inc., </author> <title> private communication, </title> <year> 1992. </year>
Reference-contexts: Other applications could include multimedia indexing interested more in extracting the nonspeech sound ef fects as content indicators, but which must handle speech appropriately [8]. Another scenario is a portable aid for the deaf, providing a textual description of the sound environment in near-real time <ref> [9] </ref>. Ultimately this same information will be required by humanlike robots of the future. 2. SYSTEM OVERVIEW mixture is fed to the front-end, which consists of a bank of bandpass filters approximating the critical bands of the human auditory system, followed by temporal envelope extraction.
Reference: [10] <author> H. Hermansky, N. Morgan, </author> <title> RASTA processing of speech, </title> <journal> IEEE Tr. Speech & Aud. Proc., </journal> <volume> 2(4), </volume> <pages> pp. 578-589, </pages> <year> 1994. </year>
Reference-contexts: Ultimately this same information will be required by humanlike robots of the future. 2. SYSTEM OVERVIEW mixture is fed to the front-end, which consists of a bank of bandpass filters approximating the critical bands of the human auditory system, followed by temporal envelope extraction. This gives a smooth filtering <ref> [10] </ref>, which applies a bandpass filter in the log-domain to the envelope of each frequency channel to remove long-term transfer characteristics. This normalized spectrum is smoothed with the socalled Perceptual Linear Prediction, then projected into a condensed and decorrelated feature space with a truncated cepstral transform.
Reference: [11] <author> M. F. Gales, S. J. Young, </author> <title> Robust continuous speech recognition using parallel model combination, </title> <journal> IEEE Tr. Speech & Aud. Proc., </journal> <volume> 4(5), </volume> <pages> pp. 352-9, </pages> <year> 1996. </year>
Reference: [12] <author> P. F. Assmann & Q. Summerfield, </author> <title> Modeling the perception of concurrent vowels: Vowels with dif ferent fundamental frequencies, </title> <journal> J. Acous. Soc. Am. </journal> <volume> 88(2), </volume> <pages> pp. 680-697, </pages> <year> 1990. </year>
Reference-contexts: The biggest distinction between the current system and most work in CASA is that it does not use the pitch cue. Speech recognizers similarly ignore periodicity, although perceptual experiments demonstrate pitch to be an important basis for sound organization <ref> [12] </ref>. Few CASA systems have exploited much structure in their signals beyond local data features; top-down components rely on stored templates [5].
References-found: 12

