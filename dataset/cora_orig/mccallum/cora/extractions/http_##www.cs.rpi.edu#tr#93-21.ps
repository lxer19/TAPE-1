URL: http://www.cs.rpi.edu/tr/93-21.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Email: stewart@cs.rpi.edu  
Title: A New Robust Operator for Computer Vision: Theoretical Analysis  
Author: Charles V. Stewart 
Date: March 23, 1994  
Address: Troy, New York 12180-3590  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Abstract: fl The author would like to acknowledge the financial support of the National Science Foundation under grant IRI-9217195, and the assistance of Robin Flatland and James Miller for their detailed comments on earlier drafts of this paper. 1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abramowitz and I. A. Stegum. </author> <title> Handbook of Mathematical Functions. </title> <publisher> Dover Publications, </publisher> <year> 1968. </year>
Reference-contexts: Properties of the criterion function follow from the binomial distribution and its equivalence to the incomplete beta function <ref> [1] </ref>: F (r; k; N ) = i=k B B N 1 C A r i r Ni N ! Z r=Z 0 t k1 (1 t) Nk dt: (2) The incomplete beta function gives a second definition of F .
Reference: [2] <author> D. H. Ballard and C. M. Brown. </author> <title> Computer Vision. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year> <month> 34 </month>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point". <p> However, if we assume such a conspiracy is unlikely, we should be able to tolerate higher percentages of bad data, at least in a probabilistic sense. This implies that we need to make some assumptions about the distribution of bad data. Hough transform techniques <ref> [2, 9] </ref> and Ransac techniques [6] are well-known techniques that can accept fits involving fewer than half of the data points by assuming the good data are within a known distance (inlier bound) of the correct fit.
Reference: [3] <author> P. J. Besl, J. B. Birch, and L. T. Watson. </author> <title> Robust window operators. </title> <booktitle> In Proceedings of the IEEE International Conference on Computer Vision, </booktitle> <pages> pages 591-600, </pages> <year> 1988. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including [2, 6, 9, 10, 14, 15, 25], while others have been borrowed from statistics <ref> [3, 11, 13, 17, 19, 21] </ref>. Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit [17]. <p> In comparing MINPRAN to other robust techniques, if there are relatively few outliers in the data and the discontinuities are relatively isolated, then techniques such M-estimators and its variants are advantageous because they are more computationally efficient than high-breakdown operators <ref> [3, 15] </ref>. Techniques that attain a 0.5 breakdown point assume that at least half of the data arise from a single surface or function [13, 21]. When this is true these techniques may be sufficient.
Reference: [4] <author> R. C. Bolles and M. A. Fischler. </author> <title> A Ransac-based approach to model fitting and its application to finding cylinders in range data. </title> <booktitle> In Proceedings Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 637-643, </pages> <year> 1981. </year>
Reference-contexts: Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms <ref> [4, 6] </ref>, practical applications of least-median of squares operators [11, 13, 17, 19], and other robust techniques [7, 10, 16]. The idea behind random sampling is as follows.
Reference: [5] <author> H. Edelsbrunner and D. L. Souvaine. </author> <title> Computing least median of squares regression lines and guided topological sweep. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 85 </volume> <pages> 115-119, </pages> <year> 1990. </year>
Reference-contexts: such a line or a plane, or it may be any other parameterized function. (In general, fits are of the form f (~x; ~a) = 0, where ~x is the vector of model variables and ~a is the vector of model parameters.) Although it may be possible to generalize algorithms <ref> [5, 20] </ref> that find the global minimum median of squares to minimize F when the fits are linear, these algorithms are already too inefficient for vision applications. Instead, MINPRAN uses random sampling to minimize F .
Reference: [6] <author> M. A. Fischler and R. C. Bolles. </author> <title> Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. </title> <journal> Communications of the ACM, </journal> <volume> 24 </volume> <pages> 381-395, </pages> <year> 1981. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point". <p> However, if we assume such a conspiracy is unlikely, we should be able to tolerate higher percentages of bad data, at least in a probabilistic sense. This implies that we need to make some assumptions about the distribution of bad data. Hough transform techniques [2, 9] and Ransac techniques <ref> [6] </ref> are well-known techniques that can accept fits involving fewer than half of the data points by assuming the good data are within a known distance (inlier bound) of the correct fit. <p> However, in order to distinguish good fits from random fits, they must also assume that the bad data are uniformly distributed <ref> [6, 8, 16] </ref>. By contrast, our new robust operator surpasses the 0.5 breakdown point solely by assuming that the bad data are uniformly distributed. From this assumption we derive a criterion function that measures the probability that a configuration of points near a hypothesized fit could have occurred randomly. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms <ref> [4, 6] </ref>, practical applications of least-median of squares operators [11, 13, 17, 19], and other robust techniques [7, 10, 16]. The idea behind random sampling is as follows.
Reference: [7] <author> M. A. Fischler and O. Firschein. </author> <title> Parallel guessing: A strategy for high-speed computation. </title> <journal> Pattern Recognition, </journal> <volume> 20 </volume> <pages> 257-263, </pages> <year> 1987. </year>
Reference-contexts: The remainder of the present paper is organized as follows. Section 2 introduces the criterion function and proves several of its important properties. Section 3 presents the MINPRAN algorithm, which is based on a random sampling search technique <ref> [7] </ref>, and derives both the number of random samples required in MINPRAN and a lower bound on the criterion function, which MINPRAN uses to distinguish good fits from random fits. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators [11, 13, 17, 19], and other robust techniques <ref> [7, 10, 16] </ref>. The idea behind random sampling is as follows.
Reference: [8] <author> W. E. L. Grimson and D. Huttenlocher. </author> <title> On the sensitivity of the Hough transform for object recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 255-274, </pages> <year> 1990. </year>
Reference-contexts: However, in order to distinguish good fits from random fits, they must also assume that the bad data are uniformly distributed <ref> [6, 8, 16] </ref>. By contrast, our new robust operator surpasses the 0.5 breakdown point solely by assuming that the bad data are uniformly distributed. From this assumption we derive a criterion function that measures the probability that a configuration of points near a hypothesized fit could have occurred randomly.
Reference: [9] <author> J. Illingworth and J. Kittler. </author> <title> A survey of the Hough transform. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 44 </volume> <pages> 87-116, </pages> <year> 1988. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point". <p> However, if we assume such a conspiracy is unlikely, we should be able to tolerate higher percentages of bad data, at least in a probabilistic sense. This implies that we need to make some assumptions about the distribution of bad data. Hough transform techniques <ref> [2, 9] </ref> and Ransac techniques [6] are well-known techniques that can accept fits involving fewer than half of the data points by assuming the good data are within a known distance (inlier bound) of the correct fit.
Reference: [10] <author> J.-M. Jolion, P. Meer, and S. Bataouche. </author> <title> Robust clustering with applications in computer vision. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 791-802, </pages> <year> 1991. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point". <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators [11, 13, 17, 19], and other robust techniques <ref> [7, 10, 16] </ref>. The idea behind random sampling is as follows.
Reference: [11] <author> R. Kumar and A. R. Hanson. </author> <title> Analysis of different robust methods for pose refinement. </title> <booktitle> In Proceedings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 167-182, </pages> <year> 1990. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including [2, 6, 9, 10, 14, 15, 25], while others have been borrowed from statistics <ref> [3, 11, 13, 17, 19, 21] </ref>. Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit [17]. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators <ref> [11, 13, 17, 19] </ref>, and other robust techniques [7, 10, 16]. The idea behind random sampling is as follows.
Reference: [12] <author> P. Meer, D. Mintz, and A. Rosenfeld. </author> <title> Least median of squares based robust analysis of image structure. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <pages> pages 231-254, </pages> <year> 1990. </year>
Reference-contexts: The problem of smoothing across multiple curves (or surfaces) is common to existing fitting techniques such as least median of squares and least sum of squares (see <ref> [12, 14] </ref> for detailed discussion). It must be addressed in any application of MINPRAN where multiple fits are possible within a given set of data.
Reference: [13] <author> P. Meer, D. Mintz, A. Rosenfeld, and D. Y. Kim. </author> <title> Robust regression methods for computer vision: A review. </title> <journal> International Journal of Computer Vision, </journal> <volume> 6 </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Robust vision techniques are designed to accurately fit parametrized functions to intensity, edge or range data while ignoring gross errors ("outliers") in the data <ref> [13, 18, 24] </ref>. Since these outliers are typical of both real-world sensors and low-level vision algorithms, robust techniques will become more important as computer vision systems move from controlled laboratory settings to real applications. <p> Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including [2, 6, 9, 10, 14, 15, 25], while others have been borrowed from statistics <ref> [3, 11, 13, 17, 19, 21] </ref>. Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit [17]. <p> We can demonstrate the strengths of our new operator, which we call "MINPRAN" (MINimize the Probability of RANdomness), by contrasting it with least median of squares (LMedS), one of the highest breakdown operators used in vision that does not assume a known inlier bound <ref> [13, 19] </ref>. * In finding the best fit, MINPRAN finds nearly the correct number of inliers, including cases where fewer than 50% of the points are inliers to any one fit. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators <ref> [11, 13, 17, 19] </ref>, and other robust techniques [7, 10, 16]. The idea behind random sampling is as follows. <p> Techniques that attain a 0.5 breakdown point assume that at least half of the data arise from a single surface or function <ref> [13, 21] </ref>. When this is true these techniques may be sufficient. However, we have demonstrated the clear advantages that MINPRAN has over these techniques when its simple assumptions are realized in the data.
Reference: [14] <author> D. Mintz. </author> <title> Robustness by consensus. </title> <type> Technical Report CAR-TR-576, </type> <institution> University of Maryland | Center for Automation Research, </institution> <year> 1991. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point". <p> The problem of smoothing across multiple curves (or surfaces) is common to existing fitting techniques such as least median of squares and least sum of squares (see <ref> [12, 14] </ref> for detailed discussion). It must be addressed in any application of MINPRAN where multiple fits are possible within a given set of data.
Reference: [15] <author> M. J. Mirza and K. L. Boyer. </author> <title> Performance evaluation of a class of M-estimators for surface parameter estimation in noisy range data. </title> <type> Technical Report SAMPL-91-03, </type> <institution> The Ohio State University, </institution> <year> 1991. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point". <p> In comparing MINPRAN to other robust techniques, if there are relatively few outliers in the data and the discontinuities are relatively isolated, then techniques such M-estimators and its variants are advantageous because they are more computationally efficient than high-breakdown operators <ref> [3, 15] </ref>. Techniques that attain a 0.5 breakdown point assume that at least half of the data arise from a single surface or function [13, 21]. When this is true these techniques may be sufficient.
Reference: [16] <author> G. Roth and M. D. Levine. </author> <title> Extracting geometric primitives. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 58 </volume> <pages> 1-22, </pages> <year> 1993. </year>
Reference-contexts: However, in order to distinguish good fits from random fits, they must also assume that the bad data are uniformly distributed <ref> [6, 8, 16] </ref>. By contrast, our new robust operator surpasses the 0.5 breakdown point solely by assuming that the bad data are uniformly distributed. From this assumption we derive a criterion function that measures the probability that a configuration of points near a hypothesized fit could have occurred randomly. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators [11, 13, 17, 19], and other robust techniques <ref> [7, 10, 16] </ref>. The idea behind random sampling is as follows. <p> The idea behind random sampling is as follows. Assuming that p points are required to completely instantiate a fit, S random subsets of p points are chosen from the N data points, and the model is fit to each random subset, forming a set of hypothesized fits. (See Roth <ref> [16] </ref> for a discussion of function fitting procedures in this context.) For each fit, the residuals of the remaining N p points are computed relative to it, and the criterion function is evaluated based on these N p residuals.
Reference: [17] <author> P. J. Rousseeuw and A. M. Leroy. </author> <title> Robust Regression and Outlier Detection. </title> <publisher> John Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including [2, 6, 9, 10, 14, 15, 25], while others have been borrowed from statistics <ref> [3, 11, 13, 17, 19, 21] </ref>. Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit [17]. <p> Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit <ref> [17] </ref>. For example, least sum of squares has a breakdown point of 0, because one arbitrarily bad point out of N points can completely corrupt the fit, regardless of N . <p> On the other hand, least median of squares has a breakdown point of 0.5, because up to half of the data may be bad without altering the fit. Theoretically, the maximum possible breakdown point is 0.5 <ref> [17] </ref>. <p> Rousseeuw argues that a breakdown point higher that 0.5 is not possible because when more than half of the data are bad they may "conspire" to look better than the correct 3 fit <ref> [17] </ref>. However, if we assume such a conspiracy is unlikely, we should be able to tolerate higher percentages of bad data, at least in a probabilistic sense. This implies that we need to make some assumptions about the distribution of bad data. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators <ref> [11, 13, 17, 19] </ref>, and other robust techniques [7, 10, 16]. The idea behind random sampling is as follows.
Reference: [18] <author> B. G. Schunck. </author> <title> Robust computational vision. </title> <booktitle> In Proceedings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 1-18, </pages> <year> 1990. </year> <month> 36 </month>
Reference-contexts: 1 Introduction Robust vision techniques are designed to accurately fit parametrized functions to intensity, edge or range data while ignoring gross errors ("outliers") in the data <ref> [13, 18, 24] </ref>. Since these outliers are typical of both real-world sensors and low-level vision algorithms, robust techniques will become more important as computer vision systems move from controlled laboratory settings to real applications.
Reference: [19] <author> S. S. Sinha and B. G. Schunck. </author> <title> A two-stage algorithm for discontinuity-preserving sur-face reconstruction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 36-55, </pages> <year> 1992. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including [2, 6, 9, 10, 14, 15, 25], while others have been borrowed from statistics <ref> [3, 11, 13, 17, 19, 21] </ref>. Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit [17]. <p> We can demonstrate the strengths of our new operator, which we call "MINPRAN" (MINimize the Probability of RANdomness), by contrasting it with least median of squares (LMedS), one of the highest breakdown operators used in vision that does not assume a known inlier bound <ref> [13, 19] </ref>. * In finding the best fit, MINPRAN finds nearly the correct number of inliers, including cases where fewer than 50% of the points are inliers to any one fit. <p> Instead, MINPRAN uses random sampling to minimize F . Random sampling techniques are the basis of Ransac algorithms [4, 6], practical applications of least-median of squares operators <ref> [11, 13, 17, 19] </ref>, and other robust techniques [7, 10, 16]. The idea behind random sampling is as follows.
Reference: [20] <author> D. L. Souvaine and J. M. Steele. </author> <title> Time- and space-efficient algorithms for least median of squares regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 794-801, </pages> <year> 1987. </year>
Reference-contexts: such a line or a plane, or it may be any other parameterized function. (In general, fits are of the form f (~x; ~a) = 0, where ~x is the vector of model variables and ~a is the vector of model parameters.) Although it may be possible to generalize algorithms <ref> [5, 20] </ref> that find the global minimum median of squares to minimize F when the fits are linear, these algorithms are already too inefficient for vision applications. Instead, MINPRAN uses random sampling to minimize F .
Reference: [21] <author> A. Stein and M. Werman. </author> <title> Robust statistics in shape fitting. </title> <booktitle> In Proceedings IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 540-546, </pages> <year> 1992. </year>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including [2, 6, 9, 10, 14, 15, 25], while others have been borrowed from statistics <ref> [3, 11, 13, 17, 19, 21] </ref>. Robust techniques may be characterized by their "breakdown point". This is the highest fraction of arbitrarily bad data that can be tolerated without these data being able to completely corrupt a fit [17]. <p> Techniques that attain a 0.5 breakdown point assume that at least half of the data arise from a single surface or function <ref> [13, 21] </ref>. When this is true these techniques may be sufficient. However, we have demonstrated the clear advantages that MINPRAN has over these techniques when its simple assumptions are realized in the data.
Reference: [22] <author> C. V. Stewart. </author> <title> A new robust operator for computer vision: Theoretical analysis. </title> <type> Technical Report 93-21, </type> <institution> Department of Computer Science, Rensselaer Polytechnic Institute, </institution> <month> August, </month> <year> 1993. </year>
Reference-contexts: We can modify the derivations in Sections 4.1 and 3.2 to study the impact on MINPRAN's evaluation of the correct fit 24 and on MINPRAN's ability to separate good fits from random fits. See <ref> [22] </ref> for details.
Reference: [23] <author> C. V. Stewart. </author> <title> A new robust operator for computer vision: Application to range and intensity images. </title> <type> Technical report, </type> <institution> Department of Computer Science, Rensselaer Polytechnic Institute, </institution> <month> September, </month> <year> 1993. </year>
Reference-contexts: It presents a theoretical analysis of MINPRAN's ability to find good fits, to avoid hallucinating, and to surpass the 0.5 breakdown point. The related paper develops a robust technique for fitting surface patches to range and intensity data based on MINPRAN <ref> [23] </ref>. In doing so, it modifies the basic MINPRAN algorithm described here to reliably find multiple fits in a set of data. The remainder of the present paper is organized as follows. Section 2 introduces the criterion function and proves several of its important properties. <p> Unfortunately, as our analysis will show, this straightforward extension of MINPRAN leads to incorrect results in many cases. Correctly handling multiple fits in a region requires a change to MINPRAN's search strategy, as described in <ref> [23] </ref>. <p> Fortunately, as shown in <ref> [23] </ref>, these random fits can be identified and eliminated because they include nearly all random points as inliers and therefore produce extremely large estimates of the variance in the data. <p> It must be addressed in any application of MINPRAN where multiple fits are possible within a given set of data. In applying MINPRAN to intensity and range images in <ref> [23] </ref> we analyze this problem further and introduce two solutions whose effectiveness 28 is demonstrated both theoretically and experimentally. 5 Experimental Results We have implemented MINPRAN and tested its ability to fit planar patches of the form z = a 0 + a 1 x + a 2 y in synthetic <p> As a preliminary result, Figure 9 presents an example fit for the data in Figure 1 (a). (Because of the analysis in Section 4.2.2, our experimentation only concentrates on single fits. The data sets in Figure 1 (b) and (c) can be handled using the extended techniques in <ref> [23] </ref>.) Of the 144 data points, there were 58 inliers. MINPRAN chose 56 inliers in its fit. For the main results, we present statistical summaries of MINPRAN's performance averaged over a large number of experiments. <p> Finally, although the analysis in this paper has pointed out many strengths of MINPRAN, it has also highlighted an important weakness: when the data arise from two functions, MINPRAN has a tendency to find a single fit that includes all inliers to both functions. In <ref> [23] </ref>, we address this problem using two techniques: one that alters MINPRAN's search strategy to find multiple fits simultaneously, and a second that applies a final randomness test to eliminate erroneous fits.
Reference: [24] <author> X. Zhuang and R. M. Haralick. </author> <title> Developing robust techniques for computer vision. </title> <booktitle> In Proceedings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 19-38, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Robust vision techniques are designed to accurately fit parametrized functions to intensity, edge or range data while ignoring gross errors ("outliers") in the data <ref> [13, 18, 24] </ref>. Since these outliers are typical of both real-world sensors and low-level vision algorithms, robust techniques will become more important as computer vision systems move from controlled laboratory settings to real applications.
Reference: [25] <author> X. Zhuang, T. Wang, and P. Zhang. </author> <title> A highly robust estimator through partially likelihood function modeling and its application in computer vision. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 19-35, </pages> <year> 1992. </year> <month> 37 </month>
Reference-contexts: Of the robust techniques that have been applied in computer vision, some have been developed within the vision field, including <ref> [2, 6, 9, 10, 14, 15, 25] </ref>, while others have been borrowed from statistics [3, 11, 13, 17, 19, 21]. Robust techniques may be characterized by their "breakdown point".
References-found: 25

