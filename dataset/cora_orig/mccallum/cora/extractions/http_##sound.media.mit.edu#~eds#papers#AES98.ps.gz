URL: http://sound.media.mit.edu/~eds/papers/AES98.ps.gz
Refering-URL: http://sound.media.mit.edu/papers.html
Root-URL: http://www.media.mit.edu
Title: Algorithmic and Wavetable Synthesis in the MPEG-4 Multimedia Standard  
Author: ERIC D. SCHEIRER, AES Student Member 
Note: AND Joint EMu/Creative  
Address: Cambridge MA  Valley CA  
Affiliation: MIT Media Laboratory, Machine Listening Group,  Technology Center, Scotts  
Abstract: The newly released MPEG-4 standard for multimedia transmission contains several novel tools for the low-bitrate coding of audio. Among these is a new codec called Structured Audio that allows sound to be transmitted in a synthetic description format and synthesized at the client. MPEG-4 Structured Audio contains facilities for both algorithmic synthesis, using a new software-synthesis language called SAOL, and wavetable synthesis, using a new format for the efficient transmission of banks of samples. We contrast the use of these techniques for various multimedia applications, discussing scenarios in which one is favored over the other, or in which they are profitably used together.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. A. Casey, </author> <title> Auditory Group Theory with Applications to Statistical Basis Methods for Structured Audio, </title> <type> unpublished Ph.D. dissertation, </type> <institution> MIT Media Laboratory (1998). </institution>
Reference-contexts: The broad scope of synthesis techniques, such as physical models, that allow more expressivity in performance than wavetable techniques are easily accessible through the MPEG-4 Structured Audio Full Profile. For use in videogames and virtual-reality environments, advanced synthetic Foley instruments <ref> [1] </ref> can provide highly-controllable parametric sound effects. Again, such techniques are not available through wavetable synthesis, but require Structured Audio capability to implement. As well as expressive sound synthesis, complex sound-generation algorithms may be written in SAOL and transmitted as content in Structured Audio.
Reference: [2] <author> M. A. Casey and P. Smaragdis, NetSound, </author> <booktitle> Proc. Int. Computer Music Conf., </booktitle> <address> Hong Kong (1996). </address>
Reference-contexts: Composers, especially in academia, have used languages such as Music-IV [9], Csound [20], and SuperCollider [10] to compose music drawing from a wide range of sonic possibilities. The recent development of using general-purpose software synthesis as a transmission protocol first in the prototype NetSound system <ref> [2] </ref> and now in MPEG-4 allows these techniques to be brought to bear in a more general audio-coding environment. However, it is only recently that general-purpose software synthesis implementations have allowed real-time live synthesis.
Reference: [3] <author> R. Dannenberg, </author> <title> Machine Tongues XIX: Nyquist, a language for composition and soundsynthesis, </title> <journal> Computer Music J., </journal> <volume> vol 21 no 3, </volume> <pages> pp. </pages> <month> 50-60 </month> <year> (1997). </year>
Reference-contexts: The remainder of this section discusses the soundsynthesis capabilities of MPEG-4 in more detail. 2.1 Algorithmic synthesis with SAOL SAOL is a powerful and flexible music language of the type known as Music-N languages. Other languages of this type are Csound [20], Nyquist <ref> [3] </ref> and SuperCollider [10]. SAOL is not a method of sound synthesis, but a language for the description of methods of sound synthesis. Any existing or future soundsynthesis algorithm may be described in SAOL.
Reference: [4] <author> W. G. Gardner, </author> <title> Reverberation algorithms, </title> <editor> in Brandenberg K. and M. Kahrs (eds), </editor> <title> Applications of Signal Processing to Audio and Acoustics (Kluwer Academic Publishing, </title> <address> New York, </address> <year> 1998). </year>
Reference-contexts: For example, to transmit the sound of a person speaking in a reverberant environment, the flat unreverberated speech is transmitted using the MPEG-4 speech coder, along with an algorithm implementing a synthetic reverbator <ref> [4] </ref>. The resulting sound is a higher-quality reverberated speech sound than could be acheived with the speech coder alone at that bitrate. Using AudioBIFS also allows sound objects, once mixed and postprocessed as described above, to be placed in 3-D space and synchronized with computer graphics or streaming video.
Reference: [5] <author> A. Gersho, </author> <title> Advances in speech and audio compression, </title> <journal> Proc. IEEE, </journal> <volume> vol. 82, </volume> <pages> pp. </pages> <month> 900-918 </month> <year> (1994 </year> <month> Jun). </month>
Reference-contexts: Techniques such as data reduction through linear prediction [8] (which is also the basis of CELP-based speech coding methods <ref> [5] </ref>) transform a sound into frames of data in an analysis stage, and then perform modifications on the data before resynthesis with a subtractive filtering process.
Reference: [6] <author> B. Grill, B. Edler, I. Kaneko, Y. Lee, M. Nishiguchi, E. Scheirer, and M. Vnnen, eds, </author> <note> ISO/IEC 14496-3 (MPEG-4 Audio) Final Committee Draft. ISO/IEC JTC1/SC29/WG11 (MPEG) document N2203, Tokyo (1998). </note>
Reference-contexts: 1 INTRODUCTION The MPEG-4 audio standard <ref> [6] </ref>, formally known as ISO/IEC 14496-3, will be completed in October 1998 and published as an International Standard in December 1998. Audio coding in MPEG-4 extends the sound-coding model in previous MPEG standards in two ways.
Reference: [7] <author> Headspace, Inc, </author> <title> Rich Music Format, </title> <note> white paper available at http://beta.choicemall.com/thomasdolby/rmf-whitepaper.html (1996). </note>
Reference-contexts: Formats such as MIDI-DLS [13], Rich Music Format <ref> [7] </ref>, and Creative SoundFonts [15] are used today to transmit music over the Internet, and to communicate between applications such as videogames and sound hardware with wavetable-synthesis capabilities.
Reference: [8] <author> J. Makhoul, </author> <title> Linear prediction: a tutorial review, </title> <journal> Proc. IEEE, </journal> <volume> vol. 63, </volume> <month> pp.561-580 </month> <year> (1975 </year> <month> Mar). </month>
Reference-contexts: Particularly with analysis/synthesis methods, in which a desired sound is analyzed, data-reduced, and streamed as a set of parameters to a particular sound model, sounds may be transmitted with a relatively concise header and a larger amount of streaming data. Techniques such as data reduction through linear prediction <ref> [8] </ref> (which is also the basis of CELP-based speech coding methods [5]) transform a sound into frames of data in an analysis stage, and then perform modifications on the data before resynthesis with a subtractive filtering process.
Reference: [9] <author> M. Mathews and J. Miller, </author> <title> Music IV programmers manual. </title> <institution> Murray Hill:Bell Telephone Laboratories (1963). </institution>
Reference-contexts: Composers, especially in academia, have used languages such as Music-IV <ref> [9] </ref>, Csound [20], and SuperCollider [10] to compose music drawing from a wide range of sonic possibilities.
Reference: [10] <author> J. McCartney, SuperCollider: </author> <title> A new real-time sound synthesis language, </title> <booktitle> Proc. Int. Computer Music Conf., </booktitle> <address> Hong Kong (1996). </address>
Reference-contexts: The remainder of this section discusses the soundsynthesis capabilities of MPEG-4 in more detail. 2.1 Algorithmic synthesis with SAOL SAOL is a powerful and flexible music language of the type known as Music-N languages. Other languages of this type are Csound [20], Nyquist [3] and SuperCollider <ref> [10] </ref>. SAOL is not a method of sound synthesis, but a language for the description of methods of sound synthesis. Any existing or future soundsynthesis algorithm may be described in SAOL. <p> Composers, especially in academia, have used languages such as Music-IV [9], Csound [20], and SuperCollider <ref> [10] </ref> to compose music drawing from a wide range of sonic possibilities. The recent development of using general-purpose software synthesis as a transmission protocol first in the prototype NetSound system [2] and now in MPEG-4 allows these techniques to be brought to bear in a more general audio-coding environment.
Reference: [11] <institution> Microsoft Corp, DirectMusic, </institution> <note> white paper available at http://www.asia.microsoft.com/DirectX/pavilion/future/dmusic.htm (1998). </note>
Reference-contexts: Formats such as MIDI-DLS [13], Rich Music Format [7], and Creative SoundFonts [15] are used today to transmit music over the Internet, and to communicate between applications such as videogames and sound hardware with wavetable-synthesis capabilities. Recent developments in operating-systems design <ref> [11] </ref> embed the ability to transfer data in this manner directly into the audio subsystem of the operating system.
Reference: [12] <author> MIDI Manufacturers Association, </author> <title> The Complete MIDI 1.0 Detailed Specification v.96.2 (1996). </title>
Reference-contexts: In addition to, or instead of, SAOL, a rich wavetable format called SASBF, for Structured Audio Sample Bank Format, may be used to transmit banks of sound-sample data to be used in wavetable, or sampling synthesis. Both of these methods may be controlled with MIDI instructions <ref> [12] </ref> or with a new control format called SASL, for Structured Audio Score Language. The remainder of this section discusses the soundsynthesis capabilities of MPEG-4 in more detail. 2.1 Algorithmic synthesis with SAOL SAOL is a powerful and flexible music language of the type known as Music-N languages.
Reference: [13] <author> MIDI Manufacturers Association, </author> <title> MIDI Downloadable Sounds Level 1 Specification (1998). </title>
Reference-contexts: The original specification was contributed by EMu Systems and was based on their SoundFont format [15]. After integration of this component in the MPEG-4 reference software was complete, the MIDI Manufacturers Association (MMA) approached MPEG requesting that MPEG-4 SASBF be compatible with their Downloaded Sounds format <ref> [13] </ref>. EMu agreed that this compatibility was desirable, and so a new format was negotiated and designed collaboratively by all parties. The resulting format is based on the DLS standard; however, several improvements allow it to have better sound and functionality than a bare-bones DLS implementation. <p> Formats such as MIDI-DLS <ref> [13] </ref>, Rich Music Format [7], and Creative SoundFonts [15] are used today to transmit music over the Internet, and to communicate between applications such as videogames and sound hardware with wavetable-synthesis capabilities.
Reference: [14] <author> S. Quackenbush, </author> <title> Coding of natural audio in MPEG-4, </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <address> Seattle, WA (1998). </address>
Reference-contexts: Audio coding in MPEG-4 extends the sound-coding model in previous MPEG standards in two ways. First, techniques are specified for low-bitrate lossy compression of sound <ref> [14] </ref>; second, MPEG-4 integrates the transmission of natural (compressed) sound and synthetic sound. In the syntheticsound coding part of the standard, called Structured Audio [16], sound is transmitted through one of several sound specification formats, in which sound is not compressed but described in the bitstream. <p> Using AudioBIFS, different parts of a sound scene may be coded in different ways. For example, in a soundtrack which requires speech voice-over with background music, the speech may be coded using a low-bitrate speech coder <ref> [14] </ref> and the music with the Structured Audio coder. These two elements are decoded separately and then mixed in software after they are received, before final presentation to the listener. As well as simple mixing, more sophisticated effects processing is possible with AudioBIFS.
Reference: [15] <author> D. Rossum, </author> <note> The SoundFont 2.0 file format, white paper available at http://www.soundfont.com (1995). </note>
Reference-contexts: The original specification was contributed by EMu Systems and was based on their SoundFont format <ref> [15] </ref>. After integration of this component in the MPEG-4 reference software was complete, the MIDI Manufacturers Association (MMA) approached MPEG requesting that MPEG-4 SASBF be compatible with their Downloaded Sounds format [13]. <p> Formats such as MIDI-DLS [13], Rich Music Format [7], and Creative SoundFonts <ref> [15] </ref> are used today to transmit music over the Internet, and to communicate between applications such as videogames and sound hardware with wavetable-synthesis capabilities. Recent developments in operating-systems design [11] embed the ability to transfer data in this manner directly into the audio subsystem of the operating system.
Reference: [16] <author> E. D. Scheirer, </author> <title> The MPEG-4 Structured Audio standard, </title> <booktitle> Proc. IEEE ICASSP, </booktitle> <address> Seattle, WA (1998). </address>
Reference-contexts: Audio coding in MPEG-4 extends the sound-coding model in previous MPEG standards in two ways. First, techniques are specified for low-bitrate lossy compression of sound [14]; second, MPEG-4 integrates the transmission of natural (compressed) sound and synthetic sound. In the syntheticsound coding part of the standard, called Structured Audio <ref> [16] </ref>, sound is transmitted through one of several sound specification formats, in which sound is not compressed but described in the bitstream. The sound description is processed at the receiver and real-time sound synthesis is used to produce sound. <p> The paper concludes with a discussion on the current state of implementation of the standard. 2 MPEG-4 STRUCTURED AUDIO Other references <ref> [16] </ref>, [18] describe the structure and capabilities of the MPEG-4 standard for Structured Audio in detail; this section provides an overview to lend context to the material that follows.
Reference: [17] <author> E. D. Scheirer, SAOL: </author> <title> The MPEG-4 Structured Audio Orchestra Language, </title> <booktitle> Proc. Int. Comp. Music Conf., </booktitle> <address> Ann Arbor, MI, </address> <year> 1998. </year>
Reference-contexts: The advanced-effects capabilities of MPEG-4 Structured Audio are described in [18]. Synthetic sound is described in MPEG-4 with two main tools. The music language SAOL <ref> [17] </ref>, which stands for Structured Audio Orchestra Language and is pronounced sail, is used to specify sound-processing and soundsynthesis algorithms to be executed in the terminal. It is a robust bitstream specification for algorithmic synthetic sound.
Reference: [18] <author> E. D. Scheirer, </author> <title> Structured Audio and effects processing in the MPEG-4 multimedia standard, </title> <journal> ACM Multimedia Sys. J., </journal> <note> to appear. </note>
Reference-contexts: The paper concludes with a discussion on the current state of implementation of the standard. 2 MPEG-4 STRUCTURED AUDIO Other references [16], <ref> [18] </ref> describe the structure and capabilities of the MPEG-4 standard for Structured Audio in detail; this section provides an overview to lend context to the material that follows. <p> The sound streams are mixed and postprocessed to create sound objects. The sound objects may then be given 3-D spatial locations, attached to visual objects in the scene, and dynamically and interactively moved about the presentation space. The advanced-effects capabilities of MPEG-4 Structured Audio are described in <ref> [18] </ref>. Synthetic sound is described in MPEG-4 with two main tools. The music language SAOL [17], which stands for Structured Audio Orchestra Language and is pronounced sail, is used to specify sound-processing and soundsynthesis algorithms to be executed in the terminal. <p> Further, the exact dispatch of SASBF synthesis is not limited to the simple way that is allowed with MIDI, but more sophisticated layering and control is possible. 2.3 Clientside postproduction with AudioBIFS The AudioBIFS format in MPEG-4 <ref> [18] </ref> enables the clientside mixing and synchronization of sound objects. AudioBIFS is part of the MPEG-4 BIFS (Binary Format for Scene Description) tool, which allows scene-graph based organization of synthetic and streaming content. Using AudioBIFS, different parts of a sound scene may be coded in different ways.
Reference: [19] <author> J. O. Smith, </author> <title> Physical modeling using digital waveguides, </title> <journal> Computer Music J., </journal> <volume> vol. 16 no. 4, </volume> <pages> pp. </pages> <month> 74 91 </month> <year> (1992). </year>
Reference-contexts: On the other hand, SAOL algorithms may be extremely concise and yet highly expressive. High-quality physical modelling synthesis <ref> [19] </ref>, for example, requires very short algorithms, and no samples, to implement. In such a case, periodic rebroadcast of the configuration header is not impossible, and so the tune-in problem is avoided.
Reference: [20] <author> B. L. Vercoe, Csound: </author> <title> A Manual for the Audio Processing System, </title> <publisher> MIT Media Laboratory (1995). </publisher>
Reference-contexts: The remainder of this section discusses the soundsynthesis capabilities of MPEG-4 in more detail. 2.1 Algorithmic synthesis with SAOL SAOL is a powerful and flexible music language of the type known as Music-N languages. Other languages of this type are Csound <ref> [20] </ref>, Nyquist [3] and SuperCollider [10]. SAOL is not a method of sound synthesis, but a language for the description of methods of sound synthesis. Any existing or future soundsynthesis algorithm may be described in SAOL. <p> Composers, especially in academia, have used languages such as Music-IV [9], Csound <ref> [20] </ref>, and SuperCollider [10] to compose music drawing from a wide range of sonic possibilities.
Reference: [21] <author> B. L. Vercoe, </author> <title> Extended Csound, </title> <booktitle> Proc. Int. Computer Music Conf., </booktitle> <address> Hong Kong (1996). </address>
Reference-contexts: At the current time, such technology is slightly beyond the state-of-the-art in audio decoding systems. Recently, Analog Devices has developed a real-time DSP accelerator for Csound processing <ref> [21] </ref>, but this system is not embedded in a streaming context, and the Csound language is somewhat simpler than the MPEG-4 SAOL language.
Reference: [22] <author> B. L. Vercoe, W. G. Gardner, and E. D. </author> <title> Scheirer Structured Audio: Creation, transmission, and rendering of parametric sound representations. </title> <journal> Proc. IEEE, </journal> <volume> vol 86, </volume> <pages> pp. </pages> <month> 922-940 </month> <year> (1998 </year> <month> May). </month>
Reference-contexts: Instead, the achievable expressivity is limited to succinct algorithms with simple control. However, this is not so much a restriction on the sound quality as on the types of sounds <ref> [22] </ref>. Tune-in functionality can be more of a problem with Full Profile bitstreams, because the header (SAOL algorithms) alone may not be enough to allow correct decoding.
References-found: 22

