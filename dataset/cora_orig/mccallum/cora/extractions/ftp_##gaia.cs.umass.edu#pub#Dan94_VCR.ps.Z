URL: ftp://gaia.cs.umass.edu/pub/Dan94:VCR.ps.Z
Refering-URL: http://www-net.cs.umass.edu/papers/papers.html
Root-URL: 
Title: Channel Allocation under Batching and VCR Control in Video-On-Demand Systems  
Author: Asit Dan Perwez Shahabuddin Dinkar Sitaram Don Towsley z 
Keyword: VCR control, Batching, Scheduling, Channel Allocation, Video-On-Demand  
Date: May 1994 (revised Sept. 1994)  
Note: IBM Research Report, RC 19588,  
Address: Yorktown Heights, NY 10598  Amherst, MA 01003  
Affiliation: IBM Research Division, T. J. Watson Research Center  Dept. of Computer Science, University of Massachusetts  
Abstract: In order to to guarantee continuous delivery of a video stream in an on-demand video server environment, a collection of resources (referred to as a logical channel) are reserved in advance. To conserve server resources, multiple client requests for the same video can be batched together and served by a single channel. Increasing the window over which all requests for a particular video are batched results in larger savings in server capacity; however, it also increases the reneging probability of a client. A complication introduced by batching is that if a batched client pauses, a new stream (which may not be immediately available) needs to be started when the client resumes. To provide short response time to resume requests, some channels are set aside and are referred to as contingency channels. To further improve resource utilization, even when a non-batched client pauses, the channel is released and reacquired upon resume. In this paper, we first develop an analytical model that predicts the reneging probability and expected resume delay, and then use this model to optimally allocate channels for batching, on-demand playback, and contingency. The effectiveness of the proposed policy over a scheme with no contingency channels and no batching is also demonstrated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, D. P., </author> <title> "Metascheduling for Continuous Media," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 11, No. 3, </volume> <month> August </month> <year> 1993, </year> <pages> pp. 226-252. </pages>
Reference-contexts: In order to guarantee the continuous delivery of a stream to the client, an admission control policy reserves sufficient resources (both at the network as well as at the server) to deliver a stream prior to initiating the playback of a video <ref> [5, 2, 1, 15] </ref>. The collection of resources required to deliver a data stream is referred to as a logical channel in this paper. <p> If two clients make a request for the same video separated by a small time interval, then by delaying the playback for the first client, the same server stream can be used to satisfy both the client requests <ref> [5, 1] </ref>, i.e., the same logical channel will be shared by both the clients (see Figure 1).
Reference: [2] <author> Dan, A., and D. Sitaram, </author> <title> "Buffer Management Policy for an On-Demand Video Server", IBM Research Report, RC 19347, </title> <address> Yorktown Heights, NY, </address> <year> 1994. </year>
Reference-contexts: In order to guarantee the continuous delivery of a stream to the client, an admission control policy reserves sufficient resources (both at the network as well as at the server) to deliver a stream prior to initiating the playback of a video <ref> [5, 2, 1, 15] </ref>. The collection of resources required to deliver a data stream is referred to as a logical channel in this paper. <p> The channels associated with these videos are referred to as dedicated channels. Access to the videos is non-uniform, i.e., some videos are more popular than others <ref> [2] </ref>. We will refer to the videos with dedicated channels as hot videos, and the remainder as cold videos. <p> On the other hand, if the video can be served from the buffer <ref> [3, 2] </ref> then a different stream type is formed that requires buffer space but no disk bandwidth. Under an integrated resource management scheme, a paused client that was being served by a particular type of stream may be served by a different type of stream upon resume. <p> Also, under an Interval Caching scheme that caches only the interval between two successive streams, a resumed stream may fall in an ongoing cached interval and hence, can be served from the buffer <ref> [3, 2] </ref>. Based on the sharing characteristics, resources can be grouped into shareable resource classes such that, within a class, resources are interchangeable. In the figure, each box represents a shareable resource class, such as disks, CPU, network, buffer, etc..
Reference: [3] <author> Dan, A., and D. Sitaram, </author> <title> "Buffer Management Policy for an On-Demand Video Server", </title> <type> U.S. </type> <note> Docket No YO994004, 1994 (patent pending). </note>
Reference-contexts: On the other hand, if the video can be served from the buffer <ref> [3, 2] </ref> then a different stream type is formed that requires buffer space but no disk bandwidth. Under an integrated resource management scheme, a paused client that was being served by a particular type of stream may be served by a different type of stream upon resume. <p> For example, if a paused client that was being served by a batched stream resumes after a short delay, it can be served by using a small buffer <ref> [3] </ref>. Also, under an Interval Caching scheme that caches only the interval between two successive streams, a resumed stream may fall in an ongoing cached interval and hence, can be served from the buffer [3, 2]. <p> Also, under an Interval Caching scheme that caches only the interval between two successive streams, a resumed stream may fall in an ongoing cached interval and hence, can be served from the buffer <ref> [3, 2] </ref>. Based on the sharing characteristics, resources can be grouped into shareable resource classes such that, within a class, resources are interchangeable. In the figure, each box represents a shareable resource class, such as disks, CPU, network, buffer, etc..
Reference: [4] <author> Dan, A., D. Sitaram, and P. Shahabuddin, </author> <title> "Scheduling Policies with Grouping for providing VCR Control Functions in a Multi-media Server", </title> <type> U.S. </type> <note> Docket No YO994030, 1994 (patent pending). </note>
Reference-contexts: Unlike the initial starting of the playback stream of a video, the resume request cannot be delayed for a long time. Therefore, it conflicts with the goal of batching multiple requests. In <ref> [4] </ref>, it is proposed that a few channels should be set aside (referred to as contingency channels) for the purpose of immediately resuming a stream. <p> However, if the pause duration is large, this may result in significant wastage of server resources. Therefore, it is preferable that the paused client return the resources to the server and reacquire them from the contingency pool upon resume <ref> [4] </ref>. <p> However, this can result is in significant wastage of server resources, particularly if the clients pause on an average for a long duration. Alternatively, the sole clients may release their channels and reacquire different channels upon resumption of playback <ref> [4] </ref>. The delay between the receipt of a resume request from a client and playback needs to be small in order to assure client satisfaction. One measure of client satisfaction is the probability that the resume delay is less than certain duration, say, T resume . <p> One measure of client satisfaction is the probability that the resume delay is less than certain duration, say, T resume . As an example, a system may be configured to guarantee that 95% of the resume delays are less than 30 seconds. A scheme is proposed in <ref> [4] </ref> where some channels are set aside (referred to as the contingency channels) for the purpose of achieving client satisfaction. This is illustrated in Figure 2. The admission control and scheduling policy determines when and which waiting requests to accept. <p> Under a dynamic policy, the number of contingency channels that need to be maintained will depend on the current state of the system. It is a function of the number of viewing clients, the number of active channels, and the number of paused clients <ref> [4] </ref>. Hence, the policy controls the size of the contingency pool to cope with not only the changes in the workload in terms of video access pattern, but also in terms of the surges in VCR-control activity.
Reference: [5] <author> Dan, A., D. Sitaram, and P. Shahabuddin, </author> <title> "Scheduling Policies for an On-Demand Video Server with Batching", </title> <booktitle> Second Annual ACM MUltimedia Conference and Exposition, </booktitle> <address> San Fransisco, CA, </address> <month> October, </month> <year> 1994. </year>
Reference-contexts: In order to guarantee the continuous delivery of a stream to the client, an admission control policy reserves sufficient resources (both at the network as well as at the server) to deliver a stream prior to initiating the playback of a video <ref> [5, 2, 1, 15] </ref>. The collection of resources required to deliver a data stream is referred to as a logical channel in this paper. <p> If two clients make a request for the same video separated by a small time interval, then by delaying the playback for the first client, the same server stream can be used to satisfy both the client requests <ref> [5, 1] </ref>, i.e., the same logical channel will be shared by both the clients (see Figure 1). <p> In general, batching multiple requests for the same video from different clients, and using a single data stream from the video server to multicast [14] the video to all batched clients, 1 can significanty reduce the number of channels required by a large scale video server <ref> [5, 6] </ref>. A simple batching policy is one that plays popular (hot) videos at regular intervals. <p> A simple batching policy is one that plays popular (hot) videos at regular intervals. However, because a client may withdraw (renege) his/her requests if the delay is significant <ref> [5] </ref>, the batching window for the hottest videos are made shorter than those of the less hot videos in order to reduce the overall probability of reneging. <p> The remaining videos share a common pool of channels, referred to as on-demand channels. If the probability of multiple requests for a video within a short time is small, then very little is gained in 2 In <ref> [5, 6] </ref> alternative models of client reneging behavior are studied where client reneging behavior is influenced by maximum waiting time guarantee. 2 delaying the playback of that video. <p> However, if channels are not pre-allocated to the popular videos, then batching can also be used for the requests sharing the common pool. In <ref> [5, 6] </ref>, various scheduling policies are studied that select videos for batching based on various objectives (e.g., minimizing reneging probability, fairness, etc.). <p> The scheduling policy determines which request (s) to serve on an available channel so as to maximize certain performance objectives <ref> [5, 6] </ref>. Multiple requests for the same video can be batched as described in [5, 6]. After receiving a pause request from a client, the admission control and scheduling policy determines if it is the sole viewer for this stream. <p> The scheduling policy determines which request (s) to serve on an available channel so as to maximize certain performance objectives <ref> [5, 6] </ref>. Multiple requests for the same video can be batched as described in [5, 6]. After receiving a pause request from a client, the admission control and scheduling policy determines if it is the sole viewer for this stream. If so, the channel is freed and returned to the contingency pool when enough contingency channels are not available in the system. <p> Relative frequencies, f k ; k = 1 : : : K, for the video k follow a Zipf distribution, f k = c=k (1) with the parameter and normalization constant c. The parameter has been taken from the earlier work on scheduling of video-on-demand systems as 0.271 (see <ref> [5] </ref>). We assume a video length, S k , of 120 minutes for all k. In line with the expected demand, we model a large server with 6000 simultaneously active clients [9]. Hence, the total arrival rates of the videos, tot , is taken to be 50 per minute. <p> As was shown in Figure 4, for the case with contingency, the required number of channels is (almost) insensitive to increase in the the mean number of pauses. 4.2 Effect of Batching In <ref> [5] </ref>, it was found that batching reduces the total required number of channels in systems which do not supply VCR control. In this study we further improve this technique by optimizing over the batching intervals.
Reference: [6] <author> Dan, A., D. Sitaram, and P. Shahabuddin, </author> <title> "Dynamic Batching Policies for an On-Demand Video Server," </title> <note> ACM Multimedia Systems (to appear). </note>
Reference-contexts: In general, batching multiple requests for the same video from different clients, and using a single data stream from the video server to multicast [14] the video to all batched clients, 1 can significanty reduce the number of channels required by a large scale video server <ref> [5, 6] </ref>. A simple batching policy is one that plays popular (hot) videos at regular intervals. <p> The remaining videos share a common pool of channels, referred to as on-demand channels. If the probability of multiple requests for a video within a short time is small, then very little is gained in 2 In <ref> [5, 6] </ref> alternative models of client reneging behavior are studied where client reneging behavior is influenced by maximum waiting time guarantee. 2 delaying the playback of that video. <p> However, if channels are not pre-allocated to the popular videos, then batching can also be used for the requests sharing the common pool. In <ref> [5, 6] </ref>, various scheduling policies are studied that select videos for batching based on various objectives (e.g., minimizing reneging probability, fairness, etc.). <p> The scheduling policy determines which request (s) to serve on an available channel so as to maximize certain performance objectives <ref> [5, 6] </ref>. Multiple requests for the same video can be batched as described in [5, 6]. After receiving a pause request from a client, the admission control and scheduling policy determines if it is the sole viewer for this stream. <p> The scheduling policy determines which request (s) to serve on an available channel so as to maximize certain performance objectives <ref> [5, 6] </ref>. Multiple requests for the same video can be batched as described in [5, 6]. After receiving a pause request from a client, the admission control and scheduling policy determines if it is the sole viewer for this stream. If so, the channel is freed and returned to the contingency pool when enough contingency channels are not available in the system. <p> We assume that there is no batching of requests for the cold videos since it is a reasonable policy as well as it simplifies our analysis. Analytical Markov modelling methods for cold videos with dynamic batching were discussed in <ref> [6] </ref>. In dynamic batching the requests to be batched (i.e., the video to be shown on an available channel) are selected based on some scheduling policy like FCFS, Maximum Queue Length, etc..
Reference: [7] <author> Dan, A., M. Kienzle and D. Sitaram, </author> <title> "Dynamic Segment Replication Policy for Load-Balancing in Video-on-Demand Servers", IBM Research Report, RC 19589, </title> <address> Yorktown Heights, NY, </address> <year> 1994. </year>
Reference-contexts: For example, to deliver data 3 Even when multiple striping groups are used, and all video files are not replicated in all striping groups, the proposed dynamic segment replication policy in <ref> [7] </ref> can be used to provide the appearance of a single shared resource pool of disk bandwidth. 15 from certain video files to a set of viewers, appropriate bandwidth needs to be reserved on the disks on which the video files are stored, on the CPU to which the disks are
Reference: [8] <author> Dykeman, H. D., M. H. Ammar, and J. W. Wong, </author> <title> "Scheduling Algorithms for Videotex Systems under Broadcast Delivery", </title> <booktitle> Proc ICC'86, </booktitle> <year> 1986, </year> <pages> pp. 1847-1851. </pages>
Reference-contexts: Therefore, it is preferable that the paused client return the resources to the server and reacquire them from the contingency pool upon resume [4]. A second important objective of this 1 The concept of batching is also used in videotex system <ref> [8, 18] </ref> where a single page is broadcasted to multiple requesters. 1 paper is to determine the number of channels that should be set aside such that a resume stream can be started within a small time delay with a very high probability (say 0:95 or 0:99).
Reference: [9] <institution> Electronic Engineering Times, </institution> <month> March 15, </month> <year> 1993, </year> <pages> pp 72. </pages>
Reference-contexts: The parameter has been taken from the earlier work on scheduling of video-on-demand systems as 0.271 (see [5]). We assume a video length, S k , of 120 minutes for all k. In line with the expected demand, we model a large server with 6000 simultaneously active clients <ref> [9] </ref>. Hence, the total arrival rates of the videos, tot , is taken to be 50 per minute. As defined in the previous section, the reneging function is defined using two parameters, n k and L k .
Reference: [10] <author> Fox, B., </author> <title> "Discrete Optimization via Marginal Analysis," </title> <journal> Management Science, </journal> <volume> Vol. 13, </volume> <month> November </month> <year> 1966, </year> <pages> pp. 210-216. </pages>
Reference-contexts: Let OBJ (c h ) be the optimal value of the objective function. Note that because the objective function is concave, OBJ (c h ) will also be concave in c h . To solve P 0 (M ) we use a version of Fox's algorithm <ref> [10, 12] </ref>.
Reference: [11] <author> Fox, E. A., </author> <title> "The Coming Revolution in Interactive Digital Video," </title> <journal> Communication of the ACM, </journal> <volume> Vol. 7, </volume> <month> July </month> <year> 1989, </year> <pages> pp. 794-801. </pages>
Reference-contexts: 1 Introduction In a distributed video-on-demand environment, requests from clients for videos of their choice randomly arrive at the video servers at arbitrary points in time <ref> [11, 16, 15] </ref>(see Figure 1). Each request can be satisfied by a new data stream on that video.
Reference: [12] <author> Ibaraki, T., N. Katoh, </author> <title> Resource Allocation Problems, </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Let OBJ (c h ) be the optimal value of the objective function. Note that because the objective function is concave, OBJ (c h ) will also be concave in c h . To solve P 0 (M ) we use a version of Fox's algorithm <ref> [10, 12] </ref>.
Reference: [13] <author> Kleinrock, L., </author> <title> Queueing Systems, Volume 1: Theory, </title> <publisher> John Wiley and Sons New York, </publisher> <address> Chichester, Bris-bane, Toronto, </address> <year> 1975, </year> <pages> pp 105. </pages>
Reference-contexts: Then g k (y 1 ) = R 1 where F W (t; ; ; c) is the waiting time distribution function for a M=M=c queue with arrival rate and service rate . F W (t; ; ; c) is given by (see <ref> [13] </ref>) F W (t; ; ; c) = 1 c!(c =) Here p 0 (; ; c) is the probability that the system is idle and is given by p 0 (; ; c) = n=0 n! n + c!(c =) 7 Using the reneging function described before we can compute
Reference: [14] <author> Le Boudec, J.-Y., </author> <title> "The Asynchronous Transfer Mode: A Tutorial," </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> Vol. 24, </volume> <year> 1992, </year> <pages> pp. 279-309. </pages>
Reference-contexts: In general, batching multiple requests for the same video from different clients, and using a single data stream from the video server to multicast <ref> [14] </ref> the video to all batched clients, 1 can significanty reduce the number of channels required by a large scale video server [5, 6]. A simple batching policy is one that plays popular (hot) videos at regular intervals.
Reference: [15] <author> Rangan, P. V., H. M. Vin, and S. Ramanathan, </author> <title> "Designing an On-Demand Multimedia Service," </title> <journal> IEEE Communication Magazine, </journal> <volume> Vol. 30, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 56-65. </pages>
Reference-contexts: 1 Introduction In a distributed video-on-demand environment, requests from clients for videos of their choice randomly arrive at the video servers at arbitrary points in time <ref> [11, 16, 15] </ref>(see Figure 1). Each request can be satisfied by a new data stream on that video. <p> In order to guarantee the continuous delivery of a stream to the client, an admission control policy reserves sufficient resources (both at the network as well as at the server) to deliver a stream prior to initiating the playback of a video <ref> [5, 2, 1, 15] </ref>. The collection of resources required to deliver a data stream is referred to as a logical channel in this paper.
Reference: [16] <author> Sincoskie, W. D., </author> <title> "System Architecture for a Large Scale Video On Demand Service," </title> <journal> Computer Networks and ISDN System, </journal> <volume> Vol. 22, </volume> <year> 1991, </year> <pages> pp. 155-162. </pages>
Reference-contexts: 1 Introduction In a distributed video-on-demand environment, requests from clients for videos of their choice randomly arrive at the video servers at arbitrary points in time <ref> [11, 16, 15] </ref>(see Figure 1). Each request can be satisfied by a new data stream on that video.
Reference: [17] <author> Video Store Magazine, </author> <month> Dec. 13, </month> <year> 1992. </year>
Reference: [18] <author> Wong, J. W. and M. H. Ammar, </author> <title> "Analysis of Broadcast Delivery in a Videotex System", </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. 34, No. 9, </volume> <month> September, </month> <year> 1985, </year> <pages> pp. </pages> <month> 863-866. </month> <title> 17 video server the cold videos as a function of the number of servers 18 number of pauses = 1) channels in contigency queue (total number of conti-gency channels is 1819) videos (mean number of pauses = 0) 19 capacity 20 capacity resources 21 </title>
Reference-contexts: Therefore, it is preferable that the paused client return the resources to the server and reacquire them from the contingency pool upon resume [4]. A second important objective of this 1 The concept of batching is also used in videotex system <ref> [8, 18] </ref> where a single page is broadcasted to multiple requesters. 1 paper is to determine the number of channels that should be set aside such that a resume stream can be started within a small time delay with a very high probability (say 0:95 or 0:99).
References-found: 18

