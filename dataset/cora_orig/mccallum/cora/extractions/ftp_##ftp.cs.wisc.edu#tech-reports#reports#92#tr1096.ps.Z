URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/92/tr1096.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors  
Author: Mark D. Hill, James R. Larus, Steven K. Reinhardt, David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in: "Proceedings of Fifth International Conference on Architectural Support for Program--ming Languages and Operating Systems (ASPLOS V)," October 1992. Reprinted by permission of ACM.  
Abstract: We believe the absence of massively-parallel, shared-memory machines follows from the lack of a shared-memory programming performance model that can inform programmers of the cost of operations (so they can avoid expensive ones) and can tell hardware designers which cases are common (so they can build simple hardware to optimize them). Cooperative shared memory, our approach to shared-memory design, addresses this problem. Our initial implementation of cooperative shared memory uses a simple programming model, called Check-In / Check-Out (CICO), in conjunction with even simpler hardware, called Dir 1 SW. In CICO, programs bracket uses of shared data with a checkout annotation marking the expected first use and a check-in annotation terminating the expected use of the data. A cooperative prefetch annotation helps hide communication latency. Dir 1 SW is a minimal directory protocol that adds little complexity to message-passing hardware, but efficiently supports programs written within the CICO model. fl This work is supported in part by NSF Presidential Young Investigator Awards CCR-9157366 and MIPS-8957278, NSF Grant CCR-9101035, a University of Wisconsin Graduate School Grant, a Wisconsin Alumni Research Foundation Fellowship and by donations from A.T.&T. Bell Laboratories, Cray Research Foundation and Digital Equipment Corporation. Our department's Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the University of Wisconsin Graduate School. c fl 1992 ACM. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and that notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve, Vikram S. Adve, Mark D. Hill, and Mary K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298-308, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Software coherence schemes invalidate far more data than dynamically necessary for two reasons not shared by CICO <ref> [1] </ref>. First, correctness requires invalidates along all possible execution paths|even those that will not occur dynamically.
Reference: [2] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [4] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols <ref> [2, 14, 20] </ref>. These protocols are complex because hardware must correctly handle many transient states and race conditions. <p> The software trap handlers will read directory entries from the hardware and send explicit messages to other processors to complete the request that trapped and to continue the program running 1 We derived the name Dir 1 SW by extending the directory protocol taxonomy of Agarwal, et al. <ref> [2] </ref>. They use Dir i B and Dir i NB to stand for directories with i pointers that do or do not use broadcast. The SW in Dir 1 SW stands for our S oftW are trap handlers. on their processor. <p> CICO's hierarchy of performance models has similar goals to Hill and Larus's models for programmers of multis [16]. CICO's models, however, provide richer options for reasoning about relinquishing data and initiating prefetches. Many researchers have investigated using directory protocols for hardware cache coherence in large-scale shared-memory systems <ref> [2] </ref>. Stanford DASH [20] connects n clusters (n 64) with a mesh and the processors within a cluster with a bus. It maintains coherence with a Dir n NB protocol between clusters and snooping within a cluster.
Reference: [3] <author> James Archibald and Jean-Loup Baer. </author> <title> An Economical Solution to the Cache Coherence Problem. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 355-362, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Dir 1 SW expects the readers to check-in the block and traps to software if this does not happen. We are aware of two other efforts to reduce directory complexity. Archibald and Baer <ref> [3] </ref> propose a directory scheme that uses four states and no pointers. As mentioned above Alewife [6] uses hardware with four pointers and traps to handle additional readers. Both are more complex than Dir 1 SW, because both must process multiple messages in hardware.
Reference: [4] <author> C. Gordon Bell. Multis: </author> <title> A New Class of Multiprocessor Computers. </title> <journal> Science, </journal> <volume> 228 </volume> <pages> 462-466, </pages> <year> 1985. </year>
Reference-contexts: For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference [20]. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis <ref> [4] </ref> are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols [2, 14, 20]. These protocols are complex because hardware must correctly handle many transient states and race conditions.
Reference: [5] <author> David Callahan, Ken Kennedy, and Allan Poterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Archibald and Baer must send messages to all processors to find two or more readers, while Alewife hardware uses multiple messages with 1-4 readers. Dir 1 SW's trapping mechanism was inspired by Alewife's. Dir 1 SW supports software-initiated prefetches that leave prefetched data in a cache. Like <ref> [5, 12] </ref> we do not prefetch into registers, so data prefetched early does not become incoherent. Dir 1 SW's cooperative prefetch support also reduces the chance that data is prefetched too early since a prefetch remains pending until a block is checked-in.
Reference: [6] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: No cases require multiple messages (beyond a single request-response pair) or transient states. Programs not conforming to the CICO model run correctly, but trap to system software that performs more complex operations (in a manner similar to MIT Alewife <ref> [6] </ref>). Measurements of programs from the SPLASH benchmark suite [25] illustrate the effectiveness of the CICO model (Section 4). Finally, Section 5 discusses related work. 2 CICO Programming Performance Model The performance component of our shared-memory programming model is named Check-In / CheckOut (CICO). It serves two roles. <p> Several state transitions in Table 1 set a trap bit and trap to a software trap handler running on the directory processor (not the requesting processor), as in MIT Alewife <ref> [6] </ref>. The trap bit serializes traps on the same block. <p> These interactions make verification extremely difficult, even for this simple bus-based protocol. Furthermore, most directory protocols, such as Stanford DASH's [20] and MIT Alewife's <ref> [6] </ref>, are far more complex than any bus-based protocol. They require hardware to support transitions involving n nodes and 2n messages, where n ranges from 4 to the number of nodes or clusters. By contrast, the base Dir 1 SW protocol (without prefetching) is simpler than most bus-based cache-coherence protocols. <p> Programs that cannot substantially eliminate these traps will not scale on Dir 1 SW hardware. One solution is to extend the hardware to Dir i SW, which maintains up to i pointers to shared copies. Dir i SW traps on requests for more than i shared copies (like Alewife <ref> [6] </ref>) and when a check out X request encounters any shared copies (unlike Alewife, which sometimes handles this transition in hardware). Like Dir 1 SW (and unlike Alewife), Dir i SW never sends more than a single request/response pair, since software handles all cases requiring multiple messages. <p> IEEE Scalable Coherent Interface (SCI) [13] allows an arbitrary interconnection network between n nodes (n &lt; 64K). It implements a Dir n NB protocol with a linked-list whose head is stored in the directory and other list elements are associated with blocks in processor caches. MIT Alewife <ref> [6] </ref> connects multithreaded nodes with a mesh and maintains coherence with a LimitLESS directory that has four pointers in hardware and supports additional pointers by trapping. Dir 1 SW shares many goals with these coherence protocols. Like all four protocols, Dir 1 SW interleaves the directory with main memory. <p> Dir 1 SW expects the readers to check-in the block and traps to software if this does not happen. We are aware of two other efforts to reduce directory complexity. Archibald and Baer [3] propose a directory scheme that uses four states and no pointers. As mentioned above Alewife <ref> [6] </ref> uses hardware with four pointers and traps to handle additional readers. Both are more complex than Dir 1 SW, because both must process multiple messages in hardware.
Reference: [7] <author> J. Cheong and A.V. Veidenbaum. </author> <title> A Cache Coherence Scheme With Fast Selective Invalidation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 299-307, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Unlike pure simulation, our system executes programs and collects statistics at close-to-hardware speeds. The current, untuned system executes the almost 80 billion instructions in the applications in several hours. 5 Related Work Inserting CICO annotations is superficially similar to inserting coherence primitives in software cache-coherent systems <ref> [9, 7, 24] </ref>. Software coherence schemes invalidate far more data than dynamically necessary for two reasons not shared by CICO [1]. First, correctness requires invalidates along all possible execution paths|even those that will not occur dynamically.
Reference: [8] <author> David R. Cheriton, Hendrick A. Goosen, and Patrick D. Boyle. </author> <title> Paradigm: A Highly Scalable Shared-Memory Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(2) </volume> <pages> 33-46, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Stanford DASH [20] connects n clusters (n 64) with a mesh and the processors within a cluster with a bus. It maintains coherence with a Dir n NB protocol between clusters and snooping within a cluster. Each multiprocessor in Stanford Paradigm <ref> [8] </ref> connects n clusters (n 13) with a bus and uses a two-level bus hierarchy within a cluster. It uses a Dir n NB protocol between clusters and a similar protocol within each cluster.
Reference: [9] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages 229-238, </pages> <month> Aug 188. </month>
Reference-contexts: Unlike pure simulation, our system executes programs and collects statistics at close-to-hardware speeds. The current, untuned system executes the almost 80 billion instructions in the applications in several hours. 5 Related Work Inserting CICO annotations is superficially similar to inserting coherence primitives in software cache-coherent systems <ref> [9, 7, 24] </ref>. Software coherence schemes invalidate far more data than dynamically necessary for two reasons not shared by CICO [1]. First, correctness requires invalidates along all possible execution paths|even those that will not occur dynamically.
Reference: [10] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 257-270, </pages> <year> 1989. </year>
Reference-contexts: For example, multi-word blocks may cause CICO violations due to the well-known problem of false sharing, which occurs when processors unwittingly use different locations in the same block for conflicting purposes <ref> [10] </ref>. Similarly, finite caches cause unintended communication when a cache block is replaced (i.e., implicitly checked-in). Detailed application of CICO must consider the behavior characteristics of real caches [19]. 2.3 Example Consider an example.
Reference: [11] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 64-77, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Dir 1 SW's cooperative prefetch support also reduces the chance that data is prefetched too early since a prefetch remains pending until a block is checked-in. This avoids having the block ping-pong from the prefetcher to the writer and back. Similar, but richer support is provided by QOSB <ref> [11] </ref>, now called QOLB. QOLB allows many prefetchers to join a list, spin locally, and obtain the data when it is released.
Reference: [12] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Archibald and Baer must send messages to all processors to find two or more readers, while Alewife hardware uses multiple messages with 1-4 readers. Dir 1 SW's trapping mechanism was inspired by Alewife's. Dir 1 SW supports software-initiated prefetches that leave prefetched data in a cache. Like <ref> [5, 12] </ref> we do not prefetch into registers, so data prefetched early does not become incoherent. Dir 1 SW's cooperative prefetch support also reduces the chance that data is prefetched too early since a prefetch remains pending until a block is checked-in.
Reference: [13] <author> David B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Each multiprocessor in Stanford Paradigm [8] connects n clusters (n 13) with a bus and uses a two-level bus hierarchy within a cluster. It uses a Dir n NB protocol between clusters and a similar protocol within each cluster. IEEE Scalable Coherent Interface (SCI) <ref> [13] </ref> allows an arbitrary interconnection network between n nodes (n &lt; 64K). It implements a Dir n NB protocol with a linked-list whose head is stored in the directory and other list elements are associated with blocks in processor caches.
Reference: [14] <author> David B. Gustavson and David V. James, </author> <title> editors. SCI: Scalable Coherent Interface: Logical, Physical and Cache Coherence Specifications, </title> <journal> volume P1596/D2.00 18Nov91. IEEE, </journal> <month> November </month> <year> 1991. </year> <note> Draft 2.00 for Recirculation to the Balloting Body. </note>
Reference-contexts: To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [4] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols <ref> [2, 14, 20] </ref>. These protocols are complex because hardware must correctly handle many transient states and race conditions.
Reference: [15] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: RISCs have shown that fast, cost-effective hardware requires hardware designers to identify common cases and cooperate with programmers to find mutually-agreeable models that can be implemented with simple hardware <ref> [15] </ref>. This combination permits hardware designers to devote their attention to making common cases run fast. Message-passing computers, which are based on 5 a simple model, are built from simple, scalable hard-ware.
Reference: [16] <author> Mark D. Hill and James R. Larus. </author> <title> Cache Considerations for Programmers of Multiprocessors. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Indeed, many existing shared-memory programs would perform poorly on massively-parallel systems because the programs were written under the predominate naive model, which assumes all memory references have equal cost. This assumption is wrong because remote references require communication and run slower than local references <ref> [16] </ref>. For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference [20]. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. <p> Dir 1 SW leaves the burden of correctness with hardware, while providing software with the ability to optimize performance. CICO's hierarchy of performance models has similar goals to Hill and Larus's models for programmers of multis <ref> [16] </ref>. CICO's models, however, provide richer options for reasoning about relinquishing data and initiating prefetches. Many researchers have investigated using directory protocols for hardware cache coherence in large-scale shared-memory systems [2].
Reference: [17] <author> Douglas Johnson. </author> <title> Trap Architectures for Lisp Systems. </title> <booktitle> In Proceedings of the 1990 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 79-86, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Shifting the burden of atomically handling multiple-message requests to software dramatically reduces the number of transient hardware states and greatly simplifies the coherence hardware. For programs that trap occasionally, the incurred costs should be small. These costs can be further reduced by microprocessors that efficiently support traps <ref> [17] </ref> or by adopting the approach used in Intel's Paragon computer of handling traps in a companion processor. 3.2 Prefetch Support This section illustrates how Dir 1 SW supports cooperative prefetch, which allows communication to be overlapped with computation.
Reference: [18] <author> Randy H. Katz, Susan J. Eggers, David A. Wood, C.L. Perkins, and R.G. Sheldon. </author> <title> Implementing a Cache Consistency Protocol. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Race conditions, and the myriad of transient states they produce, make most hardware cache-coherence protocols difficult to implement correctly. For example, although Berke-ley SPUR's bus-based Berkeley Ownership coherence protocol <ref> [18] </ref> has only six states, interactions between caches and state machines within a single cache controller produce thousands of transient states [27]. These interactions make verification extremely difficult, even for this simple bus-based protocol.
Reference: [19] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Similarly, finite caches cause unintended communication when a cache block is replaced (i.e., implicitly checked-in). Detailed application of CICO must consider the behavior characteristics of real caches <ref> [19] </ref>. 2.3 Example Consider an example. Figure 1a outlines a stencil algorithm that sets each element of a two-dimensional array to the average of its nearest neighbors. Figure 1b shows the same program with CICO annotations.
Reference: [20] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This assumption is wrong because remote references require communication and run slower than local references [16]. For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference <ref> [20] </ref>. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [4] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. <p> To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [4] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols <ref> [2, 14, 20] </ref>. These protocols are complex because hardware must correctly handle many transient states and race conditions. <p> An alternative is directory-based cache-coherence protocols [2, 14, 20]. These protocols are complex because hardware must correctly handle many transient states and race conditions. Although this complexity can be managed (as, for example, in the Stanford DASH multiprocessor <ref> [20] </ref>), architects must expend considerable effort designing, building, and testing complex hardware rather than improving the performance of simpler hardware. Nevertheless, shared memory offers many advantages, including a uniform address space and referential transparency. <p> These interactions make verification extremely difficult, even for this simple bus-based protocol. Furthermore, most directory protocols, such as Stanford DASH's <ref> [20] </ref> and MIT Alewife's [6], are far more complex than any bus-based protocol. They require hardware to support transitions involving n nodes and 2n messages, where n ranges from 4 to the number of nodes or clusters. <p> CICO's models, however, provide richer options for reasoning about relinquishing data and initiating prefetches. Many researchers have investigated using directory protocols for hardware cache coherence in large-scale shared-memory systems [2]. Stanford DASH <ref> [20] </ref> connects n clusters (n 64) with a mesh and the processors within a cluster with a bus. It maintains coherence with a Dir n NB protocol between clusters and snooping within a cluster.
Reference: [21] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: ECC, which is calculated on double words, permits a much finer granularity of sharing than does the page-level protection used in shared virtual memory <ref> [21] </ref>. ECC errors and page access violations trap to the Dir 1 SW simulator, which models the Dir 1 SW hardware by sending explicit messages to other processors to obtain cache blocks and maintain coherence.
Reference: [22] <author> Calvin Lin and Lawrence Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II-163-170, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Until now, message-passing computers have dominated this arena. Shared-memory computers are rare and currently lag in both number and speed of processors. Their absence is due, in part, to a widespread belief that neither shared-memory software nor shared-memory hardware is scalable <ref> [22] </ref>. Indeed, many existing shared-memory programs would perform poorly on massively-parallel systems because the programs were written under the predominate naive model, which assumes all memory references have equal cost. This assumption is wrong because remote references require communication and run slower than local references [16].
Reference: [23] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We are investigating whether this support is justified. 3.3 Synchronization Support Mellor-Crummey and Scott's locks and barriers are efficient if a processor can spin on a shared-memory block that is physically local <ref> [23] </ref>.
Reference: [24] <author> Sang Lyul Min and Jean-Loup Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing (Vol. I Architecture), </booktitle> <pages> pages I-23-32, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Unlike pure simulation, our system executes programs and collects statistics at close-to-hardware speeds. The current, untuned system executes the almost 80 billion instructions in the applications in several hours. 5 Related Work Inserting CICO annotations is superficially similar to inserting coherence primitives in software cache-coherent systems <ref> [9, 7, 24] </ref>. Software coherence schemes invalidate far more data than dynamically necessary for two reasons not shared by CICO [1]. First, correctness requires invalidates along all possible execution paths|even those that will not occur dynamically.
Reference: [25] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: No cases require multiple messages (beyond a single request-response pair) or transient states. Programs not conforming to the CICO model run correctly, but trap to system software that performs more complex operations (in a manner similar to MIT Alewife [6]). Measurements of programs from the SPLASH benchmark suite <ref> [25] </ref> illustrate the effectiveness of the CICO model (Section 4). Finally, Section 5 discusses related work. 2 CICO Programming Performance Model The performance component of our shared-memory programming model is named Check-In / CheckOut (CICO). It serves two roles. <p> This table describes the applications. With the exception of mm and tomcatv , they are SPLASH benchmarks <ref> [25] </ref>. (The SPLASH benchmark ocean is written in Fortran, which does not yet run on WWT.) Column Size lists the original size of each application, in lines of code. The next column lists the number of CICO annotations added to each program. <p> The next column lists the number of CICO annotations added to each program. The final column lists the number of instructions (in millions) executed by each program. billion instructions) and large data sets. 4.1 Applications The SPLASH benchmark suite is a collection of explicitly-parallel, shared-memory applications <ref> [25] </ref>. We added CICO primitives, by hand, to the SPLASH benchmarks (except ocean, which is written in Fortran) and two additional programs (mm, a blocked matrix multiply, and tomcatv , a parallel version of the SPEC benchmark). Table 3 describes the applications. <p> Dave Douglas, Danny Hillis, Roger Lee, and Steve Swartz of TMC provided invaluable advice and assistance in building the Wind Tunnel. Sarita Adve, Jim Goodman, Guri Sohi, and Mary Vernon provided helpful comments and discussions. Singh et al. <ref> [25] </ref> performed an invaluable service by writing and distributing the SPLASH benchmarks. Michael Wolf provided the mm benchmark.
Reference: [26] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Get messages (Msg Get X and Msg Get S) conflict with a pending prefetch and trap. Msg Prefetch X works well for blocks used by one processor at a time, called migratory data by Weber and Gupta <ref> [26] </ref>. It is also straightforward to augment Dir 1 SW to support a single cooperative prefetch of a shared copy|providing the block is idle or checked-out exclusive. It is, however, a much larger change to Dir 1 SW to support in hardware multiple concurrent prefetches of a block.
Reference: [27] <author> David A. Wood, Garth G. Gibson, and Randy H. Katz. </author> <title> Verifying a Multiprocessor Cache Controller Using Random Case Generation. </title> <journal> IEEE Design and Test of Computers, </journal> <volume> 7(4) </volume> <pages> 13-25, </pages> <month> August </month> <year> 1990. </year> <month> 12 </month>
Reference-contexts: For example, although Berke-ley SPUR's bus-based Berkeley Ownership coherence protocol [18] has only six states, interactions between caches and state machines within a single cache controller produce thousands of transient states <ref> [27] </ref>. These interactions make verification extremely difficult, even for this simple bus-based protocol. Furthermore, most directory protocols, such as Stanford DASH's [20] and MIT Alewife's [6], are far more complex than any bus-based protocol.
References-found: 27

