URL: http://www.cs.umd.edu/~aporter/survey.ps
Refering-URL: http://www.cs.umd.edu/~aporter/html/selected_pubs.html
Root-URL: 
Email: faporter,harveyg@cs.umd.edu votta@research.att.com  
Title: A Review of Software Inspections  
Author: Adam Porter Harvey Siy Lawrence Votta 
Date: June 3, 1996  
Address: College Park, Maryland 20742 Naperville, Illinois 60566  
Affiliation: Computer Science Department Software Production Research Department University of Maryland AT&T Bell Laboratories  
Abstract: For two decades, software inspections have proven effective for detecting defects in software. We have reviewed the different ways software inspections are done, created a taxonomy of inspection methods, and examined claims about the cost-effectiveness of different methods. We detect a disturbing pattern in the evaluation of inspection methods. Although there is universal agreement on the effectiveness of software inspection, their economics are uncertain. Our examination of several empirical studies leads us to conclude that the benefits of inspections are often overstated and the costs (especially for large software developments) are understated. Furthermore, some of the most influential studies establishing these costs and benefits are 20 years old now, which leads us to question their relevance to today's software development processes. Extensive work is needed to determine exactly how, why, and when software inspections work, and whether some defect detection techniques might be more cost-effective than others. In this article we ask some questions about measuring effectiveness of software inspections and determining how much they really cost when their effect on the rest of the development process is considered. Finding answers to these questions will enable us to improve the efficiency of software development.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Frank Ackerman, Lynne S. Buchwald, and Frank H. Lewski. </author> <title> Software inspections: An effective verification process. </title> <journal> IEEE Software, </journal> <pages> pages 31-36, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In terms of the amount of effort to fix a defect, inspections are more efficient since they find and fix several defects in one pass as opposed to testing, which tends to find and fix one defect at a time <ref> [1] </ref> . Also, there is no need for the additional step of isolating the source of the defect because inspections look directly at the design document and source code. <p> However, testing is better for finding defects related to execution, timing, traffic, Draft: June 3, 1996 33 transaction rates, and system interactions [43] . So inspections cannot completely replace testing (although some case studies argue that unit testing may be removed) <ref> [1, 49] </ref> . The following two studies compare inspection methods with testing methods. The first is a controlled experiment while the second is a retrospective case study. Comparing the Effectiveness of Software Testing Strategies.
Reference: [2] <author> Victor R. Basili and Harlan D. Mills. </author> <title> Understanding and documenting programs. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-8(3):270-283, </volume> <month> May </month> <year> 1982. </year>
Reference-contexts: Table 1 describes code reading. It uses a small teams; there are multiple sessions; the preparation uses ad hoc techniques; and holding a meeting is optional. Code Reading by Stepwise Abstraction. Code reading by stepwise abstraction <ref> [2] </ref> is a code-reading technique. The inspector decomposes the program into a set of proper subprograms where a proper subprogram is a chunk of code that performs a single function that can be conveniently documented.
Reference: [3] <author> Victor R. Basili and Richard W. Selby. </author> <title> Comparing the effectiveness of software testing strategies. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-13(12):1278-1296, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: The following two studies compare inspection methods with testing methods. The first is a controlled experiment while the second is a retrospective case study. Comparing the Effectiveness of Software Testing Strategies. Basili and Selby <ref> [3] </ref> investigated the effectiveness of 3 program validation techniques: functional (black box) testing, structural (white box) testing, and code reading by stepwise abstraction (described in Section 2.2).
Reference: [4] <author> David B. Bisant and James R. Lyle. </author> <title> A two-person inspection method to improve programming productivity. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1294-1304, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: Draft: June 3, 1996 7 Table 1 describes Fagan's method. It uses a large team of three or more persons; there is one session; the preparation uses ad hoc techniques; and there is a meeting. Two-person Inspections. Bisant and Lyle <ref> [4] </ref> proposed reducing the inspection team to two persons: the author and one reviewer. Table 1 describes Bisant and Lyle's method. It uses a small team of one reviewer; there is one session; the preparation uses ad hoc techniques; and there is a meeting between the sole reviewer and author. <p> The data showed that the number of customer-reported defects dropped by 90% after the reviewers received reading comprehension training, while results of the other two groups of reviewers showed no change. Controlled Experiments. Bisant and Lyle <ref> [4] </ref> ran an experiment using two sets of student projects in a programming language class to study the effects of using a two-person inspection team, with no moderator, on programmer productivity, or time to complete the project. The experiment used a pretest-posttest, control group design.
Reference: [5] <author> Barry Boehm. </author> <title> Verifying and validating software requirements and design specifications. </title> <journal> IEEE Software, </journal> <volume> 1(1) </volume> <pages> 75-88, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: 1 Introduction For twenty years, software inspections have been described as one of the most cost-effective ways to improve the quality of computer software <ref> [5] </ref> .
Reference: [6] <author> Robert N. Britcher. </author> <title> Using inspections to investigate program correctness. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 38-44, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Team No. of Detection Meet Post Size Sessions Method Fagan [15] Large 1 Ad hoc Yes | Bisant Small 1 Ad hoc Yes | Gilb [20] Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection [47] ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher <ref> [6] </ref> Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection [27] Sequential (Comp) (Reconcile) N-fold [44] Small &gt;1 Ad hoc Yes | Parallel Code Small 1 Ad hoc Optional | Reading [33] Table 1: Example Inspection Methods. <p> It uses small teams usually varying between 2-4 persons; there is more than 1 session; sessions are held in parallel with each examining one aspect of the artifact; the preparation uses questionnaires, a form of scenarios; and each session has a meeting. Inspecting for Program Correctness. Britcher <ref> [6] </ref> takes ADR one step further by incorporating correctness arguments into the questionnaires.
Reference: [7] <author> F. O. Buck. </author> <title> Indicators of quality inspections. </title> <type> Technical Report 21.802, </type> <institution> IBM, Kingston, </institution> <address> NY, </address> <month> Sep. </month> <year> 1981. </year>
Reference-contexts: The plot indicates that both the Method and Specification are significant; but Round, Replication, and Order are not. 4.1.5 Indicators of Quality Inspections The number of defects found in an inspection is not an adequate indicator because it is influenced by the quality of the artifact being inspected. Buck <ref> [7] </ref> conducted a study at IBM by Buck [7] to identify a variable, other than the number of defects found, that would differentiate high quality inspections from low quality ones. He collected data from 106 code inspections of a single piece of COBOL source code. <p> Buck <ref> [7] </ref> conducted a study at IBM by Buck [7] to identify a variable, other than the number of defects found, that would differentiate high quality inspections from low quality ones. He collected data from 106 code inspections of a single piece of COBOL source code.
Reference: [8] <author> Marilyn Bush. </author> <title> Improving software quality: The use of formal inspections at the Jet Propulsion Laboratory. </title> <booktitle> In Proceedings of the 12th International Conference on Software Engineering, </booktitle> <pages> pages 196-199, </pages> <year> 1990. </year>
Reference-contexts: Using assumption A1 Doolan [13] calculated that inspecting requirements specifications at Shell Research saved an average of 30 hours of maintenance work for every hour invested in inspections (not including rework). Draft: June 3, 1996 18 Bush <ref> [8] </ref> related the first 21 months of inspection experience at the Jet Propulsion Laboratory. In that time 300 inspections had been conducted over 10 projects.
Reference: [9] <author> David N. Card, Frank E. McGarry, and Gerald T. </author> <title> Page. Evaluating software engineering technologies. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-13(7):845-851, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: With respect to fault detection rate, code reading achieved the highest rate in the third phase and the same rate as the testing techniques in the other two phases. Finally, code reading found more interface faults. Evaluating Software Engineering Technologies. Card, et al. <ref> [9] </ref> describe a study measuring the importance of certain technologies (practices, tools, and techniques) on software productivity and reliability.
Reference: [10] <author> Jarir K. Chaar, Michael J. Halliday, Inderpal S. Bhandari, and Ram Chillarege. </author> <title> In-process evaluation for software inspection and test. </title> <journal> IEEE Trans. on Software Engineering, </journal> 19(11) 1055-1070, Nov. 1993. 
Reference: [11] <author> Alan R. Dennis and Joseph S. Valacich. </author> <title> Computer brainstorms: More heads are better than one. </title> <journal> Journal of Applied Psychology, </journal> <volume> 78(4) </volume> <pages> 531-537, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: It does however takes time and effort to schedule a meeting and recent studies have shown that meetings do not create as much synergy as previously believed [47] . In addition, the problems of improperly held meetings are well-documented <ref> [11, 34] </ref> . <p> Computer support can also mitigate the group-interaction-related problems by allowing meetings to be held in "nominal" fashion, where inspectors do not actually have to meet, but can just place their comments in a central repository which others can read at their convenience and extend <ref> [11] </ref> . The main disadvantage is inadequate technological support. Most computer-aided inspection systems are still in the research labs and not yet ready for industrial use. In addition, some special equipment may be needed for videoconferencing. Collaborative Software Inspection.
Reference: [12] <author> James H. Dobbins. </author> <title> Inspections as an up-front quality technique. </title> <booktitle> In Handbook of Software Quality Assurance, </booktitle> <pages> pages 137-177. </pages> <publisher> Van Nostrand Reinhold, </publisher> <year> 1987. </year>
Reference: [13] <author> E. P. Doolan. </author> <title> Experience with Fagan's inspection method. </title> <journal> Software Practice and Experience, </journal> <volume> 22(2) </volume> <pages> 173-182, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Each hour spent on inspection thus avoids an average of 33 hours of subsequent maintenance effort, assuming a 7.5-hour workday. Using assumption A1 Doolan <ref> [13] </ref> calculated that inspecting requirements specifications at Shell Research saved an average of 30 hours of maintenance work for every hour invested in inspections (not including rework). Draft: June 3, 1996 18 Bush [8] related the first 21 months of inspection experience at the Jet Propulsion Laboratory.
Reference: [14] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Scott A. Vander Wiel, and Lawrence G. Votta. </author> <title> Estimating software fault content before coding. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This is always available, but very inaccurate. * Partial estimation of detection ratio: Statistical methods such as capture-recapture estimation can be used to estimate pre-inspection defect content <ref> [14, 46] </ref> . This method can be used when there are at least two reviewers and they discover some defects in common. Under these conditions this method can be more accurate than the observed detection ratio and is available immediately after every inspection. <p> The average synergy rate is about 4% (the dashed line) for these 13 inspections. are found (during the meeting). Votta made this measurement as part of a study of capture-recapture sampling techniques for estimating the number of defects remaining in a design artifact after inspection <ref> [14] </ref> . Figure 3 displays data showing that synergy is not responsible for inspection effectiveness (it only accounted for 5% of the defects found by inspections). 4.1.2 The Effect of Different Inspection Approaches Inspection approaches are usually evaluated according to the number of defects they find. <p> Also, it is very hard to trace a certain failure in the field to a defect that was missed in the inspection of a certain artifact. One solution is to estimate the pre-inspection defect content using statistical methods. One such method is capture-recapture <ref> [14, 46] </ref> , which is based on the intuitive premise that if reviewers are finding many of the same defects in an inspection, then it is likely that there are few defects to be found in the first place.
Reference: [15] <author> Michael E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: The problems with this are the same as with other meetings: they require more effort and congest schedules as well as suffer from other group-interaction problems. 2.2 Example Inspection Methods Fagan Inspections. In 1976, Fagan <ref> [15] </ref> published an influential paper detailing a software inspection process used at IBM. Basically, it consists of six steps. : 1. Planning. The artifact to be inspected is checked to see whether it meets certain entry criteria. <p> Overview. The author meets with the inspection team. He or she provides background on the artifact, e.g., its purpose and relationship to other artifacts. Draft: June 3, 1996 6 Method Team No. of Detection Meet Post Size Sessions Method Fagan <ref> [15] </ref> Large 1 Ad hoc Yes | Bisant Small 1 Ad hoc Yes | Gilb [20] Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection [47] ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist <p> Also, units of measurement may have differing operational definitions. Fagan <ref> [15] </ref> studied the use of design and code inspections on an IBM operating system component. The data was compared against that for similar components which did not use inspections. The results showed an increase in productivity, attributed to a minimized overall amount of error rework.
Reference: [16] <author> Michael E. Fagan. </author> <title> Advances in software inspections. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-12(7):744-751, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: It should be noted that this data is 20 years old! As explained in assumption A4, the advertised benefits may have diminished over the years as technology, defect prevention methods, and software development skills improved. In a follow-up study, Fagan <ref> [16] </ref> summarized several industrial case studies of inspection performance.
Reference: [17] <author> Priscilla J. </author> <title> Fowler. </title> <journal> In-process inspections of workproducts at AT&T. AT&T Technical Journal, </journal> <volume> 65(2) </volume> <pages> 102-112, </pages> <month> March-April </month> <year> 1986. </year>
Reference-contexts: Overall, they estimated that inspections saved HP $21.4 million dollars in 1993. Fowler <ref> [17] </ref> summarizes the results of several studies on the use of inspections in industry. In one study, a major software organization increased its productivity by 14% from one release to the next after introduction of improved project phasing and tracking mechanisms, including inspections.
Reference: [18] <author> Louis A. Franz and Jonathan C. Shih. </author> <title> Estimating the value of inspections and early testing for software projects. </title> <journal> Hewlett-Packard Journal, </journal> <pages> pages 60-67, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: After inspections were instituted, they began to find defects at the cost of less than one hour per defect. In another case study, inspections of fixes dropped the number of defective fixes to half of what it had been without inspections. Franz and Shih <ref> [18] </ref> report the effects of using inspection on various artifacts of a sales and inventory tracking project at Hewlett-Packard. They calculate that inspections saved a total of 618 hours (after taking into account the 90 hours needed to perform the inspections).
Reference: [19] <author> Daniel P. Freedman and Gerald M. Weinberg. </author> <title> Handbook of Walkthroughs, Inspections, and Technical Reviews. Little, </title> <publisher> Brown and Company, 3rd edition, </publisher> <year> 1982. </year>
Reference: [20] <author> Tom Gilb and Dorothy Graham. </author> <title> Software Inspection. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1993. </year>
Reference-contexts: He or she provides background on the artifact, e.g., its purpose and relationship to other artifacts. Draft: June 3, 1996 6 Method Team No. of Detection Meet Post Size Sessions Method Fagan [15] Large 1 Ad hoc Yes | Bisant Small 1 Ad hoc Yes | Gilb <ref> [20] </ref> Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection [47] ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection [27] Sequential (Comp) (Reconcile) N-fold [44] Small &gt;1 Ad hoc Yes | <p> Table 1 describes Bisant and Lyle's method. It uses a small team of one reviewer; there is one session; the preparation uses ad hoc techniques; and there is a meeting between the sole reviewer and author. Gilb Inspections. Gilb <ref> [20] </ref> inspections are similar to Fagan inspections, but introduces a process brainstorming meeting right after the inspection meeting. This step enables process improvement through studying and discussing the causes of the defects found at the inspection to find positive recommendations for eliminating them in the future.
Reference: [21] <author> Robert B. Grady and Tom Van Slack. </author> <title> Key lessons in achieving widespread inspection use. </title> <journal> IEEE Software, </journal> <pages> pages 46-57, </pages> <month> July </month> <year> 1994. </year> <note> Draft: June 3, 1996 37 </note>
Reference-contexts: Also, the return on investment comparison between inspection and testing might be more accurate if only the savings and costs from critical defects found at inspection were considered. Grady and van Slack <ref> [21] </ref> discuss nearly 20 years of inspection experience at Hewlett-Packard.
Reference: [22] <author> Watts S. Humphrey. </author> <title> Managing the Software Process, chapter 10. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Meetingless Inspections. Many people believe that most defects are identified during the inspection meeting. However, several recent studies have indicated that most defects are actually found during the preparation step [39, 47] . Humphrey <ref> [22] </ref> states that "three-quarters of the errors found in well-run inspections are found during preparation." Votta [47] suggests replacing inspection meetings with depositions, where the author and, optionally, the moderator meet separately with each of the reviewers to get their inspection results. Table 1 describes meetingless inspection. <p> There was no difference in effectiveness between 2-session inspections held in parallel and those held in sequence. But those held in sequence had significantly longer intervals. (See Figure 5.) 4. Meeting gain rates (33%) were higher than in previous, recent studies <ref> [22, 47] </ref> . 4.1.3 Comparing Meetings and Their Alternatives Votta [47] evaluated the importance of meetings in a case study of 13 design inspections at AT&T.
Reference: [23] <author> Daniel Jackson. </author> <title> Aspect: An economical bug-detector. </title> <booktitle> In Proceedings of the 13th International Conference on Software Engineering, </booktitle> <pages> pages 13-22, </pages> <year> 1991. </year>
Reference-contexts: For example, Perry and Evangelist [37] suggest that there are significant savings in finding and repairing interface defects when formal semantic information is added to subprogram interfaces and then the software is analyzed using tools like Inscape [36] , App [42] and Aspect <ref> [23] </ref> . Also, for code, fast machines make extensive unit testing possible which again changes the benefits of inspecting. Finally, several early articles equate machine effort with human effort.
Reference: [24] <author> Philip M. Johnson. </author> <title> An instrumented approach to improving software quality through formal technical review. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 113-122, </pages> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Draft: June 3, 1996 32 FTArm. Johnson <ref> [24] </ref> presents the Formal Technical Asynchronous review method (FTArm) implemented on Collaborated Software Review System (CSRS). CSRS is a software inspection environment whose aim is not to specify inspection policy, but only to automate the support functions required for various inspection methods. FTArm is geared towards asynchronous software inspections.
Reference: [25] <author> S. C. Johnson. </author> <title> A C program checker. In UNIX(TM) Time-Sharing System - UNIX Programmer's Manual. </title> <publisher> Holt, Rinehart and Winston, </publisher> <address> New York, 7th edition, </address> <year> 1982. </year>
Reference-contexts: Software tools can also perform automated detection of simple defects, freeing inspectors to concentrate on major defects. Using such tools required that artifacts are specified with some formal notation, or programming language. For example, a C language-specific inspection tool called ICICLE [29] uses lint <ref> [25] </ref> , to identify C program constructs that may indicate the presence of defects. It also checks the C program against its own rule-based system. Computer support for meetings can reduce the cost of meetings. With videoconferencing, inspectors in different locations can easily meet.
Reference: [26] <author> John C. Kelly, Joseph S. Sherif, and Jonathan Hops. </author> <title> An analysis of defect densities found during software inspections. </title> <journal> Journal of Systems and Software, </journal> <volume> 17 </volume> <pages> 111-117, </pages> <year> 1992. </year>
Reference-contexts: Kelly, et al. <ref> [26] </ref> , report on 203 inspections at the Jet Propulsion Laboratory. They showed that inspections cost about 1.6 hours per defect, from planning, overview, preparation, meeting, root cause analysis, rework, and follow-up. This is less than the 5 to 17 hours required to fix defects found during formal testing.
Reference: [27] <author> John C. Knight and E. Ann Myers. </author> <title> An improved inspection technique. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 51-61, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: hoc Yes | Bisant Small 1 Ad hoc Yes | Gilb [20] Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection [47] ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection <ref> [27] </ref> Sequential (Comp) (Reconcile) N-fold [44] Small &gt;1 Ad hoc Yes | Parallel Code Small 1 Ad hoc Optional | Reading [33] Table 1: Example Inspection Methods. This table compares the example inspection methods based on the inspection taxonomy. 3. Preparation. <p> Table 1 describes Britcher's method. The team size is left unspecified; there are four sessions, which may be held in parallel, with each session examining one aspect of the artifact; the preparation uses scenarios; and each session has a meeting. Phased Inspections. Knight and Myers <ref> [27] </ref> present phased inspections, where the inspection step is divided into several mini-inspections or "phases." Standard inspections check for many types of defects in a single examination. With phased inspections, each phase is conducted by one or more inspectors and is aimed at detecting one class of defects. <p> For the second project, the members of the experimental group were asked to inspect, along with a classmate, each other's design or code. The results showed that the programming speed of the experimental group improved significantly in the second project. Knight and Myers <ref> [27] </ref> carried out an experiment involving 14 graduate students and using a phased inspection with four phases. Each student was involved in exactly one of the phases.
Reference: [28] <author> C. Lafferty. </author> <title> The Subarctic Survival Situation. </title> <address> Synergistics, Plymouth, MI, </address> <year> 1975. </year>
Reference-contexts: The most frequent reason cited was synergy (mentioned by 79% of those polled). Informally, synergy allows a team working together to outperform any individual or subgroup working alone. The Subarctic Survival Situation exercise <ref> [28] </ref> dramatically shows this effect. (groups outperform individuals unless the individual is an arctic survival expert.) If synergy is fundamental to the inspection process, we would expect to see many inspection defects found only by holding a meeting.
Reference: [29] <author> F. MacDonald, J. Miller, A. Brooks, M. Roper, and M. Wood. </author> <title> A review of tool support for software inspection. </title> <type> Technical Report RR-95-181, </type> <institution> University of Strathclyde, </institution> <address> Glasgow, Scotland, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: By automating some parts of the process and providing computer support for others, the inspection process can possibly be made more effective and efficient <ref> [29] </ref> 4 The study concludes with the unsatisfying result that you can always spend more preparation time and find more defects. There is no discussion of what a practical limit may be. Draft: June 3, 1996 31 . <p> Software tools can also perform automated detection of simple defects, freeing inspectors to concentrate on major defects. Using such tools required that artifacts are specified with some formal notation, or programming language. For example, a C language-specific inspection tool called ICICLE <ref> [29] </ref> uses lint [25] , to identify C program constructs that may indicate the presence of defects. It also checks the C program against its own rule-based system. Computer support for meetings can reduce the cost of meetings. With videoconferencing, inspectors in different locations can easily meet.
Reference: [30] <author> K.E. Martersteck and A.E. Spencer. </author> <title> Introduction to the 5ESS(TM) switching system. </title> <journal> AT&T Technical Journal, </journal> <volume> 64(6 part </volume> 2):1305-1314, July-August 1985. 
Reference-contexts: However, this argument is simplistic for example, it doesn't consider the powerfully negative effect inspections have on schedule. We have observed that a typical release of AT&T's 5ESS R fl switch <ref> [30] </ref> ( .5M lines of added and changed code per release on a base of 5M lines) can require roughly 1500 inspections, each with four, five or even more fl This work is supported in part by a National Science Foundation Faculty Early Career Development Award, CCR-9501354. Mr.
Reference: [31] <author> Vahid Mashayekhi, Janet M. Drake, Wei-Tek Tsai, and John Riedl. </author> <title> Distributed, collaborative software inspection. </title> <journal> IEEE Software, </journal> <pages> pages 66-75, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: The main disadvantage is inadequate technological support. Most computer-aided inspection systems are still in the research labs and not yet ready for industrial use. In addition, some special equipment may be needed for videoconferencing. Collaborative Software Inspection. Mashayekhi, et al. <ref> [31] </ref> , discuss a case study on the use of Collaborative Software Inspection (CSI), a software system to support inspections. Computer support is provided for the preparation and meeting steps. CSI assists with online examination of the artifact and recording of inspector comments.
Reference: [32] <author> Patricia McCarthy, Adam Porter, Harvey Siy, and Lawrence G. Votta. </author> <title> An experiment to assess cost-benefits of inspection meetings and their alternatives. </title> <type> Technical Report CS-TR-3520, </type> <institution> University of Maryland, College Park, MD, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Porter et al. <ref> [32] </ref> attempted to to resolve the conflicting results of two earlier industrial case studies.
Reference: [33] <author> Steve McConnell. </author> <title> Code Complete, chapter 24. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Unspecified No | Inspection [47] ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection [27] Sequential (Comp) (Reconcile) N-fold [44] Small &gt;1 Ad hoc Yes | Parallel Code Small 1 Ad hoc Optional | Reading <ref> [33] </ref> Table 1: Example Inspection Methods. This table compares the example inspection methods based on the inspection taxonomy. 3. Preparation. The inspection team independently analyzes the artifact and any supporting documentation and record potential defects. 4. Inspection. <p> Code Reading. Code reading has been proposed as an alternative to formal code inspections <ref> [33] </ref> . In code reading, the inspector simply focuses on reading source code and looking for defects. The author hands out the source listings (1K-10K lines) to two or more inspectors who read the code at a typical rate of 1K lines per day. This is the main step.
Reference: [34] <author> J.F. Nunamaker, Alan R. Dennis, Joseph S. Valacich, Douglas R. Vogel, and Joey F. George. </author> <title> Electronic meeting systems to support group work. </title> <journal> Communications of the ACM, </journal> <volume> 34(7) </volume> <pages> 40-61, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: It does however takes time and effort to schedule a meeting and recent studies have shown that meetings do not create as much synergy as previously believed [47] . In addition, the problems of improperly held meetings are well-documented <ref> [11, 34] </ref> .
Reference: [35] <author> David L. Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> Principles and practices. In Proceedings of the 8th International Conference on Software Engineering, </booktitle> <pages> pages 215-222, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Draft: June 3, 1996 6 Method Team No. of Detection Meet Post Size Sessions Method Fagan [15] Large 1 Ad hoc Yes | Bisant Small 1 Ad hoc Yes | Gilb [20] Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection [47] ADR <ref> [35] </ref> Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection [27] Sequential (Comp) (Reconcile) N-fold [44] Small &gt;1 Ad hoc Yes | Parallel Code Small 1 Ad hoc Optional | Reading [33] Table 1: Example Inspection Methods. <p> Table 1 describes meetingless inspection. It uses many small (one-person) teams; there are multiple sessions (one per reviewer); the preparation technique is left unspecified; and there are no team meetings. Instead, the author meets with each reviewer separately. Active Design Reviews. Parnas and Weiss <ref> [35] </ref> present active design reviews (ADR). The authors believe that in conventional design reviews, reviewers are given too much information to examine, and they must participate in large meetings which allow for limited interaction between reviewers and author. In ADR, the authors provide questionnaires to guide the inspectors. <p> Anecdotal Studies. The cost-effectiveness of a method may be described anecdotally. Parnas and Weiss <ref> [35] </ref> applied ADR on an actual review of the design document for the operational flight program of one of the Navy's aircraft. Case Studies. An implied requirement of inspections is understanding the artifact being reviewed.
Reference: [36] <author> Dewayne E. Perry. </author> <title> The Inscape environment. </title> <booktitle> In Proceedings of the 11th International Conference on Software Engineering, </booktitle> <pages> pages 2-12, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In the intervening 20 years, changes in technology have changed these tradeoffs. For example, Perry and Evangelist [37] suggest that there are significant savings in finding and repairing interface defects when formal semantic information is added to subprogram interfaces and then the software is analyzed using tools like Inscape <ref> [36] </ref> , App [42] and Aspect [23] . Also, for code, fast machines make extensive unit testing possible which again changes the benefits of inspecting. Finally, several early articles equate machine effort with human effort.
Reference: [37] <author> Dewayne E. Perry and W. Michael Evangelist. </author> <title> An empirical study of software interface faults | an update. </title> <booktitle> In Proceedings of the Twentieth Annual Hawaii International Conference on Systems Sciences, </booktitle> <volume> volume II, </volume> <pages> pages 113-126, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: A4. Inspection costs and benefits aren't affected by changing technology. Several early studies of inspections studied the cost and benefits they provide. In the intervening 20 years, changes in technology have changed these tradeoffs. For example, Perry and Evangelist <ref> [37] </ref> suggest that there are significant savings in finding and repairing interface defects when formal semantic information is added to subprogram interfaces and then the software is analyzed using tools like Inscape [36] , App [42] and Aspect [23] .
Reference: [38] <author> Dewayne E. Perry and Carol S. Stieg. </author> <title> Software faults in evolving a large, real-time system: a case study. </title> <booktitle> In 4th European Software Engineering Conference - ESEC93, </booktitle> <pages> pages 48-67, </pages> <month> Sept. </month> <year> 1993. </year> <type> Invited keynote paper. </type>
Reference-contexts: These defects will never be found by testing. In another example, some studies have shown that almost half the defects found in testing are interface defects <ref> [38] </ref> , suggesting that inspections are not effectively finding this class of defects, even though effort is spent looking for them. A3. Each defect found during inspection results in a linear reduction in testing effort.
Reference: [39] <author> Adam Porter, Lawrence G. Votta, and Victor Basili. </author> <title> Comparing detection methods for software requirement inspections: A replicated experiment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Meetingless Inspections. Many people believe that most defects are identified during the inspection meeting. However, several recent studies have indicated that most defects are actually found during the preparation step <ref> [39, 47] </ref> . Humphrey [22] states that "three-quarters of the errors found in well-run inspections are found during preparation." Votta [47] suggests replacing inspection meetings with depositions, where the author and, optionally, the moderator meet separately with each of the reviewers to get their inspection results. <p> Ad Hoc reviewers use nonsystematic techniques and are assigned the same general responsibilities. Checklist reviewers are given a list of items to search for. Checklists embody important lessons learned from previous inspections within a specific environment or domain. Porter et al. <ref> [39] </ref> , hypothesized that an alternative approach which assigned individual reviewers separate and distinct detection responsibilities and provided specialized techniques for meeting them would be more effective. This hypothesis is depicted in Figure 7. <p> They evaluated this hypothesis in a controlled experiment, using a 3 fi 2 4 partial factorial, randomized experimental design <ref> [39] </ref> . Forty-eight graduate students in computer science participated in this experiment. Draft: June 3, 1996 28 before inspection (top) and after an inspection using nonsystematic techniques with general and identical responsibility assignments (bottom left), and an inspection using systematic techniques with specific and distinct responsibility assignments (bottom right).
Reference: [40] <author> Adam A. Porter, Lawrence G. Votta, Harvey P. Siy, and Carol A. Toman. </author> <title> An experiment to assess the cost-benefits of code inspections in large scale software development. </title> <booktitle> In The Third Symposium on the Foundations of Software Engineering, </booktitle> <address> Washington, D.C., </address> <month> Oct. </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: For example, inspections turn up many issues which do not affect the operational behavior of the system. Figure 2 shows that in an industrial case study of more than 100 inspections, 60% of all issues recorded during an inspection meeting fall into this class <ref> [40] </ref> . These defects will never be found by testing. In another example, some studies have shown that almost half the defects found in testing are interface defects [38] , suggesting that inspections are not effectively finding this class of defects, even though effort is spent looking for them. A3. <p> To evaluate these hypotheses they conducted a controlled experiment to compare the tradeoffs between the minimum interval and effort and the maximum effectiveness of several inspection approaches <ref> [40] </ref> . They ran this experiment at AT&T on a project that is developing a compiler and environment to support developers of the AT&T 5ESS telephone switching system. The finished system contained 45 lines of C++ code, of which about 8K is reused. <p> Across all treatments, the median interval is 8.5 working days. This result was striking, but later data seems to contradict it. Porter et al. <ref> [40] </ref> conducted another study, also at AT&T, involving &gt; 100 code inspections.
Reference: [41] <author> Stan Rifkin and Lionel Deimel. </author> <title> Applying program comprehension techniques to improve software inspections. </title> <booktitle> In Proceedings of the 19th Annual NASA Software Engineering Laboratory Workshop, </booktitle> <address> Greenbelt, MD, </address> <month> Nov. </month> <year> 1994. </year> <note> Draft: June 3, 1996 38 </note>
Reference-contexts: Parnas and Weiss [35] applied ADR on an actual review of the design document for the operational flight program of one of the Navy's aircraft. Case Studies. An implied requirement of inspections is understanding the artifact being reviewed. Rifkin and Deimel <ref> [41] </ref> suggest teaching program comprehension techniques during code inspection training classes in order to improve program understanding during preparation and inspection. Using historical data they argued that while inspections reduced the number of defects discovered by testing, they did not significantly decrease the number of customer-identified defects.
Reference: [42] <author> David S. Rosenblum. </author> <title> Towards a method of programming with assertions. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 92-104, </pages> <address> Melbourne, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, Perry and Evangelist [37] suggest that there are significant savings in finding and repairing interface defects when formal semantic information is added to subprogram interfaces and then the software is analyzed using tools like Inscape [36] , App <ref> [42] </ref> and Aspect [23] . Also, for code, fast machines make extensive unit testing possible which again changes the benefits of inspecting. Finally, several early articles equate machine effort with human effort.
Reference: [43] <author> Glen W. Russell. </author> <title> Experience with inspection in ultralarge-scale developments. </title> <journal> IEEE Software, </journal> <pages> pages 25-31, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: This was determined from the usage of the inspection tool, and from the meeting times of the of the phases using more than one inspector. Acknowledging that they cannot make definitive comparisons, Knight and Myers found it interesting to compare their results to Russell <ref> [43] </ref> ; which are also described in Section 3.2. They show that while Russell found 1 defect per hour, the phases found 1.5 to 2.75 defects per hour. Mathematical Modeling. To test the cost-effectiveness of meetingless inspections, Votta [47] collected data from 13 inspections with meetings. <p> Russell <ref> [43] </ref> observed inspections for a two year period at Bell-Northern Research. These inspections found about one defect for every man-hour invested in inspections. He also concluded that each defect found before it reached the customer saved an average of 33 hours of maintenance effort. <p> However, testing is better for finding defects related to execution, timing, traffic, Draft: June 3, 1996 33 transaction rates, and system interactions <ref> [43] </ref> . So inspections cannot completely replace testing (although some case studies argue that unit testing may be removed) [1, 49] . The following two studies compare inspection methods with testing methods. The first is a controlled experiment while the second is a retrospective case study.
Reference: [44] <author> G. Michael Schneider, Johnny Martin, and Wei-Tek Tsai. </author> <title> An experimental study of fault detection in user requirements documents. </title> <journal> ACM Trans. on Software Engineering and Methodology, </journal> <volume> 1(2) </volume> <pages> 188-204, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: 1 Ad hoc Yes | Gilb [20] Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection [47] ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection [27] Sequential (Comp) (Reconcile) N-fold <ref> [44] </ref> Small &gt;1 Ad hoc Yes | Parallel Code Small 1 Ad hoc Optional | Reading [33] Table 1: Example Inspection Methods. This table compares the example inspection methods based on the inspection taxonomy. 3. Preparation. <p> N-fold Inspections. Schneider, et al. <ref> [44] </ref> , developed the N-fold inspection process. This is based on the hypotheses that a single inspection team can find only a fraction of the defects in an artifact and that multiple teams will not significantly duplicate each others efforts. <p> 5-member teams, 3. that effectiveness was also independent of major defects found per hour, 4. that additional preparation resulted in more defects being found. 4 Thus, the study suggests that quality inspections are a result of following a low inspection rate. 4.1.6 Using Multiple Inspection Teams The N-fold inspection method <ref> [44] </ref> is based on the idea that no single inspection team can find all the defects in a software requirements document, that N separate inspection teams do not significantly duplicate each others' efforts, and therefore that N inspections will be significantly more effective than one.
Reference: [45] <author> George Stalk, Jr. and Thomas M. Hout. </author> <title> Competing Against Time: How Time-Based Competition is Reshaping Global Markets. </title> <publisher> The Free Press, </publisher> <year> 1990. </year>
Reference-contexts: This could have serious economic consequences, especially in a highly competitive environment where being the first to introduce a new (even poorly implemented) feature to the market may mean the difference between success and failure of a product <ref> [45] </ref> . Obviously, it would be expensive and impractical to replicate entire development projects to see how removing inspections from the process affects the development interval. Future research will need to find more economical ways to estimate this cost.
Reference: [46] <author> Scott A. Vander Wiel and Lawrence G. Votta. </author> <title> Assessing software design using capture-recapture methods. </title> <journal> IEEE Trans. Software Eng., </journal> <volume> SE-19:1045-1054, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: This is always available, but very inaccurate. * Partial estimation of detection ratio: Statistical methods such as capture-recapture estimation can be used to estimate pre-inspection defect content <ref> [14, 46] </ref> . This method can be used when there are at least two reviewers and they discover some defects in common. Under these conditions this method can be more accurate than the observed detection ratio and is available immediately after every inspection. <p> Also, it is very hard to trace a certain failure in the field to a defect that was missed in the inspection of a certain artifact. One solution is to estimate the pre-inspection defect content using statistical methods. One such method is capture-recapture <ref> [14, 46] </ref> , which is based on the intuitive premise that if reviewers are finding many of the same defects in an inspection, then it is likely that there are few defects to be found in the first place.
Reference: [47] <author> Lawrence G. Votta. </author> <booktitle> Does every inspection need a meeting? In Proceedings of ACM SIGSOFT '93 Symposium on Foundations of Software Engineering, </booktitle> <pages> pages 107-114. </pages> <institution> Association for Computing Machinery, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Finally, meetings provide a natural milestone for the project under development. It does however takes time and effort to schedule a meeting and recent studies have shown that meetings do not create as much synergy as previously believed <ref> [47] </ref> . In addition, the problems of improperly held meetings are well-documented [11, 34] . <p> Draft: June 3, 1996 6 Method Team No. of Detection Meet Post Size Sessions Method Fagan [15] Large 1 Ad hoc Yes | Bisant Small 1 Ad hoc Yes | Gilb [20] Large 1 Checklist Yes Root cause analysis Meetingless Large 1 Unspecified No | Inspection <ref> [47] </ref> ADR [35] Small &gt;1 Scenario Yes | Parallel Britcher [6] Unspecified 4 Scenario Yes | Parallel Phased Small &gt;1 Checklist Yes | Inspection [27] Sequential (Comp) (Reconcile) N-fold [44] Small &gt;1 Ad hoc Yes | Parallel Code Small 1 Ad hoc Optional | Reading [33] Table 1: Example Inspection Methods. <p> Meetingless Inspections. Many people believe that most defects are identified during the inspection meeting. However, several recent studies have indicated that most defects are actually found during the preparation step <ref> [39, 47] </ref> . Humphrey [22] states that "three-quarters of the errors found in well-run inspections are found during preparation." Votta [47] suggests replacing inspection meetings with depositions, where the author and, optionally, the moderator meet separately with each of the reviewers to get their inspection results. <p> Many people believe that most defects are identified during the inspection meeting. However, several recent studies have indicated that most defects are actually found during the preparation step [39, 47] . Humphrey [22] states that "three-quarters of the errors found in well-run inspections are found during preparation." Votta <ref> [47] </ref> suggests replacing inspection meetings with depositions, where the author and, optionally, the moderator meet separately with each of the reviewers to get their inspection results. Table 1 describes meetingless inspection. <p> They show that while Russell found 1 defect per hour, the phases found 1.5 to 2.75 defects per hour. Mathematical Modeling. To test the cost-effectiveness of meetingless inspections, Votta <ref> [47] </ref> collected data from 13 inspections with meetings. <p> There was no difference in effectiveness between 2-session inspections held in parallel and those held in sequence. But those held in sequence had significantly longer intervals. (See Figure 5.) 4. Meeting gain rates (33%) were higher than in previous, recent studies <ref> [22, 47] </ref> . 4.1.3 Comparing Meetings and Their Alternatives Votta [47] evaluated the importance of meetings in a case study of 13 design inspections at AT&T. <p> But those held in sequence had significantly longer intervals. (See Figure 5.) 4. Meeting gain rates (33%) were higher than in previous, recent studies [22, 47] . 4.1.3 Comparing Meetings and Their Alternatives Votta <ref> [47] </ref> evaluated the importance of meetings in a case study of 13 design inspections at AT&T. To quantify the usefulness of inspection meetings, he determined the proportion of defects found during the inspection that were originally discovered at the meeting (the meeting gain rate).
Reference: [48] <author> Lawrence G. Votta. </author> <title> Does every inspection need a meeting? ACM SIGSoft Software Engineering Notes, </title> <booktitle> 18(5) </booktitle> <pages> 107-114, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: In this section we describe several empirical studies that investigate some of these tradeoffs. 4.1.1 Does Every Inspection Need a Meeting? Votta surveyed software developers in AT&T's 5ESS project to find out what factors they believed had the largest influence on inspection effectiveness <ref> [48] </ref> . The most frequent reason cited was synergy (mentioned by 79% of those polled). Informally, synergy allows a team working together to outperform any individual or subgroup working alone.
Reference: [49] <author> Edward F. Weller. </author> <title> Lessons from three years of inspection data. </title> <journal> IEEE Software, </journal> <pages> pages 38-45, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: This is less than the 5 to 17 hours required to fix defects found during formal testing. Although this calculation requires assumption A2, many of the defects found didn't affect the behavior of the software and wouldn't have been caught by testing. Weller <ref> [49] </ref> relates 3 years of inspection experience at Bull HN. In one case study, data at the end of system test showed that inspections found 70% of all defects detected up to that point. <p> However, testing is better for finding defects related to execution, timing, traffic, Draft: June 3, 1996 33 transaction rates, and system interactions [43] . So inspections cannot completely replace testing (although some case studies argue that unit testing may be removed) <ref> [1, 49] </ref> . The following two studies compare inspection methods with testing methods. The first is a controlled experiment while the second is a retrospective case study. Comparing the Effectiveness of Software Testing Strategies.
Reference: [50] <author> Alexander L. Wolf and David S. Rosenblum. </author> <title> A study in software process data capture and analysis. </title> <booktitle> In Proceedings of the Second International Conference on Software Process, </booktitle> <pages> pages 115-124, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: In order to measure inspection interval and its various subintervals, we devised an inspection time model based on visible inspection events <ref> [50] </ref> . Whenever one of these events occurs it is timestamped and the event's participants are recorded. These events occur, for example, when the artifact is ready for inspection, or when a reviewer starts or finishes his or her preparation.
References-found: 50

