URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/icassp94.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/boley/reports/
Root-URL: http://www.cs.umn.edu
Title: Generalized URV Subspace Tracking LMS Algorithm 1  
Author: S. Hosur and A. H. Tewfik and D. Boley 
Address: Minneapolis, MN 55455  
Affiliation: Dept. of Electrical Engineering and Computer Science, University of Minnesota,  
Abstract: The convergence rate of the Least Mean Squares (LMS) algorithm is poor whenever the adaptive filter input auto-correlation matrix is ill-conditioned. In this paper we propose a new LMS algorithm to alleviate this problem. It uses a data dependent signal transformation. The algorithm tracks the subspaces corresponding to clusters of eigenvalues of the auto-correlation matrix of the input to the adaptive filter, which have the same order of magnitude. The algorithm up-dates the projection of the tap weights of the adaptive filter onto each subspace using LMS algorithms with different step sizes. The technique also permits adaptation only in those subspaces, which contain strong signal components leading to a lower excess Mean Squared Error (MSE) as compared to traditional algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. F. Marshall, W. K. Jenkins and J. J. Murphy, </author> <title> "The Use of Orthogonal Transforms for Improving Performance of Adaptive Filters," </title> <journal> IEEE Trans. Circuits and Systems, </journal> <volume> Vol. 36, No. 4, </volume> <pages> pp. 474-483, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: 1 Introduction The LMS adaptive algorithm is the most popular algorithm for adaptive filtering because of its simplicity and robustness. However, its main drawback is slow convergence whenever the adaptive filter input auto-correlation matrix is ill-conditioned i.e. the eigenvalue spread of this matrix is large <ref> [1] </ref>, [2]. A class of adaptive filters known as the transform domain filters have been developed for the purpose of convergence rate improvement [1]. <p> However, its main drawback is slow convergence whenever the adaptive filter input auto-correlation matrix is ill-conditioned i.e. the eigenvalue spread of this matrix is large <ref> [1] </ref>, [2]. A class of adaptive filters known as the transform domain filters have been developed for the purpose of convergence rate improvement [1]. All transform domain adaptive filters try to approximately de-correlate and scale the input to the adaptive filter in the transform domain, in order to obtain an autocorrelation matrix with zero eigen value spread in that domain.
Reference: [2] <author> S. Haykin, </author> <title> Adaptive Filter Theory, 2nd ed., </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction The LMS adaptive algorithm is the most popular algorithm for adaptive filtering because of its simplicity and robustness. However, its main drawback is slow convergence whenever the adaptive filter input auto-correlation matrix is ill-conditioned i.e. the eigenvalue spread of this matrix is large [1], <ref> [2] </ref>. A class of adaptive filters known as the transform domain filters have been developed for the purpose of convergence rate improvement [1].
Reference: [3] <author> C. E. Davila, </author> <title> "Recursive Total Least Squares Algorithm for adaptive filtering," </title> <booktitle> Proceedings of ICASSP-91, ICASSP, </booktitle> <pages> pp 1853-1856, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Simulation results are given in Section 4. Conclusions and future work are presented in the final section. 2 Subspace Tracking Recently, some subspace updating techniques have been suggested in the context of total least squares (TLS) <ref> [3] </ref> - [7]. A Kalman filter was used to update the eigenvector corresponding to the smallest eigenvalue in [3]. However it was not suggested how to modify the algorithm in case of multiple eigenvalues corresponding to noise. [4] discusses a strategy to obtain a fast eigen-decomposition of a covariance matrix. <p> Conclusions and future work are presented in the final section. 2 Subspace Tracking Recently, some subspace updating techniques have been suggested in the context of total least squares (TLS) <ref> [3] </ref> - [7]. A Kalman filter was used to update the eigenvector corresponding to the smallest eigenvalue in [3]. However it was not suggested how to modify the algorithm in case of multiple eigenvalues corresponding to noise. [4] discusses a strategy to obtain a fast eigen-decomposition of a covariance matrix.
Reference: [4] <author> K. B. Yu, </author> <title> "Recursive updating the eigenvalue decomposition of a covariance matrix," </title> <journal> IEEE Trans. Signal Proc., </journal> <volume> Vol. 39, </volume> <pages> pp. 1136-1145, </pages> <year> 1991. </year>
Reference-contexts: A Kalman filter was used to update the eigenvector corresponding to the smallest eigenvalue in [3]. However it was not suggested how to modify the algorithm in case of multiple eigenvalues corresponding to noise. <ref> [4] </ref> discusses a strategy to obtain a fast eigen-decomposition of a covariance matrix. The eigenvalues in the noise subspace are replaced by their average value and the same is done to the signal eigenvalues. <p> In [5], [6], the eigen-problem on the covariance matrix is replaced by the singular problem, thereby reducing the condition numbers to their square roots and increasing numerical accuracy, and using the averaging technique of <ref> [4] </ref>. Again the assumption that the eigenvalues could be grouped into two tight clusters is made. In normal signal scenarios this assumption is generally not valid.
Reference: [5] <author> E. M. Dowling and R. D. DeGroat, </author> <title> "Recursive Total Least Squares Adaptive Filtering," </title> <booktitle> Proceedings of the SPIE Conf. on Adaptive Signal Proc., </booktitle> <volume> vol. 1565, </volume> <booktitle> SPIE, </booktitle> <pages> pp. 35-46, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The eigenvalues in the noise subspace are replaced by their average value and the same is done to the signal eigenvalues. This technique could work well if the exact eigenvalues could be grouped together in two tight clusters. In <ref> [5] </ref>, [6], the eigen-problem on the covariance matrix is replaced by the singular problem, thereby reducing the condition numbers to their square roots and increasing numerical accuracy, and using the averaging technique of [4]. Again the assumption that the eigenvalues could be grouped into two tight clusters is made.
Reference: [6] <author> R. D. DeGroat, </author> <title> "Noniterative subspace tracking," </title> <journal> IEEE Trans. on Sig. Proc., </journal> <volume> Vol. 40, </volume> <pages> pp. 571-577, </pages> <year> 1992. </year>
Reference-contexts: The eigenvalues in the noise subspace are replaced by their average value and the same is done to the signal eigenvalues. This technique could work well if the exact eigenvalues could be grouped together in two tight clusters. In [5], <ref> [6] </ref>, the eigen-problem on the covariance matrix is replaced by the singular problem, thereby reducing the condition numbers to their square roots and increasing numerical accuracy, and using the averaging technique of [4]. Again the assumption that the eigenvalues could be grouped into two tight clusters is made.
Reference: [7] <author> D. L. Boley and K. T. Sutherland, </author> <title> "Recursive Total Least Squares : An Alternative to the Discrete Kalman Filter," </title> <type> Technical Report, </type> <institution> Dept. of Comp. Sci., Univ. of Minn., </institution> <type> TR 93-92, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: Simulation results are given in Section 4. Conclusions and future work are presented in the final section. 2 Subspace Tracking Recently, some subspace updating techniques have been suggested in the context of total least squares (TLS) [3] - <ref> [7] </ref>. A Kalman filter was used to update the eigenvector corresponding to the smallest eigenvalue in [3]. However it was not suggested how to modify the algorithm in case of multiple eigenvalues corresponding to noise. [4] discusses a strategy to obtain a fast eigen-decomposition of a covariance matrix. <p> Each rotation applied from the right is accumulated in V, to maintain A = URV H , where U is not saved. The URV updating procedure used is similar to that used in <ref> [7] </ref> and can be described as follows :- * Absorb a new row : The matrix A is replaced by ( fiA H x ) H i.e a new row is augmented to the matrix A and fi &lt; 1 is the forgetting factor that damps out the effect of previous
Reference: [8] <author> G. W. Stewart, </author> <title> "An Updating Algorithm for Subspace Tracking," </title> <journal> IEEE Trans. Signal Proc., </journal> <volume> vol. 40, No. 6, </volume> <pages> pp. 1535-1541, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The transform should also be able to track the - 1 - signal behavior in a non-stationary environment. We develop such a data adaptive transform domain LMS algorithm, using a generalization of the rank revealing URV decomposition, first introduced by Stewart <ref> [8] </ref>. In the next section, we introduce the rank revealing URV decomposition and the idea of tracking subspaces corresponding to clusters of singular values. Section 3 generalizes the URV decomposition and introduces the idea of subspace domain LMS filtering. Simulation results are given in Section 4. <p> Again the assumption that the eigenvalues could be grouped into two tight clusters is made. In normal signal scenarios this assumption is generally not valid. The URV decomposition was first introduced by Stewart <ref> [8] </ref>, to break the eigenspace of R N , where N is the length of the impulse response of the adaptive filter, into two subspaces, one corresponding to the cluster of largest singular values and the other corresponding to the null subspace.
Reference: [9] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <address> Baltimore MD, </address> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: In essence, it updates the subspaces - 2 - corresponding to the group of large eigenvalues and that of small eigenvalues of the correlation matrix of the input to the adaptive filter. The URV updating algorithm consists of a series of plane (Givens) rotations <ref> [9] </ref>, which are multiplica tions by orthogonal matrices of the form Q = B B I 0 0 0 0 0 0 I 0 0 0 0 0 0 I C C where c 2 + s 2 = 1 and the I's represent identity matrices of appropriate dimensions. <p> The Frobenius norms of F and G and an approximation of the last singular value of R and the corresponding singular vector are estimated using a condition number estimator <ref> [9] </ref>. One can also use any of the many condition number estimators proposed in literature [10]. The URV decomposition is then deflated by one, i.e., transformations are applied to decrease the rank index by one so that the smallest singular value of R is moved into the trailing columns.
Reference: [10] <author> N. J. Higham, </author> <title> "A Survey of Condition Number Estimators For Triangular Matrices," </title> <journal> SIAM Rev. </journal> <volume> No. 29, </volume> <pages> pp. 575-596, </pages> <year> 1987. </year>
Reference-contexts: The Frobenius norms of F and G and an approximation of the last singular value of R and the corresponding singular vector are estimated using a condition number estimator [9]. One can also use any of the many condition number estimators proposed in literature <ref> [10] </ref>. The URV decomposition is then deflated by one, i.e., transformations are applied to decrease the rank index by one so that the smallest singular value of R is moved into the trailing columns. This is done by isolating the smallest singular value of R into its last column.
References-found: 10

