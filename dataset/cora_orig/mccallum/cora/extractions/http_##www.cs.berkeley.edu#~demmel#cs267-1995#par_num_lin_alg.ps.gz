URL: http://www.cs.berkeley.edu/~demmel/cs267-1995/par_num_lin_alg.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~demmel/cs267-1995/reference_material.html
Root-URL: 
Title: Parallel Numerical Linear Algebra  
Author: James W. Demmel Michael T. Heath Henk A. van der Vorst 
Address: Illinois, 405 N. Mathews Ave., Urbana, IL  P.O. Box 80.010, NL-3508 TA Utrecht, the Netherlands.  
Affiliation: Department of Computer Science and National Center for Supercomputing Applications, University of  Mathematical Institute, Utrecht University,  
Note: To appear in Acta Numerica, Cambridge University Press Computer Science Division Tech Report UCB//CSD-92-703, U. C. Berkeley,  61801. The author was supported by DARPA contract DAAL03-91-C-0047 via a subcontract from the University of Tennessee, and administered by ARO.  This work was supported in part by a NCF/Cray Research University Grant CRG 92.03.  
Date: November 4, 1993  October 1992  
Abstract: We survey general techniques and open problems in numerical linear algebra on parallel architectures. We first discuss basic principles of parallel processing, describing the costs of basic operations on parallel machines, including general principles for constructing efficient algorithms. We illustrate these principles using current architectures and software systems, and by showing how one would implement matrix multiplication. Then, we present direct and iterative algorithms for solving linear systems of equations, linear least squares problems, the symmetric eigenvalue problem, the nonsymmetric eigenvalue problem, the singular value decomposition, and generalizations of these to two matrices. We consider dense, band and sparse matrices. fl Computer Science Division and Mathematics Department, University of California, Berkeley CA 94720. The author was supported by NSF grant ASC-9005933, DARPA contract DAAL03-91-C-0047 via a subcontract from the University of Tennessee (administered by ARO), and DARPA grant DM28E04120 via a subcontract from Argonne National Laboratory. This work was partially performed during a visit to the Institute for Mathematics and its Applications at the University of Minnesota. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, J. Hopcroft, and J. Ullman. </author> <title> The design and analysis of computer algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Another possibility is Strassen's method <ref> [1] </ref>, which multiplies matrices recursively by dividing them into 2 fi 2 block matrices, and multiplying the subblocks using 7 matrix multiplications (recursively) and 15 matrix additions of half the size; this leads to an asymptotic complexity of n log 2 7 n 2:81 instead of n 3 .
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide, Release 1.0. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: These operations have been standardized, and many high performance computers have highly optimized implementations of these that are useful for building more complicated algorithms <ref> [2] </ref>; this is the subject of several succeeding sections. 13 3.1 Matrix multiplication on a shared memory machine Suppose we have two levels of memory hierarchy, fast and slow, where the slow memory is large enough to contain the n fi n matrices A, B and C, but the fast memory <p> To achieve higher performance, we modify this code first to use the Level 2 and then the Level 3 BLAS in its innermost loops. Again, 3! versions of these algorithms are possible, but we just describe the ones used in the LAPACK library <ref> [2] </ref>. <p> Other similar algorithms may be derived by conformally partitioning L, U and A, and equating partitions in A = LU . Algorithms 11 and 12 are available as subroutines sgetf2 and sgetrf in LAPACK <ref> [2] </ref>, respectively. <p> (k : m; k + n b : n) Multiply X = T l X Multiply and subtract A (k : m; k + n b : n) = A (k : m; k + n b : n) U X Algorithm 14 is available as subroutine sgeqrf from LAPACK <ref> [2] </ref>. Pivoting complicates matters slightly. In conventional column pivoting at step k we need to pivot (permute columns) so the next column of A to be processed has the largest norm in rows k through m of all remaining columns. <p> [u 1 ; :::; u k ] and V (l) = [v 1 ; :::; v k ] Update A (k : n; k : n) = A (k : n; k : n) U (l) V (l)T V (l) U (l)T Algorithms 15 and 16 are available from LAPACK <ref> [2] </ref> as subroutines ssytd2 and ssytrf, respectively. Hessenberg reduction is sgehrd, and bidiagonal reduction is sgebrd. The mapping to a distributed memory machine follows as with previous algorithms like QR and Gaussian elimination [63]. For parallel reduction of a band symmetric matrix to tridiagonal form, see [23, 125]. <p> Using either of these algorithms, we can count the number of eigenvalues in an interval. The traditional approach is to bisect each interval, say <ref> [ 1 ; 2 ] </ref>, by running Algorithm 17 or 18 at = ( 1 + 2 )=2. By continually subdividing intervals 33 containing eigenvalues, we can compute eigenvalue bounds as tight as we like (and round-off permits). <p> Second, the convergence properties degrade significantly, resulting in more overall work as well [67]. As a result, speedups have been extremely modest. This routine is available in LAPACK as shseqr <ref> [2] </ref>. Yet another way to introduce parallelism into Hessenberg QR is to pipeline several bulge chasing steps [194, 209, 210]. If we have several shifts available, then as soon as one bulge chase is launched from the upper left corner, another one may be launched, and so on.
Reference: [3] <author> E. Anderson and J. Dongarra. </author> <title> Evaluating block algorithm variants in LAPACK. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-90-103, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1990. </year> <note> (LAPACK Working Note #19). </note>
Reference-contexts: Other permutations of the nested loops lead to different algorithms, which depend on the BLAS for matrix-vector multiplication and solving a triangular system instead of rank-1 updating <ref> [3, 167] </ref>; which is faster depends on the relative speed of these on each machine.
Reference: [4] <author> ANSI/IEEE, </author> <title> New York. IEEE Standard for Binary Floating Point Arithmetic, </title> <address> Std 754-1985 edition, </address> <year> 1985. </year>
Reference-contexts: The latter error necessitates an interruption of execution; there is no reasonable way to proceed. On the other hand, there are reasonable ways to continue computing past floating point exceptions, such as infinity arithmetic as defined by the IEEE floating point arithmetic standard <ref> [4] </ref>. This increases the regularity of computations by eliminating branches. IEEE arithmetic is implemented on almost all microprocessors, which are often building blocks for larger parallel machines.
Reference: [5] <author> W. E. </author> <title> Arnoldi. The principle of minimized iteration in the solution of the matrix eigenproblem. </title> <journal> Quart. Appl. Math., </journal> <volume> 9 </volume> <pages> 17-29, </pages> <year> 1951. </year>
Reference-contexts: When A is symmetric this leads to an algorithm with can efficiently compute many, if not all, eigenvalues and eigenvectors [160]. In fact, the CG method (and Bi-CG) can be viewed as a solution process on top of Lanczos. The long recursion process is known as Arnoldi's method <ref> [5] </ref>, which we have seen already as the underlying orthogonalization procedure for GMRES.
Reference: [6] <author> C. Ashcraft, S. Eisenstat, and J. Liu. </author> <title> A fan-in algorithm for distributed sparse numerical factorization. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 </volume> <pages> 593-599, </pages> <year> 1990. </year>
Reference-contexts: The shortcomings of the fan-out algorithm motivated the formulation of the following fan-in algorithm for sparse factorization, which is a parallel implementation of column-Cholesky <ref> [6] </ref>: Algorithm 23: Distributed fan-in sparse Cholesky factorization for j = 1; n if j 2 mycols or mycols " Struct (L jfl ) 6= ; u = 0 for k 2 mycols " Struct (L jfl ) u = u + ` jk fl L flk faggregate column update sg
Reference: [7] <author> C. Ashcraft, S. Eisenstat, J. Liu, and A. Sherman. </author> <title> A comparison of three column-based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: For a unified description and comparison of parallel fan-in, fan-out, and multifrontal methods, see <ref> [7] </ref>. In this brief section on parallel direct methods for sparse systems, we have concentrated on numeric Cholesky factorization for SPD matrices. We have omitted many other aspects of the computation, even for the SPD case: computing the ordering in parallel, symbolic factorization, and triangular solution.
Reference: [8] <author> C. Ashcraft and R. Grimes. </author> <title> On vectorizing incomplete factorizations and SSOR preconditioners. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 122-151, </pages> <year> 1988. </year>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see <ref> [8, 56, 201, 202] </ref>). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach [178, 201, 202].
Reference: [9] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in sparse matrix methods for large linear systems on vector supercomputers. </title> <journal> Internat. J. Supercomp. Appl, </journal> <volume> 1(4) </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: For example, such techniques have been used to attain very high performance for sparse factorization on conventional vector supercomputers <ref> [9] </ref> and on RISC workstations [170]. 7.4 Parallelism in Sparse Factorization We now examine in greater detail the opportunities for parallelism in sparse Cholesky factorization and various algorithms for exploiting it.
Reference: [10] <author> L. Auslander and A. Tsao. </author> <note> On parallelizable eigensolvers. to appear in Advances in Applied Mathematics, </note> <year> 1992. </year>
Reference-contexts: This simplifies both the computation of f (B) and the extraction of its null space. See <ref> [10, 23, 127] </ref> for details. Of course we wish to split not just along the imaginary axis or unit circle but other boundaries as well.
Reference: [11] <author> I. Babuska. </author> <title> Numerical stability in problems of linear algebra. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 9 </volume> <pages> 53-77, </pages> <year> 1972. </year>
Reference-contexts: Most of the parallel approaches perform more arithmetic operations than standard (se quential) Gaussian elimination (typically 2:5 times as many), twisted factorization being the only exception. In twisted factorization the Gaussian elimination process is carried out in parallel from both sides. This method was first proposed in <ref> [11] </ref> for tridiagonal systems T x = b as a means to compute a specified component of x more accurately.
Reference: [12] <author> Z. Bai and J. Demmel. </author> <title> On a block implementation of Hessenberg multishift QR iteration. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1) </volume> <pages> 97-112, </pages> <year> 1989. </year> <note> (also LAPACK Working Note #8). </note>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Another way to introduce parallelism is to compute k &gt; 2 shifts from the bottom corner of the matrix (the eigenvalues of the bottom right kfik matrix, say), which permits us to work on k rows and columns of the matrix at a time using Level 2 BLAS <ref> [12] </ref>. Asymptotic convergence remains quadratic [210]. The drawbacks to this scheme are two-fold. First, any attempt to use Level 3 BLAS introduces rather small (hence inefficient) matrix-matrix operations, and raises the operation count considerably. Second, the convergence properties degrade significantly, resulting in more overall work as well [67].
Reference: [13] <author> Z. Bai and J. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox. </title> <institution> Computer Science Dept. </institution> <type> preprint, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer <ref> [13, 135, 144] </ref> In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> One such function f is the sign function <ref> [13, 108, 120, 135, 168, 190] </ref> which maps points with positive real part to +1 and those with negative real part to 1; adding 1 to this function then maps eigenvalues in the right half plane to 2 and in the left plane to 0, as desired. <p> By working on a shifted and squared real matrix, one can divide along lines at an angle of =4 and retain real arithmetic <ref> [13, 108, 190] </ref>. This method is promising because it allows us to work on just that part of the spectrum of interest to the user. It is stable because it applies only orthogonal transformations to B. <p> At this point, iterative refinement could be used to improve the factorization [46]. These methods apply to the generalized nonsymmetric eigenproblem as well <ref> [13, 144] </ref>. 7 Direct Methods for Sparse Linear Systems 7.1 Cholesky Factorization In this section we discuss parallel algorithms for solving sparse systems of linear equations by direct methods.
Reference: [14] <author> Z. Bai, D. Hu, and L. Reichel. </author> <title> A Newton basis GMRES implementation. </title> <type> Technical Report 91-03, </type> <institution> University of Kentucky, </institution> <year> 1991. </year>
Reference-contexts: Newton polynomials are suggested in <ref> [14] </ref>, and Chebychev polynomials in [42]. After generating a suitable starting set, we still have to orthogonalize it. In [42] modified Gram-Schmidt is used while avoiding communication times that cannot be overlapped. We outline this approach, since it may be of value for other orthogonalization methods.
Reference: [15] <author> D. H. Bailey, K. Lee, and H. D. Simon. </author> <title> Using Strassen's algorithm to accelerate the solution of linear systems. </title> <journal> J. Supercomputing, </journal> <volume> 4 </volume> <pages> 97-371, </pages> <year> 1991. </year> <month> 66 </month>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines <ref> [15] </ref>. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes [49, 104].
Reference: [16] <author> J. Barlow. </author> <title> Error analysis of update methods for the symmetric eigenvalue problem. </title> <note> to appear in SIAM J. Mat. Anal. Appl. Tech Report CS-91-21, </note> <institution> Computer Science Department, Penn State University, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others <ref> [16, 62, 97, 109, 114, 187] </ref>. <p> Work by several authors <ref> [16, 187] </ref> led to the conclusion that i had to be computed to double the input precision in order to get d i i accurately.
Reference: [17] <author> C. Beattie and D. Fox. </author> <title> Localization criteria and containment for Rayleigh quotient iteration. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 10(1) </volume> <pages> 80-93, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: By continually subdividing intervals 33 containing eigenvalues, we can compute eigenvalue bounds as tight as we like (and round-off permits). Convergence of the intervals can be accelerated by using a zero-finder such as zeroin [27, 139], Newton's method, Rayleigh quotient iteration <ref> [17, 134] </ref>, Laguerre's method, or other methods [130], to choose as an approximate zero of d n or p n , i.e. an approximate eigenvalue of T . There is parallelism both within Algorithm 18 and by running Algorithm 17 or 18 simultaneously for many values of .
Reference: [18] <author> K. Bell, B. Hatlestad, O. Hansteen, and P. Araldsen. </author> <title> NORSAM A programming system for the finite element method, User's Manual, Part I, General description. Institute for Structural Analysis, </title> <address> NTH, N-7034 Trondheim, Norway, </address> <month> February </month> <year> 1973. </year>
Reference-contexts: Currently one must immerse oneself in the multitudinous and often ephemeral details of these systems in order to write reasonably efficient programs. Perhaps not surprisingly, a number of techniques for dealing with data transfer in blocked fashion in the 1960s are being rediscovered and reused <ref> [18] </ref>. Our first goal is to enunciate two simple principles for identifying the important strengths and weaknesses of parallel programming systems (both hardware and software): locality and regularity of operation. We do this in section 2.
Reference: [19] <author> R. Benner, G. Montry, and G. Weigand. </author> <title> Concurrent multifrontal methods: shared memory, cache, and frontwidth issues. </title> <journal> Internat. J. Supercomp. Appl, </journal> <volume> 1(3) </volume> <pages> 26-44, </pages> <year> 1987. </year>
Reference-contexts: As a consequence, multifrontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multifrontal methods have been implemented for both shared-memory (e.g., <ref> [19, 68] </ref>) and distributed-memory (e.g., [94, 140]) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out, and multifrontal methods, see [7].
Reference: [20] <author> H. Berryman, J. Saltz, W. Gropp, and R. Mirchandaney. </author> <title> Krylov methods preconditioned with incompletely factored matrices on the CM-2. </title> <type> Technical Report 89-54, </type> <institution> NASA Langley Research Center, ICASE, Hampton, VA, </institution> <year> 1989. </year>
Reference-contexts: The problems with parallelism in the preconditioner have led to searches for other pre-conditioners. Often simple diagonal scaling is an adequate preconditioner, and of course this is trivially parallelizable. For results on a Connection Machine, see <ref> [20] </ref>. Often this approach leads to a significant increase in iteration steps. Still another approach is to use polynomial preconditioning: w = p j (A)r, i.e., K 1 = p j (A), for some suitable j-th degree polynomial.
Reference: [21] <author> D. Bertsekas and J. Tsitsiklis. </author> <title> Parallel and Distributed Comptutation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: Compute the inverse using Cayley-Hamilton Theorem (in about log 2 n steps). For a survey of other theoretical algorithms, see <ref> [21, 119] </ref>. 4.4 Solving Banded Systems These problems do not lend themselves as well to the techniques described above, especially for small bandwidth.
Reference: [22] <author> C. Bischof. </author> <title> Computing the singular value decomposition on a distributed system of vector processors. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 171-186, </pages> <year> 1989. </year>
Reference-contexts: It is possible to use the symmetric-indefinite decomposition of an indefinite symmetric matrix in the same way [184]. Jacobi done in this style is a rather fine grain algorithm, operating on pairs of columns, and so cannot exploit higher level BLAS. One can instead use block Jacobi algorithms <ref> [22, 182] </ref>, which work on blocks, and apply the resulting orthogonal matrices to the rest of the matrix using more efficient matrix-matrix multiplication. 6.5 The nonsymmetric eigenproblem Five kinds of parallel methods for the nonsymmetric eigenproblem have been investigated: 1.
Reference: [23] <author> C. Bischof and X. Sun. </author> <title> A divide and conquer method for tridiagonalizing symmetric matrices with repeated eigenvalues. </title> <type> MCS Report P286-0192, </type> <institution> Argonne National Lab, </institution> <year> 1992. </year>
Reference-contexts: Hessenberg reduction is sgehrd, and bidiagonal reduction is sgebrd. The mapping to a distributed memory machine follows as with previous algorithms like QR and Gaussian elimination [63]. For parallel reduction of a band symmetric matrix to tridiagonal form, see <ref> [23, 125] </ref>. The initial reduction of a generalized eigenproblem A B involves finding orthogonal matrices Q and Z such that QAZ is upper Hessenberg and QBZ is triangular. <p> This simplifies both the computation of f (B) and the extraction of its null space. See <ref> [10, 23, 127] </ref> for details. Of course we wish to split not just along the imaginary axis or unit circle but other boundaries as well.
Reference: [24] <author> C. Bischof and P. Tang. </author> <title> Generalized incremental condition estimation. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-132, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1991. </year> <note> (LAPACK Working Note #32). </note>
Reference-contexts: This cannot be directly combined with blocking as we have just described it, and so instead pivoting algorithms which only look among locally stored columns if possible have been developed <ref> [24, 25] </ref>. Other shared memory algorithms based on Givens rotations have also been developed [35, 86, 175], although these do not seem superior on shared memory machines.
Reference: [25] <author> C. Bischof and P. Tang. </author> <title> Robust incremental condition estimation. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-133, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1991. </year> <note> (LAPACK Working Note #33). </note>
Reference-contexts: This cannot be directly combined with blocking as we have just described it, and so instead pivoting algorithms which only look among locally stored columns if possible have been developed <ref> [24, 25] </ref>. Other shared memory algorithms based on Givens rotations have also been developed [35, 86, 175], although these do not seem superior on shared memory machines.
Reference: [26] <author> R. Brent and F. Luk. </author> <title> The solution of singular value and symmetric eigenvalue problems on multiprocessor arrays. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 </volume> <pages> 69-84, </pages> <year> 1985. </year>
Reference-contexts: It has also been of renewed interest on parallel machines because of its inherent parallelism: Jacobi rotations can be applied in parallel to disjoint pairs of rows and/or columns of the matrix, so a matrix with n rows and/or columns can have bn=2c Jacobi rotations applied simultaneously <ref> [26] </ref>. The question remains of the order in which to apply the simultaneous rotations to achieve quick convergence. A number of good parallel orderings have been developed and shown to have the same convergence properties as the usual serial implementations [141, 182]; we illustrate one here.
Reference: [27] <author> R. P. Brent. </author> <title> Algorithms for minimization without derivatives. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year>
Reference-contexts: By continually subdividing intervals 33 containing eigenvalues, we can compute eigenvalue bounds as tight as we like (and round-off permits). Convergence of the intervals can be accelerated by using a zero-finder such as zeroin <ref> [27, 139] </ref>, Newton's method, Rayleigh quotient iteration [17, 134], Laguerre's method, or other methods [130], to choose as an approximate zero of d n or p n , i.e. an approximate eigenvalue of T .
Reference: [28] <author> J. Brunet, A. Edelman, and J. Mesirov. </author> <title> An optimal hypercube direct n-body solver on the connection machine. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 748-752. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: ; x j ), i.e. an N -body interaction where f i is the force on body i, F (x i ; x j ) is the force on body i due to body j, and x i and x j are the positions of bodies i and j respectively <ref> [28] </ref>. Consider implementing this on a d-dimensional hypercube, and suppose N = d2 d for simplicity.
Reference: [29] <author> D. Calvetti, J. Petersen, and L. Reichel. </author> <title> A parallel implementation of the GMRES method. </title> <type> Technical Report ICM-9110-6, </type> <institution> Institute for Computational Mathematics, Kent, OH, </institution> <year> 1991. </year>
Reference-contexts: For larger systems, the speedup increases to 150 (or more if more processors are involved) as expected. Calvetti et al <ref> [29] </ref> report results for an implementation of m-step GMRES, using BLAS2 Householder orthogonalization, for a 4-processor IBM 6000 distributed memory system.
Reference: [30] <author> L. Cannon. </author> <title> A cellular computer to implement the Kalman filter algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, Bozeman, MN, </institution> <year> 1969. </year> <month> 67 </month>
Reference-contexts: We begin by showing how to best implement matrix multiplication without regard to the layout's suitability for other matrix operations, and return to the question of layouts in the next section. The first algorithm is due to Cannon <ref> [30] </ref> and is well suited for computers laid out in a square N fi N mesh, i.e. where each processor communicates most efficiently with the four other processors immediately north, east, south and west of itself.
Reference: [31] <author> S. C. Chen, D. J. Kuck, and A. H. Sameh. </author> <title> Practical parallel band triangular system solvers. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 4 </volume> <pages> 270-277, </pages> <year> 1978. </year>
Reference-contexts: In practical cases k is chosen so large that the process is not repeated for the resulting subsystems, as for cyclic reduction (where k = 2). This approach is referred to as a divide and conquer approach. For banded triangular systems it was first suggested in <ref> [31] </ref>, for tridiagonal systems it was proposed by Wang [208]. To illustrate, let us apply one parallel elimination step to the lower bidiagonal system Lx = b to eliminate all subdiagonal elements in all diagonal blocks.
Reference: [32] <author> A. T. Chronopoulos. </author> <title> Towards efficient parallel implementation of the CG method applied to a class of block tridiagonal linear systems. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 578-587, </pages> <address> Los Alamitos, CA, 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Their approach leads to slightly more flops than s successive steps of standard CG, and also one additional matrix vector product every s steps. The implementation issues for vector register computers and distributed memory machines are discussed in great detail in <ref> [32] </ref>.
Reference: [33] <author> A. T. Chronopoulos and C. W. Gear. </author> <title> s-Step iterative methods for symmetric linear systems. </title> <journal> J. on Comp. and Appl. Math., </journal> <volume> 25 </volume> <pages> 153-168, </pages> <year> 1989. </year>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [33, 149, 150, 198] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In the above algorithm there are two such synchronization points, namely the computation of both inner products. <p> In this scheme the ratio between computations and memory references is about 2. We show here yet another variant, proposed by Chronopoulos and Gear <ref> [33] </ref>. 57 Algorithm 25: Preconditioned Conjugate Gradients variant 2 x 0 = initial guess; r 0 = b Ax 0 ; q 1 = p 1 = 0; fi 1 = 0; Solve for w 0 in Kw 0 = r 0 ; s 0 = Aw 0 ; ff 0 <p> However, the price is 2n extra flops per iteration step. Chronopoulos and Gear <ref> [33] </ref> claim the method is stable, based on their numerical experiments. <p> Another slight advantage is that these inner products can be computed in parallel. Chronopoulos and Gear <ref> [33] </ref> propose to further improve the data locality and parallelism in CG by combining s successive steps. Their algorithm is based upon the following property of CG.
Reference: [34] <author> A. T. Chronopoulos and S. K. Kim. </author> <title> s-Step Orthomin and GMRES implemented on parallel computers. </title> <type> Technical Report 90/43R, </type> <institution> UMSI, Minneapolis, </institution> <year> 1990. </year>
Reference-contexts: The authors claim success in using this approach without serious stability problems for small values of s. Nevertheless, it seems that s-step CG still has a bad reputation [172] because of these problems. However, a similar approach, suggested by Chronopoulos and Kim <ref> [34] </ref> for other processes such as GMRES, seems to be more promising. Several authors have pursued this direction, and we will come back to this in section 8.3. 58 We consider another variant of CG, in which we may overlap all communication time with useful computations. <p> The obvious way to extract more parallelism and data locality is to generate a basis v 1 , Av 1 , ..., A m v 1 for the Krylov subspace first, and to orthogonalize this set afterwards; this is called m-step GMRES (m) <ref> [34] </ref>. This approach does not increase the computational work, and in contrast to CG, the numerical instability due to generating a possibly near-dependent set is not necessarily a drawback. One reason is that error cannot build up as in CG, because the method is restarted every m steps.
Reference: [35] <author> E. Chu. </author> <title> Orthogonal decomposition of dense and sparse matrices on multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Waterloo, </institution> <year> 1988. </year>
Reference-contexts: This cannot be directly combined with blocking as we have just described it, and so instead pivoting algorithms which only look among locally stored columns if possible have been developed [24, 25]. Other shared memory algorithms based on Givens rotations have also been developed <ref> [35, 86, 175] </ref>, although these do not seem superior on shared memory machines. <p> An interesting alternative that works with the same data layouts is based on Givens rotations <ref> [35, 164] </ref>. We consider just the first block column in the block scattered layout, where each of a subset of the processors owns a set of p r fi r subblocks of the block column evenly distributed over the column.
Reference: [36] <author> M. Chu. </author> <title> A note on the homotopy method for linear algebraic eigenvalue problems. </title> <journal> Lin. Alg. Appl, </journal> <volume> 105 </volume> <pages> 225-236, </pages> <year> 1988. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer <ref> [36, 37, 61, 132, 133, 213] </ref> 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists.
Reference: [37] <author> M. Chu, T.-Y. Li, and T. Sauer. </author> <title> Homotopy method for general -matrix problems. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 9(4) </volume> <pages> 528-536, </pages> <year> 1988. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer <ref> [36, 37, 61, 132, 133, 213] </ref> 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists.
Reference: [38] <author> L. Csanky. </author> <title> Fast parallel matrix inversion algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 5 </volume> <pages> 618-623, </pages> <year> 1977. </year>
Reference-contexts: Also, to achieve the maximum speedup O (n 3 ) processors are required, which is unrealistic for large n. We can use this algorithm to build an O (log 2 n) algorithm for the general problem Ax = b <ref> [38] </ref>, but this this algorithm is so unstable as to be entirely useless in floating point (in IEEE double precision floating point, it achieves no precision in inverting 3I, where I is an identity matrix of size 60 or larger). There are four steps: 1.
Reference: [39] <author> J.J.M. Cuppen. </author> <title> A divide and conquer method for the symmetric tridiagonal eigen-problem. </title> <journal> Numer. Math., </journal> <volume> 36 </volume> <pages> 177-195, </pages> <year> 1981. </year>
Reference-contexts: There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen <ref> [39] </ref>, and later analyzed and modified by many others [16, 62, 97, 109, 114, 187].
Reference: [40] <author> G. Davis, R. Funderlic, and G. Geist. </author> <title> A hypercube implementation of the implicit double shift QR algorithm. </title> <booktitle> In Hypercube Multiprocessors 1987, </booktitle> <pages> pages 619-626, </pages> <address> Philadel-phia, PA, </address> <year> 1987. </year> <note> SIAM. </note>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> One way to introduce parallelism is to spread the matrix across the processors, but communication costs may exceed the modest computational costs of the row and column operations <ref> [40, 84, 189, 195, 194] </ref>.
Reference: [41] <author> P. P. N. de Groen. </author> <title> Base p-cyclic reduction for tridiagonal systems of equations. </title> <journal> Appl. Num. Math., </journal> <volume> 8 </volume> <pages> 117-126, </pages> <year> 1991. </year>
Reference-contexts: Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly larger than the scalar speed <ref> [153, 41, 177] </ref>. For distributed memory computers the method requires too much data movement for the reduced system to be practical. However, the method is easily generalized to one with more parallelism.
Reference: [42] <author> E. de Sturler. </author> <title> A parallel restructured version of GMRES(m). </title> <type> Technical Report 91-85, </type> <institution> Delft University of Technology, Delft, </institution> <year> 1991. </year>
Reference-contexts: If the number of connections to these neighboring blocks is small compared to the number of internal nodes, then the communication time can be overlapped with computational work. For more detailed discussions on implementation aspects on distributed memory systems, see <ref> [42, 163] </ref>. The preconditioning part is often the most problematic part in a parallel environment. Incomplete decompositions of A form a popular class of preconditionings, in the context of solving discretized PDE's. <p> Newton polynomials are suggested in [14], and Chebychev polynomials in <ref> [42] </ref>. After generating a suitable starting set, we still have to orthogonalize it. In [42] modified Gram-Schmidt is used while avoiding communication times that cannot be overlapped. We outline this approach, since it may be of value for other orthogonalization methods. <p> Newton polynomials are suggested in [14], and Chebychev polynomials in <ref> [42] </ref>. After generating a suitable starting set, we still have to orthogonalize it. In [42] modified Gram-Schmidt is used while avoiding communication times that cannot be overlapped. We outline this approach, since it may be of value for other orthogonalization methods. <p> For a 150 processor MEIKO system, configured as a 15 by 10 torus, de Sturler <ref> [42] </ref> reports speedups of about 120 for typical discretized PDE systems with 60; 000 unknowns (i.e., only 400 unknowns per processor). For larger systems, the speedup increases to 150 (or more if more processors are involved) as expected.
Reference: [43] <author> A. Deichmoller. </author> <title> Uber die Berechnung verallgemeinerter singularer Werte mittles Jacobi-ahnlicher Verfahren. </title> <type> PhD thesis, </type> <institution> Fernuniversitat - Hagen, Hagen, Germany, </institution> <year> 1991. </year>
Reference-contexts: Recently, however, it has been shown that Jacobi's method can be much more accurate than QR in certain cases <ref> [43, 51, 184] </ref>, which makes it of some value on serial machines.
Reference: [44] <author> E. Dekel, D. Nassimi, and S. Sahni. </author> <title> Parallel matrix and graph algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 10(4) </volume> <pages> 657-675, </pages> <year> 1981. </year>
Reference-contexts: In the following algorithm, denotes the bitwise exclusive-or operator. We assume the 2 n fi 2 n grid of data is embedded in the hypercube so that A (i;j) is stored in processor i 2 n + j <ref> [44] </ref>: Algorithm 7: Dekel's matrix multiplication algorithm for k = 1 : n Let i k = (kth bit of i) 2 k forall i = 0 : 2 n 1, forall j = 0 : 2 n 1 Swap A (i;ji k ) and A (i;j) Swap B (j k
Reference: [45] <author> T. Dekker. </author> <title> A floating point technique for extending the available precision. </title> <journal> Num. Math., </journal> <volume> 18 </volume> <pages> 224-242, </pages> <year> 1971. </year>
Reference-contexts: When the input is already in double precision (or whatever is the largest precision supported by the machine), then quadruple is needed, which may be simulated using double, provided double is accurate enough <ref> [45, 165] </ref>. Recently, however, Gu and Eisenstat [97] have found a new algorithm that makes this unnecessary. There are two types of parallelism available in this algorithm and both must be exploited to speed up the whole algorithm [62, 109].
Reference: [46] <author> J. Demmel. </author> <title> Three methods for refining estimates of invariant subspaces. </title> <journal> Computing, </journal> <volume> 38 </volume> <pages> 43-57, </pages> <year> 1987. </year>
Reference-contexts: On the other hand, if it is difficult to find a good place to split the spectrum, convergence can be slow, and the final approximate invariant subspace inaccurate. At this point, iterative refinement could be used to improve the factorization <ref> [46] </ref>. These methods apply to the generalized nonsymmetric eigenproblem as well [13, 144]. 7 Direct Methods for Sparse Linear Systems 7.1 Cholesky Factorization In this section we discuss parallel algorithms for solving sparse systems of linear equations by direct methods.
Reference: [47] <author> J. Demmel. </author> <title> Specifications for robust parallel prefix operations. </title> <type> Technical report, </type> <institution> Thinking Machines Corp., </institution> <year> 1992. </year>
Reference-contexts: This requires good support for parallel prefix operations, and is not as easy to parallelize as simply having each processor refine different sets of intervals containing different eigenvalues <ref> [47] </ref>. Within a single processor one can also run Algorithm 17 or 18 for many different by pipelining or vectorizing [183].
Reference: [48] <author> J. Demmel. </author> <title> Trading off parallelism and numerical stability. </title> <institution> Computer Science Division Tech Report UCB//CSD-92-702, University of California, Berkeley, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: This is true for various kinds of linear systems and eigenvalue problems. We will point these out as they arise. Some of these tradeoffs can be mitigated by better floating point arithmetic <ref> [48] </ref>. Others can be dealt with by using the following simple paradigm: 1. Solve the problem using a fast method, provided it is rarely unstable. 2. Quickly and reliably confirm or deny the accuracy of the computed solution. With high probability, the answer just (quickly) computed is accurate enough. 3.
Reference: [49] <author> J. Demmel and N. J. Higham. </author> <title> Stability of block algorithms with fast Level 3 BLAS. </title> <note> to appear in ACM Trans. Math. Soft. </note>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines [15]. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes <ref> [49, 104] </ref>. Given the complexity of optimizing the implementation of matrix multiplication, we cannot expect all other matrix algorithms to be equally optimized on all machines, at least not in a time users are willing to wait.
Reference: [50] <author> J. Demmel and W. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(5) </volume> <pages> 873-912, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: We return to this problem in section 6.5. 6.3 The symmetric tridiagonal eigenproblem The basic algorithms to consider are QR iteration, (accelerated) bisection and inverse it eration, and divide and conquer. Since the bidiagonal SVD is equivalent to finding the nonnegative eigenvalues of a tridiagonal matrix with zero diagonal <ref> [50, 95] </ref>, our comments apply to that problem as well. 6.3.1 QR Iteration The classical algorithm is QR iteration, which produces a sequence of orthogonally similar tridiagonal matrices T = T 0 , T 1 , T 2 , ... converging to diagonal form.
Reference: [51] <author> J. Demmel and K. Veselic. </author> <title> Jacobi's method is more accurate than QR. </title> <institution> Computer Science Dept. </institution> <type> Technical Report 468, </type> <institution> Courant Institute, </institution> <address> New York, NY, </address> <month> October </month> <year> 1989. </year> <note> (also LAPACK Working Note #15), to appear in SIAM J. Mat. Anal. Appl. </note>
Reference-contexts: Recently, however, it has been shown that Jacobi's method can be much more accurate than QR in certain cases <ref> [43, 51, 184] </ref>, which makes it of some value on serial machines. <p> Such a one-sided Jacobi is natural when computing the SVD [98], but requires some preprocessing for the symmetric eigenproblem <ref> [51, 184] </ref>; for example, in the symmetric positive definite case one can perform Cholesky on A to get A = LL T , apply one-sided Jacobi on L or L T to get its (partial) SVD, and then square the singular values to get the eigenvalues of A. <p> It turns out it accelerates convergence to do the Cholesky decomposition with pivoting, and then apply Jacobi to the columns of L rather than the columns of L T <ref> [51] </ref>. It is possible to use the symmetric-indefinite decomposition of an indefinite symmetric matrix in the same way [184]. Jacobi done in this style is a rather fine grain algorithm, operating on pairs of columns, and so cannot exploit higher level BLAS.
Reference: [52] <author> S. Doi. </author> <title> On parallelism and convergence of incomplete LU factorizations. </title> <journal> Appl. Num. Math., </journal> <volume> 7 </volume> <pages> 417-436, </pages> <year> 1991. </year>
Reference-contexts: Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations [56, 200, 201]. Multicolor schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior <ref> [52] </ref>, but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. 55 3. Forced parallelism. Parallelism can also be forced by simply neglecting couplings to unknowns residing in other processors.
Reference: [53] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: 1 Introduction Accurate and efficient algorithms for many problems in numerical linear algebra have existed for years on conventional serial machines, and there are many portable software libraries that implement them efficiently <ref> [53, 58, 81, 185] </ref>. One reason for this profusion of successful software is the simplicity of the cost model: the execution time of an algorithm is roughly proportional to the number of floating point operations it performs. This simple fact makes it relatively easy to design efficient and portable algorithms. <p> All the rest of 3! permutations of i, j and k lead to valid algorithms, some of which access columns of A in the innermost loop. Algorithm 10 is one of these, and is used in the LINPACK routine sgefa <ref> [53] </ref>: 22 Algorithm 10: Column oriented Gaussian elimination (kji-LU decomposition) for k = 1 : n 1 f choose l so jA lk j = max kin jA ik j, swap A lk and A kk g for i = k + 1 : n for j = k + 1
Reference: [54] <author> J. Dongarra, J. Du Croz, I. Duff, and S. Hammarling. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Operations like matrix-matrix multiplication operate on pairs of matrices, and offer the best q values; these are called Level 3 BLAS <ref> [54] </ref>, and include solving triangular systems of equations with many right hand sides.
Reference: [55] <author> J. Dongarra, J. Du Croz, S. Hammarling, and Richard J. Hanson. </author> <title> An extended set of fortran basic linear algebra subroutines. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Operations like matrix-vector multiplication operate on matrices and vectors, and offer slightly better q values; these are called Level 2 BLAS <ref> [55] </ref>, and include solving triangular systems of equations and rank-1 updates of matrices (A + xy T , x and y column vectors).
Reference: [56] <author> J. Dongarra, I. Duff, D. Sorensen, and H. van der Vorst. </author> <title> Solving linear systems on vector and shared memory computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1991. </year>
Reference-contexts: Any survey of such a busy field is necessarily a snapshot reflecting some of the authors' biases. Other recent surveys include <ref> [56, 77] </ref>, the latter of which includes a bibliography of over 2000 entries. 1 This discussion will not entirely prepare the reader to write good programs on any particular machine, since many machine specific details will remain. 4 P 1 P 2 P n Network M 1 M 2 M n <p> From the discussions it should be clear how to combine coarse-grained and fine-grained approaches, for example when implementing a method on a parallel machine with vector processors. The implementation for such machines, in particular those with shared memory, is given much attention in <ref> [56] </ref>. <p> For vector computers this leads to a vectorizable preconditioner (see <ref> [8, 56, 201, 202] </ref>). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach [178, 201, 202]. <p> Duff and Meurant [70] have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations <ref> [56, 200, 201] </ref>. Multicolor schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior [52], but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. 55 3. Forced parallelism.
Reference: [57] <author> J. Dongarra, G. A. Geist, and C. Romine. </author> <title> Computing the eigenvalues and eigenvectors of a general matrix by reduction to tridiagonal form. </title> <type> Technical Report ORNL/TM-11669, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1990. </year> <note> to appear in ACM TOMS. </note>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form <ref> [57, 82, 83, 85] </ref> 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Parallelism is still fine-grain, however. 6.5.2 Reduction to nonsymmetric tridiagonal form This approach begins by reducing B to nonsymmetric tridiagonal form with a (necessarily) nonorthogonal similarity, and then finding the eigenvalues of the resulting nonsymmetric tridiagonal matrix using the tridiagonal LR algorithm <ref> [57, 82, 83, 85] </ref>. This method is attractive because finding eigenvalues of a tridiagonal matrix (even nonsymmetric) is much faster than for a Hessenberg matrix [212]. The drawback is that reduction to tridiagonal form may require very ill-conditioned similarity transformations, and may even break down [158].
Reference: [58] <author> J. Dongarra and E. Grosse. </author> <title> Distribution of mathematical software via electronic mail. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 403-407, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Accurate and efficient algorithms for many problems in numerical linear algebra have existed for years on conventional serial machines, and there are many portable software libraries that implement them efficiently <ref> [53, 58, 81, 185] </ref>. One reason for this profusion of successful software is the simplicity of the cost model: the execution time of an algorithm is roughly proportional to the number of floating point operations it performs. This simple fact makes it relatively easy to design efficient and portable algorithms.
Reference: [59] <author> J. Dongarra, S. Hammarling, and D. Sorensen. </author> <title> Block reduction of matrices to condensed forms for eigenvalue computations. </title> <journal> J. Comput. Appl. Math., </journal> <volume> 27 </volume> <pages> 215-227, </pages> <year> 1989. </year> <note> (LAPACK Working Note #2). </note>
Reference-contexts: However, no highly parallel and stable algorithms currently exist for the nonsymmetric problem; this remains an open problem. 6.2 Reduction to condensed forms Since the different reductions to condensed forms are so similar, we discuss only reduction to tridiagonal form; for the others see <ref> [59] </ref>. At step k we compute a Householder transformation Q k = I 2u k u T k so that column k of Q k A is zero below the first subdiagonal; these zeros are unchanged by forming the similarity transformation Q k AQ T k .
Reference: [60] <author> J. Dongarra and S. Ostrouchov. </author> <title> LAPACK block factorization algorithms on the Intel iPSC/860. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-90-115, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1990. </year> <note> (LAPACK Working Note #24). </note>
Reference-contexts: Given this function, it may be minimized as a function of b 1 , b 2 , p 1 and p 2 . Some theoretical analyses of this sort for special cases may be found in [167] and the references therein. See also <ref> [60, 64] </ref>.
Reference: [61] <author> J. Dongarra and M. Sidani. </author> <title> A parallel algorithm for the non-symmetric eigenvalue problem. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-137, </type> <institution> University of Ten-nessee, Knoxville, TN, </institution> <year> 1991. </year> <month> 69 </month>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer <ref> [36, 37, 61, 132, 133, 213] </ref> 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Evaluating the determinant of a Hessenberg matrix costs only a triangular solve and an inner product, and therefore is efficient. It shares similar advantages and disadvantages as the previous homotopy algorithm. Alternatively, one can use Newton's method to compute the eigendecomposition of H from S <ref> [61] </ref>. The function to which one applies Newton's method is f (z; ) = [(Hz z) T ; e T z 1] T , where e is a fixed unit vector.
Reference: [62] <author> J. Dongarra and D. Sorensen. </author> <title> A fully parallel algorithm for the symmetric eigenprob--lem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 139-154, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others <ref> [16, 62, 97, 109, 114, 187] </ref>. <p> Recently, however, Gu and Eisenstat [97] have found a new algorithm that makes this unnecessary. There are two types of parallelism available in this algorithm and both must be exploited to speed up the whole algorithm <ref> [62, 109] </ref>. Independent tridiagonal submatrices (such as T 1 and T 2 ) can obviously be solved in parallel. Initially there are a great many such small submatrices to solve in parallel, but after each secular equation solution, there are half as many submatrices of twice the size.
Reference: [63] <author> J. Dongarra and R. van de Geijn. </author> <title> Reduction to condensed form for the eigenvalue problem on distributed memory computers. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-130, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1991. </year> <note> (LAPACK Working Note #30), to appear in Parallel Computing. </note>
Reference-contexts: Hessenberg reduction is sgehrd, and bidiagonal reduction is sgebrd. The mapping to a distributed memory machine follows as with previous algorithms like QR and Gaussian elimination <ref> [63] </ref>. For parallel reduction of a band symmetric matrix to tridiagonal form, see [23, 125]. The initial reduction of a generalized eigenproblem A B involves finding orthogonal matrices Q and Z such that QAZ is upper Hessenberg and QBZ is triangular.
Reference: [64] <author> J. Dongarra and R. van de Geijn. </author> <title> Two dimensional Basic Linear Algebra Communication Subprograms. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-91-138, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1991. </year> <note> (LAPACK Working Note #37). </note>
Reference-contexts: Given this function, it may be minimized as a function of b 1 , b 2 , p 1 and p 2 . Some theoretical analyses of this sort for special cases may be found in [167] and the references therein. See also <ref> [60, 64] </ref>.
Reference: [65] <author> J. Du Croz, P. J. D. Mayes, and G. Radicati di Brozolo. </author> <title> Factorizations of band matrices using Level 3 BLAS. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS-90-109, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1990. </year> <note> (LAPACK Working Note #21. </note>
Reference-contexts: If the bandwidth is wide enough, however, the techniques of the previous sections still apply <ref> [65, 75] </ref>. The problem of solving banded linear systems with a narrow band has been studied by many authors, see for instance the references in [79, 153]. We will only sketch some of the main ideas and we will do so for rather simple problems.
Reference: [66] <author> P. Dubois and G. Rodrigue. </author> <title> An analysis of the recursive doubling algorithm. </title> <editor> In D. J. Kuck and A. H. Sameh, editors, </editor> <title> High speed computer and algorithm organization. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: This process can be repeated recursively for both new systems, leading to an algorithm known as recursive doubling [191]. In Algorithm 2 (section 2.2) it was presented as a special case of parallel prefix. It has been analyzed and generalized for banded systems in <ref> [66] </ref>. Its significance for modern parallel computers is limited, which we illustrate with the following examples. Suppose we perform a single step of recursive doubling. This step can be done in parallel, but it involves slightly more arithmetic than the serial elimination process for solving Lx = b.
Reference: [67] <author> A. Dubrulle. </author> <title> The multishift QR algorithm: is it worth the trouble? Palo Alto Scientific Center Report G320-3558x, </title> <institution> IBM Corp., 1530 Page Mill Road, </institution> <address> Palo Alto, CA 94304, </address> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> As described in section 6.2, reduction to Hessenberg form can be done efficiently, but so far it has been much harder to deal with a Hessenberg matrix <ref> [67, 112] </ref> 4 . 4 As noted in section 6.2, we cannot even efficiently reduce to condensed form for the generalized eigen-problem A B. 37 6.5.1 Hessenberg QR iteration Parallelizing Hessenberg QR is attractive because it would yield an algorithm that is as stable as the quite acceptable serial one. <p> Asymptotic convergence remains quadratic [210]. The drawbacks to this scheme are two-fold. First, any attempt to use Level 3 BLAS introduces rather small (hence inefficient) matrix-matrix operations, and raises the operation count considerably. Second, the convergence properties degrade significantly, resulting in more overall work as well <ref> [67] </ref>. As a result, speedups have been extremely modest. This routine is available in LAPACK as shseqr [2]. Yet another way to introduce parallelism into Hessenberg QR is to pipeline several bulge chasing steps [194, 209, 210].
Reference: [68] <author> I. Duff. </author> <title> Parallel implementation of multifrontal schemes. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 193-204, </pages> <year> 1986. </year>
Reference-contexts: As a consequence, multifrontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multifrontal methods have been implemented for both shared-memory (e.g., <ref> [19, 68] </ref>) and distributed-memory (e.g., [94, 140]) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out, and multifrontal methods, see [7].
Reference: [69] <author> I. Duff, A. Erisman, and J. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1986. </year>
Reference-contexts: These techniques include minimum degree, nested dissection, and various schemes for reducing the bandwidth or profile of a matrix (see, e.g., <ref> [69, 91] </ref> for details on these and many other concepts used in sparse matrix computations). <p> We will see that these characterizations of the column-Cholesky and submatrix-Cholesky algorithms have important implications for parallel implementations. We note that many variations and hybrid implementations are possible that lie somewhere between pure column-Cholesky and pure submatrix-Cholesky. Perhaps the most important of these are the multifrontal methods (see, e.g., <ref> [69] </ref>), in which updating operations are accumulated in and propagated through a series of front matrices until finally being incorporated into the ultimate target columns. <p> This is not a general purpose strategy for sparse matrices, but it is often used to enhance parallelism in tridiagonal and related systems, so we illustrate it for the sake of comparison with more general purpose methods. In odd-even reduction (see, e.g., <ref> [69] </ref>), odd node numbers come before even node numbers, and then this same renumbering is applied recursively within each resulting subset, and so on until all nodes are numbered.
Reference: [70] <author> I. S. Duff and G. A. Meurant. </author> <title> The effect of ordering on preconditioned conjugate gradient. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: This means that the triangular solves can be parallelized for each color. Of course, communication is required for couplings between groups of different colors. Simple coloring schemes, like red-black ordering for the 5-point discretized Poisson operator, seem to have a negative effect on the convergence behavior. Duff and Meurant <ref> [70] </ref> have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations [56, 200, 201].
Reference: [71] <author> P. Eberlein. </author> <title> A Jacobi method for the automatic computation of eigenvalues and eigenvectors of an arbitrary matrix. </title> <journal> J. SIAM, </journal> <volume> 10 </volume> <pages> 74-88, </pages> <year> 1962. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Alternatively, one can try to drive the matrix to be normal (AA T = A T A), at which point an orthogonal Jacobi method can be used to drive it to diagonal form; this still does not get around the problem of (nearly) nontrivial Jordan blocks <ref> [71, 154, 174, 181, 206] </ref>. On the other hand, if the matrix has distinct eigenvalues, asymptotic quadratic convergence is achieved [181]. Using n 2 processors arranged in a mesh, these algorithms can be implemented in time O (n log n) per sweep.
Reference: [72] <author> P. Eberlein. </author> <title> On the Schur decomposition of a matrix for parallel computation. </title> <journal> IEEE Trans. Comput., </journal> <volume> 36 </volume> <pages> 167-174, </pages> <year> 1987. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> There are two basic kinds of transformations used. Methods that use only orthogonal transformations maintain numerical stability and converge to Schur canonical form, but converge only linearly at best <ref> [72, 188] </ref>. If nonorthogonal transformations are used, one can try to drive the matrix to diagonal form, but if it is close to having a nontrivial Jordan block, the required similarity transformation will be very ill-conditioned and so stability is lost.
Reference: [73] <author> S. Eisenstat, M. Heath, C. Henkel, and C. Romine. </author> <title> Modified cyclic algorithms for solving triangular systems on distributed memory multiprocessors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 589-600, </pages> <year> 1988. </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication; see <ref> [73, 100, 129, 169] </ref>. 4.3 Clever but impractical parallel algorithms for solving Ax = b The theoretical literature provides us with a number of apparently fast but ultimately unattractive algorithms for solving Ax = b.
Reference: [74] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C.-W. Tseng, and M.- Y. Wu. </author> <title> Fortran D language specification. </title> <institution> Computer Science Department Report CRPC-TR90079, Rice University, Houston, TX, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: There is an emerging consensus about data layouts for distributed memory machines. This is being implemented in several programming languages <ref> [74, 103] </ref>, that will be available to programmers in the near future. We describe these layouts here.
Reference: [75] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving problems on concurrent processors, </title> <editor> v. I. </editor> <publisher> Prentice Hall, </publisher> <year> 1988. </year> <month> 70 </month>
Reference-contexts: Upward circular shift each column of B by 1, so B (i;j) is assigned B ((i+1)modN;j) . algorithm suitable for machines that are efficient at spreading subblocks across rows (or down columns) is to do this instead of the preshifting and rotation of A (or B) <ref> [75] </ref>. This algorithm is easily adapted to a hypercube. <p> If the bandwidth is wide enough, however, the techniques of the previous sections still apply <ref> [65, 75] </ref>. The problem of solving banded linear systems with a narrow band has been studied by many authors, see for instance the references in [79, 153]. We will only sketch some of the main ideas and we will do so for rather simple problems.
Reference: [76] <author> R. W. Freund, G. H. Golub, and N. M. Nachtigal. </author> <title> Iterative solution of linear systems. </title> <type> Technical Report NA-91-05, </type> <institution> Computer Science Department, Stanford, </institution> <year> 1991. </year>
Reference-contexts: For a good mathematical introduction to a class of successful and popular methods, the so-called Krylov subspace methods, see <ref> [76] </ref>. There are many such methods and new ones are frequently proposed. Fortunately, they share enough properties that to understand how to implement them in parallel it suffices to carefully examine just a few. <p> In other short recurrence methods, other properties of A may be required or desirable, but we will not exploit these properties explicitly here. Most often, CG is used in combination with some kind of preconditioning <ref> [76, 95, 102] </ref>. This means that the matrix A is implicitly multiplied by an approximation K 1 of A 1 . Usually, K is constructed to be an approximation of A, and so that Ky = z is easier to solve than Ax = b.
Reference: [77] <author> K. Gallivan, M. Heath, E. Ng, J. Ortega, B. Peyton, R. Plemmons, C. Romine, A. Sameh, and R. Voigt. </author> <title> Parallel algorithms for matrix computations. </title> <publisher> SIAM, </publisher> <address> Philadel-phia, PA, </address> <year> 1990. </year>
Reference-contexts: Any survey of such a busy field is necessarily a snapshot reflecting some of the authors' biases. Other recent surveys include <ref> [56, 77] </ref>, the latter of which includes a bibliography of over 2000 entries. 1 This discussion will not entirely prepare the reader to write good programs on any particular machine, since many machine specific details will remain. 4 P 1 P 2 P n Network M 1 M 2 M n
Reference: [78] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> Impact of hierarchical memory systems on linear algebra algorithm design. </title> <journal> Intl. J. Supercomputer Appl., </journal> <volume> 2 </volume> <pages> 12-48, </pages> <year> 1988. </year>
Reference-contexts: Other shared memory algorithms based on Givens rotations have also been developed [35, 86, 175], although these do not seem superior on shared memory machines. It is also possible to use Level 2 and 3 BLAS in the modified Gram-Schmidt algorithm <ref> [78] </ref>. 5.2 Distributed memory algorithms Just as we could map Algorithm 13 (Gaussian elimination with Level 3 BLAS) to a distributed memory machine with blocked and/or scattered layout by inserting appropriate communication, this can also be done for QR with Level 3 BLAS.
Reference: [79] <author> K. A. Gallivan, M. T. Heath, E. Ng, et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: If the bandwidth is wide enough, however, the techniques of the previous sections still apply [65, 75]. The problem of solving banded linear systems with a narrow band has been studied by many authors, see for instance the references in <ref> [79, 153] </ref>. We will only sketch some of the main ideas and we will do so for rather simple problems. The reader should keep in mind that these ideas can easily be generalized for more complicated situations, and many have appeared in the literature.
Reference: [80] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32 </volume> <pages> 54-135, </pages> <year> 1990. </year>
Reference-contexts: in section 3.3); * if the fast memory consists of vector registers and has vector operations supporting saxpy but not inner products, a column blocked code may be superior; * a real code will have to deal with nonsquare matrices, for which the optimal block sizes may not be square <ref> [80] </ref>.
Reference: [81] <author> B. S. Garbow, J. M. Boyle, J. J. Dongarra, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide Extension, </title> <booktitle> volume 51 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference-contexts: 1 Introduction Accurate and efficient algorithms for many problems in numerical linear algebra have existed for years on conventional serial machines, and there are many portable software libraries that implement them efficiently <ref> [53, 58, 81, 185] </ref>. One reason for this profusion of successful software is the simplicity of the cost model: the execution time of an algorithm is roughly proportional to the number of floating point operations it performs. This simple fact makes it relatively easy to design efficient and portable algorithms.
Reference: [82] <author> G. A. Geist. </author> <title> Parallel tridiagonalization of a general matrix using distributed memory multiprocessors. </title> <booktitle> In Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 29-35, </pages> <address> Philadelphia, PA, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form <ref> [57, 82, 83, 85] </ref> 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Parallelism is still fine-grain, however. 6.5.2 Reduction to nonsymmetric tridiagonal form This approach begins by reducing B to nonsymmetric tridiagonal form with a (necessarily) nonorthogonal similarity, and then finding the eigenvalues of the resulting nonsymmetric tridiagonal matrix using the tridiagonal LR algorithm <ref> [57, 82, 83, 85] </ref>. This method is attractive because finding eigenvalues of a tridiagonal matrix (even nonsymmetric) is much faster than for a Hessenberg matrix [212]. The drawback is that reduction to tridiagonal form may require very ill-conditioned similarity transformations, and may even break down [158].
Reference: [83] <author> G. A. Geist. </author> <title> Reduction of a general matrix to tridiagonal form. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 12(2) </volume> <pages> 362-373, </pages> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form <ref> [57, 82, 83, 85] </ref> 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Parallelism is still fine-grain, however. 6.5.2 Reduction to nonsymmetric tridiagonal form This approach begins by reducing B to nonsymmetric tridiagonal form with a (necessarily) nonorthogonal similarity, and then finding the eigenvalues of the resulting nonsymmetric tridiagonal matrix using the tridiagonal LR algorithm <ref> [57, 82, 83, 85] </ref>. This method is attractive because finding eigenvalues of a tridiagonal matrix (even nonsymmetric) is much faster than for a Hessenberg matrix [212]. The drawback is that reduction to tridiagonal form may require very ill-conditioned similarity transformations, and may even break down [158].
Reference: [84] <author> G. A. Geist and G. J. Davis. </author> <title> Finding eigenvalues and eigenvectors of unsymmetric matrices using a distributed memory multiprocessor. </title> <journal> Parallel Computing, </journal> <volume> 13(2) </volume> <pages> 199-209, </pages> <year> 1990. </year>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> One way to introduce parallelism is to spread the matrix across the processors, but communication costs may exceed the modest computational costs of the row and column operations <ref> [40, 84, 189, 195, 194] </ref>.
Reference: [85] <author> G. A. Geist, A. Lu, and E. </author> <title> Wachspress. Stabilized reduction of an arbitrary matrix to tridiagonal form. </title> <type> Technical Report ORNL/TM-11089, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1989. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form <ref> [57, 82, 83, 85] </ref> 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Parallelism is still fine-grain, however. 6.5.2 Reduction to nonsymmetric tridiagonal form This approach begins by reducing B to nonsymmetric tridiagonal form with a (necessarily) nonorthogonal similarity, and then finding the eigenvalues of the resulting nonsymmetric tridiagonal matrix using the tridiagonal LR algorithm <ref> [57, 82, 83, 85] </ref>. This method is attractive because finding eigenvalues of a tridiagonal matrix (even nonsymmetric) is much faster than for a Hessenberg matrix [212]. The drawback is that reduction to tridiagonal form may require very ill-conditioned similarity transformations, and may even break down [158].
Reference: [86] <author> M. Gentleman and H. T. Kung. </author> <title> Matrix triangularization by systolic arrays. </title> <booktitle> In Proceedings SPIE 298, Real Time Signal Processing, </booktitle> <pages> pages 19-26, </pages> <address> Dan Diego, CA, </address> <year> 1981. </year>
Reference-contexts: This cannot be directly combined with blocking as we have just described it, and so instead pivoting algorithms which only look among locally stored columns if possible have been developed [24, 25]. Other shared memory algorithms based on Givens rotations have also been developed <ref> [35, 86, 175] </ref>, although these do not seem superior on shared memory machines.
Reference: [87] <author> A. George. </author> <title> Nested dissection of a regular finite element mesh. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference: [88] <author> A. George, M. Heath, J. Liu, and E. Ng. </author> <title> Solution of sparse positive definite systems on a shared memory multiprocessor. </title> <journal> Internat. J. Parallel Programming, </journal> <volume> 15 </volume> <pages> 309-325, </pages> <year> 1986. </year>
Reference-contexts: This approach has the additional advantage of providing automatic load balancing to whatever degree is permitted by the chosen task granularity. An implementation of this approach for parallel sparse factorization is given in <ref> [88] </ref>. In a distributed memory environment, communication costs often prohibit dynamic task assignment or load balancing, and thus we seek a static mapping of tasks to processors.
Reference: [89] <author> A. George, M. Heath, J. Liu, and E. Ng. </author> <title> Sparse Cholesky factorization on a local-memory multiprocessor. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year> <month> 71 </month>
Reference-contexts: One of the earliest and simplest parallel algorithms for sparse Cholesky factorization is the following version of submatrix-Cholesky <ref> [89] </ref>. The algorithm given below runs on each processor, with each responsible for its own subset, mycols, of columns.
Reference: [90] <author> A. George, M. Heath, J. Liu, and E. Ng. </author> <title> Solution of sparse positive definite systems on a hypercube. </title> <journal> J. Comp. Appl. Math., </journal> <volume> 27 </volume> <pages> 129-156, </pages> <year> 1989. </year>
Reference-contexts: A better approach for sparse factorization is to preserve locality by assigning subtrees of the elimination tree to contiguous subsets of neighboring processors. A good example of this technique is the "subtree-to-subcube" mapping often used with hypercube multicomputers <ref> [90] </ref>. Of course, the same idea applies to other network topologies, such as submeshes of a larger mesh. We will assume that some such mapping is used, and we will comment further on its implications below.
Reference: [91] <author> A. George and J. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: These techniques include minimum degree, nested dissection, and various schemes for reducing the bandwidth or profile of a matrix (see, e.g., <ref> [69, 91] </ref> for details on these and many other concepts used in sparse matrix computations).
Reference: [92] <author> A. George and J. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31 </volume> <pages> 1-19, </pages> <year> 1989. </year>
Reference-contexts: known 48 i i i i 3 7 4 G (A) fi fi fi fi fi fi fi fi fi A fi fi fi fi fi fi fi i i i i 3 2 6 J T (A) Cholesky factor and elimination tree (right). for limiting fill in sparse factorization <ref> [92] </ref>. In its simplest form, this algorithm begins by selecting a node of minimum degree (i.e., one having fewest incident edges) in G (A) and numbering it first. The selected node is then deleted and new edges are added, if necessary, to make its former neighbors into a clique.
Reference: [93] <author> A. George, J. Liu, and E. Ng. </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10 </volume> <pages> 287-298, </pages> <year> 1989. </year>
Reference-contexts: In particular, performing the column updates one at a time by the receiving processors results in unnecessarily high communication frequency and volume, and in a relatively inefficient computational inner loop. The communication requirements can be reduced by careful mapping and by aggregating updating information over subtrees (see, e.g., <ref> [93, 152, 215] </ref>), but even with this improvement, the fan-out algorithm is usually not competitive with other algorithms presented below.
Reference: [94] <author> J. Gilbert and R. Schreiber. </author> <title> Highly parallel sparse cholesky factorization. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year>
Reference-contexts: As a consequence, multifrontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multifrontal methods have been implemented for both shared-memory (e.g., [19, 68]) and distributed-memory (e.g., <ref> [94, 140] </ref>) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out, and multifrontal methods, see [7].
Reference: [95] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, MD, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: In this section we concentrate on factoring P A = LU , which has the dominant number of floating point operations, 2n 3 =3 + O (n 2 ). Pivoting is required for numerical stability, and we use the standard partial pivoting scheme <ref> [95] </ref>; this means L has unit diagonal and other entries bounded in magnitude by one. <p> For simplicity we consider only QR without pivoting, and mention work incorporating pivoting at the end. The conventional approach is to premultiply A by a sequence of simple orthogonal matrices Q i chosen to introduce zeros below the diagonal of A <ref> [95] </ref>. Eventually A becomes upper triangular, and equal to R, and the product Q N Q 1 = Q. <p> in the changed rows of one column of A; a Household reflection may be written I 2uu T , where u is a unit vector with nonzeros only in the rows to be changed. 5.1 Shared memory algorithms The basic algorithm to compute a QR decomposition using Householder transformations is <ref> [95] </ref>: Algorithm 13: QR decomposition using Level 2 BLAS for k = 1 : n 1 Compute a unit vector u k so that (I 2u k u T k )A (k + 1 : m; k) = 0 Update A = A 2 fl u k (u T k A) <p> We return to this problem in section 6.5. 6.3 The symmetric tridiagonal eigenproblem The basic algorithms to consider are QR iteration, (accelerated) bisection and inverse it eration, and divide and conquer. Since the bidiagonal SVD is equivalent to finding the nonnegative eigenvalues of a tridiagonal matrix with zero diagonal <ref> [50, 95] </ref>, our comments apply to that problem as well. 6.3.1 QR Iteration The classical algorithm is QR iteration, which produces a sequence of orthogonally similar tridiagonal matrices T = T 0 , T 1 , T 2 , ... converging to diagonal form. <p> If T has diagonal entries a 1 ; :::; a n and offdiagonals b 1 ; :::; b n1 , then we can count the number of eigenvalues of T less than as follows <ref> [95] </ref>: Algorithm 17: Counting eigenvalues using Sturm Sequences (1) count = 0, d = 1, b 0 = 0 for i = 1 : n i1 =d if d &lt; 0, count = count + 1 This nonlinear recurrence may be transformed into a two-term linear recurrence in p i = <p> The current level of parallel software support on many machines can make this difficult to implement well. 6.4 Jacobi's method for the symmetric eigenproblem and SVD Jacobi's method has been used for the nonsymmetric eigenproblem, the symmetric eigen-problem, the SVD, and generalizations of these problems to pairs of matrices <ref> [95] </ref>. It works by applying a series of Jacobi rotations (a special kind of Givens rotation) to the left and/or right of the matrix in order to drive it to a desired canonical form, such as diagonal form for the symmetric eigenproblem. <p> In this case the preconditioner K = LU , where L and U have a sparsity pattern equal or close to the sparsity pattern of the corresponding parts of A (L is lower triangular, U is upper triangular). For details see <ref> [95, 147, 148] </ref>. Solving Kw = r leads to solving successively Lz = r and U w = z. These triangular solves lead to recurrence relations which are not easily parallelized. We will now discuss a number of approaches to obtain parallelism in the preconditioning part. 1. Reordering the computations. <p> In other short recurrence methods, other properties of A may be required or desirable, but we will not exploit these properties explicitly here. Most often, CG is used in combination with some kind of preconditioning <ref> [76, 95, 102] </ref>. This means that the matrix A is implicitly multiplied by an approximation K 1 of A 1 . Usually, K is constructed to be an approximation of A, and so that Ky = z is easier to solve than Ax = b. <p> Since we do not require A to be symmetric, we need long recurrences: each new vector must be explicitly orthogonalized against all previously generated basis vectors. In its most common form GMRES orthogonalizes using Modified Gram-Schmidt <ref> [95] </ref>. In order to limit memory requirements (since all basis vectors must be stored), GMRES is restarted after each cycle of m iteration steps; this is called GMRES (m).
Reference: [96] <author> A. Gottlieb and G. Almasi. </author> <title> Highly Parallel Computing. </title> <publisher> Benjamin Cummings, </publisher> <address> Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: will not entirely prepare the reader to write good programs on any particular machine, since many machine specific details will remain. 4 P 1 P 2 P n Network M 1 M 2 M n 2 Features of Parallel Systems 2.1 General Principles A large number of different parallel computers <ref> [96] </ref>, languages (see [214] and the references therein), and software tools have recently been built or proposed. Though the details of these systems vary widely, there are two basic issues they must deal with, and these will guide us in understanding how to design and analyze parallel algorithms. <p> Examples include machines from Convex, Cray, Fijitsu, Hitachi, NEC, and others <ref> [96] </ref>. The memories of these machines are organized into some number, say b, of memory banks, so that memory address m resides in memory bank m mod b.
Reference: [97] <author> M. Gu and S. Eisenstat. </author> <title> A stable and efficient algorithm for the rank-1 modification of the symmetric eigenproblem. </title> <institution> Computer Science Dept. Report YALEU/DCS/RR-916, Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others <ref> [16, 62, 97, 109, 114, 187] </ref>. <p> When the input is already in double precision (or whatever is the largest precision supported by the machine), then quadruple is needed, which may be simulated using double, provided double is accurate enough [45, 165]. Recently, however, Gu and Eisenstat <ref> [97] </ref> have found a new algorithm that makes this unnecessary. There are two types of parallelism available in this algorithm and both must be exploited to speed up the whole algorithm [62, 109]. Independent tridiagonal submatrices (such as T 1 and T 2 ) can obviously be solved in parallel.
Reference: [98] <author> V. Hari and K. Veselic. </author> <title> On Jacobi methods for singular value decompositions. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8 </volume> <pages> 741-754, </pages> <year> 1987. </year>
Reference-contexts: Such a one-sided Jacobi is natural when computing the SVD <ref> [98] </ref>, but requires some preprocessing for the symmetric eigenproblem [51, 184]; for example, in the symmetric positive definite case one can perform Cholesky on A to get A = LL T , apply one-sided Jacobi on L or L T to get its (partial) SVD, and then square the singular values
Reference: [99] <author> M. Heath, E. Ng, and B. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: Most of the lessons learned are also applicable to other matrix factorizations, such as LU and QR. We do not try to give an exhaustive survey of research in this area, which is currently very active, instead referring the reader to existing surveys, such as <ref> [99] </ref>. Our main point in the current discussion is to explain how the sparse case differs from the dense case, and examine the performance implications of those differences. We begin by considering the main features of sparse Cholesky factorization that affect its performance on serial machines.
Reference: [100] <author> M. Heath and C. Romine. </author> <title> Parallel solution of triangular systems of distributed memory multiprocessors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 558-588, </pages> <year> 1988. </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication; see <ref> [73, 100, 129, 169] </ref>. 4.3 Clever but impractical parallel algorithms for solving Ax = b The theoretical literature provides us with a number of apparently fast but ultimately unattractive algorithms for solving Ax = b.
Reference: [101] <author> D. Heller. </author> <title> Some aspects of the cyclic reduction algorithm for block tridiagonal linear systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 13 </volume> <pages> 484-496, </pages> <year> 1978. </year>
Reference-contexts: After having solved this reduced system, the odd-numbered unknowns can be computed in parallel from the odd-numbered equations. Of course, the trick can be repeated for the subsystem of half size, and this process is known as cyclic reduction <ref> [124, 101] </ref>. Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly larger than the scalar speed [153, 41, 177].
Reference: [102] <author> M. R. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <institution> J. Res. Natl. Bur. Stand., </institution> <month> 49 </month> <pages> 409-436, </pages> <year> 1954. </year>
Reference-contexts: In other short recurrence methods, other properties of A may be required or desirable, but we will not exploit these properties explicitly here. Most often, CG is used in combination with some kind of preconditioning <ref> [76, 95, 102] </ref>. This means that the matrix A is implicitly multiplied by an approximation K 1 of A 1 . Usually, K is constructed to be an approximation of A, and so that Ky = z is easier to solve than Ax = b.
Reference: [103] <institution> High Peformance Fortran. </institution> <note> documentation available via anonymous ftp from titan.cs.rice.edu in directory public/HPFF, </note> <year> 1991. </year>
Reference-contexts: There is an emerging consensus about data layouts for distributed memory machines. This is being implemented in several programming languages <ref> [74, 103] </ref>, that will be available to programmers in the near future. We describe these layouts here. <p> There is an emerging consensus about data layouts for distributed memory machines. This is being implemented in several programming languages [74, 103], that will be available to programmers in the near future. We describe these layouts here. High Performance Fortran (HPF) <ref> [103] </ref> permits the user to define a virtual array of processors, align actual data structures like matrices and arrays with this virtual array (and so with respect to each other), and then to layout the virtual processor array on an actual machine.
Reference: [104] <author> N. J. Higham. </author> <title> Exploiting fast matrix multiplication within the Level 3 BLAS. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 </volume> <pages> 352-368, </pages> <year> 1990. </year>
Reference-contexts: This approach has led to speedups on relatively large matrices on some machines [15]. A drawback is the need for significant workspace, and somewhat lower numerical stability, although it is adequate for many purposes <ref> [49, 104] </ref>. Given the complexity of optimizing the implementation of matrix multiplication, we cannot expect all other matrix algorithms to be equally optimized on all machines, at least not in a time users are willing to wait.
Reference: [105] <author> C. T. Ho. </author> <title> Optimal communication primitives and graph embeddings on hypercubes. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1990. </year>
Reference-contexts: The simplest way is to embed a grid (or 2-D torus) in a hypercube, i.e. map the processors in a grid to the processors in a hypercube, and the connections in a grid to a subset of the connections in a hypercube <ref> [105, 115] </ref>. Suppose the hypercube is d dimensional, so the 2 d processors are labeled by d bit numbers.
Reference: [106] <author> C. T. Ho, S. L. Johnsson, and A. Edelman. </author> <title> Matrix multiplication on hypercubes using full bandwidth and constant storage. </title> <booktitle> In The Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <pages> pages 447-451. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year> <month> 72 </month>
Reference-contexts: Each row (column) of the grid thus occupies an m- (n-) dimensional subcube of the original hypercube, with nearest neighbors in the grid mapped to nearest neighbors in the hypercube <ref> [106] </ref>. We illustrate for a 4fi4 grid in figure 5. <p> n forall i = 0 : 2 n 1, forall j = 0 : 2 n 1 C (ij) = C (ij) + A (ij) B (ij) Swap A (i;jg d;k ) and A (i;j) Swap B (ig d;k ;j) and B (i;j) Finally, we may speed this up further <ref> [106, 117] </ref> provided the A (i;j) blocks are large enough, by using the same algorithm as for force calculations in section 2.
Reference: [107] <author> X. Hong and H. T. Kung. </author> <title> I/O complexity: the red blue pebble game. </title> <booktitle> In Proceedings of the 13th STOC, </booktitle> <year> 1981. </year>
Reference-contexts: This yields q M=3, which is much better than the previous algorithms. In <ref> [107] </ref> an analysis of this problem leading to an upper bound near p M is given, so we cannot expect to improve much on this algorithm for square matrices.
Reference: [108] <author> J. Howland. </author> <title> The sign matrix and the separation of matrix eigenvalues. </title> <journal> Lin. Alg. Appl., </journal> <volume> 49 </volume> <pages> 221-232, </pages> <year> 1983. </year>
Reference-contexts: One such function f is the sign function <ref> [13, 108, 120, 135, 168, 190] </ref> which maps points with positive real part to +1 and those with negative real part to 1; adding 1 to this function then maps eigenvalues in the right half plane to 2 and in the left plane to 0, as desired. <p> The only operations we can easily perform on (dense) matrices are multiplication and inversion, so in practice f must be a rational function. A globally, asymptotically quadrat-ically convergent iteration to compute the sign-function of B is B i+1 = (B i + B 1 i )=2 <ref> [108, 168, 190] </ref>; this is simply Newton's method applied to B 2 = I, and can also be seen to equivalent to repeated squaring (the power method) of the Cayley transform of B. <p> By working on a shifted and squared real matrix, one can divide along lines at an angle of =4 and retain real arithmetic <ref> [13, 108, 190] </ref>. This method is promising because it allows us to work on just that part of the spectrum of interest to the user. It is stable because it applies only orthogonal transformations to B.
Reference: [109] <author> I. Ipsen and E. Jessup. </author> <title> Solving the symmetric tridiagonal eigenvalue problem on the hypercube. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(2) </volume> <pages> 203-230, </pages> <year> 1990. </year>
Reference-contexts: eigenvectors takes O (n 3 ) flops but updating T just O (n 2 ), we succeed in parallelizing the majority of the computational work. 6.3.2 Bisection and Inverse Iteration One of the two most promising methods is (accelerated) bisection for the eigenvalues, followed by inverse iteration for the eigenvectors <ref> [109, 139] </ref>. <p> There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others <ref> [16, 62, 97, 109, 114, 187] </ref>. <p> Recently, however, Gu and Eisenstat [97] have found a new algorithm that makes this unnecessary. There are two types of parallelism available in this algorithm and both must be exploited to speed up the whole algorithm <ref> [62, 109] </ref>. Independent tridiagonal submatrices (such as T 1 and T 2 ) can obviously be solved in parallel. Initially there are a great many such small submatrices to solve in parallel, but after each secular equation solution, there are half as many submatrices of twice the size.
Reference: [110] <author> B. Irons. </author> <title> A frontal solution program for finite element analysis. </title> <journal> Internat. J. Numer. Meth. Engrg., </journal> <volume> 2 </volume> <pages> 5-32, </pages> <year> 1970. </year>
Reference-contexts: Multifrontal methods have a number of attractive advantages, most of which accrue from the localization of memory references in the front matrices, thereby facilitating the effective use of memory hierarchies, including cache, virtual memory with paging, or explicit out-of-core solutions (the latter was the original motivation for these methods <ref> [110] </ref>). In addition, since the front matrices are essentially dense, the operations on them can be done using optimized kernels, such as the BLAS, to take advantage of vectorization or any other available architectural features.
Reference: [111] <author> J. Jess and H. Kees. </author> <title> A data structure for parallel L/U decomposition. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-31:231-239, </volume> <year> 1982. </year>
Reference-contexts: And just as the fill in the Cholesky factor is very sensitive to the ordering of the matrix, so is the structure of the elimination tree. This suggests that we should choose an ordering to enhance parallelism, and indeed this is possible (see, e.g., <ref> [111, 128, 137] </ref>), but such an objective may conflict to some degree with preservation of sparsity. Roughly speaking, sparsity and parallelism are largely compatible, since the large-grain parallelism is due to sparsity in the first place.
Reference: [112] <author> E. Jessup. </author> <title> A case against a divide and conquer approach to the nonsymmetric eigen-problem. </title> <type> Technical Report ORNL/TM-11903, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: As described in section 6.2, reduction to Hessenberg form can be done efficiently, but so far it has been much harder to deal with a Hessenberg matrix <ref> [67, 112] </ref> 4 . 4 As noted in section 6.2, we cannot even efficiently reduce to condensed form for the generalized eigen-problem A B. 37 6.5.1 Hessenberg QR iteration Parallelizing Hessenberg QR is attractive because it would yield an algorithm that is as stable as the quite acceptable serial one. <p> The subproblems produced by divide and conquer may be much more ill-conditioned than the original problem. These drawbacks are discussed in <ref> [112] </ref>. Homotopy methods replace the original Hessenberg matrix H by the one-parameter linear family H (t) = tS + (1 t)H, 0 t 1. As t increases from 0 to 1, the eigenvalues (and eigenvectors) trace out curves connecting the eigenvalues of S to the desired ones of H.
Reference: [113] <author> E. Jessup and I. Ipsen. </author> <title> Improving the accuracy of inverse iteration. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13(2) </volume> <pages> 550-572, </pages> <year> 1992. </year>
Reference-contexts: If we simply do inverse iteration without communication, the speedup will be nearly perfect. However, we cannot guarantee orthogonality of eigenvectors of clustered eigenvalues <ref> [113] </ref>, which currently seems to require reorthogonalization of eigenvectors within clusters (other methods are under investigation [157]).
Reference: [114] <author> E. Jessup and D. Sorensen. </author> <title> A divide and conquer algorithm for computing the singular value decomposition of a matrix. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 61-66, </pages> <address> Philadelphia, PA, </address> <year> 1989. </year> <note> SIAM. </note>
Reference-contexts: There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others <ref> [16, 62, 97, 109, 114, 187] </ref>.
Reference: [115] <author> S. L. Johnsson. </author> <title> Communication efficient basic linear algebra computations on hypercube architecture. </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 133-172, </pages> <year> 1987. </year>
Reference-contexts: The simplest way is to embed a grid (or 2-D torus) in a hypercube, i.e. map the processors in a grid to the processors in a hypercube, and the connections in a grid to a subset of the connections in a hypercube <ref> [105, 115] </ref>. Suppose the hypercube is d dimensional, so the 2 d processors are labeled by d bit numbers.
Reference: [116] <author> S. L. Johnsson. </author> <title> private communication, </title> <year> 1990. </year>
Reference-contexts: On the 3 The matrix multiplication subroutine in the CM-2 Scientific Subroutine Library took approximately 10 person-years of effort <ref> [116] </ref>. 19 0,0 0,0 0,0 0,0 0,1 0,1 0,1 0,1 0,2 0,2 0,2 0,2 0,3 0,3 0,3 0,3 0,0 0,0 0,0 0,0 0,1 0,1 0,1 0,1 0,2 0,2 0,2 0,2 0,3 0,3 0,3 0,3 1,0 1,0 1,0 1,0 1,1 1,1 1,1 1,1 1,2 1,2 1,2 1,2 1,3 1,3 1,3 1,3 1,0
Reference: [117] <author> S. L. Johnsson and C. T. Ho. </author> <title> Matrix multiplication on Boolean cubes using generic communication primitives. </title> <editor> In A. Wouk, editor, </editor> <booktitle> Parallel processing and medium scale multiprocessors, </booktitle> <pages> pages 108-156. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: n forall i = 0 : 2 n 1, forall j = 0 : 2 n 1 C (ij) = C (ij) + A (ij) B (ij) Swap A (i;jg d;k ) and A (i;j) Swap B (ig d;k ;j) and B (i;j) Finally, we may speed this up further <ref> [106, 117] </ref> provided the A (i;j) blocks are large enough, by using the same algorithm as for force calculations in section 2.
Reference: [118] <author> W. Kahan. </author> <title> Accurate eigenvalues of a symmetric tridiagonal matrix. </title> <institution> Computer Science Dept. </institution> <type> Technical Report CS41, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> July </month> <year> 1966 </year> <month> (revised June </month> <year> 1968). </year>
Reference-contexts: 0, p 0 = 1, p 1 = 0, b 0 = 0 for i = 1 : n i1 p i2 if p i p i1 &lt; 0, count = count + 1 In practice, these algorithms need to protected against over/underflow; Algorithm 17 is much easier to protect <ref> [118] </ref>. Using either of these algorithms, we can count the number of eigenvalues in an interval. The traditional approach is to bisect each interval, say [ 1 ; 2 ], by running Algorithm 17 or 18 at = ( 1 + 2 )=2. <p> The first kind of parallelism uses parallel prefix as described in (1) in section 2.2, and so care needs to be taken to avoid over/underflow. The numerical stability of the serial implementations of Algorithms 17 <ref> [118] </ref> and 18 [212] is very good, but that of the parallel prefix algorithm is unknown, although numerical experiments are promising [192].
Reference: [119] <author> R. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, </booktitle> <pages> pages 869-941. </pages> <publisher> Elsevier and MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Compute the inverse using Cayley-Hamilton Theorem (in about log 2 n steps). For a survey of other theoretical algorithms, see <ref> [21, 119] </ref>. 4.4 Solving Banded Systems These problems do not lend themselves as well to the techniques described above, especially for small bandwidth.
Reference: [120] <author> C. Kenney and A. Laub. </author> <title> Rational iteration methods for the matrix sign function. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 21 </volume> <pages> 487-494, </pages> <year> 1991. </year>
Reference-contexts: One such function f is the sign function <ref> [13, 108, 120, 135, 168, 190] </ref> which maps points with positive real part to +1 and those with negative real part to 1; adding 1 to this function then maps eigenvalues in the right half plane to 2 and in the left plane to 0, as desired. <p> It converges more slowly as eigenvalues approach the imaginary axis, and is in fact noncon-vergent if there are imaginary eigenvalues, as may be expected since the sign function is discontinuous there. Other higher order convergent schemes exist, but they can be more expensive to implement as well <ref> [120, 156] </ref>. Another scheme which divides the spectrum between the eigenvalues inside and outside the unit circle is given in [144].
Reference: [121] <author> A. S. Krishnakumar and M. Morf. </author> <title> Eigenvalues of a symmetric tridiagonal matrix: a divide and conquer approach. </title> <journal> Numer. Math., </journal> <volume> 48 </volume> <pages> 349-368, </pages> <year> 1986. </year>
Reference-contexts: Other ways to count the eigenvalues in intervals have been proposed as well <ref> [121, 192] </ref>, although these are more complicated than either Algorithm 17 or 18.
Reference: [122] <author> D. Kuck and A. Sameh. </author> <title> A parallel QR algorithm for symmetric tridiagonal matrices. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-26(2), </volume> <year> 1977. </year> <month> 73 </month>
Reference-contexts: As it stands it is not parallelizable, but by squaring the matrix entries this recurrence can be changed into a recurrence of the form (1) in section 2.2 (see <ref> [122] </ref>). The numerical stability of this method is not known, but available analyses are pessimistic [122]. Furthermore, QR iterations must be done sequentially, with usually just one eigenvalue converging at a time. If one only wants eigenvalues, this method does not appear to be competitive with the alternatives below. <p> As it stands it is not parallelizable, but by squaring the matrix entries this recurrence can be changed into a recurrence of the form (1) in section 2.2 (see <ref> [122] </ref>). The numerical stability of this method is not known, but available analyses are pessimistic [122]. Furthermore, QR iterations must be done sequentially, with usually just one eigenvalue converging at a time. If one only wants eigenvalues, this method does not appear to be competitive with the alternatives below.
Reference: [123] <author> H. T. Kung. </author> <title> New algorithms and lower bounds for the parallel evaluation of certain rational expressions. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <month> February </month> <year> 1974. </year>
Reference-contexts: Let d be the maximum of the degrees of the numerators and denominators of the rational functions f i . Then Kung <ref> [123] </ref> has shown that z i can be evaluated faster than linear time (i.e. z i can be evaluated in o (i) steps) if and only if d 1; in this case the problem reduces to 2 fi 2 matrix multiplication parallel prefix in (1).
Reference: [124] <author> J. J. Lambiotte and R. G. Voigt. </author> <title> The solution of tridiagonal linear systems on the CDC-STAR-100 computer. </title> <type> Technical report, </type> <institution> ICASE-NASA Langley Research Center, Hampton, VA, </institution> <year> 1974. </year>
Reference-contexts: After having solved this reduced system, the odd-numbered unknowns can be computed in parallel from the odd-numbered equations. Of course, the trick can be repeated for the subsystem of half size, and this process is known as cyclic reduction <ref> [124, 101] </ref>. Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly larger than the scalar speed [153, 41, 177].
Reference: [125] <author> B. Lang. </author> <title> Reducing symmetric band matrices to tridiagonal form a comparison of a new parallel algorithm with two serial algorithms on the iPSC/860. </title> <institution> Institut fur angewandte mathematik report, Universitat Karlsruhe, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Hessenberg reduction is sgehrd, and bidiagonal reduction is sgebrd. The mapping to a distributed memory machine follows as with previous algorithms like QR and Gaussian elimination [63]. For parallel reduction of a band symmetric matrix to tridiagonal form, see <ref> [23, 125] </ref>. The initial reduction of a generalized eigenproblem A B involves finding orthogonal matrices Q and Z such that QAZ is upper Hessenberg and QBZ is triangular.
Reference: [126] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for fortran usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: We see that only matrix multiplication offers us an opportunity to make this ratio large. This table reflects a hierarchy of operations: Operations like saxpy operate on vectors and offer the worst q values; these are called Level 1 BLAS <ref> [126] </ref> and include inner products and other simple operations.
Reference: [127] <author> S. Lederman, A. Tsao, and T. Turnbull. </author> <title> A parallelizable eigensolver for real diagonal-izable matrices with real eigenvalues. </title> <type> Report TR-01-042, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <year> 1992. </year>
Reference-contexts: This simplifies both the computation of f (B) and the extraction of its null space. See <ref> [10, 23, 127] </ref> for details. Of course we wish to split not just along the imaginary axis or unit circle but other boundaries as well.
Reference: [128] <author> J. Lewis, B. Peyton, and A. Pothen. </author> <title> A fast algorithm for reordering sparse matrices for parallel factorization. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 1156-1173, </pages> <year> 1989. </year>
Reference-contexts: And just as the fill in the Cholesky factor is very sensitive to the ordering of the matrix, so is the structure of the elimination tree. This suggests that we should choose an ordering to enhance parallelism, and indeed this is possible (see, e.g., <ref> [111, 128, 137] </ref>), but such an objective may conflict to some degree with preservation of sparsity. Roughly speaking, sparsity and parallelism are largely compatible, since the large-grain parallelism is due to sparsity in the first place.
Reference: [129] <author> G. Li and T. Coleman. </author> <title> A parallel triangular solver on a distributed memory multiprocessor. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 485-502, </pages> <year> 1988. </year>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication; see <ref> [73, 100, 129, 169] </ref>. 4.3 Clever but impractical parallel algorithms for solving Ax = b The theoretical literature provides us with a number of apparently fast but ultimately unattractive algorithms for solving Ax = b.
Reference: [130] <author> K. Li and T.-Y. Li. </author> <title> An algorithm for symmetric tridiagonal eigenproblems |- divide and conquer with homotopy continuation. </title> <note> to appear in SIAM J. </note> <institution> Sci. Stat. Comput. </institution>
Reference-contexts: By continually subdividing intervals 33 containing eigenvalues, we can compute eigenvalue bounds as tight as we like (and round-off permits). Convergence of the intervals can be accelerated by using a zero-finder such as zeroin [27, 139], Newton's method, Rayleigh quotient iteration [17, 134], Laguerre's method, or other methods <ref> [130] </ref>, to choose as an approximate zero of d n or p n , i.e. an approximate eigenvalue of T . There is parallelism both within Algorithm 18 and by running Algorithm 17 or 18 simultaneously for many values of .
Reference: [131] <author> R. Li. </author> <title> Solving the secular equation stably. </title> <institution> UC Berkeley Math Dept. </institution> <note> Report, in preparation, </note> <year> 1992. </year>
Reference-contexts: This lets us solve quickly using a Newton-like method (although care must be taken to guarantee convergence <ref> [131] </ref>). The corresponding eigenvector for a root j is then simply given by (D j I) 1 z.
Reference: [132] <author> T.-Y. Li and Z. Zeng. </author> <title> Homotopy-determinant algorithm for solving nonsymmetric eigenvalue problems. </title> <note> to appear in Math. Comp. </note>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer <ref> [36, 37, 61, 132, 133, 213] </ref> 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> And as mentioned above, if two paths converge to the same solution, it is hard to tell if the solution really is a multiple root or if some other root is missing. A different homotopy scheme uses only the determinant to follow eigenvalues <ref> [132, 213] </ref>; here the homotopy function is simply det (H (t) I). Evaluating the determinant of a Hessenberg matrix costs only a triangular solve and an inner product, and therefore is efficient. It shares similar advantages and disadvantages as the previous homotopy algorithm.
Reference: [133] <author> T.-Y. Li, Z. Zeng, and L. Cong. </author> <title> Solving eigenvalue problems of nonsymmetric matrices with real homotopies. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 29(1) </volume> <pages> 229-248, </pages> <year> 1992. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer <ref> [36, 37, 61, 132, 133, 213] </ref> 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> The numerical method follows these curves by standard curve following schemes, predicting the position of a nearby point on the curve using the derivative of the eigenvalue with respect to t, and then correcting its predicted value using Newton's method. Two schemes have been investigated. The first <ref> [133] </ref> follows eigenvalue/eigenvector pairs. The homotopy function is h (z; ; t) = [(H (t)z z) T ; kzk 2 2 1] T , i.e. the homotopy path is defined by choosing z (t) and (t) so that h (z (t); (t); t) = 0 along the path.
Reference: [134] <author> T.-Y. Li, H. Zhang, and X.-H. Sun. </author> <title> Parallel homotopy algorithm for symmetric tridiagonal eigenvalue problem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12(3) </volume> <pages> 469-487, </pages> <year> 1991. </year>
Reference-contexts: By continually subdividing intervals 33 containing eigenvalues, we can compute eigenvalue bounds as tight as we like (and round-off permits). Convergence of the intervals can be accelerated by using a zero-finder such as zeroin [27, 139], Newton's method, Rayleigh quotient iteration <ref> [17, 134] </ref>, Laguerre's method, or other methods [130], to choose as an approximate zero of d n or p n , i.e. an approximate eigenvalue of T . There is parallelism both within Algorithm 18 and by running Algorithm 17 or 18 simultaneously for many values of .
Reference: [135] <author> C-C. Lin and E. Zmijewski. </author> <title> A parallel algorithm for computing the eigenvalues of an unsymmetric matrix on an SIMD mesh of processors. </title> <institution> Department of Computer Science TRCS 91-15, University of California, Santa Barbara, </institution> <address> CA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer <ref> [13, 135, 144] </ref> In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> One such function f is the sign function <ref> [13, 108, 120, 135, 168, 190] </ref> which maps points with positive real part to +1 and those with negative real part to 1; adding 1 to this function then maps eigenvalues in the right half plane to 2 and in the left plane to 0, as desired.
Reference: [136] <author> J. Liu. </author> <title> Computational models and task scheduling for parallel sparse Cholesky factorization. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: The optimal choice of task size depends on the tradeoff between communication costs and the load balance across processors. We follow Liu <ref> [136] </ref> in identifying three potential levels of granularity in a parallel implementation of Cholesky factorization: 1. fine-grain, in which each task consists of only one or two floating point operations, such as a multiply-add pair, 2. medium-grain, in which each task is a single column operation, such as cmod or cdiv,
Reference: [137] <author> J. Liu. </author> <title> Reordering sparse matrices for parallel elimination. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 73-91, </pages> <year> 1989. </year>
Reference-contexts: And just as the fill in the Cholesky factor is very sensitive to the ordering of the matrix, so is the structure of the elimination tree. This suggests that we should choose an ordering to enhance parallelism, and indeed this is possible (see, e.g., <ref> [111, 128, 137] </ref>), but such an objective may conflict to some degree with preservation of sparsity. Roughly speaking, sparsity and parallelism are largely compatible, since the large-grain parallelism is due to sparsity in the first place. <p> Again, this is typical of minimum degree orderings. In view of this property, Liu <ref> [137] </ref> has developed an interesting strategy for further reordering of an initial minimum degree ordering that preserves fill while reducing the height of the elimination tree. with all edges incident upon nodes in S, disconnects G (A) into two remaining subgraphs.
Reference: [138] <author> J. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 </volume> <pages> 134-172, </pages> <year> 1990. </year> <month> 74 </month>
Reference-contexts: The elimination tree, which we denote by T (A), is a spanning tree for the filled graph F (A). The many uses of the elimination tree in analyzing and organizing sparse Cholesky factorization are surveyed in <ref> [138] </ref>. We will illustrate these concepts pictorially in several examples below. 7.3 Sparse Factorization There are three basic types of algorithms for Cholesky factorization, depending on which of the three indices is placed in the outer loop: 1.
Reference: [139] <author> S.-S. Lo, B. Phillipe, and A. Sameh. </author> <title> A multiprocessor algorithm for the symmetric eigenproblem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8(2) </volume> <pages> 155-165, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: eigenvectors takes O (n 3 ) flops but updating T just O (n 2 ), we succeed in parallelizing the majority of the computational work. 6.3.2 Bisection and Inverse Iteration One of the two most promising methods is (accelerated) bisection for the eigenvalues, followed by inverse iteration for the eigenvectors <ref> [109, 139] </ref>. <p> By continually subdividing intervals 33 containing eigenvalues, we can compute eigenvalue bounds as tight as we like (and round-off permits). Convergence of the intervals can be accelerated by using a zero-finder such as zeroin <ref> [27, 139] </ref>, Newton's method, Rayleigh quotient iteration [17, 134], Laguerre's method, or other methods [130], to choose as an approximate zero of d n or p n , i.e. an approximate eigenvalue of T .
Reference: [140] <author> R. Lucas, W. Blank, and J. Tieman. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Trans. Computer Aided Design, </journal> <volume> CAD-6:981-991, </volume> <year> 1987. </year>
Reference-contexts: As a consequence, multifrontal methods are difficult to specify succinctly, so we will not attempt to do so here, but note that multifrontal methods have been implemented for both shared-memory (e.g., [19, 68]) and distributed-memory (e.g., <ref> [94, 140] </ref>) parallel computers, and are among the most effective methods known for sparse factorization in all types of computational environments. For a unified description and comparison of parallel fan-in, fan-out, and multifrontal methods, see [7].
Reference: [141] <author> F. Luk and H. Park. </author> <title> On parallel Jacobi orderings. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(1) </volume> <pages> 18-26, </pages> <year> 1989. </year>
Reference-contexts: The question remains of the order in which to apply the simultaneous rotations to achieve quick convergence. A number of good parallel orderings have been developed and shown to have the same convergence properties as the usual serial implementations <ref> [141, 182] </ref>; we illustrate one here. Assume we have distributed n = 8 columns on p = 4 processors, two per processor.
Reference: [142] <author> S. C. Ma, M. Patrick, and D. Szyld. </author> <title> A parallel, hybrid algorithm for the generalized eigenproblem. </title> <editor> In Garry Rodrigue, editor, </editor> <booktitle> Parallel Processing for Scientific Computing, chapter 16, </booktitle> <pages> pages 82-86. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Other ways to count the eigenvalues in intervals have been proposed as well [121, 192], although these are more complicated than either Algorithm 17 or 18. There have also been generalizations to the band definite generalized symmetric eigenvalue problem <ref> [142] </ref>. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others [16, 62, 97, 109, 114, 187].
Reference: [143] <author> N. K. Madsen, G. H. Rodrigue, and J. I. Karush. </author> <title> Matrix multiplication by diagonals on a vector/parallel processor. </title> <journal> Inform. Process.Lett., </journal> <volume> 5 </volume> <pages> 41-45, </pages> <year> 1976. </year>
Reference-contexts: For p j one often selects a Chebychev polynomial, which requires some information on the spectrum of A. Finally we point out the possibility of using the truncated Neumann series for the approximate inverse of A, or parts of L and U . Madsen et al. <ref> [143] </ref> discuss approximate inversion of A, which from the implementation point of view is equivalent to polynomial preconditioning. In [197] the use of truncated Neumann series for removing some of the recurrences in the triangular solves is discussed.
Reference: [144] <author> A. N. Malyshev. </author> <title> Parallel aspects of some spectral problems in linear algebra. </title> <institution> Dept. of Numerical Mathematics Report NM-R9113, Centre for Mathematics and Computer Science, </institution> <address> Amsterdam, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer <ref> [13, 135, 144] </ref> In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Other higher order convergent schemes exist, but they can be more expensive to implement as well [120, 156]. Another scheme which divides the spectrum between the eigenvalues inside and outside the unit circle is given in <ref> [144] </ref>. If the eigenvalues are known to be real (as when the matrix is symmetric), we need only construct a function f which maps different parts of the real axis to 0 and 1 instead of the entire left and right half planes. <p> At this point, iterative refinement could be used to improve the factorization [46]. These methods apply to the generalized nonsymmetric eigenproblem as well <ref> [13, 144] </ref>. 7 Direct Methods for Sparse Linear Systems 7.1 Cholesky Factorization In this section we discuss parallel algorithms for solving sparse systems of linear equations by direct methods.
Reference: [145] <author> V. Mehrmann. </author> <title> Divide and conquer methods for block tridiagonal systems. </title> <type> Technical Report Bericht Nr. 68, </type> <institution> Inst. fuer Geometrie und Prakt. Math., Aachen, </institution> <year> 1991. </year>
Reference-contexts: There are other variants of the divide and conquer approach, which move the fill-in into other columns of the subblocks, or which are more stabile numerically. For example, in <ref> [145] </ref> the matrix is split into a block diagonal matrix and a remainder via rank-one updates. 5 Least squares problems Most algorithms for finding the x minimizing kAx bk 2 require computing a QR decomposition of A, where Q is orthogonal and R is upper triangular.
Reference: [146] <author> U. Meier. </author> <title> A parallel partition method for solving banded systems of linear equations. </title> <journal> Parallel Comput., </journal> <volume> 2 </volume> <pages> 33-43, </pages> <year> 1985. </year>
Reference-contexts: For the recurrence we need some data communication between processors. However, for k large enough with respect to n=k, one can attain speedups close to 2k=5 for this algorithm on a k processor system [203]. For a generalization of the divide and conquer approach for banded systems, see <ref> [146] </ref>; the data 28 transport aspects for distributed memory machines have been discussed in [151]. There are other variants of the divide and conquer approach, which move the fill-in into other columns of the subblocks, or which are more stabile numerically.
Reference: [147] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Math.Comp., </journal> <volume> 31 </volume> <pages> 148-162, </pages> <year> 1977. </year>
Reference-contexts: In this case the preconditioner K = LU , where L and U have a sparsity pattern equal or close to the sparsity pattern of the corresponding parts of A (L is lower triangular, U is upper triangular). For details see <ref> [95, 147, 148] </ref>. Solving Kw = r leads to solving successively Lz = r and U w = z. These triangular solves lead to recurrence relations which are not easily parallelized. We will now discuss a number of approaches to obtain parallelism in the preconditioning part. 1. Reordering the computations.
Reference: [148] <author> J. A. Meijerink and H. A. van der Vorst. </author> <title> Guidelines for the usage of incomplete decompositions in solving sets of linear equations as they occur in practical problems. </title> <journal> J. Comp. Phys., </journal> <volume> 44 </volume> <pages> 134-155, </pages> <year> 1981. </year>
Reference-contexts: In this case the preconditioner K = LU , where L and U have a sparsity pattern equal or close to the sparsity pattern of the corresponding parts of A (L is lower triangular, U is upper triangular). For details see <ref> [95, 147, 148] </ref>. Solving Kw = r leads to solving successively Lz = r and U w = z. These triangular solves lead to recurrence relations which are not easily parallelized. We will now discuss a number of approaches to obtain parallelism in the preconditioning part. 1. Reordering the computations.
Reference: [149] <author> G. Meurant. </author> <title> The block preconditioned conjugate gradient method on vector computers. </title> <journal> BIT, </journal> <volume> 24 </volume> <pages> 623-633, </pages> <year> 1984. </year>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [33, 149, 150, 198] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In the above algorithm there are two such synchronization points, namely the computation of both inner products. <p> Several authors [33, 149, 150, 198] have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In the above algorithm there are two such synchronization points, namely the computation of both inner products. Meurant <ref> [149] </ref> (see also [171]) has proposed a variant in which there is only one synchronization point, however at the cost of possibly reduced numerical stability, and one additional inner product. In this scheme the ratio between computations and memory references is about 2.
Reference: [150] <author> G. Meurant. </author> <title> Numerical experiments for the preconditioned conjugate gradient method on the CRAY X-MP/2. </title> <type> Technical Report LBL-18023, </type> <institution> University of California, </institution> <address> Berke-ley, CA, </address> <year> 1984. </year>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [33, 149, 150, 198] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In the above algorithm there are two such synchronization points, namely the computation of both inner products.
Reference: [151] <author> P. H. Michielse and H. A. van der Vorst. </author> <title> Data transport in Wang's partition method. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 87-95, </pages> <year> 1988. </year>
Reference-contexts: For parallel computers, the parallelism in eliminating these subdiagonal blocks is relatively fine-grained compared with the more coarse-grained parallelism in the first step of the algorithm. Furthermore, on distributed memory machines the data for each subdiagonal block has to be spread over all processors. In <ref> [151] </ref> it has been shown that this limits the performance of the algorithm, the speedup being bounded by the ratio of computational speed and communication speed. This ratio is often very low [151]. <p> In <ref> [151] </ref> it has been shown that this limits the performance of the algorithm, the speedup being bounded by the ratio of computational speed and communication speed. This ratio is often very low [151]. The other approach is to first eliminate successively the last nonzero elements in the subdiagonal blocks ~ L j;j1 . This can be done with a short recurrence of length n=k 1, after which all fill-in can be eliminated in parallel. <p> For a generalization of the divide and conquer approach for banded systems, see [146]; the data 28 transport aspects for distributed memory machines have been discussed in <ref> [151] </ref>. There are other variants of the divide and conquer approach, which move the fill-in into other columns of the subblocks, or which are more stabile numerically.
Reference: [152] <author> M. Mu and J. Rice. </author> <title> A grid based subtree-subcube assignment strategy for solving partial differential equations on hypercubes. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13 </volume> <pages> 826-839, </pages> <year> 1992. </year>
Reference-contexts: In particular, performing the column updates one at a time by the receiving processors results in unnecessarily high communication frequency and volume, and in a relatively inefficient computational inner loop. The communication requirements can be reduced by careful mapping and by aggregating updating information over subtrees (see, e.g., <ref> [93, 152, 215] </ref>), but even with this improvement, the fan-out algorithm is usually not competitive with other algorithms presented below.
Reference: [153] <author> J. M. Ortega. </author> <title> Introduction to Parallel and Vector Solution of Linear Systems. </title> <publisher> Plenum Press, </publisher> <address> New York and London, </address> <year> 1988. </year> <month> 75 </month>
Reference-contexts: If the bandwidth is wide enough, however, the techniques of the previous sections still apply [65, 75]. The problem of solving banded linear systems with a narrow band has been studied by many authors, see for instance the references in <ref> [79, 153] </ref>. We will only sketch some of the main ideas and we will do so for rather simple problems. The reader should keep in mind that these ideas can easily be generalized for more complicated situations, and many have appeared in the literature. <p> Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly larger than the scalar speed <ref> [153, 41, 177] </ref>. For distributed memory computers the method requires too much data movement for the reduced system to be practical. However, the method is easily generalized to one with more parallelism. <p> It can also be viewed as a "demand-driven" algorithm, since the inner products that affect a given column are not accumulated until actually needed to modify and complete that column. For this reason, Ortega <ref> [153] </ref> terms column-Cholesky a "delayed-update" algorithm. It is also sometimes referred to as a "fan-in" algorithm, since the basic operation is to combine the effects of multiple previous columns on a single target column. The column-Cholesky algorithm is the most commonly used method in commercially available sparse matrix packages. <p> It can also be viewed as a "data-driven" algorithm, since each new column is used as soon as it is completed to make all modifications to all the subsequent columns it affects. For this reason, Ortega <ref> [153] </ref> terms submatrix-Cholesky an "immediate-update" algorithm. It is also sometimes referred to as a "fan-out" algorithm, since the basic operation is for a single column to affect multiple subsequent columns. We will see that these characterizations of the column-Cholesky and submatrix-Cholesky algorithms have important implications for parallel implementations.
Reference: [154] <author> M.H.C. Paardekooper. </author> <title> A quadratically convergent parallel Jacobi process for diago-nally dominant matrices with distinct eigenvalues. </title> <journal> J. Comput. Appl. Math., </journal> <volume> 27 </volume> <pages> 3-16, </pages> <year> 1989. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Alternatively, one can try to drive the matrix to be normal (AA T = A T A), at which point an orthogonal Jacobi method can be used to drive it to diagonal form; this still does not get around the problem of (nearly) nontrivial Jordan blocks <ref> [71, 154, 174, 181, 206] </ref>. On the other hand, if the matrix has distinct eigenvalues, asymptotic quadratic convergence is achieved [181]. Using n 2 processors arranged in a mesh, these algorithms can be implemented in time O (n log n) per sweep.
Reference: [155] <author> C. C. Paige. </author> <title> Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix. </title> <journal> J. Inst. Math. Appl., </journal> <volume> 18 </volume> <pages> 341-349, </pages> <year> 1976. </year>
Reference-contexts: However, the eigensystem of T k is very sensitive to loss of orthogonality in the q m vectors. For the standard Lanczos method this loss of orthogonality goes hand in hand with the convergence of Ritz values and leads to multiple eigenvalues of T k (see <ref> [155, 160] </ref>), and so can be accounted for, for instance by selective reorthogonalization for which the converged Ritz vectors are required [160].
Reference: [156] <author> P. Pandey, C. Kenney, and A. Laub. </author> <title> A parallel algorithm for the matrix sign function. </title> <journal> Inter. J. High Speed Comput., </journal> <volume> 2(2) </volume> <pages> 181-191, </pages> <year> 1990. </year>
Reference-contexts: It converges more slowly as eigenvalues approach the imaginary axis, and is in fact noncon-vergent if there are imaginary eigenvalues, as may be expected since the sign function is discontinuous there. Other higher order convergent schemes exist, but they can be more expensive to implement as well <ref> [120, 156] </ref>. Another scheme which divides the spectrum between the eigenvalues inside and outside the unit circle is given in [144].
Reference: [157] <author> B. Parlett. </author> <title> private communication. </title>
Reference-contexts: If we simply do inverse iteration without communication, the speedup will be nearly perfect. However, we cannot guarantee orthogonality of eigenvectors of clustered eigenvalues [113], which currently seems to require reorthogonalization of eigenvectors within clusters (other methods are under investigation <ref> [157] </ref>). We can certainly reorthogonalize against eigenvectors of nearby eigenvalues stored on the same processor without communication, or even against those of neighboring processors with little communication; this leads to a tradeoff between orthogonality on the one hand and communication and load balance on the other.
Reference: [158] <author> B. Parlett. </author> <title> Reduction to tridiagonal form and minimal realizations. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(2) </volume> <pages> 567-593, </pages> <year> 1992. </year>
Reference-contexts: This method is attractive because finding eigenvalues of a tridiagonal matrix (even nonsymmetric) is much faster than for a Hessenberg matrix [212]. The drawback is that reduction to tridiagonal form may require very ill-conditioned similarity transformations, and may even break down <ref> [158] </ref>. Breakdown can be avoided by restarting the process with different initializing vectors, or by accepting a "bulge" in the tridiagonal form. This happens with relatively low probability, but keeps the algorithm from being fully reliable.
Reference: [159] <author> B. Parlett and V. Fernando. </author> <title> Accurate singular values and differential QD algorithms. </title> <institution> Math Department PAM-554, University of California, Berkeley, </institution> <address> CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Interesting linear algebra problems that can be cast in this way include tridiagonal Gaussian elimination, solving bidiagonal linear systems of equations, Sturm sequence evaluation for the symmetric tridiagonal eigenproblem, and the bidiagonal dqds algorithm for singular values <ref> [159] </ref>; we discuss some of these below. The numerical stability of these procedures remains open, although it is often good in practice [192]. We now turn to the principle of locality.
Reference: [160] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1980. </year>
Reference-contexts: For larger linear systems, they observed speedups close to 2:5. 9 Iterative Methods for Eigenproblems The oldest iterative scheme for determining a few dominant eigenvalues and corresponding eigenvectors of a matrix A is the power method <ref> [160] </ref>: Algorithm 28: Power Method select x 0 with kx 0 k 2 = 1 k = 0 repeat k = k + 1 = ky k k 2 until x k converges If the eigenvalue of A of maximum modulus is well separated from the others, then x k converges <p> This can be done by either short recurrences or long recurrences. The short (3-term) recurrence is known as the Lanczos method. When A is symmetric this leads to an algorithm with can efficiently compute many, if not all, eigenvalues and eigenvectors <ref> [160] </ref>. In fact, the CG method (and Bi-CG) can be viewed as a solution process on top of Lanczos. The long recursion process is known as Arnoldi's method [5], which we have seen already as the underlying orthogonalization procedure for GMRES. <p> Not surprisingly, a short discussion on parallelizing the Lanczos and Arnoldi methods would have much in common with our earlier discussions of CG and GMRES. 9.1 The Lanczos method Our version of the Lanczos algorithm is a slightly changed version of a scheme presented in <ref> [160] </ref> (the change has been made in order to remove one synchronization point: in the original scheme r k1 is scaled by fi k1 before computing Ar k1 ): Algorithm 29: Lanczos's Method select r 0 6= 0; q 0 = 0 k = 0 repeat k = k + 1 <p> However, the eigensystem of T k is very sensitive to loss of orthogonality in the q m vectors. For the standard Lanczos method this loss of orthogonality goes hand in hand with the convergence of Ritz values and leads to multiple eigenvalues of T k (see <ref> [155, 160] </ref>), and so can be accounted for, for instance by selective reorthogonalization for which the converged Ritz vectors are required [160]. <p> For the standard Lanczos method this loss of orthogonality goes hand in hand with the convergence of Ritz values and leads to multiple eigenvalues of T k (see [155, 160]), and so can be accounted for, for instance by selective reorthogonalization for which the converged Ritz vectors are required <ref> [160] </ref>.
Reference: [161] <author> B. N. Parlett and J. K. Reid. </author> <title> Tracking the progress of the Lanczos algorithm for large symmetric eigenproblems. </title> <journal> IMA J. Num. Anal., </journal> <volume> 1 </volume> <pages> 135-155, </pages> <year> 1981. </year>
Reference-contexts: For small k there will not be much to do in parallel, but we also need not compute the eigensystem of T k for each k, nor check convergence for each k. An elegant scheme for tracking convergence of the Ritz values is discussed in <ref> [161] </ref>.
Reference: [162] <author> S. G. Petiton. </author> <title> Parallel subspace method for non-Hermitian eigenproblems on the Connection Machine (CM2). </title> <journal> Appl. Num. Math., </journal> <volume> 10, </volume> <year> 1992. </year>
Reference-contexts: This process is known as subspace iteration, a generalization of the power method. For a description, as well as a discussion of its performance on the Connection Machine, see <ref> [162] </ref>. A different approach for computing an invariant subspace of order k, based on Arnoldi's process, is discussed in [186].
Reference: [163] <author> C. Pommerell. </author> <title> Solution of large unsymmetric systems of linear equations. </title> <type> PhD thesis, </type> <institution> Swiss Federal Institute of Technology, </institution> <address> Zurich, </address> <year> 1992. </year>
Reference-contexts: If the number of connections to these neighboring blocks is small compared to the number of internal nodes, then the communication time can be overlapped with computational work. For more detailed discussions on implementation aspects on distributed memory systems, see <ref> [42, 163] </ref>. The preconditioning part is often the most problematic part in a parallel environment. Incomplete decompositions of A form a popular class of preconditionings, in the context of solving discretized PDE's.
Reference: [164] <author> A. Pothen, S. Jha, and U. Vemulapati. </author> <title> Orthogonal factorization on a distributed memory multiprocessor. </title> <booktitle> In Hypercube Multiprocessors 1987, </booktitle> <pages> pages 587-598, </pages> <address> Knoxville, TN, </address> <year> 1987. </year> <note> SIAM. </note>
Reference-contexts: An interesting alternative that works with the same data layouts is based on Givens rotations <ref> [35, 164] </ref>. We consider just the first block column in the block scattered layout, where each of a subset of the processors owns a set of p r fi r subblocks of the block column evenly distributed over the column. <p> So in log 2 p of these steps, the first column has been reduced to a single r fi r triangle, and the algorithm moves on to the next block column. Other Givens based algorithm have been proposed, but seem to require more commu nication that this one <ref> [164] </ref>. 6 Eigenproblems and the singular value decomposition 6.1 General comments The standard serial algorithms for computing the eigendecomposition of a symmetric matrix A, a general matrix B, or the singular value decomposition (SVD) of a general matrix C have 30 the same two-phase structure: apply orthogonal transformations to reduce the
Reference: [165] <author> D. Priest. </author> <title> Algorithms for arbitrary precision floating point arithmetic. </title> <editor> In P. Kornerup and D. Matula, editors, </editor> <booktitle> Proceedings of the 10th Symposium on Computer Arithmetic, </booktitle> <pages> pages 132-145, </pages> <address> Grenoble, France, June 26-28 1991. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: When the input is already in double precision (or whatever is the largest precision supported by the machine), then quadruple is needed, which may be simulated using double, provided double is accurate enough <ref> [45, 165] </ref>. Recently, however, Gu and Eisenstat [97] have found a new algorithm that makes this unnecessary. There are two types of parallelism available in this algorithm and both must be exploited to speed up the whole algorithm [62, 109].
Reference: [166] <author> G. Radicati di Brozolo and Y. Robert. </author> <title> Vector and parallel CG-like algorithms for sparse non-symmetric systems. </title> <type> Technical Report 681-M, </type> <institution> IMAG/TIM3, Grenoble, </institution> <year> 1987. </year>
Reference-contexts: Again, this may not always reduce the overall solution time, since the effects of increased parallelism are more than undone by an increased number of iteration steps. In order to reduce this effect, it is suggested in <ref> [166] </ref> to construct incomplete decompositions on slightly overlapping domains. This requires communication similar to that of matrix vector products. In [166] results are reported on a 6-processor shared memory system (IBM3090), and speedups close to 6 have been observed. <p> In order to reduce this effect, it is suggested in <ref> [166] </ref> to construct incomplete decompositions on slightly overlapping domains. This requires communication similar to that of matrix vector products. In [166] results are reported on a 6-processor shared memory system (IBM3090), and speedups close to 6 have been observed. The problems with parallelism in the preconditioner have led to searches for other pre-conditioners. Often simple diagonal scaling is an adequate preconditioner, and of course this is trivially parallelizable.
Reference: [167] <author> Y. Robert. </author> <title> The impact of vector and parallel architectures on the Gaussian elimination algorithm. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: Other permutations of the nested loops lead to different algorithms, which depend on the BLAS for matrix-vector multiplication and solving a triangular system instead of rank-1 updating <ref> [3, 167] </ref>; which is faster depends on the relative speed of these on each machine. <p> Given this function, it may be minimized as a function of b 1 , b 2 , p 1 and p 2 . Some theoretical analyses of this sort for special cases may be found in <ref> [167] </ref> and the references therein. See also [60, 64].
Reference: [168] <author> J. Roberts. </author> <title> Linear model reduction and solution of the algebraic Riccati equation. </title> <journal> Inter. J. Control, </journal> <volume> 32 </volume> <pages> 677-687, </pages> <year> 1980. </year>
Reference-contexts: One such function f is the sign function <ref> [13, 108, 120, 135, 168, 190] </ref> which maps points with positive real part to +1 and those with negative real part to 1; adding 1 to this function then maps eigenvalues in the right half plane to 2 and in the left plane to 0, as desired. <p> The only operations we can easily perform on (dense) matrices are multiplication and inversion, so in practice f must be a rational function. A globally, asymptotically quadrat-ically convergent iteration to compute the sign-function of B is B i+1 = (B i + B 1 i )=2 <ref> [108, 168, 190] </ref>; this is simply Newton's method applied to B 2 = I, and can also be seen to equivalent to repeated squaring (the power method) of the Cayley transform of B.
Reference: [169] <author> C. Romine and J. Ortega. </author> <title> Parallel solution of triangular systems of equations. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 109-114, </pages> <year> 1988. </year> <month> 76 </month>
Reference-contexts: Designing such an algorithm on a distributed memory machine is harder, because the fewer floating point operations performed (O (n 2 ) instead of O (n 3 )) make it harder to mask the communication; see <ref> [73, 100, 129, 169] </ref>. 4.3 Clever but impractical parallel algorithms for solving Ax = b The theoretical literature provides us with a number of apparently fast but ultimately unattractive algorithms for solving Ax = b.
Reference: [170] <author> E. Rothberg and A. Gupta. </author> <title> Fast sparse matrix factorization on modern workstations. </title> <type> Technical Report STAN-CS-89-1286, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1989. </year>
Reference-contexts: For example, such techniques have been used to attain very high performance for sparse factorization on conventional vector supercomputers [9] and on RISC workstations <ref> [170] </ref>. 7.4 Parallelism in Sparse Factorization We now examine in greater detail the opportunities for parallelism in sparse Cholesky factorization and various algorithms for exploiting it.
Reference: [171] <author> Y. Saad. </author> <title> Practical use of polynomial preconditionings for the conjugate gradient method. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 </volume> <pages> 865-881, </pages> <year> 1985. </year>
Reference-contexts: Still another approach is to use polynomial preconditioning: w = p j (A)r, i.e., K 1 = p j (A), for some suitable j-th degree polynomial. This preconditioner can be implemented by forming only matrix vector products, which, depending on the structure of A, are easier to parallelize <ref> [171] </ref>. For p j one often selects a Chebychev polynomial, which requires some information on the spectrum of A. Finally we point out the possibility of using the truncated Neumann series for the approximate inverse of A, or parts of L and U . <p> In the above algorithm there are two such synchronization points, namely the computation of both inner products. Meurant [149] (see also <ref> [171] </ref>) has proposed a variant in which there is only one synchronization point, however at the cost of possibly reduced numerical stability, and one additional inner product. In this scheme the ratio between computations and memory references is about 2.
Reference: [172] <author> Y. Saad. </author> <title> Krylov subspace methods on supercomputers. </title> <type> Technical report, </type> <institution> RIACS, Moffett Field, </institution> <address> CA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: The authors claim success in using this approach without serious stability problems for small values of s. Nevertheless, it seems that s-step CG still has a bad reputation <ref> [172] </ref> because of these problems. However, a similar approach, suggested by Chronopoulos and Kim [34] for other processes such as GMRES, seems to be more promising. <p> In the above algorithm, this is done using Level 1 BLAS, which may be quite inefficient. To incorporate Level 2 BLAS we can do either Householder orthogonalization or classical Gram-Schmidt twice (which mitigates classical Gram-Schmidt's potential instability <ref> [172] </ref>). Both approaches significantly increase the computational work and do not remove the synchronization and data locality problems completely.
Reference: [173] <author> Y. Saad and M. H. Schultz. </author> <title> Conjugate Gradient-like algorithms for solving nonsymmetric linear systems. </title> <journal> Math. of Comp., </journal> <volume> 44 </volume> <pages> 417-424, </pages> <year> 1985. </year>
Reference-contexts: To this end, one might split the computation in (4) in two parts. The first part would be computed in parallel with (3), and the second part with i+1 . 8.3 Parallelism and data locality in GMRES GMRES, proposed by Saad and Schultz <ref> [173] </ref>, is a CG-like method for solving general non-singular linear systems Ax = b. GMRES minimizes the residual over the Krylov subspace span [r 0 ; Ar 0 ; A 2 r 0 ; :::; A i r 0 ], with r 0 = b Ax 0 . <p> In order to limit memory requirements (since all basis vectors must be stored), GMRES is restarted after each cycle of m iteration steps; this is called GMRES (m). A slightly simplified version of GMRES (m) with preconditioning K is as follows (for details, see <ref> [173] </ref>): Algorithm 27: GMRES (m) x 0 is an initial guess; r = b Ax 0 ; for j = 1; 2; :::: Solve for w in Kw = r; v 1 = w=kwk 2 ; Solve for w in Kw = Av i ; for k = 1; :::; i
Reference: [174] <author> A. Sameh. </author> <title> On Jacobi and Jacobi-like algorithms for a parallel computer. </title> <journal> Math. Comp., </journal> <volume> 25 </volume> <pages> 579-590, </pages> <year> 1971. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Alternatively, one can try to drive the matrix to be normal (AA T = A T A), at which point an orthogonal Jacobi method can be used to drive it to diagonal form; this still does not get around the problem of (nearly) nontrivial Jordan blocks <ref> [71, 154, 174, 181, 206] </ref>. On the other hand, if the matrix has distinct eigenvalues, asymptotic quadratic convergence is achieved [181]. Using n 2 processors arranged in a mesh, these algorithms can be implemented in time O (n log n) per sweep.
Reference: [175] <author> A. Sameh. </author> <title> On some parallel algorithms on a ring of processors. </title> <journal> Comput. Phys. Comm., </journal> <volume> 37 </volume> <pages> 159-166, </pages> <year> 1985. </year>
Reference-contexts: This cannot be directly combined with blocking as we have just described it, and so instead pivoting algorithms which only look among locally stored columns if possible have been developed [24, 25]. Other shared memory algorithms based on Givens rotations have also been developed <ref> [35, 86, 175] </ref>, although these do not seem superior on shared memory machines.
Reference: [176] <author> A. Sameh and R. Brent. </author> <title> Solving triangular systems on a parallel computer. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 14 </volume> <pages> 1101-1113, </pages> <year> 1977. </year>
Reference-contexts: Each parallel step involves multiplying n fi n matrices (which are initially quite sparse, but fill up), and so takes about log 2 n parallel substeps, for a total of log 2 2 n. The error analysis of this algorithm <ref> [176] </ref> yields an error bound proportional to (T ) 3 " where (T ) = kT k kT 1 k is the condition number and " is machine precision; this is in contrast to the error bound (T )" for the usual algorithm.
Reference: [177] <author> J. J. F. M. Schlichting and H. A. van der Vorst. </author> <title> Solving bidiagonal systems of linear equations on the CDC CYBER 205. </title> <type> Technical Report NM-R8725, </type> <institution> CWI, </institution> <address> Amsterdam, the Netherlands, </address> <year> 1987. </year>
Reference-contexts: Since the amount of serial work is halved in each step by completely parallel (or vectorizable) operations, this approach has been successfully applied on vector supercomputers, especially when the vector speed of the machine is significantly larger than the scalar speed <ref> [153, 41, 177] </ref>. For distributed memory computers the method requires too much data movement for the reduced system to be practical. However, the method is easily generalized to one with more parallelism.
Reference: [178] <author> J. J. F. M. Schlichting and H. A. van der Vorst. </author> <title> Solving 3D block bidiagonal linear systems on vector computers. </title> <journal> Journal of Comp. and Appl. Math., </journal> <volume> 27 </volume> <pages> 323-330, </pages> <year> 1989. </year>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see [8, 56, 201, 202]). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach <ref> [178, 201, 202] </ref>. The disadvantage is that the data needs to be redistributed over the processors, since the grid points, which correspond to a hyperplane in the grid, are located quite irregularly in the array. For shared memory machines this also leads to reduced performance because of indirect addressing.
Reference: [179] <author> R. Schreiber and C. Van Loan. </author> <title> A storage efficient WY representation for products of Householder transformations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 53-57, </pages> <year> 1989. </year>
Reference-contexts: To convert to level 3 BLAS requires the observation that one can write Q b Q b1 Q 1 = I U T U T where U = [u 1 ; :::; u b ] is m fi b, and T is b fi b and triangular <ref> [179] </ref>; for historical reasons this is called a compact WY transformation.
Reference: [180] <author> M. K. Seager. </author> <title> Parallelizing conjugate gradient for the CRAY X-MP. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 35-47, </pages> <year> 1986. </year>
Reference-contexts: Moreover, the ratio of computation to communication may be more unfavorable. 55 3. Forced parallelism. Parallelism can also be forced by simply neglecting couplings to unknowns residing in other processors. This is like block Jacobi preconditioning, in which the blocks may be decomposed in incomplete form <ref> [180] </ref>. Again, this may not always reduce the overall solution time, since the effects of increased parallelism are more than undone by an increased number of iteration steps. In order to reduce this effect, it is suggested in [166] to construct incomplete decompositions on slightly overlapping domains.
Reference: [181] <author> G. Shroff. </author> <title> A parallel algorithm for the eigenvalues and eigenvectors of a general complex matrix. </title> <journal> Num. Math., </journal> <volume> 58 </volume> <pages> 779-805, </pages> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Alternatively, one can try to drive the matrix to be normal (AA T = A T A), at which point an orthogonal Jacobi method can be used to drive it to diagonal form; this still does not get around the problem of (nearly) nontrivial Jordan blocks <ref> [71, 154, 174, 181, 206] </ref>. On the other hand, if the matrix has distinct eigenvalues, asymptotic quadratic convergence is achieved [181]. Using n 2 processors arranged in a mesh, these algorithms can be implemented in time O (n log n) per sweep. <p> On the other hand, if the matrix has distinct eigenvalues, asymptotic quadratic convergence is achieved <ref> [181] </ref>. Using n 2 processors arranged in a mesh, these algorithms can be implemented in time O (n log n) per sweep.
Reference: [182] <author> G. Shroff and R. Schreiber. </author> <title> On the convergence of the cyclic Jacobi method for parallel block orderings. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 10 </volume> <pages> 326-346, </pages> <year> 1989. </year>
Reference-contexts: The question remains of the order in which to apply the simultaneous rotations to achieve quick convergence. A number of good parallel orderings have been developed and shown to have the same convergence properties as the usual serial implementations <ref> [141, 182] </ref>; we illustrate one here. Assume we have distributed n = 8 columns on p = 4 processors, two per processor. <p> It is possible to use the symmetric-indefinite decomposition of an indefinite symmetric matrix in the same way [184]. Jacobi done in this style is a rather fine grain algorithm, operating on pairs of columns, and so cannot exploit higher level BLAS. One can instead use block Jacobi algorithms <ref> [22, 182] </ref>, which work on blocks, and apply the resulting orthogonal matrices to the rest of the matrix using more efficient matrix-matrix multiplication. 6.5 The nonsymmetric eigenproblem Five kinds of parallel methods for the nonsymmetric eigenproblem have been investigated: 1.
Reference: [183] <author> H. Simon. </author> <title> Bisection is not optimal on vector processors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(1) </volume> <pages> 205-209, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: This requires good support for parallel prefix operations, and is not as easy to parallelize as simply having each processor refine different sets of intervals containing different eigenvalues [47]. Within a single processor one can also run Algorithm 17 or 18 for many different by pipelining or vectorizing <ref> [183] </ref>. These many could come from disjoint intervals or from dividing a single interval into more than 2 small ones (multisection); the latter approach appears to be efficient only when a few eigenvalues are desired, so that there are not many disjoint intervals over which to parallelize [183]. <p> pipelining or vectorizing <ref> [183] </ref>. These many could come from disjoint intervals or from dividing a single interval into more than 2 small ones (multisection); the latter approach appears to be efficient only when a few eigenvalues are desired, so that there are not many disjoint intervals over which to parallelize [183]. Achieving good speedup requires load balancing, and this is not always possible to do by statically assigning work to processors.
Reference: [184] <author> I. Slapnicar. </author> <title> Accurate symmetric eigenreduction by a Jacobi method. </title> <type> PhD thesis, </type> <institution> Fernuniversitat - Hagen, Hagen, Germany, </institution> <year> 1992. </year>
Reference-contexts: Recently, however, it has been shown that Jacobi's method can be much more accurate than QR in certain cases <ref> [43, 51, 184] </ref>, which makes it of some value on serial machines. <p> Such a one-sided Jacobi is natural when computing the SVD [98], but requires some preprocessing for the symmetric eigenproblem <ref> [51, 184] </ref>; for example, in the symmetric positive definite case one can perform Cholesky on A to get A = LL T , apply one-sided Jacobi on L or L T to get its (partial) SVD, and then square the singular values to get the eigenvalues of A. <p> It turns out it accelerates convergence to do the Cholesky decomposition with pivoting, and then apply Jacobi to the columns of L rather than the columns of L T [51]. It is possible to use the symmetric-indefinite decomposition of an indefinite symmetric matrix in the same way <ref> [184] </ref>. Jacobi done in this style is a rather fine grain algorithm, operating on pairs of columns, and so cannot exploit higher level BLAS.
Reference: [185] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year> <month> 77 </month>
Reference-contexts: 1 Introduction Accurate and efficient algorithms for many problems in numerical linear algebra have existed for years on conventional serial machines, and there are many portable software libraries that implement them efficiently <ref> [53, 58, 81, 185] </ref>. One reason for this profusion of successful software is the simplicity of the cost model: the execution time of an algorithm is roughly proportional to the number of floating point operations it performs. This simple fact makes it relatively easy to design efficient and portable algorithms.
Reference: [186] <author> D. Sorensen. </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 357-385, </pages> <year> 1992. </year>
Reference-contexts: This process is known as subspace iteration, a generalization of the power method. For a description, as well as a discussion of its performance on the Connection Machine, see [162]. A different approach for computing an invariant subspace of order k, based on Arnoldi's process, is discussed in <ref> [186] </ref>. Here one starts with k steps of Arnoldi to create an initial approximation of the invariant subspace of dimension k corresponding to k desired eigen-values, say the k largest eigenvalues in modulus.
Reference: [187] <author> D. Sorensen and P. Tang. </author> <title> On the orthogonality of eigenvectors computed by divide-and-conquer techniques. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 28(6) </volume> <pages> 1752-1775, </pages> <year> 1991. </year>
Reference-contexts: There have also been generalizations to the band definite generalized symmetric eigenvalue problem [142]. 34 6.3.3 Cuppen's Divide and Conquer Algorithm The third algorithm is a divide and conquer algorithm by Cuppen [39], and later analyzed and modified by many others <ref> [16, 62, 97, 109, 114, 187] </ref>. <p> Work by several authors <ref> [16, 187] </ref> led to the conclusion that i had to be computed to double the input precision in order to get d i i accurately.
Reference: [188] <author> G. W. Stewart. </author> <title> A Jacobi-like algorithm for computing the Schur decomposition of a non-Hermitian matrix. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6 </volume> <pages> 853-864, </pages> <year> 1985. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> There are two basic kinds of transformations used. Methods that use only orthogonal transformations maintain numerical stability and converge to Schur canonical form, but converge only linearly at best <ref> [72, 188] </ref>. If nonorthogonal transformations are used, one can try to drive the matrix to diagonal form, but if it is close to having a nontrivial Jordan block, the required similarity transformation will be very ill-conditioned and so stability is lost.
Reference: [189] <author> G. W. Stewart. </author> <title> A parallel implementation of the QR algorithm. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 187-196, </pages> <year> 1987. </year>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> One way to introduce parallelism is to spread the matrix across the processors, but communication costs may exceed the modest computational costs of the row and column operations <ref> [40, 84, 189, 195, 194] </ref>.
Reference: [190] <author> E. Stickel. </author> <title> Separating eigenvalues using the matrix sign function. </title> <journal> Lin. Alg. Appl., </journal> <volume> 148 </volume> <pages> 75-88, </pages> <year> 1991. </year>
Reference-contexts: One such function f is the sign function <ref> [13, 108, 120, 135, 168, 190] </ref> which maps points with positive real part to +1 and those with negative real part to 1; adding 1 to this function then maps eigenvalues in the right half plane to 2 and in the left plane to 0, as desired. <p> The only operations we can easily perform on (dense) matrices are multiplication and inversion, so in practice f must be a rational function. A globally, asymptotically quadrat-ically convergent iteration to compute the sign-function of B is B i+1 = (B i + B 1 i )=2 <ref> [108, 168, 190] </ref>; this is simply Newton's method applied to B 2 = I, and can also be seen to equivalent to repeated squaring (the power method) of the Cayley transform of B. <p> By working on a shifted and squared real matrix, one can divide along lines at an angle of =4 and retain real arithmetic <ref> [13, 108, 190] </ref>. This method is promising because it allows us to work on just that part of the spectrum of interest to the user. It is stable because it applies only orthogonal transformations to B.
Reference: [191] <author> H. S. Stone. </author> <title> An efficient parallel algorithm for the solution of a tridiagonal linear system of equations. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 20 </volume> <pages> 27-38, </pages> <year> 1973. </year>
Reference-contexts: Thus, the original system splits in two independent lower bidiagonal systems of half the size, one for the odd-numbered unknowns, and one for the even-numbered unknowns. This process can be repeated recursively for both new systems, leading to an algorithm known as recursive doubling <ref> [191] </ref>. In Algorithm 2 (section 2.2) it was presented as a special case of parallel prefix. It has been analyzed and generalized for banded systems in [66]. Its significance for modern parallel computers is limited, which we illustrate with the following examples.
Reference: [192] <author> P. Swarztrauber. </author> <title> A parallel algorithm for computing the eigenvalues of a symmetric tridiagonal matrix. </title> <note> to appear in Math. Comp., </note> <year> 1992. </year>
Reference-contexts: The numerical stability of these procedures remains open, although it is often good in practice <ref> [192] </ref>. We now turn to the principle of locality. <p> The numerical stability of the serial implementations of Algorithms 17 [118] and 18 [212] is very good, but that of the parallel prefix algorithm is unknown, although numerical experiments are promising <ref> [192] </ref>. This requires good support for parallel prefix operations, and is not as easy to parallelize as simply having each processor refine different sets of intervals containing different eigenvalues [47]. Within a single processor one can also run Algorithm 17 or 18 for many different by pipelining or vectorizing [183]. <p> Other ways to count the eigenvalues in intervals have been proposed as well <ref> [121, 192] </ref>, although these are more complicated than either Algorithm 17 or 18.
Reference: [193] <author> Thinking Machines Corporation. </author> <title> Connection Machine Model CM-2 Technical Summary, </title> <month> April </month> <year> 1987. </year>
Reference-contexts: Whether or not we can make sense out of results that have overflowed or undergone other exceptions depends on the application; it is true often enough to be quite useful. Now we give some examples of regularity in communication. The CM-2 <ref> [193] </ref> may be thought of in different ways; for us it is convenient to think of it as 2048 processors connected in an 11-dimensional hypercube, with one processor and its memory at each of the 2048 7 corners of the cube, and a physical connection along each edge connecting each corner
Reference: [194] <author> R. van de Geijn. </author> <title> Implementing the QR Algorithm on an Array of Processors. </title> <type> PhD thesis, </type> <institution> University of Maryland, College Park, </institution> <month> August </month> <year> 1987. </year> <institution> Computer Science Department Report TR-1897. </institution>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> One way to introduce parallelism is to spread the matrix across the processors, but communication costs may exceed the modest computational costs of the row and column operations <ref> [40, 84, 189, 195, 194] </ref>. <p> Second, the convergence properties degrade significantly, resulting in more overall work as well [67]. As a result, speedups have been extremely modest. This routine is available in LAPACK as shseqr [2]. Yet another way to introduce parallelism into Hessenberg QR is to pipeline several bulge chasing steps <ref> [194, 209, 210] </ref>. If we have several shifts available, then as soon as one bulge chase is launched from the upper left corner, another one may be launched, and so on. <p> Therefore, we must use "out-of-date" shifts to have enough available to start multiple bulge chases. This destroys the usual local quadratic convergence, but it remains superlinear <ref> [194] </ref>. It has been suggested that choosing the eigenvalues of the bottom right k-by-k submatrix may have superior convergence to just choosing a sequence from the bottom 1-by-1 or 2-by-2 submatrices [209].
Reference: [195] <author> R. van de Geijn and D. Hudson. </author> <title> Efficient parallel implementation of the nonsymmetric QR algorithm. </title> <editor> In J. Gustafson, editor, </editor> <booktitle> Hypercube Concurrent Computers and Applications. ACM, </booktitle> <year> 1989. </year>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> One way to introduce parallelism is to spread the matrix across the processors, but communication costs may exceed the modest computational costs of the row and column operations <ref> [40, 84, 189, 195, 194] </ref>.
Reference: [196] <author> E. Van de Velde. </author> <title> Introduction to concurrent scientific computing. </title> <address> Caltech, Pasadena, CA, </address> <year> 1992. </year>
Reference-contexts: A different approach is to write algorithms that work independent of the location of the data, and rely on the underlying language or run-time system to optimize the necessary communications. This makes code easier to write, but puts a large burden on compiler and run-time system writers <ref> [196] </ref>. 20 0,0 0,1 0,2 0,3 0,0 0,1 0,2 0,3 0,0 0,1 0,2 0,3 0,0 0,1 0,2 0,3 2,0 2,1 2,2 2,3 2,0 2,1 2,2 2,3 2,0 2,1 2,2 2,3 2,0 2,1 2,2 2,3 0,0 0,1 0,2 0,3 0,0 0,1 0,2 0,3 0,0 0,1 0,2 0,3 0,0 0,1 0,2 0,3 2,0
Reference: [197] <author> H. A. van der Vorst. </author> <title> A vectorizable variant of some ICCG methods. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 3 </volume> <pages> 86-92, </pages> <year> 1982. </year>
Reference-contexts: Finally we point out the possibility of using the truncated Neumann series for the approximate inverse of A, or parts of L and U . Madsen et al. [143] discuss approximate inversion of A, which from the implementation point of view is equivalent to polynomial preconditioning. In <ref> [197] </ref> the use of truncated Neumann series for removing some of the recurrences in the triangular solves is discussed. This approach leads to only fine-grained parallelism (vectorization). 8.2 Parallelism and data locality in preconditioned CG To use CG to solve Ax = b, A must be symmetric and positive definite.
Reference: [198] <author> H. A. van der Vorst. </author> <title> The performance of Fortran implementations for preconditioned conjugate gradients on vector computers. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 49-58, </pages> <year> 1986. </year>
Reference-contexts: This means that for this part of the computation only 10=7 floating point operation can be carried out per memory reference on average. Several authors <ref> [33, 149, 150, 198] </ref> have attempted to improve this ratio, and to reduce the number of synchronization points (the points at which computation must wait for communication). In the above algorithm there are two such synchronization points, namely the computation of both inner products.
Reference: [199] <author> H. A. van der Vorst. </author> <title> Analysis of a parallel solution method for tridiagonal linear systems. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 303-311, </pages> <year> 1987. </year>
Reference-contexts: The twisted factorization and subsequent forward and back substitutions with P and Q take as many arithmetic operations as the standard factorization, and can be carried out with twofold parallelism by working from both ends of the matrix simultaneously. For an analysis of this process for tridiagonal systems, see <ref> [199] </ref>. Twisted factorization can be combined with any of the following techniques, often doubling the parallelism.
Reference: [200] <author> H. A. van der Vorst. </author> <title> Large tridiagonal and block tridiagonal linear systems on vector and parallel computers. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 45-54, </pages> <year> 1987. </year>
Reference-contexts: Duff and Meurant [70] have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations <ref> [56, 200, 201] </ref>. Multicolor schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior [52], but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. 55 3. Forced parallelism.
Reference: [201] <author> H. A. van der Vorst. </author> <title> High performance preconditioning. </title> <journal> SIAM J. Sci. Statist. Com-put., </journal> <volume> 10 </volume> <pages> 1174-1185, </pages> <year> 1989. </year> <month> 78 </month>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see <ref> [8, 56, 201, 202] </ref>). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach [178, 201, 202]. <p> For vector computers this leads to a vectorizable preconditioner (see [8, 56, 201, 202]). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach <ref> [178, 201, 202] </ref>. The disadvantage is that the data needs to be redistributed over the processors, since the grid points, which correspond to a hyperplane in the grid, are located quite irregularly in the array. For shared memory machines this also leads to reduced performance because of indirect addressing. <p> Duff and Meurant [70] have carried out numerical experiments for many different orderings, which show that the numbers of iterations may increase significantly for other than lexicographical ordering. Some modest degree of parallelism can be obtained, however, with so-called incomplete twisted factorizations <ref> [56, 200, 201] </ref>. Multicolor schemes with a large number of colors (e.g., 20 to 100) may lead to little or no degradation in convergence behavior [52], but also to less parallelism. Moreover, the ratio of computation to communication may be more unfavorable. 55 3. Forced parallelism.
Reference: [202] <author> H. A. van der Vorst. </author> <title> ICCG and related methods for 3D problems on vector compu-ters. </title> <journal> Computer Physics Communications, </journal> <volume> 53 </volume> <pages> 223-235, </pages> <year> 1989. </year>
Reference-contexts: For vector computers this leads to a vectorizable preconditioner (see <ref> [8, 56, 201, 202] </ref>). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach [178, 201, 202]. <p> For vector computers this leads to a vectorizable preconditioner (see [8, 56, 201, 202]). For coarse grained parallelism this approach is insufficient. By a similar approach more parallelism can be obtained in 3D situations: the so-called hyperplane approach <ref> [178, 201, 202] </ref>. The disadvantage is that the data needs to be redistributed over the processors, since the grid points, which correspond to a hyperplane in the grid, are located quite irregularly in the array. For shared memory machines this also leads to reduced performance because of indirect addressing.
Reference: [203] <author> H. A. van der Vorst. </author> <title> Practical aspects of parallel scientific computing. </title> <journal> Future Generation Computer Systems, </journal> <volume> 4 </volume> <pages> 285-291, </pages> <year> 1989. </year>
Reference-contexts: For the recurrence we need some data communication between processors. However, for k large enough with respect to n=k, one can attain speedups close to 2k=5 for this algorithm on a k processor system <ref> [203] </ref>. For a generalization of the divide and conquer approach for banded systems, see [146]; the data 28 transport aspects for distributed memory machines have been discussed in [151].
Reference: [204] <author> H. A. van der Vorst and K. Dekker. </author> <title> Vectorization of linear recurrence relations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 27-35, </pages> <year> 1989. </year>
Reference-contexts: In the original approach [208], the fill-in in the subdiagonal blocks is eliminated in parallel, or vector mode, for each subdiagonal block (note that each subdiagonal block has only one column with nonzero elements). It has been shown in <ref> [204] </ref> that this leads to very efficient vectorized code for machines such as CRAY, Fujitsu, etc. For parallel computers, the parallelism in eliminating these subdiagonal blocks is relatively fine-grained compared with the more coarse-grained parallelism in the first step of the algorithm.
Reference: [205] <author> H. A. van der Vorst and C. Vuik. GMRESR: </author> <title> A family of nested GMRES methods. </title> <type> Technical Report 91-80, </type> <institution> Delft University of Technology, Faculty of Tech. Math., </institution> <year> 1991. </year>
Reference-contexts: For some applications the additional computation required by Householder orthogonalization is paid for by improved numerical properties: the better orthogonality saves iteration steps. In <ref> [205] </ref> a variant of GMRES is proposed in which the preconditioner itself may be an iterative process, which may help to increase parallel efficiency. Similarly to CG and other iterative schemes, the major computations are matrix-vector computations (with A and K), inner products and vector updates.
Reference: [206] <author> K. Veselic. </author> <title> A quadratically convergent Jacobi-like method for real matrices with complex conjugate eigenvalues. </title> <journal> Num. Math., </journal> <volume> 33 </volume> <pages> 425-435, </pages> <year> 1979. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method <ref> [71, 72, 154, 174, 181, 188, 206] </ref>, 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> Alternatively, one can try to drive the matrix to be normal (AA T = A T A), at which point an orthogonal Jacobi method can be used to drive it to diagonal form; this still does not get around the problem of (nearly) nontrivial Jordan blocks <ref> [71, 154, 174, 181, 206] </ref>. On the other hand, if the matrix has distinct eigenvalues, asymptotic quadratic convergence is achieved [181]. Using n 2 processors arranged in a mesh, these algorithms can be implemented in time O (n log n) per sweep.
Reference: [207] <author> H. F. Walker. </author> <title> Implementation of the GMRES method using Householder transformations. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 9 </volume> <pages> 152-163, </pages> <year> 1988. </year>
Reference-contexts: Compute x m using the h k;i and v i ; r = b Ax m ; if residual r is small enough then quit else (x 0 := x m ;); end j; Another scheme for GMRES, based upon Householder orthogonalization instead of mod-ified Gram-Schmidt, has been proposed in <ref> [207] </ref>. For some applications the additional computation required by Householder orthogonalization is paid for by improved numerical properties: the better orthogonality saves iteration steps. In [205] a variant of GMRES is proposed in which the preconditioner itself may be an iterative process, which may help to increase parallel efficiency.
Reference: [208] <author> H. H. Wang. </author> <title> A parallel method for tridiagonal equations. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 </volume> <pages> 170-183, </pages> <year> 1989. </year>
Reference-contexts: This approach is referred to as a divide and conquer approach. For banded triangular systems it was first suggested in [31], for tridiagonal systems it was proposed by Wang <ref> [208] </ref>. To illustrate, let us apply one parallel elimination step to the lower bidiagonal system Lx = b to eliminate all subdiagonal elements in all diagonal blocks. <p> In the original approach <ref> [208] </ref>, the fill-in in the subdiagonal blocks is eliminated in parallel, or vector mode, for each subdiagonal block (note that each subdiagonal block has only one column with nonzero elements).
Reference: [209] <author> D. Watkins. </author> <title> Shifting strategies for the parallel QR algorithm. </title> <institution> Dept. of pure and applied math. </institution> <type> report, </type> <institution> Washington State Univ., </institution> <address> Pullman, WA, </address> <year> 1992. </year>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Second, the convergence properties degrade significantly, resulting in more overall work as well [67]. As a result, speedups have been extremely modest. This routine is available in LAPACK as shseqr [2]. Yet another way to introduce parallelism into Hessenberg QR is to pipeline several bulge chasing steps <ref> [194, 209, 210] </ref>. If we have several shifts available, then as soon as one bulge chase is launched from the upper left corner, another one may be launched, and so on. <p> This destroys the usual local quadratic convergence, but it remains superlinear [194]. It has been suggested that choosing the eigenvalues of the bottom right k-by-k submatrix may have superior convergence to just choosing a sequence from the bottom 1-by-1 or 2-by-2 submatrices <ref> [209] </ref>. Parallelism is still fine-grain, however. 6.5.2 Reduction to nonsymmetric tridiagonal form This approach begins by reducing B to nonsymmetric tridiagonal form with a (necessarily) nonorthogonal similarity, and then finding the eigenvalues of the resulting nonsymmetric tridiagonal matrix using the tridiagonal LR algorithm [57, 82, 83, 85].
Reference: [210] <author> D. Watkins and L. Elsner. </author> <title> Convergence of algorithms of decomposition type for the eigenvalue problem. </title> <journal> Lin. Alg. Appl., </journal> <volume> 143 </volume> <pages> 19-47, </pages> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration <ref> [12, 40, 67, 84, 189, 195, 194, 209, 210] </ref> 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer [36, 37, 61, 132, 133, 213] 5. <p> Asymptotic convergence remains quadratic <ref> [210] </ref>. The drawbacks to this scheme are two-fold. First, any attempt to use Level 3 BLAS introduces rather small (hence inefficient) matrix-matrix operations, and raises the operation count considerably. Second, the convergence properties degrade significantly, resulting in more overall work as well [67]. <p> Second, the convergence properties degrade significantly, resulting in more overall work as well [67]. As a result, speedups have been extremely modest. This routine is available in LAPACK as shseqr [2]. Yet another way to introduce parallelism into Hessenberg QR is to pipeline several bulge chasing steps <ref> [194, 209, 210] </ref>. If we have several shifts available, then as soon as one bulge chase is launched from the upper left corner, another one may be launched, and so on.
Reference: [211] <author> C.-P. Wen and K. Yelick. </author> <title> A survey of message passing systems. </title> <institution> Computer science division, University of California, Berkeley, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Again this is sometimes an inconvenient constraint, and makes it hard to write programs that run efficiently on more than one machine. The second issue to consider when sending messages is the semantic power of the messages <ref> [211] </ref>. The most restrictive possibility is that the processor executing "send" and the processor executing "receive" must synchronize, and so block until the transaction is completed.
Reference: [212] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965. </year>
Reference-contexts: The first kind of parallelism uses parallel prefix as described in (1) in section 2.2, and so care needs to be taken to avoid over/underflow. The numerical stability of the serial implementations of Algorithms 17 [118] and 18 <ref> [212] </ref> is very good, but that of the parallel prefix algorithm is unknown, although numerical experiments are promising [192]. This requires good support for parallel prefix operations, and is not as easy to parallelize as simply having each processor refine different sets of intervals containing different eigenvalues [47]. <p> This method is attractive because finding eigenvalues of a tridiagonal matrix (even nonsymmetric) is much faster than for a Hessenberg matrix <ref> [212] </ref>. The drawback is that reduction to tridiagonal form may require very ill-conditioned similarity transformations, and may even break down [158]. Breakdown can be avoided by restarting the process with different initializing vectors, or by accepting a "bulge" in the tridiagonal form.
Reference: [213] <author> Z. Zeng. </author> <title> Homotopy-determinant algorithm for solving matrix eigenvalue problems and its parallelizations. </title> <type> PhD thesis, </type> <institution> Michigan State University, </institution> <year> 1991. </year>
Reference-contexts: Hessenberg QR iteration [12, 40, 67, 84, 189, 195, 194, 209, 210] 2. Reduction to nonsymmetric tridiagonal form [57, 82, 83, 85] 3. Jacobi's method [71, 72, 154, 174, 181, 188, 206], 4. Hessenberg divide and conquer <ref> [36, 37, 61, 132, 133, 213] </ref> 5. Spectral divide and conquer [13, 135, 144] In contrast to the symmetric problem or SVD, no guaranteed stable and highly parallel algorithm for the nonsymmetric problem exists. <p> And as mentioned above, if two paths converge to the same solution, it is hard to tell if the solution really is a multiple root or if some other root is missing. A different homotopy scheme uses only the determinant to follow eigenvalues <ref> [132, 213] </ref>; here the homotopy function is simply det (H (t) I). Evaluating the determinant of a Hessenberg matrix costs only a triangular solve and an inner product, and therefore is efficient. It shares similar advantages and disadvantages as the previous homotopy algorithm.
Reference: [214] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for parallel and vectors computers. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: prepare the reader to write good programs on any particular machine, since many machine specific details will remain. 4 P 1 P 2 P n Network M 1 M 2 M n 2 Features of Parallel Systems 2.1 General Principles A large number of different parallel computers [96], languages (see <ref> [214] </ref> and the references therein), and software tools have recently been built or proposed. Though the details of these systems vary widely, there are two basic issues they must deal with, and these will guide us in understanding how to design and analyze parallel algorithms. <p> For a survey of such compiler optimization techniques see <ref> [214] </ref>. A hardware approach to this problem is optimistic execution, where the hardware guesses the way the branch will go and computes ahead under that assumption. The hardware retains enough information to undo what it did a few steps later if it finds out it decided incorrectly.
Reference: [215] <author> E. Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <type> Technical Report TRCS89-18, </type> <institution> Department of Computer Science, University of California, Santa Barbara, </institution> <address> CA, </address> <year> 1989. </year> <month> 79 </month>
Reference-contexts: In particular, performing the column updates one at a time by the receiving processors results in unnecessarily high communication frequency and volume, and in a relatively inefficient computational inner loop. The communication requirements can be reduced by careful mapping and by aggregating updating information over subtrees (see, e.g., <ref> [93, 152, 215] </ref>), but even with this improvement, the fan-out algorithm is usually not competitive with other algorithms presented below.
References-found: 215

