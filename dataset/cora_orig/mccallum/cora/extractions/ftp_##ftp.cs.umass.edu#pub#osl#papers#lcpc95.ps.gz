URL: ftp://ftp.cs.umass.edu/pub/osl/papers/lcpc95.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: weemsg@cs.umass.edu  
Phone: (413) 545-1249 (fax)  
Title: Compiler Architectures for Heterogeneous Systems  
Author: Kathryn S. McKinley, Sharad K. Singhai, Glen E. Weaver, Charles C. Weems fmckinley, singhai, weaver, 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Heterogeneous parallel systems incorporate diverse models of parallelism within a single machine or across machines and are better suited for diverse applications [25, 43, 30]. These systems are already pervasive in industrial and academic settings and offer a wealth of un-derutilized resources for achieving high performance. Unfortunately, heterogeneity complicates software development. We believe that compilers can and should assist in handling this complexity. We identify four goals for extending compilers to manage heterogeneity: exploiting available resources, targeting changing resources, adjusting optimization to suit a target, and allowing programming models and languages to evolve. These goals do not require changes to the individual pieces of existing compilers so much as a restructuring of a compiler's software architecture to increase its flexibility. We examine six important parallelizing compilers to identify both existing solutions and where new technology is needed. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. Amarasinghe, J. Anderson, M. Lam, and A. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures. SUIF from Stanford University is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [44, 1, 20, 37] </ref>. SUIF is designed to study parallelization for both shared memory and distributed shared memory machines as well as uniprocessor optimizations.
Reference: 2. <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: SUIF is able to derive automatic data decompositions for a given program. ParaScope and VFCS do this to some degree, however, the default computation partitioning mechanism for them is the owner computes rule and data partitioning is specified by programmers (recent work in ParaScope addresses automatic data partitioning <ref> [2] </ref>). Parafrase-2 is unique in that it exploits control parallelism by partitioning programs into separate tasks. The other compilers use only data parallelism.
Reference: 3. <author> W. Blume and R. Eigenmann. </author> <title> The range test: A dependence test for symbolic, non-linear expressions. </title> <type> CSRD 1345, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Instead of using traditional dependence analysis, Polaris builds symbolic lower and upper bounds for each variable reference and propagates these ranges throughout the program using symbolic execution. Polaris' range test then uses these ranges to disprove dependences <ref> [3] </ref>. Polaris, Parafrase-2, ParaScope, and SUIF perform control dependence analysis, albeit in a flow-insensitive manner. Parafrase-2 has additional analyses to eliminate redundant control dependences.
Reference: 4. <author> W. Blume et al. </author> <title> Effective Automatic Parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program [21, 22, 39]. Polaris is an optimizing source-to-source translator from the University of Illi-nois <ref> [4, 31, 15] </ref>. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a near-production quality compiler. The authors focus on parallelization for shared memory machines. Polaris is written in C++ and compiles Fortran 77.
Reference: 5. <author> F. Bodin et al. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <month> Fall </month> <year> 1993. </year>
Reference-contexts: The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++ <ref> [7, 46, 26, 5] </ref>. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90. Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures.
Reference: 6. <author> F. Bodin et al. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ restructuring tools. </title> <booktitle> In Second Object-Oriented Numerics Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Fixed for h. Static Measure 7 trapro cedural. 8 Used for oth analysis and optimization. 9 Only for System compiler. 6 Appears in the 8th Workshop on Languages and Compilers for Parallel Computing Sage++ is a toolkit for building source-to-source translators from Indiana University <ref> [6] </ref>. The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++ [7, 46, 26, 5]. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90.
Reference: 7. <author> F. Bodin, T. Priol, P. Mehrotra, and D. Gannon. </author> <title> Directions in parallel programming: HPF, shared virtual memory and object parallelism in pC++. </title> <type> Technical Report 94-54, </type> <institution> ICASE, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++ <ref> [7, 46, 26, 5] </ref>. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90. Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures.
Reference: 8. <author> D. Brown, S. Hackstadt, A. Malony, and B. Mohr. </author> <title> Program analysis environments for parallel language systems: the TAU environment. </title> <booktitle> In Proceedings of the 2nd Workshop on Environments and Tools For Parallel Scientific Computing, </booktitle> <pages> pages 162-171, </pages> <address> Townsend, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: ParaScope strives to provide a parallel programming environment, including an editor, debugger and an automatic data partitioner. Polaris allows programmers to provide instructions to the compiler through source code assertions. Sage++ provides a rich set of tools for pC++ named Tuning and Analysis Utilities, TAU <ref> [8, 29] </ref>. TAU includes tools for file and class display, call graph display, class hierarchy browsing, routine and data access profile display, and event and state display. Almost all of these systems are research tools that encourage user experimentation.
Reference: 9. <author> R. Butler and E. Lusk. </author> <title> Monitors, messages, and clusters: the p4 parallel programming system. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 547-564, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Unfortunately, large programs tend to use several models of parallelism. By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 <ref> [9] </ref>, and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing [36, 40, 41, 24, 18] is the well-orchestrated use of heterogeneous hardware to execute a single application [24].
Reference: 10. <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: SUIF accepts source code written in either Fortran 77 or C, however a modified version of f2c [16] is used to convert Fortran code to C code. The Vienna Fortran Compilation System (VFCS) from the University of Vi-enna is an interactive, source-to-source translator for Vienna Fortran <ref> [10, 11, 48, 49] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. VFCS outputs explicitly parallel, SPMD programs in message passing languages, Intel features, PARMACS, and MPI.
Reference: 11. <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: SUIF accepts source code written in either Fortran 77 or C, however a modified version of f2c [16] is used to convert Fortran code to C code. The Vienna Fortran Compilation System (VFCS) from the University of Vi-enna is an interactive, source-to-source translator for Vienna Fortran <ref> [10, 11, 48, 49] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. VFCS outputs explicitly parallel, SPMD programs in message passing languages, Intel features, PARMACS, and MPI.
Reference: 12. <author> K. Cooper et al. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Its design goal is to investigate compilation support for multiple languages and target architectures. It easily adapts to new language extensions because its IR emphasizes data and control dependences, rather than language syntax. Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator <ref> [12, 23] </ref>. It provides sophisticated global program analyses and a rich set of program transformations. Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program [21, 22, 39].
Reference: 13. <author> T. Fahringer. </author> <title> Using the P 3 T to guide the parallelization and optimization effort under the Vienna Fortran compilation system. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: SUIF and Polaris use a fixed ordering of transformations for each target, and therefore perform valid transformations according to a predefined strategy. VFCS relies on static performance measurement by an external tool, P 3 T, to determine profitability <ref> [13, 14] </ref>. Closely related to profitability is ordering criteria. Transformations applied in different orders can produce dramatically different results. In interactive mode, ParaScope and Parafrase-2 allow users to select any ordering of transformations. All support fixed transformation ordering via their command lines.
Reference: 14. <author> T. Fahringer and H. Zima. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: SUIF and Polaris use a fixed ordering of transformations for each target, and therefore perform valid transformations according to a predefined strategy. VFCS relies on static performance measurement by an external tool, P 3 T, to determine profitability <ref> [13, 14] </ref>. Closely related to profitability is ordering criteria. Transformations applied in different orders can produce dramatically different results. In interactive mode, ParaScope and Parafrase-2 allow users to select any ordering of transformations. All support fixed transformation ordering via their command lines.
Reference: 15. <author> K. Faigin et al. </author> <title> The polaris internal representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5) </volume> <pages> 553-586, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program [21, 22, 39]. Polaris is an optimizing source-to-source translator from the University of Illi-nois <ref> [4, 31, 15] </ref>. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a near-production quality compiler. The authors focus on parallelization for shared memory machines. Polaris is written in C++ and compiles Fortran 77.
Reference: 16. <author> S. Feldman, D. Gay, M. Maimone, and N. Schryer. </author> <title> A Fortran-to-C converter. </title> <institution> Computing Science 149, AT&T Bell Laboratories, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: SUIF is designed to study parallelization for both shared memory and distributed shared memory machines as well as uniprocessor optimizations. SUIF accepts source code written in either Fortran 77 or C, however a modified version of f2c <ref> [16] </ref> is used to convert Fortran code to C code. The Vienna Fortran Compilation System (VFCS) from the University of Vi-enna is an interactive, source-to-source translator for Vienna Fortran [10, 11, 48, 49]. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm.
Reference: 17. <author> G. Fox et al. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator [12, 23]. It provides sophisticated global program analyses and a rich set of program transformations. Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D <ref> [17] </ref>. The output of the D System is an efficient message-passing distributed memory program [21, 22, 39]. Polaris is an optimizing source-to-source translator from the University of Illi-nois [4, 31, 15].
Reference: 18. <author> A. Ghafoor and J. Yang. </author> <title> A distributed heterogeneous supercomputing management system. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 78-86, </pages> <month> June </month> <year> 1993. </year> <booktitle> Appears in the 8th Workshop on Languages and Compilers for Parallel Computing 15 </booktitle>
Reference-contexts: By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing <ref> [36, 40, 41, 24, 18] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [24]. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity.
Reference: 19. <author> M. B. Girkar and C. Polychronopoulos. </author> <title> The hierarchical task graph as a universal intermediate representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5), </volume> <year> 1994. </year>
Reference-contexts: This section describes the general approach, programming model, organization, intermediate representation, optimizations and transformations of the six systems and summarizes them in Table 1. 2 System Overviews and Goals Parafrase-2 is a source-to-source translator from the University of Illinois <ref> [19, 33] </ref>. Its design goal is to investigate compilation support for multiple languages and target architectures. It easily adapts to new language extensions because its IR emphasizes data and control dependences, rather than language syntax.
Reference: 20. <author> M. Hall, B. Murphy, and S. Amarasinghe. </author> <title> Interprocedural analysis for paralleliza-tion. </title> <booktitle> In Proceedings of the Eighth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, OH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures. SUIF from Stanford University is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [44, 1, 20, 37] </ref>. SUIF is designed to study parallelization for both shared memory and distributed shared memory machines as well as uniprocessor optimizations. <p> VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines [35]. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide interprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [32]. Parafrase-2, ParaScope, and VFCS [47] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT <ref> [20] </ref> tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [20]. Optimizations and Transformations The organization of analyses and transformations varies among the systems. <p> Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide interprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [32]. Parafrase-2, ParaScope, and VFCS [47] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT <ref> [20] </ref> tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [20]. Optimizations and Transformations The organization of analyses and transformations varies among the systems. SUIF has a flexible organization, with each analysis and transformation coded as an independent pass and the sole means of communication between passes being the annotations attached to the IR.
Reference: 21. <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report TR91-149, </type> <institution> Rice University, </institution> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: It provides sophisticated global program analyses and a rich set of program transformations. Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program <ref> [21, 22, 39] </ref>. Polaris is an optimizing source-to-source translator from the University of Illi-nois [4, 31, 15]. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a near-production quality compiler. The authors focus on parallelization for shared memory machines.
Reference: 22. <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: It provides sophisticated global program analyses and a rich set of program transformations. Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program <ref> [21, 22, 39] </ref>. Polaris is an optimizing source-to-source translator from the University of Illi-nois [4, 31, 15]. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a near-production quality compiler. The authors focus on parallelization for shared memory machines.
Reference: 23. <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 5(7) </volume> <pages> 575-602, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Its design goal is to investigate compilation support for multiple languages and target architectures. It easily adapts to new language extensions because its IR emphasizes data and control dependences, rather than language syntax. Rice University's ParaScope is an interactive parallel programming environment built around a source-to-source translator <ref> [12, 23] </ref>. It provides sophisticated global program analyses and a rich set of program transformations. Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program [21, 22, 39].
Reference: 24. <author> A. Khokhar, V. Prasanna, M. Shaaban, and C. Wang. </author> <title> Heterogeneous computing: Challenges and opportunities. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 18-27, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing <ref> [36, 40, 41, 24, 18] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [24]. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity. <p> Heterogeneous processing [36, 40, 41, 24, 18] is the well-orchestrated use of heterogeneous hardware to execute a single application <ref> [24] </ref>. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity. Another paper [27] has a preliminary description of our design along with an expanded survey section.
Reference: 25. <author> A. E. Klietz, A. V. Malevsky, and K. Chin-Purcell. </author> <title> A case study in metacomput-ing: Distributed simulations of mixing in turbulent convection. </title> <booktitle> In Workshop on Heterogeneous Processing, </booktitle> <pages> pages 101-106, </pages> <month> April </month> <year> 1993. </year>
Reference: 26. <author> A. Malony et al. </author> <title> Performance analysis of pC++: A portable data-parallel programming system for scalable parallel computers. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++ <ref> [7, 46, 26, 5] </ref>. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90. Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures.
Reference: 27. <author> K. S. McKinley, S. Singhai, G. Weaver, and C. Weems. </author> <title> Compiling for heterogeneous systems: A survey and an approach. </title> <type> Technical Report TR-95-59, </type> <institution> University of Massachusetts, </institution> <month> July </month> <year> 1995. </year> <note> http://osl-www.cs.umass.edu/~oos/papers.html. </note>
Reference-contexts: Heterogeneous processing [36, 40, 41, 24, 18] is the well-orchestrated use of heterogeneous hardware to execute a single application [24]. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity. Another paper <ref> [27] </ref> has a preliminary description of our design along with an expanded survey section. <p> The authors focus on parallelization for shared memory machines. Polaris is written in C++ and compiles Fortran 77. Programmers may convey extra information, such as parallelism, to the compiler by embedding assertions in source code. 2 We give more detailed descriptions in <ref> [27] </ref>. 4 Appears in the 8th Workshop on Languages and Compilers for Parallel Computing Table 1. Comparison table for surveyed systems.
Reference: 28. <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <type> v1.0. Technical report, </type> <institution> University of Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Unfortunately, large programs tend to use several models of parallelism. By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI <ref> [28] </ref>), heterogeneous systems provide consistent high performance. Heterogeneous processing [36, 40, 41, 24, 18] is the well-orchestrated use of heterogeneous hardware to execute a single application [24].
Reference: 29. <author> B. Mohr, D. Brown, and A. Malony. </author> <title> TAU: A portable parallel program analysis environment for pC++. </title> <booktitle> In Proceedings of CONPAR 94 - VAPP VI, </booktitle> <institution> University of Linz, Austria, </institution> <month> September </month> <year> 1994. </year> <note> LNCS 854. </note>
Reference-contexts: ParaScope strives to provide a parallel programming environment, including an editor, debugger and an automatic data partitioner. Polaris allows programmers to provide instructions to the compiler through source code assertions. Sage++ provides a rich set of tools for pC++ named Tuning and Analysis Utilities, TAU <ref> [8, 29] </ref>. TAU includes tools for file and class display, call graph display, class hierarchy browsing, routine and data access profile display, and event and state display. Almost all of these systems are research tools that encourage user experimentation.
Reference: 30. <author> H. Nicholas et al. </author> <title> Distributing the comparison of DNA and protein sequences across heterogeneous supercomputers. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 139-146, </pages> <year> 1991. </year>
Reference: 31. <author> D. A. Padua et al. </author> <title> Polaris: A new-generation parallelizing compiler for MPPs. </title> <type> Technical Report CSRD-1306, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program [21, 22, 39]. Polaris is an optimizing source-to-source translator from the University of Illi-nois <ref> [4, 31, 15] </ref>. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a near-production quality compiler. The authors focus on parallelization for shared memory machines. Polaris is written in C++ and compiles Fortran 77.
Reference: 32. <author> D. A. Pauda. </author> <title> Private communication, </title> <month> September </month> <year> 1995. </year>
Reference-contexts: VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines [35]. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide interprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation <ref> [32] </ref>. Parafrase-2, ParaScope, and VFCS [47] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT [20] tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [20]. Optimizations and Transformations The organization of analyses and transformations varies among the systems.
Reference: 33. <author> C. Polychronopoulos et al. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(1), </volume> <year> 1989. </year>
Reference-contexts: This section describes the general approach, programming model, organization, intermediate representation, optimizations and transformations of the six systems and summarizes them in Table 1. 2 System Overviews and Goals Parafrase-2 is a source-to-source translator from the University of Illinois <ref> [19, 33] </ref>. Its design goal is to investigate compilation support for multiple languages and target architectures. It easily adapts to new language extensions because its IR emphasizes data and control dependences, rather than language syntax.
Reference: 34. <author> W. Pottenger and R. Eigenmann. </author> <title> Idiom recognition in the Polaris parallelizing compiler. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Transformations performed by uniprocessor compilers are termed traditional. Except Sage++, all the systems provide traditional optimizations. In addition, Polaris and SUIF perform array privatization and reductions <ref> [34] </ref>. Because SUIF generates native code, it also includes low-level optimizations such as register allocation. All six systems provide loop transformations. ParaScope has a large set of loop transformations. SUIF too has a wide assortment of traditional and loop transformations including unimodular loop transformations (i.e., interchange, reversal, and skewing) [45].
Reference: 35. <author> J. Saltz, K. Crowely, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: Parafrase-2 has additional analyses to eliminate redundant control dependences. All the systems (except Sage++) perform intraprocedural symbolic analysis to support traditional optimizations, but ParaScope and Parafrase-2 have extensive interprocedural symbolic analysis such as forward propagation of sym-bolics. VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines <ref> [35] </ref>. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide interprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [32]. Parafrase-2, ParaScope, and VFCS [47] perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT [20] tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [20].
Reference: 36. <author> L. Smarr and C. E. Catlett. </author> <title> Metacomputing. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 45-52, </pages> <month> June </month> <year> 1992. </year> <booktitle> 16 Appears in the 8th Workshop on Languages and Compilers for Parallel Computing </booktitle>
Reference-contexts: By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing <ref> [36, 40, 41, 24, 18] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [24]. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity.
Reference: 37. <author> Stanford Compiler Group. </author> <title> The SUIF library. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures. SUIF from Stanford University is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [44, 1, 20, 37] </ref>. SUIF is designed to study parallelization for both shared memory and distributed shared memory machines as well as uniprocessor optimizations.
Reference: 38. <author> V.S. Sunderam, G.A. Geist, J. Dongarra, and P. Manchek. </author> <title> The PVM concurrent computing system: Evolution, experiences, and trends. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 531-545, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Unfortunately, large programs tend to use several models of parallelism. By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM <ref> [38] </ref>, p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing [36, 40, 41, 24, 18] is the well-orchestrated use of heterogeneous hardware to execute a single application [24].
Reference: 39. <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: It provides sophisticated global program analyses and a rich set of program transformations. Here we concentrate on the D System which is a specialized version of ParaScope for Fortran-D [17]. The output of the D System is an efficient message-passing distributed memory program <ref> [21, 22, 39] </ref>. Polaris is an optimizing source-to-source translator from the University of Illi-nois [4, 31, 15]. The authors have two major goals for Polaris: to automatically parallelize sequential programs and to be a near-production quality compiler. The authors focus on parallelization for shared memory machines.
Reference: 40. <author> L. H. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical Report MSSU-EIRS-ERC-93-2, </type> <institution> NSF Engineering Research Center, Mississippi State University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing <ref> [36, 40, 41, 24, 18] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [24]. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity.
Reference: 41. <author> L. H. Turcotte. </author> <title> Cluster computing. </title> <editor> In Albert Y. Zomaya, editor, </editor> <booktitle> Parallel and Distributed Computing Handbook, chapter 26. </booktitle> <publisher> McGraw-Hill, </publisher> <month> October </month> <year> 1995. </year>
Reference-contexts: By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA [42]) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing <ref> [36, 40, 41, 24, 18] </ref> is the well-orchestrated use of heterogeneous hardware to execute a single application [24]. When an application encompasses subtasks that employ different models of parallelism, the We are designing a new compiler architecture to meet the needs of heterogeneity.
Reference: 42. <author> C. Weems et al. </author> <title> The image understanding architecture. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(3) </volume> <pages> 251-282, </pages> <year> 1989. </year>
Reference-contexts: As long as this model matches the parallelism inherent in an application, the machines perform well. Unfortunately, large programs tend to use several models of parallelism. By incorporating multiple models of parallelism within one machine (e.g., Meiko CS-2, IBM SP-2, and IUA <ref> [42] </ref>) or across machines, creating a virtual machine (e.g., PVM [38], p4 [9], and MPI [28]), heterogeneous systems provide consistent high performance. Heterogeneous processing [36, 40, 41, 24, 18] is the well-orchestrated use of heterogeneous hardware to execute a single application [24].
Reference: 43. <author> C. Weems et al. </author> <title> The DARPA image understanding benchmark for parallel processors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 1-24, </pages> <year> 1991. </year>
Reference: 44. <author> R. Wilson et al. </author> <title> The SUIF compiler system: A parallelizing and optimizing research compiler. </title> <journal> SIGPLAN, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures. SUIF from Stanford University is a compiler framework that can be used as either a source-to-C translator or a native code compiler <ref> [44, 1, 20, 37] </ref>. SUIF is designed to study parallelization for both shared memory and distributed shared memory machines as well as uniprocessor optimizations.
Reference: 45. <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Because SUIF generates native code, it also includes low-level optimizations such as register allocation. All six systems provide loop transformations. ParaScope has a large set of loop transformations. SUIF too has a wide assortment of traditional and loop transformations including unimodular loop transformations (i.e., interchange, reversal, and skewing) <ref> [45] </ref>. All systems except Sage++, include inlining as one of their interprocedural optimizations. ParaScope, Parafrase-2, SUIF and VFCS also perform cloning. SUIF exploits its strong interprocedural analyses to provide data privatization, reductions and parallelization.
Reference: 46. <author> S. Yang et al. </author> <title> High performance fortran interface to the parallel C++. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The authors foresee optimizing translators, simulation of language extensions, language preprocessors, and code instrumentation as possible applications of Sage++ <ref> [7, 46, 26, 5] </ref>. Sage++ is written in C++ and provides parses for C, C++, pC++, Fortran 77, and Fortran 90. Because Sage++ is a toolkit instead of a compiler, it is not limited to particular hardware architectures.
Reference: 47. <author> H. Zima. </author> <title> Private communication, </title> <month> September </month> <year> 1995. </year>
Reference-contexts: VFCS provides intraprocedural irregular access pattern analysis based on PARTI routines [35]. Parafrase-2, ParaScope, Polaris, SUIF, and VFCS provide interprocedural analysis. Polaris recently incorporated interprocedural symbolic constant propagation [32]. Parafrase-2, ParaScope, and VFCS <ref> [47] </ref> perform flow-insensitive interprocedural analysis by summarizing where variables are referenced or modified. SUIF's FIAT [20] tool provides a powerful framework for both flow-insensitive and flow-sensitive analysis [20]. Optimizations and Transformations The organization of analyses and transformations varies among the systems.
Reference: 48. <author> H. Zima and B. Chapman. </author> <title> Compiling for distributed-memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 264-287, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: SUIF accepts source code written in either Fortran 77 or C, however a modified version of f2c [16] is used to convert Fortran code to C code. The Vienna Fortran Compilation System (VFCS) from the University of Vi-enna is an interactive, source-to-source translator for Vienna Fortran <ref> [10, 11, 48, 49] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. VFCS outputs explicitly parallel, SPMD programs in message passing languages, Intel features, PARMACS, and MPI.
Reference: 49. <author> H. Zima, B. Chapman, H. Moritsch, and P. Mehrotra. </author> <title> Dynamic data distributions in Vienna Fortran. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: SUIF accepts source code written in either Fortran 77 or C, however a modified version of f2c [16] is used to convert Fortran code to C code. The Vienna Fortran Compilation System (VFCS) from the University of Vi-enna is an interactive, source-to-source translator for Vienna Fortran <ref> [10, 11, 48, 49] </ref>. VFCS is based upon the data parallel model of computation with the Single-Program-Multiple-Data (SPMD) paradigm. VFCS outputs explicitly parallel, SPMD programs in message passing languages, Intel features, PARMACS, and MPI.
References-found: 49

