URL: http://www-anw.cs.umass.edu/~amy/pubs/mcgovern_ghc97.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: famyjrichjfaggg@cs.umass.edu  
Title: Roles of Macro-Actions in Accelerating Reinforcement Learning  
Author: Amy McGovern, Richard S. Sutton, Andrew H. Fagg 
Address: Amherst, MA 01003  
Affiliation: Computer Science Department, University of Massachusetts,  
Abstract: We analyze the use of built-in policies, or macro-actions, as a form of domain knowledge that can improve the speed and scaling of reinforcement learning algorithms. Such macro-actions are often used in robotics, and macro-operators are also well-known as an aid to state-space search in AI systems. The macro-actions we consider are closed-loop policies with termination conditions. The macro-actions can be chosen at the same level as primitive actions. Macro-actions commit the learning agent to act in a particular, purposeful way for a sustained period of time. Overall, macro-actions may either accelerate or retard learning, depending on the appropriateness of the macro-actions to the particular task. We analyze their effect in a simple example, breaking the acceleration effect into two parts: 1) the effect of the macro-action in changing exploratory behavior, independent of learning, and 2) the effect of the macro-action on learning, independent of its effect on behavior. In our example, both effects are significant, but the latter appears to be larger. Finally, we provide a more complex gridworld illustration of how appropriately chosen macro-actions can accelerate overall learning. 
Abstract-found: 1
Intro-found: 1
Reference: [Dayan and Hinton, 1993] <author> Dayan, P. and Hinton, G. E. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [Iba, 1989] <author> Iba, G. A. </author> <year> (1989). </year> <title> A heuristic approach to the discovery of macro-operators. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 285-317. </pages>
Reference-contexts: One way to overcome this difficulty is through macro-actions (or macros). By chunking together primitive actions into macro-actions, the effective length of the solution is shortened. Both [Korf, 1985] and <ref> [Iba, 1989] </ref> have demonstrated that using macro-actions to search for a solution has resulted in solutions in cases where the system was unable to find answers by searching in primitive state-space, and in finding faster solutions in cases where both systems could solve the problem.
Reference: [Korf, 1985] <author> Korf, R. E. </author> <year> (1985). </year> <title> Macro-operators: A weak method for learning. </title> <journal> Artificial Intelligence, </journal> <volume> 26 </volume> <pages> 35-77. </pages>
Reference-contexts: One way to overcome this difficulty is through macro-actions (or macros). By chunking together primitive actions into macro-actions, the effective length of the solution is shortened. Both <ref> [Korf, 1985] </ref> and [Iba, 1989] have demonstrated that using macro-actions to search for a solution has resulted in solutions in cases where the system was unable to find answers by searching in primitive state-space, and in finding faster solutions in cases where both systems could solve the problem.
Reference: [Mahadevan and Connell, 1992] <author> Mahadevan, S. and Connell, J. </author> <year> (1992). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365. </pages>
Reference: [Precup and Sutton, 1997] <author> Precup, D. and Sutton, R. S. </author> <year> (1997). </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Submitted to Advances in Neural Information Processing Systems 10. </booktitle> <publisher> MIT Press. </publisher>
Reference: [Singh, 1994] <author> Singh, S. P. </author> <year> (1994). </year> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst. </institution>
Reference: [Watkins, 1989] <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: Reinforcement learning (RL) is a collection of methods for discovering near-optimal solutions to stochastic sequential decision problems <ref> [Watkins, 1989] </ref>. An RL system interacts with the environment by executing actions and receiving rewards from the environment. Unlike supervised learning, RL does not rely on an outside teacher to specify the correct action for a given state. <p> The ideas and results described here should generalize to larger and more complex cases. In order to maximize the reward from the environment, the RL agent learns the estimated reward from each state s for each action a. These values are known as action values. In 1-step Q-learning <ref> [Watkins, 1989] </ref>, the action values are stored in an array called Q where Q (s t ; a t ) is updated at each time step t as follows: Q (s t ; a t ) Q (s t ; a t ) + ff fi (1) 2 a 0 Q
References-found: 7

