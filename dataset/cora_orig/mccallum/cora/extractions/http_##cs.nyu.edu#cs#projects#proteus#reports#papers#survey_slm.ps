URL: http://cs.nyu.edu/cs/projects/proteus/reports/papers/survey_slm.ps
Refering-URL: http://cs.nyu.edu/cs/projects/proteus/reports/index.html
Root-URL: http://www.cs.nyu.edu
Title: Survey Paper on Statistical Language Modeling  
Author: Andrew Borthwick 
Note: Contents  
Date: May 2, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adda, G., Lamel, L., Adda-Decker, M., and Gauvain, J. L. </author> <booktitle> Language modeling and lexicon in the LIMSI Nov96 Hub4 system. In Proceedings of the DARPA Speech Recognition Workshop (1997). </booktitle>
Reference-contexts: that "Every time somebody gets a good idea in acoustic modeling they generally get a significant gain in performance, but new ideas in language modeling generally get you a few tenths of a percent in accuracy." [21] Reinforcing this view is the fact that one of the two top-ranked systems <ref> [1] </ref> at the most recent DARPA speech evaluation relied only on n-gram models and a careful optimization of their vocabulary for the language modeling component.
Reference: [2] <author> Bahl, L. R., Brown, P. F., De Souza, P. V., and Mercer, R. L. </author> <title> A tree-based statistical language model for natural language speech recognition. </title> <booktitle> IEEE Transactions on Acoustics, Speech, and Signal Processing 37, </booktitle> <month> 7 (July </month> <year> 1989), </year> <pages> 1001-1008. </pages>
Reference: [3] <author> Bakis, R., Chen, S., Gopalakrishnan, P., Gopinath, R., Maes, S., and Polymenakos, L. </author> <title> Transcription of broadcast news shows with the IBM large vocabulary speech recognition system. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop (1997). </booktitle>
Reference-contexts: The language model which IBM used in the most recent DARPA LVCSR evaluation was a "maximum entropy trigram language model with class constraints", but IBM gives no more information about it in their paper <ref> [3] </ref>, so there is no way of 26 knowing how much they gained by using the method.
Reference: [4] <author> Baum, L. E. </author> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of markov processes. </title> <booktitle> Inequalities 3 (1972), </booktitle> <pages> 1-8. </pages>
Reference: [5] <author> Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. </author> <title> A maximum entropy approach to natural language processing. </title> <booktitle> Computational Linguistics 22, 1 (1996), </booktitle> <pages> 39-71. </pages>
Reference-contexts: Finally, it may be sig-nificant to note that Renaissance Technologies, a highly successful firm which trades in securities and commodities based on mathematical models, recently hired the Della Pietra brothers (co-authors of the IBM M.E. paper <ref> [5] </ref>) along with the Decision Tree expert, David Magerman [18]. Hedge fund managers are not noted for their support of research which is of purely theoretical interest.
Reference: [6] <author> Collins, M. J. </author> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (June 1996), </booktitle> <pages> pp. 184-191. </pages>
Reference: [7] <author> Cover, T. M., and Thomas, J. A. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1991, </year> <journal> ch. </journal> <volume> 11. </volume> <pages> 29 </pages>
Reference: [8] <author> Darroch, J., and Ratcliff, D. </author> <title> Generalized iterative scaling for log--linear models. </title> <journal> The Annals of Mathematical Statistics 43 (1972), </journal> <pages> 1470-1480. </pages>
Reference: [9] <author> Good, I. J. </author> <title> The population frequencies of species and the estimation of population parameters. </title> <type> Biometrika 40, </type> <month> 3-4 </month> <year> (1953), </year> <pages> 237-264. </pages>
Reference: [10] <author> Good, I. J. </author> <title> Maximum entropy for hypothesis formulation, especially for multidimensional contingency tables. </title> <journal> Annals of Mathematical Statistics 34 (1963), </journal> <pages> 911-934. </pages>
Reference: [11] <author> Graff, D. </author> <title> The 1996 broadcast news speech and language-model corpus. </title> <booktitle> Slides from lecture at the 1997 DARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1997. </year>
Reference: [12] <author> Jaynes, E. T. </author> <title> Information theory and statistical mechanics. </title> <journal> Physics Reviews 106 (1957), </journal> <pages> 620-630. </pages>
Reference: [13] <author> Jaynes, E. T. </author> <title> Notes on present status and future prospects. In Maximum Entropy and Bayesian Methods (1990), </title> <editor> W. T. Grandy and L. H. Schick, Eds., </editor> <publisher> Kluwer, </publisher> <pages> pp. 1-13. </pages>
Reference: [14] <author> Jaynes, E. T. </author> <title> Probability theory: The logic of science. Manuscript for book. </title> <note> Available at ftp://bayes.wustl.edu/, </note> <month> May </month> <year> 1996. </year>
Reference: [15] <author> Jelinek, F., Merialdo, B., Roukos, S., and Strauss, M. </author> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of the Speech and Natural Language DARPA Workshop (February 1991). </booktitle> <pages> 293-295. </pages>
Reference-contexts: The other one [28] simply enhanced their 4-gram model with a unigram and bigram cache, a technique which dates to 1991 <ref> [15] </ref>, but even that only gave them a 0.2% absolute gain in accuracy (0.7% relative gain).
Reference: [16] <author> Katz, S. M. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing ASSP-35 (March 1987), </journal> <pages> 400-401. </pages>
Reference: [17] <author> Knight, K., and Chander, I. </author> <title> Automated postediting of documents. </title> <booktitle> In American Association for Artificial Intelligence (1994), </booktitle> <pages> pp. 779-784. </pages>
Reference: [18] <author> Magerman, D. M. </author> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Finally, it may be sig-nificant to note that Renaissance Technologies, a highly successful firm which trades in securities and commodities based on mathematical models, recently hired the Della Pietra brothers (co-authors of the IBM M.E. paper [5]) along with the Decision Tree expert, David Magerman <ref> [18] </ref>. Hedge fund managers are not noted for their support of research which is of purely theoretical interest.
Reference: [19] <author> Magerman, D. M. </author> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (June 1995), </booktitle> <pages> pp. 276-283. </pages>
Reference: [20] <author> Mercer, R. L., and Roukos, S., </author> <year> 1992. </year> <title> Personal communication to Ronald Rosenfeld, </title> <note> described in Rosenfeld's thesis. </note>
Reference-contexts: IBM found that in 40% of the cases in which the human beat the trigram model it was because "the predicted word, or a word related to it, occurred in the history of the document." [24] <ref> [20] </ref> It is clear, then, that there is a great deal of information to be gained by semantics as well. Language modeling is currently in a state of some frustration.
Reference: [21] <author> Paul, D., </author> <month> February </month> <year> 1997. </year> <type> Personal communication from Doug Paul. </type>
Reference-contexts: a recent conference, one of the leading researchers in the field remarked to me that "Every time somebody gets a good idea in acoustic modeling they generally get a significant gain in performance, but new ideas in language modeling generally get you a few tenths of a percent in accuracy." <ref> [21] </ref> Reinforcing this view is the fact that one of the two top-ranked systems [1] at the most recent DARPA speech evaluation relied only on n-gram models and a careful optimization of their vocabulary for the language modeling component.
Reference: [22] <author> Ratnaparkhi, A. </author> <title> A maximum entropy model for part-of-speech tagging. </title> <booktitle> In Conference on Empirical Methods in Natural Language Processing (May 1996), University of Pennsylvania, </booktitle> <pages> pp. 133-142. 30 </pages>
Reference-contexts: Other work has demonstrated the utility of M.E. approaches to sentence segmentation (i.e. determining whether a "." is an end of sentence marker or signifies an abbreviation) [23] and part-of-speech tagging <ref> [22] </ref>.
Reference: [23] <author> Reynar, J. C., and Ratnaparkhi, A. </author> <title> A maximum entropy approach to identifying sentence structures. </title> <booktitle> In Fifth Conference on Applied Natural Language Processing (April 1997), </booktitle> <pages> pp. 16-19. </pages>
Reference-contexts: Other work has demonstrated the utility of M.E. approaches to sentence segmentation (i.e. determining whether a "." is an end of sentence marker or signifies an abbreviation) <ref> [23] </ref> and part-of-speech tagging [22].
Reference: [24] <author> Rosenfeld, R. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1994. </year> <type> CMU Technical Report CMU-CS-94-138. </type>
Reference-contexts: Building a parser which is good enough to discard sentences which violate basic syntactic constraints remains one of the great unsolved problems of language modeling. Rosenfeld points out <ref> [24] </ref> that these syntactic constraints seem to be orthogonal to the essentially semantic, statistical constraints which represent the current state-of-the-art in language modeling. <p> IBM found that in 40% of the cases in which the human beat the trigram model it was because "the predicted word, or a word related to it, occurred in the history of the document." <ref> [24] </ref> [20] It is clear, then, that there is a great deal of information to be gained by semantics as well. Language modeling is currently in a state of some frustration. <p> Rosenfeld parallelized this process at the workstation level, but there seems to be the potential to do this with massively parallel techniques, although he rejected this approach due to memory constraints <ref> [24] </ref>. A breakthrough on the training front would doubtless lead to a breakthrough on the modeling front since the method is currently highly computationally constrained. <p> Another way of thinking of this is that an acoustic model would have roughly the same difficulty of recognizing a language with perplexity X as it would have in recognizing an artificial language in which every word could be followed by X other words with equal probability <ref> [24] </ref>.
Reference: [25] <author> Rosenfeld, R., and Huang, X. </author> <title> Improvements in stochastic language modeling. </title> <booktitle> In Proceedings of the DARPA Workshop on Speech and Natural Language (February 1992), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 107-111. </pages>
Reference: [26] <author> Sekine, S., Borthwick, A., and Grishman, R. </author> <title> NYU language modeling experiments for the 1996 CSR evaluation. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop (1997). </booktitle>
Reference-contexts: NYU reported a 0.3% absolute gain using language modeling techniques at the same conference, 0.2% of which could be attributed to the use of cache features and 0.1% to a new method of modeling sublanguage effects <ref> [26] </ref>. There has been a good deal of interest lately in maximum entropy approaches to language modeling. <p> Many of the errors made by current speech systems are easily identifiable even without listening to the acoustics. Some of these errors are syntactic. For instance "Weary of worrying about *withdraw all* charges if you want to leave . . . " is a sentence cited by Sekine <ref> [26] </ref> as one which he was able to fix by adding a parser to the NYU language model, although overall the parser was unable to improve language model performance for evaluation purposes.
Reference: [27] <author> Seymore, K., Chen, S., Eskenazi, M., and Rosenfeld, R. </author> <title> Language and pronunciation modeling in the CMU 1996 Hub 4 evaluation. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop (1997). </booktitle>
Reference: [28] <author> Young, S., Gales, M., Pye, D., and Woodland, P. </author> <title> HTK broadcast news language model. </title> <booktitle> Slides from lecture at the 1997 DARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1997. </year> <month> 31 </month>
Reference-contexts: The other one <ref> [28] </ref> simply enhanced their 4-gram model with a unigram and bigram cache, a technique which dates to 1991 [15], but even that only gave them a 0.2% absolute gain in accuracy (0.7% relative gain).
References-found: 28

