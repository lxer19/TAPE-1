URL: http://www.cs.cmu.edu/afs/cs/user/jthomas/Web/Papers/icmas98.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/jthomas/Web/papers.html
Root-URL: 
Email: jthomas+@cs.cmu.edu  katia+@ri.cmu.edu  
Title: Heterogeneity, Stability, and Efficiency in Distributed Systems  
Author: James D Thomas Katia Sycara Matt Glickman, Bryan Routledge, John Miller, Onn Shehory, and Somesh Jha 
Note: 1 Thanks to  for helpful discussion. Sponsored by NSF grant IRI-9612131 and ONR grant N-00014-96-1-1222  
Date: November 20, 1997  
Address: Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: Department of Computer Science Carnegie Mellon University  Robotics Institute Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Brian Arthur. </author> <title> Inductive reasoning and bounded ratinality. </title> <booktitle> AEA Papers and Proceedings, </booktitle> <volume> 84(2) </volume> <pages> 406-411, </pages> <year> 1994. </year>
Reference-contexts: Kephart, Huberman & Hogg [5] noted the key to stability was increasing the heterogeneity of agent responses. Arthur <ref> [1] </ref> investigated similar issues in the context of the Santa Fe bar problem. He allowed the agents to make decisions based on predictions of the future state of the system; it turns out that for stability, the accuracy of the predictions were less important than their heterogeneity.
Reference: [2] <author> John Q. Cheng and Michael P Wellman. </author> <title> The walras algorithm: A convergent distributed implementation of general equilibrium outcomes. </title> <type> Technical report, </type> <institution> University of Michigan, </institution> <year> 1995. </year>
Reference-contexts: This can be a particular problem in systems where agents allocate resources among themselves with no central control. Problems with this characteristic include load balancing over multiple processors, the allocation of internet traffic over multiple network routes, and market-like control systems <ref> [2] </ref>. In such systems, when agents perceive that a resource is underutilized, they try to increase their utilization of it. But if all of the agents shift towards it, the other resources become underutilized the agents see this, and try to switch back; this leads to unstable behavior.
Reference: [3] <author> Drew Fudenberg and Jean Tirole. </author> <title> Game Theory. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: There are many possible approaches to this problem. Game theory <ref> [3] </ref> offers up an easy answer: a symmetric mixed strategy where every agent chooses resource one with probability 2/3, and resource two with probability 1/3.
Reference: [4] <author> Tad Hogg and Bernardo A. Huberman. </author> <title> Controlling chaos in distributed systems. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1325-1332, </pages> <month> November/December </month> <year> 1991. </year>
Reference-contexts: these techniques to the problem of allocating tasks across parallel processors to maximize throughput, and section 8 summarizes and discusses outstanding questions. 2 The Basic Model For empirical modelling, we turned to a simplified version of the computational ecosystem model pioneered by Huberman, Hogg, and their colleagues at Xerox Parc <ref> [4, 5] </ref>. We created a simple model of resource utilization by agents. The agents want to maximize their payoffs, and decide which resource to utilize using publicly available information about each resource's payoffs. <p> Using heterogeneity for stability has the advantage of not imposing an additional computational burden on the agents, and potentially stabilizing the system near the true equilibrium. Hogg & Huberman <ref> [4] </ref> approach this problem by increasing the heterogeneity of the system: by manipulating 4 payoffs, they effectively increase the heterogeneity in agents' lag times, which is enough to stabilize the system.
Reference: [5] <author> J. O. Kephart, Tad Hogg, and Bernardo A. Huberman. </author> <title> Dynamics of computational ecosystems. </title> <type> Technical report, </type> <note> Xerox PARC, </note> <year> 1989. </year>
Reference-contexts: The intuition is that since the instability is caused by too many agents wanting to shift, a heterogeneous agent population will increase the number of agents who don't want to shift, thus stabilizing the system. Kephart, Huberman & Hogg <ref> [5] </ref> noted the key to stability was increasing the heterogeneity of agent responses. Arthur [1] investigated similar issues in the context of the Santa Fe bar problem. <p> these techniques to the problem of allocating tasks across parallel processors to maximize throughput, and section 8 summarizes and discusses outstanding questions. 2 The Basic Model For empirical modelling, we turned to a simplified version of the computational ecosystem model pioneered by Huberman, Hogg, and their colleagues at Xerox Parc <ref> [4, 5] </ref>. We created a simple model of resource utilization by agents. The agents want to maximize their payoffs, and decide which resource to utilize using publicly available information about each resource's payoffs. <p> However, this equilibrium 2 is unstable; if p is smaller than 2=3 by just a little bit, then the payoffs for resource one are higher than those of resource two, and every agent has incentive to switch over to using resource one. Following Kephart et al <ref> [5] </ref>, our agents act myopically on delayed information: they expect the next payoff to be the same as the last payoff they've seen, but there is a lag time before they can see the payoffs, and must form expectations based on payoff information a few turns old. <p> However, this solution assumes that the agents know (or can learn) the true equilibrium, and would incur horrendous costs if there was a transaction cost to switch between resources. Similarly, adding noise to the system stabilizes the dynamics <ref> [5] </ref>, at the cost of moving the system away from the true equilibrium (however, as delays in the system increase, this negative effect becomes less of an issue). <p> One might try to build agents that observe the system dynamics, learn to predict the oscillations, and act in ways that stabilize the system. Kephart <ref> [5] </ref> showed that allowing some agents to predict the system did not necessarily improve the dynamics; with small numbers of predicting agents, system stability improved, but as the proportion of these predicting agents grew, the system grew unstable again.
Reference: [6] <author> Mike Litzkow and Miron Livny. </author> <title> Experience with the condor distributed batch system. </title> <type> Technical report, </type> <institution> University of Wisconsin, </institution> <year> 1996. </year>
Reference-contexts: Such an approach has several potential benefits. It is decentralized, and doesn't require a centralized processor to gather information, analyze it, and retransmit. As more in more interest is shown in distributing computation across workstations connected by the internet <ref> [6] </ref>, this feature will grow in importance. Also, it effectively performs its estimates 'on the fly' and so starts moving towards optimal performance immediately.
Reference: [7] <author> William H. Press, Saul A. Tukolsky, William T. Vertterling, and Brian P. Flannery. </author> <title> Numerical Recipies in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <month> 20 </month>
Reference-contexts: It is important to take a step back to think of the implications of this. With guaranteed convergence to the point where f = 0, the system is functioning as a decentralized root-finding algorithm. Root finding algorithms <ref> [7] </ref> are numerical methods used to solve equations, usually by moving all terms to one side and finding the point where f (x) = 0. One must usually make assumptions about the form of f , namely that it is monotonic, and sometimes continuous.
Reference: [8] <author> Andrea Schaerf, Yoav Shoham, and Moshe Tennenholtz. </author> <title> Adaptive load balancing: A study in multi-agent learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> (2):475-500, 1995. 
Reference-contexts: He allowed the agents to make decisions based on predictions of the future state of the system; it turns out that for stability, the accuracy of the predictions were less important than their heterogeneity. Schaerf et al <ref> [8] </ref> studied a distributed load balancing system and discovered that communication between agents, by reducing heterogeneity, actually worsened system performance. We explore the effects of creating heterogeneous agents by introducing bias terms to their perceptions of resource payoffs. We examine the effect of such heterogeneity both analytically and via simulations.
Reference: [9] <author> Staff. </author> <title> Contagious speculative attacks. </title> <type> Technical Report 22, </type> <institution> Bank for International Settlements, </institution> <year> 1994. </year>
Reference-contexts: The recent discussion of using the Tobin tax <ref> [11, 9] </ref> (a small transaction cost on international currency trades) to encourage stability in global currency markets has raised the profile of transaction costs as a policy method; the work here suggests that making them heterogeneous may make them more effective at a lower social cost, although issues of fairness would
Reference: [10] <author> Jaspal Subhlok. </author> <title> Experience with automatic mapping of sensor based applications. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Throughput is the aggregate rate at which data is processed; optimal 17 throughput is achieved when the the average speedup for each job is the same. One standard approach, used by that of the Fx compiler project <ref> [10] </ref> involves a central decision maker that estimates the performance characteristics and builds an explicit polynomial model (like the one described above) of the speedup characteristics of different tasks and allocates processors accordingly.
Reference: [11] <author> James Tobin. </author> <title> A proposal for international monetary reform. </title> <journal> Eastern Economic Journal, </journal> <volume> 4, </volume> <year> 1978. </year> <month> 21 </month>
Reference-contexts: The recent discussion of using the Tobin tax <ref> [11, 9] </ref> (a small transaction cost on international currency trades) to encourage stability in global currency markets has raised the profile of transaction costs as a policy method; the work here suggests that making them heterogeneous may make them more effective at a lower social cost, although issues of fairness would
References-found: 11

