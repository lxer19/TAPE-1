URL: http://polaris.cs.uiuc.edu/reports/1405.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: Polaris: Improving the Effectiveness of Parallelizing Compilers  
Author: William Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, William Pottenger, Lawrence Rauchwerger, Peng Tu and Stephen Weatherford 
Affiliation: Center for Supercomputing Research and Development Coordinated Science Laboratory University of Illinois  
Abstract: It is the goal of the Polaris project to develop a new parallelizing compiler that will overcome limitations of current compilers. While current parallelizing compilers may succeed on small kernels, they often fail to extract any meaningful parallelism from large applications. After a study of application codes, it was concluded that by adding a few new techniques to current compilers, automatic parallelization becomes possible. The techniques needed are interprocedural analysis, scalar and array privatization, symbolic dependence analysis, and advanced induction and reduction recognition and elimination, along with run-time techniques to allow data dependent behavior.
Abstract-found: 1
Intro-found: 1
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Simplified version of loop nest OLDA/100 from TRFD, before and after induction variable substitution. 3.3.1 Range Test To handle such nonlinear expressions, we have developed a symbolic dependence test called the range test [BE94a]. The range test is an extension of a symbolic version of Triangular Banerjee's Inequalities test <ref> [WB87, Ban88, HP91] </ref>. In the range test, we mark a loop as parallel if we can prove that the range of elements accessed by an iteration of that loop do not overlap with the range of elements accessed by other iterations.
Reference: [BCK + 89] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: New measurements on a representative set of real programs were made possible, thanks to the Perfect Benchmarks R fl effort, which was initiated by CSRD, with participation from many other institutions <ref> [BCK + 89] </ref>. Based on these observations, we have hand parallelized the program suite as a major new approach to identifying effective program transformations [EHLP91, EHJP92].
Reference: [BE92] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks R fl Programs. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Parallelizing compilers have been playing an important role in this quest. The present project has its early roots in a compiler evaluation effort of the late 80s, where we have found that despite the success on kernel benchmarks, available compilers were not very effective on large programs <ref> [EHLP91, BE92] </ref>. New measurements on a representative set of real programs were made possible, thanks to the Perfect Benchmarks R fl effort, which was initiated by CSRD, with participation from many other institutions [BCK + 89].
Reference: [BE94a] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <type> Technical Report 1345, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Simplified version of loop nest OLDA/100 from TRFD, before and after induction variable substitution. 3.3.1 Range Test To handle such nonlinear expressions, we have developed a symbolic dependence test called the range test <ref> [BE94a] </ref>. The range test is an extension of a symbolic version of Triangular Banerjee's Inequalities test [WB87, Ban88, HP91]. <p> See <ref> [BE94a] </ref> for other tests that use these minimum and maximum values of f and g. Returning to the example for Figure 1, we will now apply the dependence test described above to prove that A (f) does not carry any dependences for the outermost loop.
Reference: [BE94b] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic Analysis Techniques Needed for the Effective Paral-lelization of the Perfect Benchmarks. </title> <type> Technical Report 1332, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1994. </year> <note> 5 CSRD reports are available via anonymous FTP from ftp.csrd.uiuc.edu:CSRD Info, or the World Wide Web site http://www.csrd.uiuc.edu </note>
Reference-contexts: In our experience with the Perfect Benchmarks, such nonlinear expressions do occur in practice. In fact, four of the twelve codes (i.e., DYFESM, QCD, OCEAN, and TRFD) that we hand-parallelized would exhibit a speedup of at most two if we could not parallelize loops with nonlinear array subscripts <ref> [BE94b] </ref>. For some of these loops, nonlinear expressions occurred in the original program text. For other loops, nonlinear expressions were introduced by the compiler. The two most common compiler passes that can introduce nonlinearities into array subscript expressions are induction variable substitution and array linearization. <p> We expect the range test to be equally effective. In addition, the symbolic capabilities of the range test permit it to handle many of the symbolic expressions we have seen in the Perfect Benchmarks. Most current data dependence tests cannot handle symbolic subscripts <ref> [BE94b] </ref>. Our implementation of the range test in Polaris supports these claims.
Reference: [EHJP92] <author> Rudolf Eigenmann, Jay Hoeflinger, G. Jaxon, and David Padua. </author> <title> The Cedar Fortran Project. </title> <type> Techni--cal report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res, & Dev., </institution> <month> April </month> <year> 1992. </year> <note> CSRD Report No. 1262. </note>
Reference-contexts: Based on these observations, we have hand parallelized the program suite as a major new approach to identifying effective program transformations <ref> [EHLP91, EHJP92] </ref>. As a result we have found that not only can real applications be parallelized effectively, but the transformations can also be automated in a parallelizing compiler.
Reference: [EHLP91] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Parallelizing compilers have been playing an important role in this quest. The present project has its early roots in a compiler evaluation effort of the late 80s, where we have found that despite the success on kernel benchmarks, available compilers were not very effective on large programs <ref> [EHLP91, BE92] </ref>. New measurements on a representative set of real programs were made possible, thanks to the Perfect Benchmarks R fl effort, which was initiated by CSRD, with participation from many other institutions [BCK + 89]. <p> Based on these observations, we have hand parallelized the program suite as a major new approach to identifying effective program transformations <ref> [EHLP91, EHJP92] </ref>. As a result we have found that not only can real applications be parallelized effectively, but the transformations can also be automated in a parallelizing compiler.
Reference: [FHP + 93] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The Polaris Internal Representation. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. and Dev., </institution> <month> October </month> <year> 1993. </year> <note> CSRD Report No. 1317, UILU-ENG-93-8038. </note>
Reference-contexts: This work is not necessarily representative of the positions or policies of the Army or the Government. 2 Internal Organization of the Compiler The aim in the design of Polaris' internal organization <ref> [FHP + 93] </ref> was to create an internal representation (IR) that enforced correctness, was robust and, through high-level functionality, easy to use. Our view of the IR is that it is more than just the structure of the data within the compiler.
Reference: [HKM91] <author> Mary W. Hall, Ken Kennedy, and Kathryn S. McKinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> Supercomputing'91, </booktitle> <pages> pages 423-434, </pages> <year> 1991. </year>
Reference-contexts: Data collected using complete inline expansion for analysis can be used to guide selective modification of subprograms using such techniques as cloning, loop embedding, and loop extraction <ref> [HKM91] </ref>. All of the programs that we have tested were inlined successfully by Polaris. Some constructs are not easily expressible in Fortran after inline expansion. The constructs which are not fully supported involve the need for expressing an equivalence between non-conforming formal and actual parameters.
Reference: [HP91] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High-Performance Parallelizing Compilers. </title> <booktitle> Parallel and Distributed Computing: Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pages 310-330, </pages> <year> 1991. </year>
Reference-contexts: Simplified version of loop nest OLDA/100 from TRFD, before and after induction variable substitution. 3.3.1 Range Test To handle such nonlinear expressions, we have developed a symbolic dependence test called the range test [BE94a]. The range test is an extension of a symbolic version of Triangular Banerjee's Inequalities test <ref> [WB87, Ban88, HP91] </ref>. In the range test, we mark a loop as parallel if we can prove that the range of elements accessed by an iteration of that loop do not overlap with the range of elements accessed by other iterations.
Reference: [PKK91] <author> K. Psarris, D. Klappholz, and X. Kong. </author> <title> On the accuracy of the Banerjee test. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 152-157, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Simplified version of loop nest FTRVMT/109 from OCEAN The range test subsumes Banerjee's Inequalities, which have been shown to be effective <ref> [PKK91] </ref> for real programs [PP93], in proving loops to be parallel with the assumption that they contain only linear subscript expressions. We expect the range test to be equally effective.
Reference: [PP93] <author> Paul M. Petersen and David A. Padua. </author> <title> Static and Dynamic Evaluation of Data Dependence Analysis. </title> <booktitle> Presented at ICS'93, </booktitle> <address> Tokyo, Japan, </address> <pages> pages 107-116, </pages> <month> July 19-23, </month> <year> 1993. </year>
Reference-contexts: There has been much research in the area of data dependence analysis. Because of this, modern day data dependence tests have become very accurate and efficient <ref> [PP93] </ref>. <p> Simplified version of loop nest FTRVMT/109 from OCEAN The range test subsumes Banerjee's Inequalities, which have been shown to be effective [PKK91] for real programs <ref> [PP93] </ref>, in proving loops to be parallel with the assumption that they contain only linear subscript expressions. We expect the range test to be equally effective.
Reference: [RP94] <author> Lawrence Rauchwerger and David Padua. </author> <title> The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privatization . Technical Report 1329, </title> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. and Dev., </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: In order to implement such a strategy, we have developed a run-time technique, called the Privatizing Doall test (PD test), for detecting the presence of cross-iteration dependences in a loop <ref> [RP94] </ref>. If there are any such dependences, this test does not identify them; it only flags their existence. In addition, if any variables were privatized for speculative parallel execution, this test determines whether those variables were, in fact, validly privatized.
Reference: [SMC91] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: For these cases we are developing run-time methods for the recognition and implementation of parallelism. These techniques are not currently implemented in Polaris but will include inspector/executor <ref> [SMC91] </ref> style implementations as well as implementations based on speculative execution. 3.1 Inline Expansion The Polaris inliner is designed to provide three types of services: complete inline expansion of subprograms for analysis, selective inline expansion of subprograms for code generation, and selective modification of subprograms.
Reference: [TP93] <author> Peng Tu and David Padua. </author> <title> Automatic array privatization. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Com puting, volume 768 of Lecture Notes in Computer Science, </booktitle> <pages> pages 500-521, </pages> <address> Portland, OR, August 1993. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: In our experience, the most important of these transformations is array privatization <ref> [TP93] </ref>. Array privatization is used to eliminate memory-related dependences. It identifies scalars and arrays that are used as temporary work spaces by a loop iteration, and allocates a local copy of those scalars and arrays for that iteration.
Reference: [TP94] <author> Peng Tu and David Padua. </author> <title> Demand-Driven Symbolic Analysis. </title> <type> Technical Report 1336, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> Febraury </month> <year> 1994. </year>
Reference-contexts: Thus, to prove that A is privatizable, we only need to prove that MP M fl P. To prove this, we need to find out how the symbolic variables are related from their global def-use relations. In Polaris, we use a demand-driven algorithm <ref> [TP94] </ref>, based on a Static Single Assignment (SSA) representation, to obtain global information. To obtain the SSA form, program variables are renamed such that each time the variable is defined it is given a new name.
Reference: [WB87] <author> Michael Wolfe and Utpal Banerjee. </author> <title> Data Dependence and its Application to Parallel Processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <year> 1987. </year>
Reference-contexts: Simplified version of loop nest OLDA/100 from TRFD, before and after induction variable substitution. 3.3.1 Range Test To handle such nonlinear expressions, we have developed a symbolic dependence test called the range test [BE94a]. The range test is an extension of a symbolic version of Triangular Banerjee's Inequalities test <ref> [WB87, Ban88, HP91] </ref>. In the range test, we mark a loop as parallel if we can prove that the range of elements accessed by an iteration of that loop do not overlap with the range of elements accessed by other iterations.

References-found: 17

