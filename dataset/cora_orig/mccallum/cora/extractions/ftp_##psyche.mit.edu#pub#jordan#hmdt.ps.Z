URL: ftp://psyche.mit.edu/pub/jordan/hmdt.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Email: fjordan,zoubin,lksaulg@psyche.mit.edu  
Title: Hidden Markov decision trees  
Author: Michael I. Jordan Zoubin Ghahramani and Lawrence K. Saul 
Address: Cambridge, MA USA 02139  Toronto, ON Canada M5S 1A4  
Affiliation: Center for Biological and Computational Learning Massachusetts Institute of Technology  Department of Computer Science University of Toronto  
Abstract: We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales. Accepted for oral presentation at NIPS*96. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y., & Frasconi, P. </author> <year> (1995). </year> <title> An input output HMM architecture. </title> <booktitle> In G. </booktitle>
Reference-contexts: tree models of the Bach chorales dataset: mean percentage of variance explained for each attribute and mean temporal scales at the different nodes. bottom node to modeling pitch or duration. 5 Conclusions Viewed in the context of the burgeoning literature on adaptive graphical probabilistic models|which includes HMM's, HME's, CVQ's, IOHMM's <ref> (Bengio & Frasconi, 1995) </ref>, and factorial HMM's|the HMDT would appear to be a natural next step. The HMDT includes as special cases all of these architectures, moreover it arguably combines their best features: factorized state spaces, conditional densities, representation at multiple levels of resolution and recursive estimation algorithms.
Reference: <editor> Tesauro, D. S. Touretzky & T. K. Leen, (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Ghahramani, Z., & Jordan, M. I. </author> <year> (1996). </year> <title> Factorial hidden Markov models. </title> <editor> In D. </editor> <publisher> S. </publisher>
Reference: <editor> Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Jordan, M. I., & Jacobs, R. A. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6, </volume> <pages> 181-214. </pages>
Reference-contexts: described by Saul and Jordan (1996), which allow tractable substructures (e.g., the decision tree and Markov chain substructures) to be handled via exact methods, within an overall approximation that guarantees a lower bound on the log likelihood. 2 Architectures 2.1 Probabilistic decision trees The "hierarchical mixture of experts" (HME) model <ref> (Jordan & Jacobs, 1994) </ref> is a decision tree in which the decisions are modeled probabilistically, as are the outputs. The total probability of an output given an input is the sum over all paths in the tree from the input to the output.
Reference: <author> Parisi, G. </author> <year> (1988). </year> <title> Statistical Field Theory. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Thus we turn to methods that allow us to approximate the posterior probabilities of interest. 3 Algorithms 3.1 Partially factorized variational approximations Completely factorized approximations to probability distributions on graphs can often be obtained variationally as mean field theories in physics <ref> (Parisi, 1988) </ref>. For the HMDT in Figure 3, the completely factorized mean field approximation would delink all of the nodes, replacing the interactions with constant fields acting at each of the nodes.
Reference: <author> Rabiner, L. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected application s in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77, </volume> <pages> 257-285. </pages>
Reference: <author> Saul, L. K., & Jordan, M. I. </author> <year> (1996). </year> <title> Exploiting tractable substructures in intractable networks. </title> <editor> In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Shachter, R. </author> <year> (1990). </year> <title> An ordered examination of influence diagrams. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 535-563. </pages>
Reference-contexts: The recursion can be viewed as a special case of generic algorithms for calculating posterior probabilities on directed graphs <ref> (see, e.g., Shachter, 1990) </ref>. 1 Throughout the paper we restrict ourselves to three levels for simplicity of presentation. experts as a graphical model. The E step of the learning algorithm for HME's involves calculating the posterior probabilities of the hidden (unshaded) variables given the observed (shaded) variables. model.
References-found: 9

