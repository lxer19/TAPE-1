URL: http://www.cis.udel.edu/~case/papers/noisy-enum.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: Email: case@cis.udel.edu  Email: sanjay@iscs.nus.sg  Email: arun@cse.unsw.edu.au  
Title: Synthesizing Noise-Tolerant Language Learners Stephan's model of noisy data is employed, in which, roughly, correct
Author: John Case Sanjay Jain Arun Sharma 
Note: F.  
Address: 19716, USA  Singapore 119260  Sydney, NSW 2052, Australia  
Affiliation: Department of CIS University of Delaware Newark, DE  Department of ISCS National University of Singapore  School of Computer Science and Engineering The University of New South Wales  
Abstract: Many positive results, as well as some negative results, are presented regarding the existence of such synthesizers. The proofs of most of the positive results yield, as pleasant corollaries, strict subset-principle or tell-tale style characterizations for the noise-tolerant learnability of the corresponding classes or families indexed. 
Abstract-found: 1
Intro-found: 1
Reference: [Ang80] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: In fact the computational learning theory community has shown considerable interest (spanning at least from [Gol67] to [ZL95]) in language classes defined by r.e. listings of decision procedures. These classes are called uniformly decidable or indexed families. As is essentially pointed out in <ref> [Ang80] </ref>, all of the formal language style example classes are indexed families. <p> defining L, i.e., if L is an indexed family, then: L can be Bc-learned from positive data with the learner outputting grammars iff (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from <ref> [Ang80] </ref>, and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data [Ang80, Ber85, ZLK95, KB92, Cas96]. <p> (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas96] </ref>. <p> Hence, this model has the advantage that noisy data about an object nonetheless uniquely specifies that object. We note, though, that the presence of noise plays havoc with the learnability of many concrete classes that can be learned without noise. For example, the well-known class of pattern languages <ref> [Ang80] </ref> can be Ex-learned from texts but cannot be Bc-learned from noisy texts even if we allow the final grammars each to make finitely many mistakes. <p> See, for example, <ref> [Ang80, Muk92, LZK95, dJK96] </ref>. 2 of languages formed by taking the union of two pattern languages can be Ex-learned from texts [Shi83]; however, this class cannot be Bc-learned from noisy informants even if we allow the final grammars each to make finitely many mistakes. <p> In (1) from Section 1, the finite sets S are called tell-tales <ref> [Ang80] </ref>.
Reference: [Bar74] <author> J. Barzdin. </author> <title> Two theorems on the limiting synthesis of functions. In Theory of Algorithms and Programs, Latvian State University, </title> <journal> Riga, </journal> <volume> 210 </volume> <pages> 82-88, </pages> <year> 1974. </year>
Reference-contexts: Example more general learners are: Bc-learners, which, when successful on an object input, (by definition) find a final (possibly infinite) sequence of correct programs for that object after at most finitely many trial and error attempts <ref> [Bar74, CS83] </ref>. 3 Of course, if suitable learner-synthesizer algorithm lsyn is fed procedures for listing decision procedures (instead of mere grammars), one also has more success at synthesizing learners. <p> informant I. * [Gol67, CS83, BB75] M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * <ref> [Bar74, CS83] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. 4 * [Cas96, BP73].
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> The learners so synthesized are called enumeration techniques <ref> [BB75, Ful90] </ref>. These enumeration techniques yield many positive learnability results, for example, that the class of all functions computable in time polynomial in the length of input is Ex-learnable. <p> Definition 1 Suppose a; b 2 N [ fflg. (a) Below, for each of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * [Bar74, CS83]. <p> several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * [Bar74, CS83]. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. <p> We often write TxtEx 0 as TxtEx, and TxtFex a fl as TxtFex a . Similar convention applies to other criteria of inference considered in this paper. Several proofs in this paper depend on the concept of locking sequence. Definition 2 (Based on <ref> [BB75] </ref>) Suppose a; b 2 N [ fflg. (a) is said to be a TxtEx a -locking sequence for M on L iff, content () L, W M () = a L, and (8t j content (t ) L)[M ( t ) = M ()]. (b) is said to be a <p> Lemma 1 (Based on <ref> [BB75] </ref>) Suppose a; b 2 N [ fflg. Suppose J 2 fTxtEx a ; TxtFex a b ; TxtBc a g.
Reference: [BCJ96] <author> G. Baliga, J. Case, and S. Jain. </author> <title> Synthesizing enumeration techniques for language learn-ing. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, Desenzano del Garda, Italy, </booktitle> <pages> pages 169-180. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: pro-vided an amazingly negative result: there is no learner-synthesizer algorithm lsyn so that, if lsyn is fed a pair of grammars g 1 ; g 2 for a language class L = fL 1 ; L 2 g, then lsyn outputs an Ex-learner successful, from positive data, on L. 2 <ref> [BCJ96] </ref> showed how to circumvent some of the sting of this [OSW88] result by resorting to more general learners than Ex. <p> These classes are called uniformly decidable or indexed families. As is essentially pointed out in [Ang80], all of the formal language style example classes are indexed families. A sample result from <ref> [BCJ96] </ref> is: there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists decision procedures defining some indexed family L of languages which can be Bc-learned from positive data with the learner outputting grammars, then lsyn outputs a Bc-learner successful, from positive data, on L. <p> The proof of this positive result yielded the surprising characterization <ref> [BCJ96] </ref>: if there is an r.e. listing of decision procedures defining L, i.e., if L is an indexed family, then: L can be Bc-learned from positive data with the learner outputting grammars iff (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 <p> He shows that one cannot algorithmically find an Ex-learning machine for Ex-learnable indexed families of recursive languages from an index of the class. This is a bit weaker than a closely related negative result from <ref> [BCJ96] </ref>. 3 Bc is short for behaviorally correct. 4 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions.
Reference: [Ber85] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas96] </ref>.
Reference: [Blu67] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: W i denotes the domain of ' i . W i is considered as the language enumerated by the i-th program in ' system, and we say that i is a grammar or index for W i . denotes a standard Blum complexity measure <ref> [Blu67] </ref> for the programming system '. W i;s = fx &lt; s j i (x) &lt; sg. A text is a mapping from N to N [ f#g.
Reference: [BP73] <author> J. Barzdin and K. Podnieks. </author> <title> The theory of inductive inference. </title> <booktitle> In Mathematical Foundations of Computer Science, </booktitle> <year> 1973. </year>
Reference-contexts: M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. 4 * <ref> [Cas96, BP73] </ref>. M TxtFex a b -identifies L from text T iff (9S j card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. InfFex a b is defined similarly.
Reference: [Cas74] <author> J. </author> <title> Case. Periodicity in generations of automata. </title> <journal> Mathematical Systems Theory, </journal> <volume> 8 </volume> <pages> 15-32, </pages> <year> 1974. </year>
Reference-contexts: Theorem 7 NOT (9f 2 R)(9n 2 N )(8x j C x 2 NoisyTxtEx " NoisyInfEx)[C x TxtBc n (M f (x) ) [ TxtFex fl (M f (x) )]. 8 Proof. Fix f and n. By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing recursive function p such that the languages W p (i) , i 0, are defined as follows. Enumerate p (1) in W p (0) . W p (1) will be a subset of ODD. <p> The following theorem shows that effective synthesis, from decision procedures, cannot be done in the case of NoisyInfEx fl -identification. Theorem 16 NOT (9f 2 R)(8x j U x 2 NoisyInfEx fl )[U x TxtFex fl (M f (x) )]. Proof. Fix f . By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing recursive function p such that W p (0) , and ' p (i) , i 1, are defined as follows. For all x 2 N , ' p (1) (x) = 1. Enumerate p (1) in W p (0) .
Reference: [Cas94] <author> J. </author> <title> Case. Infinitary self-reference in learning theory. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 3-16, </pages> <year> 1994. </year>
Reference-contexts: Theorem 7 NOT (9f 2 R)(9n 2 N )(8x j C x 2 NoisyTxtEx " NoisyInfEx)[C x TxtBc n (M f (x) ) [ TxtFex fl (M f (x) )]. 8 Proof. Fix f and n. By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing recursive function p such that the languages W p (i) , i 0, are defined as follows. Enumerate p (1) in W p (0) . W p (1) will be a subset of ODD. <p> The following theorem shows that effective synthesis, from decision procedures, cannot be done in the case of NoisyInfEx fl -identification. Theorem 16 NOT (9f 2 R)(8x j U x 2 NoisyInfEx fl )[U x TxtFex fl (M f (x) )]. Proof. Fix f . By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing recursive function p such that W p (0) , and ' p (i) , i 1, are defined as follows. For all x 2 N , ' p (1) (x) = 1. Enumerate p (1) in W p (0) .
Reference: [Cas96] <author> J. </author> <title> Case. </title> <booktitle> The power of vacillation in language learning. Technical Report LP-96-08, Logic, Philosophy and Linguistics Series of the Institute for Logic, Language and Computation, </booktitle> <institution> University of Amsterdam, </institution> <year> 1996. </year> <note> To appear revised in SIAM Journal on Computing. </note>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas96] </ref>. <p> M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. 4 * <ref> [Cas96, BP73] </ref>. M TxtFex a b -identifies L from text T iff (9S j card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. InfFex a b is defined similarly.
Reference: [CJS96] <author> J. Case, S. Jain, and F. Stephan. </author> <title> Vacillatory and BC learning on noisy data. </title> <editor> In S. Arikawa and A. Sharma, editors, </editor> <booktitle> Proceedings of the Seventh International Workshop on Algorithmic Learning Theory, </booktitle> <address> Sydney, Australia, pages 285- 298. </address> <publisher> Springer Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: our positive results which provide the existence of learner-synthesizers which synthesize noise-tolerant learners also yield pleasant characterizations which look like strict versions of the subset principle (1). 4 We consider language learning from both texts (only positive data) and from informants (both positive and negative data), and we adopt Stephan's <ref> [Ste95, CJS96] </ref> noise model for the present study. Roughly, in this model correct information about an object occurs infinitely often while incorrect information occurs only finitely often. Hence, this model has the advantage that noisy data about an object nonetheless uniquely specifies that object. <p> In the case of informant every false item (x; L (x)) may occur a finite number of times. In the case of text, it is mathematically more interesting to require, as we do, that the total amount of false information has to be finite. 7 Definition 4 <ref> [Ste95, CJS96] </ref> Suppose a; b 2 N [ fflg. Suppose J 2 fTxtEx a b ; TxtFex a b ; TxtBc a g. Then M NoisyJ-identifies L iff, for all noisy texts T for L, M J-identifies L from T . In this case we write L 2 NoisyJ (M). <p> However, definition of locking sequence for learning from noisy informant is more involved. 7 The alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets [Ste95]. 6 Definition 5 <ref> [CJS96] </ref> Suppose a; b 2 N [ fflg. (a) is said to be a NoisyTxtEx a -locking sequence for M on L iff, W M () = a L, and (8t j content (t ) L)[M ( t ) = M ()]. (b) is said to be a NoisyTxtBc a -locking <p> For the criteria of noisy inference discussed in this paper one can prove the existence of a locking sequence as was done in [Ste95, Theorem 2, proof for NoisyEx Ex 0 [K] ]. Proposition 1 <ref> [CJS96] </ref> Suppose a; b 2 N [ fflg. <p> For all i, fU j j j 2 W i g; if (8j 2 W i )[j is a decision procedure]; ;; otherwise. 7 2.3 Some Previous Results on Noisy Text/Informant Identification We first state some results from <ref> [CJS96] </ref> which are useful. Theorem 1 [CJS96] Suppose a 2 N [ fflg. L 2 NoisyTxtBc a ) [(8L 2 L)(8L 0 2 L j L 0 L)[L = 2a L 0 ]]. <p> For all i, fU j j j 2 W i g; if (8j 2 W i )[j is a decision procedure]; ;; otherwise. 7 2.3 Some Previous Results on Noisy Text/Informant Identification We first state some results from <ref> [CJS96] </ref> which are useful. Theorem 1 [CJS96] Suppose a 2 N [ fflg. L 2 NoisyTxtBc a ) [(8L 2 L)(8L 0 2 L j L 0 L)[L = 2a L 0 ]]. As an immediate corollary to Proposition 1 we have the following two theorems, Theorem 2 Suppose a 2 N [ fflg. <p> Then for all L 2 L, there exists an n such that, (8L 0 2 L j fx 2 L j x ng = fx 2 L 0 j x ng)[L = a L 0 ]. The following theorem was proved in <ref> [CJS96] </ref>. Theorem 6 Suppose a 2 N [ fflg. Then NoisyInfBc a [ NoisyTxtBc a TxtBc a and NoisyInfEx a [ NoisyTxtEx a TxtEx a . <p> Since, from a machine M, one can effectively construct a machine M 0 which NoisyInfBc a -identifies NoisyInfEx 2a (M), (see <ref> [CJS96] </ref>) we immediately have (using Corollary 4 for the fl-case) the fol lowing result about effective synthesis for NoisyInfBc a -identification. Corollary 16 Suppose a 2 N [ fflg. (9f 2 R)(8i j U i 2 NoisyInfBc a )[U i NoisyInfBc a (M f (i) )].
Reference: [CL82] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <editor> In M. Nielsen and E. Schmidt, editors, </editor> <booktitle> Proceedings of the 9th International Colloquium on Automata, Languages and Programming, </booktitle> <volume> volume 140, </volume> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67].
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> Example more general learners are: Bc-learners, which, when successful on an object input, (by definition) find a final (possibly infinite) sequence of correct programs for that object after at most finitely many trial and error attempts <ref> [Bar74, CS83] </ref>. 3 Of course, if suitable learner-synthesizer algorithm lsyn is fed procedures for listing decision procedures (instead of mere grammars), one also has more success at synthesizing learners. <p> Definition 1 Suppose a; b 2 N [ fflg. (a) Below, for each of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * [Bar74, CS83]. <p> several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * [Bar74, CS83]. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. <p> informant I. * [Gol67, CS83, BB75] M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * <ref> [Bar74, CS83] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. InfBc a -identification is defined similarly. 4 * [Cas96, BP73].
Reference: [dJK96] <author> D. de Jongh and M. </author> <title> Kanazawa. Angluin's thoerem for indexed families of r.e. sets and applications. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, Desenzano del Garda, Italy, </booktitle> <pages> pages 193-204. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: See, for example, <ref> [Ang80, Muk92, LZK95, dJK96] </ref>. 2 of languages formed by taking the union of two pattern languages can be Ex-learned from texts [Shi83]; however, this class cannot be Bc-learned from noisy informants even if we allow the final grammars each to make finitely many mistakes.
Reference: [Ful90] <author> M. Fulk. </author> <title> Robust separations in inductive inference. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 405-410, </pages> <address> St. Louis, Missouri 1990. </address>
Reference-contexts: The learners so synthesized are called enumeration techniques <ref> [BB75, Ful90] </ref>. These enumeration techniques yield many positive learnability results, for example, that the class of all functions computable in time polynomial in the length of input is Ex-learnable.
Reference: [Gol67] <author> E. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> at most finitely many trial and error attempts [Gol67, BB75, CS83, CL82]. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S <ref> [Gol67] </ref>. The learners so synthesized are called enumeration techniques [BB75, Ful90]. These enumeration techniques yield many positive learnability results, for example, that the class of all functions computable in time polynomial in the length of input is Ex-learnable. <p> In fact the computational learning theory community has shown considerable interest (spanning at least from <ref> [Gol67] </ref> to [ZL95]) in language classes defined by r.e. listings of decision procedures. These classes are called uniformly decidable or indexed families. As is essentially pointed out in [Ang80], all of the formal language style example classes are indexed families. <p> Definition 1 Suppose a; b 2 N [ fflg. (a) Below, for each of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * [Bar74, CS83]. <p> several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CS83, BB75] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CS83, BB75] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I)# = i]. * [Bar74, CS83]. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L].
Reference: [Jan79] <author> K. </author> <title> Jantke. Automatic synthesis of programs and inductive inference of functions. </title> <booktitle> In Int. Conf. Fundamentals of Computations Theory, </booktitle> <pages> pages 219-225, </pages> <year> 1979. </year>
Reference-contexts: The learners so synthesized are called enumeration techniques [BB75, Ful90]. These enumeration techniques yield many positive learnability results, for example, that the class of all functions computable in time polynomial in the length of input is Ex-learnable. The reader is referred to Jantke <ref> [Jan79] </ref> for a discussion of synthesizing learners for classes of recursive functions that are not necessarily recursively enumerable. fl This paper is dedicated to the memory of Mark Fulk. 1 Ex is short for explanatory. 1 For language learning from positive data and with learners outputting grammars, [OSW88] pro-vided an amazingly
Reference: [Kap91] <author> S. Kapur. </author> <title> Computational Learning of Languages. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1991. </year>
Reference-contexts: The class 2 Again for language learning from positive data and with learners outputting grammars, a somewhat related negative result is provided by Kapur <ref> [Kap91] </ref>. He shows that one cannot algorithmically find an Ex-learning machine for Ex-learnable indexed families of recursive languages from an index of the class.
Reference: [KB92] <author> S. Kapur and G. Bilardi. </author> <title> Language learning without overgeneralization. </title> <booktitle> In Proceedings of the Ninth Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science 577. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> 21 </month>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas96] </ref>.
Reference: [LZK95] <author> S. Lange, T. Zeugmann, and S. Kapur. </author> <title> Monotonic and dual monotonic language learning. </title> <note> Theoretical Computer Science A, 1995. To appear. </note>
Reference-contexts: must be paid: any Ex-learner succeeding on the pattern languages from noisy informant must change its mind an unbounded finite number of times about the final grammar; however, some learner can succeed on the pattern languages from noise-free informants and on its first guess as to a correct grammar (see <ref> [LZK95] </ref>). The class 2 Again for language learning from positive data and with learners outputting grammars, a somewhat related negative result is provided by Kapur [Kap91]. He shows that one cannot algorithmically find an Ex-learning machine for Ex-learnable indexed families of recursive languages from an index of the class. <p> See, for example, <ref> [Ang80, Muk92, LZK95, dJK96] </ref>. 2 of languages formed by taking the union of two pattern languages can be Ex-learned from texts [Shi83]; however, this class cannot be Bc-learned from noisy informants even if we allow the final grammars each to make finitely many mistakes.
Reference: [Muk92] <author> Y. Mukouchi. </author> <title> Characterization of finite identification. </title> <editor> In K. P. Jantke, editor, </editor> <booktitle> Proceedings of the Third International Workshop on Analogical and Inductive Inference, </booktitle> <address> Dagstuhl Castle, Germany, </address> <pages> pages 260-267, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: See, for example, <ref> [Ang80, Muk92, LZK95, dJK96] </ref>. 2 of languages formed by taking the union of two pattern languages can be Ex-learned from texts [Shi83]; however, this class cannot be Bc-learned from noisy informants even if we allow the final grammars each to make finitely many mistakes.
Reference: [Odi89] <author> P. Odifreddi. </author> <title> Classical Recursion Theory. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: If n = 0, then we get a characterization of indexed families L noise-tolerantly Ex-learnable, from texts, also by (2) above! 2 Preliminaries 2.1 Notation and identification criteria The recursion theoretic notions are from the books of Odifreddi <ref> [Odi89] </ref> and Soare [Soa87]. N = f0; 1; 2; : : :g is the set of all natural numbers, and this paper considers r.e. subsets L of N . N + = f1; 2; 3; : : :g, the set of all positive integers.
Reference: [OSW88] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Synthesizing inductive expertise. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 138-161, </pages> <year> 1988. </year>
Reference-contexts: referred to Jantke [Jan79] for a discussion of synthesizing learners for classes of recursive functions that are not necessarily recursively enumerable. fl This paper is dedicated to the memory of Mark Fulk. 1 Ex is short for explanatory. 1 For language learning from positive data and with learners outputting grammars, <ref> [OSW88] </ref> pro-vided an amazingly negative result: there is no learner-synthesizer algorithm lsyn so that, if lsyn is fed a pair of grammars g 1 ; g 2 for a language class L = fL 1 ; L 2 g, then lsyn outputs an Ex-learner successful, from positive data, on L. 2 <p> so that, if lsyn is fed a pair of grammars g 1 ; g 2 for a language class L = fL 1 ; L 2 g, then lsyn outputs an Ex-learner successful, from positive data, on L. 2 [BCJ96] showed how to circumvent some of the sting of this <ref> [OSW88] </ref> result by resorting to more general learners than Ex.
Reference: [Shi83] <author> T. Shinohara. </author> <title> Inferring unions of two pattern languages. </title> <journal> Bulletin of Informatics and Cybernetics, </journal> <volume> 20 </volume> <pages> 83-88., </pages> <year> 1983. </year>
Reference-contexts: See, for example, [Ang80, Muk92, LZK95, dJK96]. 2 of languages formed by taking the union of two pattern languages can be Ex-learned from texts <ref> [Shi83] </ref>; however, this class cannot be Bc-learned from noisy informants even if we allow the final grammars each to make finitely many mistakes.
Reference: [Soa87] <author> R. Soare. </author> <title> Recursively Enumerable Sets and Degrees. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: If n = 0, then we get a characterization of indexed families L noise-tolerantly Ex-learnable, from texts, also by (2) above! 2 Preliminaries 2.1 Notation and identification criteria The recursion theoretic notions are from the books of Odifreddi [Odi89] and Soare <ref> [Soa87] </ref>. N = f0; 1; 2; : : :g is the set of all natural numbers, and this paper considers r.e. subsets L of N . N + = f1; 2; 3; : : :g, the set of all positive integers.
Reference: [Ste95] <author> F. Stephan. </author> <title> Noisy inference and oracles. In Algorithmic Learning Theory, </title> <booktitle> 6th International Workshop, </booktitle> <address> ALT'95, Fukuoka, Japan, </address> <pages> pages 185-200. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1995. </year> <booktitle> Lecture Notes in Artificial Intelligence 997. </booktitle>
Reference-contexts: our positive results which provide the existence of learner-synthesizers which synthesize noise-tolerant learners also yield pleasant characterizations which look like strict versions of the subset principle (1). 4 We consider language learning from both texts (only positive data) and from informants (both positive and negative data), and we adopt Stephan's <ref> [Ste95, CJS96] </ref> noise model for the present study. Roughly, in this model correct information about an object occurs infinitely often while incorrect information occurs only finitely often. Hence, this model has the advantage that noisy data about an object nonetheless uniquely specifies that object. <p> Definition 3 <ref> [Ste95] </ref> An information sequence I is a noisy information sequence (or noisy informant) for L iff (8x) [occur (I; (x; L (x))) = 1 ^ occur (I; (x; L (x))) &lt; 1]. <p> In the case of informant every false item (x; L (x)) may occur a finite number of times. In the case of text, it is mathematically more interesting to require, as we do, that the total amount of false information has to be finite. 7 Definition 4 <ref> [Ste95, CJS96] </ref> Suppose a; b 2 N [ fflg. Suppose J 2 fTxtEx a b ; TxtFex a b ; TxtBc a g. Then M NoisyJ-identifies L iff, for all noisy texts T for L, M J-identifies L from T . In this case we write L 2 NoisyJ (M). <p> However, definition of locking sequence for learning from noisy informant is more involved. 7 The alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets <ref> [Ste95] </ref>. 6 Definition 5 [CJS96] Suppose a; b 2 N [ fflg. (a) is said to be a NoisyTxtEx a -locking sequence for M on L iff, W M () = a L, and (8t j content (t ) L)[M ( t ) = M ()]. (b) is said to be <p> For the criteria of noisy inference discussed in this paper one can prove the existence of a locking sequence as was done in <ref> [Ste95, Theorem 2, proof for NoisyEx Ex 0 [K] </ref> ]. Proposition 1 [CJS96] Suppose a; b 2 N [ fflg.
Reference: [ZL95] <author> T. Zeugmann and S. Lange. </author> <title> A guided tour across the boundaries of learning recursive languages. </title> <editor> In Klaus P. Jantke and Steffen Lange, editors, </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 190-258. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: In fact the computational learning theory community has shown considerable interest (spanning at least from [Gol67] to <ref> [ZL95] </ref>) in language classes defined by r.e. listings of decision procedures. These classes are called uniformly decidable or indexed families. As is essentially pointed out in [Ang80], all of the formal language style example classes are indexed families.
Reference: [ZLK95] <author> Thomas Zeugmann, Steffen Lange, and Shyam Kapur. </author> <title> Characterizations of monotonic and dual monotonic language learning. </title> <journal> Information and Computation, </journal> <volume> 120(2) </volume> <pages> 155-173, </pages> <month> August 1 </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas96] </ref>.
References-found: 28

