URL: http://robotics.stanford.edu/users/sahami/papers-dir/thesis.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: http://www.robotics.stanford.edu
Title: USING MACHINE LEARNING TO IMPROVE INFORMATION ACCESS  
Degree: a dissertation submitted to the department of computer science and the committee on graduate studies of stanford university in partial fulfillment of the requirements for the degree of doctor of philosophy By Mehran Sahami  
Date: December 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Allan, J., Carbonell, J. G., Doddington, G., Yamron, J., and Yang, Y. </author> <title> Topic detection and tracking pilot study final report. In Proceedings of the Broadcast News Transcription and Understanding Workshop (1998). </title>
Reference-contexts: Foremost among these is the detection of new topics that do not neatly fit into an existing classification scheme. This area has the potential to benefit from combining previous work in topic detection <ref> [1, 176] </ref>, with new ideas in clustering. Indeed, this issue will become more important as the domain of digital libraries moves from the classic topic structure of existing libraries and into the realm of organizing the quickly evolving body of information on the World Wide Web.
Reference: [2] <author> Allan, J., Leouski, A. V., and Swan, R. C. </author> <title> Interactive cluster visualization for information retrieval. </title> <type> Tech. Rep. </type> <institution> IR-116, Uni. of Mass., Amherst, </institution> <note> Center for Intelligent Information Retrieval, </note> <year> 1997. </year>
Reference-contexts: The ability to automatically create meaningful document groupings relies on the validity of the Cluster Hypothesis. Recall that this hypothesis states that "closely associated documents tend to be relevant to the same requests" [166]. As we described in Chapter 4, a good deal of previous work <ref> [167, 127, 70, 71, 2] </ref> has provided significant support for the cluster hypothesis, as have our more recent results reported in Chapter 6. Thus, we have strong reason to believe that clustering may be used as an effective tool to help organize documents.
Reference: [3] <author> Allen, R. B., Obry, P., and Littman, M. </author> <title> An interface for navigating clustered document sets returned by queries. </title> <booktitle> In Proceedings of ACM SIGOIS (1993), </booktitle> <pages> pp. 166-171. </pages>
Reference: [4] <author> Almuallim, H., Akiba, Y., and Kaneda, S. </author> <title> An efficient algorithm for finding optimal gain-ratio multiple-split tests on hierarchical attibutes in decision tree learning. </title> <booktitle> In Thirteenth National Conference on Artificial Intelligence (1996), </booktitle> <pages> pp. 703-708. </pages>
Reference: [5] <author> Almuallim, H., and Dietterich, T. G. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Ninth National Conference on Artificial Intelligence (1991), </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 547-552. </pages>
Reference: [6] <institution> AltaVista. Digital Equipment Corporation internet search service. </institution> <note> http://www.altavista.digital.com/, 1995. </note>
Reference: [7] <author> Apte, C., Damerau, F., and Weiss, S. M. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> Transactions of Office Information Systems 12, </journal> <note> 3 (1994). Special Issue on Text Categorization. 205 BIBLIOGRAPHY 206 </note>
Reference: [8] <author> Balabanovic, M. </author> <title> An adaptive web page recommendation service. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents (1997). </booktitle>
Reference: [9] <author> Balabanovic, M., and Shoham, Y. Fab: </author> <title> Content-based, collaborative recommendation. </title> <journal> Communications of the ACM 40, </journal> <month> 3 </month> <year> (1997). </year>
Reference: [10] <author> Baldonado, M., Chang, C.-C. K., Gravano, L., and Paepcke, A. </author> <title> The Stanford digital library metadata architecture. </title> <booktitle> International Journal of Digital Libraries 1, </booktitle> <month> 2 </month> <year> (1997). </year>
Reference-contexts: Finally, Section 10.5 gives a summary of this work and its future directions. 10.2 SONIA on the InfoBus The focus of the Stanford Digital Libraries project is on providing interoperability among heterogeneous, distributed information sources, services and interfaces. To this end, the InfoBus architecture <ref> [10] </ref> shown in Figure 10.1 has been developed. In brief, the InfoBus is comprised of network proxies that encapsulate the protocols used by disparate interfaces, information sources, and information services.
Reference: [11] <author> Baldonado, M. Q. W., and Winograd, T. SenseMaker: </author> <title> An information-exploration interface supporting the contextual evolution of a user's interests. </title> <booktitle> In Proceedings of CHI (1997). </booktitle>
Reference-contexts: Moreover, SONIA allows a single user to save several distinct hierarchies to reflect each of his or her diverse information needs. Previously, the functionality in SONIA was accessible through the Java-based SenseMaker interface <ref> [11] </ref>. SenseMaker allows users to simultaneously query multiple heterogeneous information sources and then organize the retrieved documents by matching titles, matching URLs (for Web documents), and the like, or it can utilize SONIA to cluster documents by their full-text content [142, 143].
Reference: [12] <author> Bhattacharyya, A. </author> <title> On a measure of divergence between two statistical populations defined by their probability distributions. </title> <journal> Bull. Calcutta Math. Soc. </journal> <volume> 35 (1943), </volume> <pages> 99-109. </pages>
Reference: [13] <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <title> Occam's razor. </title> <booktitle> Information Processing Letters 24 (1987), </booktitle> <pages> 377-380. </pages>
Reference: [14] <author> Boutilier, C., Friedman, N., Goldszmidt, M., and Koller, D. </author> <title> Context-specific independence in bayesian networks. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (1996), </booktitle> <pages> pp. 115-123. </pages>
Reference-contexts: As mentioned previously, we conjecture that the reason for this shortcoming arises from the fact that the dependency models for the different classes are quite different. (This phenomenon is a form of context-specific independence <ref> [54, 14] </ref>.) The flat classifier is required to capture, within a single model, a complex dependency structure resulting from aggregating all of these disparate dependency structures.
Reference: [15] <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference: [16] <author> Buntine, W. </author> <title> Learning classification trees. </title> <journal> Statistics and Computing 2 (June 1992), </journal> <pages> 63-73. </pages>
Reference: [17] <author> Buntine, W. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research 2 (1994), </journal> <pages> 159-225. BIBLIOGRAPHY 207 </pages>
Reference: [18] <author> Callan, J. </author> <title> Characteristics of text. </title> <address> http://hobart.cs.umass.edu/~allan/cs646/char of text.html, </address> <year> 1997. </year>
Reference: [19] <author> Callan, J., Croft, W. B., and Broglio, J. </author> <title> TREC and TIPSTER experiments with INQUERY. </title> <booktitle> Information Processing and Management 32 (1995), </booktitle> <pages> 327-332. </pages>
Reference: [20] <author> Caruana, R., and Freitag, D. </author> <title> Greedy attribute selection. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference (1994), </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: [21] <author> Charniak, E. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [22] <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., and Freeman, D. </author> <title> AutoClass: a bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (1988), </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 54-64. </pages> <note> Also appears in Readings in Machine Learning edited by J. </note> <editor> Shavlik and T. </editor> <publisher> Dietterich. </publisher>
Reference: [23] <author> Cheeseman, P., and Stutz, J. </author> <title> Bayesian classification (AutoClass): Theory and results. In Advances in Knowledge Discovery and Data Mining, </title> <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Eds. </editor> <publisher> AAAI Press/MIT Press, </publisher> <year> 1996. </year>
Reference: [24] <author> Chen, S. F., and Goodman, J. T. </author> <title> An empirical study of smoothing techniques for language modeling. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (1996), </booktitle> <pages> pp. 310-318. </pages>
Reference: [25] <author> Chickering, D. M. </author> <title> Learning bayesian networks is NP-complete. </title> <booktitle> In Lecture Notes in Statistics (1995). </booktitle>
Reference: [26] <author> Chow, C., and Liu, C. </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Transactions on Information Theory 114 (1968), </journal> <pages> 462-467. BIBLIOGRAPHY 208 </pages>
Reference: [27] <author> Cohen, W. W. </author> <title> Learning rules that classify e-mail. </title> <booktitle> In Proceedings of the 1996 AAAI Spring Symposium on Machine Learning in Information Access (1996). </booktitle>
Reference: [28] <author> Cohen, W. W. </author> <title> Learning trees and rules with set-valued features. </title> <booktitle> In AAAI-96: Proceedings of the Thirteenth National Conference on Artificial Intelligence (1996). </booktitle>
Reference: [29] <author> Cohen, W. W., and Singer, Y. </author> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of the 19th Annual ACM SIGIR Conference (1996). </booktitle>
Reference: [30] <author> Cooper, G. F. </author> <title> Probabilistic inference using belief networks is NP-Hard. </title> <type> Tech. Rep. </type> <institution> KSL-87-27, Stanford Knowledge Systems Laboratory, </institution> <year> 1987. </year>
Reference: [31] <author> Cooper, G. F., and Herskovits, E. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9 (1992), </booktitle> <pages> 309-347. </pages>
Reference: [32] <author> Cooper, W. S. </author> <title> The formalism of probability theory in IR: </title> <booktitle> A foundation for an encumbrance? In Proceedings of the 17th Annual International ACM/SIGIR Conference (Dublin, </booktitle> <address> Ireland, </address> <year> 1994), </year> <pages> pp. 242-247. </pages>
Reference: [33] <author> Cooper, W. S., and Maron, M. E. </author> <title> Foundations of probabilistic and utility-theoretic indexing. </title> <journal> Journal of the Association for Computing Machinery 25, </journal> <volume> 1 (1978), </volume> <pages> 67-80. </pages>
Reference: [34] <author> Cormen, T. H., Leiserson, C. E., and Rivest, R. L. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference: [35] <author> Cover, T. M., and Thomas, J. A. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [36] <author> Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., and Slattery, S. </author> <title> Learning to extract symbolic knowledge from the world wide web. </title> <booktitle> In AAAI-98: Proceedings of the Fifteenth National Conference on Artificial Intelligence (1998). </booktitle> <address> BIBLIOGRAPHY 209 </address>
Reference: [37] <author> Crawford, S. L., Fung, R. M., Appelbaum, L. A., and Tong, R. M. </author> <title> Classification trees for information retrieval. </title> <booktitle> In Machine Learning: Proceedings of the Eighth International Workshop (1991), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 245-249. </pages>
Reference: [38] <author> Cristianini, N., Shawe-Taylor, J., and Sykacek, P. </author> <title> Bayesian classifiers are large margin hyperplanes in a Hilbert space. </title> <booktitle> In Machine Learning: Proceedings of the Fifteenth International Conference (1998). </booktitle>
Reference: [39] <author> Cutting, D. R., Karger, D. R., Pederson, J. O., and Tukey, J. W. Scatter/Gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of ACM/SIGIR (1992), </booktitle> <pages> pp. 318-329. </pages>
Reference: [40] <author> Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science 41, </journal> <volume> 6 (1990), </volume> <pages> 391-407. </pages>
Reference: [41] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B 39 (1977), </journal> <pages> 1-39. </pages>
Reference: [42] <author> Dietterich, T. G., and Shavlik, J. W., </author> <title> Eds. </title> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference: [43] <author> Domingos, P., and Pazzani, M. </author> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <booktitle> Machine Learning 29 (1997), </booktitle> <pages> 103-130. </pages>
Reference: [44] <author> Dougherty, J., Kohavi, R., and Sahami, M. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (July 1995), </booktitle> <editor> A. Prieditis and S. Russell, Eds., </editor> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: [45] <author> Draper, D., and Hanks, S. </author> <title> Localized partial evaluation of belief networks. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI '94) (1994), </booktitle> <pages> pp. 170-177. BIBLIOGRAPHY 210 </pages>
Reference: [46] <author> Duda, R., and Hart, P. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference: [47] <author> Dumais, S. T. </author> <title> Latent semantic indexing (LSI) and TREC-2. </title> <booktitle> In Proceeding of the Second Text REtrieval Conference (TREC-2), </booktitle> <editor> D. K. Harman, Ed. </editor> <booktitle> National Institute of Standards and Technology, </booktitle> <year> 1993, </year> <pages> pp. 105-115. </pages>
Reference: [48] <author> Dumais, S. T., Platt, J., Heckerman, D., and Sahami, M. </author> <title> Inductive learning algorithms and representations for text categorization. </title> <booktitle> In CIKM-98: Proceedings of the Seventh International Conference on Information and Knowledge Management (1998). </booktitle>
Reference-contexts: To further corroborate the need for variance control, we point out that Support Vector Machines (which provide an explicit variance control mechanism) have very recently shown excellent results in text domains <ref> [82, 48] </ref>. Furthermore, even our clustering results show that, by making use of strong parameter smoothing techniques (another variance control method), we can often obtain superior results.
Reference: [49] <author> Ezawa, K. J., and Schuermann, T. </author> <title> Fraud/uncollectible debt detection using a bayesian network learning system. </title> <booktitle> In Uncertainty in Artificial Intelligence (1995), </booktitle> <publisher> Elsevier Science, </publisher> <pages> pp. 157-166. </pages>
Reference: [50] <author> Frakes, W. B. </author> <title> Stemming algorithms. In Information Retrieval: Data Structures and Algorithms, </title> <editor> W. B. Frakes and R. Baeza-Yates, Eds. </editor> <publisher> Prentice Hall, </publisher> <year> 1992, </year> <pages> pp. 131-160. </pages>
Reference: [51] <author> Frakes, W. B., and Baeza-Yates, R. </author> <title> Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference: [52] <author> Friedman, N., Geiger, D., and Goldszmidt, M. </author> <title> Bayesian network classifiers. </title> <booktitle> Machine Learning 29 (1997), </booktitle> <pages> 131-163. </pages>
Reference: [53] <author> Friedman, N., and Goldszmidt, M. </author> <title> Building classifiers using bayesian networks. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (1996). </booktitle>
Reference: [54] <author> Friedman, N., and Goldszmidt, M. </author> <title> Learning bayesian networks with local structure. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (1996), </booktitle> <pages> pp. 252-262. </pages>
Reference-contexts: As mentioned previously, we conjecture that the reason for this shortcoming arises from the fact that the dependency models for the different classes are quite different. (This phenomenon is a form of context-specific independence <ref> [54, 14] </ref>.) The flat classifier is required to capture, within a single model, a complex dependency structure resulting from aggregating all of these disparate dependency structures.
Reference: [55] <author> Fuernkranz, J., Mitchell, T., and Riloff, E. </author> <title> A case study in using linguistic phrases for text categorization on the WWW. In Learning for Text BIBLIOGRAPHY 211 Categorization: </title> <note> Papers from the 1998 AAAI Workshop (Technical Report WS-98-05) (1998), M. Sahami, Ed. </note>
Reference: [56] <author> Fuhr, N. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management 25, 1 (1989), </booktitle> <pages> 55-72. </pages>
Reference: [57] <author> Fuhr, N. </author> <title> Probabilistic models in information retrieval. </title> <journal> The Computer Journal 35, </journal> <volume> 3 (1992), </volume> <pages> 243-255. </pages>
Reference: [58] <author> Fukunaga, K. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference: [59] <author> Fung, R., and DelFavero, B. </author> <title> Applying bayesian networks to information retrieval. </title> <journal> Communications of the ACM 38, </journal> <volume> 3 (1995), </volume> <pages> 42-48. </pages>
Reference: [60] <author> Geiger, D. </author> <title> An entropy-based learning algorithm of bayesian conditional trees. </title> <booktitle> In Uncertainty in Artificial Intelligence (1992), </booktitle> <publisher> Elsevier Science, </publisher> <pages> pp. 92-97. </pages>
Reference: [61] <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation 4 (1992), </booktitle> <pages> 1-48. </pages>
Reference: [62] <author> Girosi, F. </author> <title> An equivalence between sparse approximation and support vector machines. </title> <type> Tech. Rep. AI Memo 1606, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1997. </year> <note> To appear in Neural Computation. </note>
Reference: [63] <author> Goldszmidt, M., and Sahami, M. </author> <title> A probabilistic approach to full-text document clustering. </title> <type> Tech. Rep. </type> <institution> ITAD-433-MS-98-044, SRI International, </institution> <year> 1998. </year>
Reference: [64] <author> Good, I. J. </author> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press, </publisher> <year> 1965. </year>
Reference: [65] <author> Harman, D. K., and Voorhees, E. M., </author> <title> Eds. </title> <booktitle> Proceedings of the Sixth Text REtrieval Conference (TREC-6). </booktitle> <institution> National Institute of Standards and Technology, </institution> <year> 1997. </year> <note> BIBLIOGRAPHY 212 </note>
Reference: [66] <author> Harmon, D. </author> <title> Ranking algorithms. In Information Retrieval: Data Structures and Algorithms, </title> <editor> W. B. Frakes and R. Baeza-Yates, Eds. </editor> <publisher> Prentice Hall, </publisher> <year> 1992, </year> <pages> pp. 363-392. </pages>
Reference: [67] <author> Hearst, M. </author> <title> Interfaces for searching the web. </title> <journal> Scientific American (March 1997). </journal>
Reference: [68] <author> Hearst, M., and Pedersen, J. </author> <title> Revealing collection structure through information access interfaces. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (1995), </booktitle> <pages> pp. 2047-2048. </pages> <note> Video tape. </note>
Reference: [69] <author> Hearst, M. A. </author> <title> The use of categories and clusters in information access interfaces. In Natural Language Information Retrieval, </title> <editor> T. Strzalkowski, Ed. </editor> <publisher> Kluwer, </publisher> <year> 1999. </year> <note> In press. </note>
Reference-contexts: The documents in these related fine-grain topics use many of the same words. Thus, the documents generally become less distant from one another. Hearst also discusses this point in her work on information access user interfaces that make use of clustering <ref> [69] </ref>, where she explains that the focused results of complex queries may be difficult for clustering systems to meaningfully differentiate. Still, by reading just a few of the document titles in each subcollection, the contents of each subgroup becomes much more clear.
Reference: [70] <author> Hearst, M. A., Karger, D. R., and Pederson, J. O. </author> <title> Scatter/Gather as a tool for the navigation of retrieval results. </title> <booktitle> In Proceedings of AAAI Fall Symposium on Knowledge Navigation (1995). </booktitle>
Reference-contexts: The ability to automatically create meaningful document groupings relies on the validity of the Cluster Hypothesis. Recall that this hypothesis states that "closely associated documents tend to be relevant to the same requests" [166]. As we described in Chapter 4, a good deal of previous work <ref> [167, 127, 70, 71, 2] </ref> has provided significant support for the cluster hypothesis, as have our more recent results reported in Chapter 6. Thus, we have strong reason to believe that clustering may be used as an effective tool to help organize documents.
Reference: [71] <author> Hearst, M. A., and Pederson, J. O. </author> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/Gather on retrieval results. In Proceedings of ACM/SIGIR (1996). </booktitle>
Reference-contexts: The ability to automatically create meaningful document groupings relies on the validity of the Cluster Hypothesis. Recall that this hypothesis states that "closely associated documents tend to be relevant to the same requests" [166]. As we described in Chapter 4, a good deal of previous work <ref> [167, 127, 70, 71, 2] </ref> has provided significant support for the cluster hypothesis, as have our more recent results reported in Chapter 6. Thus, we have strong reason to believe that clustering may be used as an effective tool to help organize documents.
Reference: [72] <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20 (1995), </booktitle> <pages> 197-243. </pages>
Reference: [73] <author> Heckerman, D., and Platt, J. </author> <type> Personal communication, </type> <year> 1998. </year>
Reference: [74] <author> Heckerman, D. E., Horvitz, E. J., and Nathwani, B. N. </author> <title> Toward normative expert systems: Part i. The Pathfinder project. </title> <booktitle> Methods of Information in Medicine 31 (1992), </booktitle> <pages> 90-105. </pages>
Reference-contexts: In other domains, such as medicine, the argument has also been extended that making use of structure in the problem can help to reduce the complexity of a diagnosis task. For example, in the Pathfinder system <ref> [74, 75] </ref>, it was observed that by constructing a single model for several related diseases, it was possible to build more robust and specialized diagnosis models for different groupings of diseases.
Reference: [75] <author> Heckerman, D. E., and Nathwani, B. N. </author> <title> Toward normative expert systems: Part ii. Probability-based representations for efficient knowledge acquisition and inference. </title> <booktitle> Methods of Information in Medicine 31 (1992), </booktitle> <pages> 106-116. BIBLIOGRAPHY 213 </pages>
Reference-contexts: In other domains, such as medicine, the argument has also been extended that making use of structure in the problem can help to reduce the complexity of a diagnosis task. For example, in the Pathfinder system <ref> [74, 75] </ref>, it was observed that by constructing a single model for several related diseases, it was possible to build more robust and specialized diagnosis models for different groupings of diseases.
Reference: [76] <author> Hersh, W. R., Buckley, C., Leone, T. J., and Hickam, D. H. OHSUMED: </author> <title> An interactive retrieval evaluation and new large test collection for research. </title> <booktitle> In Proceedings of the 17th Annual ACM SIGIR Conference (1994), </booktitle> <pages> pp. 192-201. </pages>
Reference: [77] <author> Hull, D. A. </author> <title> The TREC-6 filtering track: Description and analysis. </title> <booktitle> In Proceeding of the Sixth Text REtrieval Conference (TREC-6), </booktitle> <editor> D. K. Harman and E. M. Voorhees, Eds. </editor> <booktitle> National Institute of Standards and Technology, </booktitle> <year> 1997. </year>
Reference: [78] <author> Infoseek. </author> <title> Internet directory and query service. </title> <note> http://www.infoseek.com/, 1995. </note>
Reference: [79] <author> Infoseek. </author> <note> Aptex categorizes more than 700,000 web sites for infoseek. http://info.infoseek.com/doc/PressReleases/hnc.html, 1996. </note>
Reference: [80] <author> Jaakkola, T. S., and Haussler, D. </author> <title> Exploiting generative models in discriminative classifiers. </title> <note> http://www.cse.ucsc.edu/research/ml/papers/Jaakola.ps, 1998. </note>
Reference: [81] <author> Joachims, T. </author> <title> A probabilistic analysis of the rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (1997), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 143-151. </pages>
Reference: [82] <author> Joachims, T. </author> <title> Text categorization with support vector machines: Learning with many relevant features. </title> <type> Tech. Rep. </type> <institution> LS8-Report, Universitaet Dortmund, </institution> <year> 1997. </year>
Reference-contexts: To further corroborate the need for variance control, we point out that Support Vector Machines (which provide an explicit variance control mechanism) have very recently shown excellent results in text domains <ref> [82, 48] </ref>. Furthermore, even our clustering results show that, by making use of strong parameter smoothing techniques (another variance control method), we can often obtain superior results.
Reference: [83] <author> Joachims, T., Freitag, D., and Mitchell, T. WebWatcher: </author> <title> A tour guide for the world wide web. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (1997). </booktitle>
Reference: [84] <author> John, G., Kohavi, R., and Pfleger, K. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference (1994), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. BIBLIOGRAPHY 214 </pages>
Reference: [85] <author> John, G. H., and Langley, P. </author> <title> Estimating continuous distributions in Bayesian classifiers. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI '95) (1995), </booktitle> <pages> pp. 338-345. </pages>
Reference: [86] <author> Kailath, T. </author> <title> The divergence and Bhattacharyya distance measures in signal selection. </title> <journal> IEEE Transactions on Communication Technology COM-15, </journal> <month> 1 (February </month> <year> 1967), </year> <pages> 52-60. </pages>
Reference: [87] <author> Kearns, M., Mansour, Y., and Ng, A. Y. </author> <title> An information-theoretic analysis of hard and soft assignment methods for clustering. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (1997), </booktitle> <pages> pp. 282-293. </pages>
Reference-contexts: Still, SONIA finds these two major themes, rather than simply trying to produce two convoluted clusters that are of more equal size (as some clustering algorithms using mixture modeling are prone to do <ref> [87] </ref>). Furthermore, the interactive nature of SONIA's interface makes it quite easy for the user to simply move the two errant documents into the proper folder. The user can then name these two subgroups of documents "Job related" and "Classes", respectively, for easier identifiability.
Reference: [88] <author> Kira, K., and Rendell, L. A. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92: Proceedings of the Tenth National Conference on Artificial Intelligence (1992), </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 129-134. </pages>
Reference: [89] <author> Kivinen, J., and Warmuth, M. </author> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Tech. Rep. </type> <institution> UCSC-CRL-94-16, Basking Center for Computer Engineering and Information Sciences; University of California, Santa Cruz, </institution> <year> 1994. </year>
Reference: [90] <author> Kohavi, R. </author> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs. </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department, </institution> <year> 1995. </year>
Reference: [91] <author> Kohavi, R., Becker, B., and Sommerfield, D. </author> <title> Improving simple bayes. </title> <booktitle> In ECML-97: Proceedings of the Ninth European Conference on Machine Learning (1997). </booktitle>
Reference: [92] <author> Kohavi, R., and John, G. </author> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (1995), </booktitle> <editor> A. Prieditis and S. Russell, Eds., </editor> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: [93] <author> Kohonen, T. </author> <title> Self-Organizing Maps. </title> <publisher> Springer, </publisher> <year> 1995. </year> <note> BIBLIOGRAPHY 215 </note>
Reference: [94] <author> Koller, D., and Sahami, M. </author> <title> Toward optimal feature selection. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (1996), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 284-292. </pages>
Reference: [95] <author> Koller, D., and Sahami, M. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (1997), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 170-178. </pages>
Reference: [96] <author> Kononenko, I. </author> <title> Semi-naive bayesian classifier. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning (1991), </booktitle> <publisher> Pitman, </publisher> <pages> pp. 206-219. </pages>
Reference: [97] <author> Kozlov, A. V., and Singh, J. P. Sensitivities: </author> <title> An alternative to conditional probabilities for Bayesian belief networks. </title> <booktitle> In Proceedings of the Eleventh Annual Conference on Uncertainty in Artificial Intelligence (UAI '95) (1995), </booktitle> <pages> pp. 376-385. </pages>
Reference: [98] <author> Krishnaiah, P. R., and Kanal, L. N. </author> <title> Classification, Pattern Recognition, and Reduction in Dimensionality. </title> <publisher> Amsterdam: North Holland, </publisher> <year> 1982. </year>
Reference: [99] <author> Kullback, S. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <year> 1959. </year>
Reference: [100] <author> Kullback, S., and Leibler, R. A. </author> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics 22 (1951), </journal> <pages> 76-86. </pages>
Reference: [101] <author> Lang, K. NewsWeeder: </author> <title> Learning to filter news. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (1995), </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: [102] <author> Langley, P. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, </address> <year> 1995. </year>
Reference: [103] <author> Langley, P., Iba, W., and Thompson, K. </author> <title> An analysis of bayesian classifiers. </title> <booktitle> In Proceedings of the tenth national conference on artificial intelligence (1992), </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference: [104] <author> Langley, P., and Sage, S. </author> <title> Induction of selective bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (1994), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 399-406. BIBLIOGRAPHY 216 </pages>
Reference: [105] <author> Lawrence, S., and Giles, C. L. </author> <title> Searching the world wide web. </title> <booktitle> Science 280, 5360 (1998), </booktitle> <pages> 98-100. </pages>
Reference: [106] <author> Lewis, D. D. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> Univ. of Massachusetts, Amherst, Department of Computer Science, </institution> <year> 1992. </year>
Reference: [107] <author> Lewis, D. D. </author> <title> The TREC-5 filtering track. </title> <booktitle> In Proceeding of the Fifth Text REtrieval Conference (TREC-5), </booktitle> <editor> D. K. Harman and E. M. Voorhees, Eds. </editor> <booktitle> National Institute of Standards and Technology, </booktitle> <year> 1996, </year> <pages> pp. 75-96. </pages>
Reference: [108] <author> Lewis, D. D. </author> <title> Naive (Bayes) at forty: The independence assumption in information retrieval. </title> <booktitle> In ECML-98: Proceedings of the Tenth European Conference on Machine Learning (1998). </booktitle>
Reference: [109] <author> Lewis, D. D., and Gale, W. A. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of the 17th Annual International ACM/SIGIR Conference (Dublin, </booktitle> <address> Ireland, </address> <year> 1994), </year> <pages> pp. 3-11. </pages>
Reference: [110] <author> Lewis, D. D., and Ringuette, M. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval (1994), </booktitle> <pages> pp. 81-93. </pages>
Reference: [111] <author> Lewis, D. D., Schapire, R. E., Callan, J. P., and Papka, R. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In Proceedings of ACM/SIGIR (1996). </booktitle>
Reference: [112] <author> Lin, X., Soergel, D., and Marchionini, G. </author> <title> A self-organizing semantic map for information retrieval. </title> <booktitle> In Proceedings of ACM/SIGIR (1991), </booktitle> <pages> pp. 262-269. </pages>
Reference: [113] <author> Maron, M. E. </author> <title> Mechanized documentation: The logic behind a probabilistic interpretation. Statistical Association Methods for Mechanized Documentation (1965), </title> <type> 9-13. </type>
Reference: [114] <author> Masand, B., Linoff, G., and Waltz, D. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of ACM/SIGIR (1992), </booktitle> <pages> pp. 59-65. BIBLIOGRAPHY 217 </pages>
Reference: [115] <author> McCallum, A., and Nigam, K. </author> <title> A comparison of event models for naive bayes text classification. In Learning for Text Categorization: </title> <note> Papers from the 1998 AAAI Workshop (Technical Report WS-98-05) (1998), M. Sahami, Ed. </note>
Reference: [116] <author> McCallum, A., Rosenfeld, R., Mitchell, T., and Ng, A. </author> <title> Improving text classification by shrinkage in a hierarchy of classes. </title> <booktitle> In Machine Learning: Proceedings of the Fifteenth International Conference (1998). </booktitle>
Reference-contexts: CHAPTER 9. HIERARCHICAL CLASSIFICATION 166 Several research issues specific to hierarchical classification still remain. In particular, we have already mentioned the problem of recovering from classification errors early in the hierarchy. Moreover, recent work by McCallum et al <ref> [116] </ref> on improved methods for estimating probabilities for classification in text hierarchies are directly applicable in our work and may even further improve our results.
Reference: [117] <author> Mitchell, T. M. </author> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference: [118] <author> Mladenic, D. </author> <title> Turning yahoo into an automatic web-page classifier. </title> <booktitle> In ECAI-98: Proceedings of the Thirteenth European Conference on Aritficial Intelligence (1998), </booktitle> <pages> pp. 473-474. </pages>
Reference-contexts: These results are still inconclusive (as Web data has a tendency to be extremely varied, as well as incorporating other media aside from text), but we are encouraged by some of our initial results and are seeking ways to improve them. More recently, Mladenic <ref> [118] </ref> has directly followed up on this work, making use of our hierarchical classification scheme also for classifying Web pages into the Yahoo! directory. Her positive results on this task lend further support to the applicability and scalability of this method on large text datasets.
Reference: [119] <author> Murphy, P. M., and Aha, D. W. </author> <title> UCI repository of machine learning databases. </title> <note> http://www.ics.uci.edu/~mlearn/MLRepository.html, 1995. </note>
Reference: [120] <author> Negroponte, N. </author> <title> Being Digital. </title> <publisher> Vintage Books, </publisher> <year> 1995. </year>
Reference: [121] <author> Nilsson, N. J. </author> <title> The Mathematical Foundations of Learning Machines. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year> <note> Previously published as: Learning Machines, </note> <year> 1965. </year>
Reference: [122] <author> Pazzani, M., and Billsus, D. </author> <title> Learning and revising user profiles: The identification of interesting web sites. </title> <booktitle> Machine Learning 27 (1997), </booktitle> <pages> 313-331. </pages>
Reference: [123] <author> Pazzani, M., Muramatsu, J., and Billsus, D. Syskill & Webert: </author> <title> Identifying interesting web sites. </title> <booktitle> In AAAI-96: Proceedings of the Thirteenth National Conference on Artificial Intelligence (1996). </booktitle>
Reference: [124] <author> Pazzani, M. J. </author> <title> Searching for dependencies in bayesian classifiers. </title> <booktitle> In Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics (Ft. </booktitle> <address> Lauderdale, FL, </address> <month> January </month> <year> 1995), </year> <editor> D. Fisher and H. Lenz, </editor> <publisher> Eds. </publisher>
Reference: [125] <author> Pearl, J. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1988. </year> <note> BIBLIOGRAPHY 218 </note>
Reference: [126] <author> Peart, N. </author> <title> Prime mover. Lyrics from the album Hold Your Fire, </title> <year> 1987. </year>
Reference-contexts: CONCLUSIONS AND FUTURE WORK 204 machine learning in information access is potentially limitless. If we consider research as the on-going search for knowledge and understanding, then perhaps we could best conclude by simply observing that "the point of the journey is not to arrive" <ref> [126] </ref>.
Reference: [127] <author> Pirolli, P., Schank, P., Hearst, M., and Diehl, C. </author> <title> Scatter/gather browsing communicates the topic structure of a very large text collection. </title> <booktitle> In Proceedings of CHI (1996). </booktitle>
Reference-contexts: The ability to automatically create meaningful document groupings relies on the validity of the Cluster Hypothesis. Recall that this hypothesis states that "closely associated documents tend to be relevant to the same requests" [166]. As we described in Chapter 4, a good deal of previous work <ref> [167, 127, 70, 71, 2] </ref> has provided significant support for the cluster hypothesis, as have our more recent results reported in Chapter 6. Thus, we have strong reason to believe that clustering may be used as an effective tool to help organize documents.
Reference: [128] <author> Porter, M. F. </author> <title> An algorithm for suffix stripping. Program 14, </title> <booktitle> 3 (1980), </booktitle> <pages> 130-137. </pages>
Reference-contexts: The retrieved document texts are then parsed into a series of alphanumeric terms (i.e., words). Optionally, these terms may be stemmed to their root as SONIA's parser includes a standard word stemming scheme <ref> [128] </ref>. We note that we currently do not make use of such stemming in the examples of system usage provided in later CHAPTER 10. SONIA A COMPLETE SYSTEM 175 sections. Empirically, we have not found stemming to create much of a difference in the results obtained with the system.
Reference: [129] <author> Quinlan, J. R. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 (1986), </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference: [130] <author> Quinlan, J. R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California, </address> <year> 1993. </year>
Reference: [131] <author> Rasmussen, E. </author> <title> Clustering algorithms. In Information Retrieval: Data Structures and Algorithms, </title> <editor> W. B. Frakes and R. Baeza-Yates, Eds. </editor> <publisher> Prentice Hall, </publisher> <year> 1992, </year> <pages> pp. 419-442. </pages>
Reference: [132] <author> Reuters. </author> <title> Reuters-22173 document collection. Distribution for research purposes has been granted by Reuters and Carnegie Group. Arrangements for access were made by David Lewis. </title> <note> An updated version of this data set, Reuters-21578, is now publically available from David Lewis at http://www.research.att.com/~lewis., 1995. </note>
Reference: [133] <author> Riloff, E., and Lehnart, W. </author> <title> Information extraction as a basis for high-precision text classification. </title> <journal> Transactions of Office Information Systems 12, </journal> <note> 3 (1994). Special Issue on Text Categorization. </note>
Reference: [134] <author> Robertson, S. E. </author> <title> The probability ranking principle in IR. </title> <journal> Journal of Documentation 33, </journal> <volume> 4 (1977), </volume> <pages> 292-304. </pages>
Reference: [135] <author> Robertson, S. E., and Sparck-Jones, K. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society of Information Science 27 (1976), </journal> <pages> 129-146. BIBLIOGRAPHY 219 </pages>
Reference: [136] <author> Robertson, S. E., and Walker, S. </author> <title> Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. </title> <booktitle> In Proceedings of the Seventeenth International Conference on Research and Development in Information Retrieval (1994), </booktitle> <pages> pp. 232-241. </pages>
Reference: [137] <author> Rocchio, J. J. </author> <title> Relevance feedback in information retrieval. In The SMART Information Retrieval System, </title> <editor> G. Salton, Ed. </editor> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1971, </year> <pages> pp. 313-323. </pages>
Reference: [138] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> Learning internal representations by error propagation. In Parallel Distributed Processing, </title> <editor> J. McClelland and D. Rumelhart, Eds. </editor> <publisher> MIT Press, </publisher> <year> 1986. </year> <note> Chapter 8. </note>
Reference: [139] <author> Sahami, M. </author> <title> Learning limited dependence Bayesian classifiers. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (1996), </booktitle> <pages> pp. 335-338. </pages>
Reference: [140] <author> Sahami, M., Dumais, S., Heckerman, D., and Horvitz, E. </author> <title> A bayesian approach to filtering junk E-mail. In Learning for Text Categorization: </title> <note> Papers from the 1998 AAAI Workshop (Technical Report WS-98-05) (1998), M. Sa-hami, Ed. </note>
Reference-contexts: Indeed, initial work along these lines in CHAPTER 11. CONCLUSIONS AND FUTURE WORK 203 the context of e-mail classification has already shown that the use of such non-textual features can have a significant impact on classification accuracy <ref> [140] </ref>. As the current push in the Digital Libraries community toward having more document metadata succeeds in making such non-textual features available for more on-line documents in the future, we believe that methods that can effectively harness this information will have significant advantages over methods that cannot.
Reference: [141] <author> Sahami, M., Hearst, M., and Saund, E. </author> <title> Applying the multiple cause mixture model to text categorization. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (1996), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 435-443. </pages>
Reference-contexts: We note that we have explored methods for tuning these thresholds to the characteristics of more varied collections in previous work <ref> [141] </ref>. However, in the current implementation of SONIA we have not observed much empirical difference CHAPTER 10. SONIA A COMPLETE SYSTEM 176 through the use of these more sophisticated techniques, and thus do not employ them here to save processing time. <p> However, they could have been equally reasonably grouped into the documents containing general Saturn information. While their current grouping is very reasonable, this example does show that allowing documents to belong to multiple clusters would be a desirable property. In previous work <ref> [141] </ref> we have taken some initial steps in this direction, but a good solution to this problem still remains an open question.
Reference: [142] <author> Sahami, M., Yusufali, S., and Baldonado, M. Q. W. </author> <title> Real-time full-text clustering of networked documents. </title> <booktitle> In AAAI-97: Proceedings of the Fourteenth National Conference on Artificial Intelligence (1997), </booktitle> <address> p. </address> <month> 845. </month>
Reference-contexts: SenseMaker allows users to simultaneously query multiple heterogeneous information sources and then organize the retrieved documents by matching titles, matching URLs (for Web documents), and the like, or it can utilize SONIA to cluster documents by their full-text content <ref> [142, 143] </ref>. However, since the the focus of the SenseMaker interface was not on the formation of topic hierarchies, we have more recently developed our own custom interface for SONIA.
Reference: [143] <author> Sahami, M., Yusufali, S., and Baldonado, M. Q. W. SONIA: </author> <title> A service for organizing networked information autonomously. </title> <booktitle> In Digital Libraries 98: Proceedings of the Third ACM Conference on Digital Libraries (1998). </booktitle> <address> BIBLIOGRAPHY 220 </address>
Reference-contexts: SenseMaker allows users to simultaneously query multiple heterogeneous information sources and then organize the retrieved documents by matching titles, matching URLs (for Web documents), and the like, or it can utilize SONIA to cluster documents by their full-text content <ref> [142, 143] </ref>. However, since the the focus of the SenseMaker interface was not on the formation of topic hierarchies, we have more recently developed our own custom interface for SONIA.
Reference: [144] <author> Salton, G. </author> <title> The SMART Information Retrieval System. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1971. </year>
Reference: [145] <author> Salton, G., and Buckley, C. </author> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management 24, 5 (1988), </booktitle> <pages> 513-523. </pages>
Reference: [146] <author> Salton, G., and McGill, M. J. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1983. </year>
Reference: [147] <author> Salton, G., Wong, A., and Yang, C. S. </author> <title> A vector space model for automatic indexing. </title> <booktitle> Communications of the ACM 18 (1975), </booktitle> <pages> 613-620. </pages>
Reference: [148] <author> Schuetze, H., Hull, D., and Pedersen, J. </author> <title> A comparison of document representations and classifiers for the routing problem. </title> <booktitle> In Proceedings of the 18th Annual ACM SIGIR Conference (1995), </booktitle> <pages> pp. 229-237. </pages>
Reference: [149] <author> Schuetze, H., and Silverstein, C. </author> <title> A comparison of projections for efficient document clustering. </title> <booktitle> In Proceedings of ACM/SIGIR (1997). </booktitle>
Reference: [150] <author> Self, G. </author> <type> Personal communication, </type> <year> 1996. </year>
Reference: [151] <author> Seltzer, R., Ray, D. S., and Ray, E. J. </author> <title> The AltaVista Search Revolution. </title> <publisher> McGraw-Hill, </publisher> <year> 1998. </year>
Reference: [152] <author> Singh, M., and Provan, G. M. </author> <title> A comparison of induction algorithms for selective and non-selective bayesian classifiers. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (July 1995). </booktitle>
Reference: [153] <author> Singh, M., and Provan, G. M. </author> <title> Efficient learning of selective bayesian network classifiers. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (1996), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 453-461. </pages>
Reference: [154] <author> Singh, M., and Valtorta, M. </author> <title> An algorithm for the construction of bayesian network structures from data. </title> <booktitle> In Uncertainty in Artificial Intelligence (1993), </booktitle> <publisher> Elsevier Science, </publisher> <pages> pp. 259-265. BIBLIOGRAPHY 221 </pages>
Reference: [155] <editor> Sparck-Jones, K., and Willett, P., Eds. </editor> <booktitle> Readings in Information Retrieval. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference: [156] <author> Spertus, E. Smokey: </author> <title> Automatic recognition of hostile messages. </title> <booktitle> In Proceedings of Innovative Applications of Artificial Intelligence (IAAI) (1997), </booktitle> <pages> pp. 1058-1065. </pages>
Reference: [157] <author> Stanfill, C., and Waltz, D. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM 29, </journal> <volume> 12 (1986), </volume> <pages> 1213-1228. </pages>
Reference: [158] <institution> Stanford Digital Libraries Group. The Stanford digital libraries project. </institution> <note> Communications of the ACM (April 1995). </note>
Reference-contexts: In this context, SONIA can be very effective at helping automatically generate and maintain portions of a user's hierarchical file directory structure. Finally, since our system exists as part of a general architecture within the Stanford Digital Libraries Testbed <ref> [158] </ref>, it has the ability to simultaneously retrieve information from a number of heterogeneous sources, thereby making our system maximally flexible. SONIA was also designed with efficiency in mind, thereby facilitating real-time user interactivity even when accessing diverse, distributed document collections.
Reference: [159] <author> Strang, G. </author> <title> Linear Algebra and its Applications. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <year> 1988. </year>
Reference: [160] <author> Sullivan, D. </author> <title> The major search engines. </title> <note> From the Search Engine Watch Report available at http://www.searchenginewatch.com/facts/major.html, 1998. </note>
Reference: [161] <author> Taylor, C., Michie, D., and Spiegalhalter, D. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Paramount Publishing International, </publisher> <year> 1994. </year>
Reference: [162] <author> Timo, H., Samuel, K., Krista, L., and Teuvo, K. </author> <title> WEBSOM self-organizing maps of document collections. </title> <booktitle> In Proceedings of WSOM'97 Workshop on Self-Organizing Maps (1997), </booktitle> <pages> pp. 310-315. </pages>
Reference: [163] <author> Turtle, H., and Croft, W. B. </author> <title> Inference networks for document retrieval. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Research and Development in Information Retrieval (1990), </booktitle> <pages> pp. 1-24. </pages>
Reference: [164] <author> Turtle, H. R. </author> <title> Inference Networks for Document Retrieval. </title> <type> PhD thesis, </type> <institution> Univ. of Massachusetts, Amherst, Department of Computer Science, </institution> <year> 1991. </year> <editor> [165] van Rijsbergen, C. J. </editor> <title> A theoretical basis for the use of co-occurrence data in information retrieval. </title> <journal> Journal of Documentation 33 (1977), </journal> <pages> 106-119. </pages> <editor> [166] van Rijsbergen, C. J. </editor> <booktitle> Information Retrieval. </booktitle> <publisher> Butterworths, </publisher> <year> 1979. </year> <title> BIBLIOGRAPHY 222 [167] van Rijsbergen, </title> <editor> C. J., and Jardine, N. </editor> <title> The use of hierarchic clustering in information retrieval. </title> <booktitle> Information Storage and Retrieval 7 (1971), </booktitle> <pages> 217-240. </pages>
Reference: [168] <author> Vapnik, V. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference: [169] <author> Voorhees, E. M. </author> <title> The Effectiveness and Efficiency of Agglomerative Hierarchical Clustering in Document Retrieval. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1986. </year>
Reference: [170] <author> Widrow, B., and Winter, R. G. </author> <title> Neural nets for adaptive filtering and adaptive pattern recognition. </title> <booktitle> IEEE Computer (March 1988), </booktitle> <pages> 25-39. </pages>
Reference: [171] <author> Wiener, E., Pedersen, J. O., and Weigand, A. S. </author> <title> A neural network approach to topic spotting. </title> <booktitle> In Symposium on Document Analysis and Information Retrieval (1995), </booktitle> <pages> pp. 317-332. </pages>
Reference: [172] <author> Willett, P. </author> <title> Recent trends in hierarchical document clustering: A critical review. </title> <booktitle> Information Processing and Management 24, 5 (1988), </booktitle> <pages> 577-597. </pages>
Reference: [173] <institution> Yahoo! On-line guide for the internet. </institution> <note> http://www.yahoo.com/, 1995. </note>
Reference: [174] <author> Yang, Y., and Chute, C. G. </author> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> Transactions of Office Information Systems 12, </journal> <note> 3 (1994). Special Issue on Text Categorization. </note>
Reference: [175] <author> Yang, Y., and Pedersen, J. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (1997), </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 412-420. </pages>
Reference: [176] <author> Yang, Y., Pierce, T., and Carbonell, J. G. </author> <title> A study on retrospective and on-line event detection. </title> <booktitle> In Proceedings of the 21st Annual ACM SIGIR Conference (1998). </booktitle>
Reference-contexts: Foremost among these is the detection of new topics that do not neatly fit into an existing classification scheme. This area has the potential to benefit from combining previous work in topic detection <ref> [1, 176] </ref>, with new ideas in clustering. Indeed, this issue will become more important as the domain of digital libraries moves from the classic topic structure of existing libraries and into the realm of organizing the quickly evolving body of information on the World Wide Web.
Reference: [177] <author> Zipf, G. K. </author> <title> Human Behavior and the Principle of Least Effort. </title> <publisher> Addison-Wesley, </publisher> <year> 1949. </year>
References-found: 174

