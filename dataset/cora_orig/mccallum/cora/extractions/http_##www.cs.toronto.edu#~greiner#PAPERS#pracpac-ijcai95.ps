URL: http://www.cs.toronto.edu/~greiner/PAPERS/pracpac-ijcai95.ps
Refering-URL: http://www.cs.toronto.edu/~greiner/PAPERS/
Root-URL: 
Email: dale@cs.toronto.edu  greiner@scr.siemens.com  
Title: Practical PAC Learning  
Author: Dale Schuurmans Russell Greiner 
Date: August 1995.  
Note: To appear in the Proceedings of the Fourteenth International Conference on Artificial Intelligence (IJCAI-95), Montreal,  
Address: Toronto, Ontario M5S 1A4, Canada  Princeton, NJ 08540, USA  
Affiliation: Department of Computer Science University of Toronto  Siemens Corporate Research  
Abstract: We present new strategies for "probably approximately correct" (pac) learning that use fewer training examples than previous approaches. The idea is to observe training examples one-at-a-time and decide "on-line" when to return a hypothesis, rather than collect a large fixed-size training sample. This yields sequential learning procedures that pac-learn by observing a small random number of examples. We provide theoretical bounds on the expected training sample size of our procedure | but establish its efficiency primarily by a series of experiments which show sequential learning actually uses many times fewer training examples in practice. These results demonstrate that pac-learning can be far more efficiently achieved in practice than previously thought.
Abstract-found: 1
Intro-found: 1
Reference: [ Aha, et al., 1991 ] <author> D. Aha, et al. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference: [ Bartlett and Williamson, 1991 ] <author> P. Bartlett and R. William-son. </author> <title> Investigating the distributional assumptions of the pac learning model. </title> <booktitle> In COLT-91, </booktitle> <pages> pages 24-32, </pages> <year> 1991. </year>
Reference: [ Baum and Haussler, 1989 ] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: Moreover, these results compare poorly to the empirical "rule of thumb" that, for a concept class defined by w free parameters, roughly T thumb = w * training examples are needed to achieve an error of * <ref> [ Baum and Haussler, 1989 ] </ref> .
Reference: [ Benedek and Itai, 1988 ] <author> G. Benedek and A. Itai. </author> <title> Learnabil-ity by fixed distributions. </title> <booktitle> In COLT-88, </booktitle> <pages> pages 80-90, </pages> <year> 1988. </year>
Reference-contexts: Notice that a sequential approach is still possible in this case; and, in fact, a variant of Procedure S can pac-learn concept spaces (C; P X ) using 5 times fewer training examples than the best known fixed-sample-size procedure developed in <ref> [ Benedek and Itai, 1988 ] </ref> . Range of applicability: Beyond improving data-efficiency, sequential learning is also applicable to a much wider range of pac-learning problems than fixed-sample-size learning.
Reference: [ Blumer, et al., 1989 ] <author> A. Blumer, et al. </author> <title> Learnability and the Vapnik-Chervonen. dimension. </title> <journal> JACM, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: I.e., cutting the training sample size in half would be a significant improvement in most applications, even if this came with a slight increase in overall computation time. <ref> [ Blumer, et al., 1989 ] </ref> , which we will assume throughout. The apparent inefficiency of pac-learning has lead to much speculation about the sources of difficulty.
Reference: [ Breiman, et al., 1984 ] <author> L. Breiman, et al. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Range of applicability: Beyond improving data-efficiency, sequential learning is also applicable to a much wider range of pac-learning problems than fixed-sample-size learning. For example, Procedure S can be directly applied to "nearest neighbor" and "decision-tree" hy-pothesizers (like CART <ref> [ Breiman, et al., 1984 ] </ref> ) which implicitly consider concept classes of infinite VCdimen-sion.
Reference: [ Clancey, 1985 ] <author> W. Clancey. </author> <title> Heuristic classification. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 289-350, </pages> <year> 1985. </year>
Reference-contexts: Motivation: Classification learning is by far the most studied in machine learning research. The immense interest in this problem arises from the fact that classification itself is an important subtask in many appli cations | in fact, comprising the central function of most expert systems <ref> [ Clancey, 1985 ] </ref> . The importance of learning in this context is that we often lack the requisite knowledge needed to specify an appropriate classifier, and yet have access to many correctly classified examples.
Reference: [ Dennis and Schnabel, 1983 ] <author> J. Dennis and R. Schnabel. </author> <title> Numerical Methods for Unconstrained and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: Interestingly, S also outperforms the simplistic procedure R on this problem. Figure 6 shows that, R performs nearly as well as S on easy problems (low dimension, accuracy, reliability), but S's advantage grows significantly as these parameters are scaled up. 5 Specifically, we used the BFGS secant optimization procedure <ref> [ Dennis and Schnabel, 1983 ] </ref> with a "relaxation" objective function [ Duda and Hart, 1973 ] . Explanations: These results demonstrate a clear ad-vantage for sequential over fixed-sample-size learning: we solve the exact same pac-learning problem using far fewer training examples in this case.
Reference: [ Duda and Hart, 1973 ] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Figure 6 shows that, R performs nearly as well as S on easy problems (low dimension, accuracy, reliability), but S's advantage grows significantly as these parameters are scaled up. 5 Specifically, we used the BFGS secant optimization procedure [ Dennis and Schnabel, 1983 ] with a "relaxation" objective function <ref> [ Duda and Hart, 1973 ] </ref> . Explanations: These results demonstrate a clear ad-vantage for sequential over fixed-sample-size learning: we solve the exact same pac-learning problem using far fewer training examples in this case.
Reference: [ Ehrenfeucht, et al., 1989 ] <author> A. Ehrenfeucht, et al. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261, </pages> <year> 1989. </year>
Reference: [ Haussler, 1990 ] <author> D. Haussler. </author> <title> Probably approximately correct learning. </title> <booktitle> In AAAI-90, </booktitle> <pages> pages 1101-1108, </pages> <year> 1990. </year>
Reference-contexts: The apparent inefficiency of pac-learning has lead to much speculation about the sources of difficulty. The predominant "folk wisdom" is that the large sample sizes follow from the worst case nature of the pac-guarantees <ref> [ Haussler, 1990 ] </ref> | that is, the worst case bounds are inherently unreasonable because they must take into account "pathological" domain distributions and target concepts which force large training sample sizes (moreover, the argument continues, these pathological situations do not arise in "typical" applications).
Reference: [ Haussler, 1992 ] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model. </title> <journal> Inf. and Comp., </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: Why? Beyond criticisms of certain modelling assumptions (e.g., noise-free examples, bivalent classifications, etc.| which actually have been addressed the pac-framework, cf. <ref> [ Haussler, 1992 ] </ref> ), the most prevalent criticism of pac-learning theory is that the actual numbers of training examples it demands are far too large to be practical. Example: Consider the hX = IR 10 ; C = halfspaces; * = 0:01; ffi = 0:05i problem mentioned earlier.
Reference: [ le Cun, et al., 1989 ] <author> Y. le Cun, et al. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: In such situations, we can attempt to exploit the wealth of available data to overcome inadequate prior knowledge | and hence, use learning as an effective classifier synthesis tool. In fact, there are numerous examples where learning systems have produced classifiers that outperform the best available "hand-coded" systems, e.g., <ref> [ le Cun, et al., 1989; Weiss and Kulikowski, 1991 ] </ref> .
Reference: [ Linial, et al., 1991 ] <author> N. Linial, et al. </author> <title> Results on learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Information and Computation, </journal> <volume> 90 </volume> <pages> 33-49, </pages> <year> 1991. </year>
Reference-contexts: Procedure S is a correct pac-learner in the exact same sense as F: The key property of S is that its call to sprt 3 Variants of Procedure R have been proposed by many authors in the past <ref> [ Linial, et al., 1991; Oblow, 1992 ] </ref> , primarily to achieve "nonuniform" pac-learning. However, the goals of nonuniform pac-learning fundamentally differ from what we are trying to accomplish here (see Footnote 6). <p> Acknowledgments Thanks to Steven Shapiro for his help with the implementations. 6 It is important not to confuse the idea of sequential with nonuniform pac-learning <ref> [ Linial, et al., 1991; Oblow, 1992 ] </ref> . Although nonuniform pac-learning procedures also use "on-line" stopping rules very similar to R, they do not share the same theoretical advantages shown for S.
Reference: [ Littlestone, 1989 ] <author> N. Littlestone. </author> <title> From online to batch learning. </title> <booktitle> In COLT-89, </booktitle> <pages> pages 269-284, </pages> <year> 1989. </year>
Reference-contexts: For example, a variant of Procedure S can serve as a sequential "mistake bounded to pac" conversion procedure that is provably more efficient than Little-stone's fixed-sample-size procedure <ref> [ Littlestone, 1989 ] </ref> (and which uses 30 times fewer training examples in empirical tests). We also obtain stronger improvements for the case of distribution specific pac-learning (where we assume the learner knows P X , but not the target concept c 2 C).
Reference: [ Oblow, 1992 ] <author> E. Oblow. </author> <title> Implementing Valiant's learnability theory using random sets. </title> <journal> Mach. Learn., </journal> <volume> 8 </volume> <pages> 45-73, </pages> <year> 1992. </year>
Reference-contexts: Procedure S is a correct pac-learner in the exact same sense as F: The key property of S is that its call to sprt 3 Variants of Procedure R have been proposed by many authors in the past <ref> [ Linial, et al., 1991; Oblow, 1992 ] </ref> , primarily to achieve "nonuniform" pac-learning. However, the goals of nonuniform pac-learning fundamentally differ from what we are trying to accomplish here (see Footnote 6). <p> Thus, the actual data-efficiency of a sequential learner depends on the specific case at hand, not on what we can prove about the worst case situation. Consequently, the sequential approach automatically takes advantage of beneficial situations like "easy" target concepts and domain distributions <ref> [ Oblow, 1992 ] </ref> , or a "good" hypoth-esizer that makes lucky guesses | without the system designer having to explicitly notice that these beneficial situations exist a priori ! More importantly, the true worst case data-efficiency of sequential learning depends on the true worst case convergence properties of the concept <p> Acknowledgments Thanks to Steven Shapiro for his help with the implementations. 6 It is important not to confuse the idea of sequential with nonuniform pac-learning <ref> [ Linial, et al., 1991; Oblow, 1992 ] </ref> . Although nonuniform pac-learning procedures also use "on-line" stopping rules very similar to R, they do not share the same theoretical advantages shown for S.
Reference: [ Schaffer, 1994 ] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings ML-94, </booktitle> <year> 1994. </year>
Reference-contexts: However, it has often been observed that there really is no such thing as a "general purpose" extrapolation strategy <ref> [ Schaffer, 1994 ] </ref> | a particular strategy performs well on a specific application only by fortuitous predisposition: it just happens to "guess right" on unseen domain objects, whether by prior knowledge or luck. To guarantee success, one must supply prior constraints.
Reference: [ Schuurmans and Greiner, 1995 ] <author> D. Schuurmans and R. Greiner. </author> <title> Sequential PAC learning. </title> <booktitle> In COLT-95, </booktitle> <year> 1995. </year>
Reference-contexts: Proofs of all results mentioned in this paper (and more) are outlined in <ref> [ Schuurmans and Greiner, 1995 ] </ref> . Complete details appear in [ Schuurmans, 1995 ] . Procedure sprt ((x), a, r, ffi acc , ffi rej ) For boolean random variable (x), test H acc : P X f (x)= 1g a vs. <p> Consequently, R is often slower than S (even though it uses less space) simply because R tends to call H more often. 4 Additional results Special cases: We have obtained even stronger results in slightly restricted settings <ref> [ Schuurmans and Greiner, 1995 ] </ref> . For example, a variant of Procedure S can serve as a sequential "mistake bounded to pac" conversion procedure that is provably more efficient than Little-stone's fixed-sample-size procedure [ Littlestone, 1989 ] (and which uses 30 times fewer training examples in empirical tests).
Reference: [ Schuurmans, 1995 ] <author> D. Schuurmans. </author> <title> Effective Classification Learning. </title> <type> PhD thesis, </type> <institution> U. Toronto, Computer Sci., </institution> <year> 1995. </year>
Reference-contexts: Although this is a crude bound, it is interesting to note that it scales the same as T BEHW and T ST AB . Moreover, this bound actually beats T BEHW and T STAB for small values of ffi <ref> [ Schuurmans, 1995 ] </ref> . However, as shown below, S actually performs much better in practice than any bounds we can prove about its performance. Since this is not a possibility for fixed-sample-sized approaches, we expect S to perform much better than T BEHW and T STAB in practical applications. <p> Proofs of all results mentioned in this paper (and more) are outlined in [ Schuurmans and Greiner, 1995 ] . Complete details appear in <ref> [ Schuurmans, 1995 ] </ref> . Procedure sprt ((x), a, r, ffi acc , ffi rej ) For boolean random variable (x), test H acc : P X f (x)= 1g a vs. <p> These results are in fact representative over the entire range of parameter settings: S's empirical advantage actually improves for increased problem dimension n (Figure 5), and is maintained at higher accuracy and reliability levels <ref> [ Schuurmans, 1995 ] </ref> . Overall, S appears to be pac-learning with near-practical data-efficiency in this example. Interestingly, S also outperforms the simplistic procedure R on this problem. <p> In particular, we considered three different transformations of the uniform [1; 1] n distribution: spherical (nonlinear compression towards origin), pyramidal (compression from opposite corners towards hyperplane), and accretive (translation towards discrete points in f1; 1g n ). Surprisingly, none of these transformations had any noticeable effect on S's performance <ref> [ Schuurmans, 1995 ] </ref> ; as demonstrated in Figure 7 for the pyramidal case. A second reason for S's advantage might be that the specific target concept (diagonal halfspace) is a particularly "easy" one for S | i.e., H could somehow be biased to guess similar hypotheses. <p> = 0:05i with C 1 = halfspaces, C 2 = disj--chains, and vc (C i ) = 2; 3; 4; 6; 11; 16; 21. (The class of disj--chains is defined by 1 d* copies of a d-dimensional "product chain" of concepts, where the concepts in different copies are mutually exclusive <ref> [ Schuurmans, 1995 ] </ref> .
Reference: [ Shawe-Taylor, et al., 1993 ] <author> J. Shawe-Taylor, et al. </author> <title> Bounding sample size with the Vapnik-Chervonenkis dimension. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 42 </volume> <pages> 65-73, </pages> <year> 1993. </year>
Reference: [ Valiant, 1984 ] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> CACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference: [ Vapnik and Chervonenkis, 1971 ] <author> V. Vapnik and A. Chervo-nenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: In particular, Blumer et al. [1989] have shown that for any 2 con 1 The VCdimension measures how "fine grained" C is by the maximum number of domain objects C can independently label <ref> [ Vapnik and Chervonenkis, 1971 ] </ref> . This is an abstract combinatorial measure which applies to arbitrary domains and concept classes.
Reference: [ Wald, 1947 ] <author> A. Wald. </author> <title> Sequential Analysis. </title> <publisher> Wiley, </publisher> <year> 1947. </year>
Reference-contexts: S is based on two ideas: First, instead of throwing away H's hypotheses after a single mistake, S saves hypotheses and continues testing them until one proves to have small error. Second, S identifies accurate hypotheses by using a sequential probability ratio test (sprt) <ref> [ Wald, 1947 ] </ref> to test each candidate "on-line" (in parallel); Figure 4. Thus, S never rejects a potentially acceptable hypothesis, and quickly identifies any sufficiently accurate candidate.
Reference: [ Weiss and Kulikowski, 1991 ] <author> S. Weiss and C. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: In such situations, we can attempt to exploit the wealth of available data to overcome inadequate prior knowledge | and hence, use learning as an effective classifier synthesis tool. In fact, there are numerous examples where learning systems have produced classifiers that outperform the best available "hand-coded" systems, e.g., <ref> [ le Cun, et al., 1989; Weiss and Kulikowski, 1991 ] </ref> .
References-found: 24

