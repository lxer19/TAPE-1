URL: ftp://psyche.mit.edu/pub/tommi/jaak-bayes.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Email: ftommi,jordang@psyche.mit.edu  
Title: A variational approach to Bayesian logistic regression models and their extensions  
Author: Tommi S. Jaakkola and Michael I. Jordan 
Date: November 2, 1996  
Address: Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Abstract: We consider a logistic regression model with a Gaussian prior distribution over the parameters. We show that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model. The results are readily extended to (binary) belief networks. For belief networks we also derive closed form posteriors in the presence of missing values. Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates.
Abstract-found: 1
Intro-found: 1
Reference: <author> J. Bernardo and A. </author> <title> Smith (1994). Bayesian theory. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: 1 Introduction The Bayesian formalism is well suited for representing uncertainties in the values of variables, model parameters, or in the model structure. The formalism further allows ready incorporation of prior knowledge and the combination of such knowledge with statistical data <ref> (Bernardo & Smith 1994, Heckerman et al. 1995) </ref>. The rigorous semantics, however, often comes with a sizable computational cost of evaluating multi-dimensional integrals. This cost precludes the use of exact Bayesian methods even in relatively simple settings, such as generalized linear models (McCullagh & Nelder 1983).
Reference: <author> D. Heckerman, D. Geiger, and D. </author> <title> Chickering (1995). Learning Bayesian networks: the combination of knowledge and statistical data. </title> <journal> Machine Learning 20 No. </journal> <volume> 3: </volume> <pages> 197. </pages>
Reference: <author> T. Jaakkola and M. </author> <title> Jordan (1996). Computing upper and lower bounds on likelihoods in intractable networks. </title> <booktitle> Proceedings of the twelfth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference: <author> T. Jaakkola and M. </author> <title> Jordan (1996). Recursive algorithms for approximating probabilities in graphical models. </title> <note> To appear in Advances in Neural Information Processing Systems 9. </note>
Reference: <author> P. McCullagh & J. A. </author> <title> Nelder (1983). Generalized linear models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: The rigorous semantics, however, often comes with a sizable computational cost of evaluating multi-dimensional integrals. This cost precludes the use of exact Bayesian methods even in relatively simple settings, such as generalized linear models <ref> (McCullagh & Nelder 1983) </ref>. We concern ourselves in this paper with a particular generalized linear model|logistic regression| and show how variational approximation techniques can restore the computational feasibility of the Bayesian formalism.
Reference: <author> R. </author> <title> Neal (1992). Connectionist learning of belief networks. </title> <booktitle> Artificial Intelligence 56: </booktitle> <pages> 71-113. </pages>
Reference-contexts: relative errors in the posterior standard deviations as a function of g (). = 1 for the prior distribution. a) b) takes the usual product form P (s 1 ; : : : ; s n ) = i We note that this is an extension of sigmoid belief networks <ref> (Neal 1992) </ref> due to the prior distributions over the parameters.
Reference: <author> R. </author> <title> Neal (1994). Bayesian Learning for Neural Networks. </title> <type> PhD Thesis. </type> <institution> University of Toronto. </institution>
Reference-contexts: The optimization of such parameters in turn often yields fixed point equations that can be solved iteratively. For the use of variational techniques in the context of graphical models see Saul et al. (1996) and Jaakkola and Jordan (1996). Variational methods should be contrasted with sampling techniques <ref> (Neal 1994) </ref> that have become standard in the context of Bayesian calculations. While surely powerful in evaluating complicated integrals, sampling techniques do not guarantee monotonically improving approximations nor do they yield explicit bounds. It is precisely these issues that are the focus of the current paper.
Reference: <author> J. </author> <title> Pearl (1988). Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo. </address>
Reference: <author> J. </author> <title> Sakurai (1985). Modern Quantum Mechanics. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: We concern ourselves in this paper with a particular generalized linear model|logistic regression| and show how variational approximation techniques can restore the computational feasibility of the Bayesian formalism. Variational techniques lead to deterministic approximations (or, in some cases, to exact results) and are used extensively in the physics literature <ref> (e.g., Sakurai 1985) </ref>. These techniques transform the problem into an equivalent minimization (or maximization) problem by means of introducing extra variables known as variational parameters. The optimization of such parameters in turn often yields fixed point equations that can be solved iteratively.
Reference: <author> L. Saul, T. Jaakkola, and M. </author> <title> Jordan (1996). Mean field theory for sigmoid belief networks. </title> <type> JAIR 4: </type> <pages> 61-76. </pages>
Reference: <author> D. Spiegelhalter and S. </author> <title> Lauritzen (1990). Sequential updating of conditional probabilities on directed graphical structures. </title> <booktitle> Networks 20: </booktitle> <pages> 579-605. </pages>
References-found: 11

