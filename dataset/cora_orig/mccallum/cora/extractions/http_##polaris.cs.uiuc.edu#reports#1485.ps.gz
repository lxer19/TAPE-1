URL: http://polaris.cs.uiuc.edu/reports/1485.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: fpaek,paduag@csrd.uiuc.edu  
Title: Automatic Parallelization for Non-cache Coherent Multiprocessors  
Author: Yunheung Paek David A. Padua 
Address: 1304 West Springfield Avenue, Urbana, IL 61801, USA  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign,  
Abstract: Although much work has been done on parallelizing compilers for cache coherent shared memory multiprocessors and message-passing multiprocessors, there is relatively little research on parallelizing compilers for non-cache coherent multiprocessors with global address space. In this paper, we present a preliminary study on automatic parallelization for the Cray T3D, a commercial scalable machine with a global memory space and non coherent caches.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Faigin, J. Hoeflinger, D. Padua, P. Petersen, S. Weatherford. </author> <title> The Polaris Internal Representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 22, No. 5, </volume> <month> Oct. </month> <year> 1994, </year> <pages> pp. 553-586 </pages>
Reference-contexts: One important characteristic of Polaris is its powerful internal representation <ref> [1] </ref>. It includes an extensive collection of program manipulation operations to facilitate the implementation of compiler transformations. After a program is converted into Polaris' internal form, it is analyzed and transformed by a sequence of compiler passes which add annotations to identify the parallelism detected by the compiler.
Reference: [2] <author> B. Blume, et al., </author> <title> Polaris: Improving the Effectiveness of Parallelizing Compilers, </title> <booktitle> Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> OR. </address> <note> Lecture Note in Computer Science, </note> <month> Aug. </month> <year> 1994, </year> <pages> pp. 141-154 </pages>
Reference-contexts: 1 Introduction Of the three main classes of today's parallel computers, namely, message-passing multiprocessors, cache coherent multiprocessors, and noncoherent cache multiprocessors with a global address space, parallelizing compilers <ref> [2, 9, 11, 12] </ref> have been extensively studied for only the first two. In this paper, we present a preliminary study on the automatic parallelization of Fortran programs for the third class machine. <p> In this paper, we present a preliminary study on the automatic parallelization of Fortran programs for the third class machine. Our translation algorithms were implemented in the Polaris restructurer <ref> [2] </ref>, which was developed by the authors and others at Illinois. Important advances in automatic parallelization for cache coherent multiprocessors have been demonstrated recently with the Polaris restructurer. <p> We also discuss some of the factors that limit the performance of the target programs. In Section 6 we discuss a few advanced techniques we plan to implement in the near future to deal with these factors. 2 Polaris The main objective of the Polaris project <ref> [2, 7] </ref> is to develop and implement effective parallelization techniques for scientific programs. One important characteristic of Polaris is its powerful internal representation [1]. It includes an extensive collection of program manipulation operations to facilitate the implementation of compiler transformations. <p> During the sequential execution of an original program, we measured the percentage of overall running time of the program for each loop, and added the percentages of each parallelizable loop to obtain the total sum. We call this total sum parallel coverage <ref> [2] </ref>. Two dotted lines in Figure 1 plot the ideal speedups for programs with parallel coverage of 99% and 90% respectively.
Reference: [3] <author> B. Pottenger, R. Eigenmann, </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler, </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995 </year>
Reference-contexts: After a program is converted into Polaris' internal form, it is analyzed and transformed by a sequence of compiler passes which add annotations to identify the parallelism detected by the compiler. Passes currently implemented in Polaris include: symbolic dependence analysis, inlining, induction variable substitution, reduction recognition, and pri-vatization <ref> [3, 5, 8] </ref>. To support data dependence analysis, Polaris applies range propagation techniques based on symbolic program analysis [10]. After the input program is restructured into the internally-represented parallel program, a final pass or backend applies machine-specific transformations and outputs the target parallel program.
Reference: [4] <author> Z. Bokus, et al, </author> <title> Compiling Fortran 90D/HPF for Distributed Memory MIMD Computers, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 21, </volume> <year> 1994, </year> <pages> pp. 15-26 </pages>
Reference-contexts: In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. Shared data can be distributed across memory using directives similar to those made popular by High Performance Fortran, Vienna Fortran and other similar languages <ref> [4, 6, 13, 20] </ref>. CRAFT uses :block for block distribution and :block (N ) for block-cyclic distribution. The do shared directive of CRAFT is used to mark parallel loops.
Reference: [5] <author> J. Grout, </author> <title> Inline Expansion for the Polaris Research Compiler, </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1995 </year>
Reference-contexts: After a program is converted into Polaris' internal form, it is analyzed and transformed by a sequence of compiler passes which add annotations to identify the parallelism detected by the compiler. Passes currently implemented in Polaris include: symbolic dependence analysis, inlining, induction variable substitution, reduction recognition, and pri-vatization <ref> [3, 5, 8] </ref>. To support data dependence analysis, Polaris applies range propagation techniques based on symbolic program analysis [10]. After the input program is restructured into the internally-represented parallel program, a final pass or backend applies machine-specific transformations and outputs the target parallel program.
Reference: [6] <author> B. Chapman, P. Mehrota, H. Moritsch, H. Zima, </author> <title> Dynamic Data Distributions in Vienna Fortran, </title> <booktitle> Supercomputing '93 Proceedings, </booktitle> <year> 1993, </year> <pages> pp. 284-293 </pages>
Reference-contexts: In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. Shared data can be distributed across memory using directives similar to those made popular by High Performance Fortran, Vienna Fortran and other similar languages <ref> [4, 6, 13, 20] </ref>. CRAFT uses :block for block distribution and :block (N ) for block-cyclic distribution. The do shared directive of CRAFT is used to mark parallel loops.
Reference: [7] <author> B. Blume, et al., </author> <title> Advanced Program Restructuring for High-Performance Computers with Polaris, </title> <type> Tech. Report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing R & D, </institution> <year> 1996, </year> <note> CSRD Report No. 1473 </note>
Reference-contexts: We also discuss some of the factors that limit the performance of the target programs. In Section 6 we discuss a few advanced techniques we plan to implement in the near future to deal with these factors. 2 Polaris The main objective of the Polaris project <ref> [2, 7] </ref> is to develop and implement effective parallelization techniques for scientific programs. One important characteristic of Polaris is its powerful internal representation [1]. It includes an extensive collection of program manipulation operations to facilitate the implementation of compiler transformations. <p> In <ref> [7] </ref>, we discuss several on-going efforts in the Polaris project to increase the parallel coverage and, in Section 4, we briefly deal with the parallel loop structure of a program. In this section, we discuss the last two factors.
Reference: [8] <author> P. Tu, D. Padua, </author> <title> Automatic array privatization, </title> <booktitle> Proc. 6th Workshop on Language and Compilers for Parallel Computing, </booktitle> <address> OR. </address> <note> Lecture Note in Computer Science, </note> <month> Aug. </month> <year> 1993, </year> <pages> pp. 500-521 </pages>
Reference-contexts: After a program is converted into Polaris' internal form, it is analyzed and transformed by a sequence of compiler passes which add annotations to identify the parallelism detected by the compiler. Passes currently implemented in Polaris include: symbolic dependence analysis, inlining, induction variable substitution, reduction recognition, and pri-vatization <ref> [3, 5, 8] </ref>. To support data dependence analysis, Polaris applies range propagation techniques based on symbolic program analysis [10]. After the input program is restructured into the internally-represented parallel program, a final pass or backend applies machine-specific transformations and outputs the target parallel program.
Reference: [9] <author> C. Polychronopoulos, et al., </author> <title> The Structure of Parafrase-2: An Advanced Paral-lelizing Compiler for C and Fortran, Languages and Compilers for Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1990 </year>
Reference-contexts: 1 Introduction Of the three main classes of today's parallel computers, namely, message-passing multiprocessors, cache coherent multiprocessors, and noncoherent cache multiprocessors with a global address space, parallelizing compilers <ref> [2, 9, 11, 12] </ref> have been extensively studied for only the first two. In this paper, we present a preliminary study on the automatic parallelization of Fortran programs for the third class machine.
Reference: [10] <author> W. Blume, R. Eigenmann, </author> <title> The Range Test: A Dependence Test for Symbolic Non-linear Expression, </title> <booktitle> SuperComputing '94 Proceedings, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. 643-656 </pages>
Reference-contexts: Passes currently implemented in Polaris include: symbolic dependence analysis, inlining, induction variable substitution, reduction recognition, and pri-vatization [3, 5, 8]. To support data dependence analysis, Polaris applies range propagation techniques based on symbolic program analysis <ref> [10] </ref>. After the input program is restructured into the internally-represented parallel program, a final pass or backend applies machine-specific transformations and outputs the target parallel program.
Reference: [11] <author> P.Banerjee, et al., </author> <title> The PARADIGM Compiler for Distributed-Memory Multicom-puters, </title> <journal> IEEE Computer, </journal> <volume> Vol. 28, No. 10, </volume> <month> Oct. </month> <year> 1995, </year> <pages> pp 37-47 </pages>
Reference-contexts: 1 Introduction Of the three main classes of today's parallel computers, namely, message-passing multiprocessors, cache coherent multiprocessors, and noncoherent cache multiprocessors with a global address space, parallelizing compilers <ref> [2, 9, 11, 12] </ref> have been extensively studied for only the first two. In this paper, we present a preliminary study on the automatic parallelization of Fortran programs for the third class machine.
Reference: [12] <author> S. Amarasinghe, et al., </author> <title> An Overview of the SUIF Compiler for Scalable Parallel Machines, </title> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> Feb. </month> <year> 1995, </year> <pages> pp. 662-667 </pages>
Reference-contexts: 1 Introduction Of the three main classes of today's parallel computers, namely, message-passing multiprocessors, cache coherent multiprocessors, and noncoherent cache multiprocessors with a global address space, parallelizing compilers <ref> [2, 9, 11, 12] </ref> have been extensively studied for only the first two. In this paper, we present a preliminary study on the automatic parallelization of Fortran programs for the third class machine.
Reference: [13] <author> C. Tseng, </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines, </title> <type> PhD Thesis, </type> <institution> Rice University, </institution> <month> Jan. </month> <year> 1993 </year>
Reference-contexts: In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. Shared data can be distributed across memory using directives similar to those made popular by High Performance Fortran, Vienna Fortran and other similar languages <ref> [4, 6, 13, 20] </ref>. CRAFT uses :block for block distribution and :block (N ) for block-cyclic distribution. The do shared directive of CRAFT is used to mark parallel loops.
Reference: [14] <author> W. Oed, </author> <title> The Cray Reseach Massively Parallel Processor System CRAY T3D, </title> <institution> Cray Research, </institution> <month> Nov </month> <year> 1993 </year>
Reference-contexts: The experimental results presented in this paper give us hope that these programming difficulties can be overcome with effective compiler techniques. In this section, we very briefly describe the Cray T3D and CRAFT. More detailed descriptions of the machine can be found in <ref> [14, 15, 21] </ref>. The Cray T3D was designed mainly for large-scale parallel scientific applications. It consists of up to 1024 processing nodes, each containing 2 processing elements (PE) and a local memory. The PEs are 150 MHz DEC Alpha 21064 microprocessors. <p> In the example, the computation happens to be distributed according to the owner computes rule, as is done in HPF; however, other distributions could also have been used. Table 1 summarizes the differences between CRAFT and HPF, as discussed in <ref> [14] </ref>.
Reference: [15] <institution> CRAY T3D System Architecture Overview, Cray Research, </institution> <year> 1993 </year>
Reference-contexts: The experimental results presented in this paper give us hope that these programming difficulties can be overcome with effective compiler techniques. In this section, we very briefly describe the Cray T3D and CRAFT. More detailed descriptions of the machine can be found in <ref> [14, 15, 21] </ref>. The Cray T3D was designed mainly for large-scale parallel scientific applications. It consists of up to 1024 processing nodes, each containing 2 processing elements (PE) and a local memory. The PEs are 150 MHz DEC Alpha 21064 microprocessors.
Reference: [16] <author> L. Rauchwerger, D. Padua, </author> <title> The PRIVATIZING DOALL Test: A Run-Time Tech--nique for DOALL Loop Identification and Array Privatization, </title> <booktitle> Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994, </year> <pages> pp. 33-43 </pages>
Reference-contexts: Work on parallelism detection continues on several fronts, including the study of efficient interprocedural analysis techniques, the parallelization of loops containing complex recurrences, and run-time dependence analysis <ref> [16] </ref>. 3 The Cray T3D and CRAFT While the lack of cache coherence in the Cray T3D helps make the machine affordable and scalable, it also introduces some difficulties in the development of efficient programs [18].
Reference: [17] <author> R. Marcelin, </author> <title> Message Passing on the CRAY T3D, Massively Parallel Computing Group, </title> <address> NERSC, </address> <year> 1995 </year>
Reference-contexts: Furthermore, the SHMEM library can be used in programs containing shared data, whereas conventional message passing libraries, such as PVM and MPI, assume all data to be private. SHMEM is more efficient than current implementations of other message-passing models. In fact, in experiments conducted on a 16-processor partition <ref> [17] </ref>, the latency of a SHMEM PUT operation was measured at 2 sec and the peak throughput was measured at 116.8 MB/s, while the equivalent figures for PVM send/receive operations are 63 sec and 26 MB/s respectively. 4 The Cray T3D backend In this section, we describe the transformations applied by
Reference: [18] <author> D. Bernstein, et al., </author> <title> Solutions and Debugging for Data Consistency in Multiprocessors with Noncoherent Caches, </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 23, No. 1, </volume> <year> 1995, </year> <pages> pp. 83-103 </pages>
Reference-contexts: efficient interprocedural analysis techniques, the parallelization of loops containing complex recurrences, and run-time dependence analysis [16]. 3 The Cray T3D and CRAFT While the lack of cache coherence in the Cray T3D helps make the machine affordable and scalable, it also introduces some difficulties in the development of efficient programs <ref> [18] </ref>. The experimental results presented in this paper give us hope that these programming difficulties can be overcome with effective compiler techniques. In this section, we very briefly describe the Cray T3D and CRAFT. More detailed descriptions of the machine can be found in [14, 15, 21].
Reference: [19] <author> M. Snir, </author> <title> Proposal for MPI-2, MPI meetings, </title> <year> 1995 </year>
Reference-contexts: This can be done using CRAFT, an extension of Fortran for the T3D which has several features in common with other languages for distributed shared-memory machines <ref> [19, 25, 28] </ref>. CRAFT follows the Single-Program Multiple-Data (SPMD) model and contains a shared address space. In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. <p> To this end we will use the SHMEM [21, 22] communication library which contains various single-sided communication primitives, such as PUT and GET, and explicit cache control routines. For example, SHMEM enables message aggregation which is not possible in ordinary CRAFT programs. The PUT/GET communication <ref> [19] </ref> allows asynchronous non-blocking access to any memory location. This means that with the SHMEM primitives, the whole memory either private or shared can be shared by all processors. Therefore, all data can be cached without losing the ability to share it. <p> Another reason is that PUT/GET is rapidly gaining widespread acceptance. In fact, several portable shared-memory programming models supporting PUT/GET are already implemented on ordinary message-passing machines, such as the IBM SP-1/2, Intel Paragon and TMC CM-5 <ref> [19, 25, 28] </ref>. Furthermore, several existing and newly proposed large-scale machines, such as the T3E, directly support these primitives in hardware [21, 27], which reduces the effect of the increased communication overhead resulting from the extra data copy operations. Communication aggregation is useful in reducing the overhead.
Reference: [20] <institution> High Performance Fortran Language Specification, High Performance Fortran Forum, </institution> <month> May </month> <year> 1993 </year>
Reference-contexts: In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. Shared data can be distributed across memory using directives similar to those made popular by High Performance Fortran, Vienna Fortran and other similar languages <ref> [4, 6, 13, 20] </ref>. CRAFT uses :block for block distribution and :block (N ) for block-cyclic distribution. The do shared directive of CRAFT is used to mark parallel loops. <p> Routines from the SHMEM library could be used to fetch and store remote sections of these logically shared arrays. 4.4 Compatibility problems between Fortran and CRAFT Many MPP Fortran extensions, such as CRAFT and HPF <ref> [20] </ref>, help the user attain high performance through distribution of data and other directives, while maintaining some degree of compatibility with conventional Fortran 77. However, total compatibility has not been achieved because these languages impose several restrictions in the name of performance.
Reference: [21] <institution> CRAY MPP Fortran Reference Manual, Cray Research, </institution> <year> 1993 </year>
Reference-contexts: The experimental results presented in this paper give us hope that these programming difficulties can be overcome with effective compiler techniques. In this section, we very briefly describe the Cray T3D and CRAFT. More detailed descriptions of the machine can be found in <ref> [14, 15, 21] </ref>. The Cray T3D was designed mainly for large-scale parallel scientific applications. It consists of up to 1024 processing nodes, each containing 2 processing elements (PE) and a local memory. The PEs are 150 MHz DEC Alpha 21064 microprocessors. <p> This significantly increases average memory latency and network contention. To avoid these inefficiencies it is necessary to explicitly control caching and data transfer. We plan to do so in future versions of the translator. To this end we will use the SHMEM <ref> [21, 22] </ref> communication library which contains various single-sided communication primitives, such as PUT and GET, and explicit cache control routines. For example, SHMEM enables message aggregation which is not possible in ordinary CRAFT programs. The PUT/GET communication [19] allows asynchronous non-blocking access to any memory location. <p> In fact, several portable shared-memory programming models supporting PUT/GET are already implemented on ordinary message-passing machines, such as the IBM SP-1/2, Intel Paragon and TMC CM-5 [19, 25, 28]. Furthermore, several existing and newly proposed large-scale machines, such as the T3E, directly support these primitives in hardware <ref> [21, 27] </ref>, which reduces the effect of the increased communication overhead resulting from the extra data copy operations. Communication aggregation is useful in reducing the overhead. In most Fortran programs, the natural program section that organizes copying from/to global memory is the loop.
Reference: [22] <institution> SHMEM Technical Note for Fortran, Cray Research, </institution> <month> Oct. </month> <year> 1994 </year>
Reference-contexts: This significantly increases average memory latency and network contention. To avoid these inefficiencies it is necessary to explicitly control caching and data transfer. We plan to do so in future versions of the translator. To this end we will use the SHMEM <ref> [21, 22] </ref> communication library which contains various single-sided communication primitives, such as PUT and GET, and explicit cache control routines. For example, SHMEM enables message aggregation which is not possible in ordinary CRAFT programs. The PUT/GET communication [19] allows asynchronous non-blocking access to any memory location.
Reference: [23] <institution> Programming Language FORTRAN, American National Standards Institute, ANSI X3.9-1978 ISO 1539-1980 </institution>
Reference-contexts: However, total compatibility has not been achieved because these languages impose several restrictions in the name of performance. The following list shows the major restrictions imposed by CRAFT: * Fortran's sequence and storage association rules <ref> [23] </ref> do not apply to shared data. * Shared data may not be in EQUIVALENCE or blank COMMON. * Shared data may not be of type CHARACTER. * The dimension size of shared arrays must be a power of two 1 . * Shared formal parameters may not be associated with
Reference: [24] <author> J. Gustafson, </author> <title> Reevaluating Amdahls Law, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 31, No. 5, </volume> <month> May </month> <year> 1988, </year> <pages> pp. 532-533 </pages>
Reference-contexts: This is primarily due to the small data set size for TRFD <ref> [24, 26] </ref>. 6 Program Optimizations and the Data Copying Strategy From the discussion above, it is clear that the main factors influencing parallel performance are: * the parallel coverage; * the parallel loop structure of a program; * the amount of shared data used in local computations; and, * the data
Reference: [25] <editor> D. Culler, et al., </editor> <booktitle> Parallel Programming in Split-C,Supercomputing '93 Proceedings, </booktitle> <year> 1993 </year>
Reference-contexts: This can be done using CRAFT, an extension of Fortran for the T3D which has several features in common with other languages for distributed shared-memory machines <ref> [19, 25, 28] </ref>. CRAFT follows the Single-Program Multiple-Data (SPMD) model and contains a shared address space. In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. <p> Another reason is that PUT/GET is rapidly gaining widespread acceptance. In fact, several portable shared-memory programming models supporting PUT/GET are already implemented on ordinary message-passing machines, such as the IBM SP-1/2, Intel Paragon and TMC CM-5 <ref> [19, 25, 28] </ref>. Furthermore, several existing and newly proposed large-scale machines, such as the T3E, directly support these primitives in hardware [21, 27], which reduces the effect of the increased communication overhead resulting from the extra data copy operations. Communication aggregation is useful in reducing the overhead.
Reference: [26] <author> A. Grama, A. Gupta, V. Kumar, Isoefficiency: </author> <title> Measuring the Scalability of Parallel Algorithms and Architectures, </title> <booktitle> IEEE Parallel & Distributed Technology, </booktitle> <month> Aug. </month> <year> 1993, </year> <pages> pp. 12-21 </pages>
Reference-contexts: This is primarily due to the small data set size for TRFD <ref> [24, 26] </ref>. 6 Program Optimizations and the Data Copying Strategy From the discussion above, it is clear that the main factors influencing parallel performance are: * the parallel coverage; * the parallel loop structure of a program; * the amount of shared data used in local computations; and, * the data
Reference: [27] <author> K. Hayashi, et al., </author> <title> AP1000+: Architectural Support of PUT/GET Interface for Parallelizing Compiler. </title> <booktitle> Proc. 6th International Conference on Architechtural Support for Programming Language and Operating Systems, </booktitle> <month> Oct. </month> <year> 1994, </year> <pages> pp. 196-207 </pages>
Reference-contexts: In fact, several portable shared-memory programming models supporting PUT/GET are already implemented on ordinary message-passing machines, such as the IBM SP-1/2, Intel Paragon and TMC CM-5 [19, 25, 28]. Furthermore, several existing and newly proposed large-scale machines, such as the T3E, directly support these primitives in hardware <ref> [21, 27] </ref>, which reduces the effect of the increased communication overhead resulting from the extra data copy operations. Communication aggregation is useful in reducing the overhead. In most Fortran programs, the natural program section that organizes copying from/to global memory is the loop.
Reference: [28] <author> J. Nielocha, R. Harrison, R. Littlefield, </author> <title> Global Arrays: A Portable Shared-Memory Programming Model for Distributed Memory Computers, </title> <booktitle> Supercomputing '94 Proceedings, </booktitle> <year> 1994, </year> <month> pp.340-349 </month>
Reference-contexts: This can be done using CRAFT, an extension of Fortran for the T3D which has several features in common with other languages for distributed shared-memory machines <ref> [19, 25, 28] </ref>. CRAFT follows the Single-Program Multiple-Data (SPMD) model and contains a shared address space. In our experiments, each CRAFT process was allocated to a separate physical processor. Data objects can be declared as shared or private. <p> Another reason is that PUT/GET is rapidly gaining widespread acceptance. In fact, several portable shared-memory programming models supporting PUT/GET are already implemented on ordinary message-passing machines, such as the IBM SP-1/2, Intel Paragon and TMC CM-5 <ref> [19, 25, 28] </ref>. Furthermore, several existing and newly proposed large-scale machines, such as the T3E, directly support these primitives in hardware [21, 27], which reduces the effect of the increased communication overhead resulting from the extra data copy operations. Communication aggregation is useful in reducing the overhead.
Reference: [29] <author> J. R. Larus, </author> <title> Compiling for Shared-Memory and Message-Passing computer, </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <year> 1996 </year>
Reference-contexts: Hence, as long as the local memory space is available, a processor can prefetch data anytime before it is needed and poststore it sometime after the computation. In <ref> [29] </ref>, some other advantages of the compiler-directed communication over cache-coherence protocol driven communication on the non-cache coherent architecture are discussed. The shared data copying scheme is especially useful when the data distribution requirements of a program are dynamic. The conventional technique for such a case is data redistribution [30].
Reference: [30] <author> K. Kenney, </author> <title> Compiler Technology for Machine-Independent Parallel Programming, </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 22, </volume> <year> 1994, </year> <pages> pp. 79-98 </pages>
Reference-contexts: In [29], some other advantages of the compiler-directed communication over cache-coherence protocol driven communication on the non-cache coherent architecture are discussed. The shared data copying scheme is especially useful when the data distribution requirements of a program are dynamic. The conventional technique for such a case is data redistribution <ref> [30] </ref>. Data redistibution usually moves much of the array across the distributed memories. Unless most elements of the array are fully used for a long time, this total data movement is inefficient.
Reference: [31] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, D. Padua, </author> <title> The Cedar Fortran Project, </title> <type> Tech. Report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing R & D, </institution> <month> Apr. </month> <year> 1992, </year> <note> CSRD Report No. 1262 </note>
Reference: [32] <author> H. Zima, B. Chapman, </author> <title> Supercompilers for Parallel and Vector Computers, </title> <publisher> ACM Press, </publisher> <year> 1992 </year>
Reference-contexts: We have endeavored to develop translation techniques to overcome these limitations. Three of these techniques are discussed below. 4.4.1 Renaming Aliasing has always been an important issue in program analysis in general, and in automatic parallelization in particular <ref> [32] </ref>. Aliasing is also one of the most difficult problems in automatic parallelization for the T3D.
Reference: [33] <author> R. Arpaci, et al., </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective, </title> <booktitle> Proceedings of ISCA, </booktitle> <year> 1995, </year> <month> pp.320-331 </month>
Reference-contexts: The PEs are 150 MHz DEC Alpha 21064 microprocessors. The interconnection network is a 3-D torus network with high throughput and low latency. Remote memory latency on the T3D ranges from 90 to 130 cycles <ref> [33] </ref>. Local memory latency is 22 cycles. The T3D also contains a special tree-like network for global barrier synchronization. Various communication primitives, including single-sided communication primitives such as GET and PUT, are supported in hardware. The local memory is logically partitioned into private and shared address spaces. <p> Barriers do not have a significant impact on overall program execution time in the T3D. One reason is the efficient hardware implementation of barriers in the machine. Table 2 shows the performance of the T3D barrier <ref> [33] </ref>. PEs Barrier Time (sec) 4 1.73 256 1.90 Table 2: Barrier Performance. Times are the average of 5000 barrier executions. Furthermore, in the experiments reported in Section 5, barriers are executed infrequently. The execution time increases by less than 1% due to barriers in all cases.
References-found: 33

