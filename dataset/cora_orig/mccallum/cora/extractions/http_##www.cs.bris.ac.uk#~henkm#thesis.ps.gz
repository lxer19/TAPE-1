URL: http://www.cs.bris.ac.uk/~henkm/thesis.ps.gz
Refering-URL: http://www.cs.bris.ac.uk/~henkm/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Simulating Computer Architectures Nice picture is missing!  
Author: HenkMuller 
Abstract-found: 0
Intro-found: 0
Reference: [Agarwal86] <author> Anant Agarwal, Richard L. Sites and Mark Horowitz, ATUM: </author> <title> A new technique for capturing address traces using microcode, </title> <booktitle> Proceedings 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pp 119-127, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Because of these drawbacks, software solutions have been searched for, running on (or in) the processor. The most elegant approach is probably the ATUM <ref> [Agarwal86] </ref>. ATUM (Address Tracing Using Microcode), intervenes in the lowest software level: the microcode. The microcode of the VAX has been modified so that all address references are captured.
Reference: [Agarwal89] <author> Anant Agarwal, Mark Horowitz, and John Hennessy, </author> <title> An analytical cache model, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol 7, No 2, </volume> <pages> pp 184-215, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In <ref> [Agarwal89] </ref> a performance model is presented that accurately predicts the miss rate of a cache in a single processor architecture (the step from miss rates to performance is straightforward, see for example [Hennessy90]). <p> The model needs a description of the cache (the associativity, size and so on), and a specification of the application behaviour (in terms of the number of uniquely referenced blocks, and the access rate) to calculate the miss rate. It would be nice if the models of [Vernon89] and <ref> [Agarwal89] </ref> could be combined so that a performance figure is predicted based on the parameters of the architecture, the cache, and the application. This combination is not feasible because of 124 Chapter 7. A Futurebus performance model n The number of processors. k The number of levels in the hierarchy. <p> The performance model of a flat system 125 the complexity of the models, and because the model of <ref> [Agarwal89] </ref> cannot be applied directly in a multiprocessor environment. The model presented in this chapter takes both the miss rate and the bus contention into account, but it uses a (drastically) simplified approach. <p> part, the total size in use by each of the b 1 processors is therefore: S = sS 0 f + (1 s)S 0 f = S 0 f = S 0 s + b 1 (1 s) The relation between the cache size and miss rate is given in <ref> [Przybylski90, Agarwal89] </ref>.
Reference: [America89a] <editor> Pierre America and Jan Rutten A parallel object-oriented language: </editor> <title> design and semantic foundations, </title> <type> Ph.D. thesis, </type> <institution> Free University, </institution> <address> Amsterdam, </address> <month> May 17, </month> <year> 1989. </year>
Reference-contexts: This chapter concludes with a description of the current status of the Oyster implementation and a comparison with some existing tools. 3.1 Pearl : The simulation language Pearl is the name of the simulation language developed for Oyster. The language design has been influenced greatly by three languages: POOL <ref> [America89a] </ref>, C 3.1. Pearl : The simulation language 39 Compiler C-compiler Loader Processor class definitions (extra C-functions) topology, array bounds program input ? ? ? ? -C executable instances [Kernighan78] and SIMULA [Birtwistle73]. <p> For Pearl, a full inheritance mechanism is considered as an overkill, and it was expected to trouble the development of the Pearl compiler. Instead of inheritance, Pearl supports a simpler concept known as subtyping <ref> [America89a] </ref>. A subtyping relation between two classes exist if the first class supports at least every feature supported by the second class. The first class may support more features, but this is not necessary. Subtypes can be used to define an abstract interface.
Reference: [America89b] <author> P.H.M. America, P0350: </author> <title> Language definition of POOL-X, PRISMA document number 0350, </title> <institution> Philips Research Laboratories Eindhoven, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: The interconnection topology is not fixed (any topology is allowed), it is stored in the routing tables of the communication processor during the bootstrap. The PRISMA machine is programmed using POOL <ref> [America89b] </ref>, a Parallel Object Oriented Language. A running POOL program consists of objects that communicate and synchronise by sending messages to each other. In general, neither the destination of a message, nor the size are known at compile time. Several POOL implementations have been made [Spek90, Beemster90].
Reference: [Annot87] <author> J.K. Annot and R. van Twist, </author> <title> A novel deadlock free and starvation free packet switching communication processor, </title> <booktitle> Proceedings of PARLE '87, Veldhoven, The netherlands, </booktitle> <pages> pp 68-85, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Three recent hardware designs in this field are the Torus Routing Chip (or TRC, [Dally86]), the Adaptive Routing Chip (the ARC, [Mooij89]), and the DOOM Communication processor (or CP, <ref> [Annot87] </ref>). These three communication processors have all been analysed, simulated and built. The analysis was made to prove the correctness of the network (deadlock freedom) or to analyse the networks performance behaviour (queuing models). The simulation was used to observe the network performance under non analysable conditions. <p> There is no DMA (Direct Memory Access) in a node, the data processor transports the data to and from the communication processor. The CP implements a deadlock free and starvation free, packet switching network <ref> [Annot87] </ref>. A packet consists of 240 data bits, preceded by a 16-bits header containing the 12-bits identification of the destination node and four administration bits. The CP's are interconnected by serial links through which the packets are routed to the destination node using a store and forward protocol. <p> The communication processor is not in the standard library. It is modelled with thirteen Pearl objects, as drawn in the dashed box in Figure 5.6. The objects forming the CP are literally as described in <ref> [Annot87] </ref>: the central store manages the internal buffer space, four input machines h I receive data from neighbouring communication processors, four output machines h O send data to these neigh-bouring nodes, two special input and output machines (marked SI and SO) handle the packets going to and coming from this node,
Reference: [Apers90] <author> P. Apers, L.O. Hertzberger, B.J.A. Hulshof, A.C.M. Oerlemans and M. Kersten, PRISMA: </author> <title> A Platform for experiments with parallelism, </title> <editor> P. America (ed), </editor> <booktitle> Proceedings of the PRISMA workshop on parallel databasesystems, Noordwijk, The Nether-lands, </booktitle> <address> September 24-26, </address> <publisher> Springer Verlag LNCS 503, </publisher> <pages> pp 169-180, </pages> <year> 1991. </year>
Reference-contexts: To get a better impression where the loss of performance in the PRISMA machine came from, the experiment described in this chapter was performed. The experiment involves the evaluation of the communication part of the PRISMA architecture (PRISMA stands for PaRallel Inference and Storage MAchine <ref> [Apers90] </ref> and is also called POOMA -Parallel Object Oriented MAchine- or DOOM -Decentralised Object Oriented Machine- [Bronnenberg87]), which is based on the DOOM-CP. Three variants of the PRISMA architecture are simulated in order to evaluate the communication performance. <p> Three variants of the PRISMA architecture are simulated in order to evaluate the communication performance. Section 5.1 presents a brief description of the PRISMA architecture, and shows where the bottleneck in the current implementation is (see <ref> [Vlot90, Apers90] </ref> and [Bronnenberg87] for a concise description of the architecture). The architecture has been modelled using Oyster, as is described in Section 5.2.
Reference: [Archibald86] <author> James Archibald and Jean-loup Baer, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol 4, No. 4, </volume> <pages> pp 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: This is for example done by <ref> [Archibald86] </ref> for the generation of an address trace of data references. In this section, a stochastical trace generator is described that is close to the model a real processor (like [Archibald86]), and that generates an address trace that is supposed to stem from a multi processor shared memory machine. <p> This is for example done by <ref> [Archibald86] </ref> for the generation of an address trace of data references. In this section, a stochastical trace generator is described that is close to the model a real processor (like [Archibald86]), and that generates an address trace that is supposed to stem from a multi processor shared memory machine. Besides ordinary locality from within the trace of one processor, the stochastical trace generator also takes care of the locality between the traces of various processors. <p> The data sections could be enhanced likewise, with LRU chains of accesses as <ref> [Archibald86] </ref>, or with access patterns for arrays and matrices, but this is considered as less important, because less references are made to the data segment, and because array or matrix access structures are not typical for all programs. We are more concerned about the locality of the shared data.
Reference: [Baer88] <author> J. Baer and W. Wang, </author> <title> On the inclusion properties for multi-level cache hierarchies, </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <year> 1988, </year> <pages> pp 73-80. </pages>
Reference-contexts: The 6.1. Introduction to the Futurebus cache consistency 101 contrast with many other studies (for example [Przybylski90] and [Hennessy90]), the studies towards line size and associativity are performed in the context of hierarchical multiprocessor architectures, as is for example also the case in <ref> [Baer88] </ref>. Because of invalidates and sharing, the optimal values for these parameters for single and multi processor systems are not necessarily identical. After determination of reasonable settings for these parameters, a study on the influence of the bus topology on the performance is presented in Section 6.5. <p> The question addressed here is what associativity should be used for lower level caches in a hierarchical architecture. A common line of reasoning is that low-level caches should be at least large enough to hold all lines of the caches upward in the hierarchy (called the inclusion property, <ref> [Baer88] </ref>). Otherwise high level caches will have to compete with neighbouring caches for space in the low-level cache, which causes the low-level caches to frequently invalidate upward copies to service requests that hit a full set. <p> The performance of high-level caches will decrease because they have to invalidate useful lines, which results in a low hit-rate. In <ref> [Baer88] </ref> it is proved that to enforce the inclusion property, the associativity of a low-level cache memory should be at least the sum of the associativity of its upward connected caches. <p> Then one set in the low-level cache can hold all lines in the processor caches which fall into that set. Since <ref> [Baer88] </ref> does not quantify the performance effects of obeying the inclusion property, an experiment with various associativities around the inclusion value has been performed.
Reference: [Barbacci81] <author> Mario R. Barbacci, </author> <title> Instruction set processor specifications (ISPS): The notation and its applications, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol C-30, No 1, </volume> <pages> pp 24-40, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: ISPS The goal of ISPS (Instruction Set Processor Specification <ref> [Barbacci81] </ref>, implemented in the N.2 system [Endot87] as well) is that both software (assembler and compiler) and hardware (the chip) are generated from the ISPS specification of a processor. A processor is specified in ISPS at register transfer level: in terms of registers and interconnections between. <p> Full emulation of the application 63 Assembly Store -Instr Stream Processor Model Memory Model Memory Model - - Data references Data, I/O Instr references Fake instructions a separate `store' (invisibly), but fetches to the instruction stream are still simulated. detail is ISPS <ref> [Barbacci81] </ref>. The user specifies the processor at register transfer level, specifies the instruction set coding, and ISPS builds a simulator for the processor, and an assembler that translates the application program to machine code.
Reference: [Barendregt92] <author> H.P. Barendregt, M. Beemster, P.H. Hartel, L.O. Hertzberger, R.F.H. Hof-man, K.G. Langendoen, L.L. Li, R. Milikowski, </author> <title> J.C. Mulder and W.G. Vree, Programming parallel reduction machines, </title> <type> Technical report CS-92-05, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The places where MiG needs to synchronise, and the way MiG is implemented are the subject of the remainder of this section MiG is constructed around the parallel functional language implementation described in <ref> [Barendregt92] </ref>. The application program is written in a functional language, with parallel tasks explicitly denoted by a fork-join annotation. The 4.2. Application derived address traces 69 application is compiled using FAST, FCG and CAT (page 66) to incorporate code for the extraction of the address trace.
Reference: [Beemster90] <author> M. Beemster, </author> <title> Back end aspects of the portable POOL implementation, </title> <editor> P. America (ed), </editor> <booktitle> Proceedings of the PRISMA workshop on parallel databasesystems, Noordwijk, </booktitle> <address> The Netherlands, September 24-26, </address> <publisher> Springer Verlag LNCS 503, </publisher> <pages> 193-228, </pages> <year> 1991. </year>
Reference-contexts: A running POOL program consists of objects that communicate and synchronise by sending messages to each other. In general, neither the destination of a message, nor the size are known at compile time. Several POOL implementations have been made <ref> [Spek90, Beemster90] </ref>. For the experiments on the communication architecture, the latter one has been used. It translates POOL into C and uses a C-compiler to generate the assembly code for a specific processor. The run time support for the compiled code is entirely written in C as well.
Reference: [BenAri82] <author> M. Ben-Ari, </author> <title> Principles of concurrent programming, </title> <publisher> Prentice Hall, </publisher> <address> ISBN 0-13-701078-8, </address> <year> 1982. </year>
Reference-contexts: It is hard to give an example of a correct program that asynchronously writes data to shared variables: such a write will lead to disaster most of the time. Only if semaphores are constructed with an asynchronous write, as is the case with Dekker's semaphore <ref> [BenAri82] </ref>, one can write a correct program that cannot be simulated using sequential blocks, but this implementation is rather uncommon.
Reference: [Bevan86] <author> D.I. Bevan, </author> <title> The implementation of an event-driven logic simulator in a functional 146 Bibliography style, ESPRIT 415B document nr 043, </title> <month> August </month> <year> 1986. </year>
Reference-contexts: In contrast with the continuous time simulator, the lists consists of the changes only: the length of the lists does not depend on the granularity of the time. This implementation is slightly different from the implementations of <ref> [Bevan86] </ref> and [Joosten89], who have to pass a state parameter around, since their tuples are defined as From T V. The correctness of the discrete time simulator is not evident.
Reference: [Bird88] <author> R.S. Bird, and P.L. Wadler, </author> <title> Introduction to functional programming, </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [Birtwistle73] <editor> G.M. Birtwistle, O.-J. Dahl, B. Myhrhaug, and K. Nygaard, SIMULA begin, </editor> <year> 1973. </year>
Reference-contexts: There are a number of simulation languages that are in use. Only one of them is discussed: SIMULA <ref> [Dahl66, Birtwistle73] </ref>. Being the first simulation language (SIMULA '67 has been designed in the late sixties as an extension of ALGOL '60), and the first object oriented language, SIMULA contains all interesting aspects of simulation languages. For modelling entities of the real world, SIMULA introduced the class concept. <p> The language design has been influenced greatly by three languages: POOL [America89a], C 3.1. Pearl : The simulation language 39 Compiler C-compiler Loader Processor class definitions (extra C-functions) topology, array bounds program input ? ? ? ? -C executable instances [Kernighan78] and SIMULA <ref> [Birtwistle73] </ref>. The computational model of objects running in parallel and communicating with messages originates from POOL, the statement structure and the syntax of C has been used, while the object oriented simulation model originally stems from SIMULA. <p> Many object oriented language supports the concept of inheritance: a class can be defined in terms of another class, plus some new features. Inheritance is introduced for two purposes: for code reusability and as a mechanism for the creation of generic classes. SIMULA '67 for example <ref> [Birtwistle73] </ref> defines a class List which can be inherited by any object that should be queueable: if a class Y inherits the properties of List, then the actions applicable to lists are applicable to Y also, which means that instances of Y can be queued and dequeued.
Reference: [Bray89] <author> B. Bray, K. Cuderman, M. Flynn and A. Zimmerman, </author> <title> The computer architect's workbench, </title> <booktitle> Proceedings of IFIP '89 on VLSI and CAD Tools, </booktitle> <year> 1989. </year>
Reference-contexts: In real cases, the applications behaviour might depend on the speed of the processor's network. A situation that cannot be modelled using PARET (see Section 4.2 for a discussion on this topic). AWB AWB, the Architecture Work Bench <ref> [Bray89, Mulder87] </ref> developed at Stanford (where the MIPS processors originated as well), is used to evaluate the design of the storage hierarchy. <p> Processors with multiple pipelines (one of the extensions proposed in <ref> [Bray89] </ref>) will need another compiler, but it will be hard to write a compiler that generates optimal code for processors with any type of (multiple) pipelines, the current generation compilers can generate optimised code for specific pipelined designs only.
Reference: [Bronnenberg87] <editor> W.J.H.J. Bronnenberg, L. Nijman, E.A.M. Odijk, and R.A.H. van Twist, DOOM: </editor> <title> a decentralized object oriented machine, </title> <journal> IEEE micro, </journal> <volume> Vol 7, No 5, </volume> <pages> pp 52-69, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The experiment involves the evaluation of the communication part of the PRISMA architecture (PRISMA stands for PaRallel Inference and Storage MAchine [Apers90] and is also called POOMA -Parallel Object Oriented MAchine- or DOOM -Decentralised Object Oriented Machine- <ref> [Bronnenberg87] </ref>), which is based on the DOOM-CP. Three variants of the PRISMA architecture are simulated in order to evaluate the communication performance. Section 5.1 presents a brief description of the PRISMA architecture, and shows where the bottleneck in the current implementation is (see [Vlot90, Apers90] and [Bronnenberg87] for a concise description <p> -Decentralised Object Oriented Machine- <ref> [Bronnenberg87] </ref>), which is based on the DOOM-CP. Three variants of the PRISMA architecture are simulated in order to evaluate the communication performance. Section 5.1 presents a brief description of the PRISMA architecture, and shows where the bottleneck in the current implementation is (see [Vlot90, Apers90] and [Bronnenberg87] for a concise description of the architecture). The architecture has been modelled using Oyster, as is described in Section 5.2. In fact, the simulation of this architecture was the first experiment performed with Oyster, so only few of the features described in Chapter 3 have actually been used.
Reference: [Brown83] <author> Harold Brown, Cristopher Tong, and Gordon Foyster, Palladio: </author> <title> An Exploratory Environment for Circuit Design, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp 41-56, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: SmallTalk is an interpreted language (the most easy way to implement dynamic interactive languages), hence slower than C or Pascal. Although INSIST is well suited for designing a circuit, large evaluation runs cannot be made with it. Other hardware simulators There are many other hardware simulators (like Palladio <ref> [Brown83] </ref>), but they do not have interesting other features. All hardware simulators have the same drawback: the architecture is simulated in the perspective of debugging the circuit, of evaluating the functionality. None of the simulators helps in analysing or evaluating the performance of an architecture.
Reference: [Bryant84] <author> R.E. Bryant, </author> <title> A Switch level model and simulator for MOS systems, </title> <journal> IEEE Transactions on computers, </journal> <volume> Vol C-33, No 2, </volume> <pages> pp 160-177, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: These simulators iteratively updates the values of the wires, until the circuit stabilises, whereupon the clock of the circuit is advanced (see amongst others <ref> [Bryant84] </ref>). This type of simulators are a kind of continuous time simulators, but in contrast with real continuous simulators, the time step ffit is bound to the clock frequency of the circuit (this eases the interfacing).
Reference: [Bugge90] <author> H.O. Bugge, E.H. Kristiansen and B.O. Bakka, </author> <title> Trace-driven simulations for a two-level cache design in open bus systems, </title> <booktitle> Proceedings of the 17th Annual Int. Symposium on Computer Architecture, </booktitle> <pages> pp 250-259, </pages> <year> 1990. </year>
Reference-contexts: These published results have been derived from real world address traces, and cover important aspects of the hierarchical multiprocessor model: * [Hennessy90, Hill87]: Simulates a trace derived from a UNIX environment on a uniprocessor system with one single level cache. The cache size and associativity are varied. * <ref> [Bugge90] </ref>: Simulates a SINTRAN III trace on a uniprocessor system with a two-level cache hierarchy. The cache size and associativity are varied. An aspect that is not verified is the locality in using the shared space. <p> The deviation is neither surprising, nor alarming. 6.3.2 Single processor, two-level cache validation This experiment considers a uniprocessor with a small fast level 1 cache, followed by a large level 0 cache. <ref> [Bugge90] </ref> reports the miss rates of the level 0 cache with various parameter settings of line size, associativity, and total cache size. The level 1 cache is fixed as a 128Kb direct-mapped cache with 16 byte lines. <p> To deal with the effects of cold start misses in the address trace, that paper contains three miss rates: a worst, best, and estimate case. The worst case assumes that cold start misses are 6.3. Validation 111 <ref> [Bugge90] </ref> Simulation Size Associativity Worst Estimated Best model 1M 2-way 22.6 22.2 22.1 20.9 8-way 16.4 16.0 15.9 15.5 4-way 9.0 8.1 8.0 9.4 4M 2-way 7.3 5.6 5.5 7.3 8-way 5.9 4.0 4.0 5.8 4-way 4.6 1.3 1.3 5.4 Table 6.1: Miss rate of the level 0 cache (in %); <p> Simulation Size Associativity Worst Estimated Best model 1M 2-way 22.6 22.2 22.1 20.9 8-way 16.4 16.0 15.9 15.5 4-way 9.0 8.1 8.0 9.4 4M 2-way 7.3 5.6 5.5 7.3 8-way 5.9 4.0 4.0 5.8 4-way 4.6 1.3 1.3 5.4 Table 6.1: Miss rate of the level 0 cache (in %); <ref> [Bugge90] </ref> and model values indeed real misses, whereas the best case counts those misses as hits. The estimated miss rate simply ignores cold start misses (nor miss, nor hit). Since our memory hierarchy model is limited to equal line sizes in all levels of the hierarchy, and [Bugge90] uses a 16 <p> cache (in %); <ref> [Bugge90] </ref> and model values indeed real misses, whereas the best case counts those misses as hits. The estimated miss rate simply ignores cold start misses (nor miss, nor hit). Since our memory hierarchy model is limited to equal line sizes in all levels of the hierarchy, and [Bugge90] uses a 16 byte line for the level 1 cache, only the miss rates for the cases with a 16 byte line size have been compared (note that this is different from the Futurebus standard of 64 bytes). <p> Since the this cache has a miss rate of 3%, the level 0 cache gets 1:4 10 6 accesses. The results in Table 6.1 show that the miss rates of the stochastical simulation model are close to the figures reported in <ref> [Bugge90] </ref>, although our miss rates are systematically close to Bugge's worst case. The simulated figures of the 8 Mbyte level 0 cache are inaccurate because the cache did not reach a steady state before the end of the simulation run.
Reference: [Bunt84] <author> Richard B. Bunt and Jennifer M. Murphy, </author> <title> The measurement of locality and the behaviour of programs, </title> <journal> The computer journal, </journal> <volume> Vol 27, No 3, </volume> <pages> pp 238-245, </pages> <year> 1984. </year>
Reference-contexts: Simulating applications Validating a synthetic trace is hard because the property which is being modelled (locality) cannot be measured or calculated, because there is no well defined unit for it yet (although attempts have been done to define a unit of locality, for example <ref> [Bunt84] </ref>). The best option to validate a synthetic trace is to apply the trace in a number of experiments with well known results, and to compare the results of these experiments.
Reference: [Chaiken90] <author> David Chaiken, Craig Fields, Kioshi Kurihara and Anant Agarwal, </author> <title> Directory based cache coherence in large scale multiprocessors, </title> <booktitle> IEEE Computer, </booktitle> <pages> pp 49-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: For architectures without a single broadcasting medium, all other copies of cached data have to be found when an update operation is performed. It is for example possible to search the data with the help of directories that point where shared data has gone <ref> [Chaiken90] </ref>. There are more alternatives, see for example [Stenstr om90] for a survey on this topic. Combining broadcasts and directories, caching shared memory architectures can be constructed that use multiple (broadcasting) bus segments with directories in between, as is for example implemented in the Data Diffusion Machine [Warren88].
Reference: [Chandy79] <author> K. Mani Chandy and Jayadev Misra, </author> <title> Distributed Simulation: A Case Study in Design and Verification of Distributed Programs, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol SE-5, No 5, </volume> <pages> pp 440-452, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: The meaningless tuples floating around are essentially caused by the distributed nature of the discrete time simulator: the two nands and wires operate completely autonomously which makes an empty tuple essential for the progress in the simulation (the problem is identical to that in the distributed simulation algorithm of <ref> [Chandy79] </ref>, where NULL-messages are used to keep a distributed simulator running). <p> Interfacing a discrete event simulator requires that the event lists of the VLSI and Pearl simulators are effectively merged (a problem similar to the synchronisation of nodes of a parallel simulation as for example in <ref> [Chandy79] </ref>). By running the two simulators as co-routines, and by passing the control to the other co-routine each time the virtual clock would advance over the first event of the other simulator, the simulators are kept synchronised.
Reference: [Cherry88] <author> James J. Cherry, Pearl: </author> <title> a CMOS timing analyzer, </title> <booktitle> Proceedings 25 t h ACM/IEEE design automation conference, </booktitle> <year> 1988. </year>
Reference-contexts: A description of the library elements currently implemented in Oyster is presented in Section 3.3. 1 Pearl stands for Pure Evaluation and Architecture Research Language and should not be confused with three other Pearls: [Deering79], [Martin81] and <ref> [Cherry88] </ref>. 38 Chapter 3. The simulation environment Other simulatorsArchitect Standard Modules Object Oriented Simulation Discrete Time Simulation Kernel Simulation Platform (stock hardware) ? ? 6 6 6 6 6 Parameter definition Pearl C Machine language The three levels of Oyster are mapped onto the respective lower levels by compilers.
Reference: [Dacapo89] <institution> Private communication with a sales manager of Dosis Germany, </institution> <year> 1989. </year>
Reference-contexts: These simulators show the most interesting aspects of hardware simulation languages. VHDL A number of popular tools exist that are used for the specification of hardware: Dacapo <ref> [Dacapo89] </ref> and VHDL [IEEE88] (based on Pascal), or Verilog [Verilog89] (based on C and Pascal). Although the syntax and details of these languages differ, the (imperative) base language was in all cases extended with the same constructs: wires and the notion of time.
Reference: [Dahl66] <author> Ole-Johan. Dahl and Kristen Nygaard, </author> <title> SIMULAan ALGOL-based simulation language, </title> <journal> Communications of the ACM, </journal> <volume> Vol 9, No 9, </volume> <pages> pp 671-678, </pages> <month> September </month> <year> 1966 </year>
Reference-contexts: The threads of control of the various objects do not interfere unless explicitly stated, a model closely resembling the real world. The perfect fit between object oriented programming and simulations is no coincidence: the language first raising the concepts of object orientedness was SIMULA '67 <ref> [Dahl66] </ref>, a simulation language. In comparison with the use of an imperative language to implement a simulator, the use of an object oriented language solves at least one problem: the structuring of a model in terms of objects is more convenient than structuring a simulator in C or Pascal. <p> There are a number of simulation languages that are in use. Only one of them is discussed: SIMULA <ref> [Dahl66, Birtwistle73] </ref>. Being the first simulation language (SIMULA '67 has been designed in the late sixties as an extension of ALGOL '60), and the first object oriented language, SIMULA contains all interesting aspects of simulation languages. For modelling entities of the real world, SIMULA introduced the class concept.
Reference: [Dally86] <author> William J. Dally and Charles L. </author> <title> Seitz The torus routing chip, </title> <booktitle> Distributed computing 1, </booktitle> <pages> pp 187-196, </pages> <year> 1986. </year>
Reference-contexts: Three recent hardware designs in this field are the Torus Routing Chip (or TRC, <ref> [Dally86] </ref>), the Adaptive Routing Chip (the ARC, [Mooij89]), and the DOOM Communication processor (or CP, [Annot87]). These three communication processors have all been analysed, simulated and built. The analysis was made to prove the correctness of the network (deadlock freedom) or to analyse the networks performance behaviour (queuing models).
Reference: [Deering79] <author> Michael Deering, Joseph Faletti and Robert Wilensky, </author> <title> PEARL A Package for Efficient Access to Representations in LISP, </title> <booktitle> Proceedings of the seventh international joint conference on Artificial Intelligence, IJCAI-81, </booktitle> <pages> August 24-28, </pages> <address> Vancouver, Canada, </address> <year> 1981. </year>
Reference-contexts: A description of the library elements currently implemented in Oyster is presented in Section 3.3. 1 Pearl stands for Pure Evaluation and Architecture Research Language and should not be confused with three other Pearls: <ref> [Deering79] </ref>, [Martin81] and [Cherry88]. 38 Chapter 3.
Reference: [Delgado88] <author> Jose C. Delgado, </author> <title> Designing computer architectures with SADAN, </title> <booktitle> Micropro-cessing and Microprogramming 22, </booktitle> <pages> pp 205-216, </pages> <publisher> North Holland, </publisher> <year> 1988. </year>
Reference-contexts: Dinero is a typical example of a very small, very specialised tool. It aims at only one aspect (cache evaluation), and is fully flexible on that particular part. All other aspects are totally ignored. SADAN SADAN <ref> [Delgado88] </ref> is an architecture simulation language aiming at the evaluation of computer architectures. The used description language is an extension of 2.2. Existing simulation systems 35 Pascal, with concurrency, primitives for event driven simulation and communication, and statistical features. SADAN distinguishes between different types of communication.
Reference: [Denning72] <author> Peter J. Denning, </author> <title> On modelling program behaviour, </title> <booktitle> AFIPS conference proceedings, </booktitle> <volume> Vol 40, </volume> <booktitle> Spring Joint computer conference, </booktitle> <address> Atlantic City, New Jersey, </address> <pages> pp 937-943, </pages> <month> May 16-18, </month> <year> 1972. </year> <title> [Digital72] pdp11/45 processor handbook, </title> <institution> Digital Equipment Corporation, </institution> <year> 1972. </year>
Reference-contexts: For these experiments, the architect can use synthetic address traces. A synthetic trace is a random sequence of addresses, that is used as a substitute for a real address trace. The word random is quoted, because a synthetic trace should exhibit temporal and spatial locality <ref> [Denning72] </ref> like real address traces. Synthetic address traces can be tuned to the wishes of the architect, and can be generated without huge memory constraints. Their use has one major drawback: the results obtained with a synthetic trace are less reliable than the results obtained with a real address trace.
Reference: [Eggers89] <author> S.J. Eggers and R.H. Katz, </author> <title> Evaluating the performance of four snooping cache coherency protocols, </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp 2-15, </pages> <year> 1989. </year>
Reference-contexts: Therefor a segment with a size of 10 Kbyte is chosen, with an average access rate of once every 10 instructions. Using these parameters, the results for the write broadcast ratio are comparable to the results reported in <ref> [Eggers89] </ref>. <p> The cache size and associativity are varied. An aspect that is not verified is the locality in using the shared space. The parameters for the shared space are calibrated so that a write broadcast ratio is comparable to <ref> [Eggers89] </ref>, but this is only a one-point-calibration. For this reason, four flavours of the parallel workload have been used in the experiments. 110 Chapter 6. <p> The parallel application results are similar to the UNIX mix, only the optimal performance is reached with a smaller line size of 64 bytes. This optimum depends on the way the shared data is used. As stated in <ref> [Eggers89] </ref> 114 Chapter 6. Simulating the Futurebus applications can be specifically coded for a certain line size.
Reference: [Endot87] <editor> N.2 Introduction and Tutorial, </editor> <publisher> Endot Inc. </publisher> <address> 11001 Cedar Ave., Cleveland, Ohio Bibliography 147 44106, </address> <year> 1987. </year>
Reference-contexts: In comparison with SIMULA, the event list of HDL (and many other simulators: Verilog, N.2 <ref> [Endot87] </ref>) is a mess of postponed assignments, while an event of a SIMULA implementation only starts a process, which must explicitly assign, and explicitly remember the old state. ART-DACO ART-DACO is an experimental simulation system aiming at the construction of high level abstractions of circuits prior to design [Krishnakumar87]. <p> ISPS The goal of ISPS (Instruction Set Processor Specification [Barbacci81], implemented in the N.2 system <ref> [Endot87] </ref> as well) is that both software (assembler and compiler) and hardware (the chip) are generated from the ISPS specification of a processor. A processor is specified in ISPS at register transfer level: in terms of registers and interconnections between.
Reference: [Futurebus89] <editor> Futurebus Logical Layer Specifications, Draft 8.1.1, </editor> <booktitle> P896.1 Working Group of the IEEE Computer Society, </booktitle> <month> December </month> <year> 1990. </year>
Reference-contexts: A straightforward implementation maps the P operation onto an indivisible Test-And-Set instruction: in one indivisible bus cycle, the semaphore value is fetched, tested and overwritten. In an architecture with distributed caches with a MOESI protocol [Sweazy86] (like the Futurebus <ref> [Futurebus89] </ref> or SCI based systems [SCI92]), the TAS-operation requires the cache line containing the semaphore to be fetched exclusively. <p> Combining broadcasts and directories, caching shared memory architectures can be constructed that use multiple (broadcasting) bus segments with directories in between, as is for example implemented in the Data Diffusion Machine [Warren88]. The Futurebus+ 1 <ref> [Futurebus89] </ref> defines an industry standard bus that supports this type of hierarchical architectures. In this chapter, a study on the performance of hierarchical cache architectures that can be built on top of the Futurebus is presented. <p> The Futurebus is defined in several layers <ref> [Futurebus89] </ref>: the wires and their electrical behaviour (timings, glitches, voltages, live insertion), the arbitration order (preferences, masters, slaves), the basic protocols for data transport (reading and writing), a cache consistency protocol for systems with multiple caches and an arbitrary number of bus segments, and a message protocol for bulk data transport. <p> Implementing these nasty features, account for about 80% of the work. 6.1.4 Discrepancies between the simulator and the real Futurebus Above only a short description is given of the part of the Futurebus used in the simulator. The details about the Futurebus specification can be found in <ref> [Futurebus89] </ref>. The most important deviations are: * The Futurebus defines an Unmodified exclusive state, for data that is exclu sive, but not (yet) modified. The simulator only implements the states Invalid, 106 Chapter 6.
Reference: [Gijsen92] <author> Nienke Gijsen, </author> <title> Simulation of a micro parallel lazy graph reducer: the G-Hinge, </title> <type> Master Thesis, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> Decem-ber </month> <year> 1992. </year>
Reference-contexts: During each experiment, Oyster is improved by adding or deleting features. Oyster will never be finished; due to new experiences gained while simulating architectures, Oyster will keep changing. The usefulness of Oyster has been tested in several case studies: for the simulation of a VLIW machine for graph reduction <ref> [Milikowski92, Gijsen92, Hendriksen90] </ref>, for the the simulated execution of functional programs on shared memory architectures [Langendoen92a, Hofman93], for a study towards the communication architecture of the PRISMA architecture and (Chapter 5), and for the simulation of hierarchical architectures based on the Futurebus (Chapter 6).
Reference: [Goor89] <editor> A.J. van de Goor, </editor> <booktitle> Computer architecure and design, </booktitle> <publisher> Addison Wesley, </publisher> <address> ISBN 0-201-18241-6, </address> <year> 1989. </year>
Reference: [Gottlieb83] <author> A. Gottlieb, R. Grishman, C.P. Kruskal, K.P. McAuliffe, L. Rudolph and M. Snir, </author> <title> The NYU Ultracomupter designing an MIMD shared memory parallel computer, </title> <journal> IEEE Transactions on computers, </journal> <volume> Vol C-32, No 2, </volume> <pages> pp 175-189, </pages> <year> 1983. </year>
Reference-contexts: In early large shared memory machines, like the NYU <ref> [Gottlieb83] </ref>, this problem was alleviated by splitting the memory in modules, and connecting the processors and the memory modules with a switch, to route memory accesses of the processors to the right memory module.
Reference: [Graham82] <author> S.L. Graham, P.B. Kessler, </author> <title> M.K. McKusick, gprof: A Call Graph Execution Profiler, </title> <booktitle> Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, SIGPLAN notices, </booktitle> <volume> Vol 17, No 6, </volume> <pages> pp 120-126, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: This means that this measure implemented in Pearl does not introduce any uncertainties. Call graph analysis The call graph analysis is the most sophisticated analysis currently in Oyster. It has close resemblance with the call graph analysers found in conventional software environments (like gprof under UNIX <ref> [Graham82] </ref>), and the critical path analysis found in electrical simulators (as for example [Wallace88]), the call graph analyser is used to find the critical paths in the architecture. This is implemented by analysing each idle period of an object.
Reference: [Gupta89] <author> Saurabh Gupta and Rami Melhem, </author> <title> A software tool for the automatic generation of memory traces for shared memory multiprocessor systems, </title> <booktitle> Proceedings of the 22 nd annual simulation symposium, </booktitle> <address> Tampa, Florida, </address> <pages> pp 93-104, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Any compiler can be extended to generate these extra function calls <ref> [Gupta89, Langendoen92b] </ref>. In the remainder of this section one such an extension, called CAT for Compiled Address Tracing, build around the FCG code generator [Langendoen92c], is described in more detail because it is used in Section 4.2.2. The FCG code generator is a back-end for the FAST compiler [Hartel91].
Reference: [Hagersten91] <author> Erik Hagersten, </author> <title> A Simulated MIMD Running Gcycles of Real Programs, </title> <booktitle> First European workshop on Performance modelling and Evaluation of Parallel Computer Systems, </booktitle> <year> 1991. </year>
Reference-contexts: The shared memory model may be of any complexity: a single bus, caches, switches, or virtual shared memory. Opposed to the emulation of a parallel program at assembly level, or other approaches to the simulation of parallel programs (for example <ref> [Hagersten91] </ref>), MiG does not synchronise after each instruction, but only after the execution of a large number of instructions. Because of the overhead (context switches, tests) involved in the synchronisation, synchronising at each instruction is inefficient; MiG allows large programs to be simulated on a parallel machine, without that overhead.
Reference: [Hartel91] <author> Pieter Hartel, Hugh Glaser, and John Wild, </author> <title> Compilation of functional languages using flow graph analysis, </title> <type> Technical report 91-03, </type> <institution> Computer science department, University of Southampton, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: In the remainder of this section one such an extension, called CAT for Compiled Address Tracing, build around the FCG code generator [Langendoen92c], is described in more detail because it is used in Section 4.2.2. The FCG code generator is a back-end for the FAST compiler <ref> [Hartel91] </ref>. FAST and FCG together translate a program written in a functional language into C. Together with a run time support system that is entirely written in C, the functional programs can be executed with reasonably good performance on any computer platform equipped with a C-compiler.
Reference: [Hendriksen90] <author> Michel Hendriksen, </author> <title> Simulation of a Sequential Graph Reduction Machine, </title> <type> Master Thesis, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> August 31, </month> <year> 1990. </year>
Reference-contexts: During each experiment, Oyster is improved by adding or deleting features. Oyster will never be finished; due to new experiences gained while simulating architectures, Oyster will keep changing. The usefulness of Oyster has been tested in several case studies: for the simulation of a VLIW machine for graph reduction <ref> [Milikowski92, Gijsen92, Hendriksen90] </ref>, for the the simulated execution of functional programs on shared memory architectures [Langendoen92a, Hofman93], for a study towards the communication architecture of the PRISMA architecture and (Chapter 5), and for the simulation of hierarchical architectures based on the Futurebus (Chapter 6).
Reference: [Hennessy90] <author> J.L. Hennessy and D.A. Patterson, </author> <title> Computer architecture: a quantitative approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, California, ISBN 1-55860-069-8, </address> <year> 1990. </year>
Reference-contexts: In 1 Note, in the sequel the `+' is omitted for readability. 2 Throughout this thesis the term line is used to denote the basic entity stored in the cache. The 6.1. Introduction to the Futurebus cache consistency 101 contrast with many other studies (for example [Przybylski90] and <ref> [Hennessy90] </ref>), the studies towards line size and associativity are performed in the context of hierarchical multiprocessor architectures, as is for example also the case in [Baer88]. Because of invalidates and sharing, the optimal values for these parameters for single and multi processor systems are not necessarily identical. <p> The static parameters of these programs (like the number of instructions, functions, sizes of the various segments) have been taken from real world systems. The dynamic parameters, like the average number of executed jumps and their distances have been taken from literature <ref> [Hennessy90] </ref> and from profile runs. The UNIX workload consists of 50 jobs (6-7 of each class). The jobs all have their own private data and stack segments, and do not use any shared data. <p> To verify the stochastical model the results of simulation runs are compared with the results of experiments described in the literature. These published results have been derived from real world address traces, and cover important aspects of the hierarchical multiprocessor model: * <ref> [Hennessy90, Hill87] </ref>: Simulates a trace derived from a UNIX environment on a uniprocessor system with one single level cache. The cache size and associativity are varied. * [Bugge90]: Simulates a SINTRAN III trace on a uniprocessor system with a two-level cache hierarchy. The cache size and associativity are varied. <p> Both experiments are performed with a UNIX workload with 10 jobs, that most closely matches the workload used in both articles. Although UNIX is not SINTRAN III, they both are large multi-programming environments. 6.3.1 Single processor, one-level cache validation This experiment is described in <ref> [Hennessy90, Hill87] </ref>. It is a trace driven simulation of a VAX processor with a single cache, running a multi programming workload. The cache size and associativity are variable. The study of [Hill87] has decomposed the cache miss rate into three fractions, but here only the total miss rates are considered. <p> Given this associativity the best linesize is measured (although the Futurebus fixes the linesize to 64 bytes, it is interesting to find the optimal linesize). 6.4.1 Tuning the associativity The associativity of a cache has a known important effect on the miss rate, see for example <ref> [Hennessy90] </ref>. The question addressed here is what associativity should be used for lower level caches in a hierarchical architecture. A common line of reasoning is that low-level caches should be at least large enough to hold all lines of the caches upward in the hierarchy (called the inclusion property, [Baer88]). <p> In [Agarwal89] a performance model is presented that accurately predicts the miss rate of a cache in a single processor architecture (the step from miss rates to performance is straightforward, see for example <ref> [Hennessy90] </ref>). The model needs a description of the cache (the associativity, size and so on), and a specification of the application behaviour (in terms of the number of uniquely referenced blocks, and the access rate) to calculate the miss rate. <p> The rest of this section shows a stepwise construction of the performance model, followed by a comparison with the simulation results of the previous chapter. 7.1.1 The performance of cache architectures The average time needed for an access to the cache is given in <ref> [Hennessy90] </ref>. On a hit, a cache provides the data after a delay of the cache cycle time. On a cache miss, the cache queries the memory and provides the data after querying the shared memory. The delay of the level below the cache is commonly known as the miss-penalty.
Reference: [Hercksen82] <author> Uwe Hercksen, Raiser Klar, Wolfgang Kleinoder, Franz Kneil, </author> <title> Measuring simultaneous events in a multiprocessor system, </title> <journal> ACM Sigmetrics, Performance Evaluation Review, </journal> <volume> Vol 11, No 4, </volume> <pages> pp 77-88, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Hardware intervention. To get a real time address trace, hardware monitors are often used. A bus-spy or some other hardware device snoops all the traffic on the memory bus, and stores the references for later usage <ref> [Hercksen82] </ref>. A specialised hardware device is the best way to fetch traces, because these traces give an exact representation of the application program running on that particular machine, without disturbing the execution in any way. Microcode intervention.
Reference: [Hill87] <author> M.D. Hill, </author> <title> Aspects of Cache Memory and Instruction Buffer Performance, </title> <type> Ph.D. Thesis, </type> <institution> Univ. of California at Berkeley Computer Science Division, </institution> <type> Tech. Rep. </type> <address> UCB/CSD 87/381, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: To verify the stochastical model the results of simulation runs are compared with the results of experiments described in the literature. These published results have been derived from real world address traces, and cover important aspects of the hierarchical multiprocessor model: * <ref> [Hennessy90, Hill87] </ref>: Simulates a trace derived from a UNIX environment on a uniprocessor system with one single level cache. The cache size and associativity are varied. * [Bugge90]: Simulates a SINTRAN III trace on a uniprocessor system with a two-level cache hierarchy. The cache size and associativity are varied. <p> Both experiments are performed with a UNIX workload with 10 jobs, that most closely matches the workload used in both articles. Although UNIX is not SINTRAN III, they both are large multi-programming environments. 6.3.1 Single processor, one-level cache validation This experiment is described in <ref> [Hennessy90, Hill87] </ref>. It is a trace driven simulation of a VAX processor with a single cache, running a multi programming workload. The cache size and associativity are variable. The study of [Hill87] has decomposed the cache miss rate into three fractions, but here only the total miss rates are considered. <p> It is a trace driven simulation of a VAX processor with a single cache, running a multi programming workload. The cache size and associativity are variable. The study of <ref> [Hill87] </ref> has decomposed the cache miss rate into three fractions, but here only the total miss rates are considered. The data in Figure 6.5 shows that the stochastical simulation model compares well to the measurements of [Hill87]. <p> The cache size and associativity are variable. The study of <ref> [Hill87] </ref> has decomposed the cache miss rate into three fractions, but here only the total miss rates are considered. The data in Figure 6.5 shows that the stochastical simulation model compares well to the measurements of [Hill87]. The miss rates for small caches are a bit too high, and the miss rates for larger caches are a bit too low. For caches with increased associativity the miss rates show similar results with a slightly higher deviation for large cache sizes.
Reference: [Hill89] <author> Dinero III cache simulator, </author> <note> on line documentation version 3.3, from markhill@cs.sic.edu. </note>
Reference-contexts: The AWB is different from the other tools in this section because the application program, compiler generated code, processor and memory hierarchy are simulated. Like the other tools, AWB cannot be extended to simulate more exotic or parallel architectures easily. Dinero Dinero <ref> [Hill89] </ref> is a very simple but rather effective cache evaluation tool. The input of dinero consists of an address trace, and a set of cache parameters amongst others specifying the size, the associativity and the fetch policy (prefetching, demand fetching).
Reference: [Hofman93] <author> R.F.H. Hofman, </author> <title> Scheduling and Grain Size Control, </title> <type> Ph.D. thesis, </type> <institution> Computer science department, University van Amsterdam, </institution> <year> 1993. </year>
Reference-contexts: The two simulation methods presented in Section 4.1 and Section 4.3 have been used for the expire-ments described in Part II of this thesis, the MiG (described in Section 4.2) is used for the applications simulated in [Langendoen92a] and <ref> [Hofman93] </ref>. 4.1 Full emulation of the application The most natural way to simulate the application program is to emulate the application at the conventional machine level. During such an emulation, all instructions that would have been executed in the real world, are simulated on the architecture under study. <p> MiG is currently only implemented for a functional language, but there is no reason why it should not work for other parallel programs obeying the restrictions listed on page 71. Examples of the use of the MiG are in <ref> [Langendoen92a, Muller92a, Hofman93] </ref> 4.3 Stochastically generated address traces Both the extraction of address traces and the emulation of a program suffer from high computational and memory demands. <p> It would have been possible to use on-line generated address traces (as the MiG), but a tunable application was preferable. The MiG has been used in a study towards memory management algorithms, as presented in [Langendoen92a], and for a study towards scheduling strategies for hierarchical caching architectures <ref> [Hofman93] </ref>. Part II Case studies Chapter 5 Simulating PRISMA's communication architecture y An interesting field to study the evaluation of computer architecture designs is the development of communication architectures for distributed memory multi processor systems. <p> The usefulness of Oyster has been tested in several case studies: for the simulation of a VLIW machine for graph reduction [Milikowski92, Gijsen92, Hendriksen90], for the the simulated execution of functional programs on shared memory architectures <ref> [Langendoen92a, Hofman93] </ref>, for a study towards the communication architecture of the PRISMA architecture and (Chapter 5), and for the simulation of hierarchical architectures based on the Futurebus (Chapter 6).
Reference: [Horbst87] <author> E. Horbst, C. M uller-Schloer, H. Schwartzel, </author> <title> Design of VLSI Circuits, </title> <publisher> Spring-weer Verlag, </publisher> <address> ISBN 0-387-17663-2, </address> <year> 1987 </year>
Reference: [IEEE88] <institution> IEEE Standard VHDL Language reference manual, </institution> <year> 1988. </year>
Reference-contexts: These simulators show the most interesting aspects of hardware simulation languages. VHDL A number of popular tools exist that are used for the specification of hardware: Dacapo [Dacapo89] and VHDL <ref> [IEEE88] </ref> (based on Pascal), or Verilog [Verilog89] (based on C and Pascal). Although the syntax and details of these languages differ, the (imperative) base language was in all cases extended with the same constructs: wires and the notion of time.
Reference: [Jain91] <author> Raj Jain, </author> <title> The art of computer systems performance analysis, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, California, ISBN 0-471-50336-3, </address> <year> 1991. </year>
Reference-contexts: The processor speed is 1=B 1 , but since only a fraction m of the accesses is passed on to the memory bus, equals m=B 1 . In contrast with an ordinary M/M/1 model <ref> [Jain91] </ref>, the access rates decrease along the chain, from n to 0, because a processor that posts a request has to wait for the answer before it can post a next request. <p> In general, Q i depends on i , i depends on Q i+1 and Q i1 , so Q i+1 and Q i1 depend on i+1 and i1 respectively, which in turn depend on Q i . This cyclic dependency between the various queueing models is hard to resolve <ref> [Jain91] </ref>. For this reason, another approach is used for multiple level hierarchies. The model is based on the assumption that the speed of the architecture is dictated by the speed of the most heavily loaded bus.
Reference: [Jefferson85] <author> David R. Jefferson, </author> <title> Virtual time, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol 7, No 3, </volume> <pages> pp 404-425, </pages> <month> July </month> <year> 1985. </year> <note> 148 Bibliography </note>
Reference: [Jog90] <author> Rajeev Jog, Philip L. Vitale, James R. Callister, </author> <title> Performance evaluation of a commercial cache-coherent shared memory multiprocessor, </title> <booktitle> Proceedings 1990 ACM SIGMET-RICS, Boulder, Colorado, May 22-35 Performance Evaluation Review Vol 18, </booktitle> <volume> No 1, </volume> <pages> pp 173-182, </pages> <year> 1990. </year>
Reference: [Joosten89] <author> S. Joosten, </author> <title> The use of functional programs in software development, </title> <type> Ph.D. Thesis, </type> <institution> Twente University, </institution> <address> The Netherlands, ISBN 90-9002729-7, </address> <month> April 4, </month> <year> 1989. </year>
Reference-contexts: In contrast with the continuous time simulator, the lists consists of the changes only: the length of the lists does not depend on the granularity of the time. This implementation is slightly different from the implementations of [Bevan86] and <ref> [Joosten89] </ref>, who have to pass a state parameter around, since their tuples are defined as From T V. The correctness of the discrete time simulator is not evident. To prove it, it has to be proven that the simulator does not deadlock, generates correct outputs, and makes progress in time.
Reference: [Kahn74] <author> Giles Kahn, </author> <title> The semantics of a simple language for parallel programming, </title> <booktitle> Information Processing 74, </booktitle> <publisher> North Holland, </publisher> <pages> pp 471-475, </pages> <year> 1974. </year>
Reference-contexts: The wires are represented by infinite lists (streams) of states. The i th element of such a list represents the value of the wire at time iffi t , where ffi t is the time increment. The components are modelled by functions working on these lists, as synchronous processes <ref> [Kahn74] </ref>. The nand-processes of the Flip-Flop thus take two lists of states as input parameters and produce a list of states as outputs. The source code for a continuous simulation of the Flip-Flop is shown in Figure 2.5.
Reference: [Kelly89] <author> Paul Kelly, </author> <title> Functional programming for loosely-coupled multiprocessors, </title> <publisher> Pitman Publishing, </publisher> <address> ISBN 0-273-08804-1, </address> <year> 1989. </year>
Reference: [Kernighan78] <author> Brian W. Kernighan and Dennis M. Ritchie, </author> <title> The C Programming Language, </title> <publisher> Prentice Hall, </publisher> <address> ISBN 0-13-110163-3, </address> <year> 1978. </year>
Reference-contexts: The language design has been influenced greatly by three languages: POOL [America89a], C 3.1. Pearl : The simulation language 39 Compiler C-compiler Loader Processor class definitions (extra C-functions) topology, array bounds program input ? ? ? ? -C executable instances <ref> [Kernighan78] </ref> and SIMULA [Birtwistle73]. The computational model of objects running in parallel and communicating with messages originates from POOL, the statement structure and the syntax of C has been used, while the object oriented simulation model originally stems from SIMULA.
Reference: [Krishnakumar87] <author> A.S. Krishnakumar, ART-DACO: </author> <title> Architectural research tool using data abstraction and concurrency, </title> <booktitle> Proceedings of the international conference on computer design, </booktitle> <pages> pp 18-21, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: ART-DACO ART-DACO is an experimental simulation system aiming at the construction of high level abstractions of circuits prior to design <ref> [Krishnakumar87] </ref>. ART-DACO (Architecture Research Tool using Data Abstraction and COncurrency) allows the designer to answer so called what-if questions, for example: What if the width of the bus is doubled? As the name explains, ART-DACO models are constructed around abstract data types.
Reference: [Langendoen91] <author> K.G. Langendoen, H.L. Muller and L.O. Hertzberger, </author> <title> Evaluation of Fu-turebus hierarchical caching, </title> <booktitle> Proceedings of PARLE 91, </booktitle> <address> June 10-13, Veldhoven, </address> <publisher> Springer Verlag LNCS 505, </publisher> <pages> pp 52-68, </pages> <year> 1991. </year>
Reference-contexts: Since only 5% of the accesses are routed to the memory, more processors can be connected to the shared memory without running into contention problems. The current generation top y This chapter is based on <ref> [Langendoen91, Muller92b] </ref>. 100 Chapter 6. Simulating the Futurebus of the line minicomputers, for example from Data General and SUN, are shared memory machines with caches.
Reference: [Langendoen92a] <author> K.G. Langendoen, H.L. Muller, </author> <title> W.G. Vree, Memory Management for parallel tasks in shared memory, </title> <editor> Y. Bekkers, and J. Cohen (eds), </editor> <booktitle> Memory Management, proceedings of the international workshop IWMM 92, </booktitle> <address> St Malo, France, </address> <publisher> LNCS 637, </publisher> <pages> pp 165-178, </pages> <month> September 16-18, </month> <year> 1992. </year>
Reference-contexts: The two simulation methods presented in Section 4.1 and Section 4.3 have been used for the expire-ments described in Part II of this thesis, the MiG (described in Section 4.2) is used for the applications simulated in <ref> [Langendoen92a] </ref> and [Hofman93]. 4.1 Full emulation of the application The most natural way to simulate the application program is to emulate the application at the conventional machine level. During such an emulation, all instructions that would have been executed in the real world, are simulated on the architecture under study. <p> MiG is currently only implemented for a functional language, but there is no reason why it should not work for other parallel programs obeying the restrictions listed on page 71. Examples of the use of the MiG are in <ref> [Langendoen92a, Muller92a, Hofman93] </ref> 4.3 Stochastically generated address traces Both the extraction of address traces and the emulation of a program suffer from high computational and memory demands. <p> A stochastically generated trace is used, because of the freedom in tuning the parameters. It would have been possible to use on-line generated address traces (as the MiG), but a tunable application was preferable. The MiG has been used in a study towards memory management algorithms, as presented in <ref> [Langendoen92a] </ref>, and for a study towards scheduling strategies for hierarchical caching architectures [Hofman93]. Part II Case studies Chapter 5 Simulating PRISMA's communication architecture y An interesting field to study the evaluation of computer architecture designs is the development of communication architectures for distributed memory multi processor systems. <p> The usefulness of Oyster has been tested in several case studies: for the simulation of a VLIW machine for graph reduction [Milikowski92, Gijsen92, Hendriksen90], for the the simulated execution of functional programs on shared memory architectures <ref> [Langendoen92a, Hofman93] </ref>, for a study towards the communication architecture of the PRISMA architecture and (Chapter 5), and for the simulation of hierarchical architectures based on the Futurebus (Chapter 6).
Reference: [Langendoen92b] <author> K.G. Langendoen and D.J. Agterkamp, </author> <title> Cache Behaviour of Lazy Functional Programs, </title> <editor> H. Kuchen and R. Loogen (eds), </editor> <booktitle> 4th International Workshop on the Parallel Implementation of Functional Languages, </booktitle> <address> Aachen, Germany, Aach-ener Informatik-Berichte 92-19, RWTH Aachen, Fachgruppe Informatik pp 125-138, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: Any compiler can be extended to generate these extra function calls <ref> [Gupta89, Langendoen92b] </ref>. In the remainder of this section one such an extension, called CAT for Compiled Address Tracing, build around the FCG code generator [Langendoen92c], is described in more detail because it is used in Section 4.2.2. The FCG code generator is a back-end for the FAST compiler [Hartel91].
Reference: [Langendoen92c] <author> K.G. Langendoen and P.H. Hartel, FCG: </author> <title> a code generator for lazy functional languages, in Compiler construction (CC), </title> <publisher> Springer Verlag LNCS 641, </publisher> <editor> U. Kastens and P. Pfahler (eds), </editor> <booktitle> Proceedings of the International Workshop on Compiler Construction, Paderborn, Germany, </booktitle> <pages> pp 278-296, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Any compiler can be extended to generate these extra function calls [Gupta89, Langendoen92b]. In the remainder of this section one such an extension, called CAT for Compiled Address Tracing, build around the FCG code generator <ref> [Langendoen92c] </ref>, is described in more detail because it is used in Section 4.2.2. The FCG code generator is a back-end for the FAST compiler [Hartel91]. FAST and FCG together translate a program written in a functional language into C.
Reference: [Lewin85] <author> Douglas Lewin, </author> <title> Design of Logic Systems, ISBN 0-442-30606-7, T.J. </title> <publisher> Press Ltd, </publisher> <address> Padstow, Cornwall, </address> <year> 1985. </year>
Reference: [MacDougall87] <author> M.H. MacDougall, </author> <title> Simulating Computer Systems, techniques and tools, ISBN 0-262-13229-X, </title> <publisher> MIT Press, </publisher> <address> Massachusetts, </address> <year> 1987. </year>
Reference: [Martin81] <author> T. Martin, </author> <title> PEARL at the age of three, </title> <booktitle> Proceedings fourth international conference on software engineering, </booktitle> <address> September 17-19, M unchen, Germany, </address> <year> 1979. </year>
Reference-contexts: A description of the library elements currently implemented in Oyster is presented in Section 3.3. 1 Pearl stands for Pure Evaluation and Architecture Research Language and should not be confused with three other Pearls: [Deering79], <ref> [Martin81] </ref> and [Cherry88]. 38 Chapter 3.
Reference: [Meulen87] <author> Pieter S. van der Meulen, INSIST: </author> <title> Interactive Simulation in SmallTalk, </title> <booktitle> Proceedings of OOPSLA '87, </booktitle> <pages> pp 366-376, </pages> <month> October 4-8, </month> <year> 1987. </year>
Reference-contexts: These two are not merged, as is the case in for example SIMULA. Because they are separate, it is easy to modify the topology of the architecture. INSIST INSIST (INteractive Simulation In SmallTalk) is a simulation system developed at Philips research laboratories in Sunneyvale <ref> [Meulen87] </ref>. INSIST is build on top of SmallTalk (an object oriented language with a graphics interface), and has a SmallTalk alike appearance: interactive, objects, and graphics. For simulation purposes, INSIST supports two important features: the virtual time and a library of standard components.
Reference: [Milikowski91] <author> R. Milikowski and W.G. Vree, </author> <title> The G-Line, a distributed processor for graph reduction, </title> <booktitle> Proceedings of PARLE 91, </booktitle> <address> June 10-13, Veldhoven, </address> <publisher> Springer Verlag LNCS 505, </publisher> <pages> pp 119-136, </pages> <year> 1991. </year>
Reference: [Milikowski92] <author> R. Milikowski, </author> <title> A description of the G-hinge, </title> <type> Technical report CS-92-xx, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: During each experiment, Oyster is improved by adding or deleting features. Oyster will never be finished; due to new experiences gained while simulating architectures, Oyster will keep changing. The usefulness of Oyster has been tested in several case studies: for the simulation of a VLIW machine for graph reduction <ref> [Milikowski92, Gijsen92, Hendriksen90] </ref>, for the the simulated execution of functional programs on shared memory architectures [Langendoen92a, Hofman93], for a study towards the communication architecture of the PRISMA architecture and (Chapter 5), and for the simulation of hierarchical architectures based on the Futurebus (Chapter 6).
Reference: [Misra86] <author> Jayadev Misra, </author> <title> Distributed discrete-event simulation, </title> <journal> Computing surveys, </journal> <volume> Vol 18, No 1, </volume> <pages> pp 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: The second algorithm is the one used in all continuous time simulations, while the fourth algorithm is in use for all sequential discrete time simulations. Discrete simulations are sometimes parallelised by using the third algorithm, it has a distributed nature, but other algorithms can be parallelised as well <ref> [Overeinder91, Misra86] </ref>. The algorithms differ in three aspects: the way the time is used and represented, the way the delay is introduced, and the efficiency. Time representation In algorithm one, the time is an explicit parameter in the definition of a wire (q 5 or qb 350).
Reference: [Mooij89] <author> W.G.P. Mooij, </author> <title> Packet Switching Communication Networks for Multiprocessor Sys Bibliography 149 tems Ph.D. </title> <type> Thesis, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> December </month> <year> 1989. </year> <title> [Motorola85] mc68020 User's manual, ISBN 0-13-566860-3, </title> <publisher> Prentice Hall Inc, </publisher> <address> Englewood Cliffs, </address> <year> 1985. </year> <title> [Motorola88] mc88200 Cache/Memory management unit User's manual, </title> <institution> Motorola Inc, </institution> <year> 1988. </year>
Reference-contexts: The simulation environment The MULGA package ([Weste81], with a clocked simulator) has actually been interfaced with Oyster, and some simple experiments were performed with it. A communication processor design, of <ref> [Mooij89] </ref>, could be placed in a network of communication processors specified in Pearl, so the Pearl network could drive the communication processor. At that level it is rather easy to test many input patterns for the communication processor. <p> Three recent hardware designs in this field are the Torus Routing Chip (or TRC, [Dally86]), the Adaptive Routing Chip (the ARC, <ref> [Mooij89] </ref>), and the DOOM Communication processor (or CP, [Annot87]). These three communication processors have all been analysed, simulated and built. The analysis was made to prove the correctness of the network (deadlock freedom) or to analyse the networks performance behaviour (queuing models).
Reference: [Mulder87] <author> Johannes M. </author> <title> Mulder, Tradeoffs in Processor-Architecture and Data-Buffer Design, </title> <type> Ph.D. Thesis, Stanford Technical Report CSL-TR-87-345, </type> <month> December </month> <year> 1987. </year>
Reference-contexts: In real cases, the applications behaviour might depend on the speed of the processor's network. A situation that cannot be modelled using PARET (see Section 4.2 for a discussion on this topic). AWB AWB, the Architecture Work Bench <ref> [Bray89, Mulder87] </ref> developed at Stanford (where the MIPS processors originated as well), is used to evaluate the design of the storage hierarchy.
Reference: [Muller90] <author> H.L. Muller, </author> <title> Evaluation of a communication architecture by means of simulation, </title> <editor> in P. America (ed), </editor> <booktitle> Proceedings of the PRISMA workshop on parallel databas-esystems, Noordwijk, </booktitle> <address> The Netherlands, September 24-26, </address> <publisher> Springer Verlag LNCS 503, </publisher> <pages> pp 275-293, </pages> <year> 1991. </year>
Reference-contexts: The abstractions above packet level running outside the communication processor have not been simulated or analysed. It can be expected that both the ARC and the TRC will have y This chapter is based on <ref> [Muller90] </ref>. 86 Chapter 5.
Reference: [Muller92a] <author> H.L. Muller, K.G. Langendoen and L.O. Hertzberger, MiG: </author> <title> Simulating parallel functional programs on hierarchical cache architectures, </title> <type> Technical report CS-92-04, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The order in which locks are fetched or the order in which memory blocks are allocated, can result in a different schedule, or in a different memory allocation pattern, both leading to a (radically) different address trace. MiG (Memory simulation Integrated with Graph reduction <ref> [Muller92a] </ref>) is a simulator capable of tracing parallel applications on-line, thus with feedback from the generated trace towards the parallel program. <p> MiG is currently only implemented for a functional language, but there is no reason why it should not work for other parallel programs obeying the restrictions listed on page 71. Examples of the use of the MiG are in <ref> [Langendoen92a, Muller92a, Hofman93] </ref> 4.3 Stochastically generated address traces Both the extraction of address traces and the emulation of a program suffer from high computational and memory demands.
Reference: [Muller92b] <author> H.L. Muller and L.O. Hertzberger, </author> <title> Evaluating all regular topologies of hierarchical cache architectures based on the Futurebus, </title> <editor> in Cristoph Eck et al (eds), </editor> <booktitle> Proceedings of Open Bus Systems '92, </booktitle> <address> Z urich, </address> <month> October 13-15, </month> <title> VITA, </title> <journal> ISBN 90-72577-11-6, </journal> <pages> pp 193-199, </pages> <year> 1992. </year>
Reference-contexts: Since only 5% of the accesses are routed to the memory, more processors can be connected to the shared memory without running into contention problems. The current generation top y This chapter is based on <ref> [Langendoen91, Muller92b] </ref>. 100 Chapter 6. Simulating the Futurebus of the line minicomputers, for example from Data General and SUN, are shared memory machines with caches.
Reference: [Nichols88] <author> Kathleen M. Nichols and John T. Edmark, </author> <title> Modelling multi computer systems with PARET, </title> <booktitle> IEEE computer, </booktitle> <pages> pp 39-48, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Within this field, the designer can obtain good results (ISPS has been used in the design of the PDP 11), ISPS itself does not suffice to easily obtain information about, for example, the hit rate of a cache in the architecture. PARET The PARET environment <ref> [Nichols88] </ref> has been developed to predict the performance of parallel applications on distributed memory MIMD machines. The architecture and application are specified by means of directed graphs. The nodes of the architecture graph represents the processing elements of the architecture, the communication channels are represented by the arcs.
Reference: [Overeinder89] <author> B.J. Overeinder, </author> <title> De implementatie van een compiler voor de object geori en-teerde taal Pearl en de literatuur studie van load balance algorithmen voor parallelle simulaties, </title> <type> Master thesis, </type> <institution> Computer science department, University of Amster-dam, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: From the autumn of 1988 till the summer of 1989, a new version of the Pearl compiler was developed by two master degree students <ref> [Overeinder89, Stil89] </ref>, which has been upgraded since then to the version that is described in this thesis. Since that date various experiments have been performed with the Pearl compiler, the layers above it, and the kernel below.
Reference: [Overeinder91] <author> Benno Overeinder, Bob Hertzberger, and Peter Sloot, </author> <title> Parallel discrete-event simulation, </title> <booktitle> Third workshop on design and realisation of computer systems, Eindhoven, </booktitle> <pages> pp 19-30, </pages> <address> ISBN 90-6144-995-2, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The second algorithm is the one used in all continuous time simulations, while the fourth algorithm is in use for all sequential discrete time simulations. Discrete simulations are sometimes parallelised by using the third algorithm, it has a distributed nature, but other algorithms can be parallelised as well <ref> [Overeinder91, Misra86] </ref>. The algorithms differ in three aspects: the way the time is used and represented, the way the delay is introduced, and the efficiency. Time representation In algorithm one, the time is an explicit parameter in the definition of a wire (q 5 or qb 350).
Reference: [Press86] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky and William T. Vetterling, </author> <title> Numerical recipes, </title> <publisher> Cambridge University Press, </publisher> <address> ISBN 0-512-30811-9, </address> <year> 1986. </year>
Reference-contexts: The solid lines are the performance figures coming from the simulator (of Chapter 6), the dashed lines are the outcomes of the model of equation 7.10, with the parameters fitted with the Levenberg-Marquardt algorithm <ref> [Press86] </ref>. Figure 7.3 shows the curves for the parallel application.
Reference: [Przybylski90] <author> Steven A. Przybylski, </author> <title> Cache and memory hierarchy design: a performance directed approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, California, ISBN 1-55860-136-8, </address> <year> 1990. </year>
Reference-contexts: In 1 Note, in the sequel the `+' is omitted for readability. 2 Throughout this thesis the term line is used to denote the basic entity stored in the cache. The 6.1. Introduction to the Futurebus cache consistency 101 contrast with many other studies (for example <ref> [Przybylski90] </ref> and [Hennessy90]), the studies towards line size and associativity are performed in the context of hierarchical multiprocessor architectures, as is for example also the case in [Baer88]. Because of invalidates and sharing, the optimal values for these parameters for single and multi processor systems are not necessarily identical. <p> Possibly colliding lines between different level 1 caches usually fall into different level 0 sets because of the spatial locality in the applications. Higher associativity does decrease the miss-rate of the level 0 cache (as observed in Section 6.3.2, and in many other studies <ref> [Przybylski90] </ref>) because colliding direct mapped lines can coexist in an associative cache, but the overall performance is dictated by the miss rates of the level 1 caches. <p> part, the total size in use by each of the b 1 processors is therefore: S = sS 0 f + (1 s)S 0 f = S 0 f = S 0 s + b 1 (1 s) The relation between the cache size and miss rate is given in <ref> [Przybylski90, Agarwal89] </ref>.
Reference: [Raina91] <author> Sanjay Raina and David H.D. Warren, </author> <title> Traffic patterns in a scalable multiprocessor through Transputer emulation, </title> <type> Technical report 91-23, </type> <institution> Computer science department, University of Bristol, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Simulating applications interesting, but it should not slow down the rest of the machine by burdening the communication hardware). A better way to implement the P is to test the semaphore value until it is free, before executing a Test-And-Set (one could call this implementation a Test-And-Test-And-Set <ref> [Raina91, Rudolph84] </ref>). First a test is done on a shared cache line; only when it succeeds, the line is fetched exclusively, and a TAS is applied. If it fails, it repeats the first test.
Reference: [Rashid86] <author> Richard F. Rashid, </author> <title> Threads of a new system, </title> <booktitle> UNIX Review 8, </booktitle> <pages> pp 37-49, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Implementing pseudo parallel execution The pseudo parallel execution is implemented with the help of so called light weight processes under Sun-UNIX (originally called threads <ref> [Rashid86] </ref>). A light weight process is one thread of control in a normal UNIX process, with a private stack, program counter and register set, but with an address space that is shared 4.2. Application derived address traces 73 with the other threads in the UNIX process.
Reference: [Rubin87] <author> Steven M. Rubin, </author> <title> Computer Aids for VLSI Design, ISBN 0-201-05824-3, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference: [Rudolph84] <author> L. Rudolph and Z. Segall, </author> <title> Dynamic decentralised Cache Schemes for MIMD parallel processors, </title> <booktitle> Proceedings of the 11 th annual international symposium on computer architecture, </booktitle> <pages> pp 340-347, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Simulating applications interesting, but it should not slow down the rest of the machine by burdening the communication hardware). A better way to implement the P is to test the semaphore value until it is free, before executing a Test-And-Set (one could call this implementation a Test-And-Test-And-Set <ref> [Raina91, Rudolph84] </ref>). First a test is done on a shared cache line; only when it succeeds, the line is fetched exclusively, and a TAS is applied. If it fails, it repeats the first test.
Reference: [Sauer81] <author> Charles H. Sauer and K. Mani Chandy, </author> <title> Computer systems performance modeling, </title> <publisher> Prentice-Hall, </publisher> <address> ISBN 0-13-165175-7, </address> <year> 1981 </year>
Reference: [SCI92] <institution> Scalable Coherent Interface standard (SCI), IEEE 1596, IEEE Computer society, </institution> <address> New York, </address> <year> 1992. </year>
Reference-contexts: A straightforward implementation maps the P operation onto an indivisible Test-And-Set instruction: in one indivisible bus cycle, the semaphore value is fetched, tested and overwritten. In an architecture with distributed caches with a MOESI protocol [Sweazy86] (like the Futurebus [Futurebus89] or SCI based systems <ref> [SCI92] </ref>), the TAS-operation requires the cache line containing the semaphore to be fetched exclusively.
Reference: [Sijtsma89] <author> Ben A. Sijtsma, </author> <title> On the Productivity of Recursive List Definitions, </title> <journal> ACM transac 150 Bibliography tions on programming languages and systems, </journal> <volume> Vol 11, No 4, </volume> <pages> pp 633-649, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: In terms of the productivity theory of <ref> [Sijtsma89] </ref>, the function nand2 is +3-productive, so a (cyclic) network of nand2 functions is productive (which means that the network will keep producing elements as long as input is provided on the input lists). 18 Chapter 2. <p> The simulator will not deadlock because of the following invariant: the consumption of a tuple on an input stream, will always result in a new tuple on the output stream. By using the theory of <ref> [Sijtsma89] </ref> again, the function nand3 is +1-productive so the simulator will not deadlock. 20 Chapter 2. Simulating architectures The simulator generates correct output values as long as all streams are ordered on their time stamp. <p> Because bufferL produces an extra element before bufferH is called, buffer is productive (also in the formal sense, as can be proven with <ref> [Sijtsma89] </ref>). In the third algorithm, the function delay adds the constant 3 to the time.
Reference: [Smith82] <author> Alan Jay Smith, </author> <title> Cache Memories, </title> <journal> Computing Surveys, </journal> <volume> Vol. 14, No 3, </volume> <pages> pp 473-530, </pages> <year> 1982. </year>
Reference-contexts: An expensive switch (crossbar) has constant delay, but costs O (n 2 ). The problem of using a switch is that one either suffers from high (quadratic) costs, or high (logarithmic) delay. Another way to attack the contention on the shared memory is to use caching techniques. A cache <ref> [Smith82] </ref> is a fast and small memory that stores the data that is frequently used by the processor. Caches can service, say, 95% of the accesses to the memory directly, while the other 5% are passed to the slower memory to be answered after some delay.
Reference: [Spek90] <author> J. vd Spek, </author> <title> Back end aspects of the portable POOL implementation, </title> <editor> P. America (ed), </editor> <booktitle> Proceedings of the PRISMA workshop on parallel databasesystems, Noordwijk, </booktitle> <address> The Netherlands, September 24-26, </address> <publisher> Springer Verlag LNCS 503, </publisher> <pages> pp 309-344, </pages> <year> 1991. </year>
Reference-contexts: A running POOL program consists of objects that communicate and synchronise by sending messages to each other. In general, neither the destination of a message, nor the size are known at compile time. Several POOL implementations have been made <ref> [Spek90, Beemster90] </ref>. For the experiments on the communication architecture, the latter one has been used. It translates POOL into C and uses a C-compiler to generate the assembly code for a specific processor. The run time support for the compiled code is entirely written in C as well.
Reference: [Stallman88] <author> R.M. Stallman, </author> <title> Using and Porting the GNU C-compiler, Free Software Foundation Inc, </title> <address> Massachusetts, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: The instruction set is further enriched with a readtimer and printtimer to get extra timing information from the simulator. As program for measuring the latency, the POOL program sketched in Figure 5.3 (page 88) is used. The POOL compiler translates the POOL program into C, and the GNU C-compiler <ref> [Stallman88] </ref> is used to translate the C code and the POOL run time support into the assembly code for the processor. The GNU compiler is used because of its portability.
Reference: [Stenstrom90] <author> Per Stenstrom, </author> <title> A survey of cache coherence schemes for multiprocessors, </title> <journal> IEEE Computer (special issue on cache coherency), </journal> <pages> pp 12-24, </pages> <month> June </month> <year> 1990. </year>
Reference: [Stil89] <author> J.G. Stil, </author> <title> De implementatie van een compiler voor de simulatietaal Pearl en een studie naar synchronisatiemechanismen voor gedistribueerde simulatie, </title> <type> Master thesis, </type> <institution> Computer science department, University of Amsterdam, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: From the autumn of 1988 till the summer of 1989, a new version of the Pearl compiler was developed by two master degree students <ref> [Overeinder89, Stil89] </ref>, which has been upgraded since then to the version that is described in this thesis. Since that date various experiments have been performed with the Pearl compiler, the layers above it, and the kernel below.
Reference: [Stroustrup87] <author> Bjarne Stroustrup, </author> <title> The C++ programming language, </title> <publisher> Addison Wesley, </publisher> <address> ISBN 0-201-12078-X, </address> <year> 1987 </year> . 
Reference: [Sweazy86] <author> Paul Sweazy and Alan Jay Smith, </author> <title> A class of compatible cache consistency protocols and their support by the IEEE Futurebus, </title> <booktitle> Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp 414-423, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: A straightforward implementation maps the P operation onto an indivisible Test-And-Set instruction: in one indivisible bus cycle, the semaphore value is fetched, tested and overwritten. In an architecture with distributed caches with a MOESI protocol <ref> [Sweazy86] </ref> (like the Futurebus [Futurebus89] or SCI based systems [SCI92]), the TAS-operation requires the cache line containing the semaphore to be fetched exclusively. <p> The section ends with an overview of the aspects that are not simulated but which are actually part of the Futurebus definition. 6.1.1 Consistency in flat architectures The Futurebus cache consistency protocol is MOESI like <ref> [Sweazy86] </ref>. Each cached line of data is in one of the following three states: Exclusive, Shared or Invalid.
Reference: [Szymanski85] <author> Thomas G. Szymanski and Christopher J. Van Wyk, Goalie: </author> <title> A Space Efficient System for VLSI Artwork Analysis, </title> <booktitle> IEEE Design and Test, </booktitle> <pages> pp 64-72, </pages> <month> June </month> <year> 1985. </year>
Reference: [Thiebaut92] <author> Dominique Thiebaut, Joel L. Wolf, Harold S. Stone, </author> <title> Synthetic traces for trace-drive simulation of cache memories, </title> <journal> IEEE Transactions on computers, </journal> <volume> vol 41, no 4, </volume> <pages> pp 388-410, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This drawback should be obviated by a systematical validation, and by experimentation with the sensitivity of the parameters. The use of a non-validated, non-calibrated synthetic address trace makes no sense. A synthetic address trace may arbitrarily abstract from a real world address trace. <ref> [Thiebaut92] </ref> for example observed that the curve plotting the number of unique words versus the number of allocated words has two knees.
Reference: [Turner90] <author> Miranda System Manual, </author> <title> Research software Limited, </title> <address> 23 St Augustines Road, Canterbury, Kent CT1 1XP, England, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: introduction to functional programming, and to <ref> [Turner90] </ref> for the definition of Mi-randa.
Reference: [Verilog89] <author> Verilog reference manual, </author> <title> Gateway design automation corporation, </title> <type> Lowell, </type> <institution> Massachusetts, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: These simulators show the most interesting aspects of hardware simulation languages. VHDL A number of popular tools exist that are used for the specification of hardware: Dacapo [Dacapo89] and VHDL [IEEE88] (based on Pascal), or Verilog <ref> [Verilog89] </ref> (based on C and Pascal). Although the syntax and details of these languages differ, the (imperative) base language was in all cases extended with the same constructs: wires and the notion of time.
Reference: [Vernon89] <author> Mary K. Vernon, Rajeev Jog and Gurindar S. Sohi, </author> <title> Performance Analysis of Hierarchical Cache-Consistent Multiprocessors, Performance evaluation 9, </title> <booktitle> 4, </booktitle> <pages> pp 287-302, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: The graphs from our simulations have some similarities with the graphs given in <ref> [Vernon89] </ref>, despite a radically different method is used. We use simulation, while [Vernon89] developed an analytical model of regular hierarchical cache architectures: given a set of detailed parameters, as the cache hit rate and the degree of sharing at the various levels, this model calculates the expected performance. <p> The graphs from our simulations have some similarities with the graphs given in <ref> [Vernon89] </ref>, despite a radically different method is used. We use simulation, while [Vernon89] developed an analytical model of regular hierarchical cache architectures: given a set of detailed parameters, as the cache hit rate and the degree of sharing at the various levels, this model calculates the expected performance. <p> The most notably similarities between the results are (of course) the saturation effects but also the preference for long level 1 busses. But since the total cache sizes are kept constant during the experiments, our figures for higher level architectures have a lower performance than those reported in <ref> [Vernon89] </ref>. In the next chapter it is shown how a less general, but much simpler, performance model for regular hierarchical cache architectures is constructed. It predicts the optimums in the performance graphs (the places where the performance stabilises), given the simulation results of some small architectures. <p> Based on the performance figures of a few topologies, the parameters of the model can be derived, that allows the performance model to calculate the performance figures of other topologies. <ref> [Vernon89] </ref> developed a model for hierarchical cache architectures that are structured in a regular tree. Given the miss rates at the various levels of the hierarchy, the timings of the various transactions of the hierarchy, and the branching factors of the hierarchy, the model described in [Vernon89] is capable of predicting <p> figures of other topologies. <ref> [Vernon89] </ref> developed a model for hierarchical cache architectures that are structured in a regular tree. Given the miss rates at the various levels of the hierarchy, the timings of the various transactions of the hierarchy, and the branching factors of the hierarchy, the model described in [Vernon89] is capable of predicting the performance of the architecture based on the calculated bus contentions. A weak point of the model is the level of detail of the parameter specification: the hit rates of the caches at the various levels of the architecture are an input to the model. <p> Unfortunately, the relation between these components and the topology of the architecture is not clear (as is shown later on, there is a relation between the miss rate and the topology). [J og90] studies a flat system with caches based on the model of <ref> [Vernon89] </ref>, but has extended it with other features, like the inclusion of I/O effects. [J og90] applied the model to the flat architecture after measuring many parameters on a real architecture (with hardware analysers) and after measuring some others with the help of a simulator. <p> The model needs a description of the cache (the associativity, size and so on), and a specification of the application behaviour (in terms of the number of uniquely referenced blocks, and the access rate) to calculate the miss rate. It would be nice if the models of <ref> [Vernon89] </ref> and [Agarwal89] could be combined so that a performance figure is predicted based on the parameters of the architecture, the cache, and the application. This combination is not feasible because of 124 Chapter 7. <p> Secondly, an average value was used for the bus speed parameters, B 0 , B 1 and B 2 . Unfortunately the bus transaction time depends on the type of the transaction (misses are handled faster than hits as is correctly modelled by for example <ref> [Vernon89] </ref>, because no data has to be transferred), and the simulator implements an optimisation that allows for multiple transactions issued by the same cache to be handled within one arbitration phase.
Reference: [Vlot90] <author> M. Vlot, </author> <title> The POOMA architecture, </title> <editor> P. America (ed), </editor> <booktitle> Proceedings of the PRISMA workshop on parallel databasesystems, Noordwijk, </booktitle> <address> The Netherlands, September 24-26, </address> <publisher> Springer Verlag LNCS 503, </publisher> <pages> pp 365-395, </pages> <year> 1991. </year>
Reference-contexts: Three variants of the PRISMA architecture are simulated in order to evaluate the communication performance. Section 5.1 presents a brief description of the PRISMA architecture, and shows where the bottleneck in the current implementation is (see <ref> [Vlot90, Apers90] </ref> and [Bronnenberg87] for a concise description of the architecture). The architecture has been modelled using Oyster, as is described in Section 5.2.
Reference: [Vree89] <author> W.G. Vree, </author> <title> Design considerations for a parallel reduction machine, </title> <type> Ph.D. thesis, </type> <institution> University of Amsterdam, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Both methods are shown in the next sections. 2.1.2 Continuous time simulation A simulation of the Flip-Flop with an implicit continuous time increment is described in <ref> [Vree89] </ref>. The wires are represented by infinite lists (streams) of states. The i th element of such a list represents the value of the wire at time iffi t , where ffi t is the time increment. <p> This type of simulator is used in simulations where the state would change continuously in time, but where the state is necessarily modelled with small discrete steps. An example given in <ref> [Vree89] </ref> is the simulation of water heights; electrical currents or the positions of planets in a solar system can be simulated with the same technique.
Reference: [Wallace88] <author> D.E. Wallace and C.H. Sequin, ATV: </author> <title> an abstract timing verifier, </title> <booktitle> 25 th ACM/IEEE design automation conference, </booktitle> <year> 1988. </year>
Reference-contexts: Call graph analysis The call graph analysis is the most sophisticated analysis currently in Oyster. It has close resemblance with the call graph analysers found in conventional software environments (like gprof under UNIX [Graham82]), and the critical path analysis found in electrical simulators (as for example <ref> [Wallace88] </ref>), the call graph analyser is used to find the critical paths in the architecture. This is implemented by analysing each idle period of an object. At the end of an idle period, the Pearl kernel traces back along which path in the call graph, the object was awoken.
Reference: [Warren88] <author> David H.D. Warren, Seif Haridi, </author> <title> Data Diffusion Machine A scalable shared virtual memory multiprocessor, </title> <booktitle> Proceedings of the international conference on fifth generation computer systems 1988, ICOT, </booktitle> <pages> pp 943-952, </pages> <year> 1988. </year>
Reference-contexts: There are more alternatives, see for example [Stenstr om90] for a survey on this topic. Combining broadcasts and directories, caching shared memory architectures can be constructed that use multiple (broadcasting) bus segments with directories in between, as is for example implemented in the Data Diffusion Machine <ref> [Warren88] </ref>. The Futurebus+ 1 [Futurebus89] defines an industry standard bus that supports this type of hierarchical architectures. In this chapter, a study on the performance of hierarchical cache architectures that can be built on top of the Futurebus is presented.
Reference: [Weste81] <author> N. Weste, </author> <title> MULGA- An interactive symbolic layout system for the design of integrated circuits, </title> <journal> The Bell System Technical Journal, </journal> <volume> Vol 60, No 6, </volume> <year> 1981. </year>

References-found: 101

