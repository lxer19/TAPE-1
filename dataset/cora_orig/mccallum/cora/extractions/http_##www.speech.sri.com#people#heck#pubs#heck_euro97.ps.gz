URL: http://www.speech.sri.com/people/heck/pubs/heck_euro97.ps.gz
Refering-URL: http://www.speech.sri.com/people/heck/pubs.html
Root-URL: 
Email: fheck,sankarg@speech.sri.com  
Title: ACOUSTIC CLUSTERING AND ADAPTATION FOR ROBUST SPEECH RECOGNITION  
Author: Larry Heck and Ananth Sankar 
Address: Menlo Park, CA  
Affiliation: Speech Technology And Research Laboratory SRI International  
Abstract: We describe an algorithm based on acoustic clustering and acoustic adaptation to significantly improve speech recognition performance. The method is particularly useful when speech from multiple speakers is to be recognized and the boundary between speakers is not known. We assume that each test data segment is relatively homogeneous with respect to the acoustic background and speaker. These segments are then grouped using an agglomerative acoustic clustering algorithm. The idea is to group together all test segments that are acoustically similar. The speech recognition models are then adapted separately to each test data cluster. Finally these adapted models are used to recognize the data from that cluster. This algorithm was used in SRI's system for the 1996 DARPA Hub4 partitioned evaluation. Experimental results are presented on the 1996 H4 development data set. It was found that an improvement of 9.5% was achieved by using this algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Digalakis, D. Rtischev, and L. Neumeyer, </author> <title> Speaker Adaptation Using Constrained Reestima-tion of Gaussian Mixtures, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 357 366, </pages> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION Recently there has been much research on acoustic adaptation <ref> [1, 2, 3, 4, 5] </ref> to improve the performance of speech recognition systems in mismatched training and testing acoustic environments. Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. <p> Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. Common approaches include maximum-likelihood (ML) transformation-based techniques <ref> [1, 2, 3] </ref>, and Bayesian algorithms [5]. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment. <p> Common approaches include maximum-likelihood (ML) transformation-based techniques <ref> [1, 2, 3] </ref>, and Bayesian algorithms [5]. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment. However, in some situations, this may be difficult to get. <p> ML transformation-based adaptation was used to adapt a seed Genonic HMM to each of the acoustic focus conditions. A parametric transformation of the HMMs is postulated, and the parameters of the transformation are estimated by maximizing the likelihood of the training data from the acoustic focus condition <ref> [1, 2, 3] </ref>. We used a block-diagonal affine matrix transformation of the HMM mean vectors in this stage [4]. This is a modification of the algo-rithm presented in [2] that results in more estimation of the adaptation transformations.
Reference: [2] <author> C. J. Legetter and P. C. Woodland, </author> <title> Flexible Speaker Adaptation Using Maximum Likelihood Linear Regression, </title> <booktitle> in Proceedings of the Spoken Language Systems Technology Workshop, </booktitle> <pages> pp. 110115, </pages> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION Recently there has been much research on acoustic adaptation <ref> [1, 2, 3, 4, 5] </ref> to improve the performance of speech recognition systems in mismatched training and testing acoustic environments. Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. <p> Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. Common approaches include maximum-likelihood (ML) transformation-based techniques <ref> [1, 2, 3] </ref>, and Bayesian algorithms [5]. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment. <p> Common approaches include maximum-likelihood (ML) transformation-based techniques <ref> [1, 2, 3] </ref>, and Bayesian algorithms [5]. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment. However, in some situations, this may be difficult to get. <p> ML transformation-based adaptation was used to adapt a seed Genonic HMM to each of the acoustic focus conditions. A parametric transformation of the HMMs is postulated, and the parameters of the transformation are estimated by maximizing the likelihood of the training data from the acoustic focus condition <ref> [1, 2, 3] </ref>. We used a block-diagonal affine matrix transformation of the HMM mean vectors in this stage [4]. This is a modification of the algo-rithm presented in [2] that results in more estimation of the adaptation transformations. <p> We used a block-diagonal affine matrix transformation of the HMM mean vectors in this stage [4]. This is a modification of the algo-rithm presented in <ref> [2] </ref> that results in more estimation of the adaptation transformations. To approximate more complex transforms, we used multiple block-diagonal affine transformations, where each transformation is tied to a group of Gaussians.
Reference: [3] <author> A. Sankar and C.-H. Lee, </author> <title> A Maximum-Likelihood Approach to Stochastic Matching for Robust Speech Recognition, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 4, </volume> <pages> pp. </pages> <address> 190202, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: 1. INTRODUCTION Recently there has been much research on acoustic adaptation <ref> [1, 2, 3, 4, 5] </ref> to improve the performance of speech recognition systems in mismatched training and testing acoustic environments. Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. <p> Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. Common approaches include maximum-likelihood (ML) transformation-based techniques <ref> [1, 2, 3] </ref>, and Bayesian algorithms [5]. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment. <p> Common approaches include maximum-likelihood (ML) transformation-based techniques <ref> [1, 2, 3] </ref>, and Bayesian algorithms [5]. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment. However, in some situations, this may be difficult to get. <p> ML transformation-based adaptation was used to adapt a seed Genonic HMM to each of the acoustic focus conditions. A parametric transformation of the HMMs is postulated, and the parameters of the transformation are estimated by maximizing the likelihood of the training data from the acoustic focus condition <ref> [1, 2, 3] </ref>. We used a block-diagonal affine matrix transformation of the HMM mean vectors in this stage [4]. This is a modification of the algo-rithm presented in [2] that results in more estimation of the adaptation transformations. <p> Since acoustic segments of the same speaker are similar, the resulting clusters are typically homogeneous with respect to speakers. Once the segments are clustered, the condition-specific models are separately adapted to each cluster by using the block-diagonal mean transformation [4], followed by a variance scaling transformation described in <ref> [3, 4] </ref>. The variance scaling transform has also been studied more recently in [9]. In this stage we used three separate transformations, including a separate transformation for the silence Gaussians. <p> Performance of test-condition-adaptation adaptation we tried both a block-diagonal affine transformation of the HMM means, and the mean transformation followed by a variance scaling transformation <ref> [3, 4] </ref>. From the table, we see that the acoustic clustering and adaptation algorithm gives a significant improvement over the baseline condition-specific models for every focus condition.
Reference: [4] <author> L. Neumeyer, A. Sankar, and V. Digalakis, </author> <title> A Comparative Study of Speaker Adaptation Techniques, </title> <booktitle> in Proceedings of EUROSPEECH, </booktitle> <pages> pp. 11271130, </pages> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION Recently there has been much research on acoustic adaptation <ref> [1, 2, 3, 4, 5] </ref> to improve the performance of speech recognition systems in mismatched training and testing acoustic environments. Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. <p> A parametric transformation of the HMMs is postulated, and the parameters of the transformation are estimated by maximizing the likelihood of the training data from the acoustic focus condition [1, 2, 3]. We used a block-diagonal affine matrix transformation of the HMM mean vectors in this stage <ref> [4] </ref>. This is a modification of the algo-rithm presented in [2] that results in more estimation of the adaptation transformations. To approximate more complex transforms, we used multiple block-diagonal affine transformations, where each transformation is tied to a group of Gaussians. <p> Since acoustic segments of the same speaker are similar, the resulting clusters are typically homogeneous with respect to speakers. Once the segments are clustered, the condition-specific models are separately adapted to each cluster by using the block-diagonal mean transformation <ref> [4] </ref>, followed by a variance scaling transformation described in [3, 4]. The variance scaling transform has also been studied more recently in [9]. In this stage we used three separate transformations, including a separate transformation for the silence Gaussians. <p> Since acoustic segments of the same speaker are similar, the resulting clusters are typically homogeneous with respect to speakers. Once the segments are clustered, the condition-specific models are separately adapted to each cluster by using the block-diagonal mean transformation [4], followed by a variance scaling transformation described in <ref> [3, 4] </ref>. The variance scaling transform has also been studied more recently in [9]. In this stage we used three separate transformations, including a separate transformation for the silence Gaussians. <p> Performance of test-condition-adaptation adaptation we tried both a block-diagonal affine transformation of the HMM means, and the mean transformation followed by a variance scaling transformation <ref> [3, 4] </ref>. From the table, we see that the acoustic clustering and adaptation algorithm gives a significant improvement over the baseline condition-specific models for every focus condition.
Reference: [5] <author> J. Gauvain and C.-H. Lee, </author> <title> Maximum a posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 2, </volume> <pages> pp. 291 298, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1. INTRODUCTION Recently there has been much research on acoustic adaptation <ref> [1, 2, 3, 4, 5] </ref> to improve the performance of speech recognition systems in mismatched training and testing acoustic environments. Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. <p> Adaptation techniques typically adapt a model trained under one condition to the test environment using a small amount of data from the test environment. Common approaches include maximum-likelihood (ML) transformation-based techniques [1, 2, 3], and Bayesian algorithms <ref> [5] </ref>. These approaches have been shown to give significant improvements in performance for cases such as mismatched speakers, and channels [1, 2, 3]. To use these techniques it is necessary to have a sufficient amount of adaptation data from the test environment.
Reference: [6] <author> R. Stern, </author> <title> Specification of the 1996 Hub4 Broadcast News Evaluation, </title> <booktitle> in Proceedings of the DARPA Speech Recognition Workshop (Chantilly, </booktitle> <address> VA), </address> <year> 1997. </year>
Reference-contexts: Each segment contained speech from a single speaker. In addition, the segments were homogeneous with respect to the acoustic background condition or speech style. The segments were classified into seven different acoustic focus conditions, F0, F1, F2, F3, F4, F5, FX, as described in <ref> [6] </ref>, and the labels were provided for use in the evaluation. The experiments reported in this paper use the PE development data.
Reference: [7] <author> V. Digalakis, P. Monaco, and H. Murveit, Genones: </author> <title> Generalized Mixture Tying in Continuous Hidden Markov Model-Based Speech Recognizers, </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 281289, </pages> <year> 1996. </year>
Reference-contexts: The experiments reported in this paper use the PE development data. Our baseline system was a gender-dependent Genonic hidden Markov model (HMM) system <ref> [7] </ref> adapted to each acoustic focus condition by using the training data provided for that focus condition. This approach results in condition-specific HMMs that can be used to recognize speech from the corresponding focus condition. <p> If the number of transforms is greater than the number of phones, then the transforms are tied to Gaussian groups generated using the HMM state clustering algorithm used to train our Genonic HMMs <ref> [7] </ref>. For the condition-specific models, we used 11 block-diagonal affine transforms of the HMM mean vectors. The number of transforms was optimized to give the lowest word error rate on the 1996 Hub4 PE development data.
Reference: [8] <author> A. Sankar, L. Heck, and A. Stolcke, </author> <title> Acoustic Modeling for the SRI Hub4 Partitioned Evaluation Continuous Speech Recognition System, </title> <booktitle> in Proceedings of the 1997 DARPA Speech Recognition Workshop (Chantilly, </booktitle> <address> VA), </address> <year> 1997. </year>
Reference-contexts: Since the F2 condition corresponds to telephone speech, we decided to use seed models trained on the Switchboard and Macrophone databases for F2. For all other focus conditions, we used seed models trained on the Wall Street Journal (WSJ) SI-284 database. Details of this approach are presented in <ref> [8] </ref>. 3. TEST DATA CLUSTERING AND ADAPTATION The condition-specific models described in Section 2 are estimated using adaptation algorithms and the training data for each focus condition. However, there may still be a mismatch between these condition-specific models and test data from the same acoustic condition. <p> Since some of the test segments were very long, we further segmented these into nominally 10-second segments so as to reduce the memory and computation requirements on the decoding process. This was done by using an automatic segmentation algorithm described in <ref> [8] </ref>. The front-end feature extraction was based on mel-frequency cepstrum processing. The original speech data was sampled at 16,000 samples per second. For the F2 (telephone) segments, the speech was band limited, and down-sampled to 8,000 samples per second.
Reference: [9] <author> M. J. F. Gales and P. C. Woodland, </author> <title> Mean and Variance Adaptation within the MLLR Framework, </title> <booktitle> Computer Speech and Language, </booktitle> <volume> vol. 10, </volume> <pages> pp. 249 264, </pages> <year> 1996. </year>
Reference-contexts: Once the segments are clustered, the condition-specific models are separately adapted to each cluster by using the block-diagonal mean transformation [4], followed by a variance scaling transformation described in [3, 4]. The variance scaling transform has also been studied more recently in <ref> [9] </ref>. In this stage we used three separate transformations, including a separate transformation for the silence Gaussians. The reference transcriptions for adaptation were derived by running a one-pass Viterbi recognition search through word lattices [10] using the condition-specific models described in Section 2.
Reference: [10] <author> H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub, </author> <title> Large-Vocabulary Dictation Using SRI's DECIPHER(TM) Speech Recognition System: </title> <booktitle> Progressive-Search Techniques, in Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. </pages> <address> II319II322, </address> <year> 1993. </year>
Reference-contexts: The variance scaling transform has also been studied more recently in [9]. In this stage we used three separate transformations, including a separate transformation for the silence Gaussians. The reference transcriptions for adaptation were derived by running a one-pass Viterbi recognition search through word lattices <ref> [10] </ref> using the condition-specific models described in Section 2. Once the models are adapted, it is possible to re-recognize the acoustic segments for each cluster and then re-adapt the models by using these new hypotheses.
Reference: [11] <author> A. Sankar, F. Beaufays, and V. Digalakis, </author> <title> Training Data Clustering for Improved Speech Recognition, </title> <booktitle> in Proceedings of EUROSPEECH, </booktitle> <year> 1995. </year>
Reference-contexts: For our system, the distance threshold was empirically determined. By examining the clusters on the 1996 H4 development set, we found that the clusters were indeed quite homogeneous with respect to speakers. This clustering procedure was previously described by us in <ref> [11] </ref>, but applied to cluster the training data speakers. In the work reported here, we used it to cluster the test data segments.
References-found: 11

