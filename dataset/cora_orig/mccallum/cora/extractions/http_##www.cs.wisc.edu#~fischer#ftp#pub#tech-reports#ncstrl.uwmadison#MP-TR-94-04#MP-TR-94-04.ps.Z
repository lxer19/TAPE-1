URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-94-04/MP-TR-94-04.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-94-04/
Root-URL: http://www.cs.wisc.edu
Title: MODIFIED PROJECTION-TYPE METHODS FOR MONOTONE VARIATIONAL INEQUALITIES  
Author: M. V. Solodovy and P. Tsengz 
Keyword: KEY WORDS. Monotone variational inequalities, projection-type methods, error bound, linear convergence.  
Abstract: Mathematical Programming Technical Report # 94-04 May 24, 1994 (revised January 11, 1995) (to appear in SIAM Journal on Control and Optimization) ABSTRACT We propose new methods for solving the variational inequality problem where the underlying function F is monotone. These methods may be viewed as projection-type methods in which the projection direction is modified by a strongly monotone mapping of the form I ffF or, if F is affine with underlying matrix M, of the form I + ffM T , with ff 2 (0; 1). We show that these methods are globally convergent and, if in addition a certain error bound based on the natural residual holds locally, the convergence is linear. Computational experience with the new methods is also reported. fl The first author is supported by Air Force Office of Scientific Research Grant F49620-94-1-0036 and National Science Foundation Grant CCR-9101801. The second author is supported by National Science Foundation Grant CCR-9311621. y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madi-son, WI 53706, U.S.A. Email: solodov@cs.wisc.edu. z Department of Mathematics, University of Washington, Seattle, WA 98195, U.S.A. Email: tseng@math.washington.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. A. Auslender. </author> <title> Optimisation Methodes Numeriques. </title> <publisher> Masson, </publisher> <address> Paris, </address> <year> 1976. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see <ref> [1, 6, 15] </ref>) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see [7, 34]). Many methods have been proposed to solve VI (X; F ). <p> Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [44] (also see <ref> [1, 2, 3, 8, 26] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> The first six test problems were randomly generated, with the entries of c uniformly generated from <ref> [1; 100] </ref>, with the number of nonzeros per column of A fixed at 5% and the nonzeros uniformly generated from [5; 5], and with b = Ax, where x = (10=l; :::; 10=l). The seventh to ninth test problems were taken from the Netlib library (see [14]).
Reference: [2] <author> A.B. Bakusinskii and B.T. Polyak. </author> <title> On the solution of variational inequalities. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 17 </volume> <pages> 1705-1710, </pages> <year> 1974. </year>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [44] (also see <ref> [1, 2, 3, 8, 26] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize.
Reference: [3] <author> D.P. Bertsekas and E.M. Gafni. </author> <title> Projection methods for variational inequalities with application to the traffic assignment problem. </title> <journal> Mathematical Programming Study, </journal> <volume> 17 </volume> <pages> 139-159, </pages> <year> 1982. </year>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [44] (also see <ref> [1, 2, 3, 8, 26] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 36, 43] </ref>. Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence.
Reference: [4] <author> J.F. Bonnans. </author> <title> Local analysis of newton-type methods for variational inequalities and nonlinear programming. </title> <journal> Applied Mathematics and Optimization, </journal> <volume> 29 </volume> <pages> 161-186, </pages> <year> 1994. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [5] <author> H.-G. Chen. </author> <title> Forward-backward splitting techniques: theory and applications. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <address> Seattle, Washington, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: And even on structured problems such as the discrete-time deterministic optimal control problem [43], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 45, 50] </ref> while its iterations are simpler. <p> The first six test problems were randomly generated, with the entries of c uniformly generated from [1; 100], with the number of nonzeros per column of A fixed at 5% and the nonzeros uniformly generated from <ref> [5; 5] </ref>, and with b = Ax, where x = (10=l; :::; 10=l). The seventh to ninth test problems were taken from the Netlib library (see [14]). <p> The first three (respectively, fourth to sixth) test problems were randomly generated with M = !EE T + E E T ; where ! = 0 (respectively, ! = 1) and every entry of the n fi n matrix E was uniformly generated from <ref> [5; 5] </ref>, and with q = M x+ y, where each entry of x has equal probability of being 0 or being uniformly generated from [5; 10] and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or <p> E T ; where ! = 0 (respectively, ! = 1) and every entry of the n fi n matrix E was uniformly generated from [5; 5], and with q = M x+ y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution). <p> = M x+ y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution).
Reference: [6] <editor> R.W. Cottle, F. Giannessi, and J.-L. Lions (editors). </editor> <title> Variational Inequalities and Complementarity Problems : Theory and Applications. </title> <address> Whiley, New York, </address> <year> 1980. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see <ref> [1, 6, 15] </ref>) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see [7, 34]). Many methods have been proposed to solve VI (X; F ).
Reference: [7] <author> R.W. Cottle, J.-S. Pang, and R.E. Stone. </author> <title> The Linear Complementarity Problem. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see [1, 6, 15]) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see <ref> [7, 34] </ref>). Many methods have been proposed to solve VI (X; F ).
Reference: [8] <author> S.C. Dafermos. </author> <title> An iterative scheme for variational inequalities. </title> <journal> Mathematical Programming, </journal> <volume> 26 </volume> <pages> 40-47, </pages> <year> 1983. </year>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [44] (also see <ref> [1, 2, 3, 8, 26] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [9] <author> S.C. Dafermos and S.C. McKelvey. </author> <title> Partitionable variational inequalities with applications to network and economic equilibria. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 73 </volume> <pages> 243-268, </pages> <year> 1992. </year>
Reference-contexts: ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 36, 43] </ref>. Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence.
Reference: [10] <author> J. Eckstein and M.C. Ferris. </author> <title> Operator splitting methods for monotone affine aria-tional inequalities. </title> <type> Technical report, </type> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, Mas-sachusetts, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: 39, 46, 49] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration 1 than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. <p> And even on structured problems such as the discrete-time deterministic optimal control problem [43], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 45, 50] </ref> while its iterations are simpler. <p> E T ; where ! = 0 (respectively, ! = 1) and every entry of the n fi n matrix E was uniformly generated from [5; 5], and with q = M x+ y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution). <p> = M x+ y, where each entry of x has equal probability of being 0 or being uniformly generated from <ref> [5; 10] </ref> and each entry of y is 0 if the corresponding entry of x is 0 and otherwise has equal probability of being 0 or being uniformly generated from [5; 10] (so x is a solution).
Reference: [11] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Error bounds and strong upper semicontinuity for monotone affine variational inequalities. </title> <journal> Annals of Operations Research, </journal> <volume> 47 </volume> <pages> 293-305, </pages> <year> 1993. </year>
Reference-contexts: Then any sequence fx i g generated by Algorithm 2.2 converges to an element of S and, if (1.3) holds for some and ffi, the convergence is R-linear. We note that Algorithm 2.2 is closely related to the following iterative method proposed in <ref> [11] </ref> x2X i (x) := (x x i ) T (M x + q) + 2 where is a positive scalar. <p> We note that in <ref> [11] </ref> no convergence result is given for (2.11). Theorem 2.2 shows that if the step (2.8) is added, the resulting method (2.8)-(2.10) converges to a solution of VI (X; F ) and, if (1.3) holds (as in the case where X is also polyhedral), the convergence is R-linear.
Reference: [12] <author> M. Fukushima. </author> <title> Equivalent differentiable optimization problems and descent methods for asymmetric variational inequality problems. </title> <journal> Mathematical Programming, </journal> <volume> 53 </volume> <pages> 99-110, </pages> <year> 1992. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [13] <author> E.M. Gafni and D.P. Bertsekas. </author> <title> Two-metric projection methods for constrained optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 </volume> <pages> 936-964, </pages> <year> 1984. </year>
Reference-contexts: i z i ffF (x i ) + ffF (z i ))k 1 kx i z i k 2 (1 ff)kP 1=2 k 1 (1 + ffL) 1 minf1; ffgkr (x i )k for all i, where L denotes the Lipschitz constant of F and the last inequality follows from <ref> [13, Lemma 1] </ref>. Thus, the rightmost term in (3.6) is bounded above by a positive constant times kr (x i )k 2 and, whenever this term converges R-linearly to zero as i ! 1, so does kx i+1 x i k 2 P ; hence fx i g converges R-linearly. <p> [x i ffF (x i )] + ) T (F (x i ) F (z i (ff))) ff 2 (1 )kr (x i )k 2 where the first inequality uses the Cauchy-Schwartz inequality and the nonexpansive property of [] + ; the last inequality uses ff 2 (0; 1] and <ref> [13, Lemma 1] </ref>. Thus, the claim holds. To show that fx i g converges to an element of S, let x fl be any element of S.
Reference: [14] <author> D.M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> COAL Newsletter, </journal> <volume> 13 </volume> <pages> 10-12, </pages> <year> 1985. </year>
Reference-contexts: The seventh to ninth test problems were taken from the Netlib library (see <ref> [14] </ref>).
Reference: [15] <author> R. Glowinski, J.-L. Lions, and R. Tremolieres. </author> <title> Numerical Analysis of Variational Inequalities. </title> <publisher> North-Holland, </publisher> <address> Amstardam, </address> <year> 1981. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see <ref> [1, 6, 15] </ref>) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see [7, 34]). Many methods have been proposed to solve VI (X; F ).
Reference: [16] <author> P.T. Harker and J.-S. Pang. </author> <title> A damped-Newton method for the linear complementarity problem. </title> <editor> In G. Allgower and K. Georg, editors, </editor> <title> Computational Solution of Nonlinear Systems of Equations. </title> <booktitle> Lectures in Applied Mathematics 26, </booktitle> <pages> pages 265-284, </pages> <year> 1990. </year>
Reference-contexts: The remaining test problems were borrowed from <ref> [16, Sec. 5] </ref>.
Reference: [17] <author> B. </author> <title> He. A new method for a class of linear variational inequalities. </title> <journal> Mathematical Programming, </journal> <volume> 66 </volume> <pages> 137-144, </pages> <year> 1994. </year>
Reference-contexts: In Section 4, we report our preliminary computational experience with the new methods on sparse LPs, dense monotone LCPs, and linearly constrained variational inequality problems. In Section 5, we give some concluding remarks. Subsequent to the writing of this paper, we learned of the recently proposed methods of He <ref> [17, 18] </ref> which may be viewed as special cases of Algorithm 2.1 in Section 2, with specific choices of the scaling matrix P .
Reference: [18] <author> B. </author> <title> He. Solving a class of linear projection equations. </title> <journal> Numerische Mathematik, </journal> <volume> 68 </volume> <pages> 71-80, </pages> <year> 1994. </year>
Reference-contexts: In Section 4, we report our preliminary computational experience with the new methods on sparse LPs, dense monotone LCPs, and linearly constrained variational inequality problems. In Section 5, we give some concluding remarks. Subsequent to the writing of this paper, we learned of the recently proposed methods of He <ref> [17, 18] </ref> which may be viewed as special cases of Algorithm 2.1 in Section 2, with specific choices of the scaling matrix P .
Reference: [19] <author> A. N. Iusem. </author> <title> An iterative algorithm for the variational inequality problem. </title> <journal> Computational and Applied Mathematics, </journal> <volume> 13, </volume> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [21] (also see <ref> [19, 20, 29] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little <p> Moreover, its convergence requires only that a solution exists <ref> [19] </ref>, while its only drawback is its, at best, linear convergence.
Reference: [20] <author> E.N. Khobotov. </author> <title> A modification of the extragradient method for the solution of variational inequalities and some optimization problems. </title> <journal> Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki, </journal> <volume> 27 </volume> <pages> 1462-1473, </pages> <year> 1987. </year>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [21] (also see <ref> [19, 20, 29] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little
Reference: [21] <author> G.M. Korpelevich. </author> <title> The extragradient method for finding saddle points and other problems. </title> <journal> Matecon, </journal> <volume> 12 </volume> <pages> 747-756, </pages> <year> 1976. </year>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method <ref> [21] </ref> (also see [19, 20, 29] for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to
Reference: [22] <author> X.-D. Luo and P. Tseng. </author> <title> On global projection-type error bound for the linear complementarity problem. Linear Algebra and Its Applications. </title> <note> To appear. 21 </note>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [22, 23, 28, 37] </ref>).
Reference: [23] <author> Z.-Q. Luo, O.L. Mangasarian, J. Ren, </author> <title> and M.V. Solodov. New error bounds for the linear complementarity problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 19 </volume> <pages> 880-892, </pages> <year> 1994. </year>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [22, 23, 28, 37] </ref>).
Reference: [24] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 43-54, </pages> <year> 1992. </year>
Reference-contexts: to S. (It is well known that an x fl 2 &lt; n solves VI (X; F ) if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods <ref> [25, 24, 47] </ref> and is known to hold whenever X is polyhedral and either F is affine (see [24, 41]) or F has certain strong monotonicity structure (see [47, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]). <p> if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods [25, 24, 47] and is known to hold whenever X is polyhedral and either F is affine (see <ref> [24, 41] </ref>) or F has certain strong monotonicity structure (see [47, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]). <p> The analysis is also similar in spirit to those for feasible descent methods (see <ref> [25, 24, 27] </ref>) but uses d (; S) 2 , rather than the objective function, as the merit function. Our main results are as follows: In Section 2, we consider the special case of VI (X; F ) where F is affine.
Reference: [25] <author> Z.-Q. Luo and P. Tseng. </author> <title> On the linear convergence of descent methods for convex essentially smooth minimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 30 </volume> <pages> 408-425, </pages> <year> 1992. </year>
Reference-contexts: to S. (It is well known that an x fl 2 &lt; n solves VI (X; F ) if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods <ref> [25, 24, 47] </ref> and is known to hold whenever X is polyhedral and either F is affine (see [24, 41]) or F has certain strong monotonicity structure (see [47, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]). <p> The analysis is also similar in spirit to those for feasible descent methods (see <ref> [25, 24, 27] </ref>) but uses d (; S) 2 , rather than the objective function, as the merit function. Our main results are as follows: In Section 2, we consider the special case of VI (X; F ) where F is affine.
Reference: [26] <author> T.L. Magnanti and G. Perakis. </author> <title> On the convergence of classical variational inequality algorithms. </title> <type> Working paper, </type> <institution> Operations Research Center, MIT, </institution> <address> Cambridge, Mas-sachusetts, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method [44] (also see <ref> [1, 2, 3, 8, 26] </ref>) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize. <p> Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [27] <author> O.L. Mangasarian. </author> <title> Convergence of iterates of an inexact matrix splitting algorithm for the symmetric monotone linear complementarity problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1 </volume> <pages> 114-122, </pages> <year> 1991. </year>
Reference-contexts: The analysis is also similar in spirit to those for feasible descent methods (see <ref> [25, 24, 27] </ref>) but uses d (; S) 2 , rather than the objective function, as the merit function. Our main results are as follows: In Section 2, we consider the special case of VI (X; F ) where F is affine.
Reference: [28] <author> O.L. Mangasarian and J. Ren. </author> <title> New improved error bounds for the linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 66 </volume> <pages> 241-255, </pages> <year> 1994. </year>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [22, 23, 28, 37] </ref>).
Reference: [29] <author> P. Marcotte. </author> <title> Application of Khobotov's algorithm to variational inequalities and network equilibrium problems. </title> <journal> Information Systems and Operational Research, </journal> <volume> 29 </volume> <pages> 258-270, </pages> <year> 1991. </year>
Reference-contexts: However, the projection method requires the restrictive assumption that F or F 1 be strongly monotone for convergence. The extragradient method [21] (also see <ref> [19, 20, 29] </ref> for extensions) overcomes this difficulty by the ingenious technique of updating x according to the double projection formula: x new := x ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little <p> see that this does not affect the convergence (and, in fact, accelerates convergence) of the methods, we use the following fact about nearest-point projection: k [y] + P ky x fl k 2 P k 2 for all y 2 &lt; n and all x fl 2 X (see, e.g., <ref> [29, Appendix] </ref>). <p> For benchmark, we compared the performance of these implementations with analogous implementations of the extragradient method as described in <ref> [29] </ref>. (We have included LPs and dense monotone LCPs in our tests not because they are problems for which the new methods are designed to solve, but because these problems are well known special cases of VI (X; F ) and tests on them give us a better overall understanding of
Reference: [30] <author> P. Marcotte and J.-P. Dussault. </author> <title> A note on a globally convergent Newton method for solving monotone variational inequalities. </title> <journal> Operations Research Letters, </journal> <volume> 6 </volume> <pages> 35-42, </pages> <year> 1987. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [31] <author> P. Marcotte and J.-P. Dussault. </author> <title> A sequential linear programming algorithm for solving monotone variational inequalities. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 27 </volume> <pages> 1260-1278, </pages> <year> 1989. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [32] <author> P. Marcotte and J.-H. Wu. </author> <title> On the convergence of projection methods: application to the decomposition of affine variational inequalities. </title> <journal> Journal of Optimization Theory and Applications. </journal> <note> To appear. 22 </note>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded <p> 39, 46, 49] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration 1 than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration.
Reference: [33] <author> L. Mathiesen. </author> <title> An algorithm based on a sequence of linear complementarity problems applied to a Walrasian equilibrium model: An example. </title> <journal> Mathematical Programming, </journal> <volume> 37 </volume> <pages> 1-18, </pages> <year> 1987. </year>
Reference-contexts: Our third set of tests was conducted on VI (X; F ) where X is not an orthant or a box. The first test problem, used first by Mathiesen <ref> [33] </ref>, and later in [38, 49], has F (x 1 ; x 2 ; x 3 ) = 6 :9 (5x 2 + 3x 3 )=x 1 3 7 ( + fi fi x 1 + x 2 + x 3 = 1 ) We had trouble finding more test problems
Reference: [34] <author> K.G. Murty. </author> <title> Linear Complementarity, Linear and Nonlinear Programming. </title> <address> Helderman-Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: This problem, which we abbreviate as VI (X; F ), is well known in optimization (see [1, 6, 15]) and, in the special case where F is affine and X is the nonnegative orthant, reduces to the classical monotone linear complementarity problem (see <ref> [7, 34] </ref>). Many methods have been proposed to solve VI (X; F ).
Reference: [35] <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: We denote by I either the identity matrix or the identity map and, by R-linear convergence and Q-linear convergence, we mean linear convergence in the root sense and in the quotient sense, respectively, as defined in <ref> [35] </ref>. 2 Algorithms for F Affine In this section we consider the case of VI (X; F ) where F is monotone and affine, i.e., F (x) = M x + q for some n fi n positive semidefinite (not necessarily symmetric) matrix M and some q 2 &lt; n .
Reference: [36] <author> J.-S. Pang. </author> <title> Asymmetric variational inequality problems over product sets: applications and iterative methods. </title> <journal> Mathematical Programming, </journal> <volume> 31 </volume> <pages> 206-219, </pages> <year> 1985. </year>
Reference-contexts: ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 36, 43] </ref>. Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. <p> Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [37] <author> J.-S. Pang. </author> <title> A posteriori error bounds for the linearly-constrained variational inequality problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 </volume> <pages> 474-484, </pages> <year> 1987. </year>
Reference-contexts: Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see <ref> [22, 23, 28, 37] </ref>).
Reference: [38] <author> J.-S. Pang and S.A. Gabriel. NE/SQP: </author> <title> A robust algorithm for the nonlinear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 60 </volume> <pages> 295-337, </pages> <year> 1993. </year>
Reference-contexts: Our third set of tests was conducted on VI (X; F ) where X is not an orthant or a box. The first test problem, used first by Mathiesen [33], and later in <ref> [38, 49] </ref>, has F (x 1 ; x 2 ; x 3 ) = 6 :9 (5x 2 + 3x 3 )=x 1 3 7 ( + fi fi x 1 + x 2 + x 3 = 1 ) We had trouble finding more test problems from the literature, so <p> &lt; n + j x 1 + + x n = ng and F and n are specified as follows: For the first three problems, F is the function from, respectively, the Kojima-Shindo NCP (with n = 4) and the Nash-Cournot NCP (with n = 5 and n = 10) <ref> [38, pp. 321-322] </ref>; for the fourth problem, F is affine and is generated as in the problem HPHard of Table 2, but with n = 20; for the fifth problem, we took the F from the fourth problem and added to its ith component the linear/quadratic term maxf0; x i g
Reference: [39] <author> J.S. Pang and D. Chan. </author> <title> Iterative methods for variational and complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 24 </volume> <pages> 284-313, </pages> <year> 1982. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [40] <author> P.M. Pardalos and N. Kovoor. </author> <title> An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds. </title> <journal> Mathematical Programming, </journal> <volume> 46 </volume> <pages> 235-238, </pages> <year> 1986. </year>
Reference-contexts: The extragradient method can be practically implemented to solve this special case of VI (X; F ) since it requires only projection onto the simplices and ellipsoids (for which many efficient methods exist <ref> [40, 48] </ref>) and multiplication of x by the sparse matrix M . In contrast, the matrix-splitting methods in [10, 32, 45, 47] require solving a nontrivial strongly monotone variational inequality problem over X at each iteration.
Reference: [41] <author> S.M. Robinson. </author> <title> Some continuity properties of polyhedral multifunctions. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 206-214, </pages> <year> 1981. </year>
Reference-contexts: if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods [25, 24, 47] and is known to hold whenever X is polyhedral and either F is affine (see <ref> [24, 41] </ref>) or F has certain strong monotonicity structure (see [47, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]).
Reference: [42] <author> R.T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14(5) </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: The remaining argument is patterned after the proof of <ref> [42, Theorem 1] </ref> and of [47, Theorem 1]. Since (2.5) holds for all i, it follows that kx i x fl k P is nonincreasing with i and that kr (x i )k ! 0 as i ! 1.
Reference: [43] <author> R.T. Rockafellar and R.J.-B. Wets. </author> <title> Generalized linear-quadratic problems of deterministic and stochastic optimal control in discrete time. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 28 </volume> <pages> 810-822, </pages> <year> 1990. </year> <month> 23 </month>
Reference-contexts: ffF [x ffF (x)] + : This method, by virtue of its using only function evaluations and projection onto X, is easy to implement, uses little storage, and can readily exploit any sparsity or separable structure in F or in X, such as those arising in the applications considered in <ref> [3, 9, 36, 43] </ref>. Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. <p> In contrast, the matrix-splitting methods in [10, 32, 45, 47] require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. And even on structured problems such as the discrete-time deterministic optimal control problem <ref> [43] </ref>, the extragradient method may yet be practical since it is linearly convergent like the methods in [5, 10, 45, 50] while its iterations are simpler.
Reference: [44] <author> M. Sibony. </author> <title> Methodes iteratives pour les equations et inequations aux derivees partielles nonlineares de type monotone. </title> <journal> Calcolo, </journal> <volume> 7 </volume> <pages> 65-183, </pages> <year> 1970. </year>
Reference-contexts: Many methods have been proposed to solve VI (X; F ). The simplest of these is the projection method <ref> [44] </ref> (also see [1, 2, 3, 8, 26]) which, starting with any x 2 &lt; n , iteratively updates x according to the formula x new := [x ffF (x)] + ; where [] + denotes the orthogonal projection map onto X and ff is a judiciously chosen positive stepsize.
Reference: [45] <author> P. Tseng. </author> <title> Further applications of a splitting algorithm to decomposition in variational inequalities and convex programming. </title> <journal> Mathematical Programming, </journal> <volume> 48 </volume> <pages> 249-263, </pages> <year> 1990. </year>
Reference-contexts: 39, 46, 49] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration 1 than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. <p> And even on structured problems such as the discrete-time deterministic optimal control problem [43], the extragradient method may yet be practical since it is linearly convergent like the methods in <ref> [5, 10, 45, 50] </ref> while its iterations are simpler.
Reference: [46] <author> P. Tseng. </author> <title> Applications of a splitting algorithm to decomposition in convex programming and variational inequalities. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 29 </volume> <pages> 119-138, </pages> <year> 1991. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded
Reference: [47] <author> P. Tseng. </author> <title> On linear convergence of iterative methods for the variational inequality problem. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <note> 1995. To appear. </note>
Reference-contexts: 39, 46, 49] require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded and polyhedral), while the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> are applicable only when F is affine (and these methods also have, at best, linear convergence). And all these methods require more computation per iteration 1 than the extragradient method. <p> In contrast, the matrix-splitting methods in <ref> [10, 32, 45, 47] </ref> require solving a nontrivial strongly monotone variational inequality problem over X at each iteration. <p> to S. (It is well known that an x fl 2 &lt; n solves VI (X; F ) if and only if r (x fl ) = 0.) This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods <ref> [25, 24, 47] </ref> and is known to hold whenever X is polyhedral and either F is affine (see [24, 41]) or F has certain strong monotonicity structure (see [47, Theorem 2]). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]). <p> This growth condition on kr ()k (also called error bound) has been used in the rate of convergence analysis of various methods [25, 24, 47] and is known to hold whenever X is polyhedral and either F is affine (see [24, 41]) or F has certain strong monotonicity structure (see <ref> [47, Theorem 2] </ref>). Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]). <p> Moreover, under additional assumptions on F , this condition holds with ffi = 1 (see [22, 23, 28, 37]). Our rate of convergence analysis, similar to that in <ref> [47] </ref>, entails (roughly) showing that d (x; S) 2 decreases by an amount in the order of kr (x)k 2 per iteration, so kr (x)k must eventually decrease below ffi, at which time (1.3) yields that d (x; S) 2 decreases at a linear rate. <p> The remaining argument is patterned after the proof of [42, Theorem 1] and of <ref> [47, Theorem 1] </ref>. Since (2.5) holds for all i, it follows that kx i x fl k P is nonincreasing with i and that kr (x i )k ! 0 as i ! 1.
Reference: [48] <author> Y. Ye. </author> <title> A new complexity result on minimization of a quadratic function with a sphere constraint. </title> <editor> In C. Floudas and P.M. Pardalos, editors, </editor> <title> Recent Advances in Global Optimization. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1992. </year>
Reference-contexts: The extragradient method can be practically implemented to solve this special case of VI (X; F ) since it requires only projection onto the simplices and ellipsoids (for which many efficient methods exist <ref> [40, 48] </ref>) and multiplication of x by the sparse matrix M . In contrast, the matrix-splitting methods in [10, 32, 45, 47] require solving a nontrivial strongly monotone variational inequality problem over X at each iteration.
Reference: [49] <author> L. Zhao and S. Dafermos. </author> <title> General economic equilibrium and variational inequalities. </title> <journal> Operations Research Letters, </journal> <volume> 10 </volume> <pages> 369-376, </pages> <year> 1991. </year>
Reference-contexts: Moreover, its convergence requires only that a solution exists [19], while its only drawback is its, at best, linear convergence. In contrast, the methods in <ref> [4, 8, 12, 26, 30, 31, 32, 36, 39, 46, 49] </ref> require restrictive assumptions on the problem (such as F or F 1 be strongly monotone or F be affine; for some of the methods, it is further required that F be continuously differentiable with nonsingular Jacobian or X be bounded <p> Our third set of tests was conducted on VI (X; F ) where X is not an orthant or a box. The first test problem, used first by Mathiesen [33], and later in <ref> [38, 49] </ref>, has F (x 1 ; x 2 ; x 3 ) = 6 :9 (5x 2 + 3x 3 )=x 1 3 7 ( + fi fi x 1 + x 2 + x 3 = 1 ) We had trouble finding more test problems from the literature, so <p> In our implementation of Algorithm 3.2, we chose P = I, ff 1 = 1, = 1:5, = :1 and fi = :3. On the Mathiesen problem, we used the same x 0 as in <ref> [49] </ref>; on the other problems, we used x 0 = (1; :::; 1). (The F from the Mathiesen problem and from the Nash-Cournot NCP are defined on the positive orthant only.) The test results are summarized in Table 3.

References-found: 49

