URL: http://www.cse.psu.edu/~zha/papers/ulv.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: AN ALGORITHM AND A STABILITY THEORY FOR DOWNDATING THE ULV DECOMPOSITION into A U V
Author: JESSE L. BARLOW, PETER A. YOON, and HONGYUAN ZHA 
Keyword: Key words: Orthogonal decomposition, downdating, error analysis, subspaces  
Address: Park PA, 16802-6106, USA  
Affiliation: Department of Computer Science and Engineering, The Pennsylvania State University University  
Note: BIT  A  These subspaces are denoted by V (V 1 V 2 where the columns of C are partitioned confor-mally into C (C 1 C 2 with k C 2 k F Here is some tolerance. In recent years, this has been called the ULVD. AMS subject classifications: (Primary) 65F20,65G05 (Secondary)  
Email: email: barlow@cse.psu.edu, payoon@cse.psu.edu, zha@cse.psu.edu  
Date: 34 (1994), 000-000.  
Web: 65F15.  
Abstract: If the matrix A results from statistical observations, it is often desired to remove old observations, thus deleting a row from A and its ULVD. In matrix terms, this is called a downdate. A downdating algorithm is proposed that preserves the structure in the downdated matrix C to the extent possible. Strong stability results are proven for these algorithms based upon a new perturbation theory. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J.L. Barlow. </author> <title> Stability analysis of the G-algorithm and a note on its application to sparse least squares problems. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 507-520, </pages> <year> 1985. </year>
Reference-contexts: This theorem is somewhat similar to error bounds on orthogonal factorization of matrices with disproportionate rows <ref> [1, 2] </ref>. It implies that the separation of the subspaces for "large" and "small" singular values remains good. The results below explain why the bounds in Theorem 4.1 are necessary.

Reference: 3. <author> J.L. Barlow, H. Zha, </author> <title> and P.A. Yoon. Efficient and stable algorithms for modifying the singular value decomposition and partial singular value decomposition. </title> <type> Technical Report CSE-93-19, </type> <institution> Department of Computer Science and Engineering, The Pennsylvania State University, University Park, </institution> <address> PA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Since the data movement patterns are similar to those of the algorithm of Stewart [25] it should be no more difficult to implement these algorithms in a systolic architecture. These techniques are also similar to the updating and downdating algorithms given by Barlow, Zha, and Yoon <ref> [3] </ref>. We give some new perturbation results for this problem, which are not included in those by Pan [20], or Elden and Park [8]. The following are the main results of this paper: 1.
Reference: 4. <author> A. Bjorck. </author> <title> Stability analysis of the method of semi-normal equations for linear least squares problems. </title> <journal> Lin. Alg. Appl., </journal> 88/89:31-48, 1987. 
Reference-contexts: First, in Bjorck et al. [5], it is recommended that we solve B T a = z by corrected semi-normal equations (CSNE) under the assumption that w = A T e 1 , the first row of A. From <ref> [4] </ref>, we know that this gives a more accurate vector a. Of course, this step is more time consuming than the standard LINPACK approach.
Reference: 5. <author> A. Bjorck, H. Park, and L. Elden. </author> <title> Accurate downdating of least squares solutions. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 15 </volume> <pages> 549-568, </pages> <year> 1994. </year>
Reference-contexts: this algorithm can yield k + 1 "large" rows, thus the rank revealing nature of C may be lost. 2.2 The LINPACK downdating algorithm The following downdating procedure due to Saunders [13] is considered the most accurate downdating procedure that does not require information from the first column of U <ref> [5] </ref> (if we have that first column, we obtain a procedure that is backward stable in the strong sense). It is the procedure that is implemented in LINPACK [7]. <p> B takes the position of C in (1) with U and V defined the same way. First, in Bjorck et al. <ref> [5] </ref>, it is recommended that we solve B T a = z by corrected semi-normal equations (CSNE) under the assumption that w = A T e 1 , the first row of A. From [4], we know that this gives a more accurate vector a. <p> From [4], we know that this gives a more accurate vector a. Of course, this step is more time consuming than the standard LINPACK approach. The CSNE seems to make the most difference when k a k is close to 1, thus Bjorck et al. <ref> [5] </ref> recommend its use only when ff = p 1 k a k 2 is smaller than a certain threshold. A second modification is the use of a two step procedure. In our notation, it is as follows. 1.
Reference: 6. <author> C. Davis and W.M. Kahan. </author> <title> The rotation of eigenvectors by a perturbation III. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 7 </volume> <pages> 1-46, </pages> <year> 1970. </year>
Reference: 7. <author> J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: It is the procedure that is implemented in LINPACK <ref> [7] </ref>. We make use of the fact that C T C zz T is positive definite if and only if k C T z k 2 &lt; 1. Algorithm 2.1 (LINPACK downdating procedure). 1.
Reference: 8. <author> L. Elden and H. Park. </author> <title> Peturbation analysis of block downdating of a Cholesky decompostion. </title> <journal> Numerische Mathematik, </journal> <note> to appear, </note> <year> 1994. </year>
Reference-contexts: These techniques are also similar to the updating and downdating algorithms given by Barlow, Zha, and Yoon [3]. We give some new perturbation results for this problem, which are not included in those by Pan [20], or Elden and Park <ref> [8] </ref>. The following are the main results of this paper: 1. <p> 2 2 &lt; nfi (nk) then k W T ~ W 2 k F 2 k (n k) oe k oe k+1 C ): The effect of ffi C is, in fact, somewhat less critical than that of ffi b as has been stated in other analyses of this problem <ref> [23, 20, 8] </ref>. If k L 1 k 1 oe k &gt; * ,these bounds are a significant improvement over those in Proposition 4.6. This is one of the reasons for maintaining the property (5). For many of the applications of this algorithm, the numerical rank will remain stable. <p> It is essentially the same as that used by Park and Elden <ref> [8] </ref> and is given by ~ Ly = a ~ L T ffia = V T 1 X T a = a + ffia ~ Lffiy = ffia t = t X j V 1 ffiy ff = ktk where X j = A j , j th window matrix augmented
Reference: 9. <author> R.D. Fierro. </author> <title> Perturbation analysis for two-sided (or complete) orthogonal decompositions. </title> <type> Technical Report 94-02, </type> <institution> Department of Mathematics, California State University, </institution> <address> San Marcos, CA, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: Moreover, to prevent a from becoming too large, we track k F k F so that it remains under certain threshold, say, k L 1 kk F k F &lt; 0:01. This is similar to recommendations made by Fierro and Bunch <ref> [9, 10] </ref>. Only steps in Algorithm 3.1 that require to update k F k F are Steps 2 and 5.
Reference: 10. <author> R.D. Fierro and J.R. Bunch. </author> <title> Bounding the subspaces from rank revealing two-sided orthogonal decompositions. </title> <type> Technical Report 94-05, </type> <institution> Department of Mathematics, California State University, </institution> <address> San Marcos, CA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Moreover, to prevent a from becoming too large, we track k F k F so that it remains under certain threshold, say, k L 1 kk F k F &lt; 0:01. This is similar to recommendations made by Fierro and Bunch <ref> [9, 10] </ref>. Only steps in Algorithm 3.1 that require to update k F k F are Steps 2 and 5.
Reference: 11. <author> R.D. Fierro and P. C. Hansen. </author> <title> Approximating the LS and TLS solutions by rank revealing two-sided orthogonal decomposition. </title> <type> Technical Report 93-16, </type> <institution> Department of Mathematics, University of California at Los Angeles, </institution> <address> Los Angeles, CA, </address> <year> 1993. </year>
Reference-contexts: It turned out that log 10 (sin 3 ) was almost indistinguishable from log 10 (sin 1 ), so we did not plot it. sin 2 is the approximation error discussed by Fierro and Hansen <ref> [11] </ref>. Finally, the TLS errors k x (SV D) x (ULV ) k are given in logarithm in the last plot. Here, x (SV D) and x (ULV ) are the TLS solutions using the SVD and the ULVD, respectively.
Reference: 12. <author> W.M. Gentleman. </author> <title> Error analysis of QR decompositions by Givens rotations. </title> <journal> Lin. Alg. Appl., </journal> <volume> 12 </volume> <pages> 189-197, </pages> <year> 1975. </year>
Reference: 13. <author> P.E. Gill, G.H. Golub, W. Murray, and M.A. Saunders. </author> <title> Methods for modifying matrix factorizations. </title> <journal> Math. Comp., </journal> <volume> 28 </volume> <pages> 505-535, </pages> <year> 1974. </year>
Reference-contexts: Our approach to downdating the ULVD (2) uses ideas from "chasing" algorithms [25] and from the downdating algorithm due to Saunders <ref> [13, 19] </ref>. Since the data movement patterns are similar to those of the algorithm of Stewart [25] it should be no more difficult to implement these algorithms in a systolic architecture. These techniques are also similar to the updating and downdating algorithms given by Barlow, Zha, and Yoon [3]. <p> if the matrix C is from a rank revealing decomposition with k "large" rows and n k "small" rows, this algorithm can yield k + 1 "large" rows, thus the rank revealing nature of C may be lost. 2.2 The LINPACK downdating algorithm The following downdating procedure due to Saunders <ref> [13] </ref> is considered the most accurate downdating procedure that does not require information from the first column of U [5] (if we have that first column, we obtain a procedure that is backward stable in the strong sense). It is the procedure that is implemented in LINPACK [7].
Reference: 14. <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations, Second Edition. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: The horizontal axis represents the window steps and the vertical axis the numerical rank of the window matrix. The distance between the subspaces is given in the next plot using the definition described in <ref> [14, pp.77-78,Corollary 2.6.2] </ref>. Let A j = Y j j W T be the SVD of A j computed by the one-sided Jacobi method at step j. Let A j = U j C j V T be the ULVD of A j computed by Algorithm 3.1.
Reference: 15. <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem: Computational Aspects and Analysis. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: is p-by-n, b, p-by-1, and k n, the minimum norm TLS solution is given by computing an (n + 1)-by-(n + 1) Householder transformation H such that V 2 H = Y d ! 1 If ffi 6= 0, the TLS solution ^x is given by ^x = d=ffi:(39) See <ref> [15] </ref> for details. Van Huffel and Zha [16] formulated the solution to the TLS problems based on the URVD or the ULVD without the explicit computation of the approximate null space basis V 2 .
Reference: 16. <author> S. Van Huffel and H. Zha. </author> <title> An efficient total least squares algorithm based on a rank revealing two-sided orthogonal decomposition. </title> <booktitle> Numerical Algorithms, </booktitle> <volume> 4 </volume> <pages> 101-133, </pages> <year> 1993. </year> <title> DOWNDATING THE ULV DECOMPOSITION 27 </title>
Reference-contexts: Van Huffel and Zha <ref> [16] </ref> formulated the solution to the TLS problems based on the URVD or the ULVD without the explicit computation of the approximate null space basis V 2 . However, with our approach we are given V 2 explicitly at every updating and downdating step.
Reference: 17. <author> T. Kato. </author> <title> A Short Introduction to Perturbation Theory for Linear Operators. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: From the Kato <ref> [17] </ref> expansion for eigenvectors we obtain j ~w T ~w T i (b (ffib) T + (ffib)b T + (ffib)(ffib) T ) ~w j ~oe 2 j j + O (k ffib k 2 ) k ffib k (jb T ~w i j + jb T ~w j j) ~oe 2 <p> Then for all i and j such that oe i 6= oe j we have j ~w T ffi b i ~oe 2 (k ~w j k jb T ~w i j+ k ~w i k jb T ~w j j) + O (ffi 2 Proof: From Kato <ref> [17] </ref>, we have that ~w T ~w T ~oe 2 j b ) where e is defined in (30). Taking norms we obtain (35). 2 Lemma 4.8. <p> Then for all i and j such that oe i &gt; oe j we have jw T ffi C k L 1 k (oe j + *) + O (ffi 2 Proof: Again using the Kato <ref> [17] </ref> exapnsion w T w T i ( Cffi C + C T ffi C)w j oe 2 j C ) Using the definition of E in (30) we have w T w T j C T Ew j i oe 2 + O (ffi 2 DOWNDATING THE ULV DECOMPOSITION 17
Reference: 18. <editor> C.L. Lawson and R.J. Hanson. </editor> <title> Solving Least Squares Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliff, NJ, </address> <year> 1974. </year>
Reference-contexts: It is assumed that k C k 1 which can be assured by setting k C k F = 1. The ULVD was described by Stewart [25] who also gives a method for updating it. It is a particular case of what Lawson and Hanson <ref> [18] </ref> called HRK decompositions. Our version of ULVD separates out columns that are exactly zero. The downdating problem is that of obtaining the ULVD of A where A = w T ! and the ULVD of A is known.
Reference: 19. <author> C.-T. Pan. </author> <title> A modification to the LINPACK downdating algorithm. </title> <journal> BIT, </journal> <volume> 30 </volume> <pages> 707-722, </pages> <year> 1990. </year>
Reference-contexts: Our approach to downdating the ULVD (2) uses ideas from "chasing" algorithms [25] and from the downdating algorithm due to Saunders <ref> [13, 19] </ref>. Since the data movement patterns are similar to those of the algorithm of Stewart [25] it should be no more difficult to implement these algorithms in a systolic architecture. These techniques are also similar to the updating and downdating algorithms given by Barlow, Zha, and Yoon [3]. <p> Pan <ref> [19] </ref> shows that for C upper triangular, this method can be sped up by combining the forward substitution phase with the application of the Givens rotations. <p> g 11 = U T ff 1 g T g 11 ; L (1) lower triangular (13) Define F (2) = (I e 1 e T 1 )F (1) ,that is, as F (1) with its first row zeroed out. 3. (Perform Downdate on L) Use Algorithm 2.1 (or alternatives <ref> [19] </ref>) to find a vector a 2 &lt; k , scalar ff, and orthogonal matrix U 3 such that [L (1) ] T a = x; ff = 1 k a k 2 ; U T a = e 1 : Then do x T ! 3 0 ! 4. (Perform
Reference: 20. <author> C.-T. Pan. </author> <title> A perturbation analysis on the problem of downdating a Cholesky factorization. Linear Alg. </title> <journal> Appl., </journal> <volume> 183 </volume> <pages> 103-115, </pages> <year> 1993. </year>
Reference-contexts: These techniques are also similar to the updating and downdating algorithms given by Barlow, Zha, and Yoon [3]. We give some new perturbation results for this problem, which are not included in those by Pan <ref> [20] </ref>, or Elden and Park [8]. The following are the main results of this paper: 1. <p> BARLOW, P.A. YOON, AND H. ZHA It is also necessary to define ! = maxf1; k L T x k; k [L (2) ] T x kg: Note that this value ! is related to the condition number given by Pan <ref> [20] </ref> for the downdating problems [L (2) ] T L (2) = [L (1) ] T L (1) xx T (28) Equation (28) is the downdating problem given by Step 3 of our algorithm, one that we minimally must be able to perform. <p> 2 2 &lt; nfi (nk) then k W T ~ W 2 k F 2 k (n k) oe k oe k+1 C ): The effect of ffi C is, in fact, somewhat less critical than that of ffi b as has been stated in other analyses of this problem <ref> [23, 20, 8] </ref>. If k L 1 k 1 oe k &gt; * ,these bounds are a significant improvement over those in Proposition 4.6. This is one of the reasons for maintaining the property (5). For many of the applications of this algorithm, the numerical rank will remain stable.
Reference: 21. <author> H. Park and L. Elden. </author> <title> Downdating the rank-revealing URV decomposition. </title> <type> Technical Report LiTH-MAT-1992-47, </type> <institution> Department of Mathematics, Linkoping University, Linkoping, Sweden, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: A downdating algorithm that works whenever L T L xx T is positive definite, the same as if we were downdating only L. We use an improvement of a technique given by Park and Elden <ref> [21] </ref> for the URV decomposition (URVD). Our technique maps back onto the original matrix A in a more satisfactory manner. 3. An error analysis of this procedure showing that the singular subspaces of the updated matrix are as good as can be expected. <p> He gives two recurrences which produce the same result. 2.3 Park and Elden's URV procedure There are some difficulties with a straightforward implementation of the LINPACK algorithm on the URVD. A recent report by Park and Elden <ref> [21] </ref> gives an alternative implementation. They consider methods for preventing k a k 1 and give a recommen dation as to how to handle problems where that condition cannot be avoided. <p> If we use the LINPACK procedure for (8) and (10), the error analysis of this procedure is very much the same as the LINPACK procedure. Park and Elden <ref> [21] </ref> recommend the use of hyperbolic rotations in (9). That can be avoided by a simple and well-known trick. Let (ff 1 ; a 1 ) T be the first column of U 1 as determined in (8). <p> Thus the expected accuracy will be about the same and the procedure should have similar stability properties. Moreover, the quantities computed in the Park- Elden procedure <ref> [21] </ref> are those computed in the LINPACK algorithm, but they are computed in a different order.
Reference: 22. <author> G.W. Stewart. </author> <title> On the perturbation of psuedo-inverses, projections, and linear least squares problems. </title> <journal> SIAM Review, </journal> <volume> 19 </volume> <pages> 634-662, </pages> <year> 1977. </year>
Reference: 23. <author> G.W. Stewart. </author> <title> The effects of rounding error on an algorithm for downdating a Cholesky factorization. </title> <journal> J. Inst. Maths. Applics., </journal> <volume> 23 </volume> <pages> 203-213, </pages> <year> 1979. </year>
Reference-contexts: This is the so-called mixed stability criterion described by Stewart <ref> [23] </ref>. <p> 2 2 &lt; nfi (nk) then k W T ~ W 2 k F 2 k (n k) oe k oe k+1 C ): The effect of ffi C is, in fact, somewhat less critical than that of ffi b as has been stated in other analyses of this problem <ref> [23, 20, 8] </ref>. If k L 1 k 1 oe k &gt; * ,these bounds are a significant improvement over those in Proposition 4.6. This is one of the reasons for maintaining the property (5). For many of the applications of this algorithm, the numerical rank will remain stable.
Reference: 24. <author> G.W. Stewart. </author> <title> Two simple residual bounds for the eigenvalues of a Hermitian matrix. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 12 </volume> <pages> 205-209, </pages> <year> 1991. </year>
Reference-contexts: first p rows of A, can be obtained by computing its QL factorization A 0 = Q 0 L 0 ! where Q 0 is a p-by-p orthogonal matrix and L 0 a n-by-n lower triangular, followed by computing the ULVD of L 0 using the deflation technique described in <ref> [24, 25] </ref>. <p> Since our downdating procedures use LINPACK downdating algorithm, it is not difficult to generate the cases where the algorithm breaks down when k a k&gt; 1, for instance, when deleting a row that contains outliers. In this case, we first refine the decomposition <ref> [24] </ref>, that is, compute an orthogonal matrix ~ U such that ~ U L 0 ! 0 ~ G :(45) If it is still true that kak &gt; 1 even after the refinement, where ~ L T a = x, the CSNE approach (indicated by '+' in the first plot) is
Reference: 25. <author> G.W. Stewart. </author> <title> Updating a rank-revealing ULV decomposition. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 </volume> <pages> 494-499, </pages> <year> 1993. </year>
Reference-contexts: We use k k to denote Euclidean norm, and k k F to denote Frobenius norm. It is assumed that k C k 1 which can be assured by setting k C k F = 1. The ULVD was described by Stewart <ref> [25] </ref> who also gives a method for updating it. It is a particular case of what Lawson and Hanson [18] called HRK decompositions. Our version of ULVD separates out columns that are exactly zero. <p> The algorithm is stated in terms of C and z. As in the updating routine of Stewart <ref> [25] </ref>, the matrices C and V can be produced using O (n) Givens rotations, thus updating the factorization in O (n 2 ). Our approach to downdating the ULVD (2) uses ideas from "chasing" algorithms [25] and from the downdating algorithm due to Saunders [13, 19]. <p> As in the updating routine of Stewart <ref> [25] </ref>, the matrices C and V can be produced using O (n) Givens rotations, thus updating the factorization in O (n 2 ). Our approach to downdating the ULVD (2) uses ideas from "chasing" algorithms [25] and from the downdating algorithm due to Saunders [13, 19]. Since the data movement patterns are similar to those of the algorithm of Stewart [25] it should be no more difficult to implement these algorithms in a systolic architecture. <p> Our approach to downdating the ULVD (2) uses ideas from "chasing" algorithms <ref> [25] </ref> and from the downdating algorithm due to Saunders [13, 19]. Since the data movement patterns are similar to those of the algorithm of Stewart [25] it should be no more difficult to implement these algorithms in a systolic architecture. These techniques are also similar to the updating and downdating algorithms given by Barlow, Zha, and Yoon [3]. <p> (i + 1 : n; i + 1); cn; sn; n i); formrot (c (i + 1; i + 1); e; cn; sn; c (i + 1; i + 1)) ; applyrot (c (i + 1; 1 : i); c (i; 1 : i); cn; sn; i); endfor endlchase Stewart <ref> [25] </ref> points out that if the matrix C is from a rank revealing decomposition with k "large" rows and n k "small" rows, this algorithm can yield k + 1 "large" rows, thus the rank revealing nature of C may be lost. 2.2 The LINPACK downdating algorithm The following downdating procedure <p> This algorithm works if 0 0 is substituted for G and (y T ; y T 0 ) T is substituted for y. That is, it is not necessary to explicitly handle the zero block, it can be made part of G. That is the original formulation in <ref> [25] </ref>. However, if that is done, whenever y 0 6= 0, any zero diagonal will be chased to the g 11 position, all of (y; y 0 ) T will be treated as "noise". <p> first p rows of A, can be obtained by computing its QL factorization A 0 = Q 0 L 0 ! where Q 0 is a p-by-p orthogonal matrix and L 0 a n-by-n lower triangular, followed by computing the ULVD of L 0 using the deflation technique described in <ref> [24, 25] </ref>. <p> One such circumstance seems to be when an update or downdate cause a sudden change in the scale of the matrix. A necessary verifiable condition for refactoring is a subject for future research. 6 Conclusion The downdating algorithms presented here coupled with updating algorithm of Stew-art <ref> [25] </ref> show that the ULV decomposition can be updated and downdated in O (n 2 ) flops (including the time for updating invariant subspaces) in a manner that preserves their structure. The backward error analysis and perturbation theory show that the downdate procedure satisfies a blockwise stability property.
Reference: 26. <author> J.H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <address> Lon-don, </address> <year> 1965. </year>
References-found: 25

