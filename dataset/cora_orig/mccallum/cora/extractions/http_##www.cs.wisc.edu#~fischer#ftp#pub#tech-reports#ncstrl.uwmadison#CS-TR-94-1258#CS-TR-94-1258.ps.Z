URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1258/CS-TR-94-1258.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1258/
Root-URL: http://www.cs.wisc.edu
Email: hollings@cs.umd.edu bart@cs.wisc.edu  
Title: An Adaptive Cost Model for Parallel Program Instrumentation  
Author: Jeffrey K. Hollingsworth Barton P. Miller 
Address: 1210 West Dayton Street College Park, MD 20742 Madison, WI 53706  
Affiliation: Computer Science Department University of Wisconsin University of Maryland  
Abstract: Software based instrumentation is frequently used to measure the performance of parallel and distributed programs. However, using software instrumentation can introduce serious perturbation of the program being measured. In this paper we present a new data collection cost system that provides programmers with feedback about the impact data collection is having on their application. In addition, we introduce a technique that permits programmers to define the perturbation their application can tolerate and then we are able to regulate the amount of instrumentation to ensure that threshold is not exceeded. We also describe an implementation of the cost model and presents results from using it to measure the instrumentation overhead for several real applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> ``System Performance Evaluation Cooperative,'' </author> <title> Capacity Management Review, </title> <journal> vol. </journal> <volume> 21, no. 8, </volume> <pages> pp. 4-12, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Evaluation of the Cost System Once we had implemented the observed cost system, we were interested in seeing how well it tracked with the actual perturbation of applications. To investigate this, we ran three sequential programs from the floating point SPEC92 benchmark suite <ref> [1] </ref>, and one parallel application. For each test program program, we recorded three cost measures: T no_inst : the user CPU time needed to run the application program with no instrumentation, as measured by UNIX timing commands.
Reference: 2. <author> Jeffrey K. Hollingsworth and Barton P. Miller, </author> <title> ``Dynamic Control of Performance Monitoring on Large Scale Parallel Systems,'' </title> <booktitle> 7th ACM International Conf. on Supercomputing, </booktitle> <pages> pp. 185-194, </pages> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Second, there is one mini-trampoline for each call to the instrumentation library. The second topic in performance measurement that we have been investigating is to help programmers to make sense out of the collected performance data. The W 3 Search Model <ref> [2] </ref>, is methodology that provides a structured way for programmers to quickly and precisely isolate a performance problem without having to examine a large amount of extraneous information. <p> However, changing the threshold does not change what - -- hypotheses get tested; it simply changes when they get tested. 5. Implementation We now describe an initial implementation of the cost model. We added the cost model to the Paradyn Parallel Performance Tools <ref> [2, 3] </ref>. Paradyn runs on a network of workstations (running PVM), or on parallel hardware such as the Thinking Machines CM-5. We also describe the results of a case study we conducted to measure the accuracy of both the observed and predicted cost models.
Reference: 3. <author> Jeffrey K. Hollingsworth, Barton P. Miller, and Jon Cargille, </author> <title> ``Dynamic Program Instrumentation for Scalable Performance Tools,'' </title> <booktitle> 1994 Scalable High-Performance Computing Conf., </booktitle> <address> Knoxville, Tenn., </address> <year> 1994. </year>
Reference-contexts: Government - -- The best way to handle instrumentation overhead is to avoid introducing it in the first place. In a previous paper <ref> [3] </ref>, we described a new approach to performance monitoring called Dynamic Instrumentation. Dynamic Instrumentation delays instrumenting an application program until it is in execution, permitting dynamic insertion and alteration of the instrumentation during program execution. <p> The sum of this information for all points is the predicted cost for an instrumentation request. Metric definitions are used to enumerate what instrumentation primitives and predicates need to be inserted and where. Based on measurements of Dynamic Instrumentation <ref> [3] </ref>, we know the precise cost of each instrumentation primitive and trampoline request. The difficult part is estimating the frequency of execution of each point. <p> However, changing the threshold does not change what - -- hypotheses get tested; it simply changes when they get tested. 5. Implementation We now describe an initial implementation of the cost model. We added the cost model to the Paradyn Parallel Performance Tools <ref> [2, 3] </ref>. Paradyn runs on a network of workstations (running PVM), or on parallel hardware such as the Thinking Machines CM-5. We also describe the results of a case study we conducted to measure the accuracy of both the observed and predicted cost models.
Reference: 4. <author> Al Malony, </author> <title> Performance Observability, </title> <type> PhD Dissertation, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> OCT </month> <year> 1990. </year>
Reference-contexts: Related Work A related topic to our two part cost model is the work that has been done on perturbation compensation <ref> [4, 8] </ref>. The goal of perturbation compensation is to reconstruct the performance of an unperturbed execution from a perturbed one. These techniques generally require a trace based instrumentation system and post-mortem analysis to reconstruct the correct ordering of events.
Reference: 5. <author> Daniel A. Reed, Ruth A. Aydt, Roger J. Noe, Phillip C. Roth, Keith A. Shields, Bradley W. Schwartz, and Luis F. Tavera, </author> <title> ``Scalable Performance Analysis: The Pablo Performance Analysis Environment,'' in Scalable Parallel Libraries Conference, </title> <editor> ed. Anthony Skjellum, </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year>
Reference-contexts: Our approach differs in that we do not try to factor out perturbation; instead we try to avoid it with the Predicted Cost Model, and quantify it using the Observed Cost Model. A second area of related work is Pablo's <ref> [5] </ref> adaptive instrumentation system. In Pablo, the programmer specifies the events to be recorded in an event log for post mortem analysis. However, if during the program's execution, the volume of data collected exceeds certain thresholds, the system will fall back from producing event logs to producing summary information.
Reference: 6. <author> Ko-Yang Wang, </author> <title> ``Precise Compile-Time Performance Prediction of Superscalar-Based Computers,'' </title> <booktitle> ACM SIGPLAN'94 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 73-84, </pages> <address> Orlando, FL, </address> <month> June 20-24, </month> <year> 1994. </year>
Reference-contexts: An alternative would be to analyze the instrumentation sequences to develop a more precise estimate of the number of cycles required for each instrumentation block. To do this, we could use Wang's framework of modeling instructions <ref> [ 6] </ref> by their functional unit requirements to get a more accurate estimate. To account for cache time, we used measured times for the instrumentation primitives. These measurements resulted in approximately one cache miss per primitive.
Reference: 7. <author> Youfeng Wu and J. R. Larus, </author> <title> ``Static Branch Frequency and Program Profile Analysis,'' </title> <note> 27th IEEE/ACM Inter'l Symposium on Microarchitecture (to appear), </note> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The values we used were 1,000 calls per second for user procedures and 100 calls per second for system and library routines. In the future, we plan to employ more sophisticated prediction techniques such as those developed by Wu and Larus <ref> [7] </ref>. 6. Evaluation of the Cost System Once we had implemented the observed cost system, we were interested in seeing how well it tracked with the actual perturbation of applications. To investigate this, we ran three sequential programs from the floating point SPEC92 benchmark suite [1], and one parallel application.
Reference: 8. <author> Jerry C Yan and S. Listgarten, </author> <title> ``Intrusion Compensation for Performance Evaluation of Parallel Programs on a Multicomputer,'' </title> <booktitle> 6th International Conference on Parallel and Distributed Systems, </booktitle> <address> Louisville, KY, OCT 14-16, </address> <year> 1993. </year> - -- - -- 
Reference-contexts: Related Work A related topic to our two part cost model is the work that has been done on perturbation compensation <ref> [4, 8] </ref>. The goal of perturbation compensation is to reconstruct the performance of an unperturbed execution from a perturbed one. These techniques generally require a trace based instrumentation system and post-mortem analysis to reconstruct the correct ordering of events.
References-found: 8

