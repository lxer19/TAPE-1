URL: http://www.cse.unsw.edu.au/~cs9417/KohaviIJCAI95.ps
Refering-URL: http://www.cse.unsw.edu.au/~cs9417/
Root-URL: http://www.cse.unsw.edu.au
Email: ronnyk@CS.Stanford.EDU  
Title: A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection  
Author: Ron Kohavi 
Web: http://robotics.stanford.edu/~ronnyk  
Address: Stanford, CA. 94305  
Affiliation: Computer Science Department Stanford University  
Date: 1995  
Note: Appears in the International Joint Conference on Artificial Intelligence (IJCAI),  
Abstract: We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leave-one-out cross-validation. We report on a large-scale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the effects of different parameters on these algorithms on real-world datasets. For cross-validation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bailey, T. L. & Elkan, C. </author> <year> (1993), </year> <title> Estimating the accuracy of learned concepts, </title> <booktitle> in "Proceedings of International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pp. 895-900. </pages>
Reference: <author> Breiman, L. & Spector, P. </author> <year> (1992), </year> <title> "Submodel selection and evaluation in regression. the x-random case", </title> <journal> International Statistical Review 60(3), </journal> <pages> 291-319. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: We conclude with a summary in Section 6. 2 Methods for Accuracy Estimation A classifier is a function that maps an unlabelled instance to a label using internal data structures. An inducer, or an induction algorithm, builds a classifier from a given dataset. CART and C4.5 <ref> (Breiman, Friedman, Olshen & Stone 1984, Quinlan 1993) </ref> are decision tree inducers that build decision tree classifiers. In this paper, we are not interested in the specific method for inducing classifiers, but assume access to a dataset and an inducer of interest.
Reference: <author> Efron, B. </author> <year> (1983), </year> <title> "Estimating the error rate of a prediction rule: improvement on cross-validation", </title> <journal> Journal of the American Statistical Association 78(382), </journal> <pages> 316-330. </pages>
Reference-contexts: Recent results, both theoretical and experimental, have shown that it is not always the case that increasing the computational cost is beneficial, especially if the relative accuracies are more important than the exact values. For example, leave-one-out is almost unbiased, but it has high variance, leading to unreliable estimates <ref> (Efron 1983) </ref>. For linear models, using leave-one-out cross-validation for model selection is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive power does not converge to one as the total number of observations approaches infinity (Zhang 1992, Shao 1993).
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993), </year> <title> An introduction to the bootstrap, </title> <publisher> Chapman & Hall. </publisher>
Reference: <author> Jain, A. K., Dubes, R. C. & Chen, C. </author> <year> (1987), </year> <title> "Bootstrap techniques for error estimation", </title> <journal> IEEE transactions on pattern analysis and machine intelligence PAMI-9(5), </journal> <pages> 628-633. </pages>
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in "Tools with Artificial Intelligence", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/mlc/toolsmlc.ps. </note>
Reference-contexts: The C4.5 algorithm (Quinlan 1993) is a descendent of ID3 that builds decision trees top-down. The Naive-Bayesian classifier (Langley, Iba & Thompson 1992) used was the one implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref> that uses the observed ratios for nominal features and assumes a Gaussian distribution for continuous features.
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of bayesian classifiers, </title> <booktitle> in "Proceedings of the tenth national conference on artificial intelligence", </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference-contexts: The C4.5 algorithm (Quinlan 1993) is a descendent of ID3 that builds decision trees top-down. The Naive-Bayesian classifier <ref> (Langley, Iba & Thompson 1992) </ref> used was the one implemented in MLC ++ (Kohavi, John, Long, Manley & Pfleger 1994) that uses the observed ratios for nominal features and assumes a Gaussian distribution for continuous features.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1995), </year> <note> UCI repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: To choose a set of datasets, we looked at the learning curves for C4.5 and Naive-Bayes for most of the supervised classification datasets at the UC Irvine repository <ref> (Murphy & Aha 1995) </ref> that contained more than 500 instances (about 25 such datasets). We felt that a minimum of 500 instances were required for testing.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California. </address>
Reference-contexts: The C4.5 algorithm <ref> (Quinlan 1993) </ref> is a descendent of ID3 that builds decision trees top-down.
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 259-265. </pages>
Reference-contexts: On one dataset, vehicle, the generalization accuracy of the Naive-Bayes algorithm deteriorated by more than 4% as more instances were given. A similar phenomenon was observed on the shuttle dataset. Such a phenomenon was predicted by Schaffer and Wolpert <ref> (Schaffer 1994, Wolpert 1994b) </ref>, but we were surprised that it was observed on two real-world datasets. To see how well an accuracy estimation method performs, we sampled instances from the dataset (uniformly without replacement), and created a training set of the desired size.
Reference: <author> Shao, J. </author> <year> (1993), </year> <title> "Linear model selection via cross-validation", </title> <journal> Journal of the American Statistical Association 88(422), </journal> <pages> 486-494. </pages>
Reference: <author> Weiss, S. M. </author> <year> (1991), </year> <title> "Small sample error rate estimation for k-nearest neighbor classifiers", </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13(3), </journal> <pages> 285-289. </pages>
Reference: <author> Weiss, S. M. & Indurkhya, N. </author> <year> (1994), </year> <title> Decision tree pruning : Biased or optimal, </title> <booktitle> in "Proceedings of the twelfth national conference on artificial intelligence", </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 626-632. </pages>
Reference-contexts: Bailey & Elkan (1993) compared leave-one-out cross-validation to .632 bootstrap using the FOIL inducer and four synthetic datasets involving Boolean concepts. They observed high variability and little bias in the leave-one-out estimates, and low variability but large bias in the .632 estimates. Weiss and Indurkyha <ref> (Weiss & Indurkhya 1994) </ref> conducted experiments on real-world data to determine the applicability of cross-validation to decision tree pruning.
Reference: <author> Wolpert, D. H. </author> <year> (1992), </year> <title> "Stacked generalization", </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. </pages>
Reference-contexts: being made that one should not use cross-validation in the real world. |Wolpert (1994a) Estimating the accuracy of a classifier induced by supervised learning algorithms is important not only to predict its future prediction accuracy, but also for choosing a classifier from a given set (model selection), or combining classifiers <ref> (Wolpert 1992) </ref>. For estimating the final accuracy of a classifier, we would like an estimation method with low bias and low variance.
Reference: <author> Wolpert, D. H. </author> <year> (1994a), </year> <title> Off-training set error and a priori distinctions between learning algorithms, </title> <type> Technical Report SFI TR 94-12-123, </type> <institution> The Sante Fe Institute. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1994b), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <type> Technical report, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe, NM. </address>
Reference-contexts: In this paper we explain some of the assumptions made by the different estimation methods, and present concrete examples where each method fails. While it is known that no accuracy estimation can be correct all the time <ref> (Wolpert 1994b, Schaffer 1994) </ref>, we are interested in identifying a method that is well suited for the biases and trends in typical real world datasets.
Reference: <author> Zhang, P. </author> <year> (1992), </year> <title> "On the distributional properties of model selection criteria", </title> <journal> Journal of the American Statistical Association 87(419), </journal> <pages> 732-737. </pages>
Reference-contexts: For linear models, using leave-one-out cross-validation for model selection is asymptotically inconsistent in the sense that the probability of selecting the model with the best predictive power does not converge to one as the total number of observations approaches infinity <ref> (Zhang 1992, Shao 1993) </ref>. This paper is organized as follows. Section 2 describes the common accuracy estimation methods and ways of computing confidence bounds that hold under some assumptions. Section 3 discusses related work comparing cross-validation variants and bootstrap variants. Section 4 discusses methodology underlying our experiment.
References-found: 18

