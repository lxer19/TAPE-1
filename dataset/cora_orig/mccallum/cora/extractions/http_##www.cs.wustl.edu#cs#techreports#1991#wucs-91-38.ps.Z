URL: http://www.cs.wustl.edu/cs/techreports/1991/wucs-91-38.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: sg@cs.wustl.edu  mkearns@icsi.berkeley.edu  schapire@das.harvard.edu  
Title: Exact Identification of Read-once Formulas Using Fixed Points of Amplification Functions  
Author: Sally A. Goldman Michael J. Kearns Robert E. Schapire 
Date: June 26, 1991  
Address: St. Louis, MO 63130  1947 Center Street, Suite 600 Berkeley, CA 94704  Cambridge, MA 02138  
Affiliation: Department of Computer Science Washington University  International Computer Science Institute  Harvard University Aiken Computation Laboratory  
Pubnum: WUCS-91-38  
Abstract: In this paper we describe a new technique for exactly identifying certain classes of read-once Boolean formulas. The method is based on sampling the input-output behavior of the target formula on a probability distribution which is determined by the fixed point of the formula's amplification function (defined as the probability that a 1 is output by the formula when each input bit is 1 independently with probability p). By performing various statistical tests on easily sampled variants of the fixed-point distribution, we are able to efficiently infer all structural information about any logarithmic-depth formula (with high probability). We apply our results to prove the existence of short universal identification sequences for large classes of formulas. We also describe extensions of our algorithms to handle high rates of noise, and to learn formulas of unbounded depth in Valiant's model with respect to specific distributions. fl Most of this research was carried out while all three authors were at M.I.T. Laboratory for Computer Science. Support was provided by NSF Grant CCR-88914428, ARO Grant DAAL03-86-K-0171, DARPA Contract N00014-89-J-1988, and a grant from the Siemens Corporation. An extended abstract of this paper appeared in the proceedings of the 31st Annual Symposium on Foundations of Computer Science. y Supported in part by a G.E. Foundation Junior Faculty Grant. z Supported by AFOSR Grant AFOSR-89-0506. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin, Lisa Hellerstein, and Marek Karpinski. </author> <title> Learning read-once formulas with queries. </title> <type> Technical Report UCB/CSD 89/528, </type> <institution> University of California Berkeley, Computer Science Division, </institution> <month> August </month> <year> 1989. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: For Boolean read-once formulas (a superset of the class of formulas constructed from nand gates) there is an efficient, exact-identification algorithm using membership and equivalence queries due to Angluin, Hellerstein and Karpinski <ref> [1, 8] </ref>. The class of read-once ma 2 jority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock [7] and Hellerstein and Karpinski [9].
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: We also prove that our algorithms are robust against a large amount of random misclas-sification noise, similar to, but slightly more general than that considered by Sloan [19] and Angluin and Laird <ref> [2] </ref>. <p> Although omitted, a similar (though slightly more involved) algorithm can be derived for nand formulas. Our algorithm is able to handle a kind of random misclassification noise which is similar, but slightly more general than that considered by Angluin and Laird <ref> [2] </ref>, and Sloan [19]. Specifically, the output of the target formula is "flipped" with some fixed probability which may depend on the formula's output.
Reference: [3] <author> Ravi B. Boppana. </author> <title> Amplification of probabilistic Boolean formulas. </title> <booktitle> In 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 20-29, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Amplification functions were first studied by Valiant [20] and Boppana <ref> [3, 4] </ref> in obtaining bounds on monotone formula size for the majority function. The method used by our algorithms is of central interest. <p> For example, his result demonstrates that the class of read-once Boolean formulas over the usual basis can be learned in polynomial time against the uniform distribution in the sense of Valiant. 2 Preliminaries Given a Boolean function f : f0; 1g n ! f0; 1g, Boppana <ref> [3, 4] </ref> defines its amplification function A f as follows: A f (p) = Pr [f (X 1 ; . . . ; X n ) = 1], where X 1 ; . . . ; X n are independent Bernoulli variables that are each 1 with probability p.
Reference: [4] <author> Ravi Babu Boppana. </author> <title> Lower Bounds for Monotone Circuits and Formulas. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: Amplification functions were first studied by Valiant [20] and Boppana <ref> [3, 4] </ref> in obtaining bounds on monotone formula size for the majority function. The method used by our algorithms is of central interest. <p> For example, his result demonstrates that the class of read-once Boolean formulas over the usual basis can be learned in polynomial time against the uniform distribution in the sense of Valiant. 2 Preliminaries Given a Boolean function f : f0; 1g n ! f0; 1g, Boppana <ref> [3, 4] </ref> defines its amplification function A f as follows: A f (p) = Pr [f (X 1 ; . . . ; X n ) = 1], where X 1 ; . . . ; X n are independent Bernoulli variables that are each 1 with probability p.
Reference: [5] <author> Merrick Furst, Jeffrey Jackson, and Sean Smith. </author> <title> Learning AC 0 functions sampled under mutually independent distributions. </title> <type> Technical Report CMU-CS-90-183, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Also, Linial, Mansour and Nisan [14] used a technique based on Fourier spectra to learn the class of constant-depth formulas (constructed from gates of unbounded fan-in) against the uniform distribution. Furst, Jackson and Smith <ref> [5] </ref> generalized this result to learn this same class against any product distribution (i.e., any distribution in which the setting of each variable is chosen independently of the settings of the other variables). Verbeurgt [21] gives a different algorithm for learning DNF-formulas against the uniform distribution.
Reference: [6] <author> Thomas Hancock and Yishay Mansour. </author> <title> Learning monotone k DNF formulas on product distributions. </title> <booktitle> In Computation Learning Theory: Proceedings of the Fourth Annual Workshop, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: A similar result, though based on a different method, was obtained by Pagallo and Haussler [15]. These results were extended by Hancock and Mansour <ref> [6] </ref>, and by Schapire [18] as described below. Also, Linial, Mansour and Nisan [14] used a technique based on Fourier spectra to learn the class of constant-depth formulas (constructed from gates of unbounded fan-in) against the uniform distribution.
Reference: [7] <author> Thomas R. Hancock. </author> <title> Identifying -formula decision trees with queries. </title> <type> Technical Report TR-16-90, </type> <institution> Harvard University, Center for Research in Computing Technology, </institution> <year> 1990. </year> <month> 31 </month>
Reference-contexts: The class of read-once ma 2 jority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock <ref> [7] </ref> and Hellerstein and Karpinski [9].
Reference: [8] <author> Lisa Hellerstein. </author> <title> On Characterizing and Learning Some Classes of Read-Once Formulas. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1989. </year>
Reference-contexts: For Boolean read-once formulas (a superset of the class of formulas constructed from nand gates) there is an efficient, exact-identification algorithm using membership and equivalence queries due to Angluin, Hellerstein and Karpinski <ref> [1, 8] </ref>. The class of read-once ma 2 jority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock [7] and Hellerstein and Karpinski [9].
Reference: [9] <author> Lisa Hellerstein and Marek Karpinski. </author> <title> Read-once formulas over different bases. </title> <type> Technical Report 8556-CS, </type> <institution> University of Bonn, </institution> <year> 1990. </year>
Reference-contexts: The class of read-once ma 2 jority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock [7] and Hellerstein and Karpinski <ref> [9] </ref>. Briefly, in the query model, the learner attempts to infer the target formula by asking questions, or queries, of a "teacher." For instance, the learner might ask the teacher what the formula's output would be for a specific assignment to the input variable; this is called a membership query.
Reference: [10] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Thus, if one estimates the value of the amplification function from a sample whose size is polynomial in 2 h , then with high probability one can determine which variables are relevant, as well as the sign and level of every relevant variable. Specifically, we can apply Chernoff bounds <ref> [10] </ref> to derive a sample size sufficient to ensure that all the above information is properly computed with high probability. We therefore assume henceforth that the level of every variable has been determined, and that (without loss of generality) all variables are relevant and unnegated. <p> The sample size required can be computed using Hoeffding's inequality <ref> [10] </ref> (also known as a form of Chernoff bounds) which is stated below: Lemma 3.9 (Hoeffding's Inequality) Let X 1 ; . . . ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p so that E [X i ] = p.
Reference: [11] <author> Michael Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Thus, for these classes, since the fixed point of the amplification function is the same for all formulas, we obtain a single simple distribution for the entire class. As proved by Kearns and Valiant <ref> [13, 11] </ref>, these same classes of formulas cannot be even weakly approximated in polynomial time when no restriction is placed on the target distribution; thus, our results may be interpreted as demonstrating that while there are some distributions which in a computationally bounded setting reveal essentially no information about the target
Reference: [12] <author> Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 285-295, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: A similar result also holds for read-once nand formulas of unbounded depth. 3 The problem of learning Boolean formulas against special distributions has been con-sidered by a number of other authors. In particular, our technique closely resembles that used by Kearns et al. <ref> [12] </ref> for learning the class of read-once formulas in disjunctive normal form (DNF) against the uniform distribution. A similar result, though based on a different method, was obtained by Pagallo and Haussler [15]. These results were extended by Hancock and Mansour [6], and by Schapire [18] as described below. <p> The resulting majority formula can further be reduced to one that is read-once using the substitution method of Kearns et al. <ref> [12] </ref>. Finally, combined with Kearns and Valiant's result that Boolean formulas are not learnable (modulo cryptographic assumptions), this shows that majority formulas are also unlearnable.
Reference: [13] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Thus, for these classes, since the fixed point of the amplification function is the same for all formulas, we obtain a single simple distribution for the entire class. As proved by Kearns and Valiant <ref> [13, 11] </ref>, these same classes of formulas cannot be even weakly approximated in polynomial time when no restriction is placed on the target distribution; thus, our results may be interpreted as demonstrating that while there are some distributions which in a computationally bounded setting reveal essentially no information about the target <p> It can be shown that the class of logarithmic-depth read-once majority formulas is not learnable in the distribution-free model (modulo some cryptographic assumptions; see Kearns and Valiant <ref> [13] </ref> for details). Briefly, this can be proved using a Pitt and Warmuth-style "prediction-preserving reduction" [16] to show that learning read-once majority formulas is at least as hard as learning general Boolean formulas.
Reference: [14] <author> Nathan Linial, Yishay Mansour, and Noam Nisan. </author> <title> Constant depth circuits, Fourier transform, and learnability. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 574-579, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: A similar result, though based on a different method, was obtained by Pagallo and Haussler [15]. These results were extended by Hancock and Mansour [6], and by Schapire [18] as described below. Also, Linial, Mansour and Nisan <ref> [14] </ref> used a technique based on Fourier spectra to learn the class of constant-depth formulas (constructed from gates of unbounded fan-in) against the uniform distribution.
Reference: [15] <author> Giulia Pagallo and David Haussler. </author> <title> A greedy method for learning DNF functions under the uniform distribution. </title> <type> Technical Report UCSC-CRL-89-12, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: In particular, our technique closely resembles that used by Kearns et al. [12] for learning the class of read-once formulas in disjunctive normal form (DNF) against the uniform distribution. A similar result, though based on a different method, was obtained by Pagallo and Haussler <ref> [15] </ref>. These results were extended by Hancock and Mansour [6], and by Schapire [18] as described below. Also, Linial, Mansour and Nisan [14] used a technique based on Fourier spectra to learn the class of constant-depth formulas (constructed from gates of unbounded fan-in) against the uniform distribution.
Reference: [16] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: It can be shown that the class of logarithmic-depth read-once majority formulas is not learnable in the distribution-free model (modulo some cryptographic assumptions; see Kearns and Valiant [13] for details). Briefly, this can be proved using a Pitt and Warmuth-style "prediction-preserving reduction" <ref> [16] </ref> to show that learning read-once majority formulas is at least as hard as learning general Boolean formulas.
Reference: [17] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: identification of read-once majority formulas In this section we use properties of amplification functions to obtain a polynomial-time algorithm that with high probability exactly identifies any read-once majority formula of logarithmic depth from random examples drawn according to a uniform distribution. 6 This type of formula is used by Schapire <ref> [17] </ref> in his proof that a concept class is weakly learnable in polynomial time if and only if it is strongly learnable in polynomial time.
Reference: [18] <author> Robert E. Schapire. </author> <title> Learning probabilistic read-once formulas on product distributions. </title> <booktitle> In Computation Learning Theory: Proceedings of the Fourth Annual Workshop, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: A similar result, though based on a different method, was obtained by Pagallo and Haussler [15]. These results were extended by Hancock and Mansour [6], and by Schapire <ref> [18] </ref> as described below. Also, Linial, Mansour and Nisan [14] used a technique based on Fourier spectra to learn the class of constant-depth formulas (constructed from gates of unbounded fan-in) against the uniform distribution. <p> Verbeurgt [21] gives a different algorithm for learning DNF-formulas against the uniform distribution. However, all three of these algorithms require quasi-polynomial (n polylog (n) ) time, though Verbeurgt's procedure only requires a polynomial-size sample. Finally, Schapire <ref> [18] </ref> has recently applied our technique to a probabilistic generalization of the class of all read-once Boolean formulas constructed from the usual basis fand; or; notg. He shows that an arbitrarily good approximation of such formulas can be inferred in polynomial time against any product distribution.
Reference: [19] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: We also prove that our algorithms are robust against a large amount of random misclas-sification noise, similar to, but slightly more general than that considered by Sloan <ref> [19] </ref> and Angluin and Laird [2]. <p> Although omitted, a similar (though slightly more involved) algorithm can be derived for nand formulas. Our algorithm is able to handle a kind of random misclassification noise which is similar, but slightly more general than that considered by Angluin and Laird [2], and Sloan <ref> [19] </ref>. Specifically, the output of the target formula is "flipped" with some fixed probability which may depend on the formula's output.
Reference: [20] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Amplification functions were first studied by Valiant <ref> [20] </ref> and Boppana [3, 4] in obtaining bounds on monotone formula size for the majority function. The method used by our algorithms is of central interest. <p> The quantity A f (p) is called the amplification of f at p. Valiant <ref> [20] </ref> uses properties of the amplification function to prove the existence of monotone Boolean formulas of size O (n 5:3 ) for the majority function on n inputs. <p> In this setting polynomial time means polynomial in n and 1=ffi. Our algorithms achieve exact identification with high probability when the example source is a particular, fixed distribution. In the distribution-free or probably approximately correct (PAC) learning model, introduced by Valiant <ref> [20] </ref>, the learner is given access to labeled (positive and negative) examples of the target concept, drawn randomly according to some unknown target distribution D. The learner is also given as input *; ffi &gt; 0.
Reference: [21] <author> Karsten Verbeurgt. </author> <title> Learning DNF under the uniform distribution in quasi-polynomial time. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 314-326, </pages> <month> August </month> <year> 1990. </year> <month> 33 </month>
Reference-contexts: Furst, Jackson and Smith [5] generalized this result to learn this same class against any product distribution (i.e., any distribution in which the setting of each variable is chosen independently of the settings of the other variables). Verbeurgt <ref> [21] </ref> gives a different algorithm for learning DNF-formulas against the uniform distribution. However, all three of these algorithms require quasi-polynomial (n polylog (n) ) time, though Verbeurgt's procedure only requires a polynomial-size sample.
References-found: 21

