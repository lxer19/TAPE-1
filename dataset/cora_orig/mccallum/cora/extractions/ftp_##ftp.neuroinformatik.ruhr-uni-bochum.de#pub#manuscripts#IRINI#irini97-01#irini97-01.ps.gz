URL: ftp://ftp.neuroinformatik.ruhr-uni-bochum.de/pub/manuscripts/IRINI/irini97-01/irini97-01.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00234.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: LBG-U method for vector quantization an improvement over LBG inspired from neural networks  
Author: by Bernd Fritzke Ruhr-Universitat Bochum 
Date: January 1997  
Address: 44780 Bochum  
Affiliation: Institut fur Neuroinformatik  
Note: The  ISSN 0943-2752 c 1997 Kluwer Academic Publishers  
Pubnum: IR-INI 97-01  
Abstract: Internal Report 97-01 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Fritzke. </author> <title> Growing cell structures a self-organizing network for unsupervised and supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9) </volume> <pages> 1441-1460, </pages> <year> 1994. </year>
Reference-contexts: In particular, accumulated local information (distortion error and utility in the case of LBG-U) has been used earlier in the context of growing self-organizing models to guide insertion 8 IR-INI 97-01, c fl 1997 Kluwer Academic Publishers and deletion of vectors <ref> [1, 2, 3] </ref>. Moreover, a move of a vector as performed in LBG-U can be interpreted as a simultaneous deletion and insertion of a vector creating another link to these neural network models. The utility measure proposed in this article is completely novel, however.
Reference: [2] <author> B. Fritzke. </author> <title> A growing neural gas network learns topologies. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 625-632. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year>
Reference-contexts: good codebooks is the main point of this paper. 2 The LBG Method A large number of codebook construction methods exist such as various flavors of k-means [7] and different neural network models (e.g., the self-organizing feature map [5], the neural gas method [8] or the growing neural gas method <ref> [2] </ref>). The most well-known approach, however, is probably the LBG algorithm [6] (named after its authors Linde, Buzo and Gray). Since LBG is the starting point of our new method we describe it in the following. <p> In particular, accumulated local information (distortion error and utility in the case of LBG-U) has been used earlier in the context of growing self-organizing models to guide insertion 8 IR-INI 97-01, c fl 1997 Kluwer Academic Publishers and deletion of vectors <ref> [1, 2, 3] </ref>. Moreover, a move of a vector as performed in LBG-U can be interpreted as a simultaneous deletion and insertion of a vector creating another link to these neural network models. The utility measure proposed in this article is completely novel, however.
Reference: [3] <author> B. Fritzke. </author> <title> Growing grid a self-organizing network with constant neighborhood range and adaptation strength. </title> <journal> Neural Processing Letters, </journal> <volume> 2(5) </volume> <pages> 9-13, </pages> <year> 1995. </year>
Reference-contexts: In particular, accumulated local information (distortion error and utility in the case of LBG-U) has been used earlier in the context of growing self-organizing models to guide insertion 8 IR-INI 97-01, c fl 1997 Kluwer Academic Publishers and deletion of vectors <ref> [1, 2, 3] </ref>. Moreover, a move of a vector as performed in LBG-U can be interpreted as a simultaneous deletion and insertion of a vector creating another link to these neural network models. The utility measure proposed in this article is completely novel, however.
Reference: [4] <author> R. M. Gray. </author> <title> Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-29, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Vector quantization (VQ) <ref> [4] </ref> is an important and powerful technique for data compression. VQ is applied, for instance, for storage of large amounts of data or for transmission of data over communication channels of limited bandwidth.
Reference: [5] <author> T. Kohonen. </author> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: A new method to find good codebooks is the main point of this paper. 2 The LBG Method A large number of codebook construction methods exist such as various flavors of k-means [7] and different neural network models (e.g., the self-organizing feature map <ref> [5] </ref>, the neural gas method [8] or the growing neural gas method [2]). The most well-known approach, however, is probably the LBG algorithm [6] (named after its authors Linde, Buzo and Gray). Since LBG is the starting point of our new method we describe it in the following.
Reference: [6] <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Transactions on Communication, </journal> <volume> COM-28:84-95, </volume> <year> 1980. </year>
Reference-contexts: The most well-known approach, however, is probably the LBG algorithm <ref> [6] </ref> (named after its authors Linde, Buzo and Gray). Since LBG is the starting point of our new method we describe it in the following. <p> LBG is guaranteed to decrease the distortion error E (D; C) in each Lloyd iteration or to leave it unchanged, in which case it has converged. The final codebook corresponds to a local minimum of the distortion error function <ref> [6] </ref>. IR-INI 97-01, c fl 1997 Kluwer Academic Publishers 3 3 The Utility Measure For the following considerations we assume that a codebook C has been constructed by LBG. In general this codebook will not be globally optimal.
Reference: [7] <author> J. MacQueen. </author> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> volume 1 of Proceedings of the Fifth Berkeley Symposium on Mathematical statistics and probability, </booktitle> <pages> pages 281-297, </pages> <address> Berkeley, </address> <year> 1967. </year> <institution> University of California Press. </institution>
Reference-contexts: A new method to find good codebooks is the main point of this paper. 2 The LBG Method A large number of codebook construction methods exist such as various flavors of k-means <ref> [7] </ref> and different neural network models (e.g., the self-organizing feature map [5], the neural gas method [8] or the growing neural gas method [2]). The most well-known approach, however, is probably the LBG algorithm [6] (named after its authors Linde, Buzo and Gray).
Reference: [8] <author> T. M. Martinetz and K. J. Schulten. </author> <title> A "neural-gas" network learns topologies. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 397-402. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: A new method to find good codebooks is the main point of this paper. 2 The LBG Method A large number of codebook construction methods exist such as various flavors of k-means [7] and different neural network models (e.g., the self-organizing feature map [5], the neural gas method <ref> [8] </ref> or the growing neural gas method [2]). The most well-known approach, however, is probably the LBG algorithm [6] (named after its authors Linde, Buzo and Gray). Since LBG is the starting point of our new method we describe it in the following.
References-found: 8

