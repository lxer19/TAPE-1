URL: http://www.cs.indiana.edu/hyplan/bramley/cimgs.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/bramley.html
Root-URL: http://www.cs.indiana.edu
Title: CIMGS: AN INCOMPLETE ORTHOGONAL FACTORIZATION PRECONDITIONER  
Author: XIAOGE WANG KYLE A. GALLIVAN AND RANDALL BRAMLEY 
Keyword: Key words. preconditioner, iterative method, incomplete factorization, incomplete orthogonal-ization, M-matrix, symmetric positive definite systems, least squares problems  
Note: AMS(MOS) subject classifications. 65F10, 65F20, 65F25, 65F50, 65G05, 65Y20  
Abstract: A new preconditioner for symmetric positive definite systems is proposed, analyzed, and tested. The preconditioner, Compressed Incomplete Modified Gram Schmidt (CIMGS), is based on an incomplete orthogonal factorization. CIMGS is robust both theoretically and empirically, existing (in exact arithmetic) for any full rank matrix. Numerically it is more robust than an incomplete Cholesky factorization preconditioner (IC) and a complete Cholesky factorization of the normal equations. Theoretical results show that the CIMGS factorization has better backward error properties than complete Cholesky factorization. For symmetric positive definite M-matrices, CIMGS induces a regular splitting and better estimates the complete Cholesky factor as the set of dropped positions gets smaller. CIMGS lies between complete Cholesky factorization and incomplete Cholesky factorization in its approximation properties. These theoretical properties usually hold numerically, even when the matrix is not an M-matrix. When the drop set satisfies a mild and easily verified (or enforced) property, the upper triangular factor CIMGS generates is the same as that generated by incomplete Cholesky factorization. This allows the existence of the IC factorization to be guaranteed, based solely on the target sparsity pattern. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Ashby, </author> <title> Polynomial Preconditioning for Conjugate Gradient Methods, </title> <type> PhD thesis, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1987. </year> <note> Also available as Tech. Rep. 1355, </note> <institution> Department of Computer Science, University of Illinois - Urbana. </institution> <note> [2] , Minimax polynomial preconditioning for Hermitian linear systems, SIAM J. Mat. Anal. Appl., </note> <month> 12 </month> <year> (1991), </year> <pages> pp. 766-789. </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning <ref> [1, 2] </ref>, and incomplete orthogonal factorization [14, 10, 18, 16]. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1.
Reference: [3] <author> M. W. Berry and R. J. Plemmons, </author> <title> Algorithms and experiments for structural mechanics on high-performance architectrues, </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, 64 (1987), </booktitle> <pages> pp. 487-507. </pages>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming [5], augmented Lagrangian methods for CFD [12], and the natural factor method in structural engineering <ref> [9, 3] </ref>. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite. There are many well-developed and reliable methods, both direct and iterative, for solving such systems.
Reference: [4] <author> A. Bj orck, </author> <title> SSOR preconditioning methods for sparse least squares problems, </title> <booktitle> in Proceedings of the Computer Science and Statistics: 12-th Annual Symposium on the Interface, </booktitle> <editor> J. F. Gentleman, ed., </editor> <address> University of Waterloo, Waterloo, Ontario, Canada, </address> <month> May </month> <year> 1979, </year> <pages> pp. 21-25. </pages>
Reference-contexts: Because the rate of convergence of the CG algorithm is related to the condition number of the matrix to which it is applied, finding an effective preconditioner is crucial. Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR <ref> [4] </ref>, incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization [14, 10, 18, 16]. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1.
Reference: [5] <author> I. Chio, C. L. Monma, and D. Shanno, </author> <title> Further development of a primal-dual interior point method, </title> <journal> ORSA Journal on Computing, </journal> <volume> 2 (1990), </volume> <pages> pp. 304-311. </pages>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming <ref> [5] </ref>, augmented Lagrangian methods for CFD [12], and the natural factor method in structural engineering [9, 3]. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite.
Reference: [6] <author> E. Chu, A. George, J. Liu, and E. Ng, SPARSPAK: </author> <title> Waterloo sparse matrix package user's guide for SPARSPAK-A, </title> <type> Tech. Rep. </type> <institution> CS-84-36, Department of Computer Science, University of Waterloo, </institution> <year> 1984. </year>
Reference-contexts: More comprehensive testing which included inconsistent problems [16] has lead to the same conclusions as in this paper. The CIMGS and incomplete Cholesky factorizations used for comparison in the experiments were implemented in standard Fortran. The packages SPARSPAK-A <ref> [6] </ref> used for Cholesky factorization and SPARSPAK-B [7] used for QR factorization are also in Fortran but benefit from the more careful consideration typical of a numerical software package. Floating point operation counts (obtained by instrumenting the Fortran code) are used to compare the efficiency of the various approaches.
Reference: [7] <author> A. George and E. Ng, SPARSPAK: </author> <title> Waterloo sparse matrix package, user's guide for SPARSPAK-B, </title> <type> Tech. Rep. </type> <institution> CS-84-37, Department of Computer Science, University of Waterloo, </institution> <year> 1984. </year>
Reference-contexts: More comprehensive testing which included inconsistent problems [16] has lead to the same conclusions as in this paper. The CIMGS and incomplete Cholesky factorizations used for comparison in the experiments were implemented in standard Fortran. The packages SPARSPAK-A [6] used for Cholesky factorization and SPARSPAK-B <ref> [7] </ref> used for QR factorization are also in Fortran but benefit from the more careful consideration typical of a numerical software package. Floating point operation counts (obtained by instrumenting the Fortran code) are used to compare the efficiency of the various approaches.
Reference: [8] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: A well-known drawback of this approach is that the condition number of the normal equations is the square of the condition number of A. Orthogonal factorization methods <ref> [8] </ref> avoid this problem, but they require more floating point operations and if Q is fl Work supported by NSF grants CDA-9309746 and CCR-9120105 1 required as occurs for systems with sequential multiple right hand sides, they potentially can require O (mn) storage, which is unacceptable for systems with large m.
Reference: [9] <author> M. T. Heath, R. J. Plemmons, and R. C. Ward, </author> <title> Sparse orthogonal schemes for structural optimization using the force method, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5 (1984), </volume> <pages> pp. 514-532. </pages>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming [5], augmented Lagrangian methods for CFD [12], and the natural factor method in structural engineering <ref> [9, 3] </ref>. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite. There are many well-developed and reliable methods, both direct and iterative, for solving such systems.
Reference: [10] <author> A. Jennings and M. A. Ajiz, </author> <title> Incomplete methods for solving A T Ax = b, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 5 (1984), </volume> <pages> pp. 978-987. </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt methods <ref> [14, 10, 16] </ref> which give a factorization A = QR with Q not necessarily orthogonal, and incomplete Givens [18], which produce A QR with Q orthogonal. Incomplete Gram-Schmidt-type methods are in general robust because they can avoid numerical breakdown when A is full rank.
Reference: [11] <author> J. A. Meijerink and H. A. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Mathematics of Computation, </journal> <volume> 31 (1977), </volume> <pages> pp. 148-162. </pages>
Reference-contexts: Because the rate of convergence of the CG algorithm is related to the condition number of the matrix to which it is applied, finding an effective preconditioner is crucial. Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization <ref> [11] </ref>, polynomial preconditioning [1, 2], and incomplete orthogonal factorization [14, 10, 18, 16]. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> For MGS one of the eigenvalues is made exactly 1, and the remaining ones are in an interval bounded by the minimum and maximum eigenvalues of the normal equations of the original matrix. A similar result in <ref> [11] </ref> shows that A T A = LL T ^ E is a regular splitting, where L is the lower triangular IC factor of A T A, ^ E 0, and ae ( ^ E) &lt; 1. It is straightforward to transform this result into one similar to Theorem 4.
Reference: [12] <author> M.Fortin and R. Glowinski, </author> <title> Augmented Lagrangian methods: applications to the numerical solution of boundary-value problems, </title> <publisher> North-Holland, </publisher> <year> 1983. </year>
Reference-contexts: Such problems occur frequently in scientific and engineering applications such as linear programming [5], augmented Lagrangian methods for CFD <ref> [12] </ref>, and the natural factor method in structural engineering [9, 3]. Minimizing (1) by solving the normal equations A T Ax = A T b is a common and often efficient approach, because A T A is symmetric and positive definite.
Reference: [13] <author> C. Paige and M. Saunders, </author> <title> LSQR: An algorithm for sparse linear equations and sparse least squares, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 (1982), </volume> <pages> pp. 43-71. </pages>
Reference-contexts: Floating point operation counts (obtained by instrumenting the Fortran code) are used to compare the efficiency of the various approaches. Although this does not measure potentially important performance features such as pipelining and data locality, it does provide a machine-independent measure. A conjugate gradient iterative method, CGLS <ref> [13] </ref> is used in the experiments as the basic iterative method to solve the least squares problems and, for square matrices, the nonsymmetric linear systems. Although CGLS does not form the normal equations explicitly, its convergence depends on their spectrum.
Reference: [14] <author> Y. Saad, </author> <title> Preconditioning techniques for nonsymmetric and indefinite linear systems, </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 24 (1988), </volume> <pages> pp. 89-105. </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt methods <ref> [14, 10, 16] </ref> which give a factorization A = QR with Q not necessarily orthogonal, and incomplete Givens [18], which produce A QR with Q orthogonal. Incomplete Gram-Schmidt-type methods are in general robust because they can avoid numerical breakdown when A is full rank. <p> One way of reducing computations is to use a numerical dropping technique to keep both the Q and R factors sparse, as is done in ILQ <ref> [14] </ref> and the incomplete Givens method of [18]. The price these methods pay for efficiency is robustness, because dropping small entries can lead to zero elements on the diagonal of R. Restart techniques have to be used for these methods to assure robustness.
Reference: [15] <author> Y. Saad, SPARSKIT: </author> <title> a basic tool kit for sparse matrix computations, </title> <type> tech. rep., </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, Illinois, </institution> <year> 1990. </year>
Reference-contexts: Note that the test problems come from different sources and a single drop tolerance as was used here may not suitable for all problems. Determining the drop tolerance is still a matter of experimentation. CIMGS shares this potential difficulty with other drop tolerance methods such as ILUT preconditioning <ref> [15] </ref>. In general, the more ill conditioned problems are, the smaller * may need to be to get a preconditioner of adequate quality. Our tests show that CIMGS has a great flexibility in pattern selection without losing robustness.
Reference: [16] <author> X. Wang, </author> <title> Incomplete factorization preconditioning for linear least squares problems, </title> <type> PhD thesis, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1993. </year> <note> Also available as Technical Report # UIUCDCS-R-93-1834, </note> <institution> Department of Computer Science, University of Illinois at Urbana - Champaign. </institution>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt methods <ref> [14, 10, 16] </ref> which give a factorization A = QR with Q not necessarily orthogonal, and incomplete Givens [18], which produce A QR with Q orthogonal. Incomplete Gram-Schmidt-type methods are in general robust because they can avoid numerical breakdown when A is full rank. <p> So if A has full rank, r kk 6= 0 for k = 1; 2; : : : ; n and the factorization must exist. 2 More detailed studies of IMGS can be found in <ref> [16] </ref>. In general, IMGS is robust and effective at reducing the number of CG iterations. Its main weakness is the much higher cost of computing the preconditioner compared to other preconditioning methods. <p> arithmetic, the natural next question to ask is how CIMGS is affected by rounding errors, that is, how does the "compression" technique affect the stability of CIMGS? Earlier analysis has shown that IMGS is less likely than modified Gram-Schmidt to break down due to possible numerical rank deficiency of A <ref> [16] </ref>. The next Theorem bounds the rounding error incurred by CIMGS and shows that CIMGS is less likely to suffer numerical breakdown than complete Cholesky factorization is. The quantity min (G) denotes the eigenvalue of minimal modulus for the matrix G. 5 Theorem 3. <p> When A T A is an M-matrix, we have the following result, the proof of which can be found in <ref> [16] </ref>: Theorem 4. Let A 2 R mfin have full rank. <p> Furthermore, it is easy to modify the target sparsity pattern in order to satisfy the hypothesis of Theorem 7; see <ref> [16] </ref> for more details. In general, CIMGS gives a different factor R from the one given by IC. <p> To establish this result, two lemmas are stated here. Their proofs can be found in <ref> [16] </ref>. Lemma 3.1. Let A; B 2 R nfin be symmetric positive definite M-matrices with A B. Then R T , where R and T are the Cholesky factors of A and B respectively. Lemma 3.2. Let A; B 2 R nfin be symmetric positive definite M-matrices, with A B. <p> This allows checking the accuracy of the methods using both the residual vector norm k Ax b k 2 and the error vector norm k x x fl k 2 . More comprehensive testing which included inconsistent problems <ref> [16] </ref> has lead to the same conclusions as in this paper. The CIMGS and incomplete Cholesky factorizations used for comparison in the experiments were implemented in standard Fortran. <p> On the other hand, not all of the elements in P will affect the final value of R and computing them involves a compute-then-drop operation, which wastes time and space. A detailed discussion of an algorithm that identifies these unnecessary computations by symbolic analysis can be found in <ref> [16] </ref>. The data shown here includes such unnecessary computations, but it should be kept in mind that performance may be improved further given an efficient symbolic analysis algorithm that identifies and eliminates unnecessary computations. Table 2 shows two sets of comparisons.
Reference: [17] <author> J. H. Wilkinson, </author> <title> A priori error analysis of algebraic processes, </title> <booktitle> in Proc. International Congress of Mathematicians, </booktitle> <year> 1968, </year> <pages> pp. 119-129. </pages>
Reference-contexts: Therefore, E (1) CIMGS and E (1) CHOL are equal in those positions for which updating is performed. For the other positions, the elements of E (1) CIMGS are equal to 0. Using Wilkinson's estimates <ref> [17] </ref>, we get k E CIMGS k 2 k E (1) CHOL k 2 c 1 k B k 2 : Since B (1) CIMGS equals the reduced matrix after applying one step of Cholesky factoriza tion to B + U 12 U T 12 without rounding error on the positions
Reference: [18] <author> Z. Zlatev and H.B.Nielson, </author> <title> Solving large and sparse linear least squares problems by conjugate gradient algorithm, Computers and Mathematics with Applications, </title> <booktitle> 15 (1988), </booktitle> <pages> pp. 185-202. 23 </pages>
Reference-contexts: Preconditioning methods that have been proposed and analyzed for the CG algorithm include column scaling, SSOR [4], incomplete Cholesky factorization [11], polynomial preconditioning [1, 2], and incomplete orthogonal factorization <ref> [14, 10, 18, 16] </ref>. When preconditioning a symmetric positive definite system Bx = f , the usual goal is to increase the clustering of the eigenvalues around 1. <p> This suggests using an incomplete orthogonal factorization. Existing incomplete orthogonal factorization preconditioners can be divided into two classes: incomplete Gram-Schmidt methods [14, 10, 16] which give a factorization A = QR with Q not necessarily orthogonal, and incomplete Givens <ref> [18] </ref>, which produce A QR with Q orthogonal. Incomplete Gram-Schmidt-type methods are in general robust because they can avoid numerical breakdown when A is full rank. Furthermore, they are effective in accelerating the convergence of CG. <p> One way of reducing computations is to use a numerical dropping technique to keep both the Q and R factors sparse, as is done in ILQ [14] and the incomplete Givens method of <ref> [18] </ref>. The price these methods pay for efficiency is robustness, because dropping small entries can lead to zero elements on the diagonal of R. Restart techniques have to be used for these methods to assure robustness.
References-found: 17

