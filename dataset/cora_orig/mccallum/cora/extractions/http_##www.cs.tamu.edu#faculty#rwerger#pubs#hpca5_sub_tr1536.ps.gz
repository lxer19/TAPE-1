URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/hpca5_sub_tr1536.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Email: torrella@cs.uiuc.edu, rwerger@cs.tamu.edu  
Title: Speculative Parallel Execution of Loops with Cross-Iteration Dependences in DSM Multiprocessors  
Author: Ye Zhang, Lawrence Rauchwerger, and Josep Torrellas 
Keyword: scalable shared-memory multiprocessors, cache coherence protocols, run-time par-allelization, speculative execution, reduction parallelization.  
Address: -zhang2,  
Affiliation: University of Illinois and Texas A&M University  
Abstract: Speculative parallel execution of non-analyzable codes on Distributed Shared-Memory (DSM) multiprocessors is challenging due to the long-latency and distribution involved. However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed. In previous work, we suggested executing the loop speculatively in parallel and adding extensions to the memory hierarchy hardware to detect any dependence violation. If the violation occurs, execution is interrupted, the variables are restored, and the code is re-executed serially. The scheme is targeted to loops where most of the invocations turn out to run in parallel without any dependence violation. In this paper, we present a more advanced scheme for the speculative parallel execution of loops that have a modest number of cross-iteration dependences. In this case, when a dependence violation is detected, we locally repair the state. Then, we restart parallel execution from that point on. We call the general algorithm the Sliding Commit algorithm. If the loop dependences are of the special form of reduction, we use a specialized algorithm. Simulations indicate significant speedups relative to sequential execution. Finally, we propose hardware for optimizing reductions and obtain very good experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Asenjo, E. Gutierrez, Y. Lin, D. Padua, B. Pottenger, and E. Zapata. </author> <title> On the automatic parallelization of sparse and irregular fortran codes. </title> <type> Tech. </type> <institution> Rept. 1512, Center for Supercomputing Research and Development, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: If it is handled as a shared array, the loop becomes partially parallel and will cause a dependence related failure just before it finishes. After recovery, the remaining iterations are executed sequentially. For the loop move3 goto100 in Dsmc3d privatization removes all dependences <ref> [1] </ref> but, due to its sparse nature, causes high initialization and final merging overhead. By treating the tested array as a shared array and applying SCA we obtain good overall results. Spark98 is a sparse matrix and dense vector multiplication C kernel.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> Int. Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Adm, Track, and Spice are Perfect Club <ref> [2] </ref> codes, Euler and Dsmc3d are HPF-2 applications [5], and Rmv is a Spark98 kernel [17].
Reference: [3] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pot-tenger, L. Rauchwerger, and P. Tu. </author> <title> Advanced Program Restructuring for High-Performance Computers with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Automatic parallelization of codes by the compiler has advanced significantly in this decade <ref> [3, 7, 10] </ref>. Unfortunately, there is still a large body of potentially parallel codes that compilers cannot parallelize because they can not fully analyze the codes' dependence structure. <p> The modeled multiprocessor has the hardware support of the basic design [24] and the enhancement proposed in this paper. The simulated applications have been pre-processed with the Polaris <ref> [3] </ref> parallelizing compiler that has been specifically enhanced to transform selected loops for speculative run-time parallelization. The compiler has inserted all necessary instructions to perform the marking and analysis phases. The modeled architecture has 200-MHz RISC processors, each with a 32-Kbyte on-chip primary cache and a 512-Kbyte off-chip secondary cache.
Reference: [4] <author> D. K. Chen, J. Torrellas, and P. C. Yew. </author> <title> An Efficient Algorithm for the Run-Time Parallelization of Do-Across Loops. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pp. 518-527, </pages> <month> November </month> <year> 1994. </year>
Reference: [5] <author> I. Duff, R. Schreiber, and P. Havlak. </author> <title> Hpf-2 scope of activities and motivating applications. </title> <type> Tech. </type> <institution> Rept. CRPC-TR94492, Rice University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Adm, Track, and Spice are Perfect Club [2] codes, Euler and Dsmc3d are HPF-2 applications <ref> [5] </ref>, and Rmv is a Spark98 kernel [17].
Reference: [6] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proc. of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pp. 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: This type of reduction is sometimes called an update. There are several known parallel methods for performing reduction operations. One method is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [6, 26] </ref>. The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations.
Reference: [7] <author> Keith Cooper et al. </author> <title> The parascope parallel programming environment. </title> <booktitle> Proc. of IEEE, </booktitle> <pages> pp. 84-89, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Automatic parallelization of codes by the compiler has advanced significantly in this decade <ref> [3, 7, 10] </ref>. Unfortunately, there is still a large body of potentially parallel codes that compilers cannot parallelize because they can not fully analyze the codes' dependence structure.
Reference: [8] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In this section, we present our simulation environment, the workloads, and conclude with the obtained experimental performance measurements. 16 4.1 Simulation Environment Our evaluation is based on execution-driven simulations of a CC-NUMA shared-memory multiprocessor using Tangolite <ref> [8] </ref>. The modeled multiprocessor has the hardware support of the basic design [24] and the enhancement proposed in this paper. The simulated applications have been pre-processed with the Polaris [3] parallelizing compiler that has been specifically enhanced to transform selected loops for speculative run-time parallelization.
Reference: [9] <author> S. Gopal, T. N. Vijaykumar, J. E. Smith, and G. S. Sohi. </author> <title> Speculative Versioning Cache. </title> <booktitle> In Proc. of the 4th Int. Symp. on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: As mentioned in Section 3.3.1 the compiler can evaluate the size of the private structures needed for the partial results and chose the appropriate technique: with or without PCLR. 5 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [9, 11, 18, 21] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [10] <author> Mary Hall, Jennifer Anderson, Saman Amarasinghe, Brian Murphy, Shih-Wei Liao, Edouard Bugnion, and Monica Lam. </author> <title> Maximizing multiprocessor performance with the suif compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Automatic parallelization of codes by the compiler has advanced significantly in this decade <ref> [3, 7, 10] </ref>. Unfortunately, there is still a large body of potentially parallel codes that compilers cannot parallelize because they can not fully analyze the codes' dependence structure.
Reference: [11] <author> V. Krishnan and J. Torrellas. </author> <title> Hardware and Software Support for Speculative Execution of Sequential Binaries on a Chip-Multiprocessor. </title> <booktitle> In Proc. of the 1998 Int. Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: As mentioned in Section 3.3.1 the compiler can evaluate the size of the private structures needed for the partial results and chose the appropriate technique: with or without PCLR. 5 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [9, 11, 18, 21] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [12] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proc. of the 1985 Int. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. A scalable method can be obtained by noting that a reduction operation is an associative and commutative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [12, 13, 14] </ref>. In this case, the reduction variable is privatized in the transformed doall. A scalar is then produced using the partial results computed in each processor as operands for a reduction operation (with the same operator) across the processors (Figure 7-(c)).
Reference: [13] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proc. of the 1986 Int. Conf. on Parallel Processing, </booktitle> <pages> pp. 869-876, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. A scalable method can be obtained by noting that a reduction operation is an associative and commutative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [12, 13, 14] </ref>. In this case, the reduction variable is privatized in the transformed doall. A scalar is then produced using the partial results computed in each processor as operands for a reduction operation (with the same operator) across the processors (Figure 7-(c)).
Reference: [14] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. A scalable method can be obtained by noting that a reduction operation is an associative and commutative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [12, 13, 14] </ref>. In this case, the reduction variable is privatized in the transformed doall. A scalar is then produced using the partial results computed in each processor as operands for a reduction operation (with the same operator) across the processors (Figure 7-(c)).
Reference: [15] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Computer Architecture, </booktitle> <pages> pp. 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The cache sizes have been purposely selected so small in order to scale with the reduced working sets of the chosen applications. Real-life working sets could not be used because they would have required impractically long simulation times. The caches are kept coherent with a DASH-like cache coherence protocol <ref> [15] </ref>. Each node has part of the global memory and the corresponding section of the directory. We have modeled the contention in the whole system with the exception of the global network, which is abstracted away as a constant latency.
Reference: [16] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In 4th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference: [17] <author> D. O'Hallaron, J. Shewchuk, and T. Gross. </author> <title> Architectural implications of a family of irregular applications. </title> <type> Tech. </type> <institution> Rept. CMU-CS-97-189, Carnegie Mellon University, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Adm, Track, and Spice are Perfect Club [2] codes, Euler and Dsmc3d are HPF-2 applications [5], and Rmv is a Spark98 kernel <ref> [17] </ref>.
Reference: [18] <author> J. Oplinger, D. Heine, S.-W. Liao, B. A. Nayfeh, M. S. Lam, and K. Olukotun. </author> <title> Software and Hardware for Exploiting Speculative Parallelism with a Multiprocessor. </title> <type> Tech. </type> <institution> Rept. CSL-TR-97-715, Stanford University, </institution> <month> February </month> <year> 1997. </year> <month> 21 </month>
Reference-contexts: As mentioned in Section 3.3.1 the compiler can evaluate the size of the private structures needed for the partial results and chose the appropriate technique: with or without PCLR. 5 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [9, 11, 18, 21] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [19] <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> A scalable method for run-time loop parallelization. </title> <journal> IJPP, </journal> <volume> 26(6):537--576, </volume> <month> July </month> <year> 1995. </year>
Reference: [20] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatiza-tion and Reduction Parallelization. </title> <booktitle> In Proc. of the SIGPLAN 1995 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 218-232, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This inspector-executor method is also applied to fully-parallel loops. Unfortunately, in general, the inspector may be computationally expensive and have side-effects. Recently, we have introduced a new framework for run-time parallelization in hardware [24]. The scheme is based on a software-based run-time parallelization scheme that we proposed earlier <ref> [20] </ref>. The idea is to execute the code (loops) speculatively in parallel. As parallel execution proceeds, extra hardware added to the memory hierarchy detects if there is a dependence violation.
Reference: [21] <author> J. G. Steffan and T. C. Mowry. </author> <title> The Potential for Using Thread-Level Data Speculation to Facilitate Automatic Parallelization. </title> <booktitle> In Proc. of the 4th Int. Symp. on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: As mentioned in Section 3.3.1 the compiler can evaluate the size of the private structures needed for the partial results and chose the appropriate technique: with or without PCLR. 5 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [9, 11, 18, 21] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [22] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proc. of the 1991 Int. Conf. on Parallel Processing, </booktitle> <pages> pp. 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference: [23] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors. CSRD-TR. </title> <type> 1523, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Among the non-privatization ones, the most effective one is the Advanced Non-Privatization Algorithm (ANPA), which was not presented in [24] for space reasons. In the rest of this section, we summarize the ANPA and APA. We assume the general case of a dynamically-scheduled loop. See <ref> [24, 23] </ref> for details. 2.1 Advanced Non-Privatization Algorithm (ANPA) We use uncacheable extra memory in the distributed directories to keep two arrays, MaxR and MaxW. Each of these arrays has as many entries as, and is distributed across nodes in the same way as the array under test. <p> The Read and the Write bits of the whole cache are cleared in hardware and the beginning of every iteration. Finally, when a line is displaced from the cache, its Read and Write bits are lost. Figure 3 shows the complete read algorithms. The write algorithms are shown in <ref> [23] </ref>. 3 if ( Cache_hit ) read data from cache if ( Read == 0 ) /* first read in current iteration */ send Read_Test_Req (Curr_Iter) to home else /* cache miss */ send Read_Req (Curr_Iter) to home Read = 1 (a): Processor read. if ( Curr_Iter &lt; MaxW ) FAIL <p> In the directory, the current iteration number is compared to MaxR1st. If the former is lower, the parallelization fails; otherwise, MinW is set to the minimum of its current value and the current iteration number. More details can be found in <ref> [23] </ref>. 3 Parallelizing Loops with Dependencies The algorithms presented thus far are targeted to loops or codes that are fully-parallel most of the invocations. Now, we present new algorithms to handle loops that often have dependences.
Reference: [24] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors. </title> <booktitle> In Proc. of the 4th Int. Symp. on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Each wavefront is then executed in parallel by the executor, with barriers separating the wavefronts. This inspector-executor method is also applied to fully-parallel loops. Unfortunately, in general, the inspector may be computationally expensive and have side-effects. Recently, we have introduced a new framework for run-time parallelization in hardware <ref> [24] </ref>. The scheme is based on a software-based run-time parallelization scheme that we proposed earlier [20]. The idea is to execute the code (loops) speculatively in parallel. As parallel execution proceeds, extra hardware added to the memory hierarchy detects if there is a dependence violation. <p> We call the general algorithm the Sliding Commit Algorithm (SCA). If the loop dependences are of the special form of reduction, we use a specialized algorithm. This paper is organized as follows: Section 2 briefly describes the speculative parallelization scheme that was introduced in <ref> [24] </ref>; Section 3 presents the new speculative parallelization algorithms for loops with cross-iteration dependencies and reduction optimizations; Section 4 evaluates these new algorithms, and Section 5 discusses related work. 2 Speculative Parallelization in Hardware for DSM In [24], we proposed a scheme for non-analyzable loops to execute speculatively in parallel in <p> follows: Section 2 briefly describes the speculative parallelization scheme that was introduced in <ref> [24] </ref>; Section 3 presents the new speculative parallelization algorithms for loops with cross-iteration dependencies and reduction optimizations; Section 4 evaluates these new algorithms, and Section 5 discusses related work. 2 Speculative Parallelization in Hardware for DSM In [24], we proposed a scheme for non-analyzable loops to execute speculatively in parallel in a DSM machine. The idea is to extend the directory-based cache coherence protocol of the machine to detect dependencies across iterations in hardware. <p> First, there are the non-privatization algorithms and the privatization algo-rithms. The latter are more costly, but are able to parallelize more loops. Among the privatization algorithms, the Advanced Privatization Algorithm (APA) is usually the most effective <ref> [24] </ref>. Among the non-privatization ones, the most effective one is the Advanced Non-Privatization Algorithm (ANPA), which was not presented in [24] for space reasons. In the rest of this section, we summarize the ANPA and APA. We assume the general case of a dynamically-scheduled loop. <p> The latter are more costly, but are able to parallelize more loops. Among the privatization algorithms, the Advanced Privatization Algorithm (APA) is usually the most effective <ref> [24] </ref>. Among the non-privatization ones, the most effective one is the Advanced Non-Privatization Algorithm (ANPA), which was not presented in [24] for space reasons. In the rest of this section, we summarize the ANPA and APA. We assume the general case of a dynamically-scheduled loop. <p> Among the non-privatization ones, the most effective one is the Advanced Non-Privatization Algorithm (ANPA), which was not presented in [24] for space reasons. In the rest of this section, we summarize the ANPA and APA. We assume the general case of a dynamically-scheduled loop. See <ref> [24, 23] </ref> for details. 2.1 Advanced Non-Privatization Algorithm (ANPA) We use uncacheable extra memory in the distributed directories to keep two arrays, MaxR and MaxW. Each of these arrays has as many entries as, and is distributed across nodes in the same way as the array under test. <p> In this section, we present our simulation environment, the workloads, and conclude with the obtained experimental performance measurements. 16 4.1 Simulation Environment Our evaluation is based on execution-driven simulations of a CC-NUMA shared-memory multiprocessor using Tangolite [8]. The modeled multiprocessor has the hardware support of the basic design <ref> [24] </ref> and the enhancement proposed in this paper. The simulated applications have been pre-processed with the Polaris [3] parallelizing compiler that has been specifically enhanced to transform selected loops for speculative run-time parallelization. The compiler has inserted all necessary instructions to perform the marking and analysis phases.
Reference: [25] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Speculative Parallel Execution of Loops with with Cross-Iteration Dependences in DSM Multiprocessors. </title> <type> CSRD - TR., </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1998. </year>
Reference: [26] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year> <month> 22 </month>
Reference-contexts: This type of reduction is sometimes called an update. There are several known parallel methods for performing reduction operations. One method is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [6, 26] </ref>. The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. <p> this problem has been handled at compile-time by syntactically pattern matching the loop statements with a template of a generic reduction, and then performing a data dependence analysis of the variable under scrutiny to guarantee that it is not used anywhere else in the loop except in the reduction statement <ref> [26] </ref>. In the cases where data dependence analysis cannot be performed at compile time, reductions have to be validated at run-time.
References-found: 26

