URL: http://cs.nyu.edu/cs/faculty/paige/papers/hash.ps
Refering-URL: http://cs.nyu.edu/cs/faculty/paige/research.html
Root-URL: http://www.cs.nyu.edu
Title: Using Multiset Discrimination To Solve Language Processing Problems Without Hashing 1 lengths of the initial
Author: Jiazhen Cai and Robert Paige 
Note: L represent the  
Address: New York, NY 10012, USA  
Affiliation: New York University/ Courant Institute,  
Abstract: It is generally assumed that hashing is essential to solve many language processing problems efficiently; e.g., symbol table formation and maintenance, grammar manipulation, basic block optimization, and global optimization. This paper questions this assumption, and initiates development of an efficient alternative compiler methodology without hashing or sorting. The methodology rests on efficient solutions to the basic problem of detecting duplicate values in a multiset, which we call multiset discrimination. Paige and Tarjan [22] gave an efficient solution to multiset discrimination for detecting duplicate elements occurring in a multiset of varying length strings. The technique was used to develop an improved algorithm for lexicographic sorting, whose importance stems largely from its use in solving a variety of iso-morphism problems [2]. The current paper and a related paper [23] show that full lexicographic sorting is not needed to solve these isomorphism problems, because they can be solved more efficiently using straightforward extensions to the simpler multiset discrimination technique. By reformulating language processing problems in terms of multiset discrimination, we also show how almost every subtask of compilation can be solved without hashing in worst case running time no worse (and frequently better) than the best previous expected time solution (under the assumption that one hash operation takes unit expected time). Because of their simplicity, our solutions may be of practical as well as theoretical interest. The various applications presented culminate with a new algorithm to solve iterated strength reduction folded with useless code elimination that runs in worst case asymptotic time and auxiliary space Q( | L | + | L | ), where | L | and The previous best solution due to Cocke and Kennedy [10] takes W( | L | 3 | L | ) hhhhhhhhhhhhhhhhhh 1. A preliminary version of this paper appeared in the Conference Record of the Eighteenth Annual ACM Symposium on 
Abstract-found: 1
Intro-found: 1
Reference: 1. <editor> no author, </editor> <booktitle> ADA UK News, </booktitle> <volume> Vol 6, Num 1, </volume> <pages> pp. 14-15, </pages> <month> Jan., </month> <year> 1985. </year>
Reference-contexts: Mairson proved that for any - 3 - 'minimal' class of universal hash functions there exists a bad input set on which every hash function will not perform much better than binary search [21]. The slow speed of SETL [31], observed in the SETL implemented ADA-ED compiler <ref> [1] </ref>, has been attributed to an overuse of hashing [33]. And a hash table implementation involving an array twice the size of the data set is another cost. Arrays lack the benefits offered by linked lists namely, easy dynamic allocation, dynamic maintenance, and easy integration with other data structures.
Reference: 2. <author> Aho, A., Hopcroft, J., and Ullman, J., </author> <title> Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Throughout this paper we assume a sequential RAM model of computation under a uniform cost criterion <ref> [2] </ref> or the more restricted pointer machine [19,34], which prohibits address arithmetic. Aho, Sethi, and Ullman [3] present only two data structures for storing symbol tables a linear linked list (with linear search time) and a hash table. <p> In [22 ] Paige and Tarjan used multiset discrimination of varying length strings to design an efficient lexicographical sorting algorithm. However, the focus of that paper was on lexicographic sorting, whose importance stems largely from its use in solving a variety of isomorphism problems <ref> [2] </ref>. No other application of multiset discrimination was mentioned. <p> Both this and the earlier array-based implementa tions are simple, and involve one pass through the prefixes of the sequences. Consequently, our proposed applications may be practical. Previously, the lexicographic sorting algorithm found in Aho, Hopcroft, and Ullman's book <ref> [2] </ref> was used to solve congruence closure [13] and also tree isomorphism [2]. However, their sorting algorithm has the theoretical disadvantage of an Q (m +k ) complexity in time and - 12 - auxiliary space, where m is the total length of all the input strings. <p> Consequently, our proposed applications may be practical. Previously, the lexicographic sorting algorithm found in Aho, Hopcroft, and Ullman's book <ref> [2] </ref> was used to solve congruence closure [13] and also tree isomorphism [2]. However, their sorting algorithm has the theoretical disadvantage of an Q (m +k ) complexity in time and - 12 - auxiliary space, where m is the total length of all the input strings. <p> Cocke and Schwartz [11] solved this problem by representing distinct node identifiers and nodes with distinct integers called `value numbers'. Their method involves extensive hashing. An alternative method related to the tree isomorphism solution found in <ref> [2] </ref> uses lexicographic sorting to compute value numbers. Although it runs in linear worst case time in the size of the forest, it is a complex multi-pass method that is not likely to outperform the value number method in practice. <p> This space bound improves the O (m) space bound that could be obtained to solve this problem using Aho, Hopcroft, and Ullman's lexicographic sorting algorithm <ref> [2] </ref>. We also show how multiset dag discrimination can be used to obtain an improved solution to acyclic instances of the many-function coarsest partition problem. The many-function coarsest partition problem, used by Hopcroft to model the problem of DFA minimization [17], has applications in program optimization and program integration. <p> As described in <ref> [2] </ref> Hopcroft's algorithm is nondeterministic, and could behave in the following way for the case where k =1 and f = E 1 : 1. Push each block of the inital partition P onto a stack W. 2. <p> Our algorithm combines multiset discrimination with other data structuring techniques. It is at this point that our solution differs from Cocke and Kennedy. They go on to compute the transitive closure Afct* in time Q (n 3 +m) using, say, Warshall's algorithm [37] (see also <ref> [2] </ref>), where n is the number of variables and constants contained in Afct, and m is the number of assignments to induction variables.
Reference: 3. <author> Aho, A., Sethi, R. and Ullman, J., </author> <title> Compilers, </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: In the context of this broader question, we initiate an investigation of efficient compilation without hashing, and, consequently, raise some doubts about the prevailing view that hashing (e.g. universal hashing [7]) is essential to an efficient implementation of the various subtasks of compilation from symbol table management <ref> [3] </ref> to reduction in strength by hashed temporaries [10]. Throughout this paper we assume a sequential RAM model of computation under a uniform cost criterion [2] or the more restricted pointer machine [19,34], which prohibits address arithmetic. Aho, Sethi, and Ullman [3] present only two data structures for storing symbol tables <p> the various subtasks of compilation from symbol table management <ref> [3] </ref> to reduction in strength by hashed temporaries [10]. Throughout this paper we assume a sequential RAM model of computation under a uniform cost criterion [2] or the more restricted pointer machine [19,34], which prohibits address arithmetic. Aho, Sethi, and Ullman [3] present only two data structures for storing symbol tables a linear linked list (with linear search time) and a hash table. They also propose these two data structures for methods to turn an expression tree into a dag, and for the more general basic block optimization of value numbering. <p> The solution depends on basic multiset discrimination. The following terminology and notation for context free grammars can be found in <ref> [3] </ref>. <p> Throughout this section it will sometimes be convenient to identify a grammar G with its set of productions. We also assume that each context free grammar G contains only nonterminals that can derive nonempty sentences and can occur within some sentential form of G. Definition 3.1. (see <ref> [3] </ref>): Two productions A fi a | b belonging to G form an LL (1) conflict iff one of the following conditions holds - 14 - (1) there exists a terminal symbol t such that a =&gt;* t x and b =&gt;* t y; (2) a =&gt;* [] and b =&gt;* <p> The problem of deciding subtree equivalence arises in common subexpression detection [9], turning an arbitrary linear tree pattern matching algorithm [16] into a nonlinear matching algorithm [28], deciding structural equivalence of type expressions <ref> [3] </ref>, and preprocessing input in the form required by Wegman and Paterson's unification algorithm [27]. Cocke and Schwartz [11] solved this problem by representing distinct node identifiers and nodes with distinct integers called `value numbers'. Their method involves extensive hashing. <p> Basic Block Optimization Value numbering has been used as a standard program analysis technique for determining equalities of the values computed by instructions within basic blocks <ref> [11, 3] </ref>. Although the technique is mostly implemented with hashing, multiset discrimination can be used to obtain a more efficient implementation without hashing and without numeric representations. <p> All concern for control flow is simplified by accepting a most conservative assumption that the control flow graph representation of L forms a clique; i.e., that every two statements in L can be executed one after the other at runtime. In accordance with conventional terminology (cf. <ref> [3] </ref>) variable occurrences on the left of an assignment are called definitions; all other variable occurrences are called uses. A - 26 - variable that has no definitions in a region of code is called a region constant variable.
Reference: 4. <author> Allen, F. E., Cocke, J., and Kennedy, K., </author> <title> ``Reduction of Operator Strength,'' in Program Flow Analysis, </title> <editor> ed. Muchnick, S. and Jones, </editor> <booktitle> N., </booktitle> <pages> pp. 79-101, </pages> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference-contexts: The strength reduction transformation presented by Cocke and Kennedy [10] may be the most practical reduction in strength algorithm published in the literature. Although the transformation due to Allen, Cocke, and Kennedy <ref> [4] </ref> is more powerful (since it can reduce multivariate products) and analyzes control flow more deeply, that algorithm can also degrade performance by introducing too many sums in order to remove nests of products (as was shown in [24]). <p> Cocke and Kennedy's transformation is concerned with replacing hidden costs of linear polynomials involved in the array access formula used in programming languages like Fortran or Algol. As was suggested by Allen, Cocke, and Kennedy <ref> [4] </ref>, the earlier transformation [10] can be improved by sharper analysis of control flow and taking safety of code motion into account. However, such improvement is orthogonal to the solution presented here. The strength reduction transformation of [10] may be defined as follows.
Reference: 5. <author> Alpern, B., Wegman, N., and Zadeck, K., </author> <title> ``Detecting Equality of Variables in Programs,'' </title> <booktitle> in Proc. 15th ACM POPL, </booktitle> <month> Jan, </month> <year> 1988. </year>
Reference-contexts: They also propose these two data structures for methods to turn an expression tree into a dag, and for the more general basic block optimization of value numbering. Hashing is involved in preprocessing for global optimizations to perform constant propagation [38], global redundant code elimination <ref> [5] </ref>, and code motion [12]. The best algorithms for strength reduction [10,4] rely on hashed temporaries to obtain practical implementations. There are several reasons why hashing is used in these applications. <p> It also leads to a faster solution to the program equivalence prob lem used in integration by Yang, Horwitz, and Reps [40,41]. g Although the main parts of algorithms for global constant propagation [38], global common subexpression detection <ref> [5] </ref>, and code motion [12] do not use hashing, the preprocessing - 4 - portions for each of these algorithms do.
Reference: 6. <author> Cai, J. and Paige, R., </author> <title> ``"Look Ma, No Hashing, And No Arrays Neither",'' </title> <booktitle> in ACM POPL, </booktitle> <pages> pp. 143 - 154, </pages> <month> Jan, </month> <year> 1991. </year>
Reference-contexts: If we interpret all leaf nodes as hav ing height 1, we can then solve acyclic coarsest partitioning by multiset dag discrimination. ` Since the original version of our paper (which included Theorem 3.10 without proof) appeared in <ref> [6] </ref>, we learned that Revuz [29] independently observed that acyclic coarsest - 24 - partitioning could be solved in linear time. Revuz rediscovered the multiset discrimination algorithm originally described in [22], and independently used this procedure without modification to obtain the result.
Reference: 7. <author> Carter, J. and Wegman, M., </author> <title> ``Universal Classes of Hash Functions,'' </title> <journal> JCSS, </journal> <volume> vol. 18, no. 2, </volume> <pages> pp. 143-154, </pages> <year> 1979. </year>
Reference-contexts: In the context of this broader question, we initiate an investigation of efficient compilation without hashing, and, consequently, raise some doubts about the prevailing view that hashing (e.g. universal hashing <ref> [7] </ref>) is essential to an efficient implementation of the various subtasks of compilation from symbol table management [3] to reduction in strength by hashed temporaries [10]. <p> Given a set of n elements, hashing supports membership testing, element addition, and element deletion of unit-space data in O (1) expected time and O (n ) auxiliary space. The method of universal hashing, due to Carter and Wegman <ref> [7] </ref>, is especially desirable, since the expected O (1) time is independent of the input distribution. Universal hashing is well suited to applications such as compilation, where a hash table used to implement the symbol table does not have to persist beyond a single compilation run.
Reference: 8. <editor> Clinger, W. and Rees, J., </editor> <title> ``Macros That Work,'' </title> <booktitle> in Proceedings 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 155-162, </pages> <month> Jan, </month> <year> 1991. </year>
Reference-contexts: These applications include the following. g Array- or list-based symbol tables can be formed during lexical scanning with unit-time curser or pointer storage/access. These tables may then be used to support an efficient implementation of hygienic macro expansion <ref> [8] </ref>. g Many grammar transformations can be implemented efficiently using multiset string discrimination. Included in this paper is a new linear time left factoring transformation. <p> Such a symbol table can also be used to implement macro expansion. For example, the hygienic macro expansion algorithm reported by Clinger and Rees <ref> [8] </ref> frequently performs the following operations: (1) replace all occurrences of a bound variable in its binding scope by a fresh identifier; (2) store a macro definition together with its environment into the symbol table (so that reference transparency can be achieved, i.e., when a macro is expanded, identifiers are referenced <p> By making a new copy of the environment that was originally stored with the macro definition, the original environment is preserved. Although Clinger and Rees <ref> [8] </ref> assume O (1) time for each environment operation, a straightforward implementation based on their definition would require a linear search through lists that can be as long as the input text. We suggest the following more efficient implementation. Let table (x) represent the symbol table entry for identifier x.
Reference: 9. <author> Cocke, J., </author> <title> ``Global common subexpression elimination,'' </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 5, no. 7, </volume> <pages> pp. 20 - 24, </pages> <year> 1970. </year>
Reference-contexts: We say that two identifiers are equivalent if they are equal; two nodes are equivalent if the components of their node lists are pairwise equivalent, and two sub-trees are equivalent if their respective root nodes are equivalent. The problem of deciding subtree equivalence arises in common subexpression detection <ref> [9] </ref>, turning an arbitrary linear tree pattern matching algorithm [16] into a nonlinear matching algorithm [28], deciding structural equivalence of type expressions [3], and preprocessing input in the form required by Wegman and Paterson's unification algorithm [27].
Reference: 10. <author> Cocke, J. and Kennedy, K., </author> <title> ``An Algorithm for Reduction of Operator Strength,'' </title> <journal> CACM, </journal> <volume> vol. 20, no. 11, </volume> <pages> pp. 850-856, </pages> <month> Nov., </month> <year> 1977. </year>
Reference-contexts: broader question, we initiate an investigation of efficient compilation without hashing, and, consequently, raise some doubts about the prevailing view that hashing (e.g. universal hashing [7]) is essential to an efficient implementation of the various subtasks of compilation from symbol table management [3] to reduction in strength by hashed temporaries <ref> [10] </ref>. Throughout this paper we assume a sequential RAM model of computation under a uniform cost criterion [2] or the more restricted pointer machine [19,34], which prohibits address arithmetic. <p> Such hashing can be eliminated without penalty by efficient construction and maintenance of the symbol table. g Strength reduction [11,10,4,18,30,15] has remained one of the most complex, least understood, and most practical of the machine independent program optimizations. The strength reduction transformation presented by Cocke and Kennedy <ref> [10] </ref> may be the most practical reduction in strength algorithm published in the literature. <p> We solve three progressively more complex versions of the Cocke and Kennedy transformation without hashing and with superior worst case time and space than the expected performance in previous hash-based solutions <ref> [10] </ref>. In the final version an algorithm is presented to solve iterated strength reduction folded with useless code elimination in worst case asymptotic time and auxiliary space linear in the maximum text length of the initial and optimized programs. <p> Ironically, the efficiency obtained stems from using batch techniques to implement strength reduction, which itself uses incremental techniques to improve program performance. 4.1. Basic Strength Reduction First we consider a new hash-free algorithm that implements Cocke and Kennedy's strength reduction transformation <ref> [10] </ref>. The algorithm runs in worst case time and space linear in the length of the final program text, which, as we will show, can be as much as two orders of magnitude better than their hash-based algorithm. <p> Cocke and Kennedy's transformation is concerned with replacing hidden costs of linear polynomials involved in the array access formula used in programming languages like Fortran or Algol. As was suggested by Allen, Cocke, and Kennedy [4], the earlier transformation <ref> [10] </ref> can be improved by sharper analysis of control flow and taking safety of code motion into account. However, such improvement is orthogonal to the solution presented here. The strength reduction transformation of [10] may be defined as follows. Let L be a strongly connected region of code. <p> As was suggested by Allen, Cocke, and Kennedy [4], the earlier transformation <ref> [10] </ref> can be improved by sharper analysis of control flow and taking safety of code motion into account. However, such improvement is orthogonal to the solution presented here. The strength reduction transformation of [10] may be defined as follows. Let L be a strongly connected region of code. We assume that this code consists of assignments to simple (non-array) variables of the form z:=op (x,y) or z:=op (x ) and conditional branches with boolean valued variables as predicates. <p> For each induction variable x, compute the set Afct (x ) = -x- -y: y is a variable or constant on the right-hand-side of any assignment to x that occurs in L-. Although no algorithmic analysis was provided in <ref> [10] </ref>, it is well known that Cocke and Kennedy's strength reduction algorithm carries out Steps (A1.-A4.) in worst case time and space linear in the length #L of program text L. <p> Iterated Strength Reduction With Useless Code Elimination Cocke and Kennedy noted that after strength reduction is applied, the new compiler generated variables t vc and other variables can become new induction variables, and new products defined in terms of these variables can be removed by further applications of strength reduction <ref> [10] </ref>. In this section we show how iterated strength reduction folded with useless code elimination can be solved in worst case time and space linear in the maximum length of the initial and final program texts.
Reference: 11. <author> Cocke, J. and Schwartz, J. T., </author> <booktitle> Programming Languages and Their Compilers, Lecture Notes, </booktitle> <address> CIMS, New York University, </address> <year> 1969. </year> <month> - 37 </month> - 
Reference-contexts: Cocke and Schwartz <ref> [11] </ref> solved this problem by representing distinct node identifiers and nodes with distinct integers called `value numbers'. Their method involves extensive hashing. An alternative method related to the tree isomorphism solution found in [2] uses lexicographic sorting to compute value numbers. <p> Basic Block Optimization Value numbering has been used as a standard program analysis technique for determining equalities of the values computed by instructions within basic blocks <ref> [11, 3] </ref>. Although the technique is mostly implemented with hashing, multiset discrimination can be used to obtain a more efficient implementation without hashing and without numeric representations. <p> A2. Compute the set IV of induction variables; that is, the set of all variables x that have definitions occurring in L such that product x c would be reducible for any constant c. This procedure was also described by Cocke and Schwartz <ref> [11] </ref>. A3. Find the set Cands of all reducible products xc that actually occur in L, and find the associated program points in L where these products occur. - 27 - A4. <p> Let L be the final code resulting from iterated strength reduction. Let Cands be the set of products in the initial code L reduced by iterated strength reduction. Following Cocke and Schwartz <ref> [11] </ref>, we say that a temporary t vc 1 ...c j is available in program region L if, whenever it is referenced during execution of L, it stores the value of v c 1 ...c j .
Reference: 12. <author> Cytron, R., Lowry, A., and Zadeck, K., </author> <title> ``Code Motion of Control Structures in High-level Languages,'' </title> <institution> IBM Research Center/Yorktown Heights, </institution> <year> 1985. </year>
Reference-contexts: They also propose these two data structures for methods to turn an expression tree into a dag, and for the more general basic block optimization of value numbering. Hashing is involved in preprocessing for global optimizations to perform constant propagation [38], global redundant code elimination [5], and code motion <ref> [12] </ref>. The best algorithms for strength reduction [10,4] rely on hashed temporaries to obtain practical implementations. There are several reasons why hashing is used in these applications. <p> It also leads to a faster solution to the program equivalence prob lem used in integration by Yang, Horwitz, and Reps [40,41]. g Although the main parts of algorithms for global constant propagation [38], global common subexpression detection [5], and code motion <ref> [12] </ref> do not use hashing, the preprocessing - 4 - portions for each of these algorithms do.
Reference: 13. <author> Downey, P., Sethi, R., and Tarjan, R., </author> <title> ``Variations on the Common Subexpression Problem,'' </title> <journal> JACM, </journal> <volume> vol. 27, no. 4, </volume> <pages> pp. 758-771, </pages> <month> Oct., </month> <year> 1980. </year>
Reference-contexts: Both this and the earlier array-based implementa tions are simple, and involve one pass through the prefixes of the sequences. Consequently, our proposed applications may be practical. Previously, the lexicographic sorting algorithm found in Aho, Hopcroft, and Ullman's book [2] was used to solve congruence closure <ref> [13] </ref> and also tree isomorphism [2]. However, their sorting algorithm has the theoretical disadvantage of an Q (m +k ) complexity in time and - 12 - auxiliary space, where m is the total length of all the input strings.
Reference: 14. <author> Earley, J., </author> <title> ``High Level Iterators and a Method for Automatically Designing Data Structure Representation,'' </title> <journal> J of Computer Languages, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 321-342, </pages> <year> 1976. </year>
Reference-contexts: This problem with its two main aspects inverting a function and finding duplicate values in a multiset is basic. It is central to the database index formation techniques within Willard's database predicate retrieval theory [39]. It is essential to Earley's program optimization by itera-tor inversion <ref> [14] </ref>. It was also used by Paige and Tarjan to design new algorithms for the minimization of one-symbol finite automata [26] and lexicographic sorting [22].
Reference: 15. <author> Fong, A., </author> <title> ``Elimination of Common Subexpressions in Very High Level Languages,'' </title> <booktitle> in Proceedings Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 48-57, </pages> <month> Jan, </month> <year> 1977. </year>
Reference: 16. <author> Hoffmann, C. and O'Donnell, J., </author> <title> ``Pattern Matching in Trees,'' </title> <journal> JACM, </journal> <volume> vol. 29, no. 1, </volume> <pages> pp. 68-95, </pages> <month> Jan, </month> <year> 1982. </year>
Reference-contexts: Several applications include one in which a linear pattern matching algorithm (e.g., <ref> [16] </ref>) can be turned into an efficient nonlinear matching algorithm, where each subtree equality check takes unit time. The method is also used to perform basic block optimizations (previously carried out by hashing 'value numbers' [11,3]) without hashing. <p> The problem of deciding subtree equivalence arises in common subexpression detection [9], turning an arbitrary linear tree pattern matching algorithm <ref> [16] </ref> into a nonlinear matching algorithm [28], deciding structural equivalence of type expressions [3], and preprocessing input in the form required by Wegman and Paterson's unification algorithm [27]. Cocke and Schwartz [11] solved this problem by representing distinct node identifiers and nodes with distinct integers called `value numbers'.
Reference: 17. <author> Hopcroft, J., </author> <title> ``An n log n Algorithm for Minimizing States in a Finite Automaton,'' in Theory of Machines and Computations, </title> <editor> ed. </editor> <booktitle> Kohavi and Paz, </booktitle> <pages> pp. 189-196, </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: We also show how multiset dag discrimination can be used to obtain an improved solution to acyclic instances of the many-function coarsest partition problem. The many-function coarsest partition problem, used by Hopcroft to model the problem of DFA minimization <ref> [17] </ref>, has applications in program optimization and program integration. It can be formulated as follows. <p> for each i = 1, ..., k, the out-degree of each vertex v V in graph (V, E i ) is at most 1. - 22 - Hopcroft gave an algorithm that solves this problem in time Q (kn log n) and space Q (k n) in the worst case <ref> [17] </ref>, where n = #V. As described in [2] Hopcroft's algorithm is nondeterministic, and could behave in the following way for the case where k =1 and f = E 1 : 1. Push each block of the inital partition P onto a stack W. 2.
Reference: 18. <author> Knoop, J. and Steffen, B., </author> <title> Strength Reduction based on Code Motion, </title> <institution> Institute Fur Informatik und Praktische Mathematik, Christian-Albrechts-University, Kiel, Ger-many, </institution> <month> Feb., </month> <year> 1991. </year>
Reference-contexts: Although Knoop and Steffen's approach <ref> [18] </ref> is more general, their algorithm is more difficult to implement, and its runtime performance may require exponential time.
Reference: 19. <author> Knuth, D. E., </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol 1: </volume> <booktitle> Fundamental Algorithms, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference: 20. <author> Lewis, F., Rosencrantz, D., and Stearns, R., </author> <title> Compiler Design Theory, </title> <publisher> Addison-Wesley, </publisher> <year> 1976. </year>
Reference: 21. <author> Mairson, H., </author> <title> ``The Program Complexity of Searching a Table,'' </title> <booktitle> in 24th IEEE FOCS, </booktitle> <pages> pp. 40-47, </pages> <month> Nov., </month> <year> 1983. </year>
Reference-contexts: Mairson proved that for any - 3 - 'minimal' class of universal hash functions there exists a bad input set on which every hash function will not perform much better than binary search <ref> [21] </ref>. The slow speed of SETL [31], observed in the SETL implemented ADA-ED compiler [1], has been attributed to an overuse of hashing [33]. And a hash table implementation involving an array twice the size of the data set is another cost.
Reference: 22. <author> Paige, R and Tarjan, R., </author> <title> ``Three Efficient Algorithms Based on Partition Refinement,'' </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 16, no. 6, </volume> <pages> pp. 973-989, </pages> <month> Dec., </month> <year> 1987. </year>
Reference-contexts: This is achieved by reformulating these language processing problems in terms of subproblems that can be solved efficiently using several simple algorithmic tools (that exclude sorting), the most important of which is multiset discrimination; i.e., finding all duplicate values in a multiset. In <ref> [22 ] </ref> Paige and Tarjan used multiset discrimination of varying length strings to design an efficient lexicographical sorting algorithm. However, the focus of that paper was on lexicographic sorting, whose importance stems largely from its use in solving a variety of isomorphism problems [2]. <p> It is essential to Earley's program optimization by itera-tor inversion [14]. It was also used by Paige and Tarjan to design new algorithms for the minimization of one-symbol finite automata [26] and lexicographic sorting <ref> [22] </ref>. <p> Paige and Tarjan solved multiset discrimination of sequences for the special case in which S is a finite alphabet implemented as an array-based directory, and the sequences belonging to M are implemented as arrays (or more precisely, as strings) <ref> [22] </ref>. In order to investigate the general utility of multiset discrimination of sequences, we extend their solution by making it polymorphic in S; i.e., by allowing the elements of S to belong to a general class of datatypes. <p> point to next list cell end loop P := P -ffdirectory (y)-- end if end loop end loop return [F, f] end Multiset Sequence Discrimination Algorithm The preceding algorithm is an abstracted and extended formulation of Paige and Tarjan's technique for multiset discrimination of varying length strings over alphabet S <ref> [22] </ref>. The earlier algorithm used an array-based directory for S and implemented strings as `packed' arrays of cur sors referencing the directory. <p> Revuz rediscovered the multiset discrimination algorithm originally described in <ref> [22] </ref>, and independently used this procedure without modification to obtain the result. He did not show that Hopcroft's algorithm can take W (nlogn) steps on acyclic input instances. <p> He did not show that Hopcroft's algorithm can take W (nlogn) steps on acyclic input instances. Our approach is likely to be more efficient than his, since our pointer-based dag discrimination avoids the numeric representation that is needed in order to make direct use of the algorithm found in <ref> [22] </ref>. 3.5. The Sequence Congruence Problem The sequence congruence problem [40,41] arises in the context of program integration. The problem is how to partition program components into classes whose members have equivalent execution behavior. <p> Extensions Two possible approaches that exploit commutative and associative laws of products may reduce the number of strings and therefore temporaries generated in the preceding strength reduction algorithms. One approach is to use a weak form of the Paige/Tarjan lexicographic sorting algorithm <ref> [22] </ref> to generate strings of constants in some arbitrarily chosen order. Another more effective, but less efficient, approach, would be to actually compute the product of constants identifying each temporary, and to use multiset constant discrimination.
Reference: 23. <author> Paige, R., </author> <title> ``Efficient Translation of External Input in a Dynamically Typed Language,'' </title> <booktitle> in Proc. IFIP Congress 94 - Vol 1, </booktitle> <editor> ed. B. Pehrson and I. </editor> <publisher> Simon, </publisher> <pages> pp. 603-608, </pages> <publisher> Elsevier North-Holland, </publisher> <year> 1994. </year>
Reference-contexts: However, the focus of that paper was on lexicographic sorting, whose importance stems largely from its use in solving a variety of isomorphism problems [2]. No other application of multiset discrimination was mentioned. In the current paper together with <ref> [23] </ref> we uncover the greater significance of multiset discrimination by showing how a straightforward extension of Paige and Tarjan's technique to a more general class of datatypes, including sequences of trees and dags, can solve these isomorphism problems more efficiently than methods based on full lexicographic sorting. <p> Such datatypes include strings, lists, ordered trees, and ordered dags. Multiset discrimination of arbitrary datatypes formed from identifiers and constructors for sequences, sets, and multisets are found in <ref> [23] </ref>. The generic nature of these methods, and their wide ranging successful applications to language processing problems demonstrates a basic algorithm design tool for solving problems in various other areas of computer science.
Reference: 24. <author> Paige, R., </author> <title> ``Symbolic Finite Differencing Part I,'' </title> <booktitle> in Proc. ESOP 90, </booktitle> <editor> ed. N. Jones, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> vol. 432, </volume> <pages> pp. 36-56, </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Although the transformation due to Allen, Cocke, and Kennedy [4] is more powerful (since it can reduce multivariate products) and analyzes control flow more deeply, that algorithm can also degrade performance by introducing too many sums in order to remove nests of products (as was shown in <ref> [24] </ref>). Although Knoop and Steffen's approach [18] is more general, their algorithm is more difficult to implement, and its runtime performance may require exponential time.
Reference: 25. <author> Paige, R., </author> <title> ``Real-time Simulation of a Set Machine on a RAM,'' in ICCI '89, </title> <editor> ed. R. Janicki and W. Koczkodaj, </editor> <booktitle> Computing and Information, </booktitle> <volume> Vol II, </volume> <pages> pp. 69-73, </pages> <year> 1989. </year>
Reference-contexts: Although hashing may seem like a panacea, it does incur costs, and one should not overlook the many contexts in which the preceding computations can be solved by an efficient hash-free approach. In <ref> [25] </ref> Paige presented a different more general discussion of principles underlying hash-free algorithms for simple set operations. A few sharper techniques with a focus on multiset discrimination are discussed below. 2.1. Terminology Throughout this paper it is convenient to make use of notations and terminology for specifying algorithms.
Reference: 26. <author> Paige, R., Tarjan, R., and Bonic, R., </author> <title> ``A Linear Time Solution to the Single Function Coarsest Partition Problem,'' </title> <journal> TCS, </journal> <volume> vol. 40, no. 1, </volume> <pages> pp. 67-84, </pages> <month> Sep, </month> <year> 1985. </year>
Reference-contexts: It is central to the database index formation techniques within Willard's database predicate retrieval theory [39]. It is essential to Earley's program optimization by itera-tor inversion [14]. It was also used by Paige and Tarjan to design new algorithms for the minimization of one-symbol finite automata <ref> [26] </ref> and lexicographic sorting [22].
Reference: 27. <author> Paterson, M. and Wegman, M., </author> <title> ``Linear Unification,'' </title> <journal> Journal of Computer and System Science, </journal> <volume> no. 16, </volume> <pages> pp. 158 - 167, </pages> <year> 1978. </year>
Reference-contexts: The problem of deciding subtree equivalence arises in common subexpression detection [9], turning an arbitrary linear tree pattern matching algorithm [16] into a nonlinear matching algorithm [28], deciding structural equivalence of type expressions [3], and preprocessing input in the form required by Wegman and Paterson's unification algorithm <ref> [27] </ref>. Cocke and Schwartz [11] solved this problem by representing distinct node identifiers and nodes with distinct integers called `value numbers'. Their method involves extensive hashing. An alternative method related to the tree isomorphism solution found in [2] uses lexicographic sorting to compute value numbers.
Reference: 28. <author> Pelegri-Llopart, E., </author> <title> ``Rewrite Systems, Pattern Matching, and Code Generation,'' </title> <type> Ph.D. Dissertation, </type> <institution> U. of CA - Berkeley, </institution> <year> 1987. </year> <month> - 38 </month> - 
Reference-contexts: The problem of deciding subtree equivalence arises in common subexpression detection [9], turning an arbitrary linear tree pattern matching algorithm [16] into a nonlinear matching algorithm <ref> [28] </ref>, deciding structural equivalence of type expressions [3], and preprocessing input in the form required by Wegman and Paterson's unification algorithm [27]. Cocke and Schwartz [11] solved this problem by representing distinct node identifiers and nodes with distinct integers called `value numbers'. Their method involves extensive hashing.
Reference: 29. <author> Revuz, D., </author> <title> ``Minimisation of acyclic deterministic automata in linear time,'' </title> <journal> Theoretical Computer Science, </journal> <volume> vol. 92, no. 1, </volume> <pages> pp. 181 - 189, </pages> <year> 1992. </year>
Reference-contexts: If we interpret all leaf nodes as hav ing height 1, we can then solve acyclic coarsest partitioning by multiset dag discrimination. ` Since the original version of our paper (which included Theorem 3.10 without proof) appeared in [6], we learned that Revuz <ref> [29] </ref> independently observed that acyclic coarsest - 24 - partitioning could be solved in linear time. Revuz rediscovered the multiset discrimination algorithm originally described in [22], and independently used this procedure without modification to obtain the result.
Reference: 30. <author> Rosen, B. K., </author> <title> ``Degrees of Availability,'' in Program Flow Analysis, </title> <editor> ed. Muchnick, S., Jones, </editor> <booktitle> N., </booktitle> <pages> pp. 55-76, </pages> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference: 31. <author> Schwartz, J., Dewar, R., Dubinsky, D., and Schonberg, E., </author> <title> Programming with Sets: An introduction to SETL, </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Mairson proved that for any - 3 - 'minimal' class of universal hash functions there exists a bad input set on which every hash function will not perform much better than binary search [21]. The slow speed of SETL <ref> [31] </ref>, observed in the SETL implemented ADA-ED compiler [1], has been attributed to an overuse of hashing [33]. And a hash table implementation involving an array twice the size of the data set is another cost.
Reference: 32. <author> Stearns, R., </author> <title> ``Deterministic top-down parsing,'' </title> <booktitle> in Proc. 5th Princeton Conf. on Information Sciences and Systems, </booktitle> <pages> pp. 182-188, </pages> <year> 1971. </year>
Reference-contexts: Included in this paper is a new linear time left factoring transformation. Simpler forms of this 'heuristic' transformation were previously studied by Stearns <ref> [32] </ref> and others (see also [20,3]) to turn non-LL context free grammars into LL grammars. g An expression tree-to-dag transformation is implemented without hashing in a simpler way than before and in linear time and space. <p> To paint an expansion, we replace each newly introduced identifier d by a pointer to a fresh symbol table entry for d, and push the top of table (d) onto table (d). 3.2. Fast Left Factoring Left factoring is a context free grammar transformation investigated by Stearns <ref> [32] </ref> and others [20,3] as a tool for turning non-LL grammars into LL grammars. They did not describe optimal forms of factoring or algorithmic details.
Reference: 33. <author> Straub, R., ``Taliere: </author> <title> An Interactive System for Data Structuring SETL Programs,'' </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Computer Science, </institution> <address> New York University, New York, NY, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The slow speed of SETL [31], observed in the SETL implemented ADA-ED compiler [1], has been attributed to an overuse of hashing <ref> [33] </ref>. And a hash table implementation involving an array twice the size of the data set is another cost. Arrays lack the benefits offered by linked lists namely, easy dynamic allocation, dynamic maintenance, and easy integration with other data structures.
Reference: 34. <author> Tarjan, R., </author> <title> ``A Class Of Algorithms Which Require Nonlinear Time To Maintain Disjoint Sets,'' </title> <journal> J. Comput. Sys. Sci., </journal> <volume> vol. 18, </volume> <pages> pp. 110-127, </pages> <year> 1979. </year>
Reference: 35. <author> Tarjan, R., </author> <title> Data Structures and Network Algorithms, </title> <publisher> SIAM, </publisher> <year> 1984. </year>
Reference-contexts: It is not necessary to distinguish between an endogenous implementation of S in which the elements of S are stored entirely within the directory, and an exogenous implementation in which the directory stores pointers to the elements of S <ref> [35] </ref>. If x is an element of the directory in an exogenous implementation, then implicit dere-ferencing is assumed in contexts where it is clear that the actual value is needed. However, it is useful to distinguish two related implementations of the directory.
Reference: 36. <author> Tarjan, R., </author> <title> ``Depth first search and linear graph algorithms,'' </title> <journal> SIAM J. Comput, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 146-160, </pages> <year> 1972. </year>
Reference-contexts: The strong component decomposition of G is computed by Tarjan's algorithm <ref> [36] </ref>. Note that the roots of G (i.e., nodes with in-degree 0) represent constants and region constant variables. All other nodes of G represent induction variables. For convenience we will sometimes refer to the nodes of G in terms of the variables and constants that they represent.
Reference: 37. <author> Warshall, S., </author> <title> ``A Theorem on Boolean matrices,'' </title> <journal> JACM, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 11-12, </pages> <year> 1962. </year>
Reference-contexts: Our algorithm combines multiset discrimination with other data structuring techniques. It is at this point that our solution differs from Cocke and Kennedy. They go on to compute the transitive closure Afct* in time Q (n 3 +m) using, say, Warshall's algorithm <ref> [37] </ref> (see also [2]), where n is the number of variables and constants contained in Afct, and m is the number of assignments to induction variables.
Reference: 38. <author> Wegman, M. N. and Zadeck, F. K., </author> <title> ``Constant Propagation with Conditional Branches,'' </title> <booktitle> in Proc. 12th ACM POPL, </booktitle> <month> Jan, </month> <year> 1985. </year>
Reference-contexts: They also propose these two data structures for methods to turn an expression tree into a dag, and for the more general basic block optimization of value numbering. Hashing is involved in preprocessing for global optimizations to perform constant propagation <ref> [38] </ref>, global redundant code elimination [5], and code motion [12]. The best algorithms for strength reduction [10,4] rely on hashed temporaries to obtain practical implementations. There are several reasons why hashing is used in these applications. <p> It also leads to a faster solution to the program equivalence prob lem used in integration by Yang, Horwitz, and Reps [40,41]. g Although the main parts of algorithms for global constant propagation <ref> [38] </ref>, global common subexpression detection [5], and code motion [12] do not use hashing, the preprocessing - 4 - portions for each of these algorithms do.
Reference: 39. <author> Willard, D., </author> <title> ``Quasilinear Algorithms for Processing Relational Calculus Expressions,'' </title> <booktitle> in Proc. PODS, </booktitle> <pages> pp. 243-257, </pages> <year> 1990. </year>
Reference-contexts: This problem with its two main aspects inverting a function and finding duplicate values in a multiset is basic. It is central to the database index formation techniques within Willard's database predicate retrieval theory <ref> [39] </ref>. It is essential to Earley's program optimization by itera-tor inversion [14]. It was also used by Paige and Tarjan to design new algorithms for the minimization of one-symbol finite automata [26] and lexicographic sorting [22].
Reference: 40. <author> Yang, W., Horwitz, S., and Reps, T., </author> <title> ``Detecting program components with equivalent behaviors,'' </title> <institution> TR-840, Computer Sciences Dept., Univ. of Wisconsin, Madison, WI, </institution> <month> April </month> <year> 1989. </year>
Reference: 41. <author> Yang, W., </author> <title> ``A new algorithm for semantics-based program integration,'' </title> <type> Ph.D. Dissertation, TR 962, </type> <institution> Computer Sciences Dept., Univ. of Wisconsin, Madison, WI, </institution> <month> August </month> <year> 1990. </year>
References-found: 41

