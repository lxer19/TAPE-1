URL: http://www.cs.princeton.edu/~felten/papers/hpca2_udma.ps
Refering-URL: http://www.cs.princeton.edu/~felten/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Protected, User-level DMA for the SHRIMP Network Interface  
Author: Matthias A. Blumrich, Cezary Dubnicki, Edward W. Felten, and Kai Li 
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science, Princeton University,  
Date: February, 1996.  
Note: In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture,  
Abstract: Traditional DMA requires the operating system to perform many tasks to initiate a transfer, with overhead on the order of hundreds or thousands of CPU instructions. This paper describes a mechanism, called User-level Direct Memory Access (UDMA), for initiating DMA transfers of input/output data, with full protection, at a cost of only two user-level memory references. The UDMA mechanism uses existing virtual memory translation hardware to perform permission checking and address translation without kernel involvement. The implementation of the UDMA mechanism is simple, requiring a small extension to the traditional DMA controller and minimal operating system kernel support. The mechanism can be used with a wide variety of I/O devices including network interfaces, data storage devices such as disks and tape drives, and memory-mapped devices such as graphics frame-buffers. As an illustration, we describe how we used UDMA in building network interface hardware for the SHRIMP multicomputer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> American National Standard for information systems. High-Performance Parallel Interface Mechanical, Electrical, and Signalling Protocol Specification (HIPPI-PH), </institution> <year> 1991. </year> <note> Draft number X3.183-199x. </note>
Reference-contexts: The high overhead of traditional DMA devices requires coarse grained transfers of large data blocks in order to achieve the available raw DMA channel bandwidths. This is particularly true for high-bandwidth devices such as network interfaces and HIPPI <ref> [1] </ref> devices. For example, the overhead of sending a piece of data over a 100 MByte/sec HIPPI channel on the Paragon multicomputer is more than 350 microsec onds [13].
Reference: [2] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Ber-shad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: Normally, a DMA transaction can be initiated only through the operating system kernel, which provides protection, memory buffer management, and related address translation. The overhead of this kernel-initiated DMA transaction is hundreds, possibly thousands of CPU instructions <ref> [2] </ref>. The high overhead of traditional DMA devices requires coarse grained transfers of large data blocks in order to achieve the available raw DMA channel bandwidths. This is particularly true for high-bandwidth devices such as network interfaces and HIPPI [1] devices.
Reference: [3] <author> Brian N. Bershad, David D. Redell, and John R. Ellis. </author> <title> Fast mutual exclusion for uniprocessors. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 223-233, </pages> <year> 1992. </year>
Reference-contexts: However, for longer messages the DMA-based controller is preferable because it makes use of the bus burst mode, which is much faster than processor-generated single word transactions. Our method for making the two-instruction transfer-initiation sequence appear to be atomic is related to Bershad's restartable atomic sequences <ref> [3] </ref>. Our approach is simpler to implement, since we have the kernel take a simple "recovery" action on every context switch, rather than first checking to see whether the application was in the middle of the two-instruction sequence.
Reference: [4] <author> M. Blumrich, C. Dubnicki, E. W. Felten, K. Li, and M. R. Mesarina. </author> <title> Two virtual memory mapped network interface designs. </title> <booktitle> In Proceedings of Hot Interconnects II Symposium, </booktitle> <pages> pages 134-142, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The overhead is the dominating factor which limits the utilization of DMA devices for fine grained data transfers. This paper describes a protected, user-level DMA mechanism (UDMA) developed at Princeton University as part of the SHRIMP project <ref> [4] </ref>. The UDMA mechanism uses virtual memory mapping to allow user processes to start DMA operations via a pair of ordinary load and store instructions. UDMA uses the existing virtual memory mechanisms address translation and permission checking to provide the same degree of protection as the traditional DMA operations.
Reference: [5] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Implementing just two queues, with the higher priority queue reserved for the system, would certainly be useful. 8 Implementation in SHRIMP The first UDMA device | the SHRIMP network interface <ref> [5] </ref> | is now working in our laboratory. Each node in the SHRIMP multicomputer is an Intel Pentium Xpress PC system [12] and the interconnect is an Intel Paragon routing backplane. <p> Our approach requires the application to explicitly check for failure and retry the operation; this does not hurt our performance since we require the application to check for other errors in any case. Several systems have used address-mapping mechanisms similar to our memory proxy space. Our original SHRIMP design <ref> [5] </ref> used memory proxy space to specify the source address of transfers. However, there was no distinction between memory proxy space and device proxy space: the same memory address played both roles. <p> In addition, our previous design did not generalize to handle device-to-memory transfers, and did not cleanly solve the problems of consistency between virtual memory and the DMA device. Our current design retains the automatic update transfer strategy described in <ref> [5] </ref> which still relies upon fixed mappings between source and destination pages. A similar address-mapping technique was used in the CM-5 [17] vector unit design.
Reference: [6] <institution> FORE Systems. TCA-100 TURBOchannel ATM Computer Interface, </institution> <note> User's Manual, </note> <year> 1992. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still hundreds of CPU instructions. In addition, the node is complex and expensive to build. Another approach to protected, user-level communication is the idea of memory-mapped network interface FIFOs <ref> [15, 6] </ref>. In this scheme, the controller has no DMA capability. Instead, the host processor communicates with the network interface by reading or writing special memory locations that correspond to the FIFOs. The special memory locations exist in physical memory and are protected by the virtual memory system.
Reference: [7] <author> John Heinlein, Kourosh Gharachorloo, Scott Dresser, and Anoop Gupta. </author> <title> Integration of message passing and shared memory in the stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In addition, the transfer is not a DMA, since the CPU stalls while the transfer is occurring. The Flash system <ref> [7] </ref> uses a technique similar to ours for communicating requests from user processes to communication hardware. Flash uses the equivalent of our memory proxy addresses (which they call "shadow addresses") to allow user programs to specify memory addresses to Flash's communication hardware.
Reference: [8] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: utilized the UDMA technique in the design of a network interface, we reiterate that it is applicable to a wide variety of high-speed I/O devices including graphics frame-buffers, audio and video devices, and disks. 2 Traditional DMA Direct Memory Access was first implemented on the IBM SAGE computer in 1955 <ref> [8] </ref> and has always been a common approach in I/O interface controller designs for data transfer between main memory and I/O devices. configured to perform DMA from memory to a device over an I/O bus.
Reference: [9] <author> Mark Homewood and Moray McLaren. </author> <title> Meiko CS-2 interconnect elan elite design. </title> <booktitle> In Proceedings of Hot Interconnects '93 Symposium, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: An increasingly common multicomputer approach to the problem of user-level transfer initiation is the addition of a separate processor to every node for message passing [16, 10]. Recent examples are the Stanford FLASH [14], Intel Paragon [11], and Meiko CS-2 <ref> [9] </ref>. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [10] <author> Jiun-Ming Hsu and Prithviraj Banerjee. </author> <title> A message passing coprocessor for distributed memory multicom-puters. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Until recently, this interest was primarily restricted to network interfaces for multicomputers. An increasingly common multicomputer approach to the problem of user-level transfer initiation is the addition of a separate processor to every node for message passing <ref> [16, 10] </ref>. Recent examples are the Stanford FLASH [14], Intel Paragon [11], and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: [11] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Until recently, this interest was primarily restricted to network interfaces for multicomputers. An increasingly common multicomputer approach to the problem of user-level transfer initiation is the addition of a separate processor to every node for message passing [16, 10]. Recent examples are the Stanford FLASH [14], Intel Paragon <ref> [11] </ref>, and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [12] <author> Intel Corporation. </author> <title> Express Platforms Technical Product Summary: System Overview, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Each node in the SHRIMP multicomputer is an Intel Pentium Xpress PC system <ref> [12] </ref> and the interconnect is an Intel Paragon routing backplane. The custom designed SHRIMP network interface is the key system component which connects each Xpress PC system to a router on the backplane. At the time of this writing, we have a four-processor prototype running.
Reference: [13] <author> Vineet Kumar. </author> <title> A host interface architecture for HIPPI. </title> <booktitle> In Proceedings of Scalable High Performance Computing Conference '94, </booktitle> <pages> pages 142-149, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This is particularly true for high-bandwidth devices such as network interfaces and HIPPI [1] devices. For example, the overhead of sending a piece of data over a 100 MByte/sec HIPPI channel on the Paragon multicomputer is more than 350 microsec onds <ref> [13] </ref>. With a data block size of 1 Kbyte, the transfer rate achieved is only 2.7 MByte/sec, which is less than 2% of the raw hardware bandwidth. Achieving a transfer rate of 80 MBytes/sec requires the data block size to be larger than 64 KBytes.
Reference: [14] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Until recently, this interest was primarily restricted to network interfaces for multicomputers. An increasingly common multicomputer approach to the problem of user-level transfer initiation is the addition of a separate processor to every node for message passing [16, 10]. Recent examples are the Stanford FLASH <ref> [14] </ref>, Intel Paragon [11], and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths. The compute and message processors can then work in parallel, to overlap communication and computation.
Reference: [15] <author> C.E. Leiserson, Z.S. Abuhamdeh, D.C. Douglas, C.R. Feynman, M.N. Ganmukhi, J.V. Hill, D. Hillis, B.C. Kuszmaul, M.A. St. Pierre, D.S. Wells, M.C. Wong, S. Yang, and R. Zak. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Proceedings of 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This approach, however, does not eliminate the overhead of the software protocol on the message processor, which is still hundreds of CPU instructions. In addition, the node is complex and expensive to build. Another approach to protected, user-level communication is the idea of memory-mapped network interface FIFOs <ref> [15, 6] </ref>. In this scheme, the controller has no DMA capability. Instead, the host processor communicates with the network interface by reading or writing special memory locations that correspond to the FIFOs. The special memory locations exist in physical memory and are protected by the virtual memory system.
Reference: [16] <author> R.S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Until recently, this interest was primarily restricted to network interfaces for multicomputers. An increasingly common multicomputer approach to the problem of user-level transfer initiation is the addition of a separate processor to every node for message passing <ref> [16, 10] </ref>. Recent examples are the Stanford FLASH [14], Intel Paragon [11], and Meiko CS-2 [9]. The basic idea is for the "compute" processor to communicate with the "message" processor through either mailboxes in shared memory or closely-coupled datapaths.
Reference: [17] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Our current design retains the automatic update transfer strategy described in [5] which still relies upon fixed mappings between source and destination pages. A similar address-mapping technique was used in the CM-5 <ref> [17] </ref> vector unit design. A program running on the main processor of a CM-5 computing node communicated command arguments to its four vector co-processors by accessing a special region of memory not unlike our memory proxy space.
References-found: 17

