URL: http://www.cs.ucsd.edu/users/ataylor/icip-analysis.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/ataylor/icip-analysis.html
Root-URL: http://www.cs.ucsd.edu
Email: Email: mgoldbaum@ucsd.edu  
Title: AUTOMATED DIAGNOSIS AND IMAGE UNDERSTANDING WITH OBJECT EXTRACTION, OBJECT CLASSIFICATION, AND INFERENCING IN RETINAL IMAGES  
Author: Michael Goldbaum, Saied Moezzi, Adam Taylor, Shankar Chatterjee, Jeff Boyd, Edward Hunter, and Ramesh Jain 
Address: La Jolla, California 92093-0946 USA  
Affiliation: Department of Ophthalmology and Department of Engineering and Computer Science University of California  
Abstract: Medical imaging is shifting from film to electronic images. The STARE (structured analysis of the retina) system is a sophisticated image management system that will automatically diagnose images, compare images, measure key features in images, annotate image contents, and search for images similar in content. We concentrate on automated diagnosis. The images are annotated by segmentation of objects of interest, classification of the extracted objects, and reasoning about the image contents. The inferencing is accomplished with Bayesian networks that learn from image examples of each disease. This effort at image understanding in fundus images anticipates the future use of medical images. As these capabilities mature, we expect that ophthalmologists and physicians in other fields that rely in images will use a system like STARE to reduce repetitive work, to provide assistance to physicians in difficult diagnoses or with unfamiliar diseases, and to manage images in large image databases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.H Goldbaum, N.P Katz, S. Chaudhuri, M. Nelson, </author> <title> Image Understanding for Automated Retinal Diagnosis, </title> <booktitle> Proceedings: The Thirteenth Annual Symposium on Computer Applications in Medical Care, </booktitle> <pages> pp. 756-760, </pages> <year> 1989. </year>
Reference: [2] <author> A. Gupta, S. Moezzi, A. Taylor, S. Chatterjee, R. Jain, S. Burgess, M. Goldbaum, </author> <title> Content-Based Retrieval of Ophthalmological Images, </title> <booktitle> IEEE International Conference on Image Processing, </booktitle> <volume> vol. ICIP-96, </volume> <year> 1996. </year>
Reference: [3] <author> W.E.L. </author> <title> Grimson, Medical Applications of Image Understanding, </title> <journal> IEEE Expert, </journal> <volume> vol.10, </volume> <pages> pp. 18-28, </pages> <year> 1995. </year>
Reference: [4] <author> S. Chaudhuri, S. Chatterjee, N. Katz, M. Nelson, M. Goldbaum, </author> <title> Detection of Blood Vessels in Retinal Images Using Two-Dimensional Matched Filters, </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> vol. 8, </volume> <pages> pp. 263-269, </pages> <year> 1989. </year>
Reference-contexts: In the horizontal orientation, ( ) ( ) K x y x y L, exp= - = 2 where ( ) K x y, is the transect profile of the blood vessel, L is the length of the segment, and s is the average blood vessel width <ref> [4] </ref>. The green plane is convolved with this template in 12 orientations over 180 with the output at each point being the maximum for the 12 orientations (figure1). This technique works in images with distorted blood vessels or confounding lesions surrounding or lying under the blood vessels.
Reference: [5] <author> S. Chaudhuri, S, Chatterjee, N. Katz, M. Goldbaum, </author> <booktitle> Automatic Detection of the Optic Nerve in Retinal Images Proceedings IEEE International Conference on Image Processing, Singapore, </booktitle> <volume> vol. 1, </volume> <pages> pp. 1-5, </pages> <year> 1989. </year>
Reference-contexts: The position of the pixel of maximum intensity in the average image indicates the position of the optic nerve. Averaging ameliorates the impact of confounding factors that occasionally make one of the properties unreliable <ref> [5] </ref>. Fovea: The fovea can be identified in the blue plane image. The fovea is located 4.5mm temporal to the optic nerve and is marked by yellow pigment in the retina, which shows as a dark spot in the blue plane image. 2.3.
Reference: [6] <author> D.E. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Classification and location of objects of interest 2.3.1. Classifier components The input vector was comprised of a feature set of mathematical properties of objects and object measurements meaningful to ophthalmologists tailored to the superclass of objects. The original feature set was reduced by genetic algorithms <ref> [6] </ref>. We tested linear discriminant function, quadratic discriminant function, logit classifier, and back propagation artificial neural networks. For each superclass of objects, we chose a classifier that balanced accuracy and computation cost. The learning was supervised. The accuracy was tested with cross validation. 2.3.2.
Reference: [7] <author> M. Fischler, J. Tenenbaum, H. Wolf, </author> <title> Detection of Roads and Linear Structures in Low Resolution Aerial Imagery Using a Multisource Knowledge Integration Technique, </title> <journal> Computer Graphics and Image Processing, </journal> <volume> Vol 15, </volume> <pages> 201-223, </pages> <year> 1981. </year>
Reference-contexts: This last feature, the Duda Road Operator, had the highest utility <ref> [7] </ref>. A linear classifier has been sufficiently accurate, yielding accuracy of 84% compared to the human expert. 2.3.3. Bright objects The list of bright objects includes exudates, cotton-wool spots, drusen, photocoagulation scars, subretinal fibrosis, and false objects.
Reference: [8] <author> M.H. Goldbaum, B.L Ct, R.F. Garcia, W.E. Hart, P. Kube, M. Nelson, </author> <title> Computer Detection and Classification of Lesions Found in Diabetic Retinopathy, </title> <journal> Invest Ophthalmol Vis Sci, </journal> <volume> vol. 33, </volume> <editor> p. </editor> <volume> 1082, </volume> <year> 1992. </year>
Reference-contexts: Bright objects The list of bright objects includes exudates, cotton-wool spots, drusen, photocoagulation scars, subretinal fibrosis, and false objects. Useful features included object color, border color, texture measures, compactness, area, edge gradient, and turns per length of the border <ref> [8] </ref>. For color and brightness measures, the image was normalize for image exposure by average background color obtained after removing all objects. The logit classifier was best, with an accuracy of 89% (figure 2,3) [9]. 2.3.4.
Reference: [9] <author> J.S. Cramer, </author> <title> The Logit Model: An Introduction for Economists, </title> <editor> Edward Arnold, </editor> <year> 1991. </year>
Reference-contexts: For color and brightness measures, the image was normalize for image exposure by average background color obtained after removing all objects. The logit classifier was best, with an accuracy of 89% (figure 2,3) <ref> [9] </ref>. 2.3.4. Dark objects The initial set of dark objects included hemorrhages, retinal blood vessels, pigment, and false objects. With the same features, the logit classifier provides accuracy of 78%. 2.3.5.
Reference: [10] <author> E. Horvitz, J. Breese, M. </author> <title> Henrion, </title> <journal> Decision Theory in Expert Systems and Artificial Intelligence, J Approximate Reasoning, </journal> <volume> Vol. 2, </volume> <pages> pp. 247-302, </pages> <year> 1988. </year>
Reference: [11] <author> D. Heckerman, </author> <title> A Tutorial on Learning with Bayesian Networks, </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research Advanced Technology Division, Microsoft Corporation, </institution> <year> 1996. </year>
Reference-contexts: Changing the classification or feature set requires repetition of this time-consuming and skill-intensive work. An expert system based on learning Bayesian networks reduces the need for such skilled labor, produces an audit trail of reasoning, and can incorporate beliefs or frequency analysis from the literature <ref> [11] </ref>. 2.4.2 Learning Bayesian probabilistic expert system A pilot inferencing system for fundus images in the STARE system has been developed. We present the format for the inferencing system as it is designed to be completed.

References-found: 11

