URL: http://omega.albany.edu:8008/robust.ps
Refering-URL: http://omega.albany.edu:8008/
Root-URL: 
Email: Email: carlos@math.albany.edu  
Title: BAYESIAN ROBUSTNESS: A NEW LOOK FROM GEOMETRY  
Author: Carlos C. Rodrguez 
Address: Albany NY 12222, USA  
Affiliation: Department of Mathematics and Statistics State University of New York at Albany  
Abstract: The geometric concept of the Lie derivative is introduced as the natural way of quantifying the intrinsic robustness of a hypothesis space. Prior and posterior probability measures are interpreted as differential forms defined invariantly on the hypothesis space. Rates of change with respect to local deformations of the model are computed by means of Lie derivatives of tensors defined on the model (like the Information metric, prior, posterior, etc.). In this way a field theory of inference is obtained. The class of deformations preserving the state of total ignorance is introduced and characterized by a partial differential equation. For location models this equation is the familiar r ~ = 0. A simple condition for the robustness of prior (or posterior) distributions is found: There is robustness when the deformation is along level surfaces of the prior (or posterior) density. These results are then applied to the class of entropic priors. It is shown that the hyper parameter controls the sensitivity with respect to local deformations. It is also shown that entropic priors are only sensitive to deformations that change the intrinsic form of the model around the initial guess. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Shun-ichi Amari. </author> <title> Differential-Geometrical Methods in Statistics, </title> <booktitle> volume 28 of Lecture Notes in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The geometrization of statistics is possible in part due to the fact that statistical models have a natural manifold structure. Fisher information endows the models with a Rieman-nian metric and the Kullback number (entropy) generates this and many other natural geodesic metrics on the model (see <ref> [1] </ref> and [7]). The main idea is to exploit the rich geometric structure available in the hypothesis space for the quantification of robustness. Once differential geometry is permitted to be the operational framework, a number of consequences for robustness are straight forward and inevitable. <p> It turns out that the Riemannian metric on the tangent space at P , g ij (), coincides with the Fisher information matrix at , see <ref> [1] </ref> and [7] for detailed definitions. A vector field ~ on the manifold P is a mapping that assigns to each P 2 P a tangent vector at P . <p> Noteworthy, this almost trivial change in point of view, helps to clarify an old puzzle of inference: How come that complete ignorance about a value x 2 <ref> [0; 1] </ref> is not complete ignorance about y = x 2 2 [0; 1]?. In other words, the change of variable theorem transforms the uniform density of x into the non uniform density 1 2 y 1=2 for y. <p> Noteworthy, this almost trivial change in point of view, helps to clarify an old puzzle of inference: How come that complete ignorance about a value x 2 <ref> [0; 1] </ref> is not complete ignorance about y = x 2 2 [0; 1]?. In other words, the change of variable theorem transforms the uniform density of x into the non uniform density 1 2 y 1=2 for y. <p> When considering densities as scalar fields there is no puzzle. The puzzle arises from the insistence, of the change of variables theorem, to keep the underlying measure to be the same (Lebesgue measure on <ref> [0; 1] </ref> in this case) for x and for y. <p> In fact, they do. But the change of variables theorem hides it by shifting the Jacobian from the volume element, where it belongs, to the density, where it does not belong. Our formula 11, assigns constant density to the numbers in <ref> [0; 1] </ref> in all coordinate systems, linear or non linear transformations of x. Formula 11 is composed as the product of two invariants. The scalar field density and the volume element. Remember that the volume element is invariant under all reparameter izations preserving orientation.
Reference: [2] <author> B.A. Dubrovin, </author> <title> A.T. Fomenko, and S.P. Novikov. Modern Geometry-Methods and Applications, Part-I, volume GTM 93 of Graduate Texts in Mathematics. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Local Deformations, Lie Derivatives of Tensors and Volume Elements We collect here some classical results from the geometry of vector fields on manifolds. The material in this section can be found on most books on modern geometry. We follow the presentation and notation of <ref> [2, chap. 23] </ref>. Regular (finite dimensional) parametric statistical models will be denoted by P = fP : 2 fig. They are Riemannian manifolds. The parameterization fi &lt; k plays the role of a coordinate system.
Reference: [3] <author> F. R. Hampel. </author> <title> A general qualitative definition of robustness. </title> <journal> Ann. Math. Statist., </journal> <volume> 42 </volume> <pages> 1887-1896, </pages> <year> 1971. </year>
Reference-contexts: There is general agreement about the desirability of a consistent theory of Statistical Robustness (Bayesian and non-Bayesian) and the large number of articles and books dedicated to the subject testify it. The technical definition of robustness is still controversial, however. For a serious criticism to the definitions of Hampel <ref> [3, p. 1980] </ref> and Huber [4, p. 10] see [6, p. 17]. In this paper, the geometric concept of the Lie derivative is introduced as a technical tool for quantifying robustness.
Reference: [4] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley and Sons, </publisher> <year> 1981. </year>
Reference-contexts: The technical definition of robustness is still controversial, however. For a serious criticism to the definitions of Hampel [3, p. 1980] and Huber <ref> [4, p. 10] </ref> see [6, p. 17]. In this paper, the geometric concept of the Lie derivative is introduced as a technical tool for quantifying robustness.
Reference: [5] <author> J. B. Kadane, </author> <title> editor. Robustness of Bayesian Analyses. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: 1. Introduction The Robustness, of a statistical procedure, is commonly defined as the stability with respect to small changes in the assumptions. This notion has immediate intuitive appeal and it has even been equated to the Holy Grail of Statistics (see <ref> [5] </ref>). There is general agreement about the desirability of a consistent theory of Statistical Robustness (Bayesian and non-Bayesian) and the large number of articles and books dedicated to the subject testify it. The technical definition of robustness is still controversial, however.
Reference: [6] <author> J. Pfanzagl. </author> <title> Contributions to a general asymptotic statistical theory. </title> <booktitle> Lecture Notes in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The technical definition of robustness is still controversial, however. For a serious criticism to the definitions of Hampel [3, p. 1980] and Huber [4, p. 10] see <ref> [6, p. 17] </ref>. In this paper, the geometric concept of the Lie derivative is introduced as a technical tool for quantifying robustness. The great arsenal of modern geometry tools provide a flexible, rigorous, and powerful framework for developing Statistical inference in general and Bayesian robustness in particular.
Reference: [7] <author> Carlos C. Rodrguez. </author> <title> The metrics induced by the kullback number. </title> <editor> In John Skilling, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: The geometrization of statistics is possible in part due to the fact that statistical models have a natural manifold structure. Fisher information endows the models with a Rieman-nian metric and the Kullback number (entropy) generates this and many other natural geodesic metrics on the model (see [1] and <ref> [7] </ref>). The main idea is to exploit the rich geometric structure available in the hypothesis space for the quantification of robustness. Once differential geometry is permitted to be the operational framework, a number of consequences for robustness are straight forward and inevitable. <p> It turns out that the Riemannian metric on the tangent space at P , g ij (), coincides with the Fisher information matrix at , see [1] and <ref> [7] </ref> for detailed definitions. A vector field ~ on the manifold P is a mapping that assigns to each P 2 P a tangent vector at P . <p> Quantifying robustness with Lie derivatives 7 3.3. Robustness of Entropic Priors The name and the derivation of Entropic Priors for the manifold of discrete distributions are due to Skilling (see [11]). The generalization to arbitrary regular parametric models appears in the same volume in <ref> [7] </ref>, see also [9], [8], [10]. Entropic priors are defined by their scalar field density. <p> This definition makes true the following: Theorem 2 Entropic priors are robust with respect to deformations preserving information at the initial guess, 0 It is well known that the Kullback number generates the Riemannian metric (see <ref> [7] </ref> or [9]). In fact, a simple Taylor expansion of the Kullback number produces: 8 Carlos C. Rodrguez ~ rI ( : 0 ) = h~; 0 i + o (j 0 j) where v = 0 is in fact a tangent vector at when 0 approaches .
Reference: [8] <author> Carlos C. Rodrguez. </author> <title> Objective bayesianism and geometry. </title> <editor> In Paul F. Fougere, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Quantifying robustness with Lie derivatives 7 3.3. Robustness of Entropic Priors The name and the derivation of Entropic Priors for the manifold of discrete distributions are due to Skilling (see [11]). The generalization to arbitrary regular parametric models appears in the same volume in [7], see also [9], <ref> [8] </ref>, [10]. Entropic priors are defined by their scalar field density. In the coordinate system of the 's they are given by () = c where, I ( : 0 ) is the Kullback number between the distributions labeled by and a given initial value 0 .
Reference: [9] <author> Carlos C. Rodrguez. </author> <title> Entropic priors. Available in Electronic Form on the Internet "gopher", </title> <address> Oct. 1991. gopher cscgoph2.albany.edu 2-4-12-1-1. </address>
Reference-contexts: Quantifying robustness with Lie derivatives 7 3.3. Robustness of Entropic Priors The name and the derivation of Entropic Priors for the manifold of discrete distributions are due to Skilling (see [11]). The generalization to arbitrary regular parametric models appears in the same volume in [7], see also <ref> [9] </ref>, [8], [10]. Entropic priors are defined by their scalar field density. In the coordinate system of the 's they are given by () = c where, I ( : 0 ) is the Kullback number between the distributions labeled by and a given initial value 0 . <p> This definition makes true the following: Theorem 2 Entropic priors are robust with respect to deformations preserving information at the initial guess, 0 It is well known that the Kullback number generates the Riemannian metric (see [7] or <ref> [9] </ref>). In fact, a simple Taylor expansion of the Kullback number produces: 8 Carlos C. Rodrguez ~ rI ( : 0 ) = h~; 0 i + o (j 0 j) where v = 0 is in fact a tangent vector at when 0 approaches . <p> This group, is known to be isomorphic to the orthochronous connected component of the identity of the Lorentz group for three dimensional space-time (i.e. a space with metric: x 2 + y 2 t 2 , see <ref> [9] </ref>). 5. Conclusions In retrospect, this paper should be consider a first attempt to demonstrate that it makes sense to use Lie derivatives for quantifying bayesian robustness. No doubt, the geometriza-tion of Inference provides a powerful language for asking questions about statistical procedures.
Reference: [10] <author> Carlos C. Rodrguez. </author> <title> From euclid to entropy. </title> <editor> In W. T. Grandy, Jr., editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Quantifying robustness with Lie derivatives 7 3.3. Robustness of Entropic Priors The name and the derivation of Entropic Priors for the manifold of discrete distributions are due to Skilling (see [11]). The generalization to arbitrary regular parametric models appears in the same volume in [7], see also [9], [8], <ref> [10] </ref>. Entropic priors are defined by their scalar field density. In the coordinate system of the 's they are given by () = c where, I ( : 0 ) is the Kullback number between the distributions labeled by and a given initial value 0 .
Reference: [11] <author> John Skilling. </author> <title> Classical Max Ent data analysis. </title> <editor> In John Skilling, editor, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Quantifying robustness with Lie derivatives 7 3.3. Robustness of Entropic Priors The name and the derivation of Entropic Priors for the manifold of discrete distributions are due to Skilling (see <ref> [11] </ref>). The generalization to arbitrary regular parametric models appears in the same volume in [7], see also [9], [8], [10]. Entropic priors are defined by their scalar field density.
References-found: 11

