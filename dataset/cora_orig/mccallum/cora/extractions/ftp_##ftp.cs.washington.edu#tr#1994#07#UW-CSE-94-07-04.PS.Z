URL: ftp://ftp.cs.washington.edu/tr/1994/07/UW-CSE-94-07-04.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Separating Data and Control Transfer in Distributed Operating Systems  
Author: Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska 
Date: October 1994.  
Note: Also appears in the Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems,  
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 94-07-04 July 17, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson et al. </author> <title> High-speed switch scheduling for local-area networks. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 11(4) </volume> <pages> 319-352, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: We wish to build tightly-integrated distributed system clusters, consisting of a modest number of high-performance workstations communicating within a single LAN-connected administrative domain. Newer LAN technologies include hardware flow-control and bandwidth reservation schemes that can guarantee that data packets are delivered reliably <ref> [1] </ref>. We therefore feel justified in treating data loss within the cluster as an extremely rare occurrence, and regard it as a catastrophic event.
Reference: [2] <author> B. N. Bershad et al. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Whenever possible, this communication is done using the remote read/write data transfer mechanism. part, control transfers are restricted between a client and its server-clerk. That is, control transfers are primarily intra-node cross-domain calls, which have been shown to be amenable to high-performance implementation <ref> [2, 13] </ref>. Notice also that our organization maintains the firewalls between untrusted clients and services and the abstractional convenience of procedural interfaces, without relying on conventional mechanisms like RPC for cross-machine communication.
Reference: [3] <author> A. Birrell et al. </author> <title> Network objects. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 217-230, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: In contrast, our system organization relies on the existence of very simple communication primitives and requires no coherency guarantees. Our system is also related to the Channel Model [10], Network Objects <ref> [3] </ref>, and other systems, like V [7], that use RPC for small data and a separate bulk data transport mechanism. Unlike most of these systems, in our model, there is no explicit activity or thread of control at the destination process to handle 17 an incoming stream of data.
Reference: [4] <author> A. D. Birrell. </author> <title> Secure communication using remote procedure calls. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 3(1) </volume> <pages> 1-14, </pages> <month> Feb. </month> <year> 1985. </year>
Reference-contexts: For instance, many environments routinely share files through NFS servers with exactly the same guarantees. However, there are environments where such trust may not be warranted. Distributed systems running in these environments have traditionally used encryption techniques to ensure authentication and security <ref> [4, 21] </ref>. The underpinning for such schemes is that data is encrypted and decrypted using secret or public key schemes. The choice of the particular communication primitiveRPC or remote memoryis irrelevant. With our communication model, this implies that each read and write has to be encrypted and decrypted.
Reference: [5] <author> A. D. Birrell et al. Grapevine: </author> <title> An exercise in distributed computing. </title> <journal> Commun. ACM, </journal> <volume> 25(4) </volume> <pages> 260-274, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Our use of clerks has much in common with earlier systems, e.g., Grapevine <ref> [5] </ref>. For example, the client is not aware that it is talking to a server's clerk. It sees the same abstract RPC interface, albeit local RPC. However, there are some important differences in our use of clerks. <p> Once again, we note that as the amount of data transferred increases, the overhead of control transfer can be amortized more effectively. 6 Related Work Section 3.1 has already described our relationship to previous work like Grapevine <ref> [5] </ref> and Spector's remote reference primitives [19]. This section discusses other related systems. Shared Virtual Memory (SVM) systems like Ivy [12] are related to our approach. In fact, we can implement our system organization over an SVM system.
Reference: [6] <author> M. A. Blumrich et al. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In particular, as mentioned in previous sections, our model explicitly separates the notion of data transfer from control transfer. Network interfaces for multicomputers like SHRIMP <ref> [6] </ref> and Hamlyn [25] share a common ancestry with us to Spector's work. Both of these interfaces use remote writes as the basic mechanism for data transfer and use segment or page descriptor based schemes to ensure protection.
Reference: [7] <author> D. R. Cheriton. </author> <title> The V kernel: A software base for distributed systems. </title> <journal> IEEE Software, </journal> <volume> 1(2) </volume> <pages> 19-42, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: In contrast, our system organization relies on the existence of very simple communication primitives and requires no coherency guarantees. Our system is also related to the Channel Model [10], Network Objects [3], and other systems, like V <ref> [7] </ref>, that use RPC for small data and a separate bulk data transport mechanism. Unlike most of these systems, in our model, there is no explicit activity or thread of control at the destination process to handle 17 an incoming stream of data.
Reference: [8] <author> W. J. Dally et al. </author> <title> Architecture of a message-driven processor. </title> <booktitle> In Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The proposed structure is applicable in environments other than distributed systems, e.g., in large-scale dedicated multiprocessors, where the cost of control transfer is high relative to that of data transfer. Traditionally, these multiprocessor systems, e.g., the J-Machine <ref> [8] </ref>, *T [16], etc., have opted for a single primitive that unifies remote transfer of data and control, in contrast to our approach. Acknowledgments The authors gratefully acknowledge Alex Klaiber's help in improving the graphical presentation of the paper.
Reference: [9] <author> G. Delp. </author> <title> The Architecture and Implementation of Memnet: A High-Speed Shared Memory Computer Communication Network. </title> <type> PhD thesis, </type> <institution> University of Delaware, </institution> <year> 1988. </year>
Reference-contexts: This large size might lead to false sharing between clerks resulting in suboptimal performance. Finally, most SVM implementations require non-trivial processing and control transfer at the machine that faults the page in, which is contrary to our approach. Specialized SVM systems like MemNet <ref> [9] </ref> have been implemented in hardware to gain performance. However, by requiring all shared memory to be coherent, these schemes rely on complex network interfaces and sometimes on broadcast primitives from the network.
Reference: [10] <author> D. K. Gifford and N. Glasser. </author> <title> Remote pipes and procedures for efficient distributed communication. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 6(3) </volume> <pages> 258-283, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: In contrast, our system organization relies on the existence of very simple communication primitives and requires no coherency guarantees. Our system is also related to the Channel Model <ref> [10] </ref>, Network Objects [3], and other systems, like V [7], that use RPC for small data and a separate bulk data transport mechanism.
Reference: [11] <author> J. H. Howard et al. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: However, these issues are independent of whether we use our structure or a traditional RPC-based structure. For example, many file system designs, e.g., Sprite [15], SNFS [20], and AFS <ref> [11] </ref>, which use RPC, require mechanisms for recoverability and cache maintenance. In traditional RPC-based distributed systems, the RPC runtime and transport implement timeout and exception mechanisms to automatically notify the user of remote machine failures. It might appear that in our organization, fault-tolerance might be a difficult goal to achieve.
Reference: [12] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: This section discusses other related systems. Shared Virtual Memory (SVM) systems like Ivy <ref> [12] </ref> are related to our approach. In fact, we can implement our system organization over an SVM system. Further, with SVM systems, the unit of sharing and data transfer is usually a page, which in modern processors can be upwards of 4K bytes.
Reference: [13] <author> J. Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-188, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Whenever possible, this communication is done using the remote read/write data transfer mechanism. part, control transfers are restricted between a client and its server-clerk. That is, control transfers are primarily intra-node cross-domain calls, which have been shown to be amenable to high-performance implementation <ref> [2, 13] </ref>. Notice also that our organization maintains the firewalls between untrusted clients and services and the abstractional convenience of procedural interfaces, without relying on conventional mechanisms like RPC for cross-machine communication.
Reference: [14] <author> A. Mohindra and M. Devarakonda. </author> <title> Distributed token management in a cluster file system. </title> <booktitle> To appear in Proceedings of the Symposium on Parallel and Distributed Processing, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: For example, workstation-cluster file system designs such as Calypso <ref> [14] </ref> use an RPC-based distributed token management scheme to handle cache coherence. This scheme can be extended to use our communication primitives without involving control transfers in most cases. Token acquire and release can be implemented using compare-and-swap operations. Token revocation is trickier.
Reference: [15] <author> M. N. Nelson, B. B. Welch, and J. K. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: However, these issues are independent of whether we use our structure or a traditional RPC-based structure. For example, many file system designs, e.g., Sprite <ref> [15] </ref>, SNFS [20], and AFS [11], which use RPC, require mechanisms for recoverability and cache maintenance. In traditional RPC-based distributed systems, the RPC runtime and transport implement timeout and exception mechanisms to automatically notify the user of remote machine failures.
Reference: [16] <author> R. S. Nikhil, G. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The proposed structure is applicable in environments other than distributed systems, e.g., in large-scale dedicated multiprocessors, where the cost of control transfer is high relative to that of data transfer. Traditionally, these multiprocessor systems, e.g., the J-Machine [8], *T <ref> [16] </ref>, etc., have opted for a single primitive that unifies remote transfer of data and control, in contrast to our approach. Acknowledgments The authors gratefully acknowledge Alex Klaiber's help in improving the graphical presentation of the paper.
Reference: [17] <author> M. D. Schroeder and M. Burrows. </author> <title> Performance of Firefly RPC. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: For example, measurements of Firefly RPC, a highly optimized system, showed that control transfer was responsible for 17 percent of the overall time of an RPC with no arguments and no results, and 7 percent of the overall time for a call with no arguments and a 1440 byte result <ref> [17] </ref>. Given this problem, we should ask whether distributed applications require a single primitive that unifies data and control transfer. To examine this question for one application, we consider NFS, which is probably the most common example of a distributed service in daily use.
Reference: [18] <author> M. D. Schroeder et al. Autonet: </author> <title> A high-speed, self-configuring local area network using point-to-point links. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(8) </volume> <pages> 1318-1335, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: The software emulation technique that we use in our implementation will not provide adequate performance in this case. However, it is feasible to do encryption and decryption in hardware. In fact, the AN1 controller <ref> [18] </ref> has mechanisms to decrypt and encrypt data using different keys as data is transmitted or received.
Reference: [19] <author> A. Z. Spector. </author> <title> Performing remote operations efficiently on a local computer network. </title> <journal> Commun. ACM, </journal> <volume> 25(4) </volume> <pages> 246-260, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: The transfer of data is not tied to the execution of a particular thread or a procedure. The model we propose is flexible yet simple and is amenable to very efficient implementations. Our model has much in common with Spector's remote references <ref> [19] </ref>. Like his approach, we have chosen simple primitives that allow an efficient implementation on contemporary workstation clusters. We have extended the remote access primitives to incorporate virtual memory, protected access, and time-slicing on workstations. <p> Once again, we note that as the amount of data transferred increases, the overhead of control transfer can be amortized more effectively. 6 Related Work Section 3.1 has already described our relationship to previous work like Grapevine [5] and Spector's remote reference primitives <ref> [19] </ref>. This section discusses other related systems. Shared Virtual Memory (SVM) systems like Ivy [12] are related to our approach. In fact, we can implement our system organization over an SVM system.
Reference: [20] <author> V. Srinivasan and J. C. Mogul. Spritely NFS: </author> <title> Experiments with cache-consistency protocols. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 45-57, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: However, these issues are independent of whether we use our structure or a traditional RPC-based structure. For example, many file system designs, e.g., Sprite [15], SNFS <ref> [20] </ref>, and AFS [11], which use RPC, require mechanisms for recoverability and cache maintenance. In traditional RPC-based distributed systems, the RPC runtime and transport implement timeout and exception mechanisms to automatically notify the user of remote machine failures.
Reference: [21] <author> J. G. Steiner, C. Neuman, and J. I. Schiller. </author> <title> Kerberos: An authentication service for open network systems. </title> <booktitle> In Proceedings of the Winter 1988 USENIX Conference, </booktitle> <pages> pages 191-202, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: For instance, many environments routinely share files through NFS servers with exactly the same guarantees. However, there are environments where such trust may not be warranted. Distributed systems running in these environments have traditionally used encryption techniques to ensure authentication and security <ref> [4, 21] </ref>. The underpinning for such schemes is that data is encrypted and decrypted using secret or public key schemes. The choice of the particular communication primitiveRPC or remote memoryis irrelevant. With our communication model, this implies that each read and write has to be encrypted and decrypted.
Reference: [22] <author> C. P. Thacker et al. </author> <title> Alto: A personal computer. </title> <editor> In Daniel P. Siewiorek, C. Gordon Bell, and Allen Newell, </editor> <booktitle> Computer Structures: Principles and Examples, chapter 33, </booktitle> <pages> pages 549-572. </pages> <publisher> McGraw-Hill Book Company, </publisher> <year> 1982. </year>
Reference-contexts: Like his approach, we have chosen simple primitives that allow an efficient implementation on contemporary workstation clusters. We have extended the remote access primitives to incorporate virtual memory, protected access, and time-slicing on workstations. These issues were largely unaddressed by Spector's work on the Alto <ref> [22] </ref>, which had neither virtual memory nor enforced address protection among a set of applications. Spector distinguished between primary operations that were performed by a special communication process, and secondary operations that were performed by a regular process.
Reference: [23] <author> C. A. Thekkath, H. M. Levy, and E. D. Lazowska. </author> <title> Efficient support for multicomputing on ATM networks. </title> <type> Technical Report 93-04-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Complete details of the model and its implementation can be found elsewhere <ref> [23] </ref>.) The co-processor contains descriptors that define remote memory segments; each descriptor includes the destination segment size, remote node address, and protection information for a segment. There are three non-privileged meta-instructions supported by the interface: WRITE, READ, and CAS (compare-and-swap).
Reference: [24] <author> T. von Eicken et al. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Also, no specific request is required by the receiver to initiate data receipt. Active Messages is a low-level mechanism that has been proposed for communicating between nodes in a dedicated, closely-coupled multicomputer <ref> [24] </ref>. The key idea in this design is that an incoming message carries with it an upcall address of a handler that integrates the message into the computation stream for the node.
Reference: [25] <author> J. Wilkes. </author> <title> Hamlynan interface for sender-based communications. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewlett Packard Laboratories, </institution> <month> Nov. </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: In particular, as mentioned in previous sections, our model explicitly separates the notion of data transfer from control transfer. Network interfaces for multicomputers like SHRIMP [6] and Hamlyn <ref> [25] </ref> share a common ancestry with us to Spector's work. Both of these interfaces use remote writes as the basic mechanism for data transfer and use segment or page descriptor based schemes to ensure protection. There are many differences as well, e.g., SHRIMP focuses on providing memory coherence.
References-found: 25

