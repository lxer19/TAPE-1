URL: http://www.cs.ucsb.edu/~kshen/lu2d_tec.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/RAPID.html
Root-URL: http://www.cs.ucsb.edu
Email: kshen@cs.ucsb.edu  jiao@cs.uiuc.edu  tyang@cs.ucsb.edu  
Title: Elimination Forest Guided 2D Sparse LU Factorization  
Author: Kai Shen Xiangmin Jiao Tao Yang 
Address: Santa Barbara, CA 93106  IL 61801  Santa Barbara, CA 93106  
Affiliation: Dept. of Computer Science University of California  Dept. of Computer Science University of Illinois Urbana-Champaign,  Dept. of Computer Science University of California  
Abstract: Sparse LU factorization with partial pivoting is important for many scientific applications and delivering high performance for this problem is difficult on distributed memory machines. Our previous work has developed an approach called S fl that incorporates static symbolic factorization, supernode partitioning and graph scheduling. This paper studies the properties of elimination forests and uses them to guide supernode partitioning/amalgamation and execution scheduling. The new design with 2D mapping effectively identifies dense structures without introducing too many zeros in the BLAS computation and exploits asynchronous parallelism with low buffer space cost. The implementation of this code, called S + , uses supernodal matrix multiplication which retains the BLAS-3 level efficiency and avoids unnecessary arithmetic operations. The experiments show that S + improves our previous code substantially and can achieve up to 11.04GFLOPS on 128 Cray T3E 450MHz nodes, which is the highest performance reported in the literature.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon. </author> <title> Progress in Sparse Matrix Methods for Large Sparse Linear Systems on Vector Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 1 </volume> <pages> 10-30, </pages> <year> 1987. </year>
Reference-contexts: Our second goal is to incorporate 2D block-based mapping in our framework. In the literature 2D mapping has been shown more scalable than 1D for dense LU and sparse Cholesky <ref> [1, 20, 21] </ref>. However there are difficulties to apply the 2D block-oriented mapping to the case of sparse LU factorization even the static structure is predicted in advance. First, pivoting operations and row interchanges require frequent and well-synchronized inter-processor communication when each column is distributed to multiprocessors. <p> Thus in designing 2D codes, we paid special attention to the usage of buffer space so that 2D codes are able to factorize large matrices under memory constraints. 3 Elimination forests and nonsymmetric supernode parti tioning In this section, we extend the previous work on elimination forests <ref> [1, 13] </ref> and identify the properties of elimination forests to design more robust strategies for supernode partitioning and detect when pivoting for different columns can be conducted concurrently. <p> Using Theorem 2 and Definition 4, this theorem is true. Our design for LU task scheduling using the above forest concept is different from the ones for Cholesky <ref> [1, 19] </ref> because pivoting and row interchanges complicate the flow control in LU. Using 13 Theorem 4, we are able to exploit some parallelism among F actor () tasks.
Reference: [2] <author> J. Demmel. </author> <title> Numerical Linear Algebra on Parallel Processors. </title> <booktitle> Lecture Notes for NSF-CBMS Regional Conference in the Mathematical Sciences, </booktitle> <month> June </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: When pivoting is required to maintain numerical stability fl A short version of this paper will appear in the 10th annual ACM Symposium on Parallel Algorithms and Architectures 1 for non-symmetric linear systems <ref> [2, 14] </ref>, it is very hard to produce high performance for this problem because partial pivoting operations dynamically change computation and communication patterns during the elimination process, and cause severe caching miss and load imbalance on modern computers with memory hierarchies. <p> The pivoting sequence is held until the factorization of the k-th column block is completed. Then the pivoting sequence is applied to the rest of the matrix. This is called "delayed pivoting" <ref> [2] </ref>. 2) Task U pdate (k; j) uses column block k (A k;k ; A k+1;k ; ; A N;k ) to modify column block j.
Reference: [3] <author> J. Demmel, S. Eisenstat, J. Gilbert, X. Li, and J. Liu. </author> <title> A Supernodal Approach to Sparse Partial Pivoting. </title> <type> Technical Report CSD-95-883, </type> <institution> EECS Department, UC Berkeley, </institution> <month> September </month> <year> 1995. </year> <note> To appear in SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting <ref> [3, 12, 13, 16] </ref>. Most notably, the recent shared memory implementation of SuperLU [3, 4, 18] has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. <p> The previous work has addressed parallelization using shared memory platforms or restricted pivoting [3, 12, 13, 16]. Most notably, the recent shared memory implementation of SuperLU <ref> [3, 4, 18] </ref> has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. <p> For example, it costs less than one second for most of our tested matrices, at worst it costs 2 seconds on a single node of Cray T3E, and the memory requirement is relatively small. The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU <ref> [3, 18] </ref>, provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines.
Reference: [4] <author> J. Demmel, J. Gilbert, and X. Li. </author> <title> An Asynchronous Parallel Supernodal Algorithm for Sparse Gaussian Elimination. </title> <type> Technical Report CSD-97-943, </type> <institution> EECS Department, UC Berkeley, </institution> <note> Febru-ary 1997. To appear in SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting [3, 12, 13, 16]. Most notably, the recent shared memory implementation of SuperLU <ref> [3, 4, 18] </ref> has achieved up to 2.58GFLOPS on 8 Cray C90 nodes.
Reference: [5] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An Extended Set of Basic Linear Algebra Subroutines. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 14 </volume> <pages> 18-32, </pages> <year> 1988. </year>
Reference-contexts: This is the key to maximize the use of BLAS-3 subroutines <ref> [5] </ref> in our algorithm. And on most current commodity processors with memory hierarchies, BLAS-3 subroutines usually outperform BLAS-2 subroutines substantially when implementing the same functionality [5]. Figure 1 illustrates an example of a partitioned sparse matrix and the black areas depict dense submatrices, subrows and subcolumns. Data mapping. <p> This is the key to maximize the use of BLAS-3 subroutines <ref> [5] </ref> in our algorithm. And on most current commodity processors with memory hierarchies, BLAS-3 subroutines usually outperform BLAS-2 subroutines substantially when implementing the same functionality [5]. Figure 1 illustrates an example of a partitioned sparse matrix and the black areas depict dense submatrices, subrows and subcolumns. Data mapping. Given an n fi n matrix A, assume that after the matrix partitioning it has N fi N submatrix blocks. <p> The BLAS-3 GEMM routine <ref> [5] </ref> may not directly be applicable because subcolumns or subrows in those submatrices may not be consecutive and the target block A i;j may have a nonzero structure different from that of product A i;k fl A k;j .
Reference: [6] <author> I. S. Duff. </author> <title> On Algorithms for Obtaining a Maximum Transversal. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 315-330, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: Notice that for any nonsingular matrix which does not have a zero-free diagonal, it is always possible to permute the rows of the matrix so that the permuted matrix has a zero-free diagonal <ref> [6] </ref>. We will use the following notations in the rest of this section. We will still call the matrix after symbolic factorization as A since this paper assumes the symbolic factorization is conducted first. <p> But the A T A approach overestimates substantially more nonzeros, which also indicates that the elimination tree of A T A introduces too many false dependency edges. All matrices are ordered using the minimum degree algorithm and the permutation algorithm for zero-free diagonal <ref> [6] </ref>.
Reference: [7] <author> C. Fu, X. Jiao, and T. Yang. </author> <title> A Comparison of 1-D and 2-D Data Mapping for Sparse LU Factorization on Distributed Memory Machines. </title> <booktitle> Proc. of 8th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: First, pivoting operations and row interchanges require frequent and well-synchronized inter-processor communication when each column is distributed to multiprocessors. Second, exploiting irregular parallelism to a maximum degree may need a substantial amount of extra buffer space. In <ref> [7] </ref>, we reported a preliminary version of the 2D code with a simple parallelism scheduling mechanism. Recently with a modified control mechanism called factor-ahead, S fl has achieved up to 6.87GFLOPS on 128 Cray T3E 300MHz nodes [8]. <p> In the 2D cyclic mapping, processors are viewed as a 2D grid, and a column block of A is assigned to a column of processors. 2D sparse LU Factorization is more scalable than the 1D data mapping <ref> [7, 17] </ref>. However 2D mapping introduces more overhead for pivoting and row swapping. Program partitioning. <p> We discuss three scheduling strategies below. The first one as reported in <ref> [7] </ref> is a basic approach in which computation flow is controlled by the pivoting tasks F actor (k).
Reference: [8] <author> C. Fu, X. Jiao, and T. Yang. </author> <title> Efficient Sparse LU Factorization with Partial Pivoting on Distributed Memory Architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 9(2) </volume> <pages> 109-125, </pages> <month> February </month> <year> 1998. </year>
Reference-contexts: In [7], we reported a preliminary version of the 2D code with a simple parallelism scheduling mechanism. Recently with a modified control mechanism called factor-ahead, S fl has achieved up to 6.87GFLOPS on 128 Cray T3E 300MHz nodes <ref> [8] </ref>. In this paper, we will briefly explain this control mechanism, and will further report several new performance-improving strategies based on elimination forests. Those strategies include supernode partitioning and amalgamation using the properties of elimination forests, efficient supernode-level matrix multiplication, and parallelism exploitation using elimination forests. <p> The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU [3, 18], provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines. In <ref> [8, 10] </ref>, we show that static factorization does not produce too many fill-ins for most of the tested matrices, even for large matrices using a simple matrix ordering strategy (minimum degree ordering). <p> Using DAGs to model irregular LU parallelism is good in helping us understand the parallelism in sparse LU and develop the first prototype of high performance message-passing LU code. In <ref> [8, 17] </ref>, we show that 1D RAPID code based on graph scheduling can actually outperform 2D codes with simpler scheduling methods when sufficient space is available. But 2D mapping exposes more parallelism, which makes 2D codes more scalable and easier to achieve load balance. <p> This idea has been used in the dense LU algorithm [14] and we extend it for asynchronous execution and incorporate a buffer space control mechanism. The details are in <ref> [8] </ref>. The factor-ahead technique still imposes a constraint that F actor (k + 1) must be executed after the completion of F actor (k). In order to exploit potential parallelism between F actor () tasks, our third design is to utilize dependence information implied by elimination forests. <p> We define the stage overlapping degree for updating tasks as maxfjk k 0 j j There exist tasks U pdate2D (k; fl) and U pdate2D (k 0 ; fl) executed concurrently.g It is proved in <ref> [8] </ref> that for the factor-ahead approach, the reachable overlapping degree is p c among all processors and the extra buffer space complexity is about 2:5BSIZE n S 1 where S 1 is the sequential space size for storing the entire sparse matrix and BSIZE is the maximum supernode size. <p> All matrices are ordered using the minimum degree algorithm and the permutation algorithm for zero-free diagonal [6]. In subsection 6.3, we will also report performance of S + for circuit simulation matrices. 6.1 Overall code performance Our previous study <ref> [8, 10] </ref> shows that even with the introduction of extra nonzero elements by static symbolic factorization, the performance of the S fl sequential code can still be competitive to SuperLU because we are able to use more BLAS-3 operations. <p> The performance improvement ratios vary from 22% to 40%. For the dense case, there is no improvement because the results of partitioning and matrix multiplication between two versions are the same in this case. For parallel performance, we compare our new code with the previous version <ref> [8] </ref> in Table 3 and the improvement ratio in terms of MFLOPS vary from 16% to 116%, in average more than 50%. Table 4 shows the absolute performance of the S + on an LBL's Cray T3E machine with 450MHz CPU.
Reference: [9] <author> C. Fu and T. Yang. </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pages 237-244, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: to eliminate the data structure variation caused by dynamic pivoting; 2) identify data regularity from the sparse structure obtained by the symbolic factorization so that efficient dense operations can be used to perform most of the computation; 3) make use of graph scheduling techniques and efficient run-time support called RAPID <ref> [9, 11] </ref> to exploit irregular parallelism. The preliminary experiments are encouraging and good performance results are obtained with 1D data mapping for a set of nonsymmetric benchmark matrices. We have achieved up to 1.35GFLOPS with RAPID code on 64 Cray T3E 300MHz nodes. <p> Figure 2 outlines the partitioned LU factorization algorithm with partial pivoting. The 1D RAPID code. We have implemented a parallel method with 1D data mapping using the RAPID runtime system <ref> [9, 10] </ref>. This code uses a DAG to model irregular parallelism and RAPID to schedule the tasks.
Reference: [10] <author> C. Fu and T. Yang. </author> <title> Sparse LU Factorization with Partial Pivoting on Distributed Memory Machines. </title> <booktitle> In Proceedings of ACM/IEEE Supercomputing, </booktitle> <address> Pittsburgh, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting [3, 12, 13, 16]. Most notably, the recent shared memory implementation of SuperLU [3, 4, 18] has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. For distributed memory machines, in <ref> [10] </ref> we proposed a novel approach called S fl that integrates three key strategies together in parallelizing this algorithm: 1) adopt a static symbolic factorization scheme [13] to eliminate the data structure variation caused by dynamic pivoting; 2) identify data regularity from the sparse structure obtained by the symbolic factorization so <p> The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU [3, 18], provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines. In <ref> [8, 10] </ref>, we show that static factorization does not produce too many fill-ins for most of the tested matrices, even for large matrices using a simple matrix ordering strategy (minimum degree ordering). <p> After an L supernode partition has been obtained on a sparse matrix A, the same partitioning is applied to the rows of the matrix to further break each supernode into submatrices. This is also known as U supernode partitioning. In <ref> [10] </ref>, we show that after the L=U supernode partitioning, each diagonal submatrix is dense, and 3 each nonzero off-diagonal submatrix in the L part contains only dense subrows, and furthermore each nonzero submatrix in the U factor of A contains only dense subcolumns. <p> Figure 2 outlines the partitioned LU factorization algorithm with partial pivoting. The 1D RAPID code. We have implemented a parallel method with 1D data mapping using the RAPID runtime system <ref> [9, 10] </ref>. This code uses a DAG to model irregular parallelism and RAPID to schedule the tasks. <p> But 2D mapping exposes more parallelism, which makes 2D codes more scalable and easier to achieve load balance. Also the RAPID implementation in <ref> [10] </ref> uses extra memory space for supporting general irregular computations. <p> Another observation is that Theorem 1 only holds under our definition of elimination forests. 3.2 2D L=U supernode partitioning and amalgamation Given a nonsymmetric matrix A after symbolic factorization, in <ref> [10] </ref> we have described a 2D L=U supernode partitioning in which two stage partitioning is applied. Stage 1: A group of consecutive columns that have the same structure in the L factor is considered as one supernode column block. <p> We do not need to check any constraint on U because as long as a child-parent pair (i; i 1) satisfies jL i j = jL i1 j 1, we can show that jU i j = jU i1 j 1 based on Theorem 1 in <ref> [10] </ref> and hence the structures of U i and U i1 are identical. Figure 7 (a) illustrates supernode partitioning of the sparse matrix in Figure 3. <p> Row and column permutations are needed if the parent is not consecutive with its children. For sparse LU, such a permutation may alter the symbolic factorization result. In our previous approach <ref> [10] </ref>, we simply compare the consecutive columns of the L factor, and make a decision on merging if the total number of difference is under a pre-set threshold. This approach is simple, resulting in a bounded number of extra zero entries included in the dense structure of L supernode. <p> All matrices are ordered using the minimum degree algorithm and the permutation algorithm for zero-free diagonal [6]. In subsection 6.3, we will also report performance of S + for circuit simulation matrices. 6.1 Overall code performance Our previous study <ref> [8, 10] </ref> shows that even with the introduction of extra nonzero elements by static symbolic factorization, the performance of the S fl sequential code can still be competitive to SuperLU because we are able to use more BLAS-3 operations. <p> The improvement over SuperLU for the dense case is the highest because our code can fully utilize BLAS-3 for this case. We also compare the sequential performance of S + with our previous design S fl <ref> [10] </ref>. The performance improvement ratios vary from 22% to 40%. For the dense case, there is no improvement because the results of partitioning and matrix multiplication between two versions are the same in this case. <p> Our new strategies for supernode partitioning with amalgamation cluster columns and rows simultaneously using structural containment information implied by an elimination forest. Our previous design S fl <ref> [10] </ref> does not consider the bounding of nonzeros in the U part. We compare our new code S + with a modified version using the previous partitioning strategy. The performance improvement ratio by using the new strategy is listed in Figure 12 and an average of 20% improvement is obtained.
Reference: [11] <author> C. Fu and T. Yang. </author> <title> Space and Time Efficient Execution of Parallel Irregular Computations. </title> <booktitle> In Proceedings of ACM Symposium on Principles & Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: to eliminate the data structure variation caused by dynamic pivoting; 2) identify data regularity from the sparse structure obtained by the symbolic factorization so that efficient dense operations can be used to perform most of the computation; 3) make use of graph scheduling techniques and efficient run-time support called RAPID <ref> [9, 11] </ref> to exploit irregular parallelism. The preliminary experiments are encouraging and good performance results are obtained with 1D data mapping for a set of nonsymmetric benchmark matrices. We have achieved up to 1.35GFLOPS with RAPID code on 64 Cray T3E 300MHz nodes.
Reference: [12] <author> K. Gallivan, B. Marsolf, and H. Wijshoff. </author> <title> The Parallel Solution of Nonsymmetric Sparse Linear Systems Using H* Reordering and an Associated Factorization. </title> <booktitle> In Proc. of ACM International Conference on Supercomputing, </booktitle> <pages> pages 419-430, </pages> <address> Manchester, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting <ref> [3, 12, 13, 16] </ref>. Most notably, the recent shared memory implementation of SuperLU [3, 4, 18] has achieved up to 2.58GFLOPS on 8 Cray C90 nodes.
Reference: [13] <author> A. George and E. Ng. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 219-240, </pages> <year> 1990. </year>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting <ref> [3, 12, 13, 16] </ref>. Most notably, the recent shared memory implementation of SuperLU [3, 4, 18] has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. <p> For distributed memory machines, in [10] we proposed a novel approach called S fl that integrates three key strategies together in parallelizing this algorithm: 1) adopt a static symbolic factorization scheme <ref> [13] </ref> to eliminate the data structure variation caused by dynamic pivoting; 2) identify data regularity from the sparse structure obtained by the symbolic factorization so that efficient dense operations can be used to perform most of the computation; 3) make use of graph scheduling techniques and efficient run-time support called RAPID <p> In this section, we briefly discuss related techniques used in our algorithm. Static symbolic factorization. Static symbolic factorization is proposed in <ref> [13] </ref> to identify the worst case nonzero patterns without knowing numerical values of elements. The basic idea is to statically consider all the possible pivoting choices at each elimination step and the space is allocated for all the possible nonzero entries. <p> Thus in designing 2D codes, we paid special attention to the usage of buffer space so that 2D codes are able to factorize large matrices under memory constraints. 3 Elimination forests and nonsymmetric supernode parti tioning In this section, we extend the previous work on elimination forests <ref> [1, 13] </ref> and identify the properties of elimination forests to design more robust strategies for supernode partitioning and detect when pivoting for different columns can be conducted concurrently. <p> Theorem 2 indicates dependency information in the numerical elimination, which can guide our parallel scheduling of asynchronous parallelism. George and Ng proposed a definition of elimination forests in <ref> [13] </ref> to control row-wise elimination. The difference between their definition and the above definition is that we impose the condition jL k j &gt; 1. In practice, we find that the tested matrices can have up to 50% of columns with zero lower-diagonal elements.
Reference: [14] <author> G. Golub and J. M. Ortega. </author> <title> Scientific Computing: An Introduction with Parallel Computing Compilers. </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: When pivoting is required to maintain numerical stability fl A short version of this paper will appear in the 10th annual ACM Symposium on Parallel Algorithms and Architectures 1 for non-symmetric linear systems <ref> [2, 14] </ref>, it is very hard to produce high performance for this problem because partial pivoting operations dynamically change computation and communication patterns during the elimination process, and cause severe caching miss and load imbalance on modern computers with memory hierarchies. <p> It is not necessary that F acor (k + 1) has to wait the completion of all tasks U pdate2D (k; fl). This idea has been used in the dense LU algorithm <ref> [14] </ref> and we extend it for asynchronous execution and incorporate a buffer space control mechanism. The details are in [8]. The factor-ahead technique still imposes a constraint that F actor (k + 1) must be executed after the completion of F actor (k).
Reference: [15] <author> A. Gupta, G. Karypis, and V. Kumar. </author> <title> Highly Scalable Parallel Algorithms for Sparse Matrix Factorization. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(5), </volume> <year> 1995. </year>
Reference-contexts: 1 Introduction Solution of sparse linear systems is a computational bottleneck in many problems. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast parallel algorithms have been developed <ref> [15, 19, 20] </ref>.
Reference: [16] <author> S. Hadfield and T. Davis. </author> <title> A Parallel Unsymmetric-pattern Multifrontal Method. </title> <type> Technical Report TR-94-028, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <month> August </month> <year> 1994. </year> <month> 23 </month>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting <ref> [3, 12, 13, 16] </ref>. Most notably, the recent shared memory implementation of SuperLU [3, 4, 18] has achieved up to 2.58GFLOPS on 8 Cray C90 nodes.
Reference: [17] <author> X. Jiao. </author> <title> Parallel Sparse Gaussian Elimination with Partial Pivoting and 2-D Data Mapping. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of California at Santa Barbara, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: As the static symbolic factorization proceeds, at step k the nonzero structure of each candidate pivot row is replaced by the union of the structures of all these candidate pivot rows except the elements in the first k 1 columns." Using an efficient implementation of the symbolic factorization algorithm <ref> [17] </ref>, this preprocessing step can be very fast. For example, it costs less than one second for most of our tested matrices, at worst it costs 2 seconds on a single node of Cray T3E, and the memory requirement is relatively small. <p> In the 2D cyclic mapping, processors are viewed as a 2D grid, and a column block of A is assigned to a column of processors. 2D sparse LU Factorization is more scalable than the 1D data mapping <ref> [7, 17] </ref>. However 2D mapping introduces more overhead for pivoting and row swapping. Program partitioning. <p> Using DAGs to model irregular LU parallelism is good in helping us understand the parallelism in sparse LU and develop the first prototype of high performance message-passing LU code. In <ref> [8, 17] </ref>, we show that 1D RAPID code based on graph scheduling can actually outperform 2D codes with simpler scheduling methods when sufficient space is available. But 2D mapping exposes more parallelism, which makes 2D codes more scalable and easier to achieve load balance.
Reference: [18] <author> X. Li. </author> <title> Sparse Gaussian Elimination on High Performance Computers. </title> <type> PhD thesis, </type> <institution> Computer Science Division, EECS, UC Berkeley, </institution> <year> 1996. </year>
Reference-contexts: The previous work has addressed parallelization using shared memory platforms or restricted pivoting [3, 12, 13, 16]. Most notably, the recent shared memory implementation of SuperLU <ref> [3, 4, 18] </ref> has achieved up to 2.58GFLOPS on 8 Cray C90 nodes. <p> For example, it costs less than one second for most of our tested matrices, at worst it costs 2 seconds on a single node of Cray T3E, and the memory requirement is relatively small. The dynamic factorization, which is used in the sequential and share-memory versions of SuperLU <ref> [3, 18] </ref>, provides more accurate data structure prediction on the fly, but it is challenging to parallelize SuperLU with low runtime control overhead on distributed memory machines. <p> L=U supernode partitioning. After the nonzero fill-in pattern of a matrix is predicted, the matrix is further partitioned using a supernodal approach to improve the caching performance. In <ref> [18] </ref>, a nonsymmetric supernode is defined as a group of consecutive columns in which the corresponding L factor has a dense lower triangular block on the diagonal and the same nonzero pattern below the diagonal. Based on this definition, in each column block the L part only contains dense subrows.
Reference: [19] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Solution of sparse linear systems is a computational bottleneck in many problems. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast parallel algorithms have been developed <ref> [15, 19, 20] </ref>. <p> This leads to relatively fine grained computation. In practice, amalgamation is commonly adopted to increase the average supernode size by introducing some extra zero entries in dense structures of supernodes. In this way, caching performance can be improved and interprocessor communication overhead may be reduced. For sparse Cholesky (e.g. <ref> [19] </ref>), the basic idea of amalgamation is to relax the restriction that all the columns in a supernode must have exactly the same off-diagonal nonzero structure. <p> Using Theorem 2 and Definition 4, this theorem is true. Our design for LU task scheduling using the above forest concept is different from the ones for Cholesky <ref> [1, 19] </ref> because pivoting and row interchanges complicate the flow control in LU. Using 13 Theorem 4, we are able to exploit some parallelism among F actor () tasks.
Reference: [20] <author> E. Rothberg and R. Schreiber. </author> <title> Improved Load Distribution in Parallel Sparse Cholesky Factorization. </title> <booktitle> In Proc. of Supercomputing'94, </booktitle> <pages> pages 783-792, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Solution of sparse linear systems is a computational bottleneck in many problems. If a matrix is symmetric and positive definite, Cholesky factorization can be used, for which fast parallel algorithms have been developed <ref> [15, 19, 20] </ref>. <p> Our second goal is to incorporate 2D block-based mapping in our framework. In the literature 2D mapping has been shown more scalable than 1D for dense LU and sparse Cholesky <ref> [1, 20, 21] </ref>. However there are difficulties to apply the 2D block-oriented mapping to the case of sparse LU factorization even the static structure is predicted in advance. First, pivoting operations and row interchanges require frequent and well-synchronized inter-processor communication when each column is distributed to multiprocessors.
Reference: [21] <author> R. Schreiber. </author> <title> Scalability of Sparse Direct Solvers, volume 56 of Graph Theory and Sparse Matrix Computation (Edited by Alan George and John R. </title> <editor> Gilbert and Joseph W.H. </editor> <booktitle> Liu), </booktitle> <pages> pages 191-209. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Our second goal is to incorporate 2D block-based mapping in our framework. In the literature 2D mapping has been shown more scalable than 1D for dense LU and sparse Cholesky <ref> [1, 20, 21] </ref>. However there are difficulties to apply the 2D block-oriented mapping to the case of sparse LU factorization even the static structure is predicted in advance. First, pivoting operations and row interchanges require frequent and well-synchronized inter-processor communication when each column is distributed to multiprocessors.
Reference: [22] <author> S. L. Scott and G. M. Thorson. </author> <title> The Cray T3E Network: Adaptive Routing in a High Performance 3D Torus. </title> <booktitle> In Proceedings of HOT Interconnects IV, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Each Cray-T3E processing element at SDSC has a clock rate of 300MHz, an 8Kbytes internal cache, 96Kbytes second level cache, and 128Mbytes memory. The peak bandwidth between nodes is reported as 500Mbytes/s and the peak round trip communication latency is about 0.5-2s <ref> [22] </ref>. We have observed that when block size is 25, double-precision GEMM achieves 388MFLOPS while double precision GEMV reaches 255MFLOPS. We have used block size 25 in our experiments. We recently obtained an access to a Cray-T3E at the NERSC division of the Lawrence Berkeley Lab.
Reference: [23] <author> A. Sherman and V. Gupta. </author> <type> Personal Communication, </type> <year> 1998. </year> <month> 24 </month>
Reference-contexts: This result is expected in the sense that the factor-ahead approach improves the degree of computation overlapping by scheduling factor tasks one step ahead while using elimination forests can exploit more parallelism. 6.3 Performance on circuit simulation matrices We recently obtained a few matrices from circuit simulation in Texas Instruments <ref> [23] </ref>, for which the static factorization may generate many extra fill-ins. We chose three of them which are large enough for parallel test and ran them using S + on 450MHz Cray T3E.
References-found: 23

