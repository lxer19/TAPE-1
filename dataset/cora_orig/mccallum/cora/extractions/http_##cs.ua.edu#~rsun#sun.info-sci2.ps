URL: http://cs.ua.edu/~rsun/sun.info-sci2.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00455.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: EMAIL: rsun, todd@cs.ua.edu  
Phone: PHONE: (205) 348-1667  
Title: Some Experiments with a Hybrid Model for Learning Sequential Decision Making  
Author: Ron Sun Todd Peterson 
Date: February 5, 1998  
Address: Tuscaloosa, AL 35487  
Affiliation: Department of Computer Science The University of Alabama  
Abstract-found: 0
Intro-found: 1
Reference: <author> J. R. Anderson, </author> <year> (1983). </year> <title> The Architecture of Cognition, </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA J. Anderson, </address> <year> (1990). </year> <title> Rules of the Mind. </title> <publisher> Lawrence Erlbaum Associates. </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: This differs from top-down learning in which low-level knowledge is acquired through "compiling" mostly externally given high-level knowledge <ref> (Anderson 1983) </ref>. Although the essential motivation for this model is cognitive modeling (see Sun 1997), this paper will focus only on computational experiments. Sequential decision tasks involve selecting and performing a sequence of actions to accomplish an objective on the basis of moment-to-moment perceptual information. <p> A two-level hybrid model provides the needed framework for representing both types of knowledge. Clarion is similar to the model in Sun (1994, 1995) but is specifically designed for sequential decision making. It consists of two levels: The bottom level contains procedural knowledge <ref> (Anderson 1983) </ref> and the top level contains declarative knowledge in the form of propositional rules. An overall pseudo-code algorithm that describes the operation of Clarion is as follows: 1. Observe the current state x. 2.
Reference: <author> J. Barnden, </author> <year> (1988). </year> <title> The right of free association: relative-position encoding for connectionist data structures, </title> <booktitle> Proc.10th Conference of Cognitive Science Society, </booktitle> <pages> 503-509, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> L. Breiman, </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, Vol.24, No.2, pp.123-140. </journal>
Reference: <author> A. Damasio et al, </author> <year> (1990). </year> <title> Neural regionalization of knowledge access. </title> <booktitle> In: Cold Spring Harbor Symp. on Quantitative Biology, </booktitle> <address> Vol.LV. </address> <publisher> CSHL Press. </publisher>
Reference: <author> H. Dreyfus and S. Dreyfus, </author> <year> (1987). </year> <title> Mind Over Machine, </title> <publisher> The Free Press, </publisher> <address> New York, NY. </address>
Reference: <author> D. Fisher, </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning. </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference: <author> P. Fitts and M. Posner, </author> <year> (1967). </year> <title> Human Performance. </title> <address> Brooks/Cole, Monterey, CA. </address>
Reference: <author> L.M. Fu, </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets, Proc.of AAAI'91, </title> <publisher> pp.590-595. </publisher>
Reference: <author> E. Gat, </author> <year> (1992). </year> <title> Integrating planning and reacting in a heterogeneous architecture. </title> <publisher> Proc.AAAI, pp.809-815. Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> J. Gelfand, D. Handelman and S. Lane, </author> <year> (1989). </year> <title> Integrating Knowledge-based Systems and Neural Networks for Robotic Skill Acquisition, </title> <publisher> Proc.IJCAI, pp.193-198. Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Others perform learning in a batch fashion, such as Miikkulainen and Dyer (1991), although their learning task (which is schema understanding) is quite difficult. Instead, Clarion performs on-line (concurrent) learning <ref> (Gelfand et al 1989) </ref>. Clarion is thus more cognitively plausible and more 29 ecologically realistic in this regard (Nosofsky et al 1994). Clarion is also capable of integrated learning (that is, developing connectionist and symbolic representation along side of each other), which is unlike any of the existing models.
Reference: <author> J. Hendler, </author> <year> (1987). </year> <title> Marker Passing and Microfeature, </title> <booktitle> Proc.10th IJCAI, </booktitle> <address> pp.151-154, </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> H. Hirsh, </author> <year> (1994). </year> <title> Generalizing version spaces. </title> <journal> Machine Learning, </journal> <volume> 17, </volume> <pages> 5-46. </pages>
Reference: <author> W. James, </author> <title> (1890). </title> <booktitle> The Principles of Psychology. </booktitle> <publisher> Dover, </publisher> <address> New York. </address>
Reference: <author> F. Keil, </author> <year> (1989). </year> <title> Concepts, Kinds, and Cognitive Development. </title> <publisher> MIT Press. </publisher> <address> Cambridge, MA. </address>
Reference: <author> L. Lin, </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. Machine Learning. </title> <editor> Vol.8, pp.293-321. 32 R. Maclin and J. Shavlik, </editor> <year> (1994). </year> <title> Incorporating advice into agents that learn from reinforce-ments. </title> <booktitle> Proc.of AAAI-94. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Meteo, CA. </address>
Reference: <author> P. Maes and R. Brooks, </author> <year> (1990). </year> <title> Learning to coordinate behaviors, </title> <booktitle> Proc.of National Conference on Artificial Intelligence. </booktitle> <address> pp.796-802. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> S. Mahadevan and J. </author> <title> Connell (1992), Automatic programming of behavior-based robot with reinforcement learning. </title> <publisher> Vol.55, pp.311-365. </publisher>
Reference: <author> R. Michalski, </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence. Vol.20, pp.111-161. </journal>
Reference: <author> R. Miikkulainen & M. Dyer, </author> <year> (1991). </year> <title> Natural language processing with modular PDP networks and distributed lexicons. </title> <journal> Cognitive Science. </journal> <volume> 15(3). </volume> <month> pp.343-399. </month>
Reference: <author> T. Mitchell, </author> <year> (1982). </year> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226. </pages>
Reference-contexts: In terms of learning high-level declarative knowledge or rules for such tasks, however, the afore-identified characteristics of the task render most existing rule learning algorithms inapplicable. This is because they require either preconstructed exemplar sets (thus learning is not online; Michalski 1983, Quinlan 1986), incrementally given consistent instances <ref> (Mitchell 1982, Fisher 1986, Utgoff 1989) </ref>, or complex manipulations of learned structures when inconsistency is discovered (which is typically more complex than the limited time a reactive agent may have; Hirsh 1994). "Drifting" as analyzed above is clearly more than noise and inconsistency as considered by some learning algorithms because it
Reference: <author> R. Nosofsky, T. Palmeri, and S. McKinley, </author> <year> (1994). </year> <title> Rule-plus-exception model of classification learning. </title> <journal> Psychological Review. </journal> <volume> 101 (1), </volume> <pages> 53-79. </pages>
Reference-contexts: Others perform learning in a batch fashion, such as Miikkulainen and Dyer (1991), although their learning task (which is schema understanding) is quite difficult. Instead, Clarion performs on-line (concurrent) learning (Gelfand et al 1989). Clarion is thus more cognitively plausible and more 29 ecologically realistic in this regard <ref> (Nosofsky et al 1994) </ref>. Clarion is also capable of integrated learning (that is, developing connectionist and symbolic representation along side of each other), which is unlike any of the existing models.
Reference: <author> R. Quinlan, </author> <year> (1986). </year> <title> Inductive learning of decision trees. </title> <journal> Machine Learning. </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> P. Rosenbloom, J. Laird, A. Newell, and R. McCarl, </author> <year> (1991). </year> <title> A preliminary analysis of the SOAR architecture as a basis for general intelligence. </title> <journal> Artificial Intelligence. </journal> <volume> 47 (1-3), </volume> <pages> 289-325. </pages>
Reference: <author> L. Shastri and V. Ajjanagadde, </author> <year> (1990). </year> <title> From simple association to systematic reasoning, </title> <type> Tech Report MS-CIS-90-05, </type> <institution> University of Pennsylvania, </institution> <address> Philodelphia, PA. </address>
Reference: <author> P. Smolensky, </author> <year> (1988). </year> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 11(1) </volume> <pages> 1-74. </pages>
Reference: <author> R. Sun, </author> <year> (1992). </year> <title> On Variable Binding in Connectionist Networks, Connection Science, </title> <type> Vol.4, No.2, </type> <institution> pp.93-124. </institution>
Reference-contexts: Although we can directly use a symbolic rule representation, to facilitate correspondence with the bottom level and to encourage uniformity and integration, we chose to use a localist connectionist model instead. Basically, we connect the nodes 8 representing the conditions of a rule to the node representing the conclusion <ref> (Sun 1992, Towell and Shavlik 1993) </ref>. That is, we directly translate the structure of a rule set to that of a network. For more complex rule forms including predicate rules and variable binding, see Sun (1992). Rule learning.
Reference: <author> R. Sun, </author> <year> (1994). </year> <title> Integrating Rules and Connectionism for Robust Commonsense Reasoning. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY. </address>
Reference-contexts: In our view, hybridness can be consistent or even principled. From a 28 cognitive perspective, the two-level architecture is principled, as it is based on the theories concerning the dichotomy of the conceptual and the subconceptual <ref> (Sun 1994, 1995, 1997) </ref>. In the experiments reported here, we see that there can also be a performance advantage in combining declarative knowledge/rule learning and procedural skills/reinforcement learning. Similarly, adopting multiple learning methods in hybrid models is sometimes dubbed the "kitchen sink approach".
Reference: <author> R. Sun, </author> <year> (1995). </year> <title> Robust reasoning: integrating rule-based and similarity-based reasoning. </title> <journal> Artificial Intelligence. </journal> <volume> 75, 2. </volume> <pages> 241-296. </pages>
Reference-contexts: When all parameter settings are considered, there is no significant difference at all. This shows that the two different rule learning methods, though different in terms of processes, are equivalent in terms of results. 3.2 Mazes We have also done extensive experiments in a maze domain <ref> (Sun and Peterson 1995) </ref> and the same basic conclusions hold. In the maze, the agent has sensory inputs regarding its immediate left, front and right side, indicating whether there is a wall, 20 The starting position is marked by `S' in which the agent faces upward to the upper wall.
Reference: <author> R. Sun, </author> <year> (1997). </year> <title> Learning, action, and consciousness: a hybrid approach towards modeling consciousness. Neural Networks, </title> <note> special issue on consciousness. in press. 33 R. </note> <author> Sun and T. Peterson, </author> <year> (1997). </year> <title> A hybrid agent architecture for reactive sequential decision making. </title> <editor> In: R. Sun and F. Alexandre, (eds.) Connectionist-Symbolic Integration. </editor> <publisher> Lawrence Erlbaum Associates. </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: This differs from top-down learning in which low-level knowledge is acquired through "compiling" mostly externally given high-level knowledge (Anderson 1983). Although the essential motivation for this model is cognitive modeling <ref> (see Sun 1997) </ref>, this paper will focus only on computational experiments. Sequential decision tasks involve selecting and performing a sequence of actions to accomplish an objective on the basis of moment-to-moment perceptual information. One example involves learning to navigate through mines (see Figure 1).
Reference: <author> R. Sun and T. Peterson, </author> <year> (1997). </year> <title> A hybrid model for learning sequential navigation. </title> <booktitle> Proc. of IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA'97). </booktitle> <address> Monterey, CA. pp.234-239. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: This differs from top-down learning in which low-level knowledge is acquired through "compiling" mostly externally given high-level knowledge (Anderson 1983). Although the essential motivation for this model is cognitive modeling <ref> (see Sun 1997) </ref>, this paper will focus only on computational experiments. Sequential decision tasks involve selecting and performing a sequence of actions to accomplish an objective on the basis of moment-to-moment perceptual information. One example involves learning to navigate through mines (see Figure 1).
Reference: <author> R. Sun, T. Peterson, and E. Merrill, </author> <year> (1996). </year> <title> Bottom-up skill learning in reactive sequential decision tasks. </title> <booktitle> Proc.of 18th Cognitive Science Society Conference, </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. pp.684-690. </address> <year> 1996. </year>
Reference-contexts: The superiority of R-moves in comparison with Q-moves demonstrates that in this case, mainly rule learning facilitates transfer to new and more complicated environments. 7 Overall, however, the transfer performance is suboptimal. This is due to the problems of partial observability and the problem of "unlearning" <ref> (identified and discussed in Sun et al. 1996) </ref>. 6 Transfer occurs because of the similarity of the two mazes (e.g., the prevalence of left turns; Willingham et al 1989). 7 The numbers in the figure indicate the differences between R-moves and Q-moves (or Moves) are significant in some cases, for example,
Reference: <author> R. Sun, E. Merrill, and T. Peterson, </author> <year> (1997). </year> <title> Skill learning using a bottom-up hybrid model. </title> <booktitle> Proc. of The First International Conference on Cognitive Science, </booktitle> <address> (Seoul, Korea. </address> <month> August 15-16, </month> <year> 1997.) </year> <month> pp.146-251. </month>
Reference-contexts: This differs from top-down learning in which low-level knowledge is acquired through "compiling" mostly externally given high-level knowledge (Anderson 1983). Although the essential motivation for this model is cognitive modeling <ref> (see Sun 1997) </ref>, this paper will focus only on computational experiments. Sequential decision tasks involve selecting and performing a sequence of actions to accomplish an objective on the basis of moment-to-moment perceptual information. One example involves learning to navigate through mines (see Figure 1).
Reference: <author> R. Sun and L. Bookman, (eds.) </author> <year> (1994). </year> <title> Computational Architectures Integrating Neural and Symbolic Processes . Kluwer Academic Publishers. </title> <publisher> Norwell, </publisher> <address> MA. </address>
Reference-contexts: In our view, hybridness can be consistent or even principled. From a 28 cognitive perspective, the two-level architecture is principled, as it is based on the theories concerning the dichotomy of the conceptual and the subconceptual <ref> (Sun 1994, 1995, 1997) </ref>. In the experiments reported here, we see that there can also be a performance advantage in combining declarative knowledge/rule learning and procedural skills/reinforcement learning. Similarly, adopting multiple learning methods in hybrid models is sometimes dubbed the "kitchen sink approach".
Reference: <author> R. Sutton, </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proc.of Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher> <address> San Mateo, CA. </address>
Reference-contexts: This approach has been applied to learning in mazes, navigation tasks, and robot control <ref> (Sutton 1990, Lin 1992, Mahadevan and Connell 1992) </ref>. But they do not learn declarative knowledge (generic rules). This approach can be extended to full-fledged dynamic programming and partially observable Markov decision process models; however, these models often require a domain model to begin with. <p> The lookup table implementation of Q-learning is out of question here because of the (likely) continuous input space and when discretized, the resulting huge state space (e.g., in the navigation task there are more than 10 12 states). Some kind of function approximator has to be used <ref> (Sutton 1990, Lin 1992, Mahadevan and Connell 1992, Tesauro 1992) </ref> and as a result the convergence of learning is not guaranteed. 2.2 The Top Level Rule encoding. Declarative knowledge is captured in a simple propositional rule form.
Reference: <author> T. Tesauro, </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning. </journal> <volume> Vol.8, </volume> <pages> 257-277. </pages>
Reference-contexts: We tested to see if removing hidden units at the bottom level would improve the performance. Our results indicated that, with hidden units removed from the bottom level, the model as a whole or the bottom level alone had great difficulty in learning the tasks <ref> (see also Tesauro 1992) </ref>. Thus, simply removing hidden units from the bottom level cannot be the answer.
Reference: <author> D. Touretzky and G. Hinton, </author> <year> (1987). </year> <title> Symbols among neurons, </title> <booktitle> Proc.9th IJCAI, </booktitle> <address> pp.23 8-243, </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> G. Towell and J. Shavlik, </author> <year> (1993). </year> <title> Extracting Refined Rules from Knowledge-Based Neural Networks, </title> <journal> Machine Learning. </journal> <volume> 13 (1), </volume> <pages> 71-101. </pages>
Reference: <author> P. </author> <title> Utgoff (1989). Incremental induction of decision trees. </title> <journal> Machine Learning. </journal> <volume> Vol.4, </volume> <pages> 161-186. </pages>
Reference: <author> C. Watkins, </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> Ph.D Thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, UK. </address>
Reference-contexts: Thus, the updating is based on the temporal difference in evaluating the current state and the action chosen. Through successive updates of the Q function, the agent can learn to take into account future steps in longer and longer sequences notably without explicit planning <ref> (Watkins 1989) </ref>. Implementation. To implement Q-learning, we chose to use a four-layered network, in which the first three layers form a backpropagation network for computing Q-values and the fourth layer (with only one node) performs stochastic decision making. <p> The output of the third layer (i.e., the output layer of the backpropagation network) indicates the Q-value of each action (represented by an individual node), and the node in the fourth layer determines probabilistically the action to be performed based on a Boltzmann distribution <ref> (Watkins 1989) </ref>: p (ajx) = e 1=ffQ (x;a) P Here ff controls the degree of randomness (temperature) of the decision-making process.
Reference: <author> G. Widmer and M. Kubat, </author> <year> (1996). </year> <title> Learning in the presence of concept drift and hidden context. Machine Learning. </title> <editor> Vol.23, No.1 D. Willingham, M. Nissen, and P. Bullemer, </editor> <year> (1989). </year> <title> On the development of procedural 34 knowledge. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition. </journal> <volume> 15, </volume> <pages> 1047-1060. </pages>
Reference-contexts: knowledge learned by an agent may be necessary. (2) Even when the world per se is stationary, it may still seem evolving to an agent learning to cope with the world, because different regions of the world may exhibit different characteristics and thus require the revision of concepts over time <ref> (Widmer and Kubat 1996) </ref>. <p> It often requires extra dedicated mechanisms <ref> (Widmer and Kubat 1996) </ref>.
References-found: 40

