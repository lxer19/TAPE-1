URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-46.ps.gz
Refering-URL: http://kmi.open.ac.uk/kmi-abstracts/kmi-tr-46-abstract.html
Root-URL: 
Title: Discovering Bayesian Networks in Incomplete Databases  
Author: Marco Ramoni Paola Sebastiani 
Date: March 1997  
Affiliation: Knowledge Media Institute  
Pubnum: KMI-TR-46  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. J. Brachman and T. Anand. </author> <title> The process of knowledge discovery in databases: A human-centered approach. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 36-58. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: The system is based on a new deterministic method to efficiently induce from an incomplete database the two components of a bbn: the network structure and the conditional probabilities. The system is designed to support kdd as an iterative and human centered process <ref> [1, 4] </ref>: bkd provides the user with the ability to define the pattern of missing data, it keeps track of information relevant to the assessment of the reliability of the estimates and of the inferred dependencies, such as the bc bounds and the log-likelihood, and it implements different search strategies to
Reference: [2] <author> W. Buntine. </author> <title> Graphical models for discovering knowledge. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 59-81. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year> <title> 9 Discovering Bayesian Networks in Incomplete Databases </title>
Reference-contexts: 1. Introduction Bayesian Belief Networks (bbns) are becoming increasingly popular in the Knowledge Discovery and Data Mining (kdd) community <ref> [2, 9] </ref>. bbns are a successful knowledge representation and reasoning formalism based on probability theory. A bbn [12] is defined by a graphical structure of conditional dependencies among the domain variables and a set of probability distributions defining these dependencies.
Reference: [3] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year>
Reference-contexts: Once the graphical model of conditional dependencies is known, efficient methods to learn the conditional probabilities take advantage of local computations and conjugate Bayesian analysis. The Bayesian approach to learn bbns from databases was pioneered by Cooper [6] and further developed by Buntine <ref> [3] </ref> and Heckerman [10]. A parallel line of research is going on in statistics, both in the general field of learning graphical models [19] and in the specific area of bbns [18].
Reference: [4] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (autoclass): Theory and results. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: The system is based on a new deterministic method to efficiently induce from an incomplete database the two components of a bbn: the network structure and the conditional probabilities. The system is designed to support kdd as an iterative and human centered process <ref> [1, 4] </ref>: bkd provides the user with the ability to define the pattern of missing data, it keeps track of information relevant to the assessment of the reliability of the estimates and of the inferred dependencies, such as the bc bounds and the log-likelihood, and it implements different search strategies to
Reference: [5] <author> D. M. Chickering and D. Heckerman. </author> <title> Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <type> Technical Report MSR-TR-96-08, </type> <institution> Microsoft Research, Microsoft Corporation, </institution> <year> 1996. </year>
Reference-contexts: Best-known methods typically involve the use of the em algorithm [7] or Markov Chain Monte Carlo methods, such as Gibbs sampling <ref> [5] </ref>. The basic strategy underlying these methods is based on the Missing Information Principle [11]: fill in the missing observations on the basis of the available information. Unfortunately, these approximate methods are prone to errors when little and/or biased information is available about the pattern of missing data.
Reference: [6] <author> G.F. Cooper and E. Herskovitz. </author> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Once the graphical model of conditional dependencies is known, efficient methods to learn the conditional probabilities take advantage of local computations and conjugate Bayesian analysis. The Bayesian approach to learn bbns from databases was pioneered by Cooper <ref> [6] </ref> and further developed by Buntine [3] and Heckerman [10]. A parallel line of research is going on in statistics, both in the general field of learning graphical models [19] and in the specific area of bbns [18]. <p> The bc method is further used to estimate the joint probability of (D inc ; M), which is then used by the model search strategies. The search methods implemented in bkd are based on the greedy search strategy devised by <ref> [6] </ref> generalized to learning from incomplete databases [14]. <p> Thus, p (DjM) depends on the updated hyper-parameters of ij jD, and the posterior precision on ij . The probability (7) is the basis of the greedy search algorithm proposed by <ref> [6] </ref>. Suppose that the possible models are equally likely a priori, and that the user can formulate a partial order on the variables so that X i X j if X i cannot be parent of X j .
Reference: [7] <author> A. Dempster, D. Laird, and Rubin D. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> J. Roy. Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: When this assumption fails, these methods have to resort to statistical techniques able to guess the missing data, or to asymptotic approximations which rely on estimates of the conditional probabilities defining a bbn. Best-known methods typically involve the use of the em algorithm <ref> [7] </ref> or Markov Chain Monte Carlo methods, such as Gibbs sampling [5]. The basic strategy underlying these methods is based on the Missing Information Principle [11]: fill in the missing observations on the basis of the available information.
Reference: [8] <author> U.M. Fayyad, G Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 1-36. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Furthermore, bbns can be easily extended into a complete decision-theoretic formalism | known as Influence Diagrams | able to provide normative decisions, that is, decisions with a formal guarantee to be the rational ones. Finally, the graphical form of the dependency model "leads itself easily to human interpretation" <ref> [8] </ref>, and provides a principled way to visualize data dependencies. <p> Still, the task of developing methods to learn from databases with missing data is one of the top priorities in the kdd research agenda <ref> [8] </ref>, and a fundamental step to move research products to applications. 1 Discovering Bayesian Networks in Incomplete Databases These paper introduces a computer system, called Bayesian Knowledge Discoverer (bkd), able to support the extraction of bbns from incomplete databases. bkd is based on a new deterministic method to extract bbns from
Reference: [9] <author> D. Heckerman. </author> <title> Bayesian networks for knowledge discovery. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: 1. Introduction Bayesian Belief Networks (bbns) are becoming increasingly popular in the Knowledge Discovery and Data Mining (kdd) community <ref> [2, 9] </ref>. bbns are a successful knowledge representation and reasoning formalism based on probability theory. A bbn [12] is defined by a graphical structure of conditional dependencies among the domain variables and a set of probability distributions defining these dependencies.
Reference: [10] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning bayesian networks: The combinations of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: Once the graphical model of conditional dependencies is known, efficient methods to learn the conditional probabilities take advantage of local computations and conjugate Bayesian analysis. The Bayesian approach to learn bbns from databases was pioneered by Cooper [6] and further developed by Buntine [3] and Heckerman <ref> [10] </ref>. A parallel line of research is going on in statistics, both in the general field of learning graphical models [19] and in the specific area of bbns [18]. Current methods are efficient under the assumption that the database is complete, i.e. it does not report any datum as unknown.
Reference: [11] <author> R.J.A. Little and D.B. Rubin. </author> <title> Statistical Analysis with Missing Data. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Best-known methods typically involve the use of the em algorithm [7] or Markov Chain Monte Carlo methods, such as Gibbs sampling [5]. The basic strategy underlying these methods is based on the Missing Information Principle <ref> [11] </ref>: fill in the missing observations on the basis of the available information. Unfortunately, these approximate methods are prone to errors when little and/or biased information is available about the pattern of missing data. We have recently identified a systematic distortion in the estimations provided by the Gibbs Sampling [17].
Reference: [12] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of plausible inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: 1. Introduction Bayesian Belief Networks (bbns) are becoming increasingly popular in the Knowledge Discovery and Data Mining (kdd) community [2, 9]. bbns are a successful knowledge representation and reasoning formalism based on probability theory. A bbn <ref> [12] </ref> is defined by a graphical structure of conditional dependencies among the domain variables and a set of probability distributions defining these dependencies.
Reference: [13] <author> M. Ramoni and P. Sebastiani. </author> <title> Efficient learning in Bayesian networks from incomplete databases. </title> <type> Technical Report KMi-TR-41, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-41. </note>
Reference-contexts: This method, called Bound and Collapse (bc), was originally conceived to estimate the conditional probabilities defining a bbn from incomplete databases <ref> [13] </ref>, but it has recently exploited to develop a method to extract the graphical model of bbns from incomplete databases [14]. <p> = l6=k The estimates ^p (x ik j ij ; D inc ; ijk ), k = 1; : : : ; c i , so found define a probability distribution since P c i k=1 ^p (x ik j ij ; D inc ; ijk ) = 1, see <ref> [13] </ref> for details. <p> The extracted conditional probabilities are almost identical, and this is consistent with the findings about the robustness of bc as parameter estimation method reported in <ref> [13, 16] </ref>. Further information can be accessed about the conditional probability value, such as the variance and the bounds computed by bc during the bounding step.
Reference: [14] <author> M. Ramoni and P. Sebastiani. </author> <title> Learning Bayesian networks from incomplete databases. </title> <type> Technical Report KMi-TR-43, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-43. </note>
Reference-contexts: This method, called Bound and Collapse (bc), was originally conceived to estimate the conditional probabilities defining a bbn from incomplete databases [13], but it has recently exploited to develop a method to extract the graphical model of bbns from incomplete databases <ref> [14] </ref>. Experimental evaluations [16] show clearly that the estimates provided by bc are equivalent to the ones provided by the Gibbs Sampling, when data are missing at random, and they are more robust to departure from the true pattern of missing data. <p> The bc method is further used to estimate the joint probability of (D inc ; M), which is then used by the model search strategies. The search methods implemented in bkd are based on the greedy search strategy devised by [6] generalized to learning from incomplete databases <ref> [14] </ref>. <p> This greedy-search strategy has been shown to be extremely cost-effective, but it can still get stuck into local minima. Therefore, bkd provides other more expensive search methods, such as random restarts, local arc-inversion, and even a form of exhaustive search over an ordered set of nodes. In <ref> [14] </ref> it was shown that the local marginal likelihood of a node X i and its parents i to the joint probability of (M; D), defined for the complete databases by (8), can be efficiently estimated as ^g (X i ; P i ) = j=1 ( ^ff ij ) k=1
Reference: [15] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust learning with missing data. </title> <type> Technical Report KMi-TR-28, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-28. </note>
Reference-contexts: If some of the entries in the database are missing, then it can be shown <ref> [15] </ref> that the Bayes estimate that would be computed from the complete database if known, is bounded above by p * (x ik j ij ; D inc ) = ff ij + h n (x ih j ij ) + n * (x ik j ij ) and below by
Reference: [16] <author> M. Ramoni and P. Sebastiani. </author> <title> The use of exogenous knowledge to learn Bayesian networks from incomplete databases. </title> <type> Technical Report KMi-TR-44, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-44. 10 Discovering Bayesian Networks in Incomplete Databases </note>
Reference-contexts: This method, called Bound and Collapse (bc), was originally conceived to estimate the conditional probabilities defining a bbn from incomplete databases [13], but it has recently exploited to develop a method to extract the graphical model of bbns from incomplete databases [14]. Experimental evaluations <ref> [16] </ref> show clearly that the estimates provided by bc are equivalent to the ones provided by the Gibbs Sampling, when data are missing at random, and they are more robust to departure from the true pattern of missing data. <p> , is then ijk jD inc ; ijk ~ D ( ~ff k1 ; ~ff k2 ), where ~ff k1 ; ~ff k2 are such that ^p (x ik j ij ; D inc ; ijk ) = ~ff k1 + ~ff k2 ~ff k1 ~ff k2 : Experimental comparisons <ref> [16] </ref> have shown that, when data are missing at random, the estimates computed by the bc method are equivalent to those obtained using stochastic methods based on the Missing Information Principle, as the Gibbs Sampling, but are more robust to departures from the true pattern of missing data. 2.2 Learning the <p> The extracted conditional probabilities are almost identical, and this is consistent with the findings about the robustness of bc as parameter estimation method reported in <ref> [13, 16] </ref>. Further information can be accessed about the conditional probability value, such as the variance and the bounds computed by bc during the bounding step.
Reference: [17] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust parameter learning in Bayesian networks with missing data. </title> <booktitle> In Proceedings of the Sixth Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 339-406, </pages> <address> Fort Lauderdale, FL, </address> <year> 1997. </year>
Reference-contexts: Unfortunately, these approximate methods are prone to errors when little and/or biased information is available about the pattern of missing data. We have recently identified a systematic distortion in the estimations provided by the Gibbs Sampling <ref> [17] </ref>. Furthermore, methods based on the Missing Information Principle are usually highly resource demanding, their convergence rates may be slow, and their execution time heavily depends on the number of missing data. <p> j ij ; D inc ); p * (x ik j ij ; D inc )] contains all posterior estimates of ijk that would be obtained from the possible completions of the database and therefore it provides a measure of the quality of information conveyed by D inc about ijk <ref> [17] </ref>.
Reference: [18] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year>
Reference-contexts: The Bayesian approach to learn bbns from databases was pioneered by Cooper [6] and further developed by Buntine [3] and Heckerman [10]. A parallel line of research is going on in statistics, both in the general field of learning graphical models [19] and in the specific area of bbns <ref> [18] </ref>. Current methods are efficient under the assumption that the database is complete, i.e. it does not report any datum as unknown.
Reference: [19] <author> J. Whittaker. </author> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The Bayesian approach to learn bbns from databases was pioneered by Cooper [6] and further developed by Buntine [3] and Heckerman [10]. A parallel line of research is going on in statistics, both in the general field of learning graphical models <ref> [19] </ref> and in the specific area of bbns [18]. Current methods are efficient under the assumption that the database is complete, i.e. it does not report any datum as unknown. <p> The database is complete. Data are reported in Whittaker <ref> [19] </ref>. 3.1 Learning the Graphical Model The first task to accomplish for bkd is extract to extract fro the database the most probable model of conditional dependencies using the algorithm described in Section 2.2.
References-found: 19

