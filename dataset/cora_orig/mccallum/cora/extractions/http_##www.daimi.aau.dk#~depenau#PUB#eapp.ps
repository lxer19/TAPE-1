URL: http://www.daimi.aau.dk/~depenau/PUB/eapp.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Title: Evaluation of The Cascade Correlation Algorithm and some new ideas  
Author: Jan Depenau 
Date: 24th October 1994  
Address: Ny Munkegade, Bldg. 540, DK-8000 Aarhus C, Denmark  
Affiliation: Denmark and DAIMI, Computer Science Department, Aarhus University,  
Note: TERMA Elektronik AS, Hovmarken 4 DK-8520 Lystrup,  
Abstract: The Cascade Correlation approach [Fahlman et al. 90] is described and some major problems with the algorithm are pointed out. In order to understand these problems and attempt to solve them, the Cascade Correlation Algorithm is reexamined and derived from an optimising theoretical point of view. New ideas to overcome these problems are suggested. 
Abstract-found: 1
Intro-found: 1
Reference: [Alpaydin 91] <author> E. </author> <month> Alpaydin </month> <year> (1991): </year> <title> GAL : Networks that grow when they learn and shrink when they forget, </title> <type> TR 91-032, </type> <institution> International Computer Science Institute. </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In the last decade several methods for successively building a neural network during training have been proposed. These methods are often called growth or constructive and sometimes "start small and add". A survey of current constructive algorithms can be found in Alpaydin <ref> [Alpaydin 91] </ref>. Among some of the more popular methods such as the Upstart algorithm [Frean 90] and the Tiling algorithm [Mezard et al. 89], the Cascade-Correlation Algorithm (CCA) has attracted most attention.
Reference: [Fahlman 88] <author> S. E. </author> <title> Fahlman (1988): Faster-Learning Variations on BackPropagation: An Empirical Study, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference: [Fahlman et al. 90] <author> S. E. Fahlman and C. </author> <booktitle> Lebiere (1990), The Cascade-Correlation Learning Architecture, in Neural Information Processing Systems 2, </booktitle> <editor> Editors D. Touretzky, </editor> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <address> Denver, Colorado, </address> <pages> pp. 524-532. </pages>
Reference-contexts: Empiri cal results <ref> [Fahlman et al. 90] </ref> show that performance is improved by using the covariance instead of the correlation. This means that by omitting the normalisation of the covariance, performance is improved. <p> to the candidate from the original input and output from previously inserted candidate units, if V c is not the first unit that is inserted. 5 What is Wrong with the CCA ? The CCA has proven to be very efficient in the task of learning almost any classification problem <ref> [Fahlman et al. 90] </ref>. The constructed networks' ability to generalize, however, has not shown the same convincing results. The reason is that CCA tends to build networks with a high capacity. <p> The reason might be that the original CCA developed by Fahlman does not use the Gradient Descent algorithm for updating the weights to the output unit, but instead 473 uses the Quick-prop algorithm. The Quick-prop learning algorithm was in-vented by Fahlman <ref> [Fahlman et al. 90] </ref>, with the purpose of speeding up the learning procedure. The idea in Quick-prop is to use information about the second order derivatives of the error. It depends on two approximations and it is therefore not expected to find the precise minimum.
Reference: [Frean 90] <author> M. </author> <month> Frean </month> <year> (1990), </year> <title> The Upstart Algorithm: A Method for Constructing and Training Feedforward Neural Networks, </title> <journal> Neural Computation, </journal> <volume> Vol. 2, </volume> <pages> pp. 198-209. </pages>
Reference-contexts: These methods are often called growth or constructive and sometimes "start small and add". A survey of current constructive algorithms can be found in Alpaydin [Alpaydin 91]. Among some of the more popular methods such as the Upstart algorithm <ref> [Frean 90] </ref> and the Tiling algorithm [Mezard et al. 89], the Cascade-Correlation Algorithm (CCA) has attracted most attention. The reason might be that the CCA works with continuous activity functions such as hyperbolic tangent , in contradiction to others which only work with binary units.
Reference: [Gallant 86] <author> M. </author> <title> Gallant (1986), Three Methods for Constructing a Neural Network, </title> <journal> Neural Computation, </journal> <volume> Vol. 2, </volume> <pages> pp. 198-209. </pages>
Reference-contexts: That is the methods try to do the optimal, but do not succeed, so one hopes that what they have done will do some good. The idea of just adding units whose weights have been randomly chosen has been proposed by Gallant <ref> [Gallant 86] </ref>. He argues that such units would be feature detectors and using enough of these units the task will eventually be solved. Gallant does not specify the features, they are just some arbitrary features.
Reference: [Hwang et al. 94] <author> Hwang, J. You S., Lay. S. and Jou, I. </author> <year> (1994), </year> <title> What's Wrong with A Cascaded Correlation Learning Network: A Projection Pursuit Learning Perspective. Found on connectionist ftp list. </title>
Reference-contexts: Secondly the CCA totally misses to incorporate the error between the function that the candidate is able to implement and the desired function. To make this more clear and get a better understanding of what really goes on, the following example has been constructed. The example is inspired by <ref> [Hwang et al. 94] </ref>, who used a similar example to illustrate another problem, more about this later. EXAMPLE 1: Consider a one-input to one-output mapping problem where the initialised network consists of one linear output unit with connection only to the input unit, no bias is applied, see figure 3.a. <p> This indicates that it might be a good idea, when the network is trained for the first time, to take into account that an additional unit, the candidate, is to be inserted. In <ref> [Hwang et al. 94] </ref>, an example similar to example 1 is given in order to illustrate another problem.
Reference: [Mezard et al. 89] <author> M. Mezard and J.P. </author> <month> Nadal </month> <year> (1989), </year> <title> Learning in Feedfor-ward Layered Networks: The Tiling Algorithm, </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> Vol. 22, </volume> <pages> pp. 2191-2203. 482 </pages>
Reference-contexts: These methods are often called growth or constructive and sometimes "start small and add". A survey of current constructive algorithms can be found in Alpaydin [Alpaydin 91]. Among some of the more popular methods such as the Upstart algorithm [Frean 90] and the Tiling algorithm <ref> [Mezard et al. 89] </ref>, the Cascade-Correlation Algorithm (CCA) has attracted most attention. The reason might be that the CCA works with continuous activity functions such as hyperbolic tangent , in contradiction to others which only work with binary units.
Reference: [Pao 89] <author> Pao. H. </author> <year> (1989). </year> <title> Adaptive Pattern Recognition and Neural Networks. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Gallant does not specify the features, they are just some arbitrary features. If more regular features were used, like the multiplication of each single input, this approach would be similar to what is known as the functional link, invented by Pao <ref> [Pao 89] </ref>. The functional link principle has shown very convincing results, see for example figure 9 where the method is used to solve the XOR problem.
Reference: [Vapnik 82] <author> Vapnik, V.N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Berlin: Springer-Verlag. </publisher> <pages> 483 </pages>
Reference-contexts: The constructed networks' ability to generalize, however, has not shown the same convincing results. The reason is that CCA tends to build networks with a high capacity. Vapnik 467 <ref> [Vapnik 82] </ref> and many others have shown that the generalization ability is closely related to the capacity of a network. If the capacity is too small, the network will not be able to learn the training set and if the capacity is too large, the network will never generalise.
References-found: 9

