URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/multipol/papers/grobner-correct.ps
Refering-URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/multipol/papers.html
Root-URL: http://www.cs.berkeley.edu
Title: On the Correctness of a Distributed Memory Grobner Basis Algorithm  
Author: Soumen Chakrabarti and Katherine Yelick 
Address: Berkeley, CA 94720, USA  
Affiliation: University of California,  
Abstract: We present an asynchronous MIMD algorithm for Grobner basis computation. The algorithm is based on the well-known sequential algorithm of Buchberger. Two factors make the correctness of our algorithm nontrivial: the nondeterminism that is inherent with asynchronous parallelism, and the distribution of data structures which leads to inconsistent views of the global state of the system. We demonstrate that by describing the algorithm as a nondeterministic sequential algorithm, and presenting the optimized parallel algorithm through a series of refinements to that algorithm, the algorithm is easier to understand and the correctness proof becomes manageable. The proof does, however, rely on algebraic properties of the polynomials in the computation, and does not follow directly from the proof of Buchberger's algorithm.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. Attardi and C. Traverso. </author> <title> A Network Implementation of Buchberger Algorithm. </title> <type> Technical Report 1177, </type> <institution> University di Pisa, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: A survey of the theory can be found in Mishra [14]. Parallel implementations have been surveyed by Vidal [19]. Earlier network implementations have been reported by Siegl [17], Attardi et al <ref> [1] </ref> and Hawley [13]. 3 Algorithm Design In this section we develop the parallel algorithm, starting from the sequential algorithm.
Reference: 2. <author> L. Bachmair, N. Dershowitz, and J. Hsiang. </author> <title> Orderings for Equational Proofs. </title> <booktitle> In Proceedings of the Symposium on Logic in Computer Science, </booktitle> <pages> pages 346-357. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: Similarly, the liveness argument is analogous to termination for Grobner basis. However, Hilbert's Basis Theorem would be replaced by the proof ordering notion of Bachmair et al as the basic measure of progress <ref> [2] </ref>. The proof ordering results are already quite general, giving the correctness of nondeterministic algorithm with interreduction, similar to IG-1 here. 6 Future work and Conclusion In this paper, we have described the design and implementation of a parallel Grobner basis procedure.
Reference: 3. <author> M. P. Bonacina. </author> <title> Distributed Automated Deduction. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, SUNY at Stony Brook, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: A precursor to this work was a shared memory implementation of the Knuth-Bendix procedure, which was also based on a transition axiom style [22]. See also [18] for a generic parallel completion procedure and <ref> [3] </ref> for some related correctness results for distributed computations. The pragmatic question of whether other completion procedures would perform well on distributed memory machines is beyond the scope of this paper.
Reference: 4. <author> B. </author> <title> Buchberger. Grobner basis: an algorithmic method in polynomial ideal theory. </title> <editor> In N. K. Bose, editor, </editor> <booktitle> Multidimensional Systems Theory, chapter 6, </booktitle> <pages> pages 184-232. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: 1 Introduction Buchberger introduced the notion of a Grobner basis of a set of polynomials and presented an algorithm for computing it <ref> [4] </ref>. We present an algorithm based on his for computing Grobner bases on a MIMD distributed memory multiprocessor. Although somewhat controversial [12], Buchberger and others believe that interreduction (keeping the basis reduced with respect to itself) is essential to performance. <p> Buchberger describes an elaborate way to keep track of polynomials that become reducible each time the basis grows, so that after each addition the basis is interreduced, i.e., basis polynomials are sequentially reduced by each other until nothing more can be reduced <ref> [4] </ref>. In a parallel algorithm, this global in-terreduction could potentially change a polynomial used by any other transition axiom, yet we cannot afford to stop other work while interreduction proceeds. We therefore introduce a single interreduction step as a separate transition axiom. computation.
Reference: 5. <author> B. </author> <title> Buchberger. A Criterion for detecting Unnecessary Reductions in the construction of Grobner Bases. </title> <booktitle> In Proceedings of the EUROSAM '79, An International Symposium on Symbolic and Algebraic Manipulation, </booktitle> <pages> pages 3-21, </pages> <address> Marseille, France, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: S reduces the tuple D Ideal (Hmono (G)); jgpqj E be verified easily by examining each axiom. (See Dershowitz and Manna [10] for similar termination proving techniques.) Algebraic optimizations to the basic algorithm have been developed that test s-polynomials to quickly detect reduction to zero, without actually performing the reduction <ref> [5] </ref>. Although our implementation includes such improvements, we omit them from the proofs for simplicity. 3.2 Transition Axiom Specification Transition axioms are a means to exploit non-determinism in a sequential algorithm description.
Reference: 6. <author> N. J. Burnett. </author> <title> The Architecture of the CM-5. </title> <booktitle> In IEEE Colloquium on `Medium Grain Distributed Computing' (Digest 070), </booktitle> <pages> pages 1-2, </pages> <address> London, </address> <month> 26 March </month> <year> 1992. </year>
Reference-contexts: on the sequential program. ? This work was supported in part by the Advanced Research Projects Agency of the Department of Defense monitored by the Office of Naval Research under contract DABT63-92-C-0026, by AT&T, and by a National Science Foundation through an Infrastructure Grant (number CDA-8722788) and Research Initiation Award <ref> (number CCR-9210260) </ref>. The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. * The proofs are structured around distributed data structures. <p> Proof. Partial correctness follows from lemma 9 and 11. For termination, we adapt the proof of lemma 9, using lemma 12 and replacing G by G in the tuple in lemma 9. 4 Implementation and Performance Our prototype runs on the CM-5 multiprocessor <ref> [6] </ref>. Each processor is a 33 MHz (15-20 MIPS) Sparc with about 8 MB of memory. The network is a fat-tree supporting at most 20 MB/s point-to-point data transfer. The prototype is in C, with the active message layer [20] for communication.
Reference: 7. <author> S. Chakrabarti. </author> <title> A distributed memory Grobner basis algorithm. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: IG-1: Transition Axiom formulation for interreduction with one copy of G. InterReduce might reduce a basis element to zero; we assume for simplicity that zero elements are left around in G but are never considered as reducers. 3.3 Interreduction We have parallelized and distributed S without interreduction <ref> [7] </ref>. The proof of that algorithm is a special case of the algorithm with interreduction: interreduc-tion introduces mutation of polynomials in the basis. We present only the more general case here, and therefore proceed by introducing interreduction into our nondeterministic algorithm.
Reference: 8. <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an Irregular Application on a Distributed Memory Multiprocessor. </title> <booktitle> In Principles and Practices of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Thus the algorithm has to ensure correctness in the presence of multiple, inconsistent copies of the basis. * Finally, the design extends the transition-based approach [21] to distributed memory machines. We have reported on engineering issues and more extensively on performance elsewhere <ref> [8] </ref>. The algorithm has been implemented on a CM-5 multiprocessor. It outperforms previous parallel algorithms on shared memory machines. <p> In the Grobner basis computation, the most important data structure in question is the basis, since it is shared most extensively. The basis could be distributed by partitioning or replication, but a pragmatic analysis of load balance, granularity and communication requirements <ref> [8] </ref> favor replication. Given a repli-cated basis, we have to address the problem of maintaining consistency without introducing excessive overhead. Fortunately, the consistency requirement on the basis is rather lax: a processor can do significant amounts of useful work while having an incomplete or even inconsistent copy of the basis. <p> Even though the algorithm has good time scalability for long running problems, the indiscriminate replication makes it scale poorly in space. We came across a few examples that are extremely long-running, but replication exhausts memory. It is clear from our analysis <ref> [8] </ref> that time scalability is favored by replication. Solving large real problems seems to need a compromise with partitioning.
Reference: 9. <author> K. M. Chandy and J. Misra. </author> <title> Parallel Program Design : a Foundation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Mass., </address> <year> 1988. </year>
Reference-contexts: Although our implementation includes such improvements, we omit them from the proofs for simplicity. 3.2 Transition Axiom Specification Transition axioms are a means to exploit non-determinism in a sequential algorithm description. Inspired by guarded command languages <ref> [9, 11] </ref>, and augmented by linearizable data types [21], this style was used to implement a shared-memory Knuth-Bendix procedure [22]. Transition axioms help break the computation into independently schedulable chunks, so the scheduling decisions are deferred until late in the design process.
Reference: 10. <author> N. Dershowitz and Z. Manna. </author> <title> Proving Termination with Multiset Orderings. </title> <journal> Communications of the ACM, </journal> <volume> 22 </volume> <pages> 465-476, </pages> <year> 1979. </year>
Reference-contexts: ideal M over K and integer p 0, ordered lexicographically as hM 1 ; p 1 i = hM 2 ; p 2 i iff Each loop iteration of S reduces the tuple D Ideal (Hmono (G)); jgpqj E be verified easily by examining each axiom. (See Dershowitz and Manna <ref> [10] </ref> for similar termination proving techniques.) Algebraic optimizations to the basic algorithm have been developed that test s-polynomials to quickly detect reduction to zero, without actually performing the reduction [5].
Reference: 11. <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1976. </year>
Reference-contexts: Although our implementation includes such improvements, we omit them from the proofs for simplicity. 3.2 Transition Axiom Specification Transition axioms are a means to exploit non-determinism in a sequential algorithm description. Inspired by guarded command languages <ref> [9, 11] </ref>, and augmented by linearizable data types [21], this style was used to implement a shared-memory Knuth-Bendix procedure [22]. Transition axioms help break the computation into independently schedulable chunks, so the scheduling decisions are deferred until late in the design process.
Reference: 12. <author> A. Giovini, T. Mora, G. Niesi, L. Robbiano, and C. Traverso. </author> <title> "One sugar cube, please" OR Selection strategies in the Buchberger algorithm. </title> <booktitle> In Proceedings of the 1991 International Symposium on Symbolic and Algebraic Computation, </booktitle> <pages> pages 49-54, </pages> <address> Bonn, Germany, </address> <month> 15-17 July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Buchberger introduced the notion of a Grobner basis of a set of polynomials and presented an algorithm for computing it [4]. We present an algorithm based on his for computing Grobner bases on a MIMD distributed memory multiprocessor. Although somewhat controversial <ref> [12] </ref>, Buchberger and others believe that interreduction (keeping the basis reduced with respect to itself) is essential to performance. Our algorithm executes interreduction steps concurrently with the standard critical pair and reduction steps.
Reference: 13. <author> D. J. Hawley. </author> <title> A Buchberger algorithm for Distributed Memory Multi-processors. </title> <booktitle> In Proceedings of the 1st International ACPC Conference on Parallel Computation, </booktitle> <pages> pages 385-390, </pages> <address> Salzburg, Austria, </address> <month> 30 September - 2 October </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: A survey of the theory can be found in Mishra [14]. Parallel implementations have been surveyed by Vidal [19]. Earlier network implementations have been reported by Siegl [17], Attardi et al [1] and Hawley <ref> [13] </ref>. 3 Algorithm Design In this section we develop the parallel algorithm, starting from the sequential algorithm.
Reference: 14. <author> B. Mishra and C. Yap. </author> <title> Notes on Grobner basis. </title> <booktitle> In Information Sciences 48, </booktitle> <pages> pages 219-252. </pages> <publisher> Elsevier Science Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: We finish with some concluding remarks in x6. 2 Notation In this section we briefly introduce some notation. A more detailed treatment can be found in <ref> [14] </ref>. Let K be a field and x 1 ; :::; x n be variables, arbitrarily ordered as x 1 &gt; x 2 &gt; &gt; x n . Then K = K [x 1 ; :::; x n ] defines a ring of polynomials under standard polynomial arithmetic. <p> A survey of the theory can be found in Mishra <ref> [14] </ref>. Parallel implementations have been surveyed by Vidal [19]. Earlier network implementations have been reported by Siegl [17], Attardi et al [1] and Hawley [13]. 3 Algorithm Design In this section we develop the parallel algorithm, starting from the sequential algorithm. <p> The effect is that polynomials that have entered the basis once are never modified or deleted. We enhance the algorithm with interreduction after we refine S to a transition axiom form. A correctness proof of S is given by Mishra and Yap <ref> [14] </ref>; we sketch their proof of partial correctness and give a different proof of termination. Our proof generalizes better to the interreducing algorithm to be described later. Theorem 1 (Buchberger). G is a Grobner basis iff for all f; g 2 G, 0 2 NF G (Spol (f; g)) ([14], Theorem <p> Proof. For partial correctness, observe the loop invariant 8 p; q 2 G; fp; qg 62 gpq ) 0 2 NF G (Spol (p; q)): (4) If S terminates, gpq = ;, so G is Grobner by theorem 1. For termination, note that Reduce is a terminating computation <ref> [14] </ref> and consider tuples of the form hM; pi, built of an ideal M over K and integer p 0, ordered lexicographically as hM 1 ; p 1 i = hM 2 ; p 2 i iff Each loop iteration of S reduces the tuple D Ideal (Hmono (G)); jgpqj E
Reference: 15. <author> Nathan Jacobson. </author> <title> Basic Algebra | Volume 2. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: 16. <author> C. G. Ponder. </author> <title> Evaluation of "performance enhancements" in algebraic manipulation systems. </title> <type> Technical Report UCB/CSD 88/438, </type> <institution> University of California, Berkeley, </institution> <year> 1988. </year> <title> Chapter 7, Parallel Algorithms for Grobner Basis Reduction. </title>
Reference-contexts: Fortunately, the consistency requirement on the basis is rather lax: a processor can do significant amounts of useful work while having an incomplete or even inconsistent copy of the basis. Allowing Inconsistent Copies An example of a consistency problem that may occur is the following "race condition" <ref> [16] </ref>. Suppose processors P 1 and P 2 both have copies of polynomials g; h, which happen to be equal. InterReduce fires on P 1 and P 2 . Say g is reduced by h to 0 on P 1 .
Reference: 17. <author> K. Siegl. </author> <title> Parallel Grobner basis computation in jjMAPLEjj. </title> <type> Technical Report 92-11, </type> <institution> Research Institute for Symbolic Computation, Linz, Austria, </institution> <year> 1992. </year>
Reference-contexts: A survey of the theory can be found in Mishra [14]. Parallel implementations have been surveyed by Vidal [19]. Earlier network implementations have been reported by Siegl <ref> [17] </ref>, Attardi et al [1] and Hawley [13]. 3 Algorithm Design In this section we develop the parallel algorithm, starting from the sequential algorithm.
Reference: 18. <author> J. K. Slaney and E. W. Lusk. </author> <title> Parallelizing the Closure Computation in Automated Deduction. </title> <booktitle> In Proceedings of the 10th International Conference on Automated Deduction, </booktitle> <pages> pages 28-29. </pages> <publisher> Springer-Verlag, LNCS 449, </publisher> <year> 1990. </year>
Reference-contexts: A precursor to this work was a shared memory implementation of the Knuth-Bendix procedure, which was also based on a transition axiom style [22]. See also <ref> [18] </ref> for a generic parallel completion procedure and [3] for some related correctness results for distributed computations. The pragmatic question of whether other completion procedures would perform well on distributed memory machines is beyond the scope of this paper.
Reference: 19. <author> J.-P. Vidal. </author> <title> The computation of Grobner bases on a shared memory multiprocessor. </title> <type> Technical Report CMU-CS-90-163, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1990. </year>
Reference-contexts: A survey of the theory can be found in Mishra [14]. Parallel implementations have been surveyed by Vidal <ref> [19] </ref>. Earlier network implementations have been reported by Siegl [17], Attardi et al [1] and Hawley [13]. 3 Algorithm Design In this section we develop the parallel algorithm, starting from the sequential algorithm. <p> This also produces a Grobner basis, but not necessarily a unique one. In Figure 4 some of the speedups on the CM-5 multiprocessor are given. The first set (a) is done on a small number of processors using some standards benchmarks <ref> [19] </ref>. Scalability The standard benchmarks complete in a few seconds. Scalability is better than some previous shared memory implementations, but is still limited by the small total number of tasks (pairs added to the pair queue). <p> A general parallelization recipe for a variety of architectures will be useful. In particular, vectorizing the infinite precision coefficient computations should improve absolute performance. In conclusion, we have presented a distributed memory MIMD algorithm for computing Grobner basis. Our implementation out-performs the shared memory implementation of Vidal <ref> [19] </ref> fairly consistently, and has the additional advantage that shared memory hardware is not assumed. The transition-based approach, previously used for shared memory [21], is extended here for distributed memory.
Reference: 20. <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: Each processor is a 33 MHz (15-20 MIPS) Sparc with about 8 MB of memory. The network is a fat-tree supporting at most 20 MB/s point-to-point data transfer. The prototype is in C, with the active message layer <ref> [20] </ref> for communication. For the benchmarks we used, there was no remarkable difference in performance with and without interreduction. The results are quoted without interreduction. Also, we reduced only head terms in Reduce, not all terms. This also produces a Grobner basis, but not necessarily a unique one.
Reference: 21. <author> K. Yelick. </author> <title> Using abstraction in explicitly parallel programs. </title> <type> Technical Report MIT/LCS/TR-507, </type> <institution> Massachusetts Institute of Technology, 545 Technology Square, </institution> <address> Cambridge, MA 02139, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Without in-terreduction, the basis grows monotonically, but with interreduction, elements may be modified and deleted. Thus the algorithm has to ensure correctness in the presence of multiple, inconsistent copies of the basis. * Finally, the design extends the transition-based approach <ref> [21] </ref> to distributed memory machines. We have reported on engineering issues and more extensively on performance elsewhere [8]. The algorithm has been implemented on a CM-5 multiprocessor. It outperforms previous parallel algorithms on shared memory machines. <p> Although our implementation includes such improvements, we omit them from the proofs for simplicity. 3.2 Transition Axiom Specification Transition axioms are a means to exploit non-determinism in a sequential algorithm description. Inspired by guarded command languages [9, 11], and augmented by linearizable data types <ref> [21] </ref>, this style was used to implement a shared-memory Knuth-Bendix procedure [22]. Transition axioms help break the computation into independently schedulable chunks, so the scheduling decisions are deferred until late in the design process. <p> In conclusion, we have presented a distributed memory MIMD algorithm for computing Grobner basis. Our implementation out-performs the shared memory implementation of Vidal [19] fairly consistently, and has the additional advantage that shared memory hardware is not assumed. The transition-based approach, previously used for shared memory <ref> [21] </ref>, is extended here for distributed memory. The key idea is to replace the shared data structures with distributed data structures, for which replication and partitioning of the data is hidden.
Reference: 22. <author> K. A. Yelick and S. J. </author> <title> Garland. A parallel completion procedure for term rewriting systems. </title> <booktitle> In Conference on Automated Deduction, </booktitle> <address> Saratoga Springs, NY, </address> <year> 1992. </year>
Reference-contexts: Inspired by guarded command languages [9, 11], and augmented by linearizable data types [21], this style was used to implement a shared-memory Knuth-Bendix procedure <ref> [22] </ref>. Transition axioms help break the computation into independently schedulable chunks, so the scheduling decisions are deferred until late in the design process. They are written in the form C ) A where C is the enabling condition (a guard predicate) and A is the action. <p> A precursor to this work was a shared memory implementation of the Knuth-Bendix procedure, which was also based on a transition axiom style <ref> [22] </ref>. See also [18] for a generic parallel completion procedure and [3] for some related correctness results for distributed computations. The pragmatic question of whether other completion procedures would perform well on distributed memory machines is beyond the scope of this paper.
References-found: 22

