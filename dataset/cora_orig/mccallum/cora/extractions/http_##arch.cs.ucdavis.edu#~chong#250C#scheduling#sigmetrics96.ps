URL: http://arch.cs.ucdavis.edu/~chong/250C/scheduling/sigmetrics96.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/scheduling/
Root-URL: http://www.cs.ucdavis.edu
Email: bhlim@watson.ibm.com  ricardo@cos.ufrj.br  
Title: Limits on the Performance Benefits of Multithreading and Prefetching  
Author: Beng-Hong Lim Ricardo Bianchini 
Address: Yorktown Heights, NY 10598  RJ 21945-970 Brazil  
Affiliation: IBM T.J. Watson Research Center  COPPE Systems Engineering/UFRJ Rio de Janeiro,  
Abstract: This paper presents new analytical models of the performance benefits of multithreading and prefetching, and experimental measurements of parallel applications on the MIT Alewife multiprocessor. For the first time, both techniques are evaluated on a real machine as opposed to simulations. The models determine the region in the parameter space where the techniques are most effective, while the measurements determine the region where the applications lie. We find that these regions do not always overlap significantly. The multithreading model shows that only 2-4 contexts are necessary to maximize this technique's potential benefit in current multiprocessors. Multithreading improves execution time by less than 10% for most of the applications that we examined. The model also shows that multithreading can significantly improve the performance of the same applications in multiprocessors with longer latencies. Reducing context-switch overhead is not crucial. The software prefetching model shows that allowing 4 outstanding prefetches is sufficient to achieve most of this technique's potential benefit on current multiprocessors. Prefetching improves performance over a wide range of parameters, and improves execution time by as much as 20-50% even on current multiprocessors. The two models show that prefetching has a significant advantage over multithreading for machines with low memory latencies and/or applications with high cache miss rates because a prefetch instruction consumes less time than a context-switch. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Performance Tradeoffs in Multithreaded Processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 525-539, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: It introduces the notion of gain to characterize the benefit, where gain is defined as the ratio of program execution times without and with the technique. The multithreading model extends Agarwal's processor utilization model <ref> [1] </ref> to predict the gain due to multithreading. The prefetching model is new, and predicts both utilization and gain due to prefetching. <p> Tera does not have caches, and requires a large number of concurrent threads and high network bandwidth to achieve high processor utilization. Previous experimental research on multithreading performance shows that multithreading is effective at tolerating memory latencies for some applications [13, 26]. Previous analytical research <ref> [1, 23, 21, 11] </ref> focuses on modeling processor utilization and predicting the number of contexts needed for good processor utilization. In contrast, this paper combines analytical models and experimental measurements in a novel way. <p> In the second case, not enough contexts exist so that the processor suffers some idle time. Thus, the utilization of a multithreaded processor is U (p) = &lt; t (p) pt (p) t (p)+T (p) otherwise (1) This is the same model for processor utilization presented in <ref> [1] </ref>. We carry the analysis a step further and derive an upper bound on the gain achievable through multithreading, G (p). The processor utilization for a non-multithreaded processor is U (1) = t (1) + T (1) As observed in [1], multithreading may shorten run-lengths and increase remote memory latencies. <p> This is the same model for processor utilization presented in <ref> [1] </ref>. We carry the analysis a step further and derive an upper bound on the gain achievable through multithreading, G (p). The processor utilization for a non-multithreaded processor is U (1) = t (1) + T (1) As observed in [1], multithreading may shorten run-lengths and increase remote memory latencies. That is, t (p) t (1) due to cache interference and finer computation grain sizes, and T (p) T (1) due to higher network and memory contention.
Reference: [2] <author> A. Agarwal, R. Bianchini, D. Chaiken, K.L. John-son, D. Kranz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture. ACM, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: They determine the range of architectural and application parameters, such as run-length, remote cache miss latency and context-switch overhead, where multi-threading and prefetching are most likely to improve performance. This paper applies the models towards evaluating multithread-ing and prefetching on the MIT Alewife Machine <ref> [2] </ref>, a distributed shared-memory multiprocessor that implements both techniques. For the first time, both techniques are evaluated on an actual machine as opposed to simulations. We measure the run-lengths of a number of applications and use the models to predict the potential benefits of multithreading and prefetching for these applications. <p> Multithreaded processors have been proposed or used in several machines, such as the HEP [25], Monsoon [22], the Tera MTA [4], McGill MTA [14], and MIT Alewife <ref> [2] </ref>. A description of the history and status of multi-threaded architectures and research appears in [7] and [15]. Early multithreaded architectures take an aggressive fine-grain approach that switches contexts at each instruction. This requires a large number of contexts and very low overhead context switches. <p> of t such that prefetching results in both a significant gain and a reasonable level of processor utilization in order to justify hardware support for prefetching. 4 Evaluating Multithreading and Prefetching on Alewife This section applies our models towards an evaluation of multi-threading and prefetching in the MIT Alewife Machine <ref> [2] </ref>. Alewife provides support for both multithreading and prefetching, and presents an ideal platform for evaluating both techniques. After describing the Alewife architecture, we use Alewife's latency, context-switch, and prefetch overhead parameters to determine the sweet spots for each of the techniques.
Reference: [3] <author> A. Agarwal, J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. D'Souza, and M. Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The main drawback of this approach is poor single-thread performance. To address this limitation, Alewife introduced the idea of block multithreading, where caches reduce the number of remote cache misses and context-switches occur only on such misses. This allows a less aggressive implementation of multithreading <ref> [3] </ref>. Some recent architectures [16, 18] promise both single-cycle context-switches and high single-thread performance by allowing instructions from multiple threads to interleave arbitrarily in the processor pipeline. The Tera MTA [4] takes an extreme approach by providing 128 contexts on each processor. <p> Last, to corroborate the models, we run a subset of the applications with multithreading and prefetching enabled, and measure the resulting gains. 4.1 The MIT Alewife Machine of Alewife. Each node consists of a Sparcle processor <ref> [3] </ref> and an associated floating point unit with up to 4 processor contexts, 64K bytes of direct-mapped cache, 8M bytes of DRAM, an Elko-series mesh routing chip (EMRC) from Caltech, and a custom-designed Communication and Memory Management Unit (CMMU).
Reference: [4] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Port-erfield, and B. Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <address> Amsterdam, </address> <month> June </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Finally, Section 6 summarizes the conclusions drawn from this research. 2 Background Multithreading and prefetching have been the focus of work on tolerating communication latency for many years. Multithreaded processors have been proposed or used in several machines, such as the HEP [25], Monsoon [22], the Tera MTA <ref> [4] </ref>, McGill MTA [14], and MIT Alewife [2]. A description of the history and status of multi-threaded architectures and research appears in [7] and [15]. Early multithreaded architectures take an aggressive fine-grain approach that switches contexts at each instruction. <p> This allows a less aggressive implementation of multithreading [3]. Some recent architectures [16, 18] promise both single-cycle context-switches and high single-thread performance by allowing instructions from multiple threads to interleave arbitrarily in the processor pipeline. The Tera MTA <ref> [4] </ref> takes an extreme approach by providing 128 contexts on each processor. At each cycle, the processor executes an instruction from one of the contexts. Tera does not have caches, and requires a large number of concurrent threads and high network bandwidth to achieve high processor utilization.
Reference: [5] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: to determine if they fall within the sweet spot. 4.3 Application Performance on Alewife In order to determine the range of run-lengths found in real applications on a real machine, we use a 32-processor Alewife multiprocessor to study a large number of parallel applications, including the SPLASH [24] and NAS <ref> [5] </ref> parallel benchmarks and a number of engineering kernels. Table 3 lists the applications and their input parameters. Please refer to the SPLASH and NAS documents for descriptions of those benchmarks.
Reference: [6] <author> R. Bianchini and T.J. LeBlanc. </author> <title> A Preliminary Evaluation of Cache-Miss-Initiated Prefetching Techniques in Scalable Multiprocessors. </title> <type> Technical Report TR 515, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It defines a range of run-lengths where multithreading is profitable, and compares this range to the run-lengths in real applications. Prefetching has been considered in many flavors, with hardware [9, 10], software [20, 8], and hybrid <ref> [6] </ref> approaches. Software pre-fetching, where a compiler inserts prefetch instructions for blocks ahead of actual use, is widely accepted as one of the most efficient prefetching techniques. It requires little hardware support and usually incurs the overhead of a single instruction per cache block prefetched. <p> It requires little hardware support and usually incurs the overhead of a single instruction per cache block prefetched. Several machines provide software prefetching, including Stanford DASH [19], the KSR1 [12], and MIT Alewife. As far as we know, performance studies of software prefetch-ing have all been experimental <ref> [20, 8, 6] </ref>. These studies indicate sizable performance improvements and consistently better performance than other prefetching strategies.
Reference: [7] <author> G. Byrd and M. Holliday. </author> <title> Multithreaded Processor Architectures. </title> <journal> IEEE Spectrum, </journal> <pages> pages 38-46, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Multithreaded processors have been proposed or used in several machines, such as the HEP [25], Monsoon [22], the Tera MTA [4], McGill MTA [14], and MIT Alewife [2]. A description of the history and status of multi-threaded architectures and research appears in <ref> [7] </ref> and [15]. Early multithreaded architectures take an aggressive fine-grain approach that switches contexts at each instruction. This requires a large number of contexts and very low overhead context switches. The main drawback of this approach is poor single-thread performance.
Reference: [8] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <booktitle> Software Pre-fetching. In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In contrast, this paper combines analytical models and experimental measurements in a novel way. It defines a range of run-lengths where multithreading is profitable, and compares this range to the run-lengths in real applications. Prefetching has been considered in many flavors, with hardware [9, 10], software <ref> [20, 8] </ref>, and hybrid [6] approaches. Software pre-fetching, where a compiler inserts prefetch instructions for blocks ahead of actual use, is widely accepted as one of the most efficient prefetching techniques. It requires little hardware support and usually incurs the overhead of a single instruction per cache block prefetched. <p> It requires little hardware support and usually incurs the overhead of a single instruction per cache block prefetched. Several machines provide software prefetching, including Stanford DASH [19], the KSR1 [12], and MIT Alewife. As far as we know, performance studies of software prefetch-ing have all been experimental <ref> [20, 8, 6] </ref>. These studies indicate sizable performance improvements and consistently better performance than other prefetching strategies.
Reference: [9] <author> T.-F. Chen and J.-L. Baer. </author> <title> A Performance Study of Software and Hardware Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: In contrast, this paper combines analytical models and experimental measurements in a novel way. It defines a range of run-lengths where multithreading is profitable, and compares this range to the run-lengths in real applications. Prefetching has been considered in many flavors, with hardware <ref> [9, 10] </ref>, software [20, 8], and hybrid [6] approaches. Software pre-fetching, where a compiler inserts prefetch instructions for blocks ahead of actual use, is widely accepted as one of the most efficient prefetching techniques.
Reference: [10] <author> F. Dahlgren, M. Dubois, and P. Stenstrom. </author> <title> Sequential Hardware Prefetching in Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(7) </volume> <pages> 733-746, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: In contrast, this paper combines analytical models and experimental measurements in a novel way. It defines a range of run-lengths where multithreading is profitable, and compares this range to the run-lengths in real applications. Prefetching has been considered in many flavors, with hardware <ref> [9, 10] </ref>, software [20, 8], and hybrid [6] approaches. Software pre-fetching, where a compiler inserts prefetch instructions for blocks ahead of actual use, is widely accepted as one of the most efficient prefetching techniques.
Reference: [11] <author> P. Dubey, A. Krishna, and M. Squillante. </author> <title> Analytic Performance Modeling for a Spectrum of Multithreaded Processor Architectures. </title> <institution> Computer Science RC 19661, IBM, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Tera does not have caches, and requires a large number of concurrent threads and high network bandwidth to achieve high processor utilization. Previous experimental research on multithreading performance shows that multithreading is effective at tolerating memory latencies for some applications [13, 26]. Previous analytical research <ref> [1, 23, 21, 11] </ref> focuses on modeling processor utilization and predicting the number of contexts needed for good processor utilization. In contrast, this paper combines analytical models and experimental measurements in a novel way.
Reference: [12] <author> S. Frank, H. Burkhardt III, and J. Rothnie. </author> <title> The KSR1: Bridging the Gap Between Shared Memory and MPPs. </title> <booktitle> In Proceedings of the 38th Annual IEEE Computer Society Computer Conference (COMPCON), </booktitle> <pages> pages 284-294, </pages> <address> San Fran-cisco, CA, 1993. </address> <publisher> IEEE. </publisher>
Reference-contexts: It requires little hardware support and usually incurs the overhead of a single instruction per cache block prefetched. Several machines provide software prefetching, including Stanford DASH [19], the KSR1 <ref> [12] </ref>, and MIT Alewife. As far as we know, performance studies of software prefetch-ing have all been experimental [20, 8, 6]. These studies indicate sizable performance improvements and consistently better performance than other prefetching strategies.
Reference: [13] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.- D. Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: Tera does not have caches, and requires a large number of concurrent threads and high network bandwidth to achieve high processor utilization. Previous experimental research on multithreading performance shows that multithreading is effective at tolerating memory latencies for some applications <ref> [13, 26] </ref>. Previous analytical research [1, 23, 21, 11] focuses on modeling processor utilization and predicting the number of contexts needed for good processor utilization. In contrast, this paper combines analytical models and experimental measurements in a novel way. <p> We significantly extend these contributions by modeling software prefetching both in terms of processor utilization and gain, defining a range of run-lengths where prefetching is profitable, and comparing this range to experimentally observed run-lengths. Empirical comparisons of multithreading and prefetching are extremely uncommon. Gupta et al. <ref> [13] </ref> find that prefetching leads to better performance when applied either in isolation or in combination with relaxed consistency. They show that multithreading with an aggressive 4-cycle context-switch overhead, combined with relaxed consistency achieves good performance.
Reference: [14] <author> H. Hum et al. </author> <title> The Multi-Threaded Architecture Multiprocessor. </title> <type> Technical Report ACAPS Technical Memo 88, </type> <institution> McGill University School of Computer Science, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Multithreaded processors have been proposed or used in several machines, such as the HEP [25], Monsoon [22], the Tera MTA [4], McGill MTA <ref> [14] </ref>, and MIT Alewife [2]. A description of the history and status of multi-threaded architectures and research appears in [7] and [15]. Early multithreaded architectures take an aggressive fine-grain approach that switches contexts at each instruction. This requires a large number of contexts and very low overhead context switches.
Reference: [15] <editor> R.A. Iannucci, editor. </editor> <booktitle> Multithreaded Computer Architecture </booktitle> - 
Reference-contexts: Multithreaded processors have been proposed or used in several machines, such as the HEP [25], Monsoon [22], the Tera MTA [4], McGill MTA [14], and MIT Alewife [2]. A description of the history and status of multi-threaded architectures and research appears in [7] and <ref> [15] </ref>. Early multithreaded architectures take an aggressive fine-grain approach that switches contexts at each instruction. This requires a large number of contexts and very low overhead context switches. The main drawback of this approach is poor single-thread performance.
References-found: 15

