URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94370.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Phone: 2  
Title: Maximizing Loop Parallelism and Improving Data Locality via Loop Fusion and Distribution  
Author: Ken Kennedy and Kathryn S. M c Kinley 
Address: Houston TX 77251-1892  Amherst MA 01003-4610  
Affiliation: 1 Rice University,  University of Massachusetts,  
Abstract: Loop fusion is a program transformation that merges multiple loops into one. It is effective for reducing the synchronization overhead of parallel loops and for improving data locality. This paper presents three results for fusion: (1) a new algorithm for fusing a collection of parallel and sequential loops, minimizing parallel loop synchronization while maximizing parallelism; (2) a proof that performing fusion to maximize data locality is NP-hard; and (3) two polynomial-time algorithms for improving data locality. These techniques also apply to loop distribution, which is shown to be essentially equivalent to loop fusion. Our approach is general enough to support other fusion heuristics. Preliminary experimental results validate our approach for improving performance by exploiting data locality and increasing the granularity of parallelism.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: Since the direction of the dependence is preserved in the first two cases, fusion is legal. Fusion is illegal when a loop-independent dependence becomes a backward carried dependence after fusion. These dependences are called fusion-preventing dependences <ref> [1, 25] </ref>. Since a loop is parallel if it contains no loop-carried dependences and is sequential otherwise, fusion in case (b) is safe but prevents parallelization of the resultant loop. If either one or both of the loops were parallel, fusion would reduce loop parallelism. <p> Abu-Sufah and Warren have addressed in depth the safety, benefits and simple application of loop fusion <ref> [1, 25] </ref>. Neither presents a general algorithm for loop fusion that supports loop reordering or any differentiation between fusion choices.
Reference: 2. <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Fusion and distribution also enable loop interchange, making them an important component for many optimization strategies. 10 Related Work In the literature, fusion has been recommended for decreasing loop overhead, improving data locality and increasing the granularity of parallelism <ref> [2, 23] </ref>. Abu-Sufah and Warren have addressed in depth the safety, benefits and simple application of loop fusion [1, 25]. Neither presents a general algorithm for loop fusion that supports loop reordering or any differentiation between fusion choices.
Reference: 3. <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: The ordering constraint is required for correctness. As discussed in Section 3, some fusion-preventing edges are necessary for correctness and others are needed to achieve an optimal solution. Allen, Callahan and Kennedy refer to maximal parallelism with minimum barrier synchronization <ref> [3, 6] </ref>. By omitting the separation constraint, they arrive at an optimal greedy algorithm. Their work also tries to partition the graph into minimal sets, but uses a model of parallelism with both loop-level and fork-join task parallelism. <p> The iterations of the parallel loop P 1 also execute concurrently. Once they both complete, the parallel loop P 2 executes concurrently. Because it ignores the node type, the greedy algorithm is provably optimal, minimizing the total number of partitions and maximizing task parallelism <ref> [3, 6] </ref>. The greedy algorithm however fails for the partition problem defined above, i.e. restricting it to loop-level parallelism, no task parallelism. <p> Allen, Callahan, and Kennedy consider a broad fusion problem that introduces both task and loop parallelism, but does not address improving data locality or granularity of loop parallelism <ref> [3, 6] </ref>. Our algorithm for maximizing loop parallelism and its granularity is a new result. Sarkar and Gao present an algorithm to perform loop fusion and array contraction to improve data locality on uniprocessors for single assignment languages [24].
Reference: 4. <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Our experimental results, a careful comparison to related work, and conclusions complete the paper. 2 Technical Background 2.1 Dependence We assume the reader is familiar with data dependence [5] and the terms true, anti, output and input dependence, as well as the distinction between loop-independent and loop-carried dependences <ref> [4] </ref>. Parallel loops have no loop-carried dependences and sequential loops have at least one. Intuitively, a control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines whether S 2 will be executed [10, 12].
Reference: 5. <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15(5) </volume> <pages> 757-763, </pages> <month> Oct. </month> <year> 1966. </year>
Reference-contexts: Our experimental results, a careful comparison to related work, and conclusions complete the paper. 2 Technical Background 2.1 Dependence We assume the reader is familiar with data dependence <ref> [5] </ref> and the terms true, anti, output and input dependence, as well as the distinction between loop-independent and loop-carried dependences [4]. Parallel loops have no loop-carried dependences and sequential loops have at least one.
Reference: 6. <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Mar. </month> <year> 1987. </year>
Reference-contexts: The ordering constraint is required for correctness. As discussed in Section 3, some fusion-preventing edges are necessary for correctness and others are needed to achieve an optimal solution. Allen, Callahan and Kennedy refer to maximal parallelism with minimum barrier synchronization <ref> [3, 6] </ref>. By omitting the separation constraint, they arrive at an optimal greedy algorithm. Their work also tries to partition the graph into minimal sets, but uses a model of parallelism with both loop-level and fork-join task parallelism. <p> The iterations of the parallel loop P 1 also execute concurrently. Once they both complete, the parallel loop P 2 executes concurrently. Because it ignores the node type, the greedy algorithm is provably optimal, minimizing the total number of partitions and maximizing task parallelism <ref> [3, 6] </ref>. The greedy algorithm however fails for the partition problem defined above, i.e. restricting it to loop-level parallelism, no task parallelism. <p> Partitioning a Component Graph. Given G c is at most the transitive closure of a dag, it must also be a dag and can be partitioned optimally using the greedy algorithm into sets of loops that can be fused (see Callahan's dissertation for the proof <ref> [6] </ref>). The greedy algorithm places the roots of all the connected components into a partition and then recursively tries to add the successors in breadth-first order. <p> With the above construction, parallel nodes and sequential nodes are never placed in the same partition, satisfying the separation constraint. Because G p is a dag, minimizing the number of parallel partitions is achieved using the greedy algorithm (the proof of minimality from Callahan's dissertation is directly applicable <ref> [6] </ref>.) We now show that the construction of G p does not introduce any constraints which would cause the component solution G p 0 to be non-optimal. Proof. All the edges which are not fusion-preventing in G p are in G o by construction. <p> Allen, Callahan, and Kennedy consider a broad fusion problem that introduces both task and loop parallelism, but does not address improving data locality or granularity of loop parallelism <ref> [3, 6] </ref>. Our algorithm for maximizing loop parallelism and its granularity is a new result. Sarkar and Gao present an algorithm to perform loop fusion and array contraction to improve data locality on uniprocessors for single assignment languages [24].
Reference: 7. <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: For example, reuse provided by fusion can be made explicit by using scalar replacement to place array references in a register <ref> [7] </ref>. However, fusion may also increase working set size, negatively impacting cache and register performance.
Reference: 8. <author> S. Carr, K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <type> Technical Report TR92-195, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Placing the I loop innermost for statement S 2 would instead provide stride-one access on the columns. Using loop distribution enables the interchange and the combination significantly improves the execution time. The algorithm that evaluates and applies loop permutation with distribution appears elsewhere <ref> [8] </ref>. <p> For parallel code generation for shared-memory multiprocessors, this work extends our previous work by providing comprehensive fusion and distribution algorithms [19, 22]. In Carr, Kennedy, McKinley and Tseng, they combine loop permutation with fusion and distribution to improve data locality on uniproces--sors <ref> [8] </ref>. The algorithms presented here are complementary to this work. 11 Summary This paper presents an optimal algorithm for performing loop fusion and its dual, loop distribution, to maximize the granularity of loop parallelism, therefore minimizing sychronization.
Reference: 9. <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Supercomputer performance eval-uation and the Perfect benchmarks. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Loop extraction pulls a loop out of the called routine and into the caller, actually increasing procedure call overhead. The increased reuse and decreased parallel loop synchronization resulting from fusion more than overcame the additional call overhead. Ocean is 3664 non-comment line program from the Perfect benchmark suite <ref> [9] </ref>. Fusion improved parallel performance by 32 % on Ocean. Thirty-one nests benefit from fusion across procedure boundaries [17, 22]. Some of the candidates were exposed after constant propagation and dead code elimination.
Reference: 10. <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experiences using control dependence in PTRAN. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Parallel loops have no loop-carried dependences and sequential loops have at least one. Intuitively, a control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines whether S 2 will be executed <ref> [10, 12] </ref>. In addition to using control dependence to describe program dependence, we will use the control dependence and postdominance relations for an arbitrary graph G. These definitions are taken from the literature [12]. <p> Differentiating between multicuts of degree 1, l and l + 1 makes these tradeoffs very difficult. Complexity. This algorithm applies the O (nmlog (n 2 =m)) maximum-flow algorithm k times splitting the graph at each application. For these types of graphs, computing control dependence is linear <ref> [10] </ref>. The complexity of our algorithm is therefore O (knmlog (n 2 =m)) time. 9 Results As our experimental vehicle we used the ParaScope Editor [21], an interactive parallelization tool which provides source-to-source program transformations using dependence analysis.
Reference: 11. <author> E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou, P. D. Seymour, and M. Yannakakis. </author> <title> The complexity of multiway cuts. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: U 2 U 3 U 4 U 8 (a) original (b) collapsed (c) cut & un-collapsed Fig. 4. Using Minimum-Cut for Fusion (fi &gt; ff) Dahlhaus et al. develop an isolation heuristic <ref> [11] </ref> and combine it with Gold-berg and Tarjan's maximum flow algorithm [16] to design a polynomial algorithm that produces a multicut that is at most (2k - 1)/k times the optimal. <p> Goldberg and Tarjan's maximum flow algorithm is to date the most efficient with time complexity O (nmlog (n 2 =m)) [16]. The running time of Dahlhaus et al.'s algorithm is O (2 2k knmlog (n 2 =m)), which is polynomial for fixed k <ref> [11] </ref>. They leave open whether a more efficient algorithm with a similar optimality guarantee exists. In the Fortran programs in our studies, k, n, and m have always been very small. The following section is devoted to the design of an O (knmlog (n 2 =m)) algorithm for reuse problem.
Reference: 12. <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Parallel loops have no loop-carried dependences and sequential loops have at least one. Intuitively, a control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines whether S 2 will be executed <ref> [10, 12] </ref>. In addition to using control dependence to describe program dependence, we will use the control dependence and postdominance relations for an arbitrary graph G. These definitions are taken from the literature [12]. <p> In addition to using control dependence to describe program dependence, we will use the control dependence and postdominance relations for an arbitrary graph G. These definitions are taken from the literature <ref> [12] </ref>. Definition 1 y postdominates x in G if every path from x to the exit node contains y.
Reference: 13. <author> Ford, Jr., L. R. and D. R. Fulkerson. </author> <title> Flows in Networks. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1962. </year>
Reference-contexts: If there is only one fusion preventing dependence (k = 1), the graph can be divided in two using the maximum-flow/minimum-cut algorithm <ref> [13, 16] </ref>. Maximum-flow/minimum-cut is polynomial time and makes a single minimum cut that maximizes flow in the two resultant graphs. The maximum-flow algorithm works by introducing flow at the source such that no capacity is exceeded and the capacity of the flow network to the sink is maximized.
Reference: 14. <author> G. Gao, R. Olsen, V. Sarkar, and R. Thekkath. </author> <title> Collective loop fusion for array contraction. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Sarkar and Gao present an algorithm to perform loop fusion and array contraction to improve data locality on uniprocessors for single assignment languages [24]. This work is limited because it does not account for constraints imposed by data dependence <ref> [14, 24] </ref>. Their more recent work on loop fusion for uniprocessors and pipelining [14] takes into consideration data dependence constraints and also is based on the maximum-flow/minimum-cut algorithm. (Our work was developed independently.) Our algorithm is distinguished because of its tight worst-case bound. <p> This work is limited because it does not account for constraints imposed by data dependence [14, 24]. Their more recent work on loop fusion for uniprocessors and pipelining <ref> [14] </ref> takes into consideration data dependence constraints and also is based on the maximum-flow/minimum-cut algorithm. (Our work was developed independently.) Our algorithm is distinguished because of its tight worst-case bound. Both of our reuse algorithms reorder loop nests which is not possible in their formulation.
Reference: 15. <author> A. Goldberg and R. Paige. </author> <title> Stream processing. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 228-234, </pages> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Abu-Sufah and Warren have addressed in depth the safety, benefits and simple application of loop fusion [1, 25]. Neither presents a general algorithm for loop fusion that supports loop reordering or any differentiation between fusion choices. Goldberg and Paige <ref> [15] </ref> address a related fusion problem for stream processing, but their problem has constraints on fusions and ordering that are not present in the general fusion problem we address.
Reference: 16. <author> A. V. Goldberg and R. E. Tarjan. </author> <title> A new approach to the maximum-flow problem. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 921-940, </pages> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: If there is only one fusion preventing dependence (k = 1), the graph can be divided in two using the maximum-flow/minimum-cut algorithm <ref> [13, 16] </ref>. Maximum-flow/minimum-cut is polynomial time and makes a single minimum cut that maximizes flow in the two resultant graphs. The maximum-flow algorithm works by introducing flow at the source such that no capacity is exceeded and the capacity of the flow network to the sink is maximized. <p> U 2 U 3 U 4 U 8 (a) original (b) collapsed (c) cut & un-collapsed Fig. 4. Using Minimum-Cut for Fusion (fi &gt; ff) Dahlhaus et al. develop an isolation heuristic [11] and combine it with Gold-berg and Tarjan's maximum flow algorithm <ref> [16] </ref> to design a polynomial algorithm that produces a multicut that is at most (2k - 1)/k times the optimal. Goldberg and Tarjan's maximum flow algorithm is to date the most efficient with time complexity O (nmlog (n 2 =m)) [16]. <p> and combine it with Gold-berg and Tarjan's maximum flow algorithm <ref> [16] </ref> to design a polynomial algorithm that produces a multicut that is at most (2k - 1)/k times the optimal. Goldberg and Tarjan's maximum flow algorithm is to date the most efficient with time complexity O (nmlog (n 2 =m)) [16]. The running time of Dahlhaus et al.'s algorithm is O (2 2k knmlog (n 2 =m)), which is polynomial for fixed k [11]. They leave open whether a more efficient algorithm with a similar optimality guarantee exists.
Reference: 17. <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: The original nests were connected by data dependence (i.e., contained reuse) and accounted for a significant portion of the total execution time. These nests were fused across procedure boundaries using loop extraction to place the nests in the same procedure <ref> [17, 22] </ref>. Loop extraction pulls a loop out of the called routine and into the caller, actually increasing procedure call overhead. The increased reuse and decreased parallel loop synchronization resulting from fusion more than overcame the additional call overhead. <p> The increased reuse and decreased parallel loop synchronization resulting from fusion more than overcame the additional call overhead. Ocean is 3664 non-comment line program from the Perfect benchmark suite [9]. Fusion improved parallel performance by 32 % on Ocean. Thirty-one nests benefit from fusion across procedure boundaries <ref> [17, 22] </ref>. Some of the candidates were exposed after constant propagation and dead code elimination.
Reference: 18. <author> K. Kennedy and K. S. M c Kinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Loop distribution preserves dependences if all statements involved in a recurrence (i.e., dependence cycle) in the original loop are placed in the same loop <ref> [18] </ref>. Loops containing recurrences must be executed sequentially. Loop distribution first places each set of statements involved in a recurrence in a separate sequential node. Statements without recurrences are parallel and are placed in nodes by themselves. Dependences are edges between nodes and some may be fusion-preventing.
Reference: 19. <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Wash-ington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Our overall approach is more flexible because it optimizes for both multiprocessors and uniprocessors. For parallel code generation for shared-memory multiprocessors, this work extends our previous work by providing comprehensive fusion and distribution algorithms <ref> [19, 22] </ref>. In Carr, Kennedy, McKinley and Tseng, they combine loop permutation with fusion and distribution to improve data locality on uniproces--sors [8].
Reference: 20. <author> K. Kennedy and K. S. M c Kinley. </author> <title> Typed fusion with applications to parallel and sequential code generation. </title> <type> Technical Report TR93-208, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The separation of concerns lends itself to problems that need to sort or partition items of different types and priority while maintaining transitive relationships. This feature has been instrumental in designing a multilevel fusion algorithm <ref> [20] </ref>. The overall structure of the algorithm also enables different partitioning and sorting algorithms to be used on the component graphs.
Reference: 21. <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice & Experience, </journal> <note> to appear 1993. </note>
Reference-contexts: For these types of graphs, computing control dependence is linear [10]. The complexity of our algorithm is therefore O (knmlog (n 2 =m)) time. 9 Results As our experimental vehicle we used the ParaScope Editor <ref> [21] </ref>, an interactive parallelization tool which provides source-to-source program transformations using dependence analysis. We implemented the tests for correctness and the update of the source program and dependence information for fusion between two adjacent loops. We also implemented the correctness tests and updates for distribution to the finest granularity.
Reference: 22. <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: The original nests were connected by data dependence (i.e., contained reuse) and accounted for a significant portion of the total execution time. These nests were fused across procedure boundaries using loop extraction to place the nests in the same procedure <ref> [17, 22] </ref>. Loop extraction pulls a loop out of the called routine and into the caller, actually increasing procedure call overhead. The increased reuse and decreased parallel loop synchronization resulting from fusion more than overcame the additional call overhead. <p> The increased reuse and decreased parallel loop synchronization resulting from fusion more than overcame the additional call overhead. Ocean is 3664 non-comment line program from the Perfect benchmark suite [9]. Fusion improved parallel performance by 32 % on Ocean. Thirty-one nests benefit from fusion across procedure boundaries <ref> [17, 22] </ref>. Some of the candidates were exposed after constant propagation and dead code elimination. <p> Our overall approach is more flexible because it optimizes for both multiprocessors and uniprocessors. For parallel code generation for shared-memory multiprocessors, this work extends our previous work by providing comprehensive fusion and distribution algorithms <ref> [19, 22] </ref>. In Carr, Kennedy, McKinley and Tseng, they combine loop permutation with fusion and distribution to improve data locality on uniproces--sors [8].
Reference: 23. <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Fusion and distribution also enable loop interchange, making them an important component for many optimization strategies. 10 Related Work In the literature, fusion has been recommended for decreasing loop overhead, improving data locality and increasing the granularity of parallelism <ref> [2, 23] </ref>. Abu-Sufah and Warren have addressed in depth the safety, benefits and simple application of loop fusion [1, 25]. Neither presents a general algorithm for loop fusion that supports loop reordering or any differentiation between fusion choices.
Reference: 24. <author> V. Sarkar and G. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Our algorithm for maximizing loop parallelism and its granularity is a new result. Sarkar and Gao present an algorithm to perform loop fusion and array contraction to improve data locality on uniprocessors for single assignment languages <ref> [24] </ref>. This work is limited because it does not account for constraints imposed by data dependence [14, 24]. <p> Sarkar and Gao present an algorithm to perform loop fusion and array contraction to improve data locality on uniprocessors for single assignment languages [24]. This work is limited because it does not account for constraints imposed by data dependence <ref> [14, 24] </ref>. Their more recent work on loop fusion for uniprocessors and pipelining [14] takes into consideration data dependence constraints and also is based on the maximum-flow/minimum-cut algorithm. (Our work was developed independently.) Our algorithm is distinguished because of its tight worst-case bound.
Reference: 25. <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <month> Jan. </month> <year> 1984. </year>
Reference-contexts: Since the direction of the dependence is preserved in the first two cases, fusion is legal. Fusion is illegal when a loop-independent dependence becomes a backward carried dependence after fusion. These dependences are called fusion-preventing dependences <ref> [1, 25] </ref>. Since a loop is parallel if it contains no loop-carried dependences and is sequential otherwise, fusion in case (b) is safe but prevents parallelization of the resultant loop. If either one or both of the loops were parallel, fusion would reduce loop parallelism. <p> Abu-Sufah and Warren have addressed in depth the safety, benefits and simple application of loop fusion <ref> [1, 25] </ref>. Neither presents a general algorithm for loop fusion that supports loop reordering or any differentiation between fusion choices.
Reference: 26. <author> M. Yannakakis, P. C. Kanellakis, S. C. Cosmadakis, and C. H. Papadimitriou. </author> <title> Cutting and partitioning a graph after a fixed pattern. Automata, </title> <booktitle> Languages, and Programming Lecture Notes in Computer Science, </booktitle> <volume> 154 </volume> <pages> 712-722, </pages> <year> 1983. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The minimum cut consists of the edges that are filled to capacity and can be determined with a breadth-first search originating at the source of flow. If k = 2, the problem is polynomial time solvable by using two applications of the 2-way cut algorithm <ref> [26] </ref>. 5 Simple replaces the greedy algorithm on the component graphs in loop parallelization. . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. ... ...... ... .. . .. . . .
References-found: 26

