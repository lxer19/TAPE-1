URL: http://www.cse.ucsc.edu/~mark/papers/nips95-fin.ps
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: fpauer,mark,manfredg@cs.ucsc.edu  
Title: Exponentially many local minima for single neurons  
Author: Peter Auer Mark Herbster Manfred K. Warmuth 
Address: Santa Cruz, California  
Affiliation: Department of Computer Science  
Abstract: We show that for a single neuron with the logistic function as the transfer function the number of local minima of the error function based on the square loss can grow exponentially in the dimension.
Abstract-found: 1
Intro-found: 1
Reference: [AHW96] <author> P. Auer, M. Herbster, and M. K. Warmuth. </author> <title> Exponentially many local minima for single neurons. </title> <type> Technical Report UCSC-CRL-96-1, </type> <institution> Univ. of Calif.Computer Research Lab, </institution> <address> Santa Cruz, CA, </address> <year> 1996. </year> <note> In preparation. </note>
Reference-contexts: We can prove that the error function might have bn=2dc d local minima if loss and transfer function are symmetric. This holds for example for the square loss and the logistic transfer function. The proofs are omitted due to space constraints. They are given in the full paper <ref> [AHW96] </ref>, together with additional results for general loss and transfer functions. Finally we show in Section 5 that with minimal assumptions on transfer and loss functions that there is only one minimum of the error function if the sequence of examples is realizable by the neuron.
Reference: [Blu89] <author> E.K. Blum. </author> <title> Approximation of boolean functions by sigmoidal networks: Part i: Xor and other two-variable functions. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 532-540, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The same holds for any increasing differentiable transfer function and its matching loss function. A number of previous papers discussed conditions necessary and sufficient for multiple local minima of the error function of single neurons or otherwise small networks <ref> [WD88, SS89, BRS89, Blu89, SS91, GT92] </ref>. This previous work only discusses the occurrence of multiple local minima whereas in this paper we show that the number of such minima can grow exponentially with the dimension.
Reference: [BRS89] <author> M.L. Brady, R. Raghavan, and J. Slawny. </author> <title> Back propagation fails to separate where perceptrons succeed. </title> <journal> IEEE Transactions On Circuits and Systems, </journal> <volume> 36(5) </volume> <pages> 665-674, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The same holds for any increasing differentiable transfer function and its matching loss function. A number of previous papers discussed conditions necessary and sufficient for multiple local minima of the error function of single neurons or otherwise small networks <ref> [WD88, SS89, BRS89, Blu89, SS91, GT92] </ref>. This previous work only discusses the occurrence of multiple local minima whereas in this paper we show that the number of such minima can grow exponentially with the dimension.
Reference: [BW88] <author> E. Baum and F. Wilczek. </author> <title> Supervised learning of probability distributions by neural networks. </title> <editor> In D.Z. Anderson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 52-61, </pages> <address> New York, </address> <year> 1988. </year> <journal> American Insitute of Physics. </journal>
Reference-contexts: If the transfer function is the logistic function then it has often been suggested in the literature to use the entropic loss in artificial neural networks in place of the square loss <ref> [BW88, WD88, SLF88, Wat92] </ref>. In that case the error function of a single neuron is convex and thus has only one minimum even in the non-realizable case.
Reference: [GT92] <author> Marco Gori and Alberto Tesi. </author> <title> On the problem of local minima in backpropagation. </title> <journal> IEEE Transaction on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(1) </volume> <pages> 76-86, </pages> <year> 1992. </year>
Reference-contexts: The same holds for any increasing differentiable transfer function and its matching loss function. A number of previous papers discussed conditions necessary and sufficient for multiple local minima of the error function of single neurons or otherwise small networks <ref> [WD88, SS89, BRS89, Blu89, SS91, GT92] </ref>. This previous work only discusses the occurrence of multiple local minima whereas in this paper we show that the number of such minima can grow exponentially with the dimension.
Reference: [Hay94] <author> S. Haykin. </author> <title> Neural Networks: a Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: In flat regions the gradients with respect to the weight vector w are small, and thus gradient-based updates of the weight vector may have a hard time moving the weight vector out of these flat regions. This phenomenon can easily be observed in practice and is sometimes called saturation <ref> [Hay94] </ref>. In contrast, if the logistic function is paired with the entropic loss (see Figure 2b), then the error function turns flat only at the global minimum. The same holds for any increasing differentiable transfer function and its matching loss function.
Reference: [SLF88] <author> S. A. Solla, E. Levin, and M. Fleisher. </author> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 625-639, </pages> <year> 1988. </year>
Reference-contexts: If the transfer function is the logistic function then it has often been suggested in the literature to use the entropic loss in artificial neural networks in place of the square loss <ref> [BW88, WD88, SLF88, Wat92] </ref>. In that case the error function of a single neuron is convex and thus has only one minimum even in the non-realizable case.
Reference: [SS89] <author> E.D. Sontag and H.J. Sussmann. </author> <title> Backpropagation can give rise to spurious local minima even for networks without hidden layers. </title> <journal> Complex Systems, </journal> <volume> 3(1) </volume> <pages> 91-106, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The same holds for any increasing differentiable transfer function and its matching loss function. A number of previous papers discussed conditions necessary and sufficient for multiple local minima of the error function of single neurons or otherwise small networks <ref> [WD88, SS89, BRS89, Blu89, SS91, GT92] </ref>. This previous work only discusses the occurrence of multiple local minima whereas in this paper we show that the number of such minima can grow exponentially with the dimension.
Reference: [SS91] <author> E.D. Sontag and H.J. Sussmann. </author> <title> Back propagation separates where perceptrons do. Neural Networks, </title> <type> 4(3), </type> <year> 1991. </year>
Reference-contexts: The same holds for any increasing differentiable transfer function and its matching loss function. A number of previous papers discussed conditions necessary and sufficient for multiple local minima of the error function of single neurons or otherwise small networks <ref> [WD88, SS89, BRS89, Blu89, SS91, GT92] </ref>. This previous work only discusses the occurrence of multiple local minima whereas in this paper we show that the number of such minima can grow exponentially with the dimension.
Reference: [Wat92] <author> R. L. Watrous. </author> <title> A comparison between squared error and relative entropy metrics using several optimization algorithms. </title> <journal> Complex Systems, </journal> <volume> 6 </volume> <pages> 495-505, </pages> <year> 1992. </year>
Reference-contexts: If the transfer function is the logistic function then it has often been suggested in the literature to use the entropic loss in artificial neural networks in place of the square loss <ref> [BW88, WD88, SLF88, Wat92] </ref>. In that case the error function of a single neuron is convex and thus has only one minimum even in the non-realizable case.
Reference: [WD88] <author> B.S. Wittner and J.S. Denker. </author> <title> Strategies for teaching layered networks classification tasks. </title> <editor> In D.Z. Anderson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 850-859, </pages> <address> New York, </address> <year> 1988. </year> <journal> American Insitute of Physics. </journal>
Reference-contexts: If the transfer function is the logistic function then it has often been suggested in the literature to use the entropic loss in artificial neural networks in place of the square loss <ref> [BW88, WD88, SLF88, Wat92] </ref>. In that case the error function of a single neuron is convex and thus has only one minimum even in the non-realizable case. <p> The same holds for any increasing differentiable transfer function and its matching loss function. A number of previous papers discussed conditions necessary and sufficient for multiple local minima of the error function of single neurons or otherwise small networks <ref> [WD88, SS89, BRS89, Blu89, SS91, GT92] </ref>. This previous work only discusses the occurrence of multiple local minima whereas in this paper we show that the number of such minima can grow exponentially with the dimension.
References-found: 11

