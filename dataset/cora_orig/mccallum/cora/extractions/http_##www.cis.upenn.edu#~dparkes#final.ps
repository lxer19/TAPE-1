URL: http://www.cis.upenn.edu/~dparkes/final.ps
Refering-URL: http://www.cis.upenn.edu/~dparkes/home.html
Root-URL: 
Email: dparkes@unagi.cis.upenn.edu ungar@cis.upenn.edu  
Title: Learning and Adaption in Multiagent Systems  
Author: David C. Parkes and Lyle H. Ungar 
Address: 200 South 33rd Street, Philadelphia, PA 19104  
Affiliation: Computer and Information Science Department University of Pennsylvania  
Abstract: The goal of a self-interested agent within a multi-agent system is to maximize its utility over time. In a situation of strategic interdependence, where the actions of one agent may affect the utilities of other agents, the optimal behavior of an agent must be conditioned on the expected behaviors of the other agents in the system. Standard game theory assumes that the rationality and preferences of all the agents is common knowledge: each agent is then able to compute the set of possible equilibria, and if there is a unique equilibrium, choose a best-response to the actions that the other agents will all play. Real agents acting within a multiagent system face multiple problems: the agents may have incomplete information about the preferences and rationality of the other agents in the game, computing the equilibria can be computationally complex, and there might be many equilibria from which to choose. An alternative explanation of the emergence of a stable equilibrium is that it arises as the long-run outcome of a repeated game, in which bounded-rational agents adapt their strategies as they learn about the other agents in the system. We review some possible models of learning for games, and then show the pros and cons of using learning in a particular game, the Compensation Mechanism, a mechanism for the efficient coordination of actions within a multiagent system. keywords: game theory, mechanism design, learning
Abstract-found: 1
Intro-found: 1
Reference: <author> Fudenberg, D., and Kreps, D. M. </author> <year> 1993. </year> <title> Learning mixed equilibria. </title> <booktitle> Games and Economic Behavior 5 </booktitle> <pages> 320-367. </pages>
Reference-contexts: For example, agent 1 and agent 2 will receive payoffs of 4 and 2 respectively at the outcome (D; R). The only pure Nash equilibrium in this game is the strategy profile (U; M ). This strategy profile can easily be learned with fictitious play <ref> (Fudenberg & Kreps 1993) </ref>. The problem with fictitious play is that we cannot expect convergence to a pure strategy, even when one exists. Consider game (b) in figure 1.
Reference: <author> Fudenberg, D., and Levine, D. </author> <year> 1997. </year> <title> Theory of Learning in Games. </title> <publisher> MIT Press. forthcoming. </publisher>
Reference-contexts: This is still approximately true in a large, but finite, economy. Example: Myopic-learning agents A simple form of model-based learning that has been suggested as a good model for learning a Nash equilibrium is fictitious play <ref> (Fudenberg & Levine 1997) </ref>. The agents use information about the past choices and payoffs of their opponents to update their own beliefs about opponent choices in the next round of the game, and then choose optimal responses to these beliefs.
Reference: <author> Gmytrasiewicz, P. J., and Durfee, E. H. </author> <year> 1995. </year> <title> A rigorous, operational formalization of recursive modeling. </title> <booktitle> In (ICMAS-95). </booktitle>
Reference: <author> Hardin, G. </author> <year> 1968. </year> <title> The tragedy of the commons. </title> <booktitle> Science 162 </booktitle> <pages> 1243-1248. </pages>
Reference-contexts: The Compensation Mechanism without learning The Compensation Mechanism (Varian 1994) is a one-shot two-stage extensive-form game that implements efficient outcomes in a multiagent system. The mechanism is designed to solve market failures due to externalities in classic economics, a canonical example of which is the Tragedy of the Commons <ref> (Hardin 1968) </ref>. The idea behind the mechanism is simple: the agents all report the compensation that they require for the actions of the other agents, and the other agents consider this level of compensation when choosing their best strategy.
Reference: <author> Hu, J., and Wellman, M. P. </author> <year> 1996. </year> <title> Self-fulfilling bias in multiagent learning. </title> <booktitle> In (ICMAS-96). </booktitle>
Reference-contexts: An equilibrium of the system need not be a Nash equilibrium if agents have inaccurate models about the preferences of the other agents <ref> (Hu & Wellman 1996) </ref>. This introduces the possibility of strategic adaptive play. A strategic agent, agent A, might choose to model the learning method of another agent, agent B.
Reference: <author> Kraus, S. </author> <year> 1996. </year> <title> An overview of incentive contracting. </title> <booktitle> Artificial Intelligence 83 </booktitle> <pages> 297-346. </pages>
Reference-contexts: The problem of mechanism design is how to establish rules of the game that promote truthful revelation of preferences and allows efficient coordination <ref> (Kraus 1996) </ref>. The system designer is only able to indirectly influence the actions of the agents through an appropriate reward and penalty structure.
Reference: <author> McAfee, R. P., and McMillan, J. </author> <year> 1987. </year> <title> Auctions and bidding. </title> <journal> Journal of Economic Literature 25 </journal> <pages> 699-738. </pages>
Reference-contexts: The system designer can weaken these assumptions by implementing a mechanism that has a dominant best-strategy for every agent in the system, (e.g. the sealed-bid second price auction <ref> (McAfee & McMillan 1987) </ref> ), or allow for a repeated game where agents learn, and adapt to a Nash equilibrium. The learning dynamic permits bounded-rational agents with incomplete knowledge about the preferences of the other agents to converge to an optimal strategy.
Reference: <author> Sandholm, T. W., and Crites, R. H. </author> <year> 1995. </year> <title> Multia-gent reinforcement learning in the iterated prisoner's dilemma. </title> <type> Biosystems 37 </type> <pages> 147-166. </pages>
Reference: <author> Varian, H. R. </author> <year> 1994. </year> <title> A solution to the problem of externalities when the agents are well-informed. </title> <journal> American Economic Review 84(5) </journal> <pages> 1278-1293. </pages>
Reference-contexts: The shared arc B ! C represents an externality, and we wish to use a mechanism with an incentive structure that promotes efficient system-wide usage of this arc. The Compensation Mechanism without learning The Compensation Mechanism <ref> (Varian 1994) </ref> is a one-shot two-stage extensive-form game that implements efficient outcomes in a multiagent system. The mechanism is designed to solve market failures due to externalities in classic economics, a canonical example of which is the Tragedy of the Commons (Hardin 1968). <p> See <ref> (Varian 1994) </ref> for a proof. This is precisely the amount of taxation required for agent 1 to choose a socially-optimal (total utility- maximizing) shipping level. <p> The agents communicate compensation levels and choose best-response flows in each round of the game, and learn the best strategy. The most straightforward model to suggest is the simple myopic-learning model <ref> (Varian 1994) </ref>, where at time t + 1: p 2 (t + 1) = p 2 (t) fl p 1 (t) + @x (5) where fl is a suitable constant. This is model-free learning because the agents make no attempt to consider the learning strategy of the other agents. <p> The flow converges within around 20 iterations to the optimal system-wide flow (illustrated as the horizontal line). The long-run equilibrium of this dynamic system is the same as for the two-stage game, so we have myopic-optimality. See <ref> (Varian 1994) </ref> for a proof.
Reference: <author> Wellman, M. P. </author> <year> 1993. </year> <title> A market-oriented programming environment and its application to distributed multicommodity flow problems. </title> <journal> Journal of Artificial Intelligence Research 1 </journal> <pages> 1-23. </pages>
Reference-contexts: We consider the case of two agents shipping over the network in figure 3 <ref> (Wellman 1993) </ref>. Agent 1 has to ship 10 units of cargo from A to D, and agent 2 has to ship 10 units from D to A. Each agent chooses a shipping strategy that will meet her goal at a minimum cost.
References-found: 10

