URL: http://www.cs.cmu.edu/~thrun/papers/christos.data_mining.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/full.html
Root-URL: 
Title: Data Mining at CALD-CMU: Tools, Experiences and Research Directions  
Author: C. Faloutsos, G. Gibson, T. Mitchell, A. Moore, S. Thrun 
Affiliation: Center for Automated Learning and Discovery (CALD) Carnegie Mellon University  
Abstract: We describe the data mining problems and solutions that we have encountered in the Center for Automated Learning and Discovery (CALD) at CMU. Specifically, we describe these settings and their operational characteristics, describe our proposed solutions, list the performance results, and finally outline future research directions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <pages> pages 207-216, </pages> <month> May </month> <year> 1993. </year> <month> 8 </month>
Reference-contexts: A NASD device can do some com-putationally simple, but data-intensive processing, reducing the amount of data to be sent to the CPUs this is appropriate for mining association rules <ref> [1] </ref> as well as for the training of neural networks. Specifically, there are two principle benefits from execution in intelligent storage devices: Bandwidth reduction: Disk drives sustain 15 MB/s now and this data rate is growing at 40% per year.
Reference: [2] <author> C. G. Atkeson, A. W. Moore, and S. A. Schaal. </author> <title> Locally Weighted Learning. </title> <journal> AI Review, </journal> <volume> 11 </volume> <pages> 11-73, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: This software fl In alphabetical order. E-addresses: fchristos,mitchell+,garth,awm,thrun+g@cs.cmu.edu 1 system (using algorithms described in <ref> [2, 3] </ref>) has been deployed in a number of pro-cesses in a food manufacturing industry (bagging, packaging, cooling and cooking) with substantial economic savings.
Reference: [3] <author> C. G. Atkeson, A. W. Moore, and S. A. Schaal. </author> <title> Locally Weighted Learning for Control. </title> <journal> AI Review, </journal> <volume> 11 </volume> <pages> 75-113, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: This software fl In alphabetical order. E-addresses: fchristos,mitchell+,garth,awm,thrun+g@cs.cmu.edu 1 system (using algorithms described in <ref> [2, 3] </ref>) has been deployed in a number of pro-cesses in a food manufacturing industry (bagging, packaging, cooling and cooking) with substantial economic savings.
Reference: [4] <author> W. Burgard, D. Fox, G. Lakemeyer, D. Hahnel, D. Schulz, W. Steiner, S. Thrun, </author> <title> and A.B. Cremers. Real robots for the real world | the RHINO museum tour-guide project. </title> <note> (submitted for publication), </note> <month> October </month> <year> 1997. </year>
Reference-contexts: Here, a mobile robot gave interactive tours to people in a densely populated museum. The robot navigated almost flawlessly at a total distance of 18.6km and at an average speed of 36cm/sec, entertaining more than 3,000 visitors (real visitors and Web users) <ref> [4] </ref>. The probabilistic algorithms were critical for position tracking and model acquisition. 3.3 Lossy Compression for Data Mining Problem Ad hoc querying is difficult on very large datasets, since it is usually not possible to have the entire dataset on disk.
Reference: [5] <author> J. H. Friedman, J. L. Bentley, and R. A. Finkel. </author> <title> An Algorithm for Finding Best Matches in Logarithmic Expected Time. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: When building a Bayes Net (and learning rules) for a Census Dataset involving 17 very non-sparse attributes, the speedup was 50-fold. Further results are given in [14], which also compares AD-trees with alternative representations such as kd-trees <ref> [5, 13] </ref>, R-trees [9] and frequent sets [12]. In current work (funded by NSF and 3M corporation) we are using AD-trees to permit tractable feature-generation algorithms (which invent new attributes useful for prediction as complex functions of the original attributes).
Reference: [6] <author> Garth A. Gibson, David F. Nagle, Khalil Amiri, Fay W. Chang, Eugene M. Fein-berg, Howard Gobioff, Chen Lee, Berend Ozceri, Erik Riedel, David Rochberg, and Jim Zelenka. </author> <title> File server scaling with network-attached secure disks. </title> <booktitle> ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics'97), </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Several data mining algorithms are very suitable for intelligent storage devices, such as those advocated by CMU's Network Attached Secure Disks (NASD) project <ref> [6, 7, 8] </ref>. A NASD device can do some com-putationally simple, but data-intensive processing, reducing the amount of data to be sent to the CPUs this is appropriate for mining association rules [1] as well as for the training of neural networks.
Reference: [7] <author> Garth A. Gibson, J.S. Vitter, and J. Wilkes. </author> <title> Working group on storage i/o issues in large-scale computing. </title> <journal> Computing Surveys, </journal> <volume> 28(4), </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: Several data mining algorithms are very suitable for intelligent storage devices, such as those advocated by CMU's Network Attached Secure Disks (NASD) project <ref> [6, 7, 8] </ref>. A NASD device can do some com-putationally simple, but data-intensive processing, reducing the amount of data to be sent to the CPUs this is appropriate for mining association rules [1] as well as for the training of neural networks.
Reference: [8] <author> Garth A. Gibson and J. Wilkes. </author> <title> Self-managing network-attached storage, strategic directions in computing research: Working group on storage i/o issues in large-scale computing. </title> <journal> Computing Surveys, 28A (online)(4), </journal> <month> December </month> <year> 1996. </year>
Reference-contexts: Several data mining algorithms are very suitable for intelligent storage devices, such as those advocated by CMU's Network Attached Secure Disks (NASD) project <ref> [6, 7, 8] </ref>. A NASD device can do some com-putationally simple, but data-intensive processing, reducing the amount of data to be sent to the CPUs this is appropriate for mining association rules [1] as well as for the training of neural networks.
Reference: [9] <author> A. Guttman. R-trees: </author> <title> A dynamic index structure for spatial searching. </title> <booktitle> In Proceedings of SIGMOD 84, </booktitle> <year> 1984. </year>
Reference-contexts: When building a Bayes Net (and learning rules) for a Census Dataset involving 17 very non-sparse attributes, the speedup was 50-fold. Further results are given in [14], which also compares AD-trees with alternative representations such as kd-trees [5, 13], R-trees <ref> [9] </ref> and frequent sets [12]. In current work (funded by NSF and 3M corporation) we are using AD-trees to permit tractable feature-generation algorithms (which invent new attributes useful for prediction as complex functions of the original attributes).
Reference: [10] <author> Venky Harinarayan, Anand Rajaraman, and Jeffrey D. Ullman. </author> <title> Implementing Data Cubes Efficiently. </title> <booktitle> In Proc. ACM SIGMOD, </booktitle> <pages> pages 205-216, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Why do we wish to compute counts and contingency tables quickly? There are many applications in data mining. A database user may wish to bring up counts or contingency tables on-line while analyzing the dataset <ref> [10] </ref>. Interactive visualization tools similarly need to compute these statistics quickly. More importantly, many machine learning, statistics and data mining algorithms (e.g. Bayes Net builders, Feature Selectors, Rule Learners, Inductive Logic Program Learners, Decision Tree Learners) spend most of their effort on counting computations.
Reference: [11] <author> Flip Korn, H.V. Jagadish, and Christos Faloutsos. </author> <title> Efficiently supporting ad hoc queries in large datasets of time sequences. </title> <booktitle> ACM SIGMOD, </booktitle> <pages> pages 289-300, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Each point in the sequence is a 4 numerical value. Our goal is to compress such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the data is uncompressed. Proposed method The idea behind our method <ref> [11] </ref> is to use the so-called Singular Value Decomposition (SVD) to approximate the data matrix; we went further to reduce the approximation error, by explicitly storing those data points that were 'outliers'. The resulting method, called `SVDD' (for "SVD with Deltas") achieves all the specified goals.
Reference: [12] <author> Heikki Mannila and Hannu Toivonen. </author> <title> Multiple uses of frequent sets and condensed representations. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1996. </year>
Reference-contexts: When building a Bayes Net (and learning rules) for a Census Dataset involving 17 very non-sparse attributes, the speedup was 50-fold. Further results are given in [14], which also compares AD-trees with alternative representations such as kd-trees [5, 13], R-trees [9] and frequent sets <ref> [12] </ref>. In current work (funded by NSF and 3M corporation) we are using AD-trees to permit tractable feature-generation algorithms (which invent new attributes useful for prediction as complex functions of the original attributes).
Reference: [13] <author> A. W. Moore, J. Schneider, and K. Deng. </author> <title> Efficient Locally Weighted Polynomial Regression Predictions. </title> <booktitle> In Proceedings of the 1997 International Machine Learning Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year> <month> 9 </month>
Reference-contexts: When building a Bayes Net (and learning rules) for a Census Dataset involving 17 very non-sparse attributes, the speedup was 50-fold. Further results are given in [14], which also compares AD-trees with alternative representations such as kd-trees <ref> [5, 13] </ref>, R-trees [9] and frequent sets [12]. In current work (funded by NSF and 3M corporation) we are using AD-trees to permit tractable feature-generation algorithms (which invent new attributes useful for prediction as complex functions of the original attributes).
Reference: [14] <author> Andrew W. Moore and Mary Soon Lee. </author> <title> Cached Sufficient Statistics for Effi--cient Machine Learning with Large Datasets. </title> <journal> TR CMU-RI-TR-97-27: </journal> <note> See also www.cs.cmu.edu/~awm/rl-papers/cache.ps, CMU Robotics Institute, </note> <month> July </month> <year> 1997. </year>
Reference-contexts: When building a Bayes Net (and learning rules) for a Census Dataset involving 17 very non-sparse attributes, the speedup was 50-fold. Further results are given in <ref> [14] </ref>, which also compares AD-trees with alternative representations such as kd-trees [5, 13], R-trees [9] and frequent sets [12].
Reference: [15] <author> S. Thrun. </author> <title> A bayesian approach to landmark discovery in mobile robot navigation. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: Results These algorithms were applied to problems such as mobile robot localization, landmark detection and recognition, mapping of large-scale environments, and others <ref> [15] </ref>. In some of these domains, the probabilistic approach led to completely new insights, that made possible solutions for previously unsolved robotics problems. For example, our probabilistic algorithms has been demonstrated to enable robots to build maps of unprecedentedly large environments [17, 16].
Reference: [16] <author> S. Thrun. </author> <title> Learning maps for indoor mobile robot navigation. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: In some of these domains, the probabilistic approach led to completely new insights, that made possible solutions for previously unsolved robotics problems. For example, our probabilistic algorithms has been demonstrated to enable robots to build maps of unprecedentedly large environments <ref> [17, 16] </ref>. Other algorithms were essential for a recent installation of a mobile robot in the "Deutsches Museum Bonn". Here, a mobile robot gave interactive tours to people in a densely populated museum.
Reference: [17] <author> Sebastian Thrun, Dieter Fox, and Wolfram Burgard. </author> <title> A probabilistic approach for concurrent map acquisition and localization. </title> <type> Technical Report CMU-CS-97-183, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: In some of these domains, the probabilistic approach led to completely new insights, that made possible solutions for previously unsolved robotics problems. For example, our probabilistic algorithms has been demonstrated to enable robots to build maps of unprecedentedly large environments <ref> [17, 16] </ref>. Other algorithms were essential for a recent installation of a mobile robot in the "Deutsches Museum Bonn". Here, a mobile robot gave interactive tours to people in a densely populated museum.
Reference: [18] <author> J.D. Ullman. </author> <title> Database and Knowledge Base Systems. </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year> <month> 10 </month>
Reference-contexts: Similarly, many algorithms need to build and test huge numbers of contingency tables. A contingency table (also known as a "DataCube" <ref> [18] </ref> in the Database community) is defined by a set of attributes. A contingency table has one row for each possible set of values that the set of attributes may take.
References-found: 18

