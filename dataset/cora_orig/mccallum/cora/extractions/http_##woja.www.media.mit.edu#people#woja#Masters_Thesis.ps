URL: http://woja.www.media.mit.edu/people/woja/Masters_Thesis.ps
Refering-URL: http://woja.www.media.mit.edu/people/woja/
Root-URL: http://www.media.mit.edu
Title: Building the BIG Picture: Enhanced Resolution from Coding  Architecture and Planning,  
Author: by Roger George Kermode Andrew B. Lippman Stephen A. Benton 
Degree: B.E. (hons) Electrical Engineering,  in Partial Fulfillment of the requirements of the degree of MASTER OF SCIENCE IN MEDIA ARTS SCIENCES at the  All Rights Reserved Signature of Author Program in Media Arts and Sciences  Certified by  Thesis Supervisor Accepted By  Chairperson Departmental Committee on Graduate Students  
Note: Submitted to the Program in  Copyright Massachusetts Institute of Technology,  Program in  
Date: 1989  1990  June 1994  1994  May 6, 1994  
Address: Melbourne,  Melbourne,  
Affiliation: University of  B.Sc. Computer Science, University of  Media Arts and Sciences, School of  Massachusetts Institute of Technology  Associate Director, MIT Media Laboratory  Media Arts and Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <editor> ISO/IEC DIS 10918-1, </editor> <title> ITU- T Rec.T.81 (JPEG) Information Technology - Digital compression and coding of continuous-tone still images, </title> <year> 1992 </year>
Reference-contexts: In essence, the pictures are transmitted as objects that are composited to form a frame on decoding, and Encoder Type Information Units Example Waveform Color D1 Transform Blocks JPEG <ref> [1] </ref> Hybrid Transform Motion+ Blocks MPEG [3], [12] 2D Region Based Regions + Motion + Blocks Mussmans Work [21] Layered Coder [4], [30] Enhanced Resolution Coder Scenic 3D Objects Holtzman Encoder [13] Semantic Dialogue, Expressions Screenplay TABLE 1.1. <p> The amount and nature of the degradation caused by the use of lossy compression techniques is highly dependent on the representation chosen. Hence, it makes sense that great care should be exercised in choosing an appropriate representation when using lossy compression techniques. 2.2 JPEG Standard <ref> [1] </ref> The Joint Photographic Experts Group (JPEG) standard for still image compression is fairly simple in its design. It is comprised of three main functional units; a transformer, a quantizer, and an entropy coder.
Reference: [2] <institution> ISO/IEC 11172 (1993) Information technology - Coding of moving picture and associated audio for digital storage media as up to about 1.5 Mbit/s </institution>
Reference-contexts: and 72 frames per second. frame n frame n - 1frame n - 2 frame nframe n - 1frame n - 2 f0 f1 f0 f1 f0 f1 f0 = field 0, even scan lines f1 = field 1, odd scan lines Progressive Scanning Interlaced Scanning 28 2.4 MPEG Standards <ref> [2] </ref>, [3] The Moving Picture Experts Group (MPEG) standard MPEG1 [2] and draft standard MPEG2 [3] are based on the same ideas that were used in the creation of the JPEG algorithm for still images. <p> 1frame n - 2 frame nframe n - 1frame n - 2 f0 f1 f0 f1 f0 f1 f0 = field 0, even scan lines f1 = field 1, odd scan lines Progressive Scanning Interlaced Scanning 28 2.4 MPEG Standards <ref> [2] </ref>, [3] The Moving Picture Experts Group (MPEG) standard MPEG1 [2] and draft standard MPEG2 [3] are based on the same ideas that were used in the creation of the JPEG algorithm for still images. In fact, MPEG coders contain the same transform, quantization and entropy coding functional blocks as the JPEG coder.
Reference: [3] <editor> ISO/IEC CD 13818-2, Recommendation H.262, </editor> <title> Generic Coding of Moving Pictures and Associated Audio, </title> <type> Committee Draft, </type> <institution> Seoul, </institution> <month> November </month> <year> 1993 </year>
Reference-contexts: Current research into new approaches to coding generally attempt to address different sets of applications, or use radically different image models. For example, the MPEG 1 group, which has issued one standard for multimedia and broadcast television (MPEG-1) and is putting the final touches on another (MPEG-2 <ref> [3] </ref>), is beginning a three-year effort deliberately designed to speed the evolution of radically different coders that break the hybrid/DCT mold. These are expected to be used primarily in extremely low-bandwidth applications, but the development spurred by the MPEG effort may extend itself past that. 1. <p> Instead, the image is coded as a montage of semi-homogenous regions in conjunction with an error signal for where the model fails. The structure of the coder borrows many elements from previous coders and image processing techniques in particular MPEG 2 <ref> [3] </ref>, Teodosios Salient Still [28] and the work by Wang, Adelson and Desai [4], [30]. <p> In essence, the pictures are transmitted as objects that are composited to form a frame on decoding, and Encoder Type Information Units Example Waveform Color D1 Transform Blocks JPEG [1] Hybrid Transform Motion+ Blocks MPEG <ref> [3] </ref>, [12] 2D Region Based Regions + Motion + Blocks Mussmans Work [21] Layered Coder [4], [30] Enhanced Resolution Coder Scenic 3D Objects Holtzman Encoder [13] Semantic Dialogue, Expressions Screenplay TABLE 1.1. <p> 72 frames per second. frame n frame n - 1frame n - 2 frame nframe n - 1frame n - 2 f0 f1 f0 f1 f0 f1 f0 = field 0, even scan lines f1 = field 1, odd scan lines Progressive Scanning Interlaced Scanning 28 2.4 MPEG Standards [2], <ref> [3] </ref> The Moving Picture Experts Group (MPEG) standard MPEG1 [2] and draft standard MPEG2 [3] are based on the same ideas that were used in the creation of the JPEG algorithm for still images. <p> nframe n - 1frame n - 2 f0 f1 f0 f1 f0 f1 f0 = field 0, even scan lines f1 = field 1, odd scan lines Progressive Scanning Interlaced Scanning 28 2.4 MPEG Standards [2], <ref> [3] </ref> The Moving Picture Experts Group (MPEG) standard MPEG1 [2] and draft standard MPEG2 [3] are based on the same ideas that were used in the creation of the JPEG algorithm for still images. In fact, MPEG coders contain the same transform, quantization and entropy coding functional blocks as the JPEG coder. <p> The other extreme is to re-transmit each model in its entirety with every frame, which of course requires a higher bit rate than necessary. Clearly the best solution lies somewhere in between these two extremes. Single frames of video form the models used within MPEG <ref> [3] </ref> which solves the problem of updating them through the use of three frame types; Intra (I), Predicted (P) and Bidirectionally predicted (B).
Reference: [4] <author> E.H. Adelson, </author> <title> Layered Representations for Image Coding, MIT Media Lab, Vision & Modelling Group Technical Report No 181, </title> <month> Dec </month> <year> 1992. </year>
Reference-contexts: The structure of the coder borrows many elements from previous coders and image processing techniques in particular MPEG 2 [3], Teodosios Salient Still [28] and the work by Wang, Adelson and Desai <ref> [4] </ref>, [30]. The resulting coding model incorporates the efficiencies of motion compensation using 6 parameter affine models as well as the increased accuracy of standard block based motion compensation for foreground regions that do not match the affine model. <p> In essence, the pictures are transmitted as objects that are composited to form a frame on decoding, and Encoder Type Information Units Example Waveform Color D1 Transform Blocks JPEG [1] Hybrid Transform Motion+ Blocks MPEG [3], [12] 2D Region Based Regions + Motion + Blocks Mussmans Work [21] Layered Coder <ref> [4] </ref>, [30] Enhanced Resolution Coder Scenic 3D Objects Holtzman Encoder [13] Semantic Dialogue, Expressions Screenplay TABLE 1.1. <p> Motion JPEG is a simple extension of JPEG for motion video in which each frame consists of a single JPEG compressed image 37 3.3 Layered Coder More recently, work in the area of layered coding has been done by Wang, Adelson and Desai <ref> [4] </ref>, [30]. Along the same lines as the Salient Still and the Structured Video Coder, the Layered Coder addresses the idea of modeling not just the background by creating a small number of regions whose motion is defined by a 6 parameter affine model. <p> This feature is particularly useful for pre loading the background in video conferencing situations. The M frame also provides an avenue for the encoder to take advantage of other more computationally expensive algorithms such as Salient Still [28] or Layered Coder <ref> [4] </ref>. As these algorithms work on significantly more frames than can be stored in the enhanced resolution encoder, they may be able to generate a more accurate background representation.
Reference: [5] <author> D.H. Ballard and C.M. Brown, </author> <title> Computer Vision, </title> <publisher> Prentice Hall 1982 </publisher>
Reference-contexts: Therefore, the SAD criterion will attempt to correlate, not the block data but the small amounts of noise present in both the block and the predictor, thereby generating an inaccurate prediction of the true motion. One solution to this problem is to use the normalized correlation coefficient <ref> [5] </ref> as the error measure shown in Eqn (A.1) instead of the SAD criterion. 90 However, the Normalized Correlation requires the computation of the variance of each possible matching block for each original block , a requirement that increases the amount of computation required considerably.
Reference: [6] <author> J.R. Bergen and R.Hinorani, </author> <title> Hierarchial Motion-Based Frame Rate Conversion, </title> <month> April </month> <year> 1990. </year>
Reference-contexts: The value of a pel at a particular point is the result of a temporally weighted median of all the pels warped to that position. The motion model used to generate the warp is a 6 parameter affine model derived by a Least Squares fit to the optical ow <ref> [6] </ref>. (3.1) Some of the problems associated with salient generation are; Optical ow is particularly noisy near motion boundaries. Inaccurate optical ow fields result in an inaccurate affine model. <p> They construct this image using optical ow <ref> [6] </ref> to estimate interframe motion. One or more 6 parameter affine models are used to represent the inter frame motion. The techniques described above work well if one is attempting to model a simple scene and can generate reasonable approximations to actual regions in a frame. <p> ( , ) 0= 39 This equation can be solved using a number of methods, the two most popular are correspondence methods (which are essentially the same as block matching) and gradient methods which attempt to solve the first order Taylor Series expansion of (3.2), (3.3) over a localized region <ref> [6] </ref>. In order for gradient based optical ow to work the following assumptions are made; The overall illumination of a scene is constant, The luminance surface is smooth, The amount of motion present is small. When these assumptions are invalid optical ow fails to give the correct velocity esti mates.
Reference: [7] <author> V.M. Bove Jr., </author> <title> Entropy-based depth from focus, </title> <journal> J.Opt.Soc. Am. A/Vol. </journal> <volume> 10, No. 4, </volume> <month> April </month> <year> 1993 </year>
Reference-contexts: Generally, most cameras are deliberately designed to be slightly unfocused in order to perform this type of pre-filtering. For more details on the effect of focus on MTF responses see <ref> [7] </ref>.
Reference: [8] <author> H. Brusewitz, </author> <title> Motion compensation with triangles, </title> <booktitle> in Proc. 3rd International Conf. on 64kbit Coding of Moving Video, free session, </booktitle> <month> Sept. </month> <year> 1990 </year>
Reference: [9] <author> R. Clarke, </author> <title> Transform Coding of Images, </title> <publisher> Academic Press, </publisher> <year> 1985 </year>
Reference: [10] <author> J. Foley, A. van Dam, S. Feiner & J. Hughes, </author> <title> Computer Graphics, </title> <publisher> Addison Wesley 1990. </publisher> <pages> 106 </pages>
Reference: [11] <author> E.W. Forgy, </author> <title> Cluster analysis of multivariate data: efficiency vs. interpretability of classifications, abstract, </title> <journal> Biometrics, </journal> <volume> Vol. 21, </volume> <year> 1965. </year>
Reference: [12] <author> D. LeGall, </author> <title> MPEG: A Video Compression Standard for Multimedia Applications, </title> <journal> Communications of the ACM, April 1991, </journal> <volume> Vol. 34, No. 4, </volume> <pages> pp. </pages> <address> 46- 58. </address>
Reference-contexts: In essence, the pictures are transmitted as objects that are composited to form a frame on decoding, and Encoder Type Information Units Example Waveform Color D1 Transform Blocks JPEG [1] Hybrid Transform Motion+ Blocks MPEG [3], <ref> [12] </ref> 2D Region Based Regions + Motion + Blocks Mussmans Work [21] Layered Coder [4], [30] Enhanced Resolution Coder Scenic 3D Objects Holtzman Encoder [13] Semantic Dialogue, Expressions Screenplay TABLE 1.1.
Reference: [13] <author> H. Holtzman, </author> <title> Three-Dimensional Representations of Video using Knowledge Based Estimation, </title> <type> Masters Thesis, </type> <institution> MIT Media Lab, </institution> <year> 1991. </year>
Reference-contexts: It is hoped that the coding scheme developed here may eventually be extended into the framework of a fully three dimensional representation as developed 14 by Holtzman <ref> [13] </ref> which promises extremely high compression ratios. The genealogy of the Enhanced Resolution Coder is shown in Figure 1.1. <p> composited to form a frame on decoding, and Encoder Type Information Units Example Waveform Color D1 Transform Blocks JPEG [1] Hybrid Transform Motion+ Blocks MPEG [3], [12] 2D Region Based Regions + Motion + Blocks Mussmans Work [21] Layered Coder [4], [30] Enhanced Resolution Coder Scenic 3D Objects Holtzman Encoder <ref> [13] </ref> Semantic Dialogue, Expressions Screenplay TABLE 1.1.
Reference: [14] <author> B.K.P. Horn, </author> <title> Robot Vision, </title> <publisher> The MIT Press/Mc Raw Hill, </publisher> <year> 1986. </year>
Reference: [15] <author> D.A. Huffman, </author> <title> A Method for the Construction of Minimum Redundancy Codes, </title> <booktitle> Proceedings IRE, 1962, </booktitle> <volume> Vol. 40, </volume> <pages> pp. 1098-1101. </pages>
Reference: [16] <author> M. Irani and S. Peleg, </author> <title> Image Sequence Enhancement Using Multiple Motions Analysis, </title> <institution> Dept. of Computer Science, The Hebrew University of Jerusalem, Israel, </institution> <type> Technical Report 91-15, </type> <month> December </month> <year> 1991. </year>
Reference-contexts: + x 4 pred xint 2+ yint 2+,( ) = newval t 1 t 2 t 3 t 4 + + += x y,( ) 54 The basic mechanism for constructing an image is very similar to that used by Irani and Peleg in their work on multiple motion analysis <ref> [16] </ref> and also that of Teodosio in her work on Salient Stills [28].
Reference: [17] <author> D.H. Kelly, </author> <title> Visual responses to time-dependent stimuli, </title> <journal> J.Opt. Soc. Am., </journal> <volume> 51, </volume> <pages> pp. 422-429, </pages> <year> 1961. </year>
Reference-contexts: The frame rate at which fusion occurs is known as the critical icker frequency (cff) and is dependent upon the picture size, ambient lighting conditions, and the amount of motion within the frame <ref> [17] </ref> & [23]. For example when a T.V. screen is viewed in a poorly lit room the cfff decreases, conversely the cfff increases when the same T.V. screen is viewed in well lit conditions.
Reference: [18] <author> P. McLean, </author> <title> Structured Video Coding, </title> <type> Masters Thesis, </type> <institution> MIT Media Lab,1992. </institution>
Reference-contexts: Inaccurate optical ow fields result in an inaccurate affine model. The affine model is incapable of accurately representing motion other than that which is parallel to the image plane. 3.2 Structured Video Coding The Lucy Encoder described in Patrick McLeans Masters thesis Structured Video Coding <ref> [18] </ref> explores what is possible if one already has an accurate representation of the background.
Reference: [19] <author> J.S. Lim, </author> <title> Two-Dimensional Signal and Image Processing, </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference: [20] <author> A.B. Lippman and R.G. Kermode, </author> <title> Generalized Predictive Coding of Movies, Picture Coding Symposium (PCS 93), </title> <address> Lausanne, Switzerland, </address> <month> March </month> <year> 1993. </year>
Reference: [21] <author> H.G Musmann, M. Hotter and J. Ostermann, </author> <title> Object-Oriented Analysis-Synthesis Coding of Moving Images, Signal Processing: </title> <journal> Image Communication, </journal> <volume> Vol. 1, No. 2, </volume> <month> October </month> <year> 1989. </year> <month> 107 </month>
Reference-contexts: In essence, the pictures are transmitted as objects that are composited to form a frame on decoding, and Encoder Type Information Units Example Waveform Color D1 Transform Blocks JPEG [1] Hybrid Transform Motion+ Blocks MPEG [3], [12] 2D Region Based Regions + Motion + Blocks Mussmans Work <ref> [21] </ref> Layered Coder [4], [30] Enhanced Resolution Coder Scenic 3D Objects Holtzman Encoder [13] Semantic Dialogue, Expressions Screenplay TABLE 1.1.
Reference: [22] <author> Y. Nakaya and H. Harashima, </author> <title> Motion Compensation at Very low Rates, Picture Coding Symposium (PCS 93), </title> <address> Lausanne, Switzerland, </address> <month> March </month> <year> 1993. </year>
Reference: [23] <author> A.N. Netravali and B.G. </author> <title> Haskell, Digital Pictures, </title> <publisher> Pienum Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The frame rate at which fusion occurs is known as the critical icker frequency (cff) and is dependent upon the picture size, ambient lighting conditions, and the amount of motion within the frame [17] & <ref> [23] </ref>. For example when a T.V. screen is viewed in a poorly lit room the cfff decreases, conversely the cfff increases when the same T.V. screen is viewed in well lit conditions.
Reference: [24] <author> S. Okubo, </author> <title> Requirements for high quality video coding standards, Signal Process., </title> <type> Vol.4, No.2, </type> <month> April </month> <year> 1992. </year>
Reference: [25] <author> W.F. Schreiber, </author> <title> Fundamentals of Electronic Imaging Systems, </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg 1986 </address>
Reference: [26] <author> J.B. Stampleman, </author> <title> Scalable Video Compression, </title> <type> Masters Thesis, </type> <institution> MIT Media Lab 1992. </institution>
Reference: [27] <author> G.J. Sullivan, </author> <title> Multiple-Hypothesis Motion Compensation for Low Bit-Rate Video Coding, </title> <booktitle> IEEE ICASSP 93Sept 1993. </booktitle>
Reference: [28] <author> L. Teodosio, </author> <title> Salient Stills, </title> <type> Masters Thesis, </type> <institution> MIT Media Lab 1992. </institution>
Reference-contexts: Instead, the image is coded as a montage of semi-homogenous regions in conjunction with an error signal for where the model fails. The structure of the coder borrows many elements from previous coders and image processing techniques in particular MPEG 2 [3], Teodosios Salient Still <ref> [28] </ref> and the work by Wang, Adelson and Desai [4], [30]. The resulting coding model incorporates the efficiencies of motion compensation using 6 parameter affine models as well as the increased accuracy of standard block based motion compensation for foreground regions that do not match the affine model. <p> This chapter describes three recent approaches to image representation undertaken at the Media Laboratory that have helped to inspire the work in this thesis. 3.1 Salient Stills The Salient Still developed by Teodosio <ref> [28] </ref> is a good example of what can be done given sufficient memory and processing time. <p> 1 t 2 t 3 t 4 + + += x y,( ) 54 The basic mechanism for constructing an image is very similar to that used by Irani and Peleg in their work on multiple motion analysis [16] and also that of Teodosio in her work on Salient Stills <ref> [28] </ref>. Each pel to be displayed is reconstructed from a number of previous frames, by collecting a candidate value from each model frame via a bicubic warp Eqn (4.5) and then evaluating a cost function to determine the best value. <p> This feature is particularly useful for pre loading the background in video conferencing situations. The M frame also provides an avenue for the encoder to take advantage of other more computationally expensive algorithms such as Salient Still <ref> [28] </ref> or Layered Coder [4]. As these algorithms work on significantly more frames than can be stored in the enhanced resolution encoder, they may be able to generate a more accurate background representation. <p> Fortunately, affine models can be added according to the equations below <ref> [28] </ref> to give a single affine model that combines the effects of warping using model 1 followed by second warp using model 2 (6.1) This property of the affine model means that each affine model needs only to be transmitted once.
Reference: [29] <author> G.K. Wallace, </author> <title> The JPEG Compression Standard, </title> <journal> Communications of the ACM, April 1991, </journal> <volume> Vol. 34, No. 4, </volume> <pages> pp. 30 - 44. </pages>

References-found: 29

