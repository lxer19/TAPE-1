URL: http://www.cs.rice.edu/MSCP/papers/arpa.sw.ps.gz
Refering-URL: http://www.cs.rice.edu/MSCP/publications.html
Root-URL: 
Title: Using Compiler Technology to Drive Advanced Microprocessors  
Author: Keith D. Cooper 
Address: Houston, Texas 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Recent years have seen the introduction of a series of ever faster, ever more complex microprocessors. These advanced microprocessors have found widespread application in machines that range from personal computers to engineering workstations to massively parallel multicomputers. Unfortunately, many of the features used to endow these processors with high peak performance numbers are difficult for either human programmers or compilers to manage. This paper looks at broad trends in microprocessor architecture, relates them back to the basic problems that they present to a compiler, and examines the kind of compiler infrastructure that will be required to address them.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau. </author> <title> Perfect pipelining: A new loop parallelization technique. </title> <booktitle> In Proceedings of the 1988 European Symposium on Programming. Springer Verlag Lecture Notes in Computer Science, </booktitle> <month> Mar. </month> <year> 1988. </year>
Reference-contexts: Most of this work dealt with ordering the instructions within a single basic block. Recently, research in this area has focused on aggressive techniques that reorder entire loops or loop nests. These techniques, like Fisher's trace scheduling [13], Aiken and Nicolau's perfect pipelining <ref> [1] </ref>, and Lam's software pipelining [22], create an order for the instructions in which pieces of several different iterations of the loop may be executing concurrently.
Reference: [2] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Multiprocessor systems present their own set of challenges to the compiler. A large and active research community is attacking the problem of compiling code that makes effective use of these machines. Over the past decade, progress has been made in detecting parallelism and in improving performance on parallel programs <ref> [2, 3, 21] </ref>. The key tool that compilers use to improve performance on multiprocessor systems is static program analysis. Restructuring compilers perform data-flow analysis, data-dependence analysis, and control-dependence analysis to understand the sequence of memory references and computation that can occur during a program's execution [16, 24].
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> PFC: A program to convert Fortran to parallel form. </title> <editor> In K. Hwang, editor, </editor> <booktitle> Supercomputers: Design and Applications, </booktitle> <pages> pages 186-203. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: Multiprocessor systems present their own set of challenges to the compiler. A large and active research community is attacking the problem of compiling code that makes effective use of these machines. Over the past decade, progress has been made in detecting parallelism and in improving performance on parallel programs <ref> [2, 3, 21] </ref>. The key tool that compilers use to improve performance on multiprocessor systems is static program analysis. Restructuring compilers perform data-flow analysis, data-dependence analysis, and control-dependence analysis to understand the sequence of memory references and computation that can occur during a program's execution [16, 24].
Reference: [4] <author> P. Briggs, K. D. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Thus, our strategy is to use interprocedural transformations in a goal-directed way that is, we identify a high-payoff transformation that can be helped by some combination of procedure cloning and inlining and use them to enable the high-payoff transformation <ref> [4] </ref>. We have used this goal-directed strategy in several experiments; we have been pleased with the results. Special case code generation. A natural extension of our work with procedure cloning is to be more aggressive about generating conditional code.
Reference: [5] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIG-PLAN 86 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: It must use the results of this analysis to transform the program into a form where it is more amenable to efficient execution on the target system. Interprocedural analysis. Algorithms for performing data-flow analysis across whole programs are both readily available and well understood <ref> [5, 7, 8, 10] </ref>. These techniques are beginning to appear in commercial systems [25]. Perhaps less well appreciated are the limitations of this kind of analysis, the role that it can play in code optimization, and the rational consequences of that role.
Reference: [6] <author> M. Burke and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <note> To appear in ACM Transactions on Programming Languages and Systems. </note>
Reference-contexts: By retaining information across compilations, the compiler can also limit the amount of recompilation that must be performed in response to an editing change to one procedure <ref> [6] </ref>. 5 Conclusions Microprocessor-based systems present many challenges to the compiler writer. Given the proliferation of systems designed around microprocessors, it is imperative that we continue to attack the problems that we already see and that we examine the trends in microprocessor design to try and predict future developments.
Reference: [7] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pages 47-56. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1988. </year>
Reference-contexts: It must use the results of this analysis to transform the program into a form where it is more amenable to efficient execution on the target system. Interprocedural analysis. Algorithms for performing data-flow analysis across whole programs are both readily available and well understood <ref> [5, 7, 8, 10] </ref>. These techniques are beginning to appear in commercial systems [25]. Perhaps less well appreciated are the limitations of this kind of analysis, the role that it can play in code optimization, and the rational consequences of that role.
Reference: [8] <author> D. Callahan, K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 152-161, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: It must use the results of this analysis to transform the program into a form where it is more amenable to efficient execution on the target system. Interprocedural analysis. Algorithms for performing data-flow analysis across whole programs are both readily available and well understood <ref> [5, 7, 8, 10] </ref>. These techniques are beginning to appear in commercial systems [25]. Perhaps less well appreciated are the limitations of this kind of analysis, the role that it can play in code optimization, and the rational consequences of that role.
Reference: [9] <author> F. C. Chow. </author> <title> Minimizing register use penalty at procedure call. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pages 85-94, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Interprocedural optimization. The compiler can apply interprocedural transformations for two very different reasons: to directly improve the code's performance and to change the program's structure in a way that enables some other transformation. The former class of optimizations are relatively straightforward. They include inline substitution, cross-procedural register allocation <ref> [9, 28] </ref>, and limited forms of interprocedural code motion [17]. The latter situation is more complex; the difficulty here is deciding when and where to apply an optimization. Inline substitution and procedure cloning can fall in this category. Procedure cloning is an unusual case.
Reference: [10] <author> K. Cooper and K. Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-59. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: It must use the results of this analysis to transform the program into a form where it is more amenable to efficient execution on the target system. Interprocedural analysis. Algorithms for performing data-flow analysis across whole programs are both readily available and well understood <ref> [5, 7, 8, 10] </ref>. These techniques are beginning to appear in commercial systems [25]. Perhaps less well appreciated are the limitations of this kind of analysis, the role that it can play in code optimization, and the rational consequences of that role.
Reference: [11] <author> K. D. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software Practice and Experience, </journal> <month> June </month> <year> 1991. </year>
Reference-contexts: We have been exploring the problem of deciding how to use inlining and cloning to create opportunities for other transformations. Our study of inline substitution (using commercial FORTRAN compilers) showed that secondary effects in the compilers often overshadowed any benefit from inlining <ref> [11] </ref>. Thus, our strategy is to use interprocedural transformations in a goal-directed way that is, we identify a high-payoff transformation that can be helped by some combination of procedure cloning and inlining and use them to enable the high-payoff transformation [4]. <p> Special case code generation. A natural extension of our work with procedure cloning is to be more aggressive about generating conditional code. In our inlining study, both the vectorizing compilers had cases where they incorrectly assumed that parallel execution of a loop was profitable <ref> [11] </ref>. The compilers should have inserted a run-time test on the number of iterations and generated both a sequential and a parallel version of the loop.
Reference: [12] <author> J. Fabri. </author> <title> Automatic storage optimization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '79 Symposium on Compiler Construction, SIGPLAN Notices 14(8), </booktitle> <pages> pages 83-91, </pages> <month> Aug. </month> <year> 1979. </year>
Reference-contexts: Work in operating systems concentrated on statistical measures of program behavior, in large part because the operating system cannot look inside a program to ascertain its possible future behavior. In the compiler construction community, work has looked at packing memory for locality <ref> [12, 27] </ref> and transforming loops to improve their cache behavior [14, 29]. The tools to improve locality exist.
Reference: [13] <author> J. A. Fisher, J. R. Ellis, J. C. Ruttenberg, and A. Nicolau. </author> <title> Parallel processing: A smart compiler and a dumb machine. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, SIGPLAN Notices 19(6), </booktitle> <pages> pages 37-47, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: This work was quickly adopted into compilers for microproces-sors [15, 19]. Most of this work dealt with ordering the instructions within a single basic block. Recently, research in this area has focused on aggressive techniques that reorder entire loops or loop nests. These techniques, like Fisher's trace scheduling <ref> [13] </ref>, Aiken and Nicolau's perfect pipelining [1], and Lam's software pipelining [22], create an order for the instructions in which pieces of several different iterations of the loop may be executing concurrently.
Reference: [14] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: In the compiler construction community, work has looked at packing memory for locality [12, 27] and transforming loops to improve their cache behavior <ref> [14, 29] </ref>. The tools to improve locality exist. Open areas for future work include improved decision algorithms to drive the transformation process, techniques that consider multiple levels of cache, and mechanisms to parameterize the entire process around a few simple characteristics of the cache. Multiprocessor parallelism.
Reference: [15] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 11-16, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Thus, a long and deep literature has developed on compiler-based techniques to capitalize on features like pipelining and superscalar instruction issue. In general, these techniques are referred to as instruction scheduling. This work was quickly adopted into compilers for microproces-sors <ref> [15, 19] </ref>. Most of this work dealt with ordering the instructions within a single basic block. Recently, research in this area has focused on aggressive techniques that reorder entire loops or loop nests.
Reference: [16] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: The key tool that compilers use to improve performance on multiprocessor systems is static program analysis. Restructuring compilers perform data-flow analysis, data-dependence analysis, and control-dependence analysis to understand the sequence of memory references and computation that can occur during a program's execution <ref> [16, 24] </ref>. These compilers examine the program, on a loop nest by loop nest basis, trying to discover loop nests whose iterations can be run in parallel.
Reference: [17] <author> M. W. Hall, K. Kennedy, and K. McKinley. </author> <title> Interprocedural transformation for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: The former class of optimizations are relatively straightforward. They include inline substitution, cross-procedural register allocation [9, 28], and limited forms of interprocedural code motion <ref> [17] </ref>. The latter situation is more complex; the difficulty here is deciding when and where to apply an optimization. Inline substitution and procedure cloning can fall in this category. Procedure cloning is an unusual case.
Reference: [18] <author> P. Havlak and K. Kennedy. </author> <title> Experience with interprocedural analysis of array side effects. </title> <booktitle> In Proceedings of Supercomputing 90, </booktitle> <pages> pages 952-961. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: A second source of approximation comes from the treatment of arrays as single units that is, if any element of an array is modified, the entire array is deemed to have been modified. Unfortunately, experience with interprocedural analyzers suggests that more precise information is needed to support dependence analysis <ref> [18] </ref>. The precision can come in two ways: deeper forms of analysis, and transformations on the program. To perform deeper analysis, we must reformulate the basic problems that are solved. As an example, Havlak has implemented a more precise form of side-effect analysis called regular section analysis [18]. <p> support dependence analysis <ref> [18] </ref>. The precision can come in two ways: deeper forms of analysis, and transformations on the program. To perform deeper analysis, we must reformulate the basic problems that are solved. As an example, Havlak has implemented a more precise form of side-effect analysis called regular section analysis [18]. It uses a simple lattice formulation to track more complex information about reference patterns within an array. A more subtle form of imprecision arises because the compiler analyzes the program as written.
Reference: [19] <author> J. L. Hennessy and T. Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Thus, a long and deep literature has developed on compiler-based techniques to capitalize on features like pipelining and superscalar instruction issue. In general, these techniques are referred to as instruction scheduling. This work was quickly adopted into compilers for microproces-sors <ref> [15, 19] </ref>. Most of this work dealt with ordering the instructions within a single basic block. Recently, research in this area has focused on aggressive techniques that reorder entire loops or loop nests.
Reference: [20] <author> Intel Corporation. </author> <title> i860 TM XP Microprocessor, </title> <year> 1991. </year>
Reference-contexts: To make matters worse, the cost of memory access may actually be dependent on the manner in which it is used. For example, on the i860XP, the double precision floating-point load instruction takes the same number of cycles as the quad-word floating-point load <ref> [20] </ref>. Thus, accessing adjacent, quad-word aligned items can halve the cost of loading data. Of course, using this feature increases the demand for registers and introduces a pairwise allocation problem. This also introduces a somewhat subtle but pervasive problem: tracking alignment.
Reference: [21] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> Analysis and transformation of programs for parallel computation. </title> <booktitle> In Proceedings of COMPSAC 80, the 4th International Computer Software and Applications Conference, </booktitle> <pages> pages 709-715, </pages> <address> Chicago, IL, </address> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: Multiprocessor systems present their own set of challenges to the compiler. A large and active research community is attacking the problem of compiling code that makes effective use of these machines. Over the past decade, progress has been made in detecting parallelism and in improving performance on parallel programs <ref> [2, 3, 21] </ref>. The key tool that compilers use to improve performance on multiprocessor systems is static program analysis. Restructuring compilers perform data-flow analysis, data-dependence analysis, and control-dependence analysis to understand the sequence of memory references and computation that can occur during a program's execution [16, 24].
Reference: [22] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pages 318-328, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Most of this work dealt with ordering the instructions within a single basic block. Recently, research in this area has focused on aggressive techniques that reorder entire loops or loop nests. These techniques, like Fisher's trace scheduling [13], Aiken and Nicolau's perfect pipelining [1], and Lam's software pipelining <ref> [22] </ref>, create an order for the instructions in which pieces of several different iterations of the loop may be executing concurrently.
Reference: [23] <author> S. S. Lavrov. </author> <title> Store economy in closed operator schemes. </title> <journal> Journal of Computational Mathematics and Mathematical Physics, </journal> <volume> 1(4) </volume> <pages> 687-701, </pages> <year> 1961. </year> <title> English translation in U.S.S.R. </title> <journal> Computational Mathematics and Mathematical Physics 3 </journal> <pages> 810-828, </pages> <year> 1962. </year>
Reference-contexts: Thus, there is broad interest in techniques for improving the locality of programs. Like scheduling, this is not a new problem. In the earliest days of compiling, researchers worked on techniques to pack memory in order to run practical problems on machines with tiny memories <ref> [23] </ref>. With the advent of virtual memory, the focus shifted to improving page locality that is, decreasing the number of page faults during a program's execution.
Reference: [24] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, SIGPLAN Notices 26(6), </booktitle> <pages> pages 1-14, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The key tool that compilers use to improve performance on multiprocessor systems is static program analysis. Restructuring compilers perform data-flow analysis, data-dependence analysis, and control-dependence analysis to understand the sequence of memory references and computation that can occur during a program's execution <ref> [16, 24] </ref>. These compilers examine the program, on a loop nest by loop nest basis, trying to discover loop nests whose iterations can be run in parallel.
Reference: [25] <author> R. Metzger and P. Smith. </author> <title> The CONVEX application compiler. </title> <journal> Fortran Journal, </journal> <volume> 3(1) </volume> <pages> 8-10, </pages> <year> 1991. </year>
Reference-contexts: Interprocedural analysis. Algorithms for performing data-flow analysis across whole programs are both readily available and well understood [5, 7, 8, 10]. These techniques are beginning to appear in commercial systems <ref> [25] </ref>. Perhaps less well appreciated are the limitations of this kind of analysis, the role that it can play in code optimization, and the rational consequences of that role. Interprocedural data-flow analysis is a technique for estimating, at compile time, sets of facts that will hold at run-time.
Reference: [26] <author> D. S. Scott and G. R. Withers. </author> <title> Performance and assembly language programming of the iPSC/860 system. </title> <booktitle> In The Sixth Distributed Memory Computer Conference Proceedings, </booktitle> <address> Portland, OR, </address> <pages> pages 534-541, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Deeper memory hierarchies. In many microprocessor-based systems, managing the memory hierarchy is as important as properly scheduling instructions. For example, on the iPSC/860 ahypercube-connected multiprocessor based on the i860XR changing DRAM pages 1 incurs a ten-cycle stall <ref> [26] </ref>. In many real loops, stalls from the memory controller will dominate the cost of the actual computation. Thus, there is broad interest in techniques for improving the locality of programs. Like scheduling, this is not a new problem.
Reference: [27] <author> K. O. Thabit. </author> <title> Cache Management by the Compiler. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1982. </year>
Reference-contexts: Work in operating systems concentrated on statistical measures of program behavior, in large part because the operating system cannot look inside a program to ascertain its possible future behavior. In the compiler construction community, work has looked at packing memory for locality <ref> [12, 27] </ref> and transforming loops to improve their cache behavior [14, 29]. The tools to improve locality exist.
Reference: [28] <author> D. Wall. </author> <title> Register windows vs. register allocation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pages 67-78. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1988. </year>
Reference-contexts: Interprocedural optimization. The compiler can apply interprocedural transformations for two very different reasons: to directly improve the code's performance and to change the program's structure in a way that enables some other transformation. The former class of optimizations are relatively straightforward. They include inline substitution, cross-procedural register allocation <ref> [9, 28] </ref>, and limited forms of interprocedural code motion [17]. The latter situation is more complex; the difficulty here is deciding when and where to apply an optimization. Inline substitution and procedure cloning can fall in this category. Procedure cloning is an unusual case.
Reference: [29] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, SIGPLAN Notices 26(6), </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In the compiler construction community, work has looked at packing memory for locality [12, 27] and transforming loops to improve their cache behavior <ref> [14, 29] </ref>. The tools to improve locality exist. Open areas for future work include improved decision algorithms to drive the transformation process, techniques that consider multiple levels of cache, and mechanisms to parameterize the entire process around a few simple characteristics of the cache. Multiprocessor parallelism.
References-found: 29

