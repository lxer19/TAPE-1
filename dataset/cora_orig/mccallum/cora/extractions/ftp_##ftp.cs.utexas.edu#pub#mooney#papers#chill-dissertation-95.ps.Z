URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/chill-dissertation-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/nl.html
Root-URL: http://www.cs.utexas.edu
Title: by  
Author: John M. Zelle 
Date: 1995  
Note: Copyright  
Abstract-found: 0
Intro-found: 1
Reference: <author> Abramson, H., & Dahl, V. </author> <year> (1989). </year> <title> Logic Grammars. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Allen, J. F. </author> <year> (1995). </year> <title> Natural Language Understanding. </title> <address> Benjamin/Cummings, Menlo Park, CA. </address>
Reference-contexts: This chapter discusses some of the experiments which have been conducted with Chill for this type of representation, some of which were first reported in (Zelle & Mooney, 1993b). 5.1 Background Semantic case analyses have proven very useful in the construction of natural language systems <ref> (Allen, 1995) </ref>. However, the construction of parsers to produce such representations is complicated by the need to use domain-specific semantic information to resolve ambiguity and produce accurate analyses.
Reference: <author> Anderson, J. R. </author> <year> (1977). </year> <title> Induction of augmented transition networks. </title> <journal> Cognitive Science, </journal> <volume> 1, </volume> <pages> 125-157. </pages>
Reference: <author> Anderson, J. R. </author> <year> (1983). </year> <title> The Architecture of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Banerji, R. B. </author> <year> (1992). </year> <title> Learning theoretical terms. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 93-110. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Beckwith, R., Fellbaum, C., Gross, D., & Miller, G. </author> <year> (1991). </year> <title> Wordnet: A lexical database organized on psycholinguistic principles. </title> <editor> In Zernik, U. (Ed.), </editor> <title> Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, </title> <journal> pp. </journal> <pages> 211-232. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: More specific lexical information could be directly inserted into the input by using a statistical tagger in a pre-processing step. Likewise, control rules might be able to use existing semantic information such as that provided by an ontological hierarchy (e.g. WordNet <ref> (Beckwith, Fellbaum, Gross, & Miller, 1991) </ref>). In principle, any relevant background knowledge should improve the performance of the system. Unfortunately, additional 129 knowledge has the side-effect of significantly slowing the search for specializations in the top-down induction component.
Reference: <author> Berwick, B. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Berwick, R. C., & Pilato, S. </author> <year> (1987). </year> <title> Learning syntax by automata induction. </title> <journal> Machine Learning, </journal> <volume> 2 (1), </volume> <pages> 9-38. </pages> <note> 182 Black, </note> <author> E., Jelineck, F., Lafferty, J., Magerman, D., Mercer, R., & Roukos, S. </author> <year> (1993). </year> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 31-37 Columbus, Ohio. </address>
Reference: <author> Black, E., Lafferty, J., & Roukaos, S. </author> <year> (1992). </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 185-192 Newark, Delaware. </address>
Reference: <author> Black, E. e. </author> <year> (1991). </year> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars.. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. </pages> <month> 306-311. </month> <title> Borland International (1988). Turbo Prolog 2.0 Reference Guide. </title> <booktitle> Borland International, </booktitle> <address> Scotts Valley, CA. </address>
Reference-contexts: Another accuracy measure, which has been used in evaluating systems that bracket the input sentence into unlabeled constituents, is the proportion of constituents in the parse that do not cross any constituent boundaries in the correct tree <ref> (Black, 1991) </ref>.
Reference: <author> Brill, E. </author> <year> (1993). </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 259-265 Columbus, Ohio. </address>
Reference-contexts: These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees (Black, Jelineck, Lafferty, Magerman, Mercer, & Roukos, 1993; Magerman, 1994) and transformations <ref> (Brill, 1993) </ref>, statistical methods dominate. A common thread in all of these approaches is that the acquired knowledge is represented in a propositional form (perhaps with associated probabilities). <p> Statistical approaches relying on n-grams or probabilistic context-free grammars would have difficulty due to the large number of terminal symbols (around 400) appearing in the modest-sized training corpus. The data for lexical selection would be too sparse to adequately train the pre-defined models. Likewise, the transformational approach of <ref> (Brill, 1993) </ref> is limited to bracketing strings of lexical classes, not words.
Reference: <author> Brown, J. S., & Burton, R. R. </author> <year> (1975). </year> <title> Multiple representations of knowledge for tutorial reasoning. </title> <editor> In Bobrow, D., & Collins, A. (Eds.), </editor> <booktitle> Representation and Understanding. </booktitle> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Cameron-Jones, R. M., & Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5 (1), </volume> <pages> 33-42. </pages>
Reference: <author> Charniak, E. </author> <year> (1993). </year> <title> Statistical Language Learning. </title> <publisher> MIT Press. </publisher>
Reference-contexts: With such an approach, the human burden is on engineering useful representations, relegating the difficult issue of constructing a parser for such representations to the acquisition system. Following in the footsteps of speech recognition research, corpus-based natural language processing has concentrated primarily on statistical techniques <ref> (Charniak, 1993) </ref> applied to such problems as part-of-speech tagging (Merialdo, 1994; Charniak, Hendrickson, Jacobson, & Perkowitz, 1993) and the induction of stochastic context-free grammars (Periera & Schabes, 1992) or transition networks (Miller, Bobrow, Ingria, & Schwartz, 1994). These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods.
Reference: <author> Charniak, E., & Carroll, G. </author> <year> (1994). </year> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address> <note> 183 Charniak, </note> <author> E., Hendrickson, C., Jacobson, N., & Perkowitz, M. </author> <year> (1993). </year> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 784-789 Washington, D.C. </address>
Reference-contexts: One major difference is the type of analysis provided. Chill learns parsers that produce complete, labeled parse trees; other systems have learned to produced simple bracketings of input sentences (Periera & Schabes, 1992; Brill, 1993), or probabilistic language models which assign sentences probabilities <ref> (Charniak & Carroll, 1994) </ref>. Another dimension of variation is the type of input provided to the learning system. While Chill requires only a suitably annotated corpus, other approaches have utilized an existing, complex, hand-crafted grammar that over-generates (Black et al., 1993; Black, Lafferty, & Roukaos, 1992).
Reference: <author> Cohen, W. W. </author> <year> (1990). </year> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 268-276 Austin, TX. </address>
Reference-contexts: Multiple solutions may be found for a given goal by asking the prover to backtrack and find alternative bindings for the output arguments. The notion of search-control in a Prolog program can be viewed as a clause selection problem <ref> (Cohen, 1990) </ref>. Clause selection is the process of deciding which of several applicable program clauses should be used to reduce a particular subgoal during the course of a proof. If program clauses are always applied appropriately, the program executes deterministically (without backtracking) and produces only correct solutions.
Reference: <author> Cohen, W. W. </author> <year> (1993). </year> <title> Pac-learning a resticted class of recursive logic programs. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 86-92 Washington, D.C. </address>
Reference-contexts: Recursive hypotheses are evaluated by using positive examples as a model of the predicate being learned. When the examples are incomplete, they provide a "noisy oracle" and Foil has difficulty learning even simple recursive concepts <ref> (Cohen, 1993) </ref>. Although specifically designed to deal with issues arising in the parser acquisition problem, Chillin is itself a novel ILP system combining elements of both top-down and bottom-up ILP methods.
Reference: <author> Cohen, W. </author> <year> (1992). </year> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 102-110 Aberdeen, Scotland. </address>
Reference: <author> Dahl, V., & McCord, M. C. </author> <year> (1983). </year> <title> Treating coordination in logic grammars. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 9 (2), </volume> <pages> 69-91. </pages>
Reference-contexts: For example, the conjunction in "What states border Texas and Oklahoma" requires two instances of next to/2 which are both introduced by the word "border." In general, satisfactory handling of conjunctions often requires special considerations in NLP systems <ref> (Dahl & McCord, 1983) </ref>, and no effort has been made to solve these problems in the context of Geoquery.
Reference: <author> DeJong, G. F., & Mooney, R. J. </author> <year> (1986). </year> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1 (2), </volume> <pages> 145-176. </pages>
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., & Harms, R. T. (Eds.), </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <publisher> Holt, Reinhart and Winston, </publisher> <address> New York. </address>
Reference-contexts: The following sections explain these criteria and the details of Chill in the context of case-role mapping. 3.2 An Example Framework: Case-Role Mapping 3.2.1 The Mapping Problem Among the most common meaning-representation languages for natural language systems are various incarnations of case-role analysis. Traditional case theory <ref> (Fillmore, 1968) </ref> decomposes a sentence into a proposition represented by the main verb and various arguments such as agent, patient, and instrument, represented by noun phrases. The basic mapping problem is to decide which sentence constituents fill which roles.
Reference: <author> Gazdar, G., & Mellish, C. </author> <year> (1989). </year> <title> Natural Language Processing in Prolog. </title> <publisher> Adison-Wesley Publishing Company, </publisher> <address> New York. </address>
Reference: <author> Hendrix, G. G., Sagalowicz, E. S. D., & Slocum, J. </author> <year> (1978). </year> <title> Developing a natural language interface to complex data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 3 (2), </volume> <pages> 105-147. </pages>
Reference: <author> Hindle, D., & Rooth, M. </author> <year> (1993). </year> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19 (1), </volume> <pages> 103-120. </pages> <note> 184 Kijsirikul, </note> <author> B., Numao, M., & Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 44-49 San Jose, CA. </address>
Reference: <author> Klein, S., & Kuppin, M. A. </author> <year> (1970). </year> <title> An interactive, heuristic program for learning transformational grammars. </title> <type> Tech. rep. </type> <institution> TR-97, Computer Sciences Department, Univeristy of Wisconsin, Madison, Madison, WI. </institution>
Reference: <author> Knowlton, K. </author> <year> (1962). </year> <title> Sentence Parsing with a Self-Organizing Heuristic Program. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Laird, J., Rosenbloom, P., & Newell, A. </author> <year> (1986). </year> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 (1). </volume>
Reference: <author> Langley, P. </author> <year> (1982). </year> <title> Language acquisition through error recovery. </title> <journal> Cognition and Brain Theory, </journal> <volume> 5. </volume>
Reference: <author> Langley, P. </author> <year> (1985). </year> <title> Learning to search: From weak methods to domain specific heuristics. </title> <journal> Cognitive Science, </journal> <volume> 9 (2), </volume> <pages> 217-260. </pages>
Reference: <author> Langley, P., & Carbonell, J. </author> <year> (1985). </year> <title> Language acquisition and machine learning. </title> <editor> In MacWhinney, B. (Ed.), </editor> <booktitle> Mechanisms of Language Acquisition, </booktitle> <pages> pp. 115-155. </pages> <publisher> Lawrence Erlbaum Associates, Inc/, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Lapointe, S., & Matwin, S. </author> <year> (1992). </year> <title> Sub-unification: A tool for efficient induction of recursive programs. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 273-281 Aberdeen, Scotland. </address>
Reference: <editor> Lavrac, N., & Dzeroski, S. (Eds.). </editor> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference: <author> Lehman, J. F. </author> <year> (1992). </year> <title> Adaptive Parsing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address> <note> 185 Lehman, </note> <author> J. F. </author> <year> (1994). </year> <title> Toward the essential nature of satistical knowledge in sense resolution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference: <author> Litman, D. J. </author> <year> (1994). </year> <title> Classifying cue phrases in text and speech using machine learning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference-contexts: Although the technique has only been used with case-role type representations, variations might also be useful for the type of lexicon required by the database-query task. ILP techniques might also be usefully applied in learning larger discourse structures <ref> (Litman, 1994) </ref> and in information extraction tasks (Soderland & Lehnert, 1994). Larger discourse units might be described in terms of scripts in a suitable logic-oriented MRL. ILP could then be used to learn rules for script-selection and role-binding using techniques similar to those used in the database-query framework in Chill.
Reference: <author> Magerman, D. M. </author> <year> (1994). </year> <title> Natrual Lagnuage Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Manning, C. D. </author> <year> (1993). </year> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 235-242 Columbus, Ohio. </address>
Reference-contexts: Corpus-based methods may be used to augment the knowledge of a traditional parser, for example by acquiring new case-frames for verbs <ref> (Manning, 1993) </ref> or acquiring models to resolve lexical or attachment ambiguities (Lehman, 1994; Hindle & Rooth, 1993). More radical approaches attempt to replace the hand-crafted components altogether, extracting all required linguistic knowledge directly from suitable corpora.
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Notice that the process of parsing a sentence is a search problem. The parser must find a sequence of operators that transforms the initial state into a final representation. A parser that never retracts (backtracks over) an operator once it has 21 been applied is said to be deterministic <ref> (Marcus, 1980) </ref>. Of course, natural languages are rife with ambiguities: lexical-class ambiguity, attachment ambiguity, semantic ambiguity, pronominal reference, etc. Given these ambiguities, choosing the correct operator to apply at any given point during parsing requires a great deal of knowledge, from information about syntax, to domain-specific world-knowledge.
Reference: <author> Marcus, M., Santorini, B., & Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of English: The Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19 (2), </volume> <pages> 313-330. </pages>
Reference-contexts: For example, if the desired system is a broad-coverage syntactic parser, then the required corpus is a large sampling of text paired with the desired syntactic parse trees. Such a corpus is sometimes called a treebank <ref> (Marcus, Santorini, & Mar-cinkiewicz, 1993) </ref>. Although some systems have used raw (not annotated) text for language acquisition, those employing annotations have proved more powerful (Peri-era & Schabes, 1992). The second task, acquisition, is a machine learning problem. <p> In order to compare with these approaches, a series of experiments was conducted using Chill to generate syntactic parsers. We selected as a test corpus a portion of the ATIS dataset (specifically the file ti tb) from a preliminary version of the Penn Treebank <ref> (Marcus et al., 1993) </ref>.
Reference: <author> McClelland, J. L., & Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The work of Miikkulainen and Dyer (1991), who used the case-role mapping task to demonstrate their Fgrep method, is illustrative. Their model employs a recurrent network which allows words of an input sentence to be processed sequentially. Following <ref> (McClelland & Kawamoto, 1986) </ref>, the network output has fixed slots for verb, agent, instrument, patient and modifier (a slot for the modifier of a patient such as "cheese" in "pasta with cheese"). <p> The network is trained using a modification of backpropagation that automatically develops distributed word encodings during the training process. Words are presented to, and read out of, the network using these learned encodings. The model was demonstrated using a set of 1475 sentence/case-structure pairs originally from <ref> (McClelland & Kawamoto, 1986) </ref> (hereafter referred to as the M & K Corpus). Connectionist models face a number of difficulties in handling natural language. Since the output structures are flat (non-recursive) it is unclear how the embedded propositions in more sophisticated analyses can be handled.
Reference: <author> Merialdo, B. </author> <year> (1994). </year> <title> Tagging English text with a probabilistic model. </title> <journal> Computational Linguistics, </journal> <volume> 20 (2), </volume> <pages> 155-172. </pages>
Reference: <author> Miikkulainen, R., & Dyer, M. G. </author> <year> (1991). </year> <title> Natural language processing with modular PDP networks and distributed lexicon. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 343-399. </pages>
Reference-contexts: The system also exhibits the desirable property that it tends to produce very few inaccurate parses. The graph in Figure 5.2 shows the probability that a produced parse is incorrect as a function of training set size. This initial experiment, following the example in <ref> (Miikkulainen & Dyer, 1991) </ref>, did not use distinct tokens for different senses of ambiguous words. However, one of the original motivations for connectionist approaches was the ability to handle lexical ambiguity (McClelland & Kawamoto, 1986; St. John & McClelland, 1990). <p> The closest comparison can be made with the results in <ref> (Miikkulainen & Dyer, 1991) </ref> where an accuracy of 95% was achieved in assigning words to case slots after training with 1439 of the 1475 pairs.
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 186 Miikkulainen, </note> <author> R. </author> <year> (1995). </year> <title> Subsymbolic case-role analysis of sentences with embedded clauses. </title> <journal> Cognitive Science. </journal> <note> in press. </note>
Reference: <author> Miller, S., Bobrow, R., Ingria, R., & Schwartz, R. </author> <year> (1994). </year> <title> Hidden understanding models of natural language. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 25-32. </pages>
Reference-contexts: Following in the footsteps of speech recognition research, corpus-based natural language processing has concentrated primarily on statistical techniques (Charniak, 1993) applied to such problems as part-of-speech tagging (Merialdo, 1994; Charniak, Hendrickson, Jacobson, & Perkowitz, 1993) and the induction of stochastic context-free grammars (Periera & Schabes, 1992) or transition networks <ref> (Miller, Bobrow, Ingria, & Schwartz, 1994) </ref>. These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees (Black, Jelineck, Lafferty, Magerman, Mercer, & Roukos, 1993; Magerman, 1994) and transformations (Brill, 1993), statistical methods dominate.
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 564-569 St. Paul, MN. </address>
Reference-contexts: These control rules may be viewed as preconditions that were previously omitted from the search operators. Adding these preconditions results in a more accurate problem solver. The learning of search-control knowledge has been investigated primarily in the context of STRIPS-like planners <ref> (Minton, 1988) </ref> and forward-chaining production systems (Mitchell, 1983; Langley, 1985; Laird, Rosenbloom, & Newell, 1986). The basic inputs to a control-rule learning system are an initial search-based problem solver, and a set of training problems. The output is an improved problem-solver.
Reference: <author> Mitchell, T. </author> <year> (1983). </year> <title> Learning and problem solving. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1139-1151 Karlsruhe, West Germany. </address>
Reference-contexts: A control-rule is a kind of heuristic that can be used to select the most useful option given the current context of a particular decision. Some systems learn control rules by using inductive (Simliarity Based Learning, or SBL) methods <ref> (Mitchell, Utgoff, & Banerji, 1983) </ref>.
Reference: <author> Mitchell, T., Utgoff, T., & Banerji, R. </author> <year> (1983). </year> <title> Learning problem solving heuristics by experimentation. </title> <editor> In Michalski, R., Mitchell, T., & Carbonell, J. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: A control-rule is a kind of heuristic that can be used to select the most useful option given the current context of a particular decision. Some systems learn control rules by using inductive (Simliarity Based Learning, or SBL) methods <ref> (Mitchell, Utgoff, & Banerji, 1983) </ref>.
Reference: <author> Mitchell, T. M. </author> <year> (1984). </year> <title> Toward combining empirical and analytic methods for learning heuristics. </title> <editor> In Elithorn, A., & Banerji, R. (Eds.), </editor> <booktitle> Human and Artificial Intelligence. </booktitle> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages>
Reference: <author> Mooney, R. J., & Califf, M. E. </author> <year> (1995). </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <note> in press. </note>
Reference-contexts: Being able to recognize the similarities between words having similar roots or resulting from similar derivations might lead to better generalization. Some initial work along these lines has applied ILP to the problem of learning to form the past tense of English verbs <ref> (Mooney & Califf, 1995) </ref>. At the lexical level, automated techniques for lexicon construction could broaden the applicability of Chill. Thompson (1995) has demonstrated an initial approach to corpus-based acquisition of lexical mapping rules suitable for use with Chill-style parser acquisition systems.
Reference: <author> Muggleton, S. </author> <year> (1992). </year> <title> Inverting implication. </title> <booktitle> In Proceedings of the Second International Workshop on Inductive Logic Programming Tokyo, </booktitle> <address> Japan. </address> <note> 187 Muggleton, </note> <author> S., & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 339-352 Ann Arbor, MI. </address>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy covering algorithm, except that new clauses are hypothesized by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy covering algorithm, except that new clauses are hypothesized by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Muggleton, S., King, R., & Sternberg, M. </author> <year> (1992). </year> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5 (7), </volume> <pages> 647-657. </pages>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy covering algorithm, except that new clauses are hypothesized by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy covering algorithm, except that new clauses are hypothesized by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Ng, H. T. </author> <year> (1988). </year> <title> A computerized prototype natural language tour guide. </title> <type> Tech. rep. </type> <institution> AI88-75, Artificial Intelligence Laboratory, University of Texas, Austin, TX. </institution>
Reference-contexts: Here we are mainly concerned with further evaluation of the ability to learn case-role parsers. A corpus of examples was created by "lifting" a portion of a semantic grammar from an extant prototype natural language database designed to support queries concerning tourist information <ref> (Ng, 1988) </ref>. The portion of the grammar used recognized over 150,000 distinct sentences. A simple case grammar, which produced labelings deemed useful for the database query task, was devised to generate case-structure analyses. The example pair shown in Figure 5.5 illustrates the type of sentences and analyses used.
Reference: <author> Pazzani, M., Brunk, C., & Silverstein, G. </author> <year> (1991). </year> <title> A knowledge-intensive approach to learning relational concepts. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pp. </pages> <address> 432-436 Evanston, IL. </address>
Reference: <author> Periera, F., & Schabes, Y. </author> <year> (1992). </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 128-135 Newark, Delaware. </address>
Reference-contexts: Following in the footsteps of speech recognition research, corpus-based natural language processing has concentrated primarily on statistical techniques (Charniak, 1993) applied to such problems as part-of-speech tagging (Merialdo, 1994; Charniak, Hendrickson, Jacobson, & Perkowitz, 1993) and the induction of stochastic context-free grammars <ref> (Periera & Schabes, 1992) </ref> or transition networks (Miller, Bobrow, Ingria, & Schwartz, 1994). These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods.
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: A successful representative of this class is Muggleton and Feng's Golem (Muggleton & Feng, 1992). Like Foil, Golem may be viewed as a greedy covering algorithm, except that new clauses are hypothesized by considering least-general generalizations (LGGs) of more specific clauses <ref> (Plotkin, 1970) </ref>. The LGG of clauses C 1 and C 2 is the least general clause which subsumes both C 1 and C 2 . An LGG is easily computed by "matching" compatible literals of the clauses; wherever the literals have differing structure, the LGG contains a variable. <p> While the Golem and Foil systems presented in Chapter 2 have certainly been successful, each has its weaknesses. As discussed in section 2.2.3, Golem is based on the construction of relative least-general generalizations, RLGGs <ref> (Plotkin, 1970) </ref> which forces the background knowledge to be expressed extensionally as a set of ground facts. This explicit model of background knowledge can be excessively large, and the clauses constructed from such models can grow explosively.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Volume II, </volume> <pages> pp. 149-166. </pages> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. </pages> <note> 3-20 Vienna. 188 Quinlan, J. </note> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference-contexts: Due to the expressiveness of first-order logic, ILP methods can learn relational and recursive concepts that cannot be represented in the feature-based languages assumed by most machine-learning algorithms. ILP methods have successfully induced small programs for sorting and list manipulation <ref> (Quinlan & Cameron-Jones, 1993) </ref> as well as produced encouraging results on important applications such as predicting protein secondary structure (Muggleton, King, & Sternberg, 1992).
Reference: <author> Reeker, L. H. </author> <year> (1976). </year> <title> The computational study of language acquisition. </title> <editor> In Yovits, M., & Rubinoff, M. (Eds.), </editor> <booktitle> Advances in Computers, </booktitle> <volume> Vol. 15. </volume> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Rouveirol, C. </author> <year> (1992). </year> <title> Extensions of inversion of resolution applied to theory completion. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 63-86. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Selfridge, M. </author> <year> (1981). </year> <title> A computer model of child language acquisition. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 106-108 Vancouver, B.C. </address>
Reference: <author> Sembugamoorthy, V. </author> <year> (1981). </year> <title> A paradigmatic language acquisition system. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 106-108 Vancouver, B.C. </address>
Reference: <author> Siklossy, L. </author> <year> (1972). </year> <title> Natural language learning by computer. </title> <editor> In Simon, H. A., & Siklossy, L. (Eds.), </editor> <title> Representation and meaning: Experiments with Information Processsing Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Simmons, R. F., & Yu, Y. </author> <year> (1992). </year> <title> The acquisition and use of context dependent grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18 (4), </volume> <pages> 391-418. </pages>
Reference: <author> Soderland, S., & Lehnert, W. </author> <year> (1994). </year> <title> Corpus-driven knowledge acquisition for discourse analysis. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference-contexts: Although the technique has only been used with case-role type representations, variations might also be useful for the type of lexicon required by the database-query task. ILP techniques might also be usefully applied in learning larger discourse structures (Litman, 1994) and in information extraction tasks <ref> (Soderland & Lehnert, 1994) </ref>. Larger discourse units might be described in terms of scripts in a suitable logic-oriented MRL. ILP could then be used to learn rules for script-selection and role-binding using techniques similar to those used in the database-query framework in Chill.
Reference: <author> Solomonoff, R. </author> <year> (1959). </year> <title> A new method for discovering the grammars of phrase structure languages. </title> <booktitle> In Proceedings of the International Conference on Information Processing. 189 St. </booktitle> <editor> John, M. F., & McClelland, J. L. </editor> <year> (1990). </year> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 217-257. </pages>
Reference: <author> Stahl, I., Tausend, B., & Wirth, R. </author> <year> (1993). </year> <title> Two methods for improving inductive logic programming systems. </title> <booktitle> In Machine Learning: ECML-93, </booktitle> <pages> pp. 41-55 Vienna. </pages>
Reference-contexts: Research in the area ILP has been expanding rapidly, and many other ILP systems have addressed issues of concern in Chillin. Like Chillin, Series (Wirth & O'Rorke, 1991) and, later Indico <ref> (Stahl, Tausend, & Wirth, 1993) </ref> make use of LGGs of examples to construct clause heads containing functions. However, both of these systems pre-compute a set of clause heads for which bodies are subsequently induced.
Reference: <author> Thompson, C. A. </author> <year> (1995). </year> <title> Acquisition of a lexicon from semantic representations of sentences. </title> <booktitle> In Proceeding of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 335-337 Boston, MA. </address>
Reference: <author> Tomita, M. </author> <year> (1986). </year> <title> Efficient Parsing for Natural Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> VanLehn, K., & Ball, W. </author> <year> (1987). </year> <title> A version space approach to learning context-free grammars. </title> <journal> Machine Learning, </journal> <volume> 2 (1), </volume> <pages> 39-74. </pages>
Reference: <author> Warren, D. H. D., & Pereira, F. C. N. </author> <year> (1982). </year> <title> An efficient easily adaptable system for interpreting natural language queries. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 8 (3-4), </volume> <pages> 110-122. </pages>
Reference-contexts: Domain independent optimization methods allow the design of the query language to be based on expressiveness and simplicity, rather than worrying about implementation issues. This is the approach 98 taken in CHAT-80 <ref> (Warren & Pereira, 1982) </ref>. 7.4 A Parsing Framework for Logical Queries Although the logical representations of Geoquery look very different from parse-trees or case-structures, they are amenable to the same general parsing scheme as that used for the shallower representations.
Reference: <author> Wirth, R., & O'Rorke, P. </author> <year> (1991). </year> <title> Constraints on predicate invention. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pp. </pages> <address> 457-461 Evanston, IL. </address>
Reference: <author> Wirth, R. </author> <year> (1988). </year> <title> Learning by failure to prove. </title> <booktitle> In Proceedings of EWSL 88, </booktitle> <pages> pp. 237-51. </pages> <publisher> Pitman. </publisher>
Reference: <author> Wirth, R. </author> <year> (1989). </year> <title> Completing logic programs by inverse resolution. </title> <booktitle> In Proceedings of the European Working Session on Learning, </booktitle> <pages> pp. </pages> <address> 239-250 Montpelier, France. </address> <publisher> Pitman. </publisher>
Reference: <author> Wolff, J. G. </author> <year> (1982). </year> <title> Language acquisition, data compression, and generalization. </title> <journal> Language and Communication, </journal> <volume> 2, </volume> <pages> 57-89. </pages> <note> 190 Woods, </note> <author> W. A. </author> <year> (1970). </year> <title> Transition network grammars for natural language analysis. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 13, </volume> <pages> 591-606. </pages>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993a). </year> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, </booktitle> <pages> pp. </pages> <address> 1106-1111 Chambery, France. </address>
Reference-contexts: Prolog always applies the first clause and only revises that choice upon backtracking caused by the final list not meeting the ordered/1 condition. The Dolphin system <ref> (Zelle & Mooney, 1993a) </ref> can automatically transform this O (n!) sorting algorithm into one which runs in O (n 2 ) time by learning from a single top-level example.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993b). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 817-822 Washington, D.C. </address>
Reference-contexts: This chapter discusses some of the experiments which have been conducted with Chill for this type of representation, some of which were first reported in <ref> (Zelle & Mooney, 1993b) </ref>. 5.1 Background Semantic case analyses have proven very useful in the construction of natural language systems (Allen, 1995). However, the construction of parsers to produce such representations is complicated by the need to use domain-specific semantic information to resolve ambiguity and produce accurate analyses.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994a). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning New Brunswick, </booktitle> <address> NJ. </address>
Reference-contexts: At the time Chill was being developed, no existing ILP system combined all of the necessary features. The control-rule induction component of Chill employs a new induction algorithm called Chillin 1 <ref> (Zelle & Mooney, 1994a) </ref> for learning relational concept definitions. While the Golem and Foil systems presented in Chapter 2 have certainly been successful, each has its weaknesses.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994b). </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 748-753 Seattle, WA. </address> <month> 191 </month>
Reference-contexts: By first computing the correct sequence of parse states, it is then possible to "simulate" the action of the shift-reduce parser deterministically, insuring that each operator application produces the correct next state in the sequence. 6.2 Intial Experiments Intial experiments (as reported in <ref> (Zelle & Mooney, 1994b) </ref>) were actually carried out on four different variations of the corpus. A subset of the corpus comprising sentences 77 of length less than 13 words was used to form a more tractable corpus for systematic evaluation and to test the effect of sentence length on performance.
References-found: 80

