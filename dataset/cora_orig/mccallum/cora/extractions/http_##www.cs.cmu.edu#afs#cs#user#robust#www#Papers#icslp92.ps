URL: http://www.cs.cmu.edu/afs/cs/user/robust/www/Papers/icslp92.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/robust/www/Papers/
Root-URL: 
Title: MULTIPLE APPROACHES TO ROBUST SPEECH RECOGNITION compare the effectiveness of three complementary maximal robustness with
Author: Richard M. Stern, Fu-Hua Liu, Yoshiaki Ohshima, Thomas M. Sullivan, Alejandro Acero 
Note: also describe and  physiologically-motivated  
Address: Pittsburgh, PA 15213  
Affiliation: Department of Electrical and Computer Engineering School of Computer Science Carnegie Mellon University  
Abstract: 2. ACOUSTICAL PRE-PROCESSING This paper compares several different approaches to robust speech We have found that two major factors degrading the performance of recognition. We review CMU's ongoing research in the use of speech recognition systems using desktop microphones in normal cluding the first evaluation of pre-processing in the context of the We showed in [2, 6] that simultaneous joint compensation for the DARPA standard ATIS domain for spoken language systems. We effects of additive noise and linear filtering is needed to achieve pre-processing, microphone array processing, and the use of We described in [2, 6] two algorithms that perform such joint com acoustical pre-processing to achieve robust speech recognition, in- office environments are additive noise and unknown linear filtering.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Juang, B. H., </author> <title> ``Speech Recognition in Adverse Envoronments'', </title> <journal> Comp. Speech and Lang., </journal> <volume> Vol. 5, </volume> <year> 1991, </year> <pages> pp. 275-294. </pages>
Reference-contexts: The need for speech recognition systems and spoken language sys tems to be robust with respect to their acoustical environment has More recently we developed several new algorithms which combine become more widely appreciated in recent years (e.g. <ref> [1] </ref>). the environmental independence of CDCN with greater computa tional simplicity. One such algorithm is the Blind SNR-Dependent Results of several studies have demonstrated that even automatic Cepstral Normalization (BSDCN) algorithm [5].
Reference: 2. <author> Acero, A. and Stern, R. M., </author> <title> ``Environmental Robustness in Automatic Speech Recognition'', </title> <address> ICASSP-90, </address> <month> April </month> <year> 1990, </year> <pages> pp. 849-852. </pages>
Reference-contexts: A correspondence is established between the SNRs in with which they were trained (e.g. <ref> [2, 3] </ref>), even in a relatively quiet the training and testing environments by use of traditional nonlinear office environment. The use of microphones other than a "close warping technique on histograms of SNRs from each of the two talking" headset also tends to severely degrades speech recognition environments. <p> While most of our work presented to date PZM6FS microphones in the context of a speaker-independent has described new acoustical pre-processing algorithms (e.g. continuous-speech alphanumeric task with perplexity 65, recorded in <ref> [2, 4, 5] </ref>, we have always regarded pre-processing as one of several an office environment [2]. Our previous analyses of this database approaches that must be developed in concert to achieve robust recog indicate that the PZM6FS data are corrupted by linear filtering as well nition. as by additive noise. <p> While most of our work presented to date PZM6FS microphones in the context of a speaker-independent has described new acoustical pre-processing algorithms (e.g. continuous-speech alphanumeric task with perplexity 65, recorded in [2, 4, 5], we have always regarded pre-processing as one of several an office environment <ref> [2] </ref>. Our previous analyses of this database approaches that must be developed in concert to achieve robust recog indicate that the PZM6FS data are corrupted by linear filtering as well nition. as by additive noise.
Reference: 3. <author> Erell, A. and Weintraub, M., </author> <title> ``Estimation Using Log-Spectral Distance Criterion for Noise-Robust Speech Recognition'', </title> <address> ICASSP-90, </address> <month> April </month> <year> 1990, </year> <pages> pp. 853-856. </pages>
Reference-contexts: A correspondence is established between the SNRs in with which they were trained (e.g. <ref> [2, 3] </ref>), even in a relatively quiet the training and testing environments by use of traditional nonlinear office environment. The use of microphones other than a "close warping technique on histograms of SNRs from each of the two talking" headset also tends to severely degrades speech recognition environments.
Reference: 4. <author> Acero, A. and Stern, R. M., </author> <title> ``Robust Speech Recognition by Nor malization of the Acoustic Space'', </title> <address> ICASSP-91, </address> <month> May </month> <year> 1991, </year> <pages> pp. 893-896. </pages>
Reference-contexts: While most of our work presented to date PZM6FS microphones in the context of a speaker-independent has described new acoustical pre-processing algorithms (e.g. continuous-speech alphanumeric task with perplexity 65, recorded in <ref> [2, 4, 5] </ref>, we have always regarded pre-processing as one of several an office environment [2]. Our previous analyses of this database approaches that must be developed in concert to achieve robust recog indicate that the PZM6FS data are corrupted by linear filtering as well nition. as by additive noise.
Reference: 5. <author> Liu, F.-H., Acero, A., and Stern, R. M., </author> <title> ``Efficient Joint Compensation of Speech for the Effects of Additive Noise and Linear Filtering'', </title> <address> ICASSP-92, </address> <month> March </month> <year> 1992, </year> <pages> pp. 865-868. </pages>
Reference-contexts: One such algorithm is the Blind SNR-Dependent Results of several studies have demonstrated that even automatic Cepstral Normalization (BSDCN) algorithm <ref> [5] </ref>. <p> While most of our work presented to date PZM6FS microphones in the context of a speaker-independent has described new acoustical pre-processing algorithms (e.g. continuous-speech alphanumeric task with perplexity 65, recorded in <ref> [2, 4, 5] </ref>, we have always regarded pre-processing as one of several an office environment [2]. Our previous analyses of this database approaches that must be developed in concert to achieve robust recog indicate that the PZM6FS data are corrupted by linear filtering as well nition. as by additive noise. <p> First-order gradient microphonesaccuracy using CDCN converges with only 2 seconds of speech in the are used, which develop a null response in the vertical plane. We test environment for best performance, while BSDCN requires about compared the recognition accuracy on the census task obtained using70 seconds of speech <ref> [5] </ref>.
Reference: 6. <author> Acero, A. and Stern, R. M., </author> <title> ``Acoustical Pre-Processing for Robust using the conventional LPC-based processing of SPHINX with results Spoken Language Systems'', </title> <address> ICSLP-90, </address> <month> November </month> <year> 1990, </year> <title> pp. obtained using the mean rate and synchrony outputs of the Seneff 1121-1124. auditory model. SPHINX was trained on the CLSTLK microphone in all </title>

Reference: 9. <author> Widrow, B., and Stearns, S. D., </author> <title> Adaptive Signal Processing, Prentice-using the auditory model could further improve if greater attention Hall, </title> <address> Englewood Cliffs, NJ, </address> <year> 1985. </year> <title> were paid to tuning it to more closely match the characteristics of </title>
Reference-contexts: A second Test CLSTLK Test PZM6FS option is to use an adaptation algorithm based on minimizing mean square energy such as the Frost or Griffiths-Jim algorithm <ref> [9] </ref>. These algorithms can provide nulls in the direction of noise sources as well as more sharply focused beam patterns, but they assume that the desired signal is statistically independent of all sources of degradation.
Reference: 10. <author> Peterson, P. M., </author> <title> ``Adaptive Array Processing for Multiple Microphone SPHINX. Hearing Aids''. </title> <type> RLE TR No. 541, </type> <institution> Res. Lab. of Electronics, MIT, </institution> <address> Cambridge, MA </address>
Reference-contexts: Consequently, these algorithms can provide good improvement in SNR when signal degradations are caused by additive independent noise sources, but they do not perform well in reverberant environments when the distortion is at least in part a delayed version of the desired speech signal <ref> [10, 11] </ref>. (This problem can be avoided by only adapting during non-speech segments [12]).

Reference: 15. <author> Ghitza, O., </author> <title> ``Auditory Nerve Representation as a Front-End for Speech speech system in reverberant as well as in noisy environments, results Recognition in a Noisy Environment'', Comp. Speech and Lang., Vol. were no better than those obtained with CDCN alone. </title> <type> 1, </type> <year> 1986, </year> <pages> pp. 109-130. </pages>
Reference-contexts: More interestingly, the results in Fig. 2 show that both the method described in <ref> [15] </ref>. <p> We have also explored several ways of combining auditory physiology and perception, and a number of such schemes CDCN with the outputs of the auditory model, but so far we have not have been proposed (e.g. <ref> [14, 15, 16, 17] </ref>). Most auditory models been able to obtain a better error rate than the error rate observed include a set of linear bandpass filters with bandwidth that increases using CDCN alone.
Reference: 16. <author> Lyon, R. F., </author> <title> ``A Computational Model of Filtering, Detection, and Compression in the Cochlea'', </title> <address> ICASSP-82, </address> <month> May </month> <year> 1982, </year> <pages> pp. 1282-1285. </pages> <month> ACKNOWLEDGMENTS </month>
Reference-contexts: We have also explored several ways of combining auditory physiology and perception, and a number of such schemes CDCN with the outputs of the auditory model, but so far we have not have been proposed (e.g. <ref> [14, 15, 16, 17] </ref>). Most auditory models been able to obtain a better error rate than the error rate observed include a set of linear bandpass filters with bandwidth that increases using CDCN alone.
Reference: 17. <author> Seneff, S., </author> <title> ``A Joint Synchrony/Mean-Rate Model of Auditory Speech Processing'', </title> <journal> Journal of Phonetics, </journal> <volume> Vol. 16, No. 1, </volume> <month> January </month> <year> 1988, </year> <pages> pp. </pages> <note> This research is sponsored by the Defense Advanced Research 55-76. Projects Agency, DoD, through ARPA Order 7239, and monitored by </note>
Reference-contexts: Data were obtained from simultaneous recordings using the three microphones at distances of 1 and 3 meters (for the PZM6FS Pilot evalution of the Seneff auditory model. We recently com and the array). pleted a series of pilot evaluations using an implementation of the Seneff auditory model <ref> [17] </ref> and the census database. <p> cepstral coefficients normally input to the SPHINX system, with and samples were obtained at each distance and because the sample size is without CDCN, with those obtained using an implementation of the small. (It is also possible that speakers may have spoken less 40-channel mean-rate output of the Seneff model <ref> [17] </ref>, and with the naturally when the recording microphone is placed at the 3-meter 40-channel outputs of Seneff's Generalized Synchrony Detectors distance.) The SPHINX system had been previously trained on speech (GSDs). The system was evaluated using the original testing obtained using the CLSTLK microphone. <p> We have also explored several ways of combining auditory physiology and perception, and a number of such schemes CDCN with the outputs of the auditory model, but so far we have not have been proposed (e.g. <ref> [14, 15, 16, 17] </ref>). Most auditory models been able to obtain a better error rate than the error rate observed include a set of linear bandpass filters with bandwidth that increases using CDCN alone.
Reference: 18. <author> Hunt, M., </author> <title> ``A Comparison of Several Acoustic Representations for the Space and Naval Warfare Systems Command under contract Speech Recognition with Degraded and Undegraded Speech'', </title> <journal> N00039-91-C-0158. </journal> <note> Views and conclusions contained in this docu-ICASSP, </note> <month> May </month> <year> 1989. </year> <title> ment are those of the authors and should not be interpreted as </title>
Reference-contexts: accuracy that is comparable to that obtained with conventional LPC-based or DFT-based signal processing schemes, but that these auditory models can provide greater robustness with respect to environmental changes when the quality of the incoming speech (or the extent to which it resembles speech used in training the system) decreases <ref> [18, 19] </ref>. Despite the apparent utility of such processing schemes, no one has a deep-level understanding of why they work as well as they do, and in fact different researchers choose to emphasize rather different aspects of the peripheral auditory system's response to sound in their work.
Reference: 19. <author> Meng, H., and Zue, V. W., </author> <title> ``A Comparative Study of Acoustic representing official policies, either expressed or implied, of the Representations of Speech for Vowel Classification Using Multi-Layer Defense Advanced Research Projects Agency or of the United States Perceptrons'', </title> <address> ICSLP-90, </address> <month> November </month> <year> 1990, </year> <pages> pp. 1053-1056. </pages> <note> Government. </note> <author> We thank Hsiao-Wuen Hon, Xuedong Huang, Kai-Fu Lee, Raj Reddy, Eric Thayer, Bob Weide, </author> <title> and the rest of the speech group for their contributions to this work. </title> <note> We also thank Jim </note>
Reference-contexts: accuracy that is comparable to that obtained with conventional LPC-based or DFT-based signal processing schemes, but that these auditory models can provide greater robustness with respect to environmental changes when the quality of the incoming speech (or the extent to which it resembles speech used in training the system) decreases <ref> [18, 19] </ref>. Despite the apparent utility of such processing schemes, no one has a deep-level understanding of why they work as well as they do, and in fact different researchers choose to emphasize rather different aspects of the peripheral auditory system's response to sound in their work.
References-found: 13

