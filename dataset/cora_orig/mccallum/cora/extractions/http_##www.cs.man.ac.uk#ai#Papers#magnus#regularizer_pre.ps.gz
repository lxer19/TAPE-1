URL: http://www.cs.man.ac.uk/ai/Papers/magnus/regularizer_pre.ps.gz
Refering-URL: http://www.cs.man.ac.uk/~magnus/magnus.html
Root-URL: http://www.cs.man.ac.uk
Title: Learning with Regularizers in Multi-layer Neural Networks  
Author: David Saad and Magnus Rattray 
Address: B4 7ET, UK.  
Affiliation: Aston University, Computer Science Applied Mathematics, Birmingham,  
Abstract: We study the effect of regularization in an on-line gradient-descent learning scenario for a general two-layer student network with an arbitrary number of hidden units. Training examples are randomly drawn input vectors labelled by a two-layer teacher network with an arbitrary number of hidden units which may be corrupted by Gaussian output noise. We examine the effect of weight decay regularization on the dynamical evolution of the order parameters and generalization error in various phases of the learning process, in both noiseless and noisy scenarios. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Biehl and H. Schwarze, J. </author> <note> Phys. A 28, 643 (1995). </note>
Reference: [2] <author> D. Saad and S.A. </author> <title> Solla, </title> <journal> Phys. Rev. </journal> <volume> E 52, </volume> <month> 4225 </month> <year> (1995). </year>
Reference: [3] <author> D. Saad and S.A. Solla, </author> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> edited by D. S. Touretzky, M. C. Mozer and M. E. </editor> <publisher> Hasselmo (MIT Press, </publisher> <address> Cambridge MA, </address> <note> 1996) Vol. 8, p. 302. </note>
Reference: [4] <author> D. Saad and S.A. Solla, </author> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> edited by M. C. Mozer, M. I. Jordan and T. </editor> <publisher> Petsche (MIT press, </publisher> <address> Cambridge MA, </address> <note> 1997) Vol. 9, p. 260. </note>
Reference-contexts: To demonstrate the effect of weight decay on the evolution of the generalization error 8 in the case of corrupted examples and in the presence of redundant parameters, we show in Fig. 4 two typical training scenarios where weight decay has been applied. We consider additive Gaussian output noise <ref> [4] </ref> so that the teacher output is i = ae + P K where the random variable ae is taken to be Gaussian with zero mean and variance oe 2 .
Reference: [5] <author> P. Riegler and M. Biehl, J. </author> <note> Phys. A 28, L507 (1995). </note>
Reference: [6] <author> G. Cybenko, </author> <title> Math. </title> <booktitle> Control Signals and Systems 2, </booktitle> <month> 303 </month> <year> (1989). </year>
Reference: [7] <author> C.M. Bishop, </author> <title> Neural Networks for Pattern Recognition, </title> <publisher> (Oxford University Press, Oxford, </publisher> <year> 1995). </year>
Reference: [8] <author> T.L.H. Watkin, A. Rau, and M. Biehl, </author> <title> Rev. Mod. </title> <journal> Phys. </journal> <volume> 65, </volume> <month> 499 </month> <year> (1993). </year>
Reference: [9] <author> S. Amari, N. Murata, K. R. Muller, M. Finke and H. Yang, </author> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> edited by D. S. Touretzky, M. C. Mozer and M. E. </editor> <publisher> Hasselmo (MIT Press, </publisher> <address> Cambridge MA, </address> <note> 1996) Vol. 8, p. 176. </note>
Reference: [10] <author> S. J. Hanson and L. Y. Pratt, </author> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> edited by D. S. </editor> <publisher> Touretzky (Morgan Kaufmann Publishers, </publisher> <address> Palo-Alto, U.S.A., </address> <note> 1988) Vol. 1, p. 177. </note>
Reference: [11] <author> A. Krogh and J. A. Hertz J Phys. </author> <note> A 25, 1135 (1992). </note>
Reference: [12] <author> A. D. Bruce and D. Saad, </author> <title> J Phys. A 27, </title> <month> 3355 </month> <year> (1994). </year>
Reference: [13] <author> S. A. Solla, </author> <title> in Neural Networks for Signal Processing II, edited by S. </title> <editor> Y. Kung, F. </editor> <title> Fall 14 side, </title> <editor> J. Aa. Sorenson and C. A. </editor> <publisher> Kamm (IEEE, </publisher> <address> NJ, U.S.A., 1992) p. </address> <month> 255. </month>
Reference-contexts: Noisy examples and redundant parameters From the analysis of the role played by the weight decay in the linear perceptron one would expect the weight decay to alleviate the problem of noise [11,12] and to suppress redundant parameters <ref> [13] </ref>, reducing the generalization error. We therefore examined the effect of weight decay on various learning scenarios in which training examples are corrupted by noise and in the presence of redundant weights, for small and intermediate learning rates.
Reference: [14] <author> D. Saad and M. </author> <title> Rattray, </title> <journal> Phys. Rev. Lett. </journal> <volume> 79, </volume> <month> 2578 </month> <year> (1997). </year>
Reference-contexts: In this section we take a different approach, aiming at global optimization of a time-dependent weight decay term on the basis of previous work on globally optimal learning rates <ref> [14] </ref> and learning rules [15]. <p> The learning rate chosen in this example (j = 0:7) is close to the optimal value in the absence of weight decay (as determined by similar methods to those employed here for the determination of fl opt <ref> [14] </ref>) and we therefore see that the inclusion of weight decay can result in an improvement on the optimal performance of standard gradient descent learning. <p> rate is given its optimal time-dependent value in the absence of weight decay (shown by the dotted line in Fig. 6 (a)), which is initially constant at j ' 1:6 until a decay towards the end of the given time-window as required for the system to achieve optimal asymptotic performance <ref> [14] </ref>. As in the previous example, Fig. 6 (b) shows a significant shortening of the symmetric phase when compared to learning without weight decay. <p> In order to determine the behaviour for arbitrary learning rates we employ recent methods for determining optimal time-dependent parameters over a fixed time window <ref> [14] </ref>. For small 12 learning rates we find results consistent with the above discussion: the optimal weight decay parameter is very small and mostly negative during the symmetric phase, for realizable, over-realizable and noisy learning scenarios.
Reference: [15] <author> M. Rattray and D. Saad, </author> <title> J Phys. A 30, </title> <month> L771 </month> <year> (1997). </year>
Reference-contexts: In this section we take a different approach, aiming at global optimization of a time-dependent weight decay term on the basis of previous work on globally optimal learning rates [14] and learning rules <ref> [15] </ref>.

References-found: 15

