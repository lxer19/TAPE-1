URL: http://www.cs.berkeley.edu/~dfb/papers/masters.ps
Refering-URL: http://www.cs.berkeley.edu/~dfb/publist.html
Root-URL: 
Title: Fallacies of the Multiprocessor Approach to Achieving Large Scale Computing Capabilities: A Case Study  
Author: by David F. Bacon 
Degree: 1985 Committee in charge: Professor Susan L. Graham Professor James Demmel  
Date: 1995  
Affiliation: A.B. (Columbia University)  
Abstract: A research project report submitted in partial satisfaction of the requirements for the degree of Master of Science, Plan II in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aho, A. V., Sethi, R., and Ullman, J. D. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachussetts, </address> <year> 1986. </year>
Reference-contexts: machines by first determining the best loop nest for locality, unrolling any short inner loops, and then determining the best loop nest for vector length of the remaining outer loops. 3.6.3 Strength Reduction of Exponentials The conversion of x 2:5 to p xfix 2 is a straightforward reduction in strength <ref> [1] </ref> which yields a significant increase in performance due to the high cost of general exponentials. On 17 the Cray, a general exponential is about 20 times slower than a multiply, and significantly slower than a square root.
Reference: [2] <author> Amdahl, G. M. </author> <title> Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> In Proceedings of the AFIPS 1967 Spring Joint Computer Conference (Atlantic City, </booktitle> <address> New Jersey, </address> <month> Apr. </month> <journal> 1967), </journal> <volume> vol. 30, </volume> <pages> pp. 483-485. </pages>
Reference-contexts: While the papers by the proponents of vector, array-processor, and associative memory architecture have long been forgotten, Gene Amdahl's argument in Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities has since become famous as Amdahl's Law <ref> [2] </ref>: If a sequential program with running time T of which F s is the sequential fraction and F p = 1 F s is the parallel fraction is run on a machine with parallelism P then the potential speedup through parallelism S is bounded by S F s + P
Reference: [3] <author> Bacon, D. F., Graham, S. L., and Sharp, O. J. </author> <title> Compiler transformations for high-performance computing. </title> <journal> ACM Comput. Surv. </journal> <volume> 26, </volume> <month> 4 (Dec. </month> <year> 1994). </year>
Reference-contexts: Most of the transformations we performed can be expressed as combinations of traditional program transformations: constant propagation, loop interchange, loop distribution, loop unrolling, inlining, strength reduction, and strip-mining <ref> [3] </ref>.
Reference: [4] <author> Bailey, D. H., Barszcz, E., Barton, J. T., Browning, D. S., Carter, R. L., Dagum, L., Fatoohi, R. A., Frederickson, P. O., Lasinski, T. A., Schreiber, R. S., Simon, H. D., Venkatakrishnan, V., and Weeratunga, S. K. </author> <title> The NAS parallel benchmarks. </title> <booktitle> International Journal of Supercomputer Applications 5, 3 (Fall 1991), </booktitle> <pages> 63-73. </pages>
Reference-contexts: Such programs are quite unlike those developed by computer scientists; it is therefore necessary for the computer science community to cultivate an understanding of a body of programs developed by a different community. While several suites of parallel applications have been collected for study by computer scientists <ref> [21, 4, 7] </ref>, many of the applications are simplified versions of the actual production codes. The parallel versions of these codes usually are not used by the community from which they were drawn.
Reference: [5] <author> Balasundaram, V. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: the Data Access Descriptor. </title> <journal> Journal of Parallel and Distributed Computing 9, </journal> <month> 2 (June </month> <year> 1990), </year> <pages> 154-170. </pages>
Reference-contexts: that adding the r; dimensions to the arrays a; b; c might eliminate cross-iteration dependencies; no automated techniques exist to perform such analysis. * proving the absence of cross iteration dependencies, which entails doing dependency tracking at the level of array sub-regions using regular sections [8] or data access descriptors <ref> [5] </ref>. * deciding that these two loops, when distributed and pushed into the next level procedures, will yield significant speedups.
Reference: [6] <author> Banerjee, U. </author> <title> An introduction to a formal theory of dependence analysis. </title> <journal> Journal of Supercomputing 2, </journal> <month> 2 (Oct. </month> <year> 1988), </year> <pages> 133-149. </pages>
Reference-contexts: Cotten in the Spring 1969 Joint Computer Conference. Almost twenty years later, Uptal Banerjee made the statement that "the parallel processing era started with the realization that sequential computers were reaching their 32 limit because of restrictions imposed by the laws of physics" <ref> [6] </ref>. While some of the speed increases have certainly come from low-level instruction parallelism, the raw speed of the hardware has also continued to increase.
Reference: [7] <author> Berry, M., Chen, D., Koss, P., Kuck, D., Lo, S., Pang, Y., Pointer, L., Roloff, R., Sameh, A., Clementi, E., Chin, S., Schneider, D., Fox, G., Messina, P., Walker, D., Hsiung, C., Schwarzmeier, J., Lue, K., Orszag, S., Seidl, F., Johnson, O., Goodrum, R., and Martin, J. </author> <title> The Perfect Club 39 benchmarks: Effective performance evaluation of supercomputers. </title> <booktitle> International Journal of Supercomputer Applications 3, 3 (Fall 1989), </booktitle> <pages> 5-40. </pages>
Reference-contexts: Such programs are quite unlike those developed by computer scientists; it is therefore necessary for the computer science community to cultivate an understanding of a body of programs developed by a different community. While several suites of parallel applications have been collected for study by computer scientists <ref> [21, 4, 7] </ref>, many of the applications are simplified versions of the actual production codes. The parallel versions of these codes usually are not used by the community from which they were drawn.
Reference: [8] <author> Callahan, D. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Rice University, </institution> <month> Apr. </month> <year> 1987. </year> <type> Technical Report 87-50. </type>
Reference-contexts: analyzing the code to determine that adding the r; dimensions to the arrays a; b; c might eliminate cross-iteration dependencies; no automated techniques exist to perform such analysis. * proving the absence of cross iteration dependencies, which entails doing dependency tracking at the level of array sub-regions using regular sections <ref> [8] </ref> or data access descriptors [5]. * deciding that these two loops, when distributed and pushed into the next level procedures, will yield significant speedups.
Reference: [9] <author> Callahan, D., Cooper, K., Kennedy, K., and Torczon, L. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Compiler Construction (Palo Alto, </booktitle> <address> California, June 1986), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 152-161. </pages>
Reference-contexts: This step might conceivably be automatable, but currently available techniques are not sufficiently powerful. * as part of the above decision, finding the loop bounds, either by interprocedural constant propagation <ref> [9] </ref> or by run-time profiling. 13 Phase Optimization Application Cum. Appl.
Reference: [10] <author> Feautrier, P. </author> <title> Array expansion. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing (St. </booktitle> <address> Malo, France, July 1988), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 429-441. </pages>
Reference-contexts: Once these steps have been performed, the transformation can be performed using array expansion <ref> [10] </ref>, loop distribution [15], and either inlining or movement of loops into called procedures [23, 17].
Reference: [11] <author> Gustafson, J. L. </author> <title> Reevaluating Amdahl's law. </title> <journal> Commun. ACM 31, </journal> <month> 5 (May </month> <year> 1988), </year> <pages> 532-533. </pages>
Reference-contexts: The usual answer to this problem is that scientists are always eager to increase resolution, so that the grid size can simply be increased to the point where each processor can be utilized efficiently and the serial portions of the code can be made insignificant | thereby cheating Amdahl's law <ref> [11] </ref>. Unfortunately the primary scientific concern at this stage was to simulate enough time steps to reach steady-state behavior, so that while increased grid size was of interest, it was a secondary issue. <p> processor machine, and a maximum potential speedup of 250. 5.2 Fallacy: Grid Size is the Limiting Factor In recent years, a new dogma has made its way into the high-performance computing community: that Amdahl's Law does not strictly apply to massively parallel machines, because it assumes a constant problem size <ref> [11] </ref>. In fact, so the argument goes, as machines are scaled up, numerically intensive applications will be scaled up as well so that they are always running at the limit of the machine capacity to provide the best results possible.
Reference: [12] <author> Hennessy, J. L., and Patterson, D. A. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: As a result, it was possible to use a body of programs drawn largely from within the computer science community to provide the basis for quantitative study of compiler technology and architecture decisions <ref> [12] </ref>. For parallel programs, however, this is not the case. The class of programs for which parallelization is important tends to come from the engineering and scientific domain, in particular physical simulation over time.
Reference: [13] <author> Hillis, D. W. </author> <title> The Connection Machine. </title> <publisher> ACM Distinguished Dissertation. MIT Press, </publisher> <address> Cambridge, Massachussetts, </address> <year> 1985. </year>
Reference-contexts: Now the sword has hung there an additional ten years, and it still shows no signs of crashing down upon us. Danny Hillis made a forceful argument for massively parallel machines with relatively slow individual processors <ref> [13] </ref> The result was the CM-2, which was quite successful for a substantial set of applications whose fundamental structure allowed them to be decomposed into many small processes.
Reference: [14] <author> Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., and Tseng, C. W. </author> <title> An overview of the Fortran D Programming System. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing (Santa Clara, </booktitle> <address> California, </address> <month> Aug. </month> <year> 1991), </year> <editor> U. Banerjee, D. Gelernter, A. Nicolau, and D. A. Padua, Eds., </editor> <volume> vol. </volume> <booktitle> 589 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <pages> pp. 18-34. </pages>
Reference-contexts: Such an approach is clearly viable if an expert on supercomputing tunes the application, but it it less likely to work well for the majority of developers. While it has been claimed that vectorization is a solved problem provided that application developers use a "vectorizable style" <ref> [14] </ref>, our experience shows that the interplay of factors that defeat vectorization is sometimes too complex to require an application programmer to understand. Sometimes "vectorizable style" means "coding to the quirks of the compiler internals".
Reference: [15] <author> Kuck, D. J., Kuhn, R. H., Padua, D., Leasure, B., and Wolfe, M. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on Principles of Programming Languages (POPL) (Williamsburg, </booktitle> <address> Virginia, Jan. 1981), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 207-218. </pages>
Reference-contexts: Once these steps have been performed, the transformation can be performed using array expansion [10], loop distribution <ref> [15] </ref>, and either inlining or movement of loops into called procedures [23, 17].
Reference: [16] <author> Lam, M. S., Rothberg, E. E., and Wolf, M. E. </author> <title> The cache performance and optimization of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference 40 on Architectural Support for Programming Languages and Operating Systems (Santa Clara, </booktitle> <address> California, </address> <month> Apr. </month> <title> 1991), </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 63-74. </pages>
Reference-contexts: The main benefit was that many values were loaded by more than one of the statements of the unrolled loop, so that data movement was decreased. While a number of recent compilers have used sophisticated methods to pick the best loop nesting to optimize data re-use in the cache <ref> [20, 16] </ref>, this has not been combined with a consideration of unrolling as a method to create scalar re-use in the inner loop.
Reference: [17] <author> Li, Z., and Yew, P.-C. </author> <title> Efficient interprocedural analysis for program parallelization and restructuring. </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS Symposium on Parallel Programming: Experience with Applications, Languages, </booktitle> <institution> and Systems (New Haven, </institution> <address> Connecticut, Sept. 1988), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 85-99. </pages>
Reference-contexts: Once these steps have been performed, the transformation can be performed using array expansion [10], loop distribution [15], and either inlining or movement of loops into called procedures <ref> [23, 17] </ref>.
Reference: [18] <author> Lucco, S. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation (PLDI) (San Francisco, </booktitle> <address> California, June 1992), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 200-211. </pages>
Reference-contexts: We also found that the most profitable optimizations could not be performed using state of the art compiler technology, even though they are clearly theoretically automatable. The experience of other members of our group with other applications is also instructive <ref> [18, 19] </ref>. They found that by using a manually-guided transformation system in concert with a sophisticated run-time environment, it was possible to get substantial speed-ups. However, applying the manually-guided transformations required a detailed understanding of the code and the way it mapped to the target architecture.
Reference: [19] <author> Lucco, S., and Nichols, K. </author> <title> A performance analysis of two parallel programming methodologies in the context of mos timing simulation. </title> <booktitle> In Digest of Papers: IEEE Compcon (1987), </booktitle> <pages> pp. 205-210. </pages>
Reference-contexts: We also found that the most profitable optimizations could not be performed using state of the art compiler technology, even though they are clearly theoretically automatable. The experience of other members of our group with other applications is also instructive <ref> [18, 19] </ref>. They found that by using a manually-guided transformation system in concert with a sophisticated run-time environment, it was possible to get substantial speed-ups. However, applying the manually-guided transformations required a detailed understanding of the code and the way it mapped to the target architecture.
Reference: [20] <author> Sarkar, V., and Thekkath, R. </author> <title> A general framework for iteration-reordering transformations. </title> <booktitle> In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation (PLDI) (San Francisco, </booktitle> <address> California, June 1992), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 175-187. </pages>
Reference-contexts: The main benefit was that many values were loaded by more than one of the statements of the unrolled loop, so that data movement was decreased. While a number of recent compilers have used sophisticated methods to pick the best loop nesting to optimize data re-use in the cache <ref> [20, 16] </ref>, this has not been combined with a consideration of unrolling as a method to create scalar re-use in the inner loop.
Reference: [21] <author> Singh, J. P., Weber, W.-D., and Gupta, A. </author> <title> SPLASH: Stanford parallel applications for shared memory. SIGARCH Computer Architecture News 20, </title> <month> 1 (Mar. </month> <year> 1992), </year> <pages> 5-44. </pages> <note> Also available as Stanford University Technical Report CSL-TR-92-526. </note>
Reference-contexts: Such programs are quite unlike those developed by computer scientists; it is therefore necessary for the computer science community to cultivate an understanding of a body of programs developed by a different community. While several suites of parallel applications have been collected for study by computer scientists <ref> [21, 4, 7] </ref>, many of the applications are simplified versions of the actual production codes. The parallel versions of these codes usually are not used by the community from which they were drawn.
Reference: [22] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <address> Oct. </address> <year> 1991. </year>
Reference-contexts: This points to the need for high-quality, easy-to-use profiling tools in addition to good compilers. 20 Chapter 4 Modeling a Parallel Implementation Having achieved significant speedups for accrete on the Cray, we turned our attention to the CM-5 <ref> [22] </ref>. The CM-5 is a hybrid vector-parallel machine which consists of RISC nodes augmented with vector capability, connected by a fat-tree network.
Reference: [23] <author> Triolet, R., Irigoin, F., and Feautrier, P. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Compiler Construction (Palo Alto, </booktitle> <address> California, June 1986), </address> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <pages> pp. 176-185. </pages>
Reference-contexts: Once these steps have been performed, the transformation can be performed using array expansion [10], loop distribution [15], and either inlining or movement of loops into called procedures <ref> [23, 17] </ref>.
References-found: 23

