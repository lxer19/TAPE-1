URL: http://www.isi.edu/isd/rickel/agents97.ps
Refering-URL: http://www.isi.edu/isd/rickel/publications.html
Root-URL: http://www.isi.edu
Email: rickel, johnson@isi.edu  
Title: Integrating Pedagogical Capabilities in a Virtual Environment Agent  
Author: Jeff Rickel and W. Lewis Johnson 
Web: http://www.isi.edu/isd/VET/vet.html  
Address: 4676 Admiralty Way, Marina del Rey, CA 90292-6695  
Affiliation: Information Sciences Institute Computer Science Department University of Southern California  
Date: Feb. 1997.  
Note: To appear in Proc. of First International Conference on Autonomous Agents,  
Abstract: Virtual environments are a promising milieu for education and training, because they allow students to practice their skills in 3D simulations of work settings. Autonomous agents can improve the effectiveness of such environments by assisting and collaborating with students as appropriate. This paper describes an autonomous pedagogical agent called Steve that can support the training of procedural skills such as operating or repairing complex equipment. Steve's architecture permits him to sense and manipulate dynamic virtual worlds. The architecture also enables Steve to assume alternative realizations, either as a full, articulated, human figure or as abstract pointers and disembodied hands. Steve employs a combination of intelligent capabilities in his interactions with students and the environment: plan revision and execution, explanation, and student monitoring. These capabilities are employed in multiple ways in order to support alternative pedagogical styles. Steve's knowledge representation is designed so that agent capabilities can be authored without detailed knowledge of agent architectures and languages. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ambros-Ingerson, J. A., and Steel, S. </author> <year> 1988. </year> <title> Integrating planning, execution and monitoring. </title> <booktitle> In Proc. Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 83-88. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Anderson, J. R.; Boyle, C. F.; Corbett, A. T.; and Lewis, M. W. </author> <year> 1990. </year> <title> Cognitive modeling and intelligent tutoring. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 7-49. </pages>
Reference-contexts: Our work so far has focused on providing help when the student requests it. This capability provides a foundation for later work on automatically recognizing when the student needs help. Unlike most model tracing tutors <ref> (Anderson et al. 1990) </ref>, which require the student to follow the tutor's plan, we want to give the student the flexibility to deviate from the standard plan, make mistakes, and learn to recover from them.
Reference: <author> Badler, N. I.; Phillips, C. B.; and Webber, B. L. </author> <year> 1993. </year> <title> Simulating Humans. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: This representation for Steve is capable of drawing a student's attention to objects without blocking his field of view or distracting him unnecessarily. As an embodied agent, we are developing methods by which Steve can appear as a Jack human figure <ref> (Badler, Phillips, & Webber 1993) </ref> in the virtual environment. For example, Figure 4 shows one Steve agent, represented by a human figure, watching (via dynamic gaze control) a demonstration by another Steve agent, represented by a hand.
Reference: <author> Billinghurst, M., and Savage, J. </author> <year> 1996. </year> <title> Adding intelligence to the interface. </title> <booktitle> In Proc. IEEE Virtual Reality Annual International Symposium, </booktitle> <pages> 168-175. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Cassell, J.; Pelachaud, C.; Badler, N.; Steedman, M.; Achorn, B.; Becket, T.; Douville, B.; Prevost, S.; and Stone, M. </author> <year> 1994. </year> <title> Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> In Proc. ACM SIGGRAPH '94. </booktitle>
Reference: <author> Firby, R. J. </author> <year> 1994. </year> <title> Task networks for controlling continuous processes. </title> <booktitle> In Proc. Second International Conference on AI Planning Systems. </booktitle>
Reference-contexts: For example, Soar productions are more appropriate for cognitive tasks, while the C language is more appropriate for sensorimotor processing. The division of labor between Steve's cognitive and sensorimotor components is similar in several ways to Firby's RAP system <ref> (Firby 1994) </ref>. Both use the cognitive component to represent plans and goals and send action commands to the sensorimotor component, both allow multiple action commands to be executed concurrently, and both rely on signals from the sensorimo-tor component to the cognitive component to provide feedback on execution of actions.
Reference: <author> Geib, C.; Levison, L.; and Moore, M. B. </author> <year> 1994. </year> <title> Soda-jack: An architecture for agents that search and manipulate objects. </title> <type> Tech. Report MS-CIS-94-16/LINC LAB 265, </type> <institution> Dept. of Computer and Information Science, Univ. of Pennsylvania. </institution>
Reference: <author> Hill, Jr., R. W., and Johnson, W. L. </author> <year> 1995. </year> <title> Situated plan attribution. </title> <journal> Journal of Artificial Intelligence in Education 6(1) </journal> <pages> 35-66. </pages>
Reference-contexts: Building on our current student monitoring capabilities, we intend to incorporate techniques from Hill and Johnson's "Situated Plan Attribution" <ref> (Hill & Johnson 1995) </ref> in order to recognize impasses. Once impasses are recognized, Steve can provide guidance as if the student requested it, but that guidance can be customized based on an understanding of the student's failed intentions.
Reference: <author> Johnson, W. L. </author> <year> 1994. </year> <title> Agents that learn to explain themselves. </title> <booktitle> In Proc. Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1257-1263. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Student: Why? Steve: That goal is relevant because it will allow us to check the alarm lights. Student: Why? Steve: That action is relevant because we want to know whether the alarm lights are functional. Debrief system <ref> (Johnson 1994) </ref> to maintain an episodic memory of the actions he performs. At the student's request, Debrief can generate a list of the actions in episodic memory. The student can ask why one of these actions was performed, and Debrief will recall the situation in which the action was performed. <p> Tambe et al. (1995) designed a Soar fighter pilot that can engage in air combat with other simulated pilots. Their pilot can execute missions, monitor other pilots as they execute their missions (Tambe & Rosen-bloom 1995), and explain the rationale behind actions <ref> (Johnson 1994) </ref>.
Reference: <author> Laird, J. E.; Newell, A.; and Rosenbloom, P. S. </author> <year> 1987. </year> <title> Soar: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33(1) </booktitle> <pages> 1-64. </pages>
Reference: <author> Maes, P.; Darrell, T.; Blumberg, B.; and Pentland, A. </author> <year> 1995. </year> <title> The ALIVE system: Full-body interaction with autonomous agents. </title> <booktitle> In Proc. Computer Animation '95, </booktitle> <pages> 11-18. </pages> <publisher> IEEE Press. </publisher>
Reference-contexts: This modular approach, in some ways similar to that used in the alive system <ref> (Maes et al. 1995) </ref>, allows us to experiment with alternative agent realizations without modifying Steve's cognitive component. As a disembodied agent, Steve is currently represented by a virtual hand that can point to objects as well as grasp and manipulate them.
Reference: <author> Munro, A.; Johnson, M.; Surmon, D.; and Wogulis, J. </author> <year> 1993. </year> <title> Attribute-centered simulation authoring for instruction. </title> <booktitle> In Proc. World Conference on Artificial Intelligence in Education, </booktitle> <pages> 82-89. </pages>
Reference-contexts: The communication bus is currently implemented using the Sun ToolTalk T M software. The simulator that controls the virtual world is constructed by the course author using the RIDES <ref> (Munro et al. 1993) </ref> software, which was developed at the USC Behavioral Technology Lab. RIDES is a program for authoring object-oriented simulators. That is, each physical object in the virtual world is represented as an object with attributes.
Reference: <author> Newell, A. </author> <year> 1990. </year> <title> Unified Theories of Cognition. </title> <publisher> Har-vard University Press. </publisher>
Reference: <author> Russell, S., and Norvig, P. </author> <year> 1995. </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Sacerdoti, E. </author> <year> 1977. </year> <title> A Structure for Plans and Behavior. </title> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: To satisfy these criteria, we chose an intermediate point on the spectrum from rote execution to a full plan revision capability. Given a task to demonstrate, Steve first constructs a plan for performing it, using top-down task decomposition <ref> (Sacerdoti 1977) </ref>. That is, Steve repeatedly expands any complex step in the evolving plan with the subplan (specified by the course author) for achieving it, until the plan has been fully decomposed. However, rather than execute the plan by rote, Steve continually re-evaluates the plan as he executes it.
Reference: <author> Schank, R., and Abelson, R. </author> <year> 1977. </year> <title> Scripts, Plans, Goals and Understanding. </title> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Like Steve, their agent can communicate with the student using speech and gestures. However, unlike Steve, their agent is also capable of speech recognition and natural language understanding. Their agent represents domain tasks as hierarchical scripts <ref> (Schank & Abelson 1977) </ref>, which are very similar to Steve's hi erarchical plans.
Reference: <author> Stiles, R.; McCarthy, L.; and Pontecorvo, M. </author> <year> 1995. </year> <title> Training studio: A virtual environment for training. In Workshop on Simulation and Interaction in Virtual Environments. </title> <publisher> ACM Press. </publisher>
Reference-contexts: This object-oriented representation makes it easy for other programs, such as Steve, to monitor the state of the simulation, as will be described in the next section. Students and human instructors interact with the virtual world via Vista Viewer software <ref> (Stiles, Mc-Carthy, & Pontecorvo 1995) </ref> (developed at Lockheed Martin), which monitors the communication bus to determine the state of objects and then produces graphical renderings of those objects. Each student and instructor has their own Vista Viewer process, which controls the scene on their head-mounted display.
Reference: <author> Tambe, M., and Rosenbloom, P. S. </author> <year> 1995. </year> <title> Resc: An approach for real-time, dynamic agent tracking. </title> <booktitle> In Proc. Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 103-110. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Tambe et al. (1995) designed a Soar fighter pilot that can engage in air combat with other simulated pilots. Their pilot can execute missions, monitor other pilots as they execute their missions <ref> (Tambe & Rosen-bloom 1995) </ref>, and explain the rationale behind actions (Johnson 1994).
Reference: <author> Tambe, M.; Johnson, W. L.; Jones, R. M.; Koss, F.; Laird, J. E.; Rosenbloom, P. S.; and Schwamb, K. </author> <year> 1995. </year> <title> Intelligent agents for interactive simulation environments. </title> <journal> AI Magazine 16(1) </journal> <pages> 15-39. </pages>
Reference-contexts: Tambe et al. (1995) designed a Soar fighter pilot that can engage in air combat with other simulated pilots. Their pilot can execute missions, monitor other pilots as they execute their missions <ref> (Tambe & Rosen-bloom 1995) </ref>, and explain the rationale behind actions (Johnson 1994).
Reference: <author> Trias, T. S.; Chopra, S.; Reich, B. D.; Moore, M. B.; Badler, N. I.; Webber, B. L.; and Geib, C. W. </author> <year> 1996. </year> <title> Decision networks for integrating the behaviors of virtual agents and avatars. </title> <booktitle> In Proc. IEEE Virtual Reality Annual International Symposium, </booktitle> <pages> 156-162. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Vere, S., and Bickmore, T. </author> <year> 1990. </year> <title> A basic agent. </title> <booktitle> Computational Intelligence 6 </booktitle> <pages> 41-60. </pages>
Reference: <author> Weld, D. S. </author> <year> 1994. </year> <title> An introduction to least commitment planning. </title> <journal> AI Magazine 15(4) </journal> <pages> 27-61. 9 </pages>
Reference-contexts: Each time the virtual world changes, whether the changes are due to his own actions or not, Steve looks for a subset of the plan that will allow him to complete the task. Steve's method for finding such a subset resembles partial-order planning <ref> (Weld 1994) </ref>. A partial-order planner starts with the top-level goals of the task and adds steps to the plan in order to achieve these goals. Each step that is added may have unsatisfied preconditions, and each such precondition becomes a new goal that must likewise be achieved.
References-found: 22

