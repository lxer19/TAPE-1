URL: ftp://cns-ftp.bu.edu/pub/paolo/PedGau:94.ps.gz
Refering-URL: http://www.inns.org/Misc-info/Online-pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Correspondence:  e-mail: gaudiano@cns.bu.edu  
Phone: Phone: (617) 353-9482 Fax: (617) 353-7755  
Title: Pedini and Gaudiano ART-Family Simulators A collection of ART-family graphical simulators  
Author: David Pedini and Paolo Gaudiano Paolo Gaudiano 
Address: 111 Cummington Street, Boston, MA 02215  111 Cummington Street Boston, MA 02215  
Affiliation: Boston University, Dept. of Cognitive and Neural Systems  Boston University Department of Cognitive and Neural Systems  
Abstract-found: 0
Intro-found: 1
Reference: <author> Carpenter, G. A., & Grossberg, S. </author> <year> (1987a). </year> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37, </volume> <pages> 54-115. </pages>
Reference: <author> Carpenter, G. A., & Grossberg, S. </author> <year> (1987b). </year> <title> ART 2: Self-organization of stable category recognition codes for analog input patterns. </title> <journal> Applied Optics, </journal> <volume> 26, </volume> <pages> 4919-4930. </pages>
Reference-contexts: Recent ART architectures have been designed to work in a supervised fashion, offering a viable alternative to supervised neural networks such as backpropagation (Rumelhart, Hinton, & Williams, 1986). Perhaps the best-known variant of ART is ART2 <ref> (Carpenter & Grossberg, 1987b) </ref>, an unsupervised neural network that handles analog inputs. We have developed a series of simulators for some of the ART-family neural architectures, namely, ART2 (Carpenter & Grossberg, 1987b), ART2-A (Carpenter, Grossberg, & Rosen, 1991b), Fuzzy ART (Carpenter, Grossberg, & Rosen, 1990), and Fuzzy ARTMAP (Carpenter, Grossberg, Markuzon, <p> Perhaps the best-known variant of ART is ART2 <ref> (Carpenter & Grossberg, 1987b) </ref>, an unsupervised neural network that handles analog inputs. We have developed a series of simulators for some of the ART-family neural architectures, namely, ART2 (Carpenter & Grossberg, 1987b), ART2-A (Carpenter, Grossberg, & Rosen, 1991b), Fuzzy ART (Carpenter, Grossberg, & Rosen, 1990), and Fuzzy ARTMAP (Carpenter, Grossberg, Markuzon, & Reynolds, 1992). <p> The ART2 architecture <ref> (Carpenter & Grossberg, 1987b) </ref>, shown in Fig. 2, extends ART to allow analog input vectors. Stability can no longer be proven, but in practice the system can be shown to be stable under most conditions. (Carpenter et al., 1991b) implemented a fast, algorithmic version of ART2, which they termed ART2-A. <p> Compiled binaries for SUN and SGI workstations are also available in the subdirectory bin. Aside form the source code, the full distribution includes UNIX-style man pages, and sample input files, including the input file used in the original ART2 article <ref> (Carpenter & Grossberg, 1987b) </ref>. Acknowledgments Paolo Gaudiano is supported in part by the Air Force Office of Scientific Research (AFOSR F49620-92-J-0499) and by the Sloan Foundation (BR-3122). David Pedini is supported in by ????. 8 Pedini and Gaudiano ART-Family Simulators
Reference: <author> Carpenter, G. A., Grossberg, S., Markuzon, N., & Reynolds, J. </author> <year> (1992). </year> <title> Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 698-713. </pages>
Reference-contexts: We have developed a series of simulators for some of the ART-family neural architectures, namely, ART2 (Carpenter & Grossberg, 1987b), ART2-A (Carpenter, Grossberg, & Rosen, 1991b), Fuzzy ART (Carpenter, Grossberg, & Rosen, 1990), and Fuzzy ARTMAP <ref> (Carpenter, Grossberg, Markuzon, & Reynolds, 1992) </ref>. <p> Recent research efforts have shown that ARTMAP can achieve high performance level on several benchmarks, while being computationally very efficient. In order to allow supervised categorization of analog inputs, each ART1 module can be replaced by a Fuzzy ART module. Such an architecture has been presented by <ref> (Carpenter et al., 1992) </ref>, and is known as Fuzzy ARTMAP. 2 Simulation software The best way to learn how to use ART is probably to read some of the original articles and to write a program to implement the architecture.
Reference: <author> Carpenter, G. A., Grossberg, S., & Reynolds, J. </author> <year> (1991a). </year> <title> ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 565-588. </pages>
Reference-contexts: In order to take advantage of the advantages of both supervised and unsupervised learning, <ref> (Carpenter, Grossberg, & Reynolds, 1991a) </ref> developed ARTMAP, a neural network that combines two ART1 modules for supervised learning: one ART1 module is presented with the input, while the other ART1 module is given the desired output.
Reference: <author> Carpenter, G. A., Grossberg, S., & Rosen, D. B. </author> <year> (1991b). </year> <title> ART 2-A: An adaptive resonance algorithm for rapid category learning and recognition.. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 493-504. </pages>
Reference-contexts: Perhaps the best-known variant of ART is ART2 (Carpenter & Grossberg, 1987b), an unsupervised neural network that handles analog inputs. We have developed a series of simulators for some of the ART-family neural architectures, namely, ART2 (Carpenter & Grossberg, 1987b), ART2-A <ref> (Carpenter, Grossberg, & Rosen, 1991b) </ref>, Fuzzy ART (Carpenter, Grossberg, & Rosen, 1990), and Fuzzy ARTMAP (Carpenter, Grossberg, Markuzon, & Reynolds, 1992). <p> The ART2 architecture (Carpenter & Grossberg, 1987b), shown in Fig. 2, extends ART to allow analog input vectors. Stability can no longer be proven, but in practice the system can be shown to be stable under most conditions. <ref> (Carpenter et al., 1991b) </ref> implemented a fast, algorithmic version of ART2, which they termed ART2-A. The ART2-A algorithm is equivalent to running ART in fast 4 Pedini and Gaudiano ART-Family Simulators learning mode, which means that all weights are allowed to reach equilibrium in response to each input pattern.
Reference: <author> Carpenter, G. A., Grossberg, S., & Rosen, D. </author> <year> (1990). </year> <title> Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system. </title> <booktitle> Neural Networks, </booktitle> <volume> 4(1), </volume> <pages> 759-771. </pages>
Reference-contexts: Perhaps the best-known variant of ART is ART2 (Carpenter & Grossberg, 1987b), an unsupervised neural network that handles analog inputs. We have developed a series of simulators for some of the ART-family neural architectures, namely, ART2 (Carpenter & Grossberg, 1987b), ART2-A (Carpenter, Grossberg, & Rosen, 1991b), Fuzzy ART <ref> (Carpenter, Grossberg, & Rosen, 1990) </ref>, and Fuzzy ARTMAP (Carpenter, Grossberg, Markuzon, & Reynolds, 1992). <p> The ART2-A algorithm offers a significant speed-up compared to the regular ART2 network, and comparable performance. Another version of ART, known as Fuzzy ART <ref> (Carpenter et al., 1990) </ref>, utilizes fuzzy logic operators, and is able to handle binary or analog inputs. Many of today's neural network applications utilize supervised networks. The use of supervised networks may or may not be biologically plausible, but it is sometimes the best solution for applied problem-solving.
Reference: <author> Grossberg, S. </author> <year> (1976a). </year> <title> Adaptive pattern classification and universal recoding II: Feedback, expectation, </title> <journal> olfaction, and illusions.. Biological Cybernetics, </journal> <volume> 23, </volume> <pages> 187-202. </pages>
Reference-contexts: Somehow biological learning mechanisms must be able to buffer stored memories against transient changes, while retaining plasticity to learn novel events in the environment. This tradeoff between continued learning and buffering of old memories has been called the stability-plasticity dilemma <ref> (Grossberg, 1976a) </ref>. Consideration of the stability-plasticity dilemma led Grossberg (1976a) to develop ART. In an earlier article, Grossberg (1976b) had shown that neural networks that learn through competitive learning are unable to satisfy the stability-plasticity dilemma.
Reference: <author> Grossberg, S. </author> <year> (1976b). </year> <title> Adaptive pattern classification and universal recoding, I: Parallel development and coding of neural feature detectors. </title> <journal> Biological Cybernetics, </journal> <volume> 23, </volume> <pages> 121-134. </pages>
Reference-contexts: 1 Introduction The Adaptive Resonance Theory (ART) architecture, first proposed by <ref> (Grossberg, 1976b, 1976a) </ref>, is a self-organizing neural network for stable pattern categorization in response to arbitrary input sequences. Since its original formulation, several versions of ART have been proposed, each designed to handle a particular task or input format.
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323, </volume> <pages> 533-536. 9 </pages>
Reference-contexts: Since its original formulation, several versions of ART have been proposed, each designed to handle a particular task or input format. Recent ART architectures have been designed to work in a supervised fashion, offering a viable alternative to supervised neural networks such as backpropagation <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. Perhaps the best-known variant of ART is ART2 (Carpenter & Grossberg, 1987b), an unsupervised neural network that handles analog inputs.
References-found: 9

