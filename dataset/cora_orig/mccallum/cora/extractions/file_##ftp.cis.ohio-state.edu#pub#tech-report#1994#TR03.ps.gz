URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR03.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: martens@cis.ohio-state.edu jayasim@cis.ohio-state.edu  
Phone: (614) 292-1932  
Title: Compiling for Hierarchical Shared-Memory Multiprocessors  
Author: J. D. Martens and D. N. Jayasimha 
Note: Both authors were supported in part by the National Science Foundation under grant number NSF CCR-8909189.  
Date: January 24, 1994  
Address: Columbus, Ohio 43210-1277  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: A preliminary version of the first three sections of this paper appeared in the Proceedings of the 1993 International Conference on Parallel Processing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Zahira Ammarguellat and W. L. Harrison III. </author> <title> Automatic Recognition of Induction Variables and Recurrence Relations by Abstract Interpretation. </title> <booktitle> In ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: of nesting (it is assumed that the nest is more than one level deep, or that it is a non-nested loop in which any dependence cycles can be broken; otherwise the loop must be executed as a doacross, executed sequentially, or rewritten as a recurrence calculation similar to parallel prefix|see <ref> [1, 38] </ref>). If the loops are not perfectly nested, an attempt is made to obtain outer loop parallelism either directly or via loop interchange. Failing this, an attempt is made to distributed the loop nest and apply unimodular techniques. <p> This can be expressed as in figure 8. 3.1.4 Step Three The next step is to tile the iteration space using block partitions. PE m , 0 m &lt; p, executes the code shown in figure 9. 9 L 1 uses f a <ref> [1, 2:n+1] </ref> from Entry, a [2:n, 2:n-1] from L 3 , a [2:n, n] from L 2 , a [2:n, n+1] from Entry g for i := 1 to n do S 1 a [i, j+1] := g (a [i, j+1]) endfor Barrier L 2 for j := lo m to <p> Prior to L 1 , PE m must fetch the data as shown in figure 11. The first two regular sections in figure 11 are the data 1. use i m (L 1 ) " fa <ref> [1; 2 : n + 1] </ref>g from Entry; 2. use i m (L 1 ) " fa [2 : n; n + 1]g from Entry; 3. 8m 0 : 0 m 0 &lt; p ^ m 0 6= m : m (L 1 ) " def 1:i1 m 0 (L 2 <p> When condition one is not satisfied and the dependence distance is d, then there will be potential communication between PE m and d of its neighbors, so there will be d sections; in this case, d = 2. 11 1. f a <ref> [1, lo m : hi m ] </ref> g from Entry if i = 1 2. f a [i; n + 1] g from Entry if i 6= 1 ^ n = hi m 3. f a [i; n] g from commonLevel (m, m+1) if hi m = n 4. f a <p> 1,n2 do 12: doall k := 1,n3 do 13: b [i1+1, i2, k] := a [i1, i2, k+1]+c [i1, i2, k] 12: endfor 11: endfor 5: endfor flow dependence 8: --&gt; 13:(=) (0) flow dependence 13: --&gt; 8:(&lt;) (1) 14 Array dataflow analysis shows that at S 8 , b <ref> [1, 1:n2, 1:n3] </ref> and d [1:n1, 1:n2, 1:n3] are needed from Entry, b [2:n1, 2:n2, 2:n3] are needed from S 13 , and a [1:n1, 2:n2, 2:n3] are defined and subsequently used at S 13 . <p> At S 13 , the following sections are used: fa [1:n1, 2:n2, 2:n3] from S 8 , a <ref> [1:n1, 1, 2:n3+1] </ref> from Entry, a [1:n1, 2:n2, n3+1] from Entry, c [1:n1, 1:n2, 1:n3] from Entryg. b [2:n1, 1:n2, 1:n3] is defined and subsequently used in S 8 .
Reference: [2] <author> Arvind and Robert A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <type> Technical Report 226-6, </type> <institution> Laboratory for Computer Science, MIT, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: HSMAs are appealing since they reduce latency and synchronization overheads often associated associated with large-scale multiprocessing <ref> [2] </ref> (see [15] for a detailed discussion on how different types of communication and synchronization costs are reduced).
Reference: [3] <author> Vasanth Balasundaram. </author> <title> A Mechanism for Keeping Useful Internal Information in Parallel Programming Tools: The Data Access Descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Given such a d-dimensional space, two concise ways of representing subsets of arrays are simple sections <ref> [3, 4] </ref> and regular sections [13]. A regular section is a rectangular portion of an array, with each regular section boundary parallel to a coordinate axis. Simple sections are a bit more flexible in that they also allow diagonal boundaries at 45 degree angles to the coordinate axes.
Reference: [4] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> A Technique for Summarizing Data Access and its Use in Parallelism Enhancing transformations. </title> <booktitle> In SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 41-53, </pages> <year> 1989. </year>
Reference-contexts: Given such a d-dimensional space, two concise ways of representing subsets of arrays are simple sections <ref> [3, 4] </ref> and regular sections [13]. A regular section is a rectangular portion of an array, with each regular section boundary parallel to a coordinate axis. Simple sections are a bit more flexible in that they also allow diagonal boundaries at 45 degree angles to the coordinate axes.
Reference: [5] <author> Eugene D. Brooks III. </author> <title> The Shared Memory Hypercube. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 235-245, </pages> <year> 1988. </year>
Reference-contexts: Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes <ref> [5, 20] </ref>. The use of local and hierarchical memory is not to be confused with the use of caches, since caches form part of the physical hierarchy; in particular, the issue of cache coherence is orthogonal to the issues discussed in this paper.
Reference: [6] <author> Tony F. Chan. </author> <title> Hierarchical Algorithms and Architectures for Parallel Scientific Computing. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 318-329, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Many parallel algorithms (e.g., matrix multiplication, associative operations, solution of partial differential equations by differencing techniques, prefix computations, barrier synchronization, etc.) have data whose movement behaves in a structured, hierarchical manner; such algorithms map well to hierarchical architectures <ref> [6, 15, 24, 28, 31, 40] </ref>. In many algorithms the various data need not be globally shared, but rather can be partially shared among processing elements (PEs)[15].
Reference: [7] <author> Hoichi Cheong and Alexander V. Veidenbaum. </author> <title> Compiler-Directed Cache Management in Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Finally, for convenience, nonlocal is referred to as if it were a specific memory unit in which nonlocal variables can be stored. 2.1.2 Generation of Communication Statements Producers use atomic statements to set the states of shared variables to loop-specific values (this is similar to what is done in <ref> [7] </ref> and [16]); the consumer waits for the state to reach the appropriate value.
Reference: [8] <author> Raja Das, Ravi Ponnusamy, Joel Saltz, and Dimitri Mavriplis. </author> <title> Distributed Memory Compiler Methods for Irregular Problems|Data Copy Reuse and Runtime Partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <pages> pages 185-219. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference: [9] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring FORTRAN Programs for Cedar. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 57-66, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: This scheme of fetching needed data, executing, and then sending live data elsewhere is also used by Ramanujam and Sadayappan [29]. In Cedar terminology the initialization and the finalization are referred to as the preamble and the postamble, respectively <ref> [9] </ref>. The approximate code generated is given in figure 4 (for simplicity, boundary conditions and the ceiling function of the lo m ; hi m calculations have been omitted). <p> Among the approaches related to that of this paper are those of the Cedar, Kali, and FORTRAN D projects. 5.1 Cedar In the Cedar restructuring compiler, two passes are used to optimize data placement for the hierarchical memory consisting of three levels: local, intermediate or cluster, and global <ref> [9, 32] </ref>. A globalization pass determines which data is shared among more than one cluster; this data is placed in the global memory. A privatization pass then determines which variables have a lifetime of just one loop iteration; these variables can be placed in local memory.
Reference: [10] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D Language Specification. </title> <type> Technical Report COMP TR90-141, </type> <institution> Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year> <note> Revised April, </note> <year> 1991. </year>
Reference-contexts: A block partitioning strategy is used throughout this paper, but cyclic and block-cyclic partitions may be investigated as future research (definitions of block, cyclic, and block-cyclic may be in found in <ref> [10] </ref>). The appeal of a block partition is twofold: (1) if loop alignment is off slightly 5 between successive loops, relatively little additional communication results; (2) the regular and simple section formalisms discussed in sections 2.2.4ff are better suited to block partitions than to cyclic partitions. <p> Such dynamic movement of data has been identified as a motivation for hierarchical shared memory architectures [15]. 5.2 Kali and FORTRAN D Kali [18] and FORTRAN D <ref> [10, 14] </ref> are parallel programming languages with explicit data distribution directives and provisions for specifying forall style parallelism.
Reference: [11] <author> Daniel Gajski, David Kuck, Duncan Lawrie, and Ahmed Sameh. </author> <title> Cedar|A Large Scale Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <year> 1983. </year>
Reference-contexts: An architecture incorporating a hierarchical memory would have local memory at its lowest level and fully shared global memory at its highest level with a "continuum" in between. Architectures which can be considered HSMAs include explicitly hierarchical machines such as the Tree-structured Hierarchical Memory Multiprocessor (THMM)[15, 23], Cedar <ref> [11, 19] </ref>, Mabbs' Hierarchical Memory Structure [21], Wei and Levy's multilevel hierarchy [33], and the hierarchical multiprocessor of Mahgoub and Elmagarmid [22]. Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes [5, 20].
Reference: [12] <author> Thomas Gross and Peter Steenkiste. </author> <title> Structured Dataflow Analysis for Arrays and its Use in an Optimizing Compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: This communication is addressed in subsequent steps of the basic method. 2.2.2 Array Dataflow Analysis Gross and Steenkiste developed a method for performing interval-based dataflow analysis on arrays <ref> [12] </ref>. Using their method, it is possible to determine which sections of an array are live at any given point in the program, and the definition which produced that section can be identified. A section in this context is a regular section, as defined in section 2.2.4.
Reference: [13] <author> Paul Havlak and Ken Kennedy. </author> <title> An Implementation of Interprocedural Bounded Regular Section Analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Given such a d-dimensional space, two concise ways of representing subsets of arrays are simple sections [3, 4] and regular sections <ref> [13] </ref>. A regular section is a rectangular portion of an array, with each regular section boundary parallel to a coordinate axis. Simple sections are a bit more flexible in that they also allow diagonal boundaries at 45 degree angles to the coordinate axes.
Reference: [14] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler support for machine--independent parallel programming in fortran d. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <pages> pages 139-176. </pages> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Such dynamic movement of data has been identified as a motivation for hierarchical shared memory architectures [15]. 5.2 Kali and FORTRAN D Kali [18] and FORTRAN D <ref> [10, 14] </ref> are parallel programming languages with explicit data distribution directives and provisions for specifying forall style parallelism.
Reference: [15] <author> D. N. Jayasimha. </author> <title> Partially Shared Variables and Hierarchical Shared Memory Multiprocessor Architectures. </title> <booktitle> In 11th Annual IEEE International Conference on Computers and Communications, </booktitle> <pages> pages 63-71, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: HSMAs are appealing since they reduce latency and synchronization overheads often associated associated with large-scale multiprocessing [2] (see <ref> [15] </ref> for a detailed discussion on how different types of communication and synchronization costs are reduced). <p> Many parallel algorithms (e.g., matrix multiplication, associative operations, solution of partial differential equations by differencing techniques, prefix computations, barrier synchronization, etc.) have data whose movement behaves in a structured, hierarchical manner; such algorithms map well to hierarchical architectures <ref> [6, 15, 24, 28, 31, 40] </ref>. In many algorithms the various data need not be globally shared, but rather can be partially shared among processing elements (PEs)[15]. <p> This "intermediate level" may be fewer network hops from the PEs than the global level, thus reducing latency (see <ref> [15] </ref> for more details). In the next section a method of parallelizing loops for HSMAs is described; this includes discussion of communication and synchronization primitives, partitioning, and code generation issues. Examples of the application of the method are provided in section 3, along with discussion of related issues. <p> On an HSMA, a prerequisite for communication is that a variable be placed at a location in the hierarchy accessible by every PE needing the variable, but it is not necessary that the variable be accessible by other PEs. This is one characteristic of a partially-shared variable <ref> [15] </ref>. Additionally, each partially-shared variable has state and data fields associated with it. <p> For example, if our scheme were adopted for Cedar, more data would move between global and cluster levels as a function of time than is currently the case. Such dynamic movement of data has been identified as a motivation for hierarchical shared memory architectures <ref> [15] </ref>. 5.2 Kali and FORTRAN D Kali [18] and FORTRAN D [10, 14] are parallel programming languages with explicit data distribution directives and provisions for specifying forall style parallelism. <p> studied. * It would be desirable to automatically exploit inter-procedural and inter-object parallelism in an HSMA. * In addition to the patterns of partial sharing of variables supported by the compilation methods of this paper, partially-shared variables can be split and fused in time as their degree of sharing changes <ref> [15] </ref>. This splitting and fusing is a generalization of ascend/descend algorithms [28] and software combining [40, 31].
Reference: [16] <author> D. N. Jayasimha and J. D. Martens. </author> <title> Some Architectural and Compilation Issues in the Design of Hierarchical Shared Memory Multiprocessors. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 567-572, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: for convenience, nonlocal is referred to as if it were a specific memory unit in which nonlocal variables can be stored. 2.1.2 Generation of Communication Statements Producers use atomic statements to set the states of shared variables to loop-specific values (this is similar to what is done in [7] and <ref> [16] </ref>); the consumer waits for the state to reach the appropriate value.
Reference: [17] <author> Harry F. Jordan. </author> <title> HEP Architecture, Programming, and Performance. </title> <editor> In Janusz S. Kowalik, editor, </editor> <title> Parallel MIMD Computation: </title> <booktitle> HEP Supercomputer and Its Applications, </booktitle> <pages> pages 1-40. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: This is one characteristic of a partially-shared variable [15]. Additionally, each partially-shared variable has state and data fields associated with it. As in [41], the state field is a generalization of the full/empty bit of the Denelcor HEP <ref> [17] </ref>, being an arbitrary integer rather than just a single bit; the state is used for synchronization: producers set the state of a variable to a predetermined value when they write the datum, and consumers wait for the state to reach a particular value before reading the datum.
Reference: [18] <author> Charles Koelbel and Piyush Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Such dynamic movement of data has been identified as a motivation for hierarchical shared memory architectures [15]. 5.2 Kali and FORTRAN D Kali <ref> [18] </ref> and FORTRAN D [10, 14] are parallel programming languages with explicit data distribution directives and provisions for specifying forall style parallelism. <p> In our approach, iterations are assigned to PEs, and data is moved wherever it is needed. We expect to get better load balancing than is seen in these languages (e.g., for the Gaussian 28 elimination example in <ref> [18] </ref>), but at the cost of more complicated analysis and possibly more communication. * In Kali, data needed by other PEs is sent, then loop iterations requiring only local data are executed, data sent by other PEs is received, and finally iterations needing nonlocal data are executed.
Reference: [19] <author> J. Konicek, T. Tilton, A. Veidenbaum, C. Q. Zhu, E. S. Davidson, R. Downing, M. Haney, M. Sharma, P. C. Yew, P. M. Farmwald, D. Kuck, D. Lavery, R. Lindsey, D. Pointer, J. An-drews, T. Beck, T. Murphy, S. Turner, and N. Warter. </author> <title> The Organization of the Cedar System. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 49-56, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: An architecture incorporating a hierarchical memory would have local memory at its lowest level and fully shared global memory at its highest level with a "continuum" in between. Architectures which can be considered HSMAs include explicitly hierarchical machines such as the Tree-structured Hierarchical Memory Multiprocessor (THMM)[15, 23], Cedar <ref> [11, 19] </ref>, Mabbs' Hierarchical Memory Structure [21], Wei and Levy's multilevel hierarchy [33], and the hierarchical multiprocessor of Mahgoub and Elmagarmid [22]. Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes [5, 20].
Reference: [20] <author> Donald C. Lindsay. </author> <title> Towards a Shared Memory Hypercube. </title> <type> Technical Report CMU-CS-88-190, </type> <institution> Carnegie Mellon University, Department of Computer Science, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes <ref> [5, 20] </ref>. The use of local and hierarchical memory is not to be confused with the use of caches, since caches form part of the physical hierarchy; in particular, the issue of cache coherence is orthogonal to the issues discussed in this paper.
Reference: [21] <author> Stephen A. Mabbs and Kevin E. </author> <title> Forward. Optimizing the Communication Architecture of a Hierarchical Parallel Processor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 516-520, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Architectures which can be considered HSMAs include explicitly hierarchical machines such as the Tree-structured Hierarchical Memory Multiprocessor (THMM)[15, 23], Cedar [11, 19], Mabbs' Hierarchical Memory Structure <ref> [21] </ref>, Wei and Levy's multilevel hierarchy [33], and the hierarchical multiprocessor of Mahgoub and Elmagarmid [22]. Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes [5, 20].
Reference: [22] <author> I. O. Mahgoub and A. K. Elmagarmid. </author> <title> Performance Analysis of a Generalized Class of m-Level Hierarchical Multiprocessor Systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 129-138, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Architectures which can be considered HSMAs include explicitly hierarchical machines such as the Tree-structured Hierarchical Memory Multiprocessor (THMM)[15, 23], Cedar [11, 19], Mabbs' Hierarchical Memory Structure [21], Wei and Levy's multilevel hierarchy [33], and the hierarchical multiprocessor of Mahgoub and Elmagarmid <ref> [22] </ref>. Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes [5, 20].
Reference: [23] <author> J. D. Martens and D. N. Jayasimha. </author> <title> A Tree-Structured Hierarchical Memory Multiprocessor. </title> <type> Technical Report 61, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> December </month> <year> 1989. </year> <month> Revised May </month> <year> 1990. </year> <booktitle> An extended abstract appeared in International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 561-562, </pages> <month> August </month> <year> 1990. </year>
Reference: [24] <author> Henk Meijer and Selim G. Akl. </author> <title> Optimal Computation of Prefix Sums on a Binary Tree of Processors. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 127-136, </pages> <year> 1987. </year>
Reference-contexts: Many parallel algorithms (e.g., matrix multiplication, associative operations, solution of partial differential equations by differencing techniques, prefix computations, barrier synchronization, etc.) have data whose movement behaves in a structured, hierarchical manner; such algorithms map well to hierarchical architectures <ref> [6, 15, 24, 28, 31, 40] </ref>. In many algorithms the various data need not be globally shared, but rather can be partially shared among processing elements (PEs)[15].
Reference: [25] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: In a typical NUMA machine commonLevel (L) would place the item in one of the processors in the list L. Ideally, to reduce the cost of busy waiting, the item would be placed in a consumer rather than the producer <ref> [25, 26] </ref>; this may require an additional parameter to commonLevel ().
Reference: [26] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Synchronization without Contention. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 269-278, </pages> <month> April </month> <year> 1991. </year> <month> 31 </month>
Reference-contexts: In a typical NUMA machine commonLevel (L) would place the item in one of the processors in the list L. Ideally, to reduce the cost of busy waiting, the item would be placed in a consumer rather than the producer <ref> [25, 26] </ref>; this may require an additional parameter to commonLevel ().
Reference: [27] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Com--munications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: These methods may be implemented as part of a compiler, but can also be followed by a programmer as a part of his or her programming methodology. 2.1 Background and Notation It is assumed that the reader is familiar with standard parallelizing compiler terminology and techniques as discussed in <ref> [27, 38] </ref>. This paper focuses on nested parallel loops in which subscripts along each dimension of the loop are of the form i k, where i is a loop index and k is a constant; it is not necessary that the loops be perfectly nested.
Reference: [28] <author> Franco P. Preparata and Jean Vuillemin. </author> <title> The Cube Connected Cycles: a Versatile Network for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 24(5) </volume> <pages> 300-309, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Many parallel algorithms (e.g., matrix multiplication, associative operations, solution of partial differential equations by differencing techniques, prefix computations, barrier synchronization, etc.) have data whose movement behaves in a structured, hierarchical manner; such algorithms map well to hierarchical architectures <ref> [6, 15, 24, 28, 31, 40] </ref>. In many algorithms the various data need not be globally shared, but rather can be partially shared among processing elements (PEs)[15]. <p> This splitting and fusing is a generalization of ascend/descend algorithms <ref> [28] </ref> and software combining [40, 31].
Reference: [29] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling of Iteration Spaces for Multicomputers. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 179-186, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Each tile is statically mapped to a particular PE. Optimum "shape" and dimensionality of the tiles should be determined experimentally for the target machine and individual program. Tiling has been discussed in depth elsewhere <ref> [29, 34, 35, 36, 37] </ref>. Associated with each tile is the set of data defined or used within the tile; this set is termed a footprint. The footprints are not necessarily disjoint|often footprints of neighboring tiles overlap. <p> This scheme of fetching needed data, executing, and then sending live data elsewhere is also used by Ramanujam and Sadayappan <ref> [29] </ref>. In Cedar terminology the initialization and the finalization are referred to as the preamble and the postamble, respectively [9]. The approximate code generated is given in figure 4 (for simplicity, boundary conditions and the ceiling function of the lo m ; hi m calculations have been omitted).
Reference: [30] <author> Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. </author> <title> An Empirical Study of FORTRAN Programs for Parallelizing Compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Since loops with constant dependence distances tend to have fairly small dependence distances <ref> [30] </ref> and a block partition is assumed, any given item is likely to be shared among at most a few PEs. Thus the cost of the additional writes is likely to be small, but the benefit of reduced latency reads may also be small.
Reference: [31] <author> Peiyi Tang and Pen-Chung Yew. </author> <title> Software Combining Algorithms for Distributing Hot-Spot Addressing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10, </volume> <year> 1990. </year>
Reference-contexts: Many parallel algorithms (e.g., matrix multiplication, associative operations, solution of partial differential equations by differencing techniques, prefix computations, barrier synchronization, etc.) have data whose movement behaves in a structured, hierarchical manner; such algorithms map well to hierarchical architectures <ref> [6, 15, 24, 28, 31, 40] </ref>. In many algorithms the various data need not be globally shared, but rather can be partially shared among processing elements (PEs)[15]. <p> This splitting and fusing is a generalization of ascend/descend algorithms [28] and software combining <ref> [40, 31] </ref>.
Reference: [32] <author> Peng Tu and David Padua. </author> <title> Array Privatization for Shared and Distributed Memory Machines. </title> <booktitle> In Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <pages> pages 64-67, </pages> <month> September </month> <year> 1992. </year> <note> Published in the January, 1993 issue of ACM SIGPLAN Notices. </note>
Reference-contexts: Among the approaches related to that of this paper are those of the Cedar, Kali, and FORTRAN D projects. 5.1 Cedar In the Cedar restructuring compiler, two passes are used to optimize data placement for the hierarchical memory consisting of three levels: local, intermediate or cluster, and global <ref> [9, 32] </ref>. A globalization pass determines which data is shared among more than one cluster; this data is placed in the global memory. A privatization pass then determines which variables have a lifetime of just one loop iteration; these variables can be placed in local memory.
Reference: [33] <author> Sizheng Wei and Saul Levy. </author> <title> Efficient Hierarchical Interconnection for Multiprocessor Systems. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 708-717, </pages> <year> 1992. </year>
Reference-contexts: Architectures which can be considered HSMAs include explicitly hierarchical machines such as the Tree-structured Hierarchical Memory Multiprocessor (THMM)[15, 23], Cedar [11, 19], Mabbs' Hierarchical Memory Structure [21], Wei and Levy's multilevel hierarchy <ref> [33] </ref>, and the hierarchical multiprocessor of Mahgoub and Elmagarmid [22]. Other architectures which may be considered hierarchical using this definition are non-uniform memory access (NUMA) architectures, such as the BBN Butterfly and shared-memory hypercubes [5, 20].
Reference: [34] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Each tile is statically mapped to a particular PE. Optimum "shape" and dimensionality of the tiles should be determined experimentally for the target machine and individual program. Tiling has been discussed in depth elsewhere <ref> [29, 34, 35, 36, 37] </ref>. Associated with each tile is the set of data defined or used within the tile; this set is termed a footprint. The footprints are not necessarily disjoint|often footprints of neighboring tiles overlap.
Reference: [35] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Loop Transformation Theory and an Algorithm to Maximize Parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Each tile is statically mapped to a particular PE. Optimum "shape" and dimensionality of the tiles should be determined experimentally for the target machine and individual program. Tiling has been discussed in depth elsewhere <ref> [29, 34, 35, 36, 37] </ref>. Associated with each tile is the set of data defined or used within the tile; this set is termed a footprint. The footprints are not necessarily disjoint|often footprints of neighboring tiles overlap. <p> These vectors each have just one component in this example since the loop nest is just one level deep. In other 2 A unimodular transformation maps an n-dimensional iteration space into an n-dimensional iteration space, has only integral components, and has determinant 1. See <ref> [35] </ref> for a detailed introduction to the use of unimodular transformations in the context of parallelizing compilers. 4 2: for i := 1,n do 4: c [i] := a [i-1]*2 2: endfor flow dependence 3: --&gt; 4:(&lt;) (1) examples, an asterisk indicates unavailable (or inapplicable) information.
Reference: [36] <author> Michael Wolfe. </author> <title> Iteration Space Tiling for Memory Hierarchies. In Parallel Processing for Scientific Computing. </title> <publisher> SIAM, </publisher> <year> 1987. </year> <note> Published 1989. </note>
Reference-contexts: Each tile is statically mapped to a particular PE. Optimum "shape" and dimensionality of the tiles should be determined experimentally for the target machine and individual program. Tiling has been discussed in depth elsewhere <ref> [29, 34, 35, 36, 37] </ref>. Associated with each tile is the set of data defined or used within the tile; this set is termed a footprint. The footprints are not necessarily disjoint|often footprints of neighboring tiles overlap.
Reference: [37] <author> Michael Wolfe. </author> <title> More Iteration Space Tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Each tile is statically mapped to a particular PE. Optimum "shape" and dimensionality of the tiles should be determined experimentally for the target machine and individual program. Tiling has been discussed in depth elsewhere <ref> [29, 34, 35, 36, 37] </ref>. Associated with each tile is the set of data defined or used within the tile; this set is termed a footprint. The footprints are not necessarily disjoint|often footprints of neighboring tiles overlap.
Reference: [38] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: These methods may be implemented as part of a compiler, but can also be followed by a programmer as a part of his or her programming methodology. 2.1 Background and Notation It is assumed that the reader is familiar with standard parallelizing compiler terminology and techniques as discussed in <ref> [27, 38] </ref>. This paper focuses on nested parallel loops in which subscripts along each dimension of the loop are of the form i k, where i is a loop index and k is a constant; it is not necessary that the loops be perfectly nested. <p> of nesting (it is assumed that the nest is more than one level deep, or that it is a non-nested loop in which any dependence cycles can be broken; otherwise the loop must be executed as a doacross, executed sequentially, or rewritten as a recurrence calculation similar to parallel prefix|see <ref> [1, 38] </ref>). If the loops are not perfectly nested, an attempt is made to obtain outer loop parallelism either directly or via loop interchange. Failing this, an attempt is made to distributed the loop nest and apply unimodular techniques.
Reference: [39] <author> Michael Wolfe. </author> <title> The Tiny Loop Restructuring Research Tool. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 46-53, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: As an example, consider the program fragment and associated dependence information (produced by the Tiny program restructuring tool <ref> [39] </ref>) shown in figure 2. Tiny indicates a flow dependence from S 3 to S 4 ; in parentheses are the direction vector and the distance vector. These vectors each have just one component in this example since the loop nest is just one level deep.
Reference: [40] <author> Pen-Chung Yew, Nian-Feng Tzeng, and Duncan H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 51-58, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Many parallel algorithms (e.g., matrix multiplication, associative operations, solution of partial differential equations by differencing techniques, prefix computations, barrier synchronization, etc.) have data whose movement behaves in a structured, hierarchical manner; such algorithms map well to hierarchical architectures <ref> [6, 15, 24, 28, 31, 40] </ref>. In many algorithms the various data need not be globally shared, but rather can be partially shared among processing elements (PEs)[15]. <p> This splitting and fusing is a generalization of ascend/descend algorithms [28] and software combining <ref> [40, 31] </ref>.
Reference: [41] <author> Chuan-Qi Zhu and Pen-Chung Yew. </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year> <month> 32 </month>
Reference-contexts: This is one characteristic of a partially-shared variable [15]. Additionally, each partially-shared variable has state and data fields associated with it. As in <ref> [41] </ref>, the state field is a generalization of the full/empty bit of the Denelcor HEP [17], being an arbitrary integer rather than just a single bit; the state is used for synchronization: producers set the state of a variable to a predetermined value when they write the datum, and consumers wait <p> Language primitives for communication are also needed; those used here are based upon the atomic instruction proposed by Zhu and Yew <ref> [41] </ref>, with the following form: f address; condition; operation on state; operation on value g. The operational semantics is given in figure 1.
References-found: 41

