URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.lll_case_study.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.lll_case_study.html
Root-URL: 
Title: Lifelong Learning: A Case Study  
Author: Sebastian Thrun 
Note: The author is also affiliated with the  where part of this research was carried out.  
Address: Pittsburgh, PA 15213  Germany,  
Affiliation: School of Computer Science Carnegie Mellon University  Computer Science Department III of the University of Bonn,  
Date: November 1995  
Pubnum: CMU-CS-95-208  
Abstract: This research is sponsored in part by the National Science Foundation under award IRI-9313367, and by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing official policies or endorsements, either expressed or implied, of NSF, Wright Laboratory or the United States Government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ahn, W.-K. and Brewer, W. F. </author> <title> Psychological Studies of Explanation-Based Learning. in: Investigating Explanation-Based Learning, edited by G. DeJong. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston/Dordrecht/London, </address> <year> 1993. </year>
Reference-contexts: To date, virtually all approaches studied in machine learning are concerned with learning a single function based on a single data set only, isolated from a more general learning context. Studying learning in a lifelong context provides the opportunity to transfer knowledge between learning tasks. For example, in <ref> [1, 2] </ref> psychological experiments are reported in which humans acquire complex language concepts based on a single training example. The learning problem studied there involves the distinction of relevant from irrelevant features to generalize the training example. <p> The learning problem studied there involves the distinction of relevant from irrelevant features to generalize the training example. It is shown that humans can spot relevant features very well, even if the number of potentially relevant features is huge and the target concept is rather complex. As argued in <ref> [1, 2] </ref>, the ability to do so relies on previously learned knowledge, which had been acquired earlier in the lifetime of the tested subjects. <p> One way to do this is to learn a comparator d : I fi I ! <ref> [0; 1] </ref> [63]. A comparator d accepts two input patterns, say x and x, and outputs 1 if x and x are members of the same concept, and 0 otherwise. <p> This algorithm is a special version of both the Tangent-Prop algorithm [56] and the explanation-based neural network learning (EBNN) algorithm [34, 61]. Here we will refer to it as EBNN. EBNN approximates f n using an artificial neural network, denoted by h : I ! <ref> [0; 1] </ref>, just like the conventional Back-Propagation approach to supervised learning. However, in addition to the target values given by the training set X n , EBNN also constructs the slopes (tangents) of the target function f n at the examples in X n . <p> The slope r x f n (x) is obtained using d in the following way. Suppose h x; yi 2 X n is a positive training example in X n , i.e., y = 1. Then, the function d x : I ! <ref> [0; 1] </ref>, defined as d x (z) := d (z; x) (9) examples hx 1 ; f n (x 1 )i, hx 2 ; f n (x 2 )i, and hx 3 ; f n (x 3 )i are known. <p> Based on these points the learner might generate the hypothesis h 1 . If the slopes are also known, the learner can do much better: h 2 . maps a single input z pattern to <ref> [0; 1] </ref>, and is an approximation of the target function f n . Since d (z; x) is differentiable, the gradient @d x (z) (10) is defined and is an estimate of the slope of f n at z.
Reference: [2] <author> Ahn, W.-K., Mooney, R., Brewer, W. F., and DeJong, G. F. </author> <title> Schema Acquisition from One Example: Psychological Evidence for Explanation-Based Learning. </title> <booktitle> in: Proceedings of the Ninth Annual Conference of the Cognitive Science Society, edited by . Seattle, </booktitle> <address> WA, </address> <year> 1987, </year> <note> p. </note> . 
Reference-contexts: To date, virtually all approaches studied in machine learning are concerned with learning a single function based on a single data set only, isolated from a more general learning context. Studying learning in a lifelong context provides the opportunity to transfer knowledge between learning tasks. For example, in <ref> [1, 2] </ref> psychological experiments are reported in which humans acquire complex language concepts based on a single training example. The learning problem studied there involves the distinction of relevant from irrelevant features to generalize the training example. <p> The learning problem studied there involves the distinction of relevant from irrelevant features to generalize the training example. It is shown that humans can spot relevant features very well, even if the number of potentially relevant features is huge and the target concept is rather complex. As argued in <ref> [1, 2] </ref>, the ability to do so relies on previously learned knowledge, which had been acquired earlier in the lifetime of the tested subjects.
Reference: [3] <author> Atkeson, C. A. </author> <title> Using Locally Weighted Regression for Robot Learning. </title> <booktitle> in: Proceedings of the 1991 IEEE International Conference on Robotics and Automation, edited by . Sacramento, </booktitle> <address> CA, </address> <year> 1991, </year> <pages> pp. 958-962. </pages>
Reference-contexts: Both approaches are in fact powerful lifelong learning approaches. They illustrate how a carefully designed meta-level bias can improve the recognition rate dramatically, in the domain of face recognition. * Learning distance metrics. Various researchers have proposed methods for adapting the distance metric in memory-based learning <ref> [3, 36, 16, 20] </ref>. Methods for spotting irrelevant features also fall into this category [27, 10]. With the exception of the (aforementioned) algorithm proposed in [36], all these approaches focus exclusively on single learning tasks.
Reference: [4] <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <title> Learning to Act using Real-Time Dynamic Programming. </title> <journal> Artificial Intelligence, </journal> <note> vol. (to appear), p. </note> . 
Reference-contexts: While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning [29, 50, 14, 25] or reinforcement learning <ref> [70, 59, 4, 23] </ref>. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function. This is clearly impractical if the number of support sets is large.
Reference: [5] <author> Baxter, J. </author> <title> Learning Internal Representations. </title> <booktitle> in: Proceedings of the Conference on Computation Learning Theory, edited by . 1995, </booktitle> <address> p. </address> . <note> To appear. </note>
Reference-contexts: In the context of neural network learning, several researchers have proposed methods for learning data representations that are tailored towards the built-in bias of artificial neural networks <ref> [58, 52, 44, 9, 5] </ref>. The basic idea here is the same as in Section 3.3. To re-represent the data, these approaches train a neural network, g : I ! I 0 , which maps input patterns in I to a new space, I 0 . <p> Hence, it is possible to use standard Back-Propagation to tune the weights of the transformation network g, along with the weights of the respective classification network. While some authors [52, 44] have proposed to process the support sets and the training set sequentially, others <ref> [58, 9, 5] </ref> are in favor of training g in parallel, using all n tasks simultaneously. Sequential training offers the advantage that not all training data has to be available at all time. <p> The first algorithm gradually learns a domain-specific data representation, which improves the generalization in memory-based learning. 2. The second algorithm replaces the fixed distance metric in memory-based learning by a domain-specific comparator function, which is learned using previous datasets. 3. The third algorithm (see also <ref> [58, 52, 44, 9, 5, 55] </ref>) learns a domain-specific representation, like the first algorithm, but this representation is tailored towards neural network learning. 4.
Reference: [6] <author> Bergadano, F. and Giordana, A. </author> <title> Guiding Induction with Domain Theories. in: Guiding Induction with Domain Theories, </title> <editor> by F. Bergadano and A. Giordana. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 474-492. </pages>
Reference-contexts: Knowledge-based approaches to machine learning investigate the feasibility of hand-coding prior knowledge into inductive learning approaches. Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., <ref> [6, 41] </ref>). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods [53, 18, 28, 65] basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms.
Reference: [7] <author> Beymer, D., Shashua, A., and Poggio, T. </author> <title> Example Based Image Analysis and Synthesis. </title> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory, </institution> <month> November </month> <year> 1993. </year> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431. </pages>
Reference-contexts: These changes are assumed to be equivalent for all faceshence they can be used to project new faces back into a canonical (frontal) view, in which they are easier to recognize. Beymer and his coauthors <ref> [7] </ref> propose to learn the parameters for the rotation and change in face expression directly, using a supervised learning scheme. Both approaches are in fact powerful lifelong learning approaches.
Reference: [8] <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> vol. 24 (1987), </volume> <pages> pp. 377-380. </pages>
Reference-contexts: PAC-Learning extends Vapnik's approach to empirical risk minimization [68] by an additional computational complexity argument. The following standard result by Blumer and colleagues relates the size of the hypothesis space and the number of (noise-free) training examples required for learning a function: 23 Theorem <ref> [8] </ref>. Given a function f n in a space of functions H , the probability that any hypothesis h 2 H with error larger than " is consistent with f n on a (noise-free) dataset of size N is less than (1 ") N jH j.
Reference: [9] <author> Caruana, R. </author> <title> Multitask Learning: A Knowledge-Based of Source of Inductive Bias. </title> <booktitle> in: Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <editor> edited by P. E. Utgoff. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 41-48. </pages>
Reference-contexts: In the context of neural network learning, several researchers have proposed methods for learning data representations that are tailored towards the built-in bias of artificial neural networks <ref> [58, 52, 44, 9, 5] </ref>. The basic idea here is the same as in Section 3.3. To re-represent the data, these approaches train a neural network, g : I ! I 0 , which maps input patterns in I to a new space, I 0 . <p> Hence, it is possible to use standard Back-Propagation to tune the weights of the transformation network g, along with the weights of the respective classification network. While some authors [52, 44] have proposed to process the support sets and the training set sequentially, others <ref> [58, 9, 5] </ref> are in favor of training g in parallel, using all n tasks simultaneously. Sequential training offers the advantage that not all training data has to be available at all time. <p> The first algorithm gradually learns a domain-specific data representation, which improves the generalization in memory-based learning. 2. The second algorithm replaces the fixed distance metric in memory-based learning by a domain-specific comparator function, which is learned using previous datasets. 3. The third algorithm (see also <ref> [58, 52, 44, 9, 5, 55] </ref>) learns a domain-specific representation, like the first algorithm, but this representation is tailored towards neural network learning. 4.
Reference: [10] <author> Caruana, R. and Freitag, D. </author> <title> Greedy Attribute Selection. </title> <booktitle> in: Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <publisher> edited by . Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994, </year> <note> p. </note> . 
Reference-contexts: Various researchers have proposed methods for adapting the distance metric in memory-based learning [3, 36, 16, 20]. Methods for spotting irrelevant features also fall into this category <ref> [27, 10] </ref>. With the exception of the (aforementioned) algorithm proposed in [36], all these approaches focus exclusively on single learning tasks. However, they could potentially be applied to lifelong learning, and so provide a good basis for research on lifelong learning.
Reference: [11] <editor> Investigating Explanation-Based Learning. edited by G. DeJong. </editor> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1993. </year> <month> 30 </month>
Reference: [12] <author> DeJong, G. and Mooney, R. </author> <title> Explanation-Based Learning: An Alternative View. </title> <journal> Machine Learning, </journal> <volume> vol. 1 (1986), </volume> <pages> pp. 145-176. </pages>
Reference-contexts: The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain. In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules <ref> [33, 12, 65, 41, 40, 38] </ref>. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning [45, 17, 22], to name two popular examples.
Reference: [13] <author> Ehrenfeucht, A., Haussler, D., Kearns, M., and Valiant, L. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> vol. 82 (1989), </volume> <pages> pp. 247-261. </pages>
Reference-contexts: It also holds independently of the choice of f n and the sampling distribution, as long as this distribution is the same during training and testing. Notice that (12) is logarithmic in the hypothesis set size jH j. An analogous logarithmic lower bound can be found in <ref> [13, 24] </ref>. By applying the Lemma to Blumer et al.'s Theorem (12), the advantage of smaller hypothesis spaces can be expressed as the reduction in the sampling complexity when learning the n-th function. Corollary.
Reference: [14] <author> Fisher, D. H. </author> <title> Knowledge Acquisition Via Incremental Conceptual Clustering. </title> <journal> Machine Learning, </journal> <volume> vol. 2 (1987), </volume> <pages> pp. 139-172. </pages>
Reference-contexts: This paper exclusively address concept learning problems, which are a version of supervised learning involving only two output values. While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning <ref> [29, 50, 14, 25] </ref> or reinforcement learning [70, 59, 4, 23]. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function.
Reference: [15] <author> Franke, R. </author> <title> Scattered Data Interpolation: Tests of Some Methods. </title> <journal> Mathematics of Computation, </journal> <volume> vol. 38 (1982), </volume> <pages> pp. 181-200. </pages>
Reference-contexts: In what follows, we will first sketch two well-known approaches to memory-based learning, then propose meta-level components that take the support sets into account. 3.1 Nearest Neighbor Probably the most widely used memory-based learning algorithm is K-nearest neighbor (KNN) <ref> [15, 57] </ref>. Suppose x is a query pattern, for which we would like to know the output y = f n (x).
Reference: [16] <author> Friedman, J. H. </author> <title> Flexible Metric Nearest Neighbor Classification. </title> <institution> Department of Statistics and Linear Accelerator Center, Stanford University, Stanford. </institution> <address> CA 94305, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Both approaches are in fact powerful lifelong learning approaches. They illustrate how a carefully designed meta-level bias can improve the recognition rate dramatically, in the domain of face recognition. * Learning distance metrics. Various researchers have proposed methods for adapting the distance metric in memory-based learning <ref> [3, 36, 16, 20] </ref>. Methods for spotting irrelevant features also fall into this category [27, 10]. With the exception of the (aforementioned) algorithm proposed in [36], all these approaches focus exclusively on single learning tasks.
Reference: [17] <author> Friedman, J. H. </author> <title> Multivariate Adaptive Regression Splines. </title> <journal> Annals of Statistics, </journal> <volume> vol. 19 (1991), </volume> <pages> pp. 1-141. </pages>
Reference-contexts: In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning <ref> [45, 17, 22] </ref>, to name two popular examples. All these approaches have in common that the available data consists exclusively of input-output examples of the target function.
Reference: [18] <author> Fu, L.-M. </author> <title> Integration of Neural Heuristics into Knowledge-Based Inference. </title> <journal> Connection Science, </journal> <volume> vol. 1 (1989), </volume> <pages> pp. 325-339. </pages>
Reference-contexts: Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., [6, 41]). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods <ref> [53, 18, 28, 65] </ref> basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms. All these approaches are related to the work reported here, since they employ prior knowledge to reduce the sample complexity.
Reference: [19] <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <title> Neural Networks and the Bias/Variance Dilemma. </title> <journal> Neural Computation, </journal> <volume> vol. 4 (1992), </volume> <pages> pp. 1-58. </pages>
Reference-contexts: Since deducing the output of unseen, future data is impossible without making further assumptions <ref> [31, 68, 19, 73] </ref>, every learning algorithm makes inherent assumptions concerning the nature of the data. These assumptionsoften referred to as hypothesis space, preferences, or prior, and henceforth called bias [30] enables an algorithm to favor one particular generalization over all others, hence to generalize. <p> It is well-known that the complexity of the base-level hypothesis space is related to the number of training examples required for base-level learning (see e.g., <ref> [32, 68, 19, 24] </ref>). One learning model, which recently has received considerable attention in the computational learning theory community, is Valiant's PAC-learning model [67] (PAC stands for probably approximately correct). PAC-Learning extends Vapnik's approach to empirical risk minimization [68] by an additional computational complexity argument.
Reference: [20] <author> Hastie, T. and Tibshirani, R. </author> <title> Discriminant Adaptive Nearest Neighbor Classification. </title> <institution> Dept. of Statistics and Biostatistics, Stanford University, Stanford, </institution> <address> CA, </address> <month> December </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: Both approaches are in fact powerful lifelong learning approaches. They illustrate how a carefully designed meta-level bias can improve the recognition rate dramatically, in the domain of face recognition. * Learning distance metrics. Various researchers have proposed methods for adapting the distance metric in memory-based learning <ref> [3, 36, 16, 20] </ref>. Methods for spotting irrelevant features also fall into this category [27, 10]. With the exception of the (aforementioned) algorithm proposed in [36], all these approaches focus exclusively on single learning tasks.
Reference: [21] <author> Hild, H. and Waibel, A. </author> <title> Multi-Speaker/Speaker-Independent Architectures for the Multi-State Time Delay Neural Network. </title> <booktitle> in: Proceedings of the International Conference on Acoustics, Speech and Signal Processing, IEEE, edited by . 1993, </booktitle> <pages> pp. II 255-258. </pages>
Reference-contexts: Other methods, that fit neither of these categories, improve the generalization accuracy of an inductive machine learning algorithm by generating additional training data based on domain knowledge [43], adapt data of multiple tasks to fit a single-task description <ref> [21] </ref>, or provide more flexible mechanisms to encode known invariances of the domain [56]. 8 Conclusion This paper studies approaches to lifelong learning. In lifelong learning, the learner faces a collection of learning tasks over its entire lifetime.
Reference: [22] <author> Jordan, M. I. and Jacobs, R. A. </author> <title> Hierarchies of adaptive experts. </title> <booktitle> in: Advances in Neural Information Processing Systems 4, </booktitle> <editor> edited by J. E. Moody, S. J. Hanson, and R. P. Lippmann. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992, </year> <pages> pp. 985-992. </pages>
Reference-contexts: In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning <ref> [45, 17, 22] </ref>, to name two popular examples. All these approaches have in common that the available data consists exclusively of input-output examples of the target function.
Reference: [23] <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <title> An Introduction to Reinforcement Learning. in: The Biology and Technology of Intelligent Autonomous Agents, edited by L. Steels. </title> <publisher> Springer Publishers, </publisher> <address> Berlin, Hei-delberg, </address> <year> 1995, </year> <pages> pp. 90-127. 31 </pages>
Reference-contexts: While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning [29, 50, 14, 25] or reinforcement learning <ref> [70, 59, 4, 23] </ref>. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function. This is clearly impractical if the number of support sets is large.
Reference: [24] <author> Kearns, M. and Vazirani, U. </author> <title> Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: It is well-known that the complexity of the base-level hypothesis space is related to the number of training examples required for base-level learning (see e.g., <ref> [32, 68, 19, 24] </ref>). One learning model, which recently has received considerable attention in the computational learning theory community, is Valiant's PAC-learning model [67] (PAC stands for probably approximately correct). PAC-Learning extends Vapnik's approach to empirical risk minimization [68] by an additional computational complexity argument. <p> It also holds independently of the choice of f n and the sampling distribution, as long as this distribution is the same during training and testing. Notice that (12) is logarithmic in the hypothesis set size jH j. An analogous logarithmic lower bound can be found in <ref> [13, 24] </ref>. By applying the Lemma to Blumer et al.'s Theorem (12), the advantage of smaller hypothesis spaces can be expressed as the reduction in the sampling complexity when learning the n-th function. Corollary.
Reference: [25] <author> Kohonen, T. </author> <title> Self-Organization and Associative Memory, </title> <booktitle> 2nd. </booktitle> <address> edition. </address> <publisher> Springer, </publisher> <address> Berlin New York, </address> <year> 1988. </year>
Reference-contexts: This paper exclusively address concept learning problems, which are a version of supervised learning involving only two output values. While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning <ref> [29, 50, 14, 25] </ref> or reinforcement learning [70, 59, 4, 23]. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function.
Reference: [26] <author> Lando, M. and Edelman, S. </author> <title> Generalizing from a single view in face recognition. no. </title> <type> CS-TR 95-02, </type> <institution> Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: In these experiments, it is shown empirically that the recognition rate of faces in an upright position is significantly better than that of faces in an inverted position. As argued there and in <ref> [26] </ref>, this finding provides evidence that humans can transfer knowledge for the recognition of faces across different face recognition tasksunless the human visual system is genetically pre-biased to the recognition of upright human faces (in which case evolution learned a good strategy for us). <p> This is because their meta-level hypothesis spaces comprise only of a considerably small number base-level learning parameters. * Learning invariances in face recognition. In the face recognition context, techniques exist for learning the directions (sub-manifolds) along which face images are invariant. In <ref> [26] </ref>, this is done by learning changes in activations when faces are rotated or translated, in a specific internal representational space.
Reference: [27] <author> Littlestone, N. </author> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm. </title> <journal> Machine Learning, </journal> <volume> vol. 2 (1987), </volume> <pages> pp. 285-318. </pages>
Reference-contexts: Various researchers have proposed methods for adapting the distance metric in memory-based learning [3, 36, 16, 20]. Methods for spotting irrelevant features also fall into this category <ref> [27, 10] </ref>. With the exception of the (aforementioned) algorithm proposed in [36], all these approaches focus exclusively on single learning tasks. However, they could potentially be applied to lifelong learning, and so provide a good basis for research on lifelong learning.
Reference: [28] <author> Mahoney, J. J. and Mooney, R. J. </author> <title> Combining Symbolic and Neural Learning to Revise Probabilistic Theories. </title> <booktitle> in: Proceedings of the 1992 Machine Learning Workshop on Integrated Learning in Real Domains, edited by . Aberdeen Scotland, 1992, </booktitle> <address> p. </address> . 
Reference-contexts: Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., [6, 41]). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods <ref> [53, 18, 28, 65] </ref> basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms. All these approaches are related to the work reported here, since they employ prior knowledge to reduce the sample complexity.
Reference: [29] <author> Michalski, R. S. </author> <title> Knowledge acquisition through conceptual clustering: A theoretical framework and algorithm for partitioning data into conjunctive concepts. </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> vol. 4 (1980), </volume> <pages> pp. 219-243. </pages>
Reference-contexts: This paper exclusively address concept learning problems, which are a version of supervised learning involving only two output values. While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning <ref> [29, 50, 14, 25] </ref> or reinforcement learning [70, 59, 4, 23]. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function.
Reference: [30] <author> Mitchell, T. M. </author> <title> Generalization as Search. </title> <journal> Artificial Intelligence, </journal> <volume> vol. 18 (1982), </volume> <pages> pp. 203-226. </pages>
Reference-contexts: Since deducing the output of unseen, future data is impossible without making further assumptions [31, 68, 19, 73], every learning algorithm makes inherent assumptions concerning the nature of the data. These assumptionsoften referred to as hypothesis space, preferences, or prior, and henceforth called bias <ref> [30] </ref> enables an algorithm to favor one particular generalization over all others, hence to generalize. The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain.
Reference: [31] <author> Mitchell, T. M. </author> <title> The Need for Biases in Learning Generalizations. no. </title> <institution> CBM-TR-117, Computer Science Department, Rutgers University, </institution> <address> New Brunswick, NJ 08904, </address> <year> 1980. </year> <note> Also appeared in: Readings in Machine Learning, </note> <editor> J. Shavlik and T.G. Dietterich (eds.), </editor> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Since deducing the output of unseen, future data is impossible without making further assumptions <ref> [31, 68, 19, 73] </ref>, every learning algorithm makes inherent assumptions concerning the nature of the data. These assumptionsoften referred to as hypothesis space, preferences, or prior, and henceforth called bias [30] enables an algorithm to favor one particular generalization over all others, hence to generalize.
Reference: [32] <author> Mitchell, T. M. </author> <title> Version Spaces: An approach to concept learning. </title> <institution> Stanford University, California, </institution> <month> December </month> <year> 1978. </year> <note> Also Stanford CS Report STAN-CS-78-711, HPP-79-2. </note>
Reference-contexts: A simple example of learning bias is shown in Figure 1. Different biases are represented by different hypothesis sets <ref> [32] </ref> (preferences within these hypothesis sets are ignored to simplify the presentation). <p> It is well-known that the complexity of the base-level hypothesis space is related to the number of training examples required for base-level learning (see e.g., <ref> [32, 68, 19, 24] </ref>). One learning model, which recently has received considerable attention in the computational learning theory community, is Valiant's PAC-learning model [67] (PAC stands for probably approximately correct). PAC-Learning extends Vapnik's approach to empirical risk minimization [68] by an additional computational complexity argument. <p> VBMS chooses the most appropriate algorithm out of a pool of conventional inductive learning algorithms based on previous, related learning tasks. A related approach, the STABB algorithm [66], is able shift gradually towards weaker bias. Bias is represented by a restriction on the hypothesis space <ref> [32] </ref>. Whenever the hypothesis class cannot match the training examples exactly, STABB analyzes this failure and enlarges the hypothesis space correspondingly. STABB could potentially be applied to noise-free lifelong concept learning tasks. In [36] an approach is described that estimates a variety of learning parameters using cross-validation.
Reference: [33] <author> Mitchell, T. M., Keller, R., and Kedar-Cabelli, S. </author> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning, </journal> <volume> vol. 1 (1986), </volume> <pages> pp. 47-80. </pages>
Reference-contexts: The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain. In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules <ref> [33, 12, 65, 41, 40, 38] </ref>. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning [45, 17, 22], to name two popular examples.
Reference: [34] <author> Mitchell, T. M. and Thrun, S. </author> <title> Explanation-Based Neural Network Learning for Robot Control. </title> <booktitle> in: Advances in Neural Information Processing Systems 5, </booktitle> <editor> edited by S. J. Hanson, J. Cowan, and C. L. Giles. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 287-294. 32 </pages>
Reference-contexts: This algorithm is a special version of both the Tangent-Prop algorithm [56] and the explanation-based neural network learning (EBNN) algorithm <ref> [34, 61] </ref>. Here we will refer to it as EBNN. EBNN approximates f n using an artificial neural network, denoted by h : I ! [0; 1], just like the conventional Back-Propagation approach to supervised learning. <p> Hence, even in the presence of a poor comparator d, the built-in bias of neural network Back-Propagation is conceivably able to override errors in the meta-level knowledgean effect that was confirmed by extensive studies in other application domains <ref> [39, 34] </ref>. The results shown in Figure 9 confirm our expectations. The results for EBNN, shown in the left diagram, are approximately the same as long as support sets are available (approximately 74% generalization accuracy). Hence, even a poorly trained comparator d still improves the overall generalization accuracy in EBNN.
Reference: [35] <author> Moore, A. W. </author> <title> Efficient Memory-based Learning for Robot Control. </title> <publisher> Trinity Hall, </publisher> <address> University of Cambridge, England, </address> <year> 1990. </year>
Reference-contexts: Memory-based approaches memorize all training examples explicitly, and interpolate between them at query-time. Notice that memory-based learning has been applied with significant success to a variety of challenging learning problems <ref> [35, 51, 69] </ref>. In what follows, we will first sketch two well-known approaches to memory-based learning, then propose meta-level components that take the support sets into account. 3.1 Nearest Neighbor Probably the most widely used memory-based learning algorithm is K-nearest neighbor (KNN) [15, 57].
Reference: [36] <author> Moore, A. W., Hill, D. J., and Johnson, M. P. </author> <title> An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators. </title> <booktitle> in: Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3, </volume> <editor> edited by S. Hanson, S. Judd, and T. Petsche. </editor> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Bias is represented by a restriction on the hypothesis space [32]. Whenever the hypothesis class cannot match the training examples exactly, STABB analyzes this failure and enlarges the hypothesis space correspondingly. STABB could potentially be applied to noise-free lifelong concept learning tasks. In <ref> [36] </ref> an approach is described that estimates a variety of learning parameters using cross-validation. In particular their approach used yesterday's training data to tune the learning parameters for today's learning experiments. <p> Both approaches are in fact powerful lifelong learning approaches. They illustrate how a carefully designed meta-level bias can improve the recognition rate dramatically, in the domain of face recognition. * Learning distance metrics. Various researchers have proposed methods for adapting the distance metric in memory-based learning <ref> [3, 36, 16, 20] </ref>. Methods for spotting irrelevant features also fall into this category [27, 10]. With the exception of the (aforementioned) algorithm proposed in [36], all these approaches focus exclusively on single learning tasks. <p> Various researchers have proposed methods for adapting the distance metric in memory-based learning [3, 36, 16, 20]. Methods for spotting irrelevant features also fall into this category [27, 10]. With the exception of the (aforementioned) algorithm proposed in <ref> [36] </ref>, all these approaches focus exclusively on single learning tasks. However, they could potentially be applied to lifelong learning, and so provide a good basis for research on lifelong learning.
Reference: [37] <author> Moses, Y., Ullman, S., and Edelman, S. </author> <title> Generalization across changes in illumination and viewing position in upright and inverted faces. no. </title> <type> CS-TR 93-14, </type> <institution> Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel, </institution> <year> 1993. </year>
Reference-contexts: As argued in [1, 2], the ability to do so relies on previously learned knowledge, which had been acquired earlier in the lifetime of the tested subjects. Another recent study 1 <ref> [37] </ref> illustrates that humans employ very specific routines for the robust recognition of human faces, so that they are able to learn to recognize new faces from very few training examples.
Reference: [38] <author> Muggelton, S. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain. In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules <ref> [33, 12, 65, 41, 40, 38] </ref>. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning [45, 17, 22], to name two popular examples.
Reference: [39] <author> O'Sullivan, J., Mitchell, T. M., and Thrun, S. </author> <title> Explanation-Based Neural Network Learning from Mobile Robot Perception. in: Symbolic Visual Learning, edited by K. </title> <editor> Ikeuchi and M. Veloso. </editor> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: Hence, even in the presence of a poor comparator d, the built-in bias of neural network Back-Propagation is conceivably able to override errors in the meta-level knowledgean effect that was confirmed by extensive studies in other application domains <ref> [39, 34] </ref>. The results shown in Figure 9 confirm our expectations. The results for EBNN, shown in the left diagram, are approximately the same as long as support sets are available (approximately 74% generalization accuracy). Hence, even a poorly trained comparator d still improves the overall generalization accuracy in EBNN.
Reference: [40] <author> Ourston, D. and Mooney, R. J. </author> <title> Theory Refinement with Noisy Data. no. </title> <type> AI 91-153, </type> <institution> Artificial Intelligence Lab, University of Texas at Austin, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain. In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules <ref> [33, 12, 65, 41, 40, 38] </ref>. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning [45, 17, 22], to name two popular examples. <p> Knowledge-based approaches to machine learning investigate the feasibility of hand-coding prior knowledge into inductive learning approaches. Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., [6, 41]). For example, EITHER <ref> [40] </ref> inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods [53, 18, 28, 65] basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms.
Reference: [41] <author> Pazzani, M. J., Brunk, C. A., and Silverstein, G. </author> <title> A knowledge-intensive approach to learning relational concepts. </title> <booktitle> in: Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Evanston, IL, </address> <year> 1991, </year> <pages> pp. 432-436. </pages>
Reference-contexts: The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain. In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules <ref> [33, 12, 65, 41, 40, 38] </ref>. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning [45, 17, 22], to name two popular examples. <p> Knowledge-based approaches to machine learning investigate the feasibility of hand-coding prior knowledge into inductive learning approaches. Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., <ref> [6, 41] </ref>). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods [53, 18, 28, 65] basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms.
Reference: [42] <author> Pearl, J. </author> <title> Probabilistic reasoning in intelligent systems: networks of plausible inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Obviously, Equation (6) delivers the right answer when only a single positive training example is available. If multiple examples are available in X n , their votes can be combined using Bayes' rule <ref> [42] </ref>, leading to Bel (f n (x)=1) := 1 1 + h x; y=1i2X n 1 d (x; x) The somewhat lengthy derivation of (7), which is given in [61], is straightforward if one interprets the output of d as a conditional probability for the class of a query point x
Reference: [43] <author> Pomerleau, D. A. </author> <title> Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving. in: Robot Learning, edited by J. </title> <editor> H. Connell and S. Mahadevan. </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993, </year> <pages> pp. 19-43. </pages>
Reference-contexts: Neural networks have been applied successfully to a variety of real-world learning problems <ref> [47, 43, 49] </ref>. 4.1 Back-Propagation Probably the most common way to learn a function f n : &lt; d ! f0; 1g with an artificial neural network is to approximate it using the Back-Propagation algorithm (or a variation thereof). <p> Lifelong learning approaches can be viewed as knowledge-based approaches that instead learn domain knowledge. * Other methods. Other methods, that fit neither of these categories, improve the generalization accuracy of an inductive machine learning algorithm by generating additional training data based on domain knowledge <ref> [43] </ref>, adapt data of multiple tasks to fit a single-task description [21], or provide more flexible mechanisms to encode known invariances of the domain [56]. 8 Conclusion This paper studies approaches to lifelong learning. In lifelong learning, the learner faces a collection of learning tasks over its entire lifetime.
Reference: [44] <author> Pratt, L. Y. </author> <title> Transferring Previously Learned Back-Propagation Neural Networks to New Learning Tasks. </title> <institution> Rutgers University, Department of Computer Science, </institution> <address> New Brunswick, NJ 08904, </address> <month> May </month> <year> 1993. </year> <note> also appeared as Technical Report ML-TR-37. 33 </note>
Reference-contexts: In the context of neural network learning, several researchers have proposed methods for learning data representations that are tailored towards the built-in bias of artificial neural networks <ref> [58, 52, 44, 9, 5] </ref>. The basic idea here is the same as in Section 3.3. To re-represent the data, these approaches train a neural network, g : I ! I 0 , which maps input patterns in I to a new space, I 0 . <p> Hence, it is possible to use standard Back-Propagation to tune the weights of the transformation network g, along with the weights of the respective classification network. While some authors <ref> [52, 44] </ref> have proposed to process the support sets and the training set sequentially, others [58, 9, 5] are in favor of training g in parallel, using all n tasks simultaneously. Sequential training offers the advantage that not all training data has to be available at all time. <p> The first algorithm gradually learns a domain-specific data representation, which improves the generalization in memory-based learning. 2. The second algorithm replaces the fixed distance metric in memory-based learning by a domain-specific comparator function, which is learned using previous datasets. 3. The third algorithm (see also <ref> [58, 52, 44, 9, 5, 55] </ref>) learns a domain-specific representation, like the first algorithm, but this representation is tailored towards neural network learning. 4.
Reference: [45] <author> Quinlan, J. R. </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> vol. 1 (1986), </volume> <pages> pp. 81-106. </pages>
Reference-contexts: In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning <ref> [45, 17, 22] </ref>, to name two popular examples. All these approaches have in common that the available data consists exclusively of input-output examples of the target function. <p> Knowledge-based approaches to machine learning investigate the feasibility of hand-coding prior knowledge into inductive learning approaches. Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., [6, 41]). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 <ref> [45] </ref> as the inductive component. Neural network-based methods [53, 18, 28, 65] basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms. All these approaches are related to the work reported here, since they employ prior knowledge to reduce the sample complexity.
Reference: [46] <author> Rendell, L., Seshu, R., and Tcheng, D. </author> <title> Layered Concept-Learning and Dynamically-Variable Bias Management. </title> <booktitle> in: Proceedings of IJCAI-87. </booktitle> <year> 1987, </year> <pages> pp. 308-314. </pages>
Reference-contexts: Since previous learning tasks also are sampled from F , learning that H 4 is the best bias in fH 0 ; H 1 ; : : : ; H 4 g appears to be feasible. Following the terminology in <ref> [46] </ref>, we will refer to the problem of learning bias as the meta-level learning problem. <p> They can roughly be grouped into the following categories. * Choosing learning parameters and algorithms. One of the earliest approaches that is able to learn at the meta-level is the VBMS system <ref> [46] </ref>. VBMS chooses the most appropriate algorithm out of a pool of conventional inductive learning algorithms based on previous, related learning tasks. A related approach, the STABB algorithm [66], is able shift gradually towards weaker bias. Bias is represented by a restriction on the hypothesis space [32].
Reference: [47] <author> Rennie, J. </author> <title> Cancer Catcher: Neural Net catches errors that slip through pap tests. </title> <journal> Scientific American, </journal> <volume> vol. </volume> <month> 262 </month> <year> (1990). </year>
Reference-contexts: Neural networks have been applied successfully to a variety of real-world learning problems <ref> [47, 43, 49] </ref>. 4.1 Back-Propagation Probably the most common way to learn a function f n : &lt; d ! f0; 1g with an artificial neural network is to approximate it using the Back-Propagation algorithm (or a variation thereof).
Reference: [48] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> Learning Internal Representations by Error Propagation. </title> <booktitle> in: Parallel Distributed Processing. </booktitle> <volume> Vol. I + II, </volume> <editor> edited by D. E. Rumelhart and J. L. McClelland. </editor> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules [33, 12, 65, 41, 40, 38]. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation <ref> [72, 71, 48] </ref> or inductive tree learning [45, 17, 22], to name two popular examples. All these approaches have in common that the available data consists exclusively of input-output examples of the target function. <p> Memory-based learning is then performed on the re-represented training set fhg (x); yig (with X = fhx; yig). In our implementation, g is realized by an artificial neural network and trained using the Back-Propagation algorithm <ref> [48] </ref>. It is important to notice that the transformation g is obtained using the support sets. In the object recognition example described in Section 1, g willin the ideal casemap images of the same object to an identical representation, regardless of where in the original image the object appears.
Reference: [49] <author> Rumelhart, D. E., Widrow, B., and Lehr, M. A. </author> <title> The basic Ideas in Neural Networks. </title> <journal> Communications of the ACM, </journal> <volume> vol. 37 (1994), </volume> <pages> pp. 87-92. </pages>
Reference-contexts: Neural networks have been applied successfully to a variety of real-world learning problems <ref> [47, 43, 49] </ref>. 4.1 Back-Propagation Probably the most common way to learn a function f n : &lt; d ! f0; 1g with an artificial neural network is to approximate it using the Back-Propagation algorithm (or a variation thereof).
Reference: [50] <author> Rumelhart, D. E. and Zipser, D. </author> <title> Feature Discovery by Competitive Learning. </title> <booktitle> in: Parallel Distributed Processing. </booktitle> <volume> Vol. I + II, </volume> <editor> edited by D. E. Rumelhart and J. L. McClelland. </editor> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This paper exclusively address concept learning problems, which are a version of supervised learning involving only two output values. While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning <ref> [29, 50, 14, 25] </ref> or reinforcement learning [70, 59, 4, 23]. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function.
Reference: [51] <author> Schaal, S. and Atkeson, C. G. </author> <title> Robot Learning By Nonparametric Regression. </title> <booktitle> in: Proceedings of the IEEE/RSJ/GI International Conference on Intelligent Robots and Systems, edited by . 1994, </booktitle> <pages> pp. 478-485. </pages>
Reference-contexts: Memory-based approaches memorize all training examples explicitly, and interpolate between them at query-time. Notice that memory-based learning has been applied with significant success to a variety of challenging learning problems <ref> [35, 51, 69] </ref>. In what follows, we will first sketch two well-known approaches to memory-based learning, then propose meta-level components that take the support sets into account. 3.1 Nearest Neighbor Probably the most widely used memory-based learning algorithm is K-nearest neighbor (KNN) [15, 57].
Reference: [52] <author> Sharkey, N. E. and Sharkey, A. J. C. </author> <title> Adaptive Generalization and the Transfer of Knowledge. </title> <booktitle> in: Proceedings of the Second Irish Neural Networks Conference, edited by . Belfast, 1992, </booktitle> <address> p. </address> . 
Reference-contexts: In the context of neural network learning, several researchers have proposed methods for learning data representations that are tailored towards the built-in bias of artificial neural networks <ref> [58, 52, 44, 9, 5] </ref>. The basic idea here is the same as in Section 3.3. To re-represent the data, these approaches train a neural network, g : I ! I 0 , which maps input patterns in I to a new space, I 0 . <p> Hence, it is possible to use standard Back-Propagation to tune the weights of the transformation network g, along with the weights of the respective classification network. While some authors <ref> [52, 44] </ref> have proposed to process the support sets and the training set sequentially, others [58, 9, 5] are in favor of training g in parallel, using all n tasks simultaneously. Sequential training offers the advantage that not all training data has to be available at all time. <p> The first algorithm gradually learns a domain-specific data representation, which improves the generalization in memory-based learning. 2. The second algorithm replaces the fixed distance metric in memory-based learning by a domain-specific comparator function, which is learned using previous datasets. 3. The third algorithm (see also <ref> [58, 52, 44, 9, 5, 55] </ref>) learns a domain-specific representation, like the first algorithm, but this representation is tailored towards neural network learning. 4.
Reference: [53] <author> Shavlik, J. W. and Towell, G. G. </author> <title> An approach to combining Explanation-based and Neural Learning Algorithms. </title> <journal> Connection Science, </journal> <volume> vol. 1 (1989), </volume> <pages> pp. 231-253. </pages>
Reference-contexts: Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., [6, 41]). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods <ref> [53, 18, 28, 65] </ref> basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms. All these approaches are related to the work reported here, since they employ prior knowledge to reduce the sample complexity.
Reference: [54] <author> Shepard, D. </author> <title> A Two-Dimensional Interpolation Function for Irregularly Spaced Data. </title> <booktitle> in: 23rd National Conference ACM, edited by . 1968, </booktitle> <pages> pp. 517-523. </pages>
Reference-contexts: In the context of concept learning, KNN returns the majority vote of the K nearest neighbors: 1 X i where (z) := ( 0 if z 0:5 3.2 Shepard's Method Another popular method is due to Shepard <ref> [54] </ref>. When computing the y for a query point x, Shepard's method averages the output values of all training examples in X n .
Reference: [55] <author> Silver, D. and Mercer, R. </author> <title> Toward a model of consolidation: The retention and transfer of neural net task knowledge. </title> <booktitle> in: Proceedings of the INNS World Congress on Neural Networks, edited by . Washington, </booktitle> <address> DC, </address> <year> 1995, </year> <pages> pp. 164-169, </pages> <booktitle> Volume III. </booktitle> <pages> 34 </pages>
Reference-contexts: The first algorithm gradually learns a domain-specific data representation, which improves the generalization in memory-based learning. 2. The second algorithm replaces the fixed distance metric in memory-based learning by a domain-specific comparator function, which is learned using previous datasets. 3. The third algorithm (see also <ref> [58, 52, 44, 9, 5, 55] </ref>) learns a domain-specific representation, like the first algorithm, but this representation is tailored towards neural network learning. 4.
Reference: [56] <author> Simard, P., Victorri, B., LeCun, Y., and Denker, J. </author> <title> Tangent Prop A Formalism for Specifying Selected Invariances in an Adaptive Network. </title> <booktitle> in: Advances in Neural Information Processing Systems 4, </booktitle> <editor> edited by J. E. Moody, S. J. Hanson, and R. P. Lippmann. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992, </year> <pages> pp. 895-903. </pages>
Reference-contexts: Both strategies learn at the meta-level through developing new data representations. 4.3 Explanation-Based Neural Network Learning The remainder of this section describes a hybrid neural network learning algorithm for learning f n . This algorithm is a special version of both the Tangent-Prop algorithm <ref> [56] </ref> and the explanation-based neural network learning (EBNN) algorithm [34, 61]. Here we will refer to it as EBNN. EBNN approximates f n using an artificial neural network, denoted by h : I ! [0; 1], just like the conventional Back-Propagation approach to supervised learning. <p> When refining the weights of the target network that approximates f n , for each training example x 2 X n both the target value f n (x) and the slope vector r x f n (x) are approximated using the Tangent-Prop algorithm <ref> [56] </ref>. The slope r x f n , if correct, provides additional information about the target function f n . Since d is learned using the support sets, the EBNN approach transfers knowledge from the support sets to the new learning task. <p> methods, that fit neither of these categories, improve the generalization accuracy of an inductive machine learning algorithm by generating additional training data based on domain knowledge [43], adapt data of multiple tasks to fit a single-task description [21], or provide more flexible mechanisms to encode known invariances of the domain <ref> [56] </ref>. 8 Conclusion This paper studies approaches to lifelong learning. In lifelong learning, the learner faces a collection of learning tasks over its entire lifetime. When faced with the n-th thing to learn, knowledge acquired in the previous n 1 learning tasks can be used to bias learning the n-th.
Reference: [57] <author> Stanfill, C. and Waltz, D. </author> <title> Towards Memory-Based Reasoning. </title> <journal> Communications of the ACM, </journal> <volume> vol. 29 (1986), </volume> <pages> pp. 1213-1228. </pages>
Reference-contexts: In what follows, we will first sketch two well-known approaches to memory-based learning, then propose meta-level components that take the support sets into account. 3.1 Nearest Neighbor Probably the most widely used memory-based learning algorithm is K-nearest neighbor (KNN) <ref> [15, 57] </ref>. Suppose x is a query pattern, for which we would like to know the output y = f n (x).
Reference: [58] <author> Suddarth, S. C. and Holden, A. </author> <title> Symbolic neural systems and the use of hints for developing complex systems. </title> <journal> International Journal of Machine Studies, </journal> <volume> vol. </volume> <month> 35 </month> <year> (1991), </year> <note> p. </note> . 
Reference-contexts: In the context of neural network learning, several researchers have proposed methods for learning data representations that are tailored towards the built-in bias of artificial neural networks <ref> [58, 52, 44, 9, 5] </ref>. The basic idea here is the same as in Section 3.3. To re-represent the data, these approaches train a neural network, g : I ! I 0 , which maps input patterns in I to a new space, I 0 . <p> Hence, it is possible to use standard Back-Propagation to tune the weights of the transformation network g, along with the weights of the respective classification network. While some authors [52, 44] have proposed to process the support sets and the training set sequentially, others <ref> [58, 9, 5] </ref> are in favor of training g in parallel, using all n tasks simultaneously. Sequential training offers the advantage that not all training data has to be available at all time. <p> The first algorithm gradually learns a domain-specific data representation, which improves the generalization in memory-based learning. 2. The second algorithm replaces the fixed distance metric in memory-based learning by a domain-specific comparator function, which is learned using previous datasets. 3. The third algorithm (see also <ref> [58, 52, 44, 9, 5, 55] </ref>) learns a domain-specific representation, like the first algorithm, but this representation is tailored towards neural network learning. 4.
Reference: [59] <author> Sutton, R. S. </author> <title> Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming. </title> <booktitle> in: Advances in Neural Information Processing Systems 3, </booktitle> <editor> edited by R. P. Lippmann, J. E. Moody, and D. S. Touretzky. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1991, </year> <pages> pp. 471-478. </pages>
Reference-contexts: While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning [29, 50, 14, 25] or reinforcement learning <ref> [70, 59, 4, 23] </ref>. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function. This is clearly impractical if the number of support sets is large.
Reference: [60] <author> Thrun, S. </author> <title> An Approach to Learning Mobile Robot Navigation. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference-contexts: Some recent results for applying EBNN to reinforcement learning can be found elsewhere <ref> [60, 61] </ref>. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function. This is clearly impractical if the number of support sets is large. Designing incremental lifelong learning algorithms is an important issue of future research.
Reference: [61] <author> Thrun, S. </author> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: If multiple examples are available in X n , their votes can be combined using Bayes' rule [42], leading to Bel (f n (x)=1) := 1 1 + h x; y=1i2X n 1 d (x; x) The somewhat lengthy derivation of (7), which is given in <ref> [61] </ref>, is straightforward if one interprets the output of d as a conditional probability for the class of a query point x given a training example h x; yi, and if one assumes (conditionally) independent sampling noise X n . <p> This algorithm is a special version of both the Tangent-Prop algorithm [56] and the explanation-based neural network learning (EBNN) algorithm <ref> [34, 61] </ref>. Here we will refer to it as EBNN. EBNN approximates f n using an artificial neural network, denoted by h : I ! [0; 1], just like the conventional Back-Propagation approach to supervised learning. <p> For example, EBNN derives slopes not only for positive training examples, but also for negative ones. See <ref> [63, 61] </ref> for more details. 14 of the shoe and the sunglasses, and the support sets contained images of the other five objects. <p> Some recent results for applying EBNN to reinforcement learning can be found elsewhere <ref> [60, 61] </ref>. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function. This is clearly impractical if the number of support sets is large. Designing incremental lifelong learning algorithms is an important issue of future research.
Reference: [62] <author> Thrun, S. </author> <title> Is Learning the n-th Thing Any Easier Than Learning the First? in: </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <editor> edited by D. Touretzky and M. Mozer. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996, </year> <note> p. . to appear. </note>
Reference-contexts: Obviously, a good transformation g maps multiple examples of a single concept to similar representations, whereas an an example and a counterexample should have distinctly different representations. This property can directly be transformed into an energy function for g <ref> [62] </ref>: E := k=1 hx;y=1i2X k B @ h x; yi2X k ; y=y | -z - h x; yi2X k ; y6=y | -z - (flfl) 1 C Adjusting g to minimize E forces the distance (fl) between pairs of examples of the same concept to be small, and the
Reference: [63] <author> Thrun, S. and Mitchell, T. M. </author> <title> Learning One More Thing. </title> <booktitle> in: Proceedings of IJCAI-95, IJCAI, </booktitle> <publisher> Inc. </publisher> <address> Montreal, Canada, </address> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: One way to do this is to learn a comparator d : I fi I ! [0; 1] <ref> [63] </ref>. A comparator d accepts two input patterns, say x and x, and outputs 1 if x and x are members of the same concept, and 0 otherwise. <p> For example, EBNN derives slopes not only for positive training examples, but also for negative ones. See <ref> [63, 61] </ref> for more details. 14 of the shoe and the sunglasses, and the support sets contained images of the other five objects.
Reference: [64] <author> Thrun, S. and O'Sullivan, J. </author> <title> Clustering Learning Tasks and the Selective Cross-Task Transfer of Knowledge. no. </title> <institution> CMU-CS-95-209, Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Algorithms that can handle a whole hierarchy of relations (relations among points, among functions (or sets of points), and among sets of functions) are clearly desirable and subject of ongoing research (see also <ref> [64] </ref>). Despite these open questions, we envision a variety of practical application domains for the methods and ideas presented here. Meta-level learning is particular relevant to learning problems in which the cost of collecting training data is the dominating factor when applying machine learning techniques.
Reference: [65] <author> Towell, G. G. and Shavlik, J. W. </author> <title> Knowledge-Based Artificial Neural Networks. </title> <journal> Artificial Intelligence, </journal> <volume> vol. 70 (1994), </volume> <pages> pp. 119-165. </pages>
Reference-contexts: The choice of bias is crucial in machine learning, as it represents both the designer's knowledge and his/her ignorance about the domain. In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules <ref> [33, 12, 65, 41, 40, 38] </ref>. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation [72, 71, 48] or inductive tree learning [45, 17, 22], to name two popular examples. <p> Various systems have been proposed for inductively refining hand-coded domain theories (see e.g., [6, 41]). For example, EITHER [40] inductively refines an initial domain theory based on noisy training data using ID3 [45] as the inductive component. Neural network-based methods <ref> [53, 18, 28, 65] </ref> basically initialize neural network weights using domain knowledge, then train the network using conventional neural network training algorithms. All these approaches are related to the work reported here, since they employ prior knowledge to reduce the sample complexity.
Reference: [66] <author> Utgoff, P. E. </author> <title> Machine Learning of Inductive Bias. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year> <month> 35 </month>
Reference-contexts: One of the earliest approaches that is able to learn at the meta-level is the VBMS system [46]. VBMS chooses the most appropriate algorithm out of a pool of conventional inductive learning algorithms based on previous, related learning tasks. A related approach, the STABB algorithm <ref> [66] </ref>, is able shift gradually towards weaker bias. Bias is represented by a restriction on the hypothesis space [32]. Whenever the hypothesis class cannot match the training examples exactly, STABB analyzes this failure and enlarges the hypothesis space correspondingly. STABB could potentially be applied to noise-free lifelong concept learning tasks.
Reference: [67] <author> Valiant, L. G. </author> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM, </journal> <volume> vol. 27 (1984), </volume> <pages> pp. 1134-1142. </pages>
Reference-contexts: It is well-known that the complexity of the base-level hypothesis space is related to the number of training examples required for base-level learning (see e.g., [32, 68, 19, 24]). One learning model, which recently has received considerable attention in the computational learning theory community, is Valiant's PAC-learning model <ref> [67] </ref> (PAC stands for probably approximately correct). PAC-Learning extends Vapnik's approach to empirical risk minimization [68] by an additional computational complexity argument.
Reference: [68] <author> Vapnik, V. </author> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher, </publisher> <year> 1982. </year>
Reference-contexts: Since deducing the output of unseen, future data is impossible without making further assumptions <ref> [31, 68, 19, 73] </ref>, every learning algorithm makes inherent assumptions concerning the nature of the data. These assumptionsoften referred to as hypothesis space, preferences, or prior, and henceforth called bias [30] enables an algorithm to favor one particular generalization over all others, hence to generalize. <p> It is well-known that the complexity of the base-level hypothesis space is related to the number of training examples required for base-level learning (see e.g., <ref> [32, 68, 19, 24] </ref>). One learning model, which recently has received considerable attention in the computational learning theory community, is Valiant's PAC-learning model [67] (PAC stands for probably approximately correct). PAC-Learning extends Vapnik's approach to empirical risk minimization [68] by an additional computational complexity argument. <p> One learning model, which recently has received considerable attention in the computational learning theory community, is Valiant's PAC-learning model [67] (PAC stands for probably approximately correct). PAC-Learning extends Vapnik's approach to empirical risk minimization <ref> [68] </ref> by an additional computational complexity argument. The following standard result by Blumer and colleagues relates the size of the hypothesis space and the number of (noise-free) training examples required for learning a function: 23 Theorem [8].
Reference: [69] <author> Veloso, M. M. </author> <title> Learning by Analogical Reasoning in General Problem Solving. </title> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Memory-based approaches memorize all training examples explicitly, and interpolate between them at query-time. Notice that memory-based learning has been applied with significant success to a variety of challenging learning problems <ref> [35, 51, 69] </ref>. In what follows, we will first sketch two well-known approaches to memory-based learning, then propose meta-level components that take the support sets into account. 3.1 Nearest Neighbor Probably the most widely used memory-based learning algorithm is K-nearest neighbor (KNN) [15, 57].
Reference: [70] <author> Watkins, C. J. C. H. </author> <title> Learning from Delayed Rewards. </title> <address> King's College, Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: While it seems feasible to extend these approaches to supervised learning in general, little is known about the transfer of knowledge in other learning paradigms, such as unsupervised learning [29, 50, 14, 25] or reinforcement learning <ref> [70, 59, 4, 23] </ref>. Some recent results for applying EBNN to reinforcement learning can be found elsewhere [60, 61]. 2. Support sets. In all experiments, it was assumed that all data be available when learning the n-th function. This is clearly impractical if the number of support sets is large.
Reference: [71] <author> Werbos, P. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavorial Sciences. </title> <institution> Harvard University, Committee on Applied Mathematics, </institution> <address> Cambridge, MA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules [33, 12, 65, 41, 40, 38]. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation <ref> [72, 71, 48] </ref> or inductive tree learning [45, 17, 22], to name two popular examples. All these approaches have in common that the available data consists exclusively of input-output examples of the target function.
Reference: [72] <author> Widrow, B. and Hoff, M. E. </author> <title> Adaptive Switching Circuits. </title> <booktitle> Institute of Radio Engineers, Western Electronic Show and Convention, Convention Record, </booktitle> <address> Part4, </address> <year> 1960. </year>
Reference-contexts: In some approaches, bias is obtained explicitly through the expertise of a human expert of the domain, communicated by symbolic if-then rules [33, 12, 65, 41, 40, 38]. In others, it arises from an uninformed set of equations, as is the case in neural network Back-Propagation <ref> [72, 71, 48] </ref> or inductive tree learning [45, 17, 22], to name two popular examples. All these approaches have in common that the available data consists exclusively of input-output examples of the target function.
Reference: [73] <author> Wolpert, D. H. </author> <title> Off-Training set error and a priori distinctions between learning algorithms. no. </title> <type> SFI TR 95-01-003, </type> <institution> Santa Fe Institute, </institution> <address> Santa Fe, NM 87501, </address> <year> 1994. </year>
Reference-contexts: Since deducing the output of unseen, future data is impossible without making further assumptions <ref> [31, 68, 19, 73] </ref>, every learning algorithm makes inherent assumptions concerning the nature of the data. These assumptionsoften referred to as hypothesis space, preferences, or prior, and henceforth called bias [30] enables an algorithm to favor one particular generalization over all others, hence to generalize.
References-found: 73

