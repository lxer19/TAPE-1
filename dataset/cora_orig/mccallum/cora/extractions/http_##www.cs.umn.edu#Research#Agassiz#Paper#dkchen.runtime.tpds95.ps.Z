URL: http://www.cs.umn.edu/Research/Agassiz/Paper/dkchen.runtime.tpds95.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: Efficient Algorithm for the Run-time Parallelization of DOACROSS Loops  
Author: Ding-Kai Chen Josep Torrellas Pen-Chung Yew 
Affiliation: Silicon Graphics Center for Supercomputing Dept. of Computer Science Computer Systems Research and Development University of Minnesota University of Illinois at Urbana-Champaign  
Note: An  This work was supported in part by the National Science Foundation under Grant Nos. NSF MIP-8920891, NSF MIP-9307910, and the U.S. Department of Energy, Grant No. DOE DE-FG02-85ER25001.  
Abstract: A preliminary version of this paper appeared in Supercomputing '94, November 1994. Abstract While loop parallelization usually relies on compile-time analysis of data dependences, for some loops the data dependences cannot be determined at compile time. An example is loops referencing arrays with subscripted subscripts. To parallelize these loops, it is necessary to use run-time analysis. In this paper, we present a new run-time algorithm to parallelize these loops. Our scheme handles any type of data dependence in the loop without requiring any special architectural support in the multiprocessor. Furthermore, compared to an older scheme with the same generality, our scheme significantly reduces the amount of processor communication required and increases the overlap among dependent iterations. We evaluate our algorithm with an extensive set of loops running on the 32-processor Cedar shared-memory multiprocessor. The execution times show speedups over the serial case that reach up to 13 with the full overhead of run-time analysis and up to 26 if part of the work is reused across loop invocations. Moreover, our algorithm outperforms the older scheme with the same generality in nearly all cases, reaching a 37-fold speedup over the older scheme when the loop has many dependences. keywords: Data Dependences, Parallelization, Run-Time Support, Synchronization.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Loop-level parallelism is often the main source of speed-up in multiprocessors that exploit medium-grain parallelism. To parallelize a loop, it is necessary to identify the dependence relations between loop iterations via dependence analysis <ref> [1] </ref>. If the loop is inherently parallel, that is, it does not have any dependences across iterations, then it can run in parallel. Similarly, if it can be transformed to eliminate such dependences, it can run in parallel too.
Reference: [2] <author> M. Berry, D.-K. Chen, et al. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <booktitle> Int'l. J. of Supercomputer Applications, </booktitle> <pages> pages 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Column 4 shows the range of values chosen. We use three loops selected from two widely used benchmark programs to illustrate corresponding parameter values. They are loop 100 of subroutine PREDIC (L1) and loop 900 of subroutine CORREC (L2) of BDNA from Perfect Benchmark Suite <ref> [2] </ref>, and loop 2 of subroutine COOCSR (L3) from the Sparskit [12]. Note that loops from the benchmarks tend to have smaller problem size, hence the smaller number of iterations, than real application codes.
Reference: [3] <author> D.-K. Chen and P.-C. Yew. </author> <title> A scheme for effective execution of irregular DOACROSS loops. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 285-292, </pages> <month> August </month> <year> 1992. </year> <note> Also available as CSRD tech report No. 1192. </note>
Reference-contexts: Such a loop is called a DOACROSS loop. DOACROSS loops are parallelized in different ways. Sometimes, the difference in iteration number between dependent iterations (called dependence distance) is known at compile time. In this case, techniques like loop-alignment [9], loop skewing [17], or dependence uniformization <ref> [3, 15] </ref> can be used to parallelize the loop. Unfortunately, many other times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. In these cases, the compiler cannot prove the independence of iterations. <p> ENDDO (b) Dependence Example Parallelizing Type f (i) g (i) Methods Uniform Distance 2i+3 2i+1 Loop Alignment [9], Loop Skewing [17] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization <ref> [15, 3] </ref> (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-time Parallelization Table 1: Sample dependence types and parallelization methods. and evaluate the distance (i 0 i) within the constraints of the loop limits.
Reference: [4] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer designing an MIMD shared memory parallel computer. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-32(2), </volume> <month> February </month> <year> 1983. </year>
Reference-contexts: The cycles in column 4 are 170 ns processor cycles. fetch-and-OE operations of the NYU Ultracomputer <ref> [4] </ref>. Hence, only one trip for each synchronization access is required, and no remote busy-waiting across the network is necessary. In the case of Temp1 and Temp2 in operations from different processors can all proceed concurrently in the Omega network.
Reference: [5] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In ACM Int'l. Conf. on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18, 19] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. In all cases, however, the key to success is to minimize the time spent on the analysis of dependences and synchronization. <p> To do so, the scheme requires data synchronization with more than one key field. However, it still requires high communication as Zhu-Yew's scheme. Other less general schemes have been proposed <ref> [5, 10] </ref>. They restrict the problem in three ways. First, no two index elements of a given index array (I1 and I2 in Figure 1 (b)) have the same values. This makes scheduling simpler. Second, they require a serial pre-processing loop, which can reduce speedup.
Reference: [6] <author> D. Kuck et al. </author> <title> The Cedar system and an initial performance study. </title> <booktitle> In 20th Int'l Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year> <title> (a) Sensitivity to the number of references per iteration. (b) Sensitivity to the fraction of accesses to hot locations for H size =10%. </title>
Reference-contexts: Furthermore, compared to an older scheme with the same generality [18], it speeds up execution by significantly reducing the amount of communication required and by increasing the overlap among dependent iterations. The effectiveness of this algorithm is evaluated via measurements in the 32-processor Cedar shared-memory multiprocessor <ref> [6] </ref>. The results show speedups that reach up to 13 with the full overhead of the analysis overhead and up to 26 if part of the analysis work is reused over loop invocations. <p> In this section, we describe the machine, the synchronization support, and the workloads used. 4.1 The Cedar Multiprocessor Our experiments are timing runs on the Cedar <ref> [6] </ref>, a 32-processor scalable shared-memory multiprocessor designed at the Center for Supercomputing Research and Development. The machine has 4 clusters of 8 11 processors each (Figure 8). Each cluster is a bus-based Alliant FX/8 and it has 32 Mbytes of memory shared by the processors in the cluster.
Reference: [7] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18, 19] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. In all cases, however, the key to success is to minimize the time spent on the analysis of dependences and synchronization. <p> Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. [13, 14] and Leung and Zahorjan <ref> [7] </ref>. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [8] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18, 19] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. In all cases, however, the key to success is to minimize the time spent on the analysis of dependences and synchronization. <p> Unfortunately, it requires many memory accesses. Indeed, for each iteration of the repeat loop, each unexecuted iteration i requires at least 3r memory accesses, where r is the number of references to array A per iteration. A second scheme, proposed by Midkiff and Padua <ref> [8] </ref>, improved Zhu-Yew's scheme by allowing concurrent reads to the same array element by several iterations. To do so, the scheme requires data synchronization with more than one key field. However, it still requires high communication as Zhu-Yew's scheme. Other less general schemes have been proposed [5, 10].
Reference: [9] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: Such a loop is called a DOACROSS loop. DOACROSS loops are parallelized in different ways. Sometimes, the difference in iteration number between dependent iterations (called dependence distance) is known at compile time. In this case, techniques like loop-alignment <ref> [9] </ref>, loop skewing [17], or dependence uniformization [3, 15] can be used to parallelize the loop. Unfortunately, many other times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. <p> ENDDO (b) Dependence Example Parallelizing Type f (i) g (i) Methods Uniform Distance 2i+3 2i+1 Loop Alignment <ref> [9] </ref>, Loop Skewing [17] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization [15, 3] (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-time Parallelization Table 1: Sample dependence types and parallelization methods. and evaluate the distance (i 0 i) within the constraints of the loop limits.
Reference: [10] <author> C. Polychronopoulos. </author> <title> Advanced loop optimizations for parallel computers. </title> <editor> In E. Houstis, T. Papatheodorou, and C. Polychronopoulos, editors, </editor> <booktitle> Lecture Notes in Computer Science No. 297: Proc. First Int'l. Conf. on Supercomputing, </booktitle> <address> Athens, Greece, </address> <pages> pages 255-277, </pages> <address> New York, June 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To do so, the scheme requires data synchronization with more than one key field. However, it still requires high communication as Zhu-Yew's scheme. Other less general schemes have been proposed <ref> [5, 10] </ref>. They restrict the problem in three ways. First, no two index elements of a given index array (I1 and I2 in Figure 1 (b)) have the same values. This makes scheduling simpler. Second, they require a serial pre-processing loop, which can reduce speedup.
Reference: [11] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 361-370, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition. Their recent work <ref> [11] </ref> focuses on run-time compilation techniques for FORALL loops with only output dependences for reduction operations on distributed memory computers. There are similar issues, but the context is different. A common limitation of most of the previous works is the lack of performance results.
Reference: [12] <author> Y. Saad. SPARSKIT: </author> <title> A Basic Tool Kit for Sparse Matrix Computation. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> August </month> <year> 1990. </year> <note> CSRD Report No. 1029. </note>
Reference-contexts: We use three loops selected from two widely used benchmark programs to illustrate corresponding parameter values. They are loop 100 of subroutine PREDIC (L1) and loop 900 of subroutine CORREC (L2) of BDNA from Perfect Benchmark Suite [2], and loop 2 of subroutine COOCSR (L3) from the Sparskit <ref> [12] </ref>. Note that loops from the benchmarks tend to have smaller problem size, hence the smaller number of iterations, than real application codes. In the table, iteration count N is assumed to be the same as the size of the array A.
Reference: [13] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 174-179, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Finally, if the index arrays are not one-to-one mappings, they require shared-memory allocation. Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. <ref> [13, 14] </ref> and Leung and Zahorjan [7]. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [14] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(5) </volume> <pages> 603-611, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18, 19] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. In all cases, however, the key to success is to minimize the time spent on the analysis of dependences and synchronization. <p> These accesses form a dependence (or access) chain. The values of arrays I1 and I2 are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. Run-time parallelization schemes usually have two stages, namely inspector and executor <ref> [14, 19] </ref>, both performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in an order that preserves the dependences. <p> Finally, if the index arrays are not one-to-one mappings, they require shared-memory allocation. Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. <ref> [13, 14] </ref> and Leung and Zahorjan [7]. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [15] <author> T. H. Tzen and L. Ni. </author> <title> Dependence uniformization: A loop parallelization technique. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(5), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Such a loop is called a DOACROSS loop. DOACROSS loops are parallelized in different ways. Sometimes, the difference in iteration number between dependent iterations (called dependence distance) is known at compile time. In this case, techniques like loop-alignment [9], loop skewing [17], or dependence uniformization <ref> [3, 15] </ref> can be used to parallelize the loop. Unfortunately, many other times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. In these cases, the compiler cannot prove the independence of iterations. <p> ENDDO (b) Dependence Example Parallelizing Type f (i) g (i) Methods Uniform Distance 2i+3 2i+1 Loop Alignment [9], Loop Skewing [17] (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization <ref> [15, 3] </ref> (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-time Parallelization Table 1: Sample dependence types and parallelization methods. and evaluate the distance (i 0 i) within the constraints of the loop limits.
Reference: [16] <author> M. J. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: In order to determine if there is any loop-carried dependence <ref> [16] </ref> between statements S p and S q across iterations and to compute its dependence distance, we have to solve the following integer equation f (i) = g (i 0 ) DO i=1,N S p : A (f (i))= : : : S q : : : : = A (g
Reference: [17] <author> M. J. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 4(15), </volume> <year> 1986. </year>
Reference-contexts: Such a loop is called a DOACROSS loop. DOACROSS loops are parallelized in different ways. Sometimes, the difference in iteration number between dependent iterations (called dependence distance) is known at compile time. In this case, techniques like loop-alignment [9], loop skewing <ref> [17] </ref>, or dependence uniformization [3, 15] can be used to parallelize the loop. Unfortunately, many other times it is not possible to determine the dependence distance at compile time. An example is when array elements are accessed via subscripted subscripts. <p> ENDDO (b) Dependence Example Parallelizing Type f (i) g (i) Methods Uniform Distance 2i+3 2i+1 Loop Alignment [9], Loop Skewing <ref> [17] </ref> (if loop level&gt; 1) Non-Uniform Distance 2i+9 3i+3 Dependence Uniformization [15, 3] (if loop level&gt; 1) Unknown Distance I1 (i) I2 (i) Run-time Parallelization Table 1: Sample dependence types and parallelization methods. and evaluate the distance (i 0 i) within the constraints of the loop limits.
Reference: [18] <author> C.-Q. Zhu and P.-C. Yew. </author> <title> A synchronization scheme and its application for large multiprocessor systems. </title> <booktitle> In 4h Int. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 486-493, </pages> <month> May </month> <year> 1984. </year> <month> 20 </month>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18, 19] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. In all cases, however, the key to success is to minimize the time spent on the analysis of dependences and synchronization. <p> In this paper, we describe and evaluate a new algorithm for the run-time parallelization of loops. Our scheme handles any type of dependence pattern without requiring any special architectural support. Furthermore, compared to an older scheme with the same generality <ref> [18] </ref>, it speeds up execution by significantly reducing the amount of communication required and by increasing the overlap among dependent iterations. The effectiveness of this algorithm is evaluated via measurements in the 32-processor Cedar shared-memory multiprocessor [6]. <p> The first one was proposed by Zhu and Yew <ref> [18] </ref>. Their scheme is general enough to handle any dependence pattern. Given a loop like that in Figure 1 (b), the compiler transforms it into the code shown in Figure 2. In the figure, each element of array A has two fields, Done (1:N)= .FALSE. <p> We used loops with varying parameters, such as number of iterations and references. The results show that our algorithm gives good speedups that reach 13 if the inspector is not reused and 26 if it is. Furthermore, our algorithm outperforms Zhu-Yew's scheme <ref> [18] </ref> in nearly all cases, reaching a 37-fold speedup when the loop has many dependences. There are a few issues in run-time parallelization that we are currently working on.
Reference: [19] <author> C.-Q. Zhu and P.-C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. on Software Enginering, </journal> <pages> pages 726-739, </pages> <month> June </month> <year> 1987. </year> <month> 21 </month>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at run-time and maybe execute the loop in parallel. This approach is called run-time parallelization. Much previous research has been done to design effective run-time parallelization algorithms <ref> [5, 7, 8, 14, 18, 19] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. In all cases, however, the key to success is to minimize the time spent on the analysis of dependences and synchronization. <p> These accesses form a dependence (or access) chain. The values of arrays I1 and I2 are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. Run-time parallelization schemes usually have two stages, namely inspector and executor <ref> [14, 19] </ref>, both performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in an order that preserves the dependences.
References-found: 19

