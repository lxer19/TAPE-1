URL: http://ai.eecs.umich.edu/people/huffman/papers/thesis.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/huffman/hufpubs.html
Root-URL: http://www.eecs.umich.edu
Title: Instructable Autonomous Agents  
Author: by Scott Bradley Huffman 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee: Associate Professor John E. Laird, Chair Assistant Professor W. P. Birmingham Professor R. K. Lindsay Professor W. Rounds Associate Professor E. Soloway  
Date: January 1994  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: [Akatsuka, 1986] <author> Noriko Akatsuka. </author> <title> Conditionals are discourse-bound. </title> <editor> In Elizabeth Closs Traugott, editor, </editor> <booktitle> On Conditionals, </booktitle> <pages> pages 333-51. </pages> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: For example, consider the following exchange between an instructor and an agent: Block open our office door. How do I do that? Pick up a red block. OK. Now, drop it here, next to the door. 1 Some types of conditionals do not follow this pattern <ref> [Akatsuka, 1986] </ref>, but they are not relevant to tutorial instruction. 12 Speaker Instruction 19 Instructor You have some flaps, which are control surfaces on your wings 20 that help you take off and land. 21 And these are initially set up. 22 You'll have to get rid of them once you've
Reference: [Allen and Perrault, 1980] <author> James F. Allen and C. Raymond Perrault. </author> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15 </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference: [Alterman and Carpenter, 1991] <author> Richard Alterman and Tamitha Carpenter. </author> <title> Reading instructions. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 653-657, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Martin and Firby [1991] discuss an approach to interpreting and learning from elliptical instructions ("Use the shovel") by matching the instruction to expectations generated from a task execution failure. Alterman and his colleagues <ref> [Alterman et al., 1991; Alterman and Carpenter, 1991] </ref> have produced FLOBN, a system that learns to operate a device by using instruction and analogy to a previously known device.
Reference: [Alterman and Zito-Wolf, 1993] <author> Richard Alterman and Roland Zito-Wolf. </author> <title> Agents, habitats, and routine behavior. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 305-310, </pages> <year> 1993. </year>
Reference-contexts: However, the explanation process allows knowledge learned about each step in a sequence to affect learning of the remaining steps; the end result can be viewed as a distributed case or schema that encodes the overall procedure (as, e.g., <ref> [Alterman and Zito-Wolf, 1993] </ref>). * Generalization at storage versus retrieval time. Case-based methods typically generalize cases primarily at retrieval time. That is, when a case is given, it is stored in a rote form.
Reference: [Alterman et al., 1991] <author> Richard Alterman, Roland Zito-Wolf, and Tamitha Carpenter. </author> <title> Interaction, comprehension, and instruction usage. </title> <type> Technical Report CS-91-161, </type> <institution> Computer Science Department, Brandeis University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Martin and Firby [1991] discuss an approach to interpreting and learning from elliptical instructions ("Use the shovel") by matching the instruction to expectations generated from a task execution failure. Alterman and his colleagues <ref> [Alterman et al., 1991; Alterman and Carpenter, 1991] </ref> have produced FLOBN, a system that learns to operate a device by using instruction and analogy to a previously known device. <p> However, induction over multiple examples could also be employed within an explanation-based approach (e.g., [Miller, 1993]). 32 2.3.2 Case-based learning A second alternative would be to employ case-based reasoning methods (e.g., <ref> [Redmond, 1992; Alterman et al., 1991; Schank and Leake, 1989] </ref>) to learn from instruction. A case might consist of the sequence of instructions for a procedure.
Reference: [Anderson, 1983] <author> John R. Anderson. </author> <title> The architecture of cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: Finally, this work can be compared with other cognitive models of procedural learning. Most recent models are based on Anderson's model of procedural learning in ACT* <ref> [Anderson, 1983; Anderson, 1987; Singley and Anderson, 1989] </ref>, which in turn is based on Thorndike's [1903] theory of identical elements. Thorndike proposed that all transfer occurred because of the use of identical elements specific stimulus-response rules across multiple tasks. <p> Entailments provide an intermediate level of granularity that can be used to compare models within different unified theories. Where another architecture's entailments differ from Soar's, a figure like Figure 5.1 indicates the model properties that will be affected by the differing entailments. For instance, ACT* <ref> [Anderson, 1983] </ref> has a declarative long-term memory in addition to a production memory. This entails a fully penetrable, non-associative memory, that could be used to store instructions as they are read. <p> These properties are indeed found in an ACT* model of procedural learning from instruction <ref> [Anderson, 1983] </ref>. This model will be contrasted further with Instructo-Soar after a discussion of Instructo-Soar's behavioral predictions. 5.5 Model predictions Thus far, the Instructo-Soar model's properties have been described in terms dependent on the model's data structures and processes. <p> Most recent models fall into two major classes: models based on ACT*, and case-based models. 89 5.7.1 ACT*-based theories Many recent models of procedural learning are based on Anderson's ACT* model <ref> [Anderson, 1983; Anderson, 1987; Singley and Anderson, 1989] </ref>. Anderson's model employs a three stage process. First, instructions are converted into a declarative form in long-term memory. Second, the declarative memories guide task execution via weak methods, and through this process, new task-specific rules for the procedure are learned.
Reference: [Anderson, 1987] <author> John R. Anderson. </author> <title> Skill acquisition: Compilation of weak-method problem solutions. </title> <journal> Psychological Review, </journal> <volume> 94(2) </volume> <pages> 192-210, </pages> <year> 1987. </year>
Reference-contexts: Finally, this work can be compared with other cognitive models of procedural learning. Most recent models are based on Anderson's model of procedural learning in ACT* <ref> [Anderson, 1983; Anderson, 1987; Singley and Anderson, 1989] </ref>, which in turn is based on Thorndike's [1903] theory of identical elements. Thorndike proposed that all transfer occurred because of the use of identical elements specific stimulus-response rules across multiple tasks. <p> Most recent models fall into two major classes: models based on ACT*, and case-based models. 89 5.7.1 ACT*-based theories Many recent models of procedural learning are based on Anderson's ACT* model <ref> [Anderson, 1983; Anderson, 1987; Singley and Anderson, 1989] </ref>. Anderson's model employs a three stage process. First, instructions are converted into a declarative form in long-term memory. Second, the declarative memories guide task execution via weak methods, and through this process, new task-specific rules for the procedure are learned.
Reference: [Annett, 1991] <author> John Annett. </author> <title> Skill acquisition. </title> <editor> In J. E. Morrison, editor, </editor> <title> Training for performance: </title> <booktitle> Principles of applied human learning, </booktitle> <pages> pages 13-51. </pages> <publisher> John Wiley and Sons Ltd., </publisher> <year> 1991. </year>
Reference-contexts: Instructo-Soar's self-explanations take the form of mental simulations of instructed actions. There is ample evidence that this type of "mental practice" results in useful learning for human subjects <ref> [Annett, 1991] </ref>. Feltz and Landers [1983] and Johnson [1982] found evidence that mental practice causes rehearsal of cognitive processes (as in Instructo-Soar), not only low-level motor responses. 5.6.4 Transfer Other than the self-explanation effect, two other general transfer effects are found in Instructo-Soar's behavior.
Reference: [Balkany et al., 1991] <author> Alan Balkany, William P. Birmingham, and Iris D. Tommelein. </author> <title> A knowledge-level analysis of several design tools. </title> <booktitle> In Artificial Intelligence in Design, </booktitle> <year> 1991. </year>
Reference-contexts: difference is that those PSCM descriptions included "task formulation" (selecting and specifying a task to perform) as a basic operation, whereas here it is assumed that task formulation can be done by operators. 2 These knowledge types are more general than the typical knowledge roles for particular problem solving methods <ref> [Balkany et al., 1991] </ref>, such as heuristic classification or propose-and-revise.
Reference: [Birmingham and Klinker, 1993] <author> William Birmingham and Georg Klinker. </author> <title> Knowledge acquisition tools with explicit problem-solving methods. </title> <journal> The Knowledge Engineering Review, </journal> <volume> 8(1), </volume> <year> 1993. </year>
Reference-contexts: More recent work has focused on method-specific elicitation techniques, based on McDer-mott's idea of knowledge roles [McDermott, 1988] and Chandrasekaran's generic tasks framework [Chandrasekaran, 1986]. These knowledge-level techniques gain their power by limiting application to a specific problem solving method, with a pre-defined set of knowledge roles <ref> [Birmingham and Klinker, 1993] </ref>. Method-based knowledge acquisition tools differ from tutorial instruction in that elicitation is unsituated and focused on a particular class of tasks. <p> If an instructable system will always be applied to problems requiring a particular global control method, it may be more effective (although less general) to build the control method into the system from the start. This is the approach of method-based knowledge acquisition tools <ref> [Birmingham and Klinker, 1993] </ref>. A second weakness of the PSCM is that it provides a theory of the types of knowledge used by an intelligent agent, but gives no indication of the possible content of that knowledge.
Reference: [Birmingham and Siewiorek, 1989] <author> William P. Birmingham and Daniel P. Siewiorek. </author> <title> Automated knowledge acquisition for a computer hardware synthesis system. </title> <journal> Knowledge Acquisition, </journal> <volume> 1 </volume> <pages> 321-340, </pages> <year> 1989. </year>
Reference: [Bloom, 1984] <author> Benjamin S. Bloom. </author> <title> The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring. </title> <journal> Educational Researcher, </journal> <volume> 13(6) </volume> <pages> 4-16, </pages> <year> 1984. </year>
Reference-contexts: These studies have shown that one-to-one tutoring is a highly effective teaching technique <ref> [Bloom, 1984; Palinscar, 1986] </ref>. It robustly produces stronger learning than classroom instruction by two standard deviations [Bloom, 1984]. A number of studies have shown that students' amount of prior knowledge has a large impact on learning from instruction. <p> These studies have shown that one-to-one tutoring is a highly effective teaching technique [Bloom, 1984; Palinscar, 1986]. It robustly produces stronger learning than classroom instruction by two standard deviations <ref> [Bloom, 1984] </ref>. A number of studies have shown that students' amount of prior knowledge has a large impact on learning from instruction. These results are relevant to the model of learning from tutorial instruction presented here because the model makes heavy use of prior knowledge to leverage learning.
Reference: [Bovair and Kieras, 1991] <author> Susan Bovair and David E. Kieras. </author> <title> Toward a model of acquiring procedures from text. </title> <editor> In Barr, Kamil, Rosenthal, and Pearson, editors, </editor> <booktitle> Handbook of Reading Research, Volume II. </booktitle> <publisher> Longman, </publisher> <year> 1991. </year>
Reference-contexts: Non-native speakers with a science background performed better than native speakers without a science background. In both of these studies, subjects with prior knowledge may have made up for incomplete comprehension of the instructions by inferring the missing steps, whereas low knowledge subjects could not do this <ref> [Bovair and Kieras, 1991] </ref>. Learning and transfer: Prior domain knowledge has been shown to improve students' instructional learning and transfer. Judd [1908] had two groups of boys throw darts at underwater targets. One group, taught about refraction beforehand, outperformed the other after the basic mechanics of the task were mastered. <p> Since productions can be very general, transfer can occur across disparate tasks. However, the majority of productions for any particular task will be quite specific. Likewise, most results on transfer indicate that it is very narrow <ref> [Singley and Anderson, 1989; Bovair and Kieras, 1991] </ref>. For instance, Foltz et al. [1988] found that simply changing the name of a procedure from delete to erase caused a failure to transfer. <p> Models built within the ACT* theory have focused primarily on situations in which there is little prior domain knowledge, and initial performance of tasks requires applying weak methods. Other ACT* type theories include models by Lewis et al. [1989] and Bovair and Kieras <ref> [Bovair, 1991; Bovair and Kieras, 1991; Kieras and Bovair, 1986] </ref>. Lewis et al. [1989] describe a system, built in Soar, that reads instructions to perform a subject's part in simple psychological experiments. The instructions are steps in a procedure, that are read as a "recipe" in a non-interactive fashion. <p> However, the instructions in these experiments were non-interactive; subjects were required to memorize all of the instructions for a procedure before attempting to 90 perform it. Learning under these conditions could be very different from interactive instruction <ref> [Bovair and Kieras, 1991] </ref>. Indeed, one of Bovair and Kieras' key results concerned a drop in reading times for instructions read after they had already been mastered by a student; in interactive instruction, however, a student typically does not ever receive an instruction again once it has been mastered.
Reference: [Bovair, 1991] <author> Susan Jane Bovair. </author> <title> A model of procedure acquisition from written instructions. </title> <type> PhD thesis, </type> <institution> The University of Michigan, Dept. of Psychology, </institution> <year> 1991. </year>
Reference-contexts: Models built within the ACT* theory have focused primarily on situations in which there is little prior domain knowledge, and initial performance of tasks requires applying weak methods. Other ACT* type theories include models by Lewis et al. [1989] and Bovair and Kieras <ref> [Bovair, 1991; Bovair and Kieras, 1991; Kieras and Bovair, 1986] </ref>. Lewis et al. [1989] describe a system, built in Soar, that reads instructions to perform a subject's part in simple psychological experiments. The instructions are steps in a procedure, that are read as a "recipe" in a non-interactive fashion.
Reference: [Brachman, 1980] <author> R.J. Brachman. </author> <title> An introduction to kl-one. </title> <editor> In Brachman R. J., editor, </editor> <booktitle> Research in Natural Language Understanding, </booktitle> <pages> pages 13-46. </pages> <institution> Bolt, Beranek and Newman Inc., </institution> <address> Cambridge, MA, </address> <year> 1980. </year> <month> 133 </month>
Reference-contexts: Such theories define the functions and structures that are to be used in representing knowledge of various kinds (e.g., KL-ONE <ref> [Brachman, 1980] </ref>); in addition, some define the possible content of those structures (e.g., conceptual dependency theory [Schank, 1975], CYC [Guha and Lenat, 1990]). However, since these theories do not specify computational operations, they do not indicate how procedural knowledge is used within an agent.
Reference: [Brown et al., 1989] <author> J. S. Brown, A. Collins, and P. Duguid. </author> <title> Situated cognition and the culture of learning. </title> <journal> Educational Researcher, </journal> <volume> 18 </volume> <pages> 32-42, </pages> <year> 1989. </year>
Reference: [Carbonell and Gil, 1987] <author> Jaime G. Carbonell and Yolanda Gil. </author> <title> Learning by experimentation. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 256-265, </pages> <year> 1987. </year>
Reference: [Carbonell et al., 1983] <author> Jaime G. Carbonell, R. S. Michalski, and T. M. Mitchell. </author> <title> An overview of machine learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: However, Homer does not learn procedures or other kinds of domain knowledge from sequences of instruction, or make use of prior knowledge to produce general learning. Learning from tutorial instruction is related to work on learning from external guidance and advice taking <ref> [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983] </ref>. Some early systems learned declarative, ontological knowledge from instructions [Lindsay, 1963; Haas and Hendrix, 1983]. UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction.
Reference: [Carroll, 1984] <author> John M. Carroll. </author> <title> Minimalist training. </title> <journal> Datamation, </journal> <volume> 30(18) </volume> <pages> 125-136, </pages> <year> 1984. </year>
Reference-contexts: As Carroll [1984] observes, students "don't appreciate overviews, reviews, and previews; they want to do things." This preference is so strong that "they are apt to plunge into a procedure as soon as it is mentioned or will try to execute purely expository descriptions" <ref> [Carroll, 1984, p. 125] </ref>. Carroll's studies have shown that training that leads students through execution of actual tasks is more effective than other kinds of training. Instructo-Soar predicts the effectiveness of situated learning because its central learning process is situated explanation.
Reference: [Chandrasekaran, 1986] <author> B. Chandrasekaran. </author> <title> Generic tasks in knowledge-based reasoning: High-level building blocks for expert system design. </title> <journal> IEEE Expert, </journal> <volume> 1(3) </volume> <pages> 23-29, </pages> <year> 1986. </year>
Reference-contexts: Thus, the user had to understand the internal workings of the 16 system. More recent work has focused on method-specific elicitation techniques, based on McDer-mott's idea of knowledge roles [McDermott, 1988] and Chandrasekaran's generic tasks framework <ref> [Chandrasekaran, 1986] </ref>. These knowledge-level techniques gain their power by limiting application to a specific problem solving method, with a pre-defined set of knowledge roles [Birmingham and Klinker, 1993]. Method-based knowledge acquisition tools differ from tutorial instruction in that elicitation is unsituated and focused on a particular class of tasks. <p> Another class of computational models with knowledge level components are the problem solving methods that have been defined in the knowledge acquisition community; for example, heuristic classification [Clancy, 1985], propose-and-refine constraint satisfaction [Marcus and McDermott, 1989], and the various generic tasks methods <ref> [Chandrasekaran, 1986] </ref>. These methods specify the knowledge they require in terms of knowledge roles [McDermott, 1988] within a pre-defined control structure.
Reference: [Chapman, 1990] <author> David Chapman. </author> <title> Vision, Instruction, and Action. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory, </institution> <month> April </month> <year> 1990. </year>
Reference: [Chi and VanLehn, 1991] <author> M. T. H. Chi and K. VanLehn. </author> <title> The content of physics self-explanations. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 1(1) </volume> <pages> 69-105, </pages> <year> 1991. </year>
Reference-contexts: For instance, in Kieras and Bovair's [1984] study, subjects were given instructions to operate a device. Subjects given prior knowledge of the device performed much better than those without such knowledge, in terms of speed of performance, accuracy, and retention. A related result involves the self-explanation effect <ref> [Chi and VanLehn, 1991; Chi et al., 1989] </ref>. Subjects who attempt to self-explain examples of problem solutions while studying them achieve higher performance when solving problems later than subjects who do not. <p> They found that subjects who performed well on the test problems had produced significantly more self-explanations while studying the examples. These self-explanations involved a deliberate, conscious effort to justify the steps in the example, by explicating consequences and preconditions of actions <ref> [Chi and VanLehn, 1991] </ref>. Students formed explanations of individual steps in each example, rather than explaining multiple steps [VanLehn et al., 1992]. <p> data suggest that students can learn, with understanding, from a single or a few examples...However, only those students who provide adequate explanations during studying are able to see the degree to which they can generalize...Our results suggest that unless the student self-explains from examples, there is little opportunity for transfer <ref> [Chi and VanLehn, 1991, p. 101] </ref>. Instructo-Soar's self-explanations take the form of mental simulations of instructed actions. There is ample evidence that this type of "mental practice" results in useful learning for human subjects [Annett, 1991].
Reference: [Chi et al., 1989] <author> M. T. H. Chi, M. Bassok, M. W. Lewis, P. Reimann, and R. Glaser. Self-explanations: </author> <title> How students study and use examples in learning to solve problems. </title> <journal> Cognitive Science, </journal> <volume> 13 </volume> <pages> 145-182, </pages> <year> 1989. </year>
Reference-contexts: For instance, in Kieras and Bovair's [1984] study, subjects were given instructions to operate a device. Subjects given prior knowledge of the device performed much better than those without such knowledge, in terms of speed of performance, accuracy, and retention. A related result involves the self-explanation effect <ref> [Chi and VanLehn, 1991; Chi et al., 1989] </ref>. Subjects who attempt to self-explain examples of problem solutions while studying them achieve higher performance when solving problems later than subjects who do not. <p> The method involves a deliberate recall and explanation process, applied to each individual instruction, using prior domain knowledge (e.g., knowledge of basic operators' effects). Thus: (10). Self-explanation <ref> [Chi et al., 1989] </ref> by subjects improves transfer. Subjects who do not attempt such explanations will achieve only low transfer learning (rote learning); subjects who successfully self-explain will achieve high transfer, general learning. (11). <p> Knowledge is only useful to the extent that it can be applied correctly in particular situations <ref> [Singley and Anderson, 1989; Chi et al., 1989] </ref>; and it appears that people learn to apply knowledge within situations by performing tasks in those situations. This idea underlies the increase in training through computer simulated practice (e.g., pilot training) [Lintern, 1991]. <p> Examples include classroom lectures, instruction manuals and textbooks. One issue in using this type of instruction is locating and extracting the information that is needed for a particular problem at hand. Some information conveyed non-interactively may be situated; e.g., in worked-out example problems <ref> [Chi et al., 1989; VanLehn, 1987] </ref>. However, non-interactive instruction often includes unsituated information as well; e.g., general expository text. Unsituated instruction conveys general or abstract knowledge that can be applied in a large number of different situations.
Reference: [Churchill and Young, 1991] <author> Elizabeth F. Churchill and Richard M. Young. </author> <title> Modelling representations of device knowledge in soar. </title> <editor> In Luc Steels and Barbara Smith, editors, </editor> <booktitle> Artificial Intelligence and SImulation of Behavior, </booktitle> <pages> pages 247-255. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [Clancy, 1985] <author> W. J. Clancy. </author> <title> Heuristic classification. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 289-350, </pages> <year> 1985. </year>
Reference-contexts: Thus, it is desirable for the computational model of an instructable agent to include control knowledge. Another class of computational models with knowledge level components are the problem solving methods that have been defined in the knowledge acquisition community; for example, heuristic classification <ref> [Clancy, 1985] </ref>, propose-and-refine constraint satisfaction [Marcus and McDermott, 1989], and the various generic tasks methods [Chandrasekaran, 1986]. These methods specify the knowledge they require in terms of knowledge roles [McDermott, 1988] within a pre-defined control structure.
Reference: [Davis, 1979] <author> R. Davis. </author> <title> Interactive transfer of expertise: Acquisition of new inference rules. </title> <journal> Artificial Intelligence, </journal> <volume> 12(2) </volume> <pages> 409-427, </pages> <year> 1979. </year>
Reference-contexts: A number of authors have described the advantages of situation specific knowledge elicitation (e.g., <ref> [Davis, 1979; Gruber, 1989] </ref>). When instructions apply to the current situation, the situation can help to disambiguate them during their interpretation. Actions are performed in the world when they are instructed, giving the instructor direct feedback about their effects. <p> Instructo-Soar meets the requirement, demonstrating a wider breadth of learning than previous learning apprentice systems. Other interactive knowledge acquisition tools besides learning apprentices are more distantly related to tutorial instruction. Early interactive tools, such as TIERESIAS <ref> [Davis, 1979] </ref>, required direct manipulation of symbol level structures used by the knowledge-based system, instead of allowing interaction at the knowledge level. Thus, the user had to understand the internal workings of the 16 system.
Reference: [DeJong and Mooney, 1986] <author> Gerald F. DeJong and Raymond J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: To complete an explanation of an instruction, an agent must bring its prior knowledge to bear to complete the path through the instruction to achievement of the goal the instruction applies to. Early work in explanation-based learning (EBL) <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref> required deductive explanations or "proofs" built from a complete and correct domain theory. Tutorial instruction requires a broader notion of explanation, since the point of receiving instruction at all is that the agent's knowledge is incomplete in various ways. <p> The overall example of learning the "pick up" procedure from instruction is described in Chapter 3. C.1 Chunking General rules are created by Soar's learning mechanism, chunking, applied to the successful forward projection of an instruction. Chunking is a form of explanation-based learning <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref>, but is organized somewhat differently than standard EBL methods like EBG [Mitchell et al., 1986].
Reference: [Dietterich, 1986] <author> T. G. Dietterich. </author> <title> Learning at the knowledge level. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 287-315, </pages> <year> 1986. </year>
Reference-contexts: Taking inductive steps within explanations allows explanation-based approaches to do knowledge level learning <ref> [Dietterich, 1986] </ref>, rather than simply compiling knowledge that is already present within the performance system.
Reference: [DiEugenio and Webber, 1992] <author> B. DiEugenio and B. Webber. </author> <title> Plan recognition in understanding instructions. </title> <editor> In J. Hendler, editor, </editor> <booktitle> Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 52-61, </pages> <address> College Park, MD, </address> <year> 1992. </year>
Reference: [DiEugenio and White, 1992] <author> B. DiEugenio and M. White. </author> <title> On the interpretation of natural language instructions. </title> <booktitle> In Proceedings COLING 92, </booktitle> <month> July </month> <year> 1992. </year>
Reference: [DiEugenio, 1992] <author> B. DiEugenio. </author> <title> Understanding natural language instructions: The case of purpose clauses. </title> <booktitle> In Proceedings of Annual Meeting of the ACL, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Here, the language indicates that the instruction applies whenever there is a hot stove present, no matter what other tasks are being performed. Other explicitly situated instructions include conditionals and instructions with purpose clauses <ref> [DiEugenio, 1992] </ref>, such as: For normal tape, employ the SF/NORM position. When using chocolate chips, add them to coconut mixture just before pressing into pie pan. To restart this, you can hit R or shift-R. <p> Within a single instruction, an explicitly specified goal takes the form of a purpose clause <ref> [DiEugenio, 1992] </ref>, that indicates the purpose (goal) that an action is meant to achieve. Such instructions have the form "To do X, do Y" (e.g., "To turn on the light, push the button").
Reference: [Dixon et al., 1988] <author> P. Dixon, J. Faries, and G. Gabrys. </author> <title> The role of explicit action statements in understanding and using written directions. </title> <journal> Journal of Memory and Language, </journal> <volume> 27 </volume> <pages> 649-667, </pages> <year> 1988. </year>
Reference: [Dixon, 1982] <author> Peter Dixon. </author> <title> Plans and written directions for complex tasks. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 21 </volume> <pages> 70-84, </pages> <year> 1982. </year>
Reference: [Dixon, 1987] <author> Peter Dixon. </author> <title> The processing of organizational and component step information in written directions. </title> <journal> Journal of Memory and Language, </journal> <volume> 26 </volume> <pages> 24-35, </pages> <year> 1987. </year> <month> 134 </month>
Reference-contexts: Hierarchical instruction that provides information about intermediate goals has been shown to be more effective than purely linear step-by-step instruction [Smith and Goodman, 1984; Konoske and Ellis, 1986]. Goal information also helps students interpret commands correctly <ref> [Dixon, 1987] </ref>; this is a part of the mapping problem that is not modeled by Instructo-Soar. Instructo-Soar uses a situated learning process even for instructions that apply to a situation different from the current situation.
Reference: [Drummond, 1989] <author> Mark Drummond. </author> <title> Situated control rules. </title> <booktitle> In Proceedings of the First International Conference on Principles of Knowledge Representation, </booktitle> <address> Toronto, Canada, May 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning factored pieces of knowledge rather than monolithic schemas or plan structures allows the knowledge to be applied in a more reactive fashion, since knowledge is accessed locally based on the current situation <ref> [Laird and Rosenbloom, 1990; Drummond, 1989] </ref>. This meshes with the PSCM's local control structure.
Reference: [Ellman, 1989] <author> Thomas Ellman. </author> <title> Explanation-based learning: A survey of programs and perspectives. </title> <journal> Computing Surveys, </journal> <volume> 21(2) </volume> <pages> 163-221, </pages> <year> 1989. </year>
Reference: [Emihovich and Miller, 1988] <author> Catherine Emihovich and Gloria E. Miller. </author> <title> Talking to the turtle: A discourse analysis of Logo instruction. </title> <booktitle> Discourse Processes, </booktitle> <volume> 11 </volume> <pages> 183-201, </pages> <year> 1988. </year>
Reference-contexts: Flexible initiation In human tutorial dialogues, initiation of instruction is mixed between student and teacher. One study indicates that teacher initiation is more prevalent early in instruction; student initiation increases as the student learns more, and then drops off again, as the student masters the task <ref> [Emihovich and Miller, 1988] </ref>. Teacher initiation may dominate early on because the student has so little knowledge that the teacher assumes instruction will be constantly needed. Supporting teacher-initiated instruction events is difficult because the instruction event interrupts other ongoing processing that the agent may be engaged in. <p> Agent-initiated interaction is a poor approximation to the interaction between a human instructor and student, in which both parties initiate a variety of kinds of instruction events. One study has shown that teacher-initiated instruction dominates early in learning, with student initiation increasing later <ref> [Emihovich and Miller, 1988] </ref>. Instructo-Soar does not attempt to model this pattern of interaction. There are five major behavioral properties of Instructo-Soar's procedural learning: 1. Interactive instruction requests. The agent asks for instruction as needed. 2. Initial episodic learning. The initial learning of a procedure is rote and episodic. 3.
Reference: [Eshelman et al., 1987] <author> L. Eshelman, D. Ehret, J. McDermott, and M. Tan. MOLE: </author> <title> A tenacious knowledge-acquisition tool. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 26(1) </volume> <pages> 41-54, </pages> <year> 1987. </year>
Reference: [Feltz and Landers, 1983] <author> D. L. Feltz and D. M. Landers. </author> <title> The effects of mental practice on motor skill learning and performance: A meta-analysis. </title> <journal> Journal of Sport Psychology, </journal> <volume> 5 </volume> <pages> 25-57, </pages> <year> 1983. </year>
Reference: [Fikes and Nilsson, 1971] <author> R. E. Fikes and N. J. Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving in problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: If the expected goal outcome is reached within this projection, then the projected path from the situation state through the instructed step to the goal comprises an explanation of the instruction. Forward projection is a common technique in planning (e.g., <ref> [Fikes and Nilsson, 1971; Hanks, 1990] </ref>; many others). In contrast to planning, however, the aim here is not to search by projecting possible paths, but to project the effects of a specified step.
Reference: [Fikes et al., 1972] <author> R. E. Fikes, P. E. Hart, and N. J. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3 </volume> <pages> 251-288, </pages> <year> 1972. </year>
Reference: [File and Jew, 1973] <author> S. E. File and A. Jew. </author> <title> Syntax and the recall of instructions in a realistic situation. </title> <journal> British Journal of Psychology, </journal> <volume> 64 </volume> <pages> 65-70, </pages> <year> 1973. </year>
Reference: [Fisher, 1987] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., <ref> [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982] </ref>), connectionist methods (e.g., [Rumelhart and McClelland, 1986]), and other reinforcement learning techniques (e.g., [Sutton, 1988; Holland, 1986]).
Reference: [Flann and Dietterich, 1989] <author> Nicholas S. Flann and Thomas G. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-226, </pages> <year> 1989. </year>
Reference: [Foltz et al., 1988] <author> P. E. Foltz, S. E. Davies, P. G. Polson, and D. E. Kieras. </author> <title> Transfer between menu systems. </title> <booktitle> In Proceedings of the CHI 1988 Conference on Human Factors in Computing Systems. ACM, </booktitle> <year> 1988. </year>
Reference: [Ford and Thompson, 1986] <author> Cecilia A. Ford and Sandra A. Thompson. </author> <title> Conditionals in discourse: A text-based study from English. </title> <editor> In Elizabeth Closs Traugott, editor, </editor> <booktitle> On Conditionals, </booktitle> <pages> pages 353-72. </pages> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: When using chocolate chips, add them to coconut mixture just before pressing into pie pan. To restart this, you can hit R or shift-R. As a number of researchers have pointed out <ref> [Ford and Thompson, 1986; Johnson-Laird, 1986; Haiman, 1978] </ref>, conditional clauses introduce a shared reference between speaker and hearer that forms an explicit background for interpreting or evaluating the consequent. 1 Here, the clauses in italics indicate a hypothetical situation that the command in the remainder of the instruction is meant to
Reference: [Frederking, 1988] <author> R. E. Frederking. </author> <title> Integrated natural language dialogue: A computational model. </title> <publisher> Kluwer Academic Press, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: For example: * Move arm to the right. How far? * Sprinkle with seasoned salt. Sprinkle what? How much? * Pull back left. How fast? For how long? Some of these are examples of ellipsis, where the missing information can be filled in by looking at the linguistic context <ref> [Frederking, 1988] </ref>. Other cases require domain knowledge or task context. For example, in Sprinkle with seasoned salt, the agent must use domain knowledge about seasoned salt, perhaps about what amount is typically used, to further specify the sprinkling action.
Reference: [Gershman, 1979] <author> Anatole V. Gershman. </author> <title> Knowledge-Based Parsing. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1979. </year>
Reference-contexts: Even if these types of constructions are disallowed, two difficult problems in interactive instruction are referent resolution and incompleteness. Referent resolution determining what a noun phrase refers to has been the topic of multiple Ph.D. theses (e.g., <ref> [Gershman, 1979; Grosz, 1977] </ref>). Grosz [1980,1977] specifically targeted referent resolution in expert/apprentice dialogues. Many standard reference problems appear in the sample instructions that have been collected, such as anaphors, comparatives and superlatives, generics, references to composite constructions, and episodic references.
Reference: [Ginsberg, 1988] <author> Allen Ginsberg. </author> <title> Theory revision via prior operationalization. </title> <booktitle> In Proceedings of the Seventh National Conference on Artifical Intelligence, </booktitle> <pages> pages 590-595, </pages> <year> 1988. </year>
Reference-contexts: There has been some work on recovering from incorrect knowledge in Soar [Laird, 1988; Laird et al., 1989], but the broad range of possible incorrectness in an instructable agent will require more research on basic recovery and theory revision techniques (as, e.g., <ref> [Ourston and Mooney, 1990; Ginsberg, 1988] </ref>). The combination of these changes will produce an agent that has a much more realistic pattern of interaction with its instructor.
Reference: [Golding et al., 1987] <author> A. Golding, P. S. Rosenbloom, and J. E. Laird. </author> <title> Learning search control from outside guidance. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 334-337, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: The advantage of this approach is that each step is examined by the instructor; the disadvantage, of course, is that each step must be examined. An alternative approach is to drive instruction requests by impasses in the agent's task performance <ref> [Golding et al., 1987; Laird et al., 1990] </ref>. This is the approach followed here. An impasse in task performance indicates that the agent's knowledge is lacking and it needs instruction.
Reference: [Gordon and Subramanian, 1993] <author> Diana F. Gordon and Devika Subramanian. </author> <title> A multistrategy learning scheme for assimilating advice in embedded agents. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <address> Harper's Ferry, WV, </address> <year> 1993. </year>
Reference: [Gray and Orasanu, 1987] <author> Wayne D. Gray and Judith M. Orasanu. </author> <title> Transfer of cognitive skills. </title> <editor> In S. M. Cormier and J. D. Hagman, editors, </editor> <booktitle> Transfer of learning: Contemporary research and applications, </booktitle> <pages> pages 183-215. </pages> <publisher> Academic Press, Inc., </publisher> <year> 1987. </year> <month> 135 </month>
Reference-contexts: This is negative transfer. Instructo-Soar, and Soar in general, is one of a class of theories that explain transfer by a modernized version of Thorndike's [1903] transfer of identical elements theory <ref> [Singley and Anderson, 1989; Gray and Orasanu, 1987] </ref>. The shortcoming of Thorndike's original formulation is that the identical elements were highly specific stimulus-response rules. In the modernized versions, these overly specific rules have been replaced by more general associative constructs, such as productions.
Reference: [Grosz, 1977] <author> Barabara J. Grosz. </author> <title> The Representation and use of focus in dialogue understanding. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1977. </year>
Reference-contexts: It is interactive in that the agent may request instruction as needed to complete tasks or to understand aspects of the domain or of previous instructions. This type of instruction is common in dialogues between experts and apprentices working on various tasks <ref> [Grosz, 1980; Grosz, 1977] </ref>. A segment of tutorial instruction, given to the Instructo-Soar agent within a robotic domain, is shown in Figure 1.1. Tutorial instruction can be viewed as a way of incrementally eliciting knowledge from an instructor. <p> Even if these types of constructions are disallowed, two difficult problems in interactive instruction are referent resolution and incompleteness. Referent resolution determining what a noun phrase refers to has been the topic of multiple Ph.D. theses (e.g., <ref> [Gershman, 1979; Grosz, 1977] </ref>). Grosz [1980,1977] specifically targeted referent resolution in expert/apprentice dialogues. Many standard reference problems appear in the sample instructions that have been collected, such as anaphors, comparatives and superlatives, generics, references to composite constructions, and episodic references. <p> While reading each sentence, the agent learns a set of rules that encode the sentence's semantic features. These rules allow NL-Soar to resolve referents in later sentences, by providing a memory of past referents (implementing a simple version of Grosz's focus space mechanism <ref> [Grosz, 1977] </ref>). The rules record each instruction, indexed by the goal it applies to and its place in the instruction sequence. The episodic case that results corresponds to a lock-step, overspecific sequencing of the instructions given to perform the new operator, as illustrated in Figure 3.5.
Reference: [Grosz, 1980] <author> Barbara J. Grosz. </author> <title> Focusing and description in natural language dialogues. </title> <editor> In A. K. Joshi, I. A. Sag, and B. L. Webber, editors, </editor> <booktitle> Elements of discourse understanding: Proceedings of a workshop on computational aspects of linguistic structure and discourse setting, </booktitle> <pages> pages 84-105. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England, </address> <year> 1980. </year>
Reference-contexts: It is interactive in that the agent may request instruction as needed to complete tasks or to understand aspects of the domain or of previous instructions. This type of instruction is common in dialogues between experts and apprentices working on various tasks <ref> [Grosz, 1980; Grosz, 1977] </ref>. A segment of tutorial instruction, given to the Instructo-Soar agent within a robotic domain, is shown in Figure 1.1. Tutorial instruction can be viewed as a way of incrementally eliciting knowledge from an instructor.
Reference: [Gruber, 1989] <author> T. Gruber. </author> <title> Automated knowledge acquisition for strategic knowledge. </title> <booktitle> Machine Learning, </booktitle> <address> 4(3-4):293-336, </address> <year> 1989. </year>
Reference-contexts: A number of authors have described the advantages of situation specific knowledge elicitation (e.g., <ref> [Davis, 1979; Gruber, 1989] </ref>). When instructions apply to the current situation, the situation can help to disambiguate them during their interpretation. Actions are performed in the world when they are instructed, giving the instructor direct feedback about their effects. <p> However, specific LAS's have focussed on learning particular types of knowledge: e.g., operator implementations (LEAP [Mitchell et al., 1990]), goal decomposition rules (DISCIPLE [Kodratoff and Tecuci, 1987b]), operational versions of abstract (functional) goals (ARMS [Segre, 1987]), control knowledge and control features (ASK <ref> [Gruber, 1989] </ref>), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA [VanLehn, 1987]), and heuristic classification knowledge (PROTOS [Porter et al., 1990], ODYSSEUS [Wilkins, 1990]). <p> This is the process followed by systems such as ASK <ref> [Gruber, 1989] </ref> and Robo-Soar [Laird et al., 1989]. 2. Complete the explanation of I by learning M K . 2.1. Induce M K to complete the explanation. In some cases, the agent can make a good guess at what knowledge, M K , is missing.
Reference: [Guha and Lenat, 1990] <author> R. V. Guha and D. B. Lenat. </author> <title> Cyc: A mid-term report. </title> <journal> AI Magazine, </journal> <volume> 11(3) </volume> <pages> 32-59, </pages> <year> 1990. </year>
Reference-contexts: Such theories define the functions and structures that are to be used in representing knowledge of various kinds (e.g., KL-ONE [Brachman, 1980]); in addition, some define the possible content of those structures (e.g., conceptual dependency theory [Schank, 1975], CYC <ref> [Guha and Lenat, 1990] </ref>). However, since these theories do not specify computational operations, they do not indicate how procedural knowledge is used within an agent. Computational structure must be added to these theories to produce working agents. <p> A content theory of knowledge would be useful in specifying the complete space of possible instruction, to provide a finer grained analysis of instructability within the knowledge types analysis provided by the PSCM. Projects such as Cyc <ref> [Guha and Lenat, 1990] </ref> have the goal of developing detailed knowledge content theories. Within the PSCM, an agent has been implemented with a particular set of basic capabilities, to develop and test the tutorial learning theory. Next, the limitations of this agent's capabilities are discussed.
Reference: [Haas and Hendrix, 1983] <author> Norman Haas and Gary G. Hendrix. </author> <title> Learning by being told: Acquiring knowledge for information management. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: Learning from tutorial instruction is related to work on learning from external guidance and advice taking [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983]. Some early systems learned declarative, ontological knowledge from instructions <ref> [Lindsay, 1963; Haas and Hendrix, 1983] </ref>. UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction.
Reference: [Haiman, 1978] <author> John Haiman. </author> <title> Conditionals are topics. </title> <booktitle> Language, </booktitle> <volume> 54 </volume> <pages> 564-89, </pages> <year> 1978. </year>
Reference-contexts: When using chocolate chips, add them to coconut mixture just before pressing into pie pan. To restart this, you can hit R or shift-R. As a number of researchers have pointed out <ref> [Ford and Thompson, 1986; Johnson-Laird, 1986; Haiman, 1978] </ref>, conditional clauses introduce a shared reference between speaker and hearer that forms an explicit background for interpreting or evaluating the consequent. 1 Here, the clauses in italics indicate a hypothetical situation that the command in the remainder of the instruction is meant to
Reference: [Hall, 1986] <author> R.J. Hall. </author> <title> Learning by failing to explain. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 568-572, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference: [Hanks, 1990] <author> Steven Hanks. </author> <title> Practical temporal projection. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, Mass., 1990. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: If the expected goal outcome is reached within this projection, then the projected path from the situation state through the instructed step to the goal comprises an explanation of the instruction. Forward projection is a common technique in planning (e.g., <ref> [Fikes and Nilsson, 1971; Hanks, 1990] </ref>; many others). In contrast to planning, however, the aim here is not to search by projecting possible paths, but to project the effects of a specified step.
Reference: [Hayes-Roth et al., 1981] <author> Frederick Hayes-Roth, Philip Klahr, and David J. Mostow. </author> <title> Advice taking and knowledge refinement: An iterative view of skill acquisition. </title> <editor> In John R. Anderson, editor, </editor> <booktitle> Cognitive skills and their acquisition, </booktitle> <pages> pages 231-253. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year>
Reference-contexts: UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction. It took a natural language description of a task that was isomorphic to the Tower of Hanoi problem, and built GPS-style operators and difference tables to solve the task. 15 Mostow's FOO <ref> [Mostow, 1983a; Mostow, 1981; Hayes-Roth et al., 1981] </ref> and BAR [Mostow, 1983b] were early attempts to build a system that could learn from advice. FOO and BAR take very general advice, such as "Avoid taking points" for the card game Hearts (the actual input is not in natural language).
Reference: [Holland, 1986] <author> John H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982]), connectionist methods (e.g., [Rumelhart and McClelland, 1986]), and other reinforcement learning techniques (e.g., <ref> [Sutton, 1988; Holland, 1986] </ref>). Since instructions may be explicitly situated, determining the situation for an instruction would have to be included as a precursor to correlational learning, so that the features of the situation could serve as inputs to the learning algorithm.
Reference: [Huffman and Laird, 1992] <author> Scott B. Huffman and John E. Laird. </author> <title> Dimensions of complexity in learning from interactive instruction. </title> <editor> In Jon Erickson, editor, </editor> <booktitle> Proceedings of Cooperative Intelligent Robotics in Space III, SPIE Volume 1829, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Conditional instructions are an example: "If the fluid in the cutting machine runs low, turn it off immediately." This type of instruction has been termed explicitly situated <ref> [Huffman and Laird, 1992] </ref>; the language of the instruction explicitly indicates aspects of the situation it applies to. <p> Instructions that apply to the current situation are called implicitly situated <ref> [Huffman and Laird, 1992] </ref>; the situation they are intended to apply to is implicit rather than explicitly stated. Since the instruction itself says nothing about the situation it should be applied in, the current situation (the task being performed and the current state) is implied. <p> In contrast, instructions that specify elements of the situation they are meant to apply to are explicitly situated <ref> [Huffman and Laird, 1992] </ref>. The language of the instruction explicitly indicates a hypothetical situation.
Reference: [Huffman and Laird, 1993a] <author> Scott B. Huffman and John E. Laird. Instructo-Soar: </author> <title> Learning procedures from interactive instruction (video abstract). </title> <editor> In R. Fikes and W. Lehnert, editors, </editor> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> page 857, </pages> <year> 1993. </year> <note> Video presented at AAAI-93. </note>
Reference-contexts: The computational model indicates the types of knowledge that determine an agent's performance, and thus, that should be acquirable via instruction. The learning technique specifies how the agent acquires general knowledge from instructions. 3. An implemented instructable agent that embodies the theory. The agent, called Instructo-Soar <ref> [Huffman and Laird, 1993b; Huffman and Laird, 1993a] </ref>, is built within the Soar architecture [Laird et al., 1987]. Instructo-Soar learns to perform new tasks, extends known tasks to apply in new situations, and acquires a variety of types of domain knowledge, from interactive natural language instructions. 4.
Reference: [Huffman and Laird, 1993b] <author> Scott B. Huffman and John E. Laird. </author> <title> Learning procedures from interactive natural language instructions. </title> <editor> In P. Utgoff, editor, </editor> <booktitle> Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The computational model indicates the types of knowledge that determine an agent's performance, and thus, that should be acquirable via instruction. The learning technique specifies how the agent acquires general knowledge from instructions. 3. An implemented instructable agent that embodies the theory. The agent, called Instructo-Soar <ref> [Huffman and Laird, 1993b; Huffman and Laird, 1993a] </ref>, is built within the Soar architecture [Laird et al., 1987]. Instructo-Soar learns to perform new tasks, extends known tasks to apply in new situations, and acquires a variety of types of domain knowledge, from interactive natural language instructions. 4. <p> Chapter 4 describes learning of each of the remaining types of PSCM knowledge, and the handling of explicitly situated instructions. Chapter 3 Instructo-Soar: Learning and Extending Procedures This chapter and the following one present Instructo-Soar <ref> [Huffman and Laird, 1993b] </ref>, an implemented instructable agent that embodies the theory of learning from tutorial instruction described in Chapter 2.
Reference: [Huffman and Laird, 1994] <author> Scott B. Huffman and John E. Laird. </author> <title> Acquiring procedures from tutorial instruction. </title> <editor> In B. Gaines, editor, </editor> <booktitle> Proceedings of the 1994 Knowledge Acquisition for Knowledge-Based Systems Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: Specifying these types of knowledge defines the tutorial learning task and provides an evaluation metric for an instructable agent: it should be able to learn each of its knowledge types from instruction. Second, a learning technique, called situated explanation <ref> [Huffman and Laird, 1994] </ref>, specifies how an instructable agent can derive general knowledge from the instructions it is given. The notion of explanation that is employed allows for both deductive and inductive steps, and supports interactive instruction by allowing multiple, embedded levels of explanation and instruction taking.
Reference: [Huffman et al., 1993a] <author> Scott B. Huffman, Craig S. Miller, and John E. Laird. </author> <title> Learning from instruction: A knowledge-level capability within a unified theory of cognition. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 114-119, </pages> <year> 1993. </year> <month> 136 </month>
Reference-contexts: A key contribution is that this breadth of behavior is produced not by nineteen different mechanisms, but by a single theory applied across a breadth of tutorial learning tasks. A secondary goal has been to evaluate the theory as a cognitive model of human tutorial learning <ref> [Huffman et al., 1993a] </ref>. The results are broadly consistent with what is known about human procedural learning, and make some specific behavioral predictions. These are discussed in Chapter 5. The remainder of this chapter discusses the properties and requirements of tutorial instruction. <p> of Soar's impact on Instructo-Soar indicates that the properties of Soar have a large influence on properties and predictions of the model, showing how the use of a UTC can provide constraint in model building. 1 This methodology for UTC-based microtheory analysis was developed with Craig Miller and John Laird <ref> [Huffman et al., 1993a] </ref>. 75 5.1 Instructo-Soar as a model of procedural learning from tutorial in struction To focus the discussion, only the central functionality of Instructo-Soar will be examined as a cognitive model: the learning of new procedures from situated interactive instructions, described in Chapter 3.
Reference: [Huffman et al., 1993b] <author> Scott B. Huffman, Douglas J. Pearson, and John E. Laird. </author> <title> Correcting imperfect domain theories: A knowledge-level analysis. </title> <editor> In Susan Chipman and Alan L. Mey-rowitz, editors, </editor> <title> Foundations of Knowledge Acquisition: Cognitive Models of Complex Learning. </title> <publisher> Kluwer Academic, </publisher> <year> 1993. </year> <note> Also available as technical report number CSE-TR-114-91, </note> <institution> University of Michigan Department of Electrical Engineering and Computer Science, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: A complete tutorable agent must be able to perform and learn effectively in the face of incorrect knowledge. 7. Learning from instruction coexisting with learning from other sources. In addition to instruction, a complete agent should be able to learn from other sources of knowledge that are available <ref> [Huffman et al., 1993b] </ref>, such as external observation, analogy to other domains, etc. These other types of learning may interact with instructional learning. Instructions may suggest their use (e.g., "Try this out and notice what happens").
Reference: [Huffman, 1992] <author> Scott B. Huffman. </author> <title> A problem space and symbol level description of Instructo-Soar. </title> <institution> Artificial Intelligence Laboratory, University of Michigan., </institution> <year> 1992. </year>
Reference-contexts: The following sections describe the learning of a new procedure, the extension of a known procedure to apply in a new situation, and procedure learning when faced with incomplete underlying domain knowledge. More technical details about Instructo-Soar's implementation can be found in <ref> [Huffman, 1992] </ref>. In what follows, the instructor's inputs are given in typewriter font and Instructo-Soar's responses in italics. 39 Pick up the red block. Move to the yellow table. Move the arm above the red block. Move up. Move down. Close the hand. Move up.
Reference: [Johnson-Laird, 1986] <author> P. N. Johnson-Laird. </author> <title> Conditionals and mental models. </title> <editor> In Elizabeth Closs Traugott, editor, </editor> <booktitle> On Conditionals. </booktitle> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: When using chocolate chips, add them to coconut mixture just before pressing into pie pan. To restart this, you can hit R or shift-R. As a number of researchers have pointed out <ref> [Ford and Thompson, 1986; Johnson-Laird, 1986; Haiman, 1978] </ref>, conditional clauses introduce a shared reference between speaker and hearer that forms an explicit background for interpreting or evaluating the consequent. 1 Here, the clauses in italics indicate a hypothetical situation that the command in the remainder of the instruction is meant to
Reference: [Johnson, 1982] <author> P. Johnson. </author> <title> The functional equivalence of imagery and movement. </title> <journal> Quarterly Journal of Experimental Psychology, </journal> <volume> 34A:349-365, </volume> <year> 1982. </year>
Reference: [Jones, 1966] <author> S. Jones. </author> <title> The effect of a negative qualifier in an instruction. </title> <journal> Journal of Verbal Learning and Verbal Behavior, </journal> <volume> 5 </volume> <pages> 497-501, </pages> <year> 1966. </year>
Reference: [Judd, 1908] <author> C. H. Judd. </author> <title> The relation of special training and intelligence. </title> <journal> Educational Review, </journal> <volume> 36 </volume> <pages> 28-42, </pages> <year> 1908. </year>
Reference: [Just and Carpenter, 1976] <author> Marcel A. Just and Patricia A. Carpenter. </author> <title> Verbal comprehension in instructional situations. </title> <editor> In David Klahr, editor, </editor> <title> Cognition and Instruction. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1976. </year>
Reference: [Kalita and Badler, 1990] <author> Jugal K. Kalita and Norman I. Badler. </author> <title> A semantic analysis of action verbs based on physical primitives. </title> <booktitle> In Proceedings of the Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 412-419, </pages> <year> 1990. </year>
Reference: [Kalita, 1991] <author> Jugal K. Kalita. </author> <title> Natural language control of animation of task performance in a physical domain. </title> <type> PhD thesis, </type> <institution> Univ. of Pennsylvania, Dept. of Computer and Information Science, </institution> <month> June </month> <year> 1991. </year>
Reference: [Kautz and Allen, 1986] <author> Henry A. Kautz and James F. Allen. </author> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 32-37, </pages> <year> 1986. </year>
Reference-contexts: An exception is learning apprentice systems that learn by observing expert behavior, where the goals that observed actions are meant to achieve may not be stated. Determining goals by observing actions is a process of plan recognition <ref> [Redmond, 1992; Kautz and Allen, 1986] </ref>. In the type of instruction considered here, however, plan recognition is not necessary, because the goal that an instruction is intended to achieve is always either the current goal or a stated, hypothetical goal. * Other required reasoning steps.
Reference: [Kieras and Bovair, 1984] <author> David E. Kieras and Susan Bovair. </author> <title> The role of a mental model in learning to operate a device. </title> <journal> Cognitive Science, </journal> <volume> 8 </volume> <pages> 255-273, </pages> <year> 1984. </year>
Reference-contexts: In a more complex domain, inferring general implementations would be even less successful. Psychological research shows that subjects' learning when given procedural instructions degrades if they do not have a domain model <ref> [Kieras and Bovair, 1984] </ref>. The process of directly learning operator proposal rules allows the agent to overcome incomplete domain knowledge, but is burdensome to the instructor, who must examine and alter/verify the conditions of proposing each operator.
Reference: [Kieras and Bovair, 1986] <author> David E. Kieras and Susan Bovair. </author> <title> The acquisition of procedures from text: A production-system analysis of transfer of training. </title> <journal> Journal of Memory and Language, </journal> <volume> 25 </volume> <pages> 507-524, </pages> <year> 1986. </year>
Reference-contexts: Models built within the ACT* theory have focused primarily on situations in which there is little prior domain knowledge, and initial performance of tasks requires applying weak methods. Other ACT* type theories include models by Lewis et al. [1989] and Bovair and Kieras <ref> [Bovair, 1991; Bovair and Kieras, 1991; Kieras and Bovair, 1986] </ref>. Lewis et al. [1989] describe a system, built in Soar, that reads instructions to perform a subject's part in simple psychological experiments. The instructions are steps in a procedure, that are read as a "recipe" in a non-interactive fashion.
Reference: [Kintsch, 1986] <author> Walter Kintsch. </author> <title> Learing from text. </title> <journal> Cognition and Instruction, </journal> <volume> 3(2) </volume> <pages> 87-108, </pages> <year> 1986. </year>
Reference-contexts: This idea underlies the increase in training through computer simulated practice (e.g., pilot training) [Lintern, 1991]. Actually performing procedural instructions produces different memory effects than simply memorizing them as text <ref> [Kintsch, 1986] </ref>. In fact, students seem to prefer instructions that can be performed to achieve a particular task over more general, unsituated instruction.
Reference: [Kodratoff and Tecuci, 1987a] <editor> Yves Kodratoff and Gheorghe Tecuci. DISCIPLE-1: </editor> <title> Interactive apprentice system in weak theory fields. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 271-273, </pages> <month> August </month> <year> 1987. </year>
Reference: [Kodratoff and Tecuci, 1987b] <editor> Yves Kodratoff and Gheorghe Tecuci. </editor> <title> Techniques of design and DISCIPLE learning apprentice. </title> <journal> International Journal of Expert Systems, </journal> <volume> 1(1) </volume> <pages> 39-66, </pages> <year> 1987. </year>
Reference-contexts: Likewise, in this work, instructor-initiated instruction events are not supported. 9 Agent-initiated instruction can be directed in (at least) two possible ways: verification driven or impasse driven. Some learning apprentice systems, such as LEAP [Mitchell et al., 1990] and DISCIPLE <ref> [Kodratoff and Tecuci, 1987b] </ref> ask the instructor to verify or alter each reasoning step. The advantage of this approach is that each step is examined by the instructor; the disadvantage, of course, is that each step must be examined. <p> LAS's have used both analytic and inductive learning methods to learn in a variety of domains. However, specific LAS's have focussed on learning particular types of knowledge: e.g., operator implementations (LEAP [Mitchell et al., 1990]), goal decomposition rules (DISCIPLE <ref> [Kodratoff and Tecuci, 1987b] </ref>), operational versions of abstract (functional) goals (ARMS [Segre, 1987]), control knowledge and control features (ASK [Gruber, 1989]), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA [VanLehn, 1987]), and heuristic classification knowledge (PROTOS [Porter et al., 1990], ODYSSEUS [Wilkins, 1990]).
Reference: [Konoske and Ellis, 1986] <author> P. J. Konoske and E. G. Ellis. </author> <title> Cognitive factors in learning and retention of procedural tasks. </title> <type> Technical Report NPRDC 87-14, </type> <institution> Navy Personnel Research and Development Center, </institution> <address> San Diego, CA, </address> <year> 1986. </year>
Reference-contexts: Instructo-Soar predicts an advantage for hierarchical instruction because it breaks a procedure into smaller intermediate goals that are easier to explain. Hierarchical instruction that provides information about intermediate goals has been shown to be more effective than purely linear step-by-step instruction <ref> [Smith and Goodman, 1984; Konoske and Ellis, 1986] </ref>. Goal information also helps students interpret commands correctly [Dixon, 1987]; this is a part of the mapping problem that is not modeled by Instructo-Soar.
Reference: [Laird and Rosenbloom, 1990] <author> John E. Laird and Paul S. Rosenbloom. </author> <title> Integrating execution, planning, and learning in Soar for external environments. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1029, </pages> <address> Boston, Mass., 1990. </address> <publisher> AAAI Press. </publisher> <pages> 137 </pages>
Reference-contexts: Learning factored pieces of knowledge rather than monolithic schemas or plan structures allows the knowledge to be applied in a more reactive fashion, since knowledge is accessed locally based on the current situation <ref> [Laird and Rosenbloom, 1990; Drummond, 1989] </ref>. This meshes with the PSCM's local control structure.
Reference: [Laird et al., 1987] <author> John E. Laird, Allen Newell, and Paul S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: The learning technique specifies how the agent acquires general knowledge from instructions. 3. An implemented instructable agent that embodies the theory. The agent, called Instructo-Soar [Huffman and Laird, 1993b; Huffman and Laird, 1993a], is built within the Soar architecture <ref> [Laird et al., 1987] </ref>. Instructo-Soar learns to perform new tasks, extends known tasks to apply in new situations, and acquires a variety of types of domain knowledge, from interactive natural language instructions. 4. An evaluation of the agent based on the requirements for complete tutorability. <p> The "operations and entities" within the PSCM are at a level above the symbol level, and approximating the knowledge level [Newell et al., 1990], thus meeting the requirement of close correspondence to knowledge level components. Soar <ref> [Laird et al., 1987] </ref> is one symbol level implementation of the PSCM, and has been selected as the platform in which to implement the instructable agent described here. 20 perceptual modules motor modules external environment represent problem spaces; squares, states; arrows, operators; and ovals, impasses. 21 A schematic of a PSCM <p> Chapter 3 Instructo-Soar: Learning and Extending Procedures This chapter and the following one present Instructo-Soar [Huffman and Laird, 1993b], an implemented instructable agent that embodies the theory of learning from tutorial instruction described in Chapter 2. Instructo-Soar is implemented in the Soar architecture <ref> [Laird et al., 1987] </ref>, and has been applied to a simulated robotic domain that includes a robot, tables, blocks, and various other objects to be controlled or manipulated.
Reference: [Laird et al., 1989] <author> John E. Laird, Eric S. Yager, Christopher M. Tuck, and Michael Hucka. </author> <title> Learning in tele-autonomous systems using Soar. </title> <booktitle> In Proceedings of the NASA Conference on Space Telerobotics, </booktitle> <year> 1989. </year>
Reference-contexts: This is the process followed by systems such as ASK [Gruber, 1989] and Robo-Soar <ref> [Laird et al., 1989] </ref>. 2. Complete the explanation of I by learning M K . 2.1. Induce M K to complete the explanation. In some cases, the agent can make a good guess at what knowledge, M K , is missing. <p> Since the inferred knowledge may be incorrect, however, the agent will require the ability to recover from incorrect knowledge. There has been some work on recovering from incorrect knowledge in Soar <ref> [Laird, 1988; Laird et al., 1989] </ref>, but the broad range of possible incorrectness in an instructable agent will require more research on basic recovery and theory revision techniques (as, e.g., [Ourston and Mooney, 1990; Ginsberg, 1988]).
Reference: [Laird et al., 1990] <author> John E. Laird, Michael Hucka, Eric S. Yager, and Christopher M. Tuck. </author> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <year> 1990. </year>
Reference-contexts: The advantage of this approach is that each step is examined by the instructor; the disadvantage, of course, is that each step must be examined. An alternative approach is to drive instruction requests by impasses in the agent's task performance <ref> [Golding et al., 1987; Laird et al., 1990] </ref>. This is the approach followed here. An impasse in task performance indicates that the agent's knowledge is lacking and it needs instruction.
Reference: [Laird et al., 1993] <author> John E. Laird, Clare Bates Congdon, Erik Altmann, and Robert Doorenbos. </author> <title> Soar user's manual, </title> <type> version 6, </type> <year> 1993. </year>
Reference-contexts: There 1 The details of how Soar variablizes rules are given in <ref> [Laird et al., 1993] </ref>. 130 State: Op: new-op14 (blk-r) Op: recall &gt; move-to-table (tbl-y) Op: eval [move-to-table (tbl-y)] on (blk-r,tbl-y) open (gripper) color (blk-r,red) color (tbl-y,yellow) size (blk-r,small) not at (robot,tbl-y) move-to-table (tbl-y) at (robot,tbl-y) on (blk-r,tbl-y) m ove arm (d own ) m o v e arm (u p )
Reference: [Laird, 1983] <author> John E. Laird. </author> <title> Universal Subgoaling. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie-Mellon University, </institution> <year> 1983. </year>
Reference-contexts: Newell [1980, 1990] has gone so far as to propose problem spaces as the basis for general intelligent behavior (the problem space hypothesis). The PSCM supports a full range of behavior from search based knowledge lean behavior (any weak method can be built in PSCM <ref> [Laird, 1983] </ref>) to knowledge rich behavior that does not require search. A least commitment, local type of control is supported, because operators are selected sequentially based on local information.
Reference: [Laird, 1988] <author> John E. Laird. </author> <title> Recovery from incorrect knowledge in Soar. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 618-623, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Productions are never removed from long-term memory; once learned, chunks cannot be forgotten. This contrasts with work in machine learning involving deliberate forgetting mechanisms (e.g., [Markovitch and Scott, 1988; Salgani-coff, 1993]). However, the effects of a chunk can be overridden by other chunks to recover from incorrect learning <ref> [Laird, 1988] </ref>. [2,4,6] 2 However, since learning is dependent on performance and knowledge, chunking can form the basis for many different learning strategies (e.g., induction, analogical learning, abstraction, etc.). <p> Since the inferred knowledge may be incorrect, however, the agent will require the ability to recover from incorrect knowledge. There has been some work on recovering from incorrect knowledge in Soar <ref> [Laird, 1988; Laird et al., 1989] </ref>, but the broad range of possible incorrectness in an instructable agent will require more research on basic recovery and theory revision techniques (as, e.g., [Ourston and Mooney, 1990; Ginsberg, 1988]).
Reference: [Lave and Wenger, 1991] <author> Jean Lave and Etienne Wenger. </author> <title> Situated Learning: Legitimate peripheral participation. </title> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference: [Lehman et al., 1991] <author> Jill Fain Lehman, Richard L. Lewis, and Allen Newell. </author> <title> Natural language comprehension in Soar: Spring 1991. </title> <type> Technical Report CMU-CS-91-117, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: The problem spaces implement an agent with three main categories of knowledge: natural language processing knowledge, originally developed for NL-Soar <ref> [Lewis, 1993; Lehman et al., 1991] </ref>; knowledge about obtaining and using instruction; and knowledge of the task domain itself. This task knowledge is extended through learning from instruction. The agent's knowledge of how to obtain, use, and learn from instructions codifies the theory of learning from tutorial instruction. <p> The agent does not expand its interaction or natural language capabilities per se as it takes instruction, although it does learn how sentences map onto new operators that are learned. Reading is done by an augmented version of NL-Soar <ref> [Lewis, 1993; Lehman et al., 1991] </ref>, a natural language theory being developed within Soar. NL-Soar learns to speed up its processing as it reads more and more sentences.
Reference: [Lehman et al., 1993] <author> Jill Fain Lehman, Allen Newell, Thad Polk, and Richard Lewis. </author> <title> The role of language in cognition: A computational inquiry. In Conceptions of the Human Mind. </title> <publisher> Lawrence Erlbaum Associates, Inc, </publisher> <year> 1993. </year>
Reference-contexts: There has been some initial work on how NL-Soar might make use of domain knowledge during language comprehension <ref> [Lehman et al., 1993] </ref>, but Instructo-Soar does not do this kind of reasoning in interpreting instructions. In addition to these general problems, Instructo-Soar does not support the full range of linguistic forms that might be used to communicate each of the types of PSCM knowledge that it can learn.
Reference: [Lewis et al., 1989] <author> Richard L. Lewis, Allen Newell, and Thad A. Polk. </author> <title> Toward a Soar theory of taking instructions for immediate reasoning tasks. </title> <booktitle> In Proceedings of the Annual Conference of the Cognitive Science Society, </booktitle> <month> August </month> <year> 1989. </year>
Reference: [Lewis, 1988] <author> Clayton Lewis. </author> <title> Why and how to learn why: Analysis-based generalization of procedures. </title> <journal> Cognitive Science, </journal> <volume> 12 </volume> <pages> 211-256, </pages> <year> 1988. </year>
Reference-contexts: Thus, Instructo-Soar takes the overly conservative approach of leaving the case in a fully specific, rote form. There is some evidence that people are also conservative in generalizing memory of procedural steps <ref> [Lewis, 1988] </ref>. Finally, after carrying out all of the steps for picking up the red block, the agent is told "The operator is finished," indicating that the goal of the new operator has been achieved.
Reference: [Lewis, 1993] <author> Richard L. Lewis. </author> <title> An Architecturally-Based Theory of Human Sentence Comprehension. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1993. </year> <month> Forthcoming. </month>
Reference-contexts: The problem spaces implement an agent with three main categories of knowledge: natural language processing knowledge, originally developed for NL-Soar <ref> [Lewis, 1993; Lehman et al., 1991] </ref>; knowledge about obtaining and using instruction; and knowledge of the task domain itself. This task knowledge is extended through learning from instruction. The agent's knowledge of how to obtain, use, and learn from instructions codifies the theory of learning from tutorial instruction. <p> The agent does not expand its interaction or natural language capabilities per se as it takes instruction, although it does learn how sentences map onto new operators that are learned. Reading is done by an augmented version of NL-Soar <ref> [Lewis, 1993; Lehman et al., 1991] </ref>, a natural language theory being developed within Soar. NL-Soar learns to speed up its processing as it reads more and more sentences.
Reference: [Lindsay, 1963] <author> Robert K. Lindsay. </author> <title> Inferential memory as the basis of machines which understand natural language. </title> <editor> In E. A. Feigenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 217-233. </pages> <editor> R. Oldenbourg KG., </editor> <year> 1963. </year>
Reference-contexts: Learning from tutorial instruction is related to work on learning from external guidance and advice taking [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983]. Some early systems learned declarative, ontological knowledge from instructions <ref> [Lindsay, 1963; Haas and Hendrix, 1983] </ref>. UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction.
Reference: [Lintern, 1991] <author> Gavan Lintern. </author> <title> Instructional strategies. </title> <editor> In J. E. Morrison, editor, </editor> <title> Training for performance: </title> <booktitle> Principles of applied human learning, </booktitle> <pages> pages 167-191. </pages> <publisher> John Wiley and Sons Ltd., </publisher> <year> 1991. </year>
Reference-contexts: This idea underlies the increase in training through computer simulated practice (e.g., pilot training) <ref> [Lintern, 1991] </ref>. Actually performing procedural instructions produces different memory effects than simply memorizing them as text [Kintsch, 1986]. In fact, students seem to prefer instructions that can be performed to achieve a particular task over more general, unsituated instruction.
Reference: [Litman and Allen, 1987] <author> Diane J. Litman and James F. Allen. </author> <title> A plan recognition model for subdialogues in conversations. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 163-200, </pages> <year> 1987. </year>
Reference: [Litman and Allen, 1990] <author> Diane J. Litman and James F. Allen. </author> <title> Discourse processing and common-sense plans. </title> <editor> In P. Cohen, J. Morgan, and M. Pollack, editors, </editor> <booktitle> Intention in Communication, </booktitle> <pages> pages 365-388. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference: [Luchins, 1942] <author> A. Luchins. </author> <title> Mechanization of problem solving. </title> <journal> Psychological Monographs, </journal> <volume> 54(248), </volume> <year> 1942. </year> <month> 138 </month>
Reference-contexts: At this point: (7). Performance is automatic, occurring recognitionally; thus, performance is rapid. Subjects' knowledge of what action to perform next is retrieved directly from long-term memory based on current situational features. (8). Subjects will exhibit the Einstellung effect <ref> [Luchins, 1942] </ref>; that is, once a method is learned for a particular task, that method will continue to be used even if a more efficient method is available. This is because after general learning, performance is automatic and recognitional. (9).
Reference: [Mann and Thompson, 1988] <author> William C. Mann and Sandra A. Thompson. </author> <title> Rhetorical structure theory: Toward a functional theory of text organization. </title> <booktitle> Text, </booktitle> <volume> 8(3) </volume> <pages> 243-281, </pages> <year> 1988. </year>
Reference-contexts: Flexibility of knowledge content A complete tutorable agent must handle instruction events involving any knowledge that is applicable in some way within the ongoing task and discourse context. Similar to the notion of discourse coherence <ref> [Mann and Thompson, 1988] </ref>, a complete tutorable agent needs to support any instruction event with knowledge coherence; that is, any instruction event delivering knowledge that makes sense in current context.
Reference: [Marcus and McDermott, 1989] <author> S. Marcus and J. McDermott. </author> <title> SALT: A knowledge acquisition language for propose-and-revise systems. </title> <journal> Artificial Intelligence, </journal> <volume> 39(1) </volume> <pages> 1-37, </pages> <year> 1989. </year>
Reference-contexts: Thus, it is desirable for the computational model of an instructable agent to include control knowledge. Another class of computational models with knowledge level components are the problem solving methods that have been defined in the knowledge acquisition community; for example, heuristic classification [Clancy, 1985], propose-and-refine constraint satisfaction <ref> [Marcus and McDermott, 1989] </ref>, and the various generic tasks methods [Chandrasekaran, 1986]. These methods specify the knowledge they require in terms of knowledge roles [McDermott, 1988] within a pre-defined control structure.
Reference: [Markovitch and Scott, 1988] <author> Shaul Markovitch and Paul D. Scott. </author> <title> The role of forgetting in learning. </title> <editor> In J. Laird, editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> Ann Arbor, MI, </address> <year> 1988. </year>
Reference-contexts: The chunking process is impenetrable to the agent; it cannot learn to alter its architectural learning process. 2 [6] 13. Monotonic learning. Productions are never removed from long-term memory; once learned, chunks cannot be forgotten. This contrasts with work in machine learning involving deliberate forgetting mechanisms (e.g., <ref> [Markovitch and Scott, 1988; Salgani-coff, 1993] </ref>).
Reference: [Martin and Firby, 1991] <author> Charles E. Martin and R. James Firby. </author> <title> Generating natural language expectations from a reactive execution system. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 811-815, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Instructions that give incomplete information are common in human instructional dialogues. For instance, objects to be acted upon might be incompletely specified, as in "Line up three objects." Alternatively, an action itself may be incompletely specified, as in "Use the shovel" <ref> [Martin and Firby, 1991] </ref>. Instructo-Soar does not handle these kinds of incomplete instructions; it requires well specified actions with objects given in a particular order. Finally, interpreting an instruction may require reasoning about the task itself.
Reference: [McCarthy, 1968] <author> J. McCarthy. </author> <title> The advice taker. </title> <editor> In M. Minsky, editor, </editor> <booktitle> Semantic Information Processing, </booktitle> <pages> pages 403-410. </pages> <publisher> MIT Press, </publisher> <year> 1968. </year>
Reference-contexts: However, Homer does not learn procedures or other kinds of domain knowledge from sequences of instruction, or make use of prior knowledge to produce general learning. Learning from tutorial instruction is related to work on learning from external guidance and advice taking <ref> [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983] </ref>. Some early systems learned declarative, ontological knowledge from instructions [Lindsay, 1963; Haas and Hendrix, 1983]. UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction.
Reference: [McDermott, 1988] <author> J. McDermott. </author> <title> Preliminary steps toward a taxonomy of problem-solving methods. </title> <editor> In S. Marcus, editor, </editor> <booktitle> Automating Knowledge Acquisition for Knowledge Based Systems, </booktitle> <pages> pages 120-146. </pages> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: Thus, the user had to understand the internal workings of the 16 system. More recent work has focused on method-specific elicitation techniques, based on McDer-mott's idea of knowledge roles <ref> [McDermott, 1988] </ref> and Chandrasekaran's generic tasks framework [Chandrasekaran, 1986]. These knowledge-level techniques gain their power by limiting application to a specific problem solving method, with a pre-defined set of knowledge roles [Birmingham and Klinker, 1993]. <p> These methods specify the knowledge they require in terms of knowledge roles <ref> [McDermott, 1988] </ref> within a pre-defined control structure. Because they are designed for particular families of tasks, these methods do not support general computation in a flexible way, and thus are not an appropriate choice for the computational model of a general instructable agent.
Reference: [Michalski and Stepp, 1983] <author> R. S. Michalski and Robert E. Stepp. </author> <title> Learning from observation: Conceptual clustering. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning AnArtificial Intelligence Approach. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., <ref> [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982] </ref>), connectionist methods (e.g., [Rumelhart and McClelland, 1986]), and other reinforcement learning techniques (e.g., [Sutton, 1988; Holland, 1986]).
Reference: [Miller and Johnson-Laird, 1976] <author> George A. Miller and Philip N. Johnson-Laird. </author> <title> Language and Perception. </title> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge, </address> <year> 1976. </year>
Reference-contexts: Unfortunately, a complete characterization of actions that may need to be mapped from instruction is hard to find. There have been some limited attempts to characterize the space of possible actions in AI (e.g., [Schank, 1975; Wilks, 1975]) and linguistics (e.g, <ref> [Talmy, 1985; Miller and Johnson-Laird, 1976] </ref>). Talmy [1985], for instance, analyzes the semantics of utterances specifying motion, laying out a dizzying thirty-seven motion-related semantic categories. Some of the most relevant are shown in Table 1.1.
Reference: [Miller, 1993] <author> Craig M. Miller. </author> <title> A model of concept acquisition in the context of a unified theory of cognition. </title> <type> PhD thesis, </type> <institution> The University of Michigan, Dept. of Computer Science and Electrical Engineering, </institution> <year> 1993. </year>
Reference-contexts: However, induction over multiple examples could also be employed within an explanation-based approach (e.g., <ref> [Miller, 1993] </ref>). 32 2.3.2 Case-based learning A second alternative would be to employ case-based reasoning methods (e.g., [Redmond, 1992; Alterman et al., 1991; Schank and Leake, 1989]) to learn from instruction. A case might consist of the sequence of instructions for a procedure. <p> Instructo-Soar performs the inductive learning of termination conditions by EBL (chunking) over an overgeneral theory one that is able to generate any of the features of the current state (similar to <ref> [Miller, 1993; VanLehn et al., 1990; Rosenbloom and Aasman, 1990] </ref>).
Reference: [Minton et al., 1989] <author> Steven Minton, Jaime G. Carbonell, Craig A. Knoblock, Daniel R. Kuokka, Oren Etzioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problem-solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference: [Mitchell et al., 1986] <author> Tom M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: Knowledge of the operator's application in this case equals knowledge of the selection of the sub-operators. Thus, this is equivalent to operator selection knowledge (2, above). 4. Operator termination. An operator must be terminated when its actions have been completed. The termination conditions, or goal concept <ref> [Mitchell et al., 1986] </ref>, of an operator indicate the state conditions that the operator is meant to achieve. For example, the termi nation conditions of pick-up (blk) might be that blk is held and the arm is raised. Each of these functions is performed with knowledge. <p> To complete an explanation of an instruction, an agent must bring its prior knowledge to bear to complete the path through the instruction to achievement of the goal the instruction applies to. Early work in explanation-based learning (EBL) <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref> required deductive explanations or "proofs" built from a complete and correct domain theory. Tutorial instruction requires a broader notion of explanation, since the point of receiving instruction at all is that the agent's knowledge is incomplete in various ways. <p> The overall example of learning the "pick up" procedure from instruction is described in Chapter 3. C.1 Chunking General rules are created by Soar's learning mechanism, chunking, applied to the successful forward projection of an instruction. Chunking is a form of explanation-based learning <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref>, but is organized somewhat differently than standard EBL methods like EBG [Mitchell et al., 1986]. <p> C.1 Chunking General rules are created by Soar's learning mechanism, chunking, applied to the successful forward projection of an instruction. Chunking is a form of explanation-based learning [Mitchell et al., 1986; DeJong and Mooney, 1986], but is organized somewhat differently than standard EBL methods like EBG <ref> [Mitchell et al., 1986] </ref>. The relationship between chunking and standard EBL notions like training examples and goal concepts is described in more detail by Laird and Rosenbloom [1986] and by Ellman [1988]. In Soar, learning occurs whenever a result is returned from within a subgoal to resolve an impasse.
Reference: [Mitchell et al., 1990] <author> T. M. Mitchell, Sridhar Mahadevan, and Louis I. Steinberg. </author> <title> LEAP: A learning apprentice system for VLSI design. </title> <editor> In Yves Kodratoff and R. S. Michalski, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Vol. III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Likewise, in this work, instructor-initiated instruction events are not supported. 9 Agent-initiated instruction can be directed in (at least) two possible ways: verification driven or impasse driven. Some learning apprentice systems, such as LEAP <ref> [Mitchell et al., 1990] </ref> and DISCIPLE [Kodratoff and Tecuci, 1987b] ask the instructor to verify or alter each reasoning step. The advantage of this approach is that each step is examined by the instructor; the disadvantage, of course, is that each step must be examined. <p> Learning apprentice systems have been defined as "interactive, knowledge-based consultants that directly assimilate new knowledge by observing and analyzing the problem-solving steps contributed by their users through their normal use of the system" <ref> [Mitchell et al., 1990] </ref>. Tutorial instruction extends past work in LAS's in two directions: interaction flexibility and types of knowledge learned. <p> LAS's have used both analytic and inductive learning methods to learn in a variety of domains. However, specific LAS's have focussed on learning particular types of knowledge: e.g., operator implementations (LEAP <ref> [Mitchell et al., 1990] </ref>), goal decomposition rules (DISCIPLE [Kodratoff and Tecuci, 1987b]), operational versions of abstract (functional) goals (ARMS [Segre, 1987]), control knowledge and control features (ASK [Gruber, 1989]), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA [VanLehn, 1987]), and heuristic classification knowledge (PROTOS [Porter et
Reference: [Mitchell, 1982] <author> Tom M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., <ref> [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982] </ref>), connectionist methods (e.g., [Rumelhart and McClelland, 1986]), and other reinforcement learning techniques (e.g., [Sutton, 1988; Holland, 1986]). <p> Since Instructo-Soar remembers a single version of each piece of knowledge, inability to recover from incorrect knowledge rules out learning from multiple examples, which would require changing the single hypothesis (an alternative would be to explicitly represent multiple hypotheses; e.g., in a version space <ref> [Mitchell, 1982] </ref>). Similarly, it precludes instruction by general case and exceptions; for instance, "Never grasp red blocks," and then later, "It's ok to grasp the ones with safety signs on them." 1 Recovery from incorrect knowledge is discussed further below as one component of a specific proposal for future work.
Reference: [Mohammed and Swales, 1984] <author> M. A. H. Mohammed and J. M. Swales. </author> <title> Factors affecting the successful reading of technical instructions. Reading in a Foreign Language, </title> <booktitle> 2 </booktitle> <pages> 206-217, </pages> <year> 1984. </year>
Reference: [Mooney, 1990] <author> Raymond J. Mooney. </author> <title> Learning plan schemata from observation: Explanation-based learning for plan recognition. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 483-509, </pages> <year> 1990. </year>
Reference-contexts: Applying explanation to single steps results in knowl 24 edge applicable at each step (e.g.,[Laird et al., 1989; Golding et al., 1987]); explaining full sequences of reasoning steps results in learning schemas that encode the whole reasoning episode (e.g., <ref> [Mooney, 1990; Schank and Leake, 1989; VanLehn, 1987] </ref>). Learning factored pieces of knowledge rather than monolithic schemas or plan structures allows the knowledge to be applied in a more reactive fashion, since knowledge is accessed locally based on the current situation [Laird and Rosenbloom, 1990; Drummond, 1989].
Reference: [Mostow, 1981] <author> David J. Mostow. </author> <title> Mechanical transformation of task heurisitics into operational procedures. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, Department of Computer Science, </institution> <month> April </month> <year> 1981. </year>
Reference-contexts: UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction. It took a natural language description of a task that was isomorphic to the Tower of Hanoi problem, and built GPS-style operators and difference tables to solve the task. 15 Mostow's FOO <ref> [Mostow, 1983a; Mostow, 1981; Hayes-Roth et al., 1981] </ref> and BAR [Mostow, 1983b] were early attempts to build a system that could learn from advice. FOO and BAR take very general advice, such as "Avoid taking points" for the card game Hearts (the actual input is not in natural language).
Reference: [Mostow, 1983a] <author> D. J. Mostow. </author> <title> Learning by being told: Machine transformation of advice into a heuristic search procedure. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year> <month> 139 </month>
Reference-contexts: UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction. It took a natural language description of a task that was isomorphic to the Tower of Hanoi problem, and built GPS-style operators and difference tables to solve the task. 15 Mostow's FOO <ref> [Mostow, 1983a; Mostow, 1981; Hayes-Roth et al., 1981] </ref> and BAR [Mostow, 1983b] were early attempts to build a system that could learn from advice. FOO and BAR take very general advice, such as "Avoid taking points" for the card game Hearts (the actual input is not in natural language).
Reference: [Mostow, 1983b] <author> Jack Mostow. </author> <title> A problem-solver for making advice operational. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 279-283, </pages> <year> 1983. </year>
Reference-contexts: It took a natural language description of a task that was isomorphic to the Tower of Hanoi problem, and built GPS-style operators and difference tables to solve the task. 15 Mostow's FOO [Mostow, 1983a; Mostow, 1981; Hayes-Roth et al., 1981] and BAR <ref> [Mostow, 1983b] </ref> were early attempts to build a system that could learn from advice. FOO and BAR take very general advice, such as "Avoid taking points" for the card game Hearts (the actual input is not in natural language).
Reference: [Musen, 1989] <author> M. A. Musen. </author> <title> Automated support for building and extending expert models. </title> <booktitle> Machine Learning, </booktitle> <address> 4(3-4):347-376, </address> <year> 1989. </year>
Reference: [Newell and Simon, 1972] <author> Allen Newell and Herbert A. Simon. </author> <title> Human Problem Solving. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1972. </year>
Reference-contexts: Whenever the agent does not know what to do next, it asks for more instruction. It never tries to determine a solution through reasoning techniques like means-ends analysis. Human students, on the other hand, make extensive use of means-ends analysis and related methods <ref> [Newell and Simon, 1972] </ref>. Adding these capabilities to Instructo-Soar would produce a more realistic instructional dialogue, decreasing the agent's need for instruction. * Single agent. There is only one agent in the domain.
Reference: [Newell et al., 1990] <author> Allen Newell, Gregg Yost, John E. Laird, Paul S. Rosenbloom, and Erik Alt-mann. </author> <title> Formulating the problem space computational model. </title> <booktitle> In Proceedings of the 25th Anniversary Symposium, </booktitle> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: General learning from specific cases yes (via situated explanation) 2. Fast learning yes (new procedures only taught once) 3. Maximal use of prior knowledge yes 4. Incremental learning yes 5. Learning all types of knowledge used in task performance yes (all knowledge types of the problem space computational model <ref> [Newell et al., 1990] </ref>) 6. Dealing with incorrect knowledge no (only extending incomplete knowledge) 7. <p> Chapter 2 A Theory of Learning from Tutorial Instruction This chapter presents a general theory of learning from tutorial instruction. The theory has two parts. First, a computational model of an intelligent agent, known as the problem space computational model (PSCM) <ref> [Newell et al., 1990; Yost, 1993] </ref> specifies the general form of the agent's processing, indicating each of the types of knowledge that the agent has and how they affect its performance. <p> The implemented instructable agent that embodies the theory is described in Chapters 3 and 4. 2.1 The problem space computational model (PSCM) A computational model is a "description of a class of systems in terms of a set of operations on entities that can be interpreted in computational terms" <ref> [Newell et al., 1990, p. 6] </ref>. Specifying the computational model of an agent defines the complete set of "operations on entities" that the agent carries out during its performance. <p> As the mapping problem is not the focus of this work, a content theory of knowledge representation is not included as a part of the theory developed here. The computational model adopted here is called the problem space computational model (PSCM) <ref> [Yost, 1993; Newell et al., 1990; Yost and Newell, 1989] </ref>. The PSCM allows us to describe an agent's structure in a particular computational framework computation within problem spaces without reference to the symbol level structures the agent uses to implement the computation. <p> The "operations and entities" within the PSCM are at a level above the symbol level, and approximating the knowledge level <ref> [Newell et al., 1990] </ref>, thus meeting the requirement of close correspondence to knowledge level components. <p> of the recursive nature of the PSCM, what can be thought of as the "goals" to be achieved in a particular subgoal are typically PSCM operators in higher subgoals that must be 1 This version of the PSCM is slightly different than that presented in [Yost and Newell, 1989] and <ref> [Newell et al., 1990] </ref>.
Reference: [Newell, 1973] <author> Allen Newell. </author> <title> You can't play 20 questions with nature and win: Projective comments on the papers of this symposium. </title> <editor> In W. G. Chase, editor, </editor> <booktitle> Visual Information Processing. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Thus, many models can account for the same data. A recurring theme of research on Soar, and prior research of Allen Newell's, has been the useful constraint that a unified theory of cognition (UTC) offers in designing cognitive models <ref> [Newell, 1990; Newell, 1973] </ref>. A UTC, as embodied in the form of an architecture, posits a set of mechanisms 76 capable of supporting intelligent behavior. By constructing a model within a UTC, we restrict ourselves to designs consistent with the architecture's properties.
Reference: [Newell, 1980] <author> Allen Newell. </author> <title> Reasoning, problem solving and decision processes: The problem space as a fundamental category. </title> <editor> In R. Nickerson, editor, </editor> <title> Attention and Performance VIII. </title> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, N.J., </address> <year> 1980. </year>
Reference: [Newell, 1981] <author> Allen Newell. </author> <title> The knowledge level. </title> <journal> AI Magazine, </journal> <volume> 2(2) </volume> <pages> 1-20, </pages> <year> 1981. </year>
Reference-contexts: If the agent does not know what an instructed action is, or how to perform it in the situation at hand, more instruction will be requested. P5. Knowledge-level interaction. The instructor provides knowledge to the agent at the knowledge level <ref> [Newell, 1981] </ref>, because the instructions refer to objects and actions in the world, not to symbol-level structures (e.g., data structures) within the agent. <p> Support of general computation. For a general agent, any type of computation might be required. The computational model must allow this flexibility. 2. Close correspondence to knowledge level components. The components of the computational model the "operations" and "entities" must correspond closely to the knowledge level <ref> [Newell, 1981] </ref>. This is required because the goal in identifying the computational model is to elucidate classes of knowledge that instructions map to and that are learned.
Reference: [Newell, 1990] <author> Allen Newell. </author> <title> Unified Theories of Cognition. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Thus, these models are not appropriate as the top-level computational model for an instructable agent. However, because higher levels of description of a computational system are implemented by lower levels <ref> [Newell, 1990] </ref>, these models of computation might be used as the implementation substrate for the higher level computational model of an instructable agent. Logic is another possible alternative. The entities within logics (e.g., propositions, well-formed formulas) are well matched to the knowledge level. <p> Soar has been applied to a large number of problems in both AI and cognitive science (see [Rosenbloom et al., 1993] for many examples), and has been proposed as the basis for a unified theory of cognition <ref> [Newell, 1990] </ref>. 34 35 Requirement From 1. General learning from specific cases Transfer 2. Fast learning (each task instructed only once) Transfer 3. Maximal use of prior knowledge Transfer 4. Incremental learning Transfer 5. Ability to learn all types of knowledge used in task performance Transfer 6. <p> Instructo-Soar's development has been strongly influenced by the PSCM and by Soar. Soar has been put forward as a candidate unified theory of cognition (UTC) <ref> [Newell, 1990] </ref>; thus, one key issue to examine is what kind of impact this unified theory has had on the model's development. <p> Thus, many models can account for the same data. A recurring theme of research on Soar, and prior research of Allen Newell's, has been the useful constraint that a unified theory of cognition (UTC) offers in designing cognitive models <ref> [Newell, 1990; Newell, 1973] </ref>. A UTC, as embodied in the form of an architecture, posits a set of mechanisms 76 capable of supporting intelligent behavior. By constructing a model within a UTC, we restrict ourselves to designs consistent with the architecture's properties. <p> However, as the timescale increases, to the level of minutes or hours for task performance, behavior is determined more and more by an intelligent agent's knowledge. This is Newell's intendedly rational band <ref> [Newell, 1990] </ref>, in which the agent moves towards approximating a pure knowledge-level system. Learning from instruction falls into this band, because performance is dominated by knowledge, and takes on the order of minutes to hours. <p> knowledge and little computational apparatus and indirection...One of the general arguments for the importance of getting the architecture right, even though it appears to recede as the time scale increases, is that doing what is natural within the architecture will tend to be the way humans actually do a task." <ref> [Newell, 1990, p. 416-17] </ref> A UTC's entailments derive from its basic properties. 5.3.1 Basic properties of Soar Following Newell [1990, p. 160], Soar can be described as having six basic properties: 1. Problem spaces represent all tasks.
Reference: [Ourston and Mooney, 1990] <author> Dirk Ourston and Raymond J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <year> 1990. </year>
Reference-contexts: There has been some work on recovering from incorrect knowledge in Soar [Laird, 1988; Laird et al., 1989], but the broad range of possible incorrectness in an instructable agent will require more research on basic recovery and theory revision techniques (as, e.g., <ref> [Ourston and Mooney, 1990; Ginsberg, 1988] </ref>). The combination of these changes will produce an agent that has a much more realistic pattern of interaction with its instructor.
Reference: [Palinscar, 1986] <author> A. Palinscar. </author> <title> The role of dialogue in providing scaffolded instruction. </title> <journal> Educational Psychologist, </journal> <volume> 21 </volume> <pages> 73-98, </pages> <year> 1986. </year>
Reference-contexts: These studies have shown that one-to-one tutoring is a highly effective teaching technique <ref> [Bloom, 1984; Palinscar, 1986] </ref>. It robustly produces stronger learning than classroom instruction by two standard deviations [Bloom, 1984]. A number of studies have shown that students' amount of prior knowledge has a large impact on learning from instruction.
Reference: [Pazzani, 1991a] <author> Michael Pazzani. </author> <title> A computational theory of learning causal relationships. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 401-424, </pages> <year> 1991. </year>
Reference-contexts: In addition, further instruction may be required to complete missing parts of an explanation, as described below. A number of researchers have discussed learning from explanations involving inductive steps. These inductive steps may be based on naive theories of explanation or causality (e.g., TDL in OCCAM <ref> [Pazzani, 1991a] </ref>, SWALE's explanation patterns [Schank and Leake, 1989]); or they may be based on unexplained features of the example (s) being explained (e.g., [VanLehn et al., 1990; Rosenbloom and Aasman, 1990; VanLehn, 1987; Hall, 1986]). <p> However, this effect (and effects of operators in general) is conditional on particular features of the state that hold before the operator is executed. To guess at these conditions, Instructo-Soar uses two simple heuristics that operate as a very naive causality theory <ref> [Pazzani, 1991a] </ref> for determining the causes of the inferred operator effect. The OP-to-G-path heuristic (described in Section 3.6) includes any features on paths between objects attached to the operator and to the goal.
Reference: [Pazzani, 1991b] <author> Michael Pazzani. </author> <title> Learning to predict and explain: An integration of similarity-based, theory driven, and explanation-based learning. </title> <journal> Journal of the Learning Sciences, </journal> <volume> 1(2) </volume> <pages> 153-199, </pages> <year> 1991. </year>
Reference: [Pearson et al., 1993] <author> Douglas J. Pearson, Scott B. Huffman, Mark B. Willis, John E. Laird, and Randolph M. Jones. </author> <title> A symbolic solution to intelligent real-time control. </title> <booktitle> IEEE Robotics and Autonomous Systems, </booktitle> <year> 1993. </year> <note> In press. </note>
Reference-contexts: A 3-D graphical simulator for the domain has been built that runs on Silicon Graphics machines. Instructo-Soar's techniques have also been applied in a more limited way to a flight domain <ref> [Pearson et al., 1993] </ref>, in which Soar controls a flight simulator and instructions are given for taking off The agent in Figure 3.1 is a Hero robot with one arm and gripper. <p> This contrasts with dynamic domains such as flying an airplane, where multiple goals at multiple levels of granularity may be active at once <ref> [Pearson et al., 1993] </ref>.
Reference: [Porter and Kibler, 1986] <author> B. W. Porter and D. F. Kibler. </author> <title> Experimental goal regression: A method for learning problem-solving heuristics. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 249-286, </pages> <year> 1986. </year>
Reference: [Porter et al., 1990] <author> B. W. Porter, R. Bareiss, and R. C. Holte. </author> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45(3) </volume> <pages> 229-263, </pages> <year> 1990. </year>
Reference-contexts: (LEAP [Mitchell et al., 1990]), goal decomposition rules (DISCIPLE [Kodratoff and Tecuci, 1987b]), operational versions of abstract (functional) goals (ARMS [Segre, 1987]), control knowledge and control features (ASK [Gruber, 1989]), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA [VanLehn, 1987]), and heuristic classification knowledge (PROTOS <ref> [Porter et al., 1990] </ref>, ODYSSEUS [Wilkins, 1990]). A complete tutorable agent should be able to learn any of the types of knowledge it uses for task performance via instruction. <p> However, these methods are appropriate as computational models for instructable systems targeted at a particular type of problem; PROTOS <ref> [Porter et al., 1990] </ref>, a learning apprentice for heuristic classification tasks, is an excellent example.
Reference: [Quinlan, 1986] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., <ref> [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982] </ref>), connectionist methods (e.g., [Rumelhart and McClelland, 1986]), and other reinforcement learning techniques (e.g., [Sutton, 1988; Holland, 1986]).
Reference: [Redmond, 1992] <author> Michael A. </author> <title> Redmond. Learning by observing and understanding expert problem solving. </title> <type> PhD thesis, </type> <institution> Georgia Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: An exception is learning apprentice systems that learn by observing expert behavior, where the goals that observed actions are meant to achieve may not be stated. Determining goals by observing actions is a process of plan recognition <ref> [Redmond, 1992; Kautz and Allen, 1986] </ref>. In the type of instruction considered here, however, plan recognition is not necessary, because the goal that an instruction is intended to achieve is always either the current goal or a stated, hypothetical goal. * Other required reasoning steps. <p> However, induction over multiple examples could also be employed within an explanation-based approach (e.g., [Miller, 1993]). 32 2.3.2 Case-based learning A second alternative would be to employ case-based reasoning methods (e.g., <ref> [Redmond, 1992; Alterman et al., 1991; Schank and Leake, 1989] </ref>) to learn from instruction. A case might consist of the sequence of instructions for a procedure. <p> Human tutors often refer to one hypothetical situation over the course of multiple instructions. In Instructo-Soar, the situation an instruction applies to must be determined when that instruction is received. In human instruction, the goal of an action may be unknown when the action is performed <ref> [Redmond, 1992] </ref>.
Reference: [Rosenbloom and Aasman, 1990] <author> Paul S. Rosenbloom and Jans Aasman. </author> <title> Knowledge level and inductive uses of chunking (EBL). </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year> <month> 140 </month>
Reference-contexts: Instructo-Soar performs the inductive learning of termination conditions by EBL (chunking) over an overgeneral theory one that is able to generate any of the features of the current state (similar to <ref> [Miller, 1993; VanLehn et al., 1990; Rosenbloom and Aasman, 1990] </ref>).
Reference: [Rosenbloom and Laird, 1986] <author> Paul S. Rosenbloom and John E. Laird. </author> <title> Mapping explanation-based generalization onto Soar. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 561-567, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: When similar situations arise in the future, chunks allow the impasse that caused the original subgoal to be avoided by producing their results directly. Chunking is a form of explanation-based learning <ref> [Rosenbloom and Laird, 1986] </ref>. Although it is a summarization mechanism, through taking both inductive and deductive steps in subgoals, chunking can lead to both inductive and deductive learning.
Reference: [Rosenbloom and Newell, 1986] <author> Paul S. Rosenbloom and Allen Newell. </author> <title> The chunking of goal hierarchies: A generalized model of practice. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: This pattern continues back through the entire sequence until the full implementation has been learned generally. As Figure 3.9 shows for learning "pick up", the resulting learning curve closely approximates the power law of practice <ref> [Rosenbloom and Newell, 1986] </ref> (r = 0:98). Figure 3.10 shows a similar curve for learning to move objects left of one another (r = 0:98). * Effectiveness of hierarchical instruction. <p> First, over multiple executions, the execution time of a procedure being learned follows the power law of practice <ref> [Rosenbloom and Newell, 1986] </ref>. This is a within-task positive transfer. Second, once learned generally, an inefficient procedure will exhibit the Einstellung effect; it will be difficult to learn a more efficient procedure. This is negative transfer.
Reference: [Rosenbloom et al., 1988] <author> Paul S. Rosenbloom, John E. Laird, and Allen Newell. </author> <title> The chunking of skill and knowledge. </title> <editor> In H. Bouma and A. G. Elsendoorn, editors, </editor> <booktitle> Working Models of Human Perception, </booktitle> <pages> pages 391-410. </pages> <publisher> Academic Press, </publisher> <address> London, England, </address> <year> 1988. </year>
Reference: [Rosenbloom et al., 1993] <author> P. S. Rosenbloom, J. E. Laird, and A. </author> <title> Newell, </title> <editor> editors. </editor> <booktitle> The Soar Papers: Research on integrated intelligence. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1993. </year>
Reference-contexts: Soar has been applied to a large number of problems in both AI and cognitive science (see <ref> [Rosenbloom et al., 1993] </ref> for many examples), and has been proposed as the basis for a unified theory of cognition [Newell, 1990]. 34 35 Requirement From 1. General learning from specific cases Transfer 2. Fast learning (each task instructed only once) Transfer 3. Maximal use of prior knowledge Transfer 4.
Reference: [Rumelhart and McClelland, 1986] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982]), connectionist methods (e.g., <ref> [Rumelhart and McClelland, 1986] </ref>), and other reinforcement learning techniques (e.g., [Sutton, 1988; Holland, 1986]).
Reference: [Rychener, 1983] <author> Michael D. Rychener. </author> <title> The instructible production system: A retrospective analysis. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, </booktitle> <pages> pages 429-460. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: However, Homer does not learn procedures or other kinds of domain knowledge from sequences of instruction, or make use of prior knowledge to produce general learning. Learning from tutorial instruction is related to work on learning from external guidance and advice taking <ref> [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983] </ref>. Some early systems learned declarative, ontological knowledge from instructions [Lindsay, 1963; Haas and Hendrix, 1983]. UNDERSTAND [Simon, 1977; Simon and Hayes, 1976] was among the first to do procedural learning from instruction.
Reference: [Salganicoff, 1993] <author> Marcos Salganicoff. </author> <title> Density-adaptive learning and forgetting. </title> <editor> In P. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 276-283, </pages> <address> Amheart, Mass., </address> <year> 1993. </year>
Reference: [Sandberg and Wielinga, 1991] <author> Jacobijn Sandberg and Bob Wielinga. </author> <booktitle> How situated is cognition? In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 341-346, </pages> <year> 1991. </year>
Reference-contexts: For example, in physics class students are taught that F = m a; this general equation applies in specific ways to a great variety of situations. The advantage of unsituated instruction is precisely this ability to compactly communicate abstract knowledge that is broadly applicable <ref> [Sandberg and Wielinga, 1991] </ref>. Singley and Anderson [1989] point out that to use such abstract knowledge, students must learn how it applies to specific situations, thus transforming it from unsituated to situated knowledge.
Reference: [Schank and Leake, 1989] <author> Roger C. Schank and David B. Leake. </author> <title> Creativity and learning in a case-based explainer. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 353-385, </pages> <year> 1989. </year>
Reference-contexts: Applying explanation to single steps results in knowl 24 edge applicable at each step (e.g.,[Laird et al., 1989; Golding et al., 1987]); explaining full sequences of reasoning steps results in learning schemas that encode the whole reasoning episode (e.g., <ref> [Mooney, 1990; Schank and Leake, 1989; VanLehn, 1987] </ref>). Learning factored pieces of knowledge rather than monolithic schemas or plan structures allows the knowledge to be applied in a more reactive fashion, since knowledge is accessed locally based on the current situation [Laird and Rosenbloom, 1990; Drummond, 1989]. <p> A number of researchers have discussed learning from explanations involving inductive steps. These inductive steps may be based on naive theories of explanation or causality (e.g., TDL in OCCAM [Pazzani, 1991a], SWALE's explanation patterns <ref> [Schank and Leake, 1989] </ref>); or they may be based on unexplained features of the example (s) being explained (e.g., [VanLehn et al., 1990; Rosenbloom and Aasman, 1990; VanLehn, 1987; Hall, 1986]). <p> However, induction over multiple examples could also be employed within an explanation-based approach (e.g., [Miller, 1993]). 32 2.3.2 Case-based learning A second alternative would be to employ case-based reasoning methods (e.g., <ref> [Redmond, 1992; Alterman et al., 1991; Schank and Leake, 1989] </ref>) to learn from instruction. A case might consist of the sequence of instructions for a procedure.
Reference: [Schank, 1975] <author> Roger C. Schank. </author> <title> Conceptual Information Processing. </title> <publisher> American Elsevier, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Unfortunately, a complete characterization of actions that may need to be mapped from instruction is hard to find. There have been some limited attempts to characterize the space of possible actions in AI (e.g., <ref> [Schank, 1975; Wilks, 1975] </ref>) and linguistics (e.g, [Talmy, 1985; Miller and Johnson-Laird, 1976]). Talmy [1985], for instance, analyzes the semantics of utterances specifying motion, laying out a dizzying thirty-seven motion-related semantic categories. Some of the most relevant are shown in Table 1.1. <p> Such theories define the functions and structures that are to be used in representing knowledge of various kinds (e.g., KL-ONE [Brachman, 1980]); in addition, some define the possible content of those structures (e.g., conceptual dependency theory <ref> [Schank, 1975] </ref>, CYC [Guha and Lenat, 1990]). However, since these theories do not specify computational operations, they do not indicate how procedural knowledge is used within an agent. Computational structure must be added to these theories to produce working agents.
Reference: [Segre, 1987] <author> Alberto Maria Segre. </author> <title> A learning apprentice system for mechanical assembly. </title> <booktitle> In Third IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> pages 112-117, </pages> <year> 1987. </year>
Reference-contexts: However, specific LAS's have focussed on learning particular types of knowledge: e.g., operator implementations (LEAP [Mitchell et al., 1990]), goal decomposition rules (DISCIPLE [Kodratoff and Tecuci, 1987b]), operational versions of abstract (functional) goals (ARMS <ref> [Segre, 1987] </ref>), control knowledge and control features (ASK [Gruber, 1989]), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA [VanLehn, 1987]), and heuristic classification knowledge (PROTOS [Porter et al., 1990], ODYSSEUS [Wilkins, 1990]). <p> have largely avoided the problem of learning from variable instruction sequences, either because they do not learn procedures from sequences of instruction (e.g., SHRDLU [Winograd, 1972], HOMER [Vere and Bickmore, 1990]), or because they require that instructions be given in the canonical execution order (e.g., using a robotic teach pendant <ref> [Segre, 1987] </ref>). B.1 Flexibility in procedural instruction Consider being taught to change a tire. Say the desired sequence of physical operations is &lt;open-trunk, retrieve-jack, place-jack-under-frame,...&gt;. This task might be taught in a number of ways.
Reference: [Shavlik et al., 1991] <author> Jude W. Shavlik, Raymond J. Mooney, and Geoffrey G. Towell. </author> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6(2) </volume> <pages> 111-144, </pages> <year> 1991. </year>
Reference: [Shen and Simon, 1989] <author> Wei-Min Shen and Herbert A. Simon. </author> <title> Rule creation and rule learning through environmental exploration. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 675-680, </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference: [Simon and Hayes, 1976] <author> Herbert A. Simon and John R. Hayes. </author> <title> Understanding complex task instructions. </title> <editor> In David Klahr, editor, </editor> <title> Cognition and Instruction. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1976. </year>
Reference-contexts: Learning from tutorial instruction is related to work on learning from external guidance and advice taking [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983]. Some early systems learned declarative, ontological knowledge from instructions [Lindsay, 1963; Haas and Hendrix, 1983]. UNDERSTAND <ref> [Simon, 1977; Simon and Hayes, 1976] </ref> was among the first to do procedural learning from instruction.
Reference: [Simon, 1977] <author> Herbert A. Simon. </author> <title> Artificial intelligence systems that understand. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1059-1073, </pages> <address> Cambridge, Mass., </address> <year> 1977. </year>
Reference-contexts: Learning from tutorial instruction is related to work on learning from external guidance and advice taking [McCarthy, 1968; Rychener, 1983; Carbonell et al., 1983]. Some early systems learned declarative, ontological knowledge from instructions [Lindsay, 1963; Haas and Hendrix, 1983]. UNDERSTAND <ref> [Simon, 1977; Simon and Hayes, 1976] </ref> was among the first to do procedural learning from instruction.
Reference: [Singley and Anderson, 1989] <author> Mark K. Singley and John R. Anderson. </author> <title> The transfer of cognitive skill. </title> <publisher> Harvard University Press, </publisher> <year> 1989. </year> <month> 141 </month>
Reference-contexts: Most instructional studies have either examined the effects of various linguistic forms of individual instructions (e.g., [Jones, 1966; File and Jew, 1973; Just and Carpenter, 1976; Wright and Wilcox, 1979; Dixon, 1982]), or the learning effectiveness, measured in terms of transfer, of different global instructional and training strategies (see <ref> [Singley and Anderson, 1989] </ref> for a review). These studies have shown that one-to-one tutoring is a highly effective teaching technique [Bloom, 1984; Palinscar, 1986]. It robustly produces stronger learning than classroom instruction by two standard deviations [Bloom, 1984]. <p> Finally, this work can be compared with other cognitive models of procedural learning. Most recent models are based on Anderson's model of procedural learning in ACT* <ref> [Anderson, 1983; Anderson, 1987; Singley and Anderson, 1989] </ref>, which in turn is based on Thorndike's [1903] theory of identical elements. Thorndike proposed that all transfer occurred because of the use of identical elements specific stimulus-response rules across multiple tasks. <p> Knowledge is only useful to the extent that it can be applied correctly in particular situations <ref> [Singley and Anderson, 1989; Chi et al., 1989] </ref>; and it appears that people learn to apply knowledge within situations by performing tasks in those situations. This idea underlies the increase in training through computer simulated practice (e.g., pilot training) [Lintern, 1991]. <p> This is negative transfer. Instructo-Soar, and Soar in general, is one of a class of theories that explain transfer by a modernized version of Thorndike's [1903] transfer of identical elements theory <ref> [Singley and Anderson, 1989; Gray and Orasanu, 1987] </ref>. The shortcoming of Thorndike's original formulation is that the identical elements were highly specific stimulus-response rules. In the modernized versions, these overly specific rules have been replaced by more general associative constructs, such as productions. <p> Since productions can be very general, transfer can occur across disparate tasks. However, the majority of productions for any particular task will be quite specific. Likewise, most results on transfer indicate that it is very narrow <ref> [Singley and Anderson, 1989; Bovair and Kieras, 1991] </ref>. For instance, Foltz et al. [1988] found that simply changing the name of a procedure from delete to erase caused a failure to transfer. <p> Most recent models fall into two major classes: models based on ACT*, and case-based models. 89 5.7.1 ACT*-based theories Many recent models of procedural learning are based on Anderson's ACT* model <ref> [Anderson, 1983; Anderson, 1987; Singley and Anderson, 1989] </ref>. Anderson's model employs a three stage process. First, instructions are converted into a declarative form in long-term memory. Second, the declarative memories guide task execution via weak methods, and through this process, new task-specific rules for the procedure are learned. <p> However, non-interactive instruction often includes unsituated information as well; e.g., general expository text. Unsituated instruction conveys general or abstract knowledge that can be applied in a large number of different situations. Such general purpose knowledge is often described as "declarative" knowledge <ref> [Singley and Anderson, 1989] </ref>. For example, in physics class students are taught that F = m a; this general equation applies in specific ways to a great variety of situations.
Reference: [Smith and Goodman, 1984] <author> E. E. Smith and L. Goodman. </author> <title> Understanding written instructions: The role of an explanatory schema. </title> <journal> Cognition and Instruction, </journal> <volume> 1 </volume> <pages> 359-396, </pages> <year> 1984. </year>
Reference-contexts: Instructo-Soar predicts an advantage for hierarchical instruction because it breaks a procedure into smaller intermediate goals that are easier to explain. Hierarchical instruction that provides information about intermediate goals has been shown to be more effective than purely linear step-by-step instruction <ref> [Smith and Goodman, 1984; Konoske and Ellis, 1986] </ref>. Goal information also helps students interpret commands correctly [Dixon, 1987]; this is a part of the mapping problem that is not modeled by Instructo-Soar.
Reference: [Sutton and Pinette, 1985] <author> R. S. Sutton and B. Pinette. </author> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 54-64, </pages> <year> 1985. </year>
Reference: [Sutton, 1988] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Given enough examples, the correlations indicate which features are important to the concept being learned. Correlational techniques include symbolic concept learning and clustering methods (e.g., [Fisher, 1987; Quinlan, 1986; Michalski and Stepp, 1983; Mitchell, 1982]), connectionist methods (e.g., [Rumelhart and McClelland, 1986]), and other reinforcement learning techniques (e.g., <ref> [Sutton, 1988; Holland, 1986] </ref>). Since instructions may be explicitly situated, determining the situation for an instruction would have to be included as a precursor to correlational learning, so that the features of the situation could serve as inputs to the learning algorithm.
Reference: [Talmy, 1985] <author> Leonard Talmy. </author> <title> Lexicalization patterns: semantic structure in lexical forms. </title> <editor> In Timothy Shopen, editor, </editor> <title> Language typology and syntactic description. </title> <publisher> Cambridge Univ. Press, </publisher> <year> 1985. </year>
Reference-contexts: E.g. "I", "you", "they." 15. relation to comparable events (whether action occurs alone, in addition to, or in place of comparable events - "He only danced", "He also danced", etc.). 16. temporal setting 17. spatial setting 18. direction (of motion) Table 1.1: Some of Talmy's semantic categories for motion constructions <ref> [Talmy, 1985] </ref> * Turn back to the airport. 2. <p> Unfortunately, a complete characterization of actions that may need to be mapped from instruction is hard to find. There have been some limited attempts to characterize the space of possible actions in AI (e.g., [Schank, 1975; Wilks, 1975]) and linguistics (e.g, <ref> [Talmy, 1985; Miller and Johnson-Laird, 1976] </ref>). Talmy [1985], for instance, analyzes the semantics of utterances specifying motion, laying out a dizzying thirty-seven motion-related semantic categories. Some of the most relevant are shown in Table 1.1.
Reference: [Thorndike, 1903] <author> E. L. Thorndike. </author> <title> Educational Psychology. Lemke and Buechner, </title> <address> New York, </address> <year> 1903. </year>
Reference: [Thrun and Mitchell, 1993] <author> S. B. Thrun and T. M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 930-936, </pages> <year> 1993. </year>
Reference: [Towell et al., 1990] <author> G. G. Towell, J. W. Shavlik, and M. O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference: [VanLehn and Jones, 1991] <author> K. VanLehn and R. Jones. </author> <title> Learning physics via explanation-based learning of correctness and analogical search control. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <year> 1991. </year>
Reference-contexts: Explaining individual instructions is also supported by psychological results on the self-explanation effect, which have shown that subjects who self-explain instructional examples do so by re-deriving individual lines of the example. "Students virtually never reflect on the overall solution and try to recognize a plan that spans all the lines" <ref> [VanLehn and Jones, 1991, p. 111] </ref>. * Endpoints of explanation. The endpoints of the explanation the initial state of reasoning and the goal to be achieved together comprise the situation that the instruction applies to.
Reference: [VanLehn et al., 1990] <author> K. VanLehn, W. Ball, and B. Kowalski. </author> <title> Explanation-based learning of correctness: Towards a model of the self-explanation effect. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 717-724, </pages> <year> 1990. </year>
Reference-contexts: Instructo-Soar performs the inductive learning of termination conditions by EBL (chunking) over an overgeneral theory one that is able to generate any of the features of the current state (similar to <ref> [Miller, 1993; VanLehn et al., 1990; Rosenbloom and Aasman, 1990] </ref>).
Reference: [VanLehn et al., 1992] <author> Kurt VanLehn, Randolph M. Jones, and Michelene T. H. Chi. </author> <title> A model of the self-explanation effect. </title> <journal> Journal of the learning sciences, </journal> <volume> 2(1) </volume> <pages> 1-59, </pages> <year> 1992. </year>
Reference-contexts: These self-explanations involved a deliberate, conscious effort to justify the steps in the example, by explicating consequences and preconditions of actions [Chi and VanLehn, 1991]. Students formed explanations of individual steps in each example, rather than explaining multiple steps <ref> [VanLehn et al., 1992] </ref>. Poor students did not self-explain but rather processed the examples in a verbatim fashion. 88 During problem solving, good solvers (those who self-explained the examples) referred to examples sparingly, and in a targeted fashion, looking for specific information (e.g., a specific equation).
Reference: [VanLehn, 1987] <author> Kurt VanLehn. </author> <title> Learning one subprocedure per lesson. </title> <journal> Artificial Intelligence, </journal> <volume> 31(1) </volume> <pages> 1-40, </pages> <year> 1987. </year>
Reference-contexts: This constraint, called the one-goal-at-a-time constraint, is assumed as a felicity condition of tutorial instruction; that is, a communication convention that is naturally followed <ref> [VanLehn, 1987] </ref>. The interaction pattern that results, in which procedures are commanded and then taught as needed, has been observed in human tutorial instruction. <p> particular types of knowledge: e.g., operator implementations (LEAP [Mitchell et al., 1990]), goal decomposition rules (DISCIPLE [Kodratoff and Tecuci, 1987b]), operational versions of abstract (functional) goals (ARMS [Segre, 1987]), control knowledge and control features (ASK [Gruber, 1989]), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA <ref> [VanLehn, 1987] </ref>), and heuristic classification knowledge (PROTOS [Porter et al., 1990], ODYSSEUS [Wilkins, 1990]). A complete tutorable agent should be able to learn any of the types of knowledge it uses for task performance via instruction. <p> Applying explanation to single steps results in knowl 24 edge applicable at each step (e.g.,[Laird et al., 1989; Golding et al., 1987]); explaining full sequences of reasoning steps results in learning schemas that encode the whole reasoning episode (e.g., <ref> [Mooney, 1990; Schank and Leake, 1989; VanLehn, 1987] </ref>). Learning factored pieces of knowledge rather than monolithic schemas or plan structures allows the knowledge to be applied in a more reactive fashion, since knowledge is accessed locally based on the current situation [Laird and Rosenbloom, 1990; Drummond, 1989]. <p> Examples include classroom lectures, instruction manuals and textbooks. One issue in using this type of instruction is locating and extracting the information that is needed for a particular problem at hand. Some information conveyed non-interactively may be situated; e.g., in worked-out example problems <ref> [Chi et al., 1989; VanLehn, 1987] </ref>. However, non-interactive instruction often includes unsituated information as well; e.g., general expository text. Unsituated instruction conveys general or abstract knowledge that can be applied in a large number of different situations.
Reference: [VanLehn, 1990] <author> K. VanLehn. </author> <title> Mind bugs: The origins of procedural misconceptions. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference: [Vera et al., 1993] <author> Alonzo H. Vera, Richard L. Lewis, and F. Javier Lerch. </author> <title> Situated decision-making and recognition-based learning: Applying symbolic theories to interactive tasks. </title> <booktitle> In Proceedings of the 15th Annual Conference of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference: [Vere and Bickmore, 1990] <author> Steven Vere and Timothy Bickmore. </author> <title> A basic agent. </title> <journal> Computational Intelligence, </journal> <volume> 6 </volume> <pages> 41-60, </pages> <year> 1990. </year>
Reference-contexts: The results indicate the kind of flexibility required of an agent purporting to learn from tutorial instruction. Previous instructable agents have largely avoided the problem of learning from variable instruction sequences, either because they do not learn procedures from sequences of instruction (e.g., SHRDLU [Winograd, 1972], HOMER <ref> [Vere and Bickmore, 1990] </ref>), or because they require that instructions be given in the canonical execution order (e.g., using a robotic teach pendant [Segre, 1987]). B.1 Flexibility in procedural instruction Consider being taught to change a tire. Say the desired sequence of physical operations is &lt;open-trunk, retrieve-jack, place-jack-under-frame,...&gt;.
Reference: [Wertsch, 1979] <author> James V. Wertsch. </author> <title> From social interaction to higher psychological processes: A clarification and application of Vygotsky's theory. </title> <booktitle> Human Development, </booktitle> <volume> 22 </volume> <pages> 1-22, </pages> <year> 1979. </year>
Reference: [Wilkins, 1990] <author> David C. Wilkins. </author> <title> Knowledge base refinement as improving an incomplete and incorrect domain theory. </title> <editor> In Y. Kodratoff and R. S. Michalski, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume III, </booktitle> <pages> pages 493-514. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: goal decomposition rules (DISCIPLE [Kodratoff and Tecuci, 1987b]), operational versions of abstract (functional) goals (ARMS [Segre, 1987]), control knowledge and control features (ASK [Gruber, 1989]), procedure schemas (a combination of goal decomposition and control knowledge; learned by SIERRA [VanLehn, 1987]), and heuristic classification knowledge (PROTOS [Porter et al., 1990], ODYSSEUS <ref> [Wilkins, 1990] </ref>). A complete tutorable agent should be able to learn any of the types of knowledge it uses for task performance via instruction. <p> In situated explanation, since the explanation process involves applying (projecting) the new knowledge in the appropriate situation, the agent maintains a kind of "performance consistency," rather than standard notions of semantic consistency. However, in some domains more explicit 31 measures of consistency (such as that used by ODYSSEUS <ref> [Wilkins, 1990] </ref>) may be required.
Reference: [Wilks, 1975] <author> Y.A. Wilks. </author> <title> A preferential, pattern-seeking, semantics for natural language inference. </title> <journal> Artificial Intelligence, </journal> <volume> 6, </volume> <year> 1975. </year> <month> 142 </month>
Reference-contexts: Unfortunately, a complete characterization of actions that may need to be mapped from instruction is hard to find. There have been some limited attempts to characterize the space of possible actions in AI (e.g., <ref> [Schank, 1975; Wilks, 1975] </ref>) and linguistics (e.g, [Talmy, 1985; Miller and Johnson-Laird, 1976]). Talmy [1985], for instance, analyzes the semantics of utterances specifying motion, laying out a dizzying thirty-seven motion-related semantic categories. Some of the most relevant are shown in Table 1.1.
Reference: [Winograd, 1972] <author> Terry Winograd. </author> <title> Understanding Natural Language. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: One of the earliest and most famous is SHRDLU <ref> [Winograd, 1972] </ref>. SHRDLU plans and acts in a blocks-world domain in response to natural language commands. The focus is primarily on planning and on the mapping problem; SHRDLU transforms natural language input into code fragments that are run to produce the instructed action. <p> The results indicate the kind of flexibility required of an agent purporting to learn from tutorial instruction. Previous instructable agents have largely avoided the problem of learning from variable instruction sequences, either because they do not learn procedures from sequences of instruction (e.g., SHRDLU <ref> [Winograd, 1972] </ref>, HOMER [Vere and Bickmore, 1990]), or because they require that instructions be given in the canonical execution order (e.g., using a robotic teach pendant [Segre, 1987]). B.1 Flexibility in procedural instruction Consider being taught to change a tire.
Reference: [Wood et al., 1976] <author> David Wood, Jerome S. Bruner, and Gail Ross. </author> <title> The role of tutoring in problem solving. </title> <journal> Journal of Child Psychology and Psychiatry, </journal> <volume> 17 </volume> <pages> 89-100, </pages> <year> 1976. </year>
Reference-contexts: Newly learned operators may be specified in instructions for later operators, leading to learning of operator hierarchies. For example, a hierarchy of operators learned by Instructo-Soar is shown in has been identified as a fundamental component of children's skill acquisition from tutorial instruction <ref> [Wood et al., 1976] </ref>.
Reference: [Wright and Wilcox, 1979] <author> Patricia Wright and Penelope Wilcox. </author> <title> When two no's nearly make a yes: A study of conditional imperatives. </title> <editor> In P. A. Kolers, M. E. Wrolstad, and H. Bouma, editors, </editor> <booktitle> Processing of visible language. </booktitle> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [Yost and Newell, 1989] <author> Gregg R. Yost and Allen Newell. </author> <title> A problem space approach to expert system specification. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 621-7, </pages> <year> 1989. </year>
Reference-contexts: In contrast, problem solving methods like constraint satisfaction and heuristic classification involve global control. It is possible to produce a global control strategy using a combination of local decisions <ref> [Yost and Newell, 1989] </ref>. However, teaching a global method by casting it purely as a sequence of local decisions may be difficult. <p> As the mapping problem is not the focus of this work, a content theory of knowledge representation is not included as a part of the theory developed here. The computational model adopted here is called the problem space computational model (PSCM) <ref> [Yost, 1993; Newell et al., 1990; Yost and Newell, 1989] </ref>. The PSCM allows us to describe an agent's structure in a particular computational framework computation within problem spaces without reference to the symbol level structures the agent uses to implement the computation. <p> in Table 2.1. 2 Because of the recursive nature of the PSCM, what can be thought of as the "goals" to be achieved in a particular subgoal are typically PSCM operators in higher subgoals that must be 1 This version of the PSCM is slightly different than that presented in <ref> [Yost and Newell, 1989] </ref> and [Newell et al., 1990]. <p> This type of control is well matched to the kinds of tasks most readily taught by tutorial instruction, as discussed in Chapter 1. However, local control decisions in the PSCM can be combined to exhibit global control behavior, as demonstrated by Yost's work on TAQL <ref> [Yost, 1993; Yost and Newell, 1989] </ref>, a PSCM-based knowledge acquisition language that acquired tasks requiring global control. Finally, the PSCM supports incremental, monotonic extensions of an agent's knowledge.
Reference: [Yost, 1993] <author> Gregg R. Yost. </author> <title> Acquiring knowledge in Soar. </title> <journal> IEEE Expert, </journal> <volume> 8(3) </volume> <pages> 26-34, </pages> <year> 1993. </year>
Reference-contexts: Chapter 2 A Theory of Learning from Tutorial Instruction This chapter presents a general theory of learning from tutorial instruction. The theory has two parts. First, a computational model of an intelligent agent, known as the problem space computational model (PSCM) <ref> [Newell et al., 1990; Yost, 1993] </ref> specifies the general form of the agent's processing, indicating each of the types of knowledge that the agent has and how they affect its performance. <p> As the mapping problem is not the focus of this work, a content theory of knowledge representation is not included as a part of the theory developed here. The computational model adopted here is called the problem space computational model (PSCM) <ref> [Yost, 1993; Newell et al., 1990; Yost and Newell, 1989] </ref>. The PSCM allows us to describe an agent's structure in a particular computational framework computation within problem spaces without reference to the symbol level structures the agent uses to implement the computation. <p> This type of control is well matched to the kinds of tasks most readily taught by tutorial instruction, as discussed in Chapter 1. However, local control decisions in the PSCM can be combined to exhibit global control behavior, as demonstrated by Yost's work on TAQL <ref> [Yost, 1993; Yost and Newell, 1989] </ref>, a PSCM-based knowledge acquisition language that acquired tasks requiring global control. Finally, the PSCM supports incremental, monotonic extensions of an agent's knowledge.
References-found: 174

