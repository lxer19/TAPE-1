URL: http://www.iscs.nus.sg/~plong/papers/cgc.ps
Refering-URL: 
Root-URL: 
Email: fred@mira.ucsc.edu and plong@saturn.ucsc.edu.  
Title: Composite Geometric Concepts and Polynomial Predictability  
Author: Philip M. Long Manfred K. Warmuth 
Note: This work supported by ONR grants N00014-86-K-0454 and N00014-91-J-1162. The authors' email addresses are man  
Address: Santa Cruz  
Affiliation: University of California at  
Abstract-found: 0
Intro-found: 1
Reference: [Bau89] <author> BAUM, E.B. </author> <year> (1990), </year> <title> On learning a union of halfspaces. </title> <journal> Journal of Complexity, </journal> <volume> 6 </volume> <pages> 67-101. </pages>
Reference-contexts: However any one-way function that is hard on its iterates [GKL88] [Lev87] leads to a problem in B P that is not predictable [PW90]. Thus modulo the minimal cryptographic assumption that such functions exist, any prediction complete problem for B P (including vertex-represented convex polytopes) is not predictable. Baum <ref> [Bau89] </ref> gives an algorithm for predicting the class of unions of halfspaces which requires resources polynomial in the number of halfspaces and the inverse of the accuracy, but exponential in the domain 1 In the original model proposed by Valiant, the algorithm is required to output a hypothesis from the target
Reference: [Bau90] <author> BAUM, E.B. </author> <year> (1990), </year> <title> Polynomial time algorithms for learning neural nets. </title> <booktitle> Proceedings of the Third Workshop on Computational Learning Theory. </booktitle>
Reference-contexts: Elements of R are representations of concepts, and c maps representations to the concepts they represent, so c (R) is the associated concept class. Throughout the paper, we assume that integers are encoded in binary, so that encoding the integer n 3 Recently, Baum <ref> [Bau90] </ref> gave an elegant learning algorithm for a union of two homogeneous halfspaces that requires resources which grow only polynomially in domain dimension. Unfortunately, his method does not appear to generalize to unions of nonhomogeneous halfspaces or to unions of more than two homogeneous halfspaces.
Reference: [Blu89] <author> BLUM, A. </author> <year> (1989), </year> <title> On the Computational Complexity of Training Simple Neural Networks. </title> <type> Technical Report MIT/LCS/TR-445 (Master's Thesis). </type> <institution> MIT. </institution>
Reference-contexts: Flats are translations of subspaces of Euclidian space. Our proof of the predictability of fixed finite unions of flats consists of reducing this prediction problem to that of predicting flats. The class of flats was shown to be predictable in [Shv88]. In [VW89] and independently in <ref> [Blu89] </ref>, a similar technique was applied to show that the class of "border augmented symmetric differences of halfspaces" 4 is predictable. Our result for predicting a fixed number of flats holds even if the dimension varies with the target concept.
Reference: [BEHW87] <author> BLUMER, A., EHRENFEUCHT, A., HAUSSLER, D., and WARMUTH, M.K. </author> <year> (1987), </year> <title> Oc-cam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380. </pages>
Reference-contexts: Our result for predicting a fixed number of flats holds even if the dimension varies with the target concept. Finally, in Section 4, we give an Occam algorithm <ref> [BEHW87] </ref> for predicting unions of a fixed number of "boxes" (Cartesian products of intervals). Again the dimension is allowed to vary.
Reference: [BEHW89] <author> BLUMER, A., EHRENFEUCHT, A., HAUSSLER, D., and WARMUTH, M.K. </author> <year> (1989), </year> <title> Learn-ability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4). </volume>
Reference-contexts: Since any set of points on a sphere can be shattered by the class of convex polytopes, this class has infinite Vapnik-Chervonenkis (VC) dimension 2 [VC71], and therefore, if we do not allow the algorithm's resources to grow with the complexity of the target concept, this class is not predictable <ref> [BEHW89] </ref>. To address the question of the predictability of this class when resources are allowed to grow with the length of the representation of the hidden concept, we must choose a representation language for the class of convex polytopes. <p> Provided that the example sequence is large enough, the hypothesis produced is an accurate predictor. The class of single boxes was shown to be predictable in <ref> [BEHW89] </ref> using single boxes as hypotheses. Note that the class of intersections of halfspaces is a generalization of CNF (boolean formulae in conjunctive normal form). Also, the class of unions of axis-parallel rectangles and the class of unions of flats are generalizations of DNF (boolean formulae in disjunctive normal form). <p> For our second positive result, we give an prediction algorithm for U k (B 2 ) for each k 2 N. For k = 1, this problem has been solved in <ref> [BEHW89] </ref>. Our algorithm consists of finding a concept h of U k (2d) k (B 2 ) consistent with the sample, and using h for prediction. We make use of the following lemma. Lemma 10 ([BEHW89]) Let B be any prediction problem whose associated concept class has finite VC dimension d <p> This completes the induction. 2 The above algorithm clearly requires time polynomial in m, but exponential in k. Since the output hypothesis is in the concept class of U k (2d) k (B 2 ) boxes, and the VC-dimension of B 2 is 2d <ref> [BEHW89] </ref>, by Lemma 10, the VC dimension of the hypothesis class of this algorithm is no more than 2 k+2 kd k+1 log 2 k 3kd k , which is polynomial in d for fixed k, which, by the results of [BEHW89][HKLW91], implies that only poly nomially many examples are required <p> Since the intersection of any box with the vertices of the unit cube consists of the vertices of some subcube, our algorithm for learning a fixed number of boxes leads to an Occam algorithm <ref> [BEHW89] </ref> for learning k-term DNF using hypotheses in DNF. The number of terms in the DNF expression returned by our algorithm is bounded by k (2n) k , where n is the number of variables. <p> Thus we can use a standard greedy covering algorithm <ref> [BEHW89] </ref>, to obtain an intersection of at most (2d) k ln m + 1 generalized clauses consistent with any sample of size m. <p> The greedy algorithm iteratively finds a generalized clause consistent with all the positive examples and at least a fraction (2d) k of the as yet "uncovered" negative examples. By the results of <ref> [BEHW89] </ref>, this implies the predictability of U k (B 2 ) if the greedy algorithm requires polynomial time.
Reference: [BS90] <author> BLUM, A. and SINGH, M. </author> <title> Learning functions of k terms. </title> <booktitle> Proceedings of the Third Workshop on Computational Learning Theory. </booktitle>
Reference-contexts: The hypotheses produced are k (2n) k -term DNF, where n is the number of variables. The "standard" algorithm for k-term DNF uses k-CNF as hypotheses [PV88]. Another algorithm for learning k-term DNF in terms of DNF was discovered in parallel <ref> [BS90] </ref>. In Section 5, we summarize the paper and give a number of open problems. 2 Preliminary Definitions We begin by formalizing the definition of predictability discussed in the introduction [HLW88] [PW90]. Let and be finite alphabets. If is a string, let jj denote the length of .
Reference: [Ede84] <author> EDELSBRUNNER, E. </author> <year> (1984), </year> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer-Verlag. </publisher> <address> New York. </address>
Reference-contexts: Our approach is motivated by the concept of a dual relationship between points and hyperplanes. Edelsbrunner <ref> [Ede84] </ref> describes the following mapping D from nonzero points to hyperplanes and nonhomogeneous hyperplanes to points 6 * If p is a nonzero point in R d , then D (p) = fx 2 R d : p x = 1g. * If H is a nonhomogeneous hyperplane in R d <p> are contained in the unit cube, we may substitute hyperplanes for subcubes in a reduction to B HY P=P OLY and obtain hardness results for a similar class in which hyperplanes replace subcubes as 6 A geometric interpretation of the mapping D which might give some intuition is described in <ref> [Ede84] </ref>. 6 -x 1 6 x 2 1 6 H (1) @ @ @I @ @ H (3) B BN B B B B B - ? x 2 1 s D (H (2) ) s D (H (3) ) fD (H (1) ); D (H (2) ); D (H (3) <p> Similar facts are proved in <ref> [Ede84] </ref>. Lemma 3 Let A = fa (i) : 1 i ng Q d . Let b 2 Q d .
Reference: [GKL88] <author> GOLDREICH, O., KRWACZYK, H., and LUBY, M. </author> <year> (1988), </year> <title> On the existence of pseudorandom generators. </title> <booktitle> Proceedings of the 29th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 12-24. </pages>
Reference-contexts: Thus if vertex-represented convex polytopes are predictable, then so is every prediction problem in B P . However any one-way function that is hard on its iterates <ref> [GKL88] </ref> [Lev87] leads to a problem in B P that is not predictable [PW90]. Thus modulo the minimal cryptographic assumption that such functions exist, any prediction complete problem for B P (including vertex-represented convex polytopes) is not predictable.
Reference: [Gol77] <author> GOLDSCHLAGER, L.M. </author> <year> (1977), </year> <title> The monotone and planar circuit value problems are log space complete for P . SIGACT News, </title> <journal> vol. </journal> <volume> 9, no. 2, </volume> <pages> pp. 25-29. </pages>
Reference-contexts: We can easily extend the preceding argument to establish that the following problem is log space complete for P : given a finite set S Q d , and x 2 Q d , is x in the convex hull of S? First, it was established in <ref> [Gol77] </ref> that the evaluation problem for monotone circuits is log space complete for P . 7 Using the reduction of the previous section, we can now prove the following.
Reference: [Gru67] <author> GR UNBAUM, B. </author> <year> (1967), </year> <title> Convex Polytopes. </title> <publisher> Interscience. </publisher> <address> New York. </address> <month> 18 </month>
Reference-contexts: R d , we say that y is in the convex hull of fa (1) ; :::; a (n) g if and only if there exist nonnegative 1 ; :::; n 2 R such that P y = i=1 The convex hull of a finite set is a convex polytope <ref> [Gru67] </ref>. <p> Our proof is similar to that of a related theorem in <ref> [Gru67, page 11] </ref>. Lemma 2 Let C be a closed, convex subset of R d and let y 2 R d be an element outside of C. Then there exists a hyperplane H containing y such that H " C = ;.
Reference: [HKLW91] <author> HAUSSLER, D., KEARNS, M., LITTLESTONE, N. and WARMUTH, M.K. </author> <year> (1991), </year> <title> Equiv--alence of models for polynomial learnability. Information and Computation, to appear. </title> <booktitle> An extended abstract appeared in Proceedings of the 1st Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The notion of polynomial predictability is equivalent to that of PAC learnability in terms of any "reasonable" hypothesis class <ref> [HKLW91] </ref>. 2 Called capacity in [Vap82]. 3 dimension. <p> Throughout the paper, we will use predictable as a synonym for polynomially predictable. A number of equivalent models are described in <ref> [HKLW91] </ref>. Let B 0 = (R 0 ; c 0 ) and B 1 = (R 1 ; c 1 ) be prediction problems.
Reference: [HLW88] <author> HAUSSLER, D., LITTLESTONE, N. and WARMUTH, M.K. </author> <year> (1988), </year> <title> Predicting f0; 1g functions on randomly drawn points. </title> <booktitle> Proceedings of the 29th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 100-109. </pages>
Reference-contexts: A more formal definition of the model (which was introduced in <ref> [HLW88] </ref> and [PW90]) will be given in the following section. <p> Another algorithm for learning k-term DNF in terms of DNF was discovered in parallel [BS90]. In Section 5, we summarize the paper and give a number of open problems. 2 Preliminary Definitions We begin by formalizing the definition of predictability discussed in the introduction <ref> [HLW88] </ref> [PW90]. Let and be finite alphabets. If is a string, let jj denote the length of . If n is a natural number, let [n] be the set of all strings over of length at most n. A concept is any subset of fl .
Reference: [HR85] <author> HOOVER, H.J. and RUZZO, W.L. </author> <year> (1985), </year> <title> A compendium of problems complete for P. </title> <type> Technical Report, </type> <institution> University of Washington. </institution>
Reference-contexts: Theorem 6 The problem of determining whether a point is in the convex hull of a finite set of points is log space complete for P . 7 Surveys of P -complete problems can be found in <ref> [HR85] </ref> [MSS89]. 11 Proof: The only question is whether the inequalities of the reduction of Theorem 1 (with right hand sides normalized to 1) can be output using log space.
Reference: [HSW91] <author> HELMBOLD, D., SLOAN, R., and WARMUTH, M.K. </author> <year> (1991), </year> <title> Learning Integer Lattices. </title> <journal> SIAM Journal on Computing, </journal> <note> to appear. </note>
Reference-contexts: As our first positive result, we show that U k (B F LAT ) is predictable by reducing this prediction problem to B F LAT . The fact that B F LAT is predictable was proved in [Shv88] and <ref> [HSW91] </ref>. Our reduction is similar to that of [Blu89][VW89] which showed that the class of border augmented symmetric differences of halfspaces reduces to the class of halfspaces. Theorem 7 U 2 (B F LAT ) fi B F LAT . <p> Proof: Follows from Corollary 8, together with the fact that fi preserves predictability [PW90] and B F LAT is predictable [Shv88] <ref> [HSW91] </ref>. 2 Note that there is a trivial prediction preserving reduction to U k (B F LAT ) from the corresponding prediction problem in which the flats are not restricted to be homogeneous, so our result extends to unions of arbitrary flats. <p> It is an open problem whether the class of unbounded unions of flats is predictable. Another interesting open question is whether the class of unions of a fixed number of integer lattices <ref> [HSW91] </ref> is predictable. In addition, we showed that unions of a fixed number of boxes are predictable. The problem of predicting the class of unbounded unions of boxes remains open.
Reference: [HP89] <author> HELMBOLD, D. and PAGALLO, G. </author> <year> (1989), </year> <title> There is No Continuous Prediction Preserving Reduction Between the Intersection of Two Halfspaces and a Single Halfspace. </title>
Reference-contexts: In [War89], it was conjectured that no such reduction exists, and if the instance transformation is restricted to be continuous, there is provably 17 no such reduction <ref> [HP89] </ref>. Still, the question of whether there is a reduction with a discontinuous instance transformation remains open. On the positive side, we showed that unions of a fixed number of subspaces, and therefore of a fixed number of flats, are predictable.
Reference: [Kar84] <author> KARMARKAR, N. </author> <year> (1984), </year> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Com-binatorica, </journal> <volume> 4 </volume> <pages> 373-395. </pages>
Reference-contexts: Lemma 3, w 2 c (r) if and only if f (w) 62 c 0 (g (r)). 2 By the transitivity of fi, together with the fact that one can test whether a point is a convex combination of a finite set of points in polynomial time using linear programming <ref> [Kar84] </ref>, we get the main result (see Theorem 5 B CHULL is prediction complete for B P ffi y 1 = 1 y 3 = 1 y 5 Constraints In B PLUS form Dual points x 1 x 4 &lt; 1 y 4 x 2 x 4 &lt; 1 x 4
Reference: [Lev87] <author> LEVIN, L. </author> <year> (1987), </year> <title> One-way functions and pseudorandom generators. </title> <journal> Combinatorica, </journal> <volume> 7(4) </volume> <pages> 357-363. </pages>
Reference-contexts: Thus if vertex-represented convex polytopes are predictable, then so is every prediction problem in B P . However any one-way function that is hard on its iterates [GKL88] <ref> [Lev87] </ref> leads to a problem in B P that is not predictable [PW90]. Thus modulo the minimal cryptographic assumption that such functions exist, any prediction complete problem for B P (including vertex-represented convex polytopes) is not predictable.
Reference: [MSS89] <author> MIYANO, S., SHIRAISHI, S. and SHOUDAI, T. </author> <year> (1989), </year> <title> A list of P-complete problems. </title> <type> Technical Report RIFIS-TR-CS-17, </type> <institution> Kyushu University, </institution> <address> Japan. </address>
Reference-contexts: Theorem 6 The problem of determining whether a point is in the convex hull of a finite set of points is log space complete for P . 7 Surveys of P -complete problems can be found in [HR85] <ref> [MSS89] </ref>. 11 Proof: The only question is whether the inequalities of the reduction of Theorem 1 (with right hand sides normalized to 1) can be output using log space.
Reference: [Par87] <author> PARBERRY, I. </author> <year> (1987), </year> <title> Parallel Complexity Theory. </title> <publisher> Pitman, London. </publisher>
Reference: [PV88] <author> PITT, L. and VALIANT, L.G. </author> <year> (1988), </year> <title> Computational limitations on learning from examples. </title> <journal> JACM, </journal> <volume> 35(4) </volume> <pages> 965-984. </pages>
Reference-contexts: The hypotheses produced are k (2n) k -term DNF, where n is the number of variables. The "standard" algorithm for k-term DNF uses k-CNF as hypotheses <ref> [PV88] </ref>. Another algorithm for learning k-term DNF in terms of DNF was discovered in parallel [BS90]. In Section 5, we summarize the paper and give a number of open problems. 2 Preliminary Definitions We begin by formalizing the definition of predictability discussed in the introduction [HLW88] [PW90]. <p> In contrast, it is NP-hard to produce a consistent DNF with no more than 2k 3 terms. Note that the standard algorithm for learning k-term DNF <ref> [PV88] </ref> uses hypotheses in k-CNF (CNF expressions with at most k literals per clause). <p> However, even the problem of whether intersections of two halfspaces can be predicted for arbitrary distributions remains open (Note that the boolean restriction of this problem to 2-clause CNF is predictable <ref> [PV88] </ref>). The fact that the class of border augmented symmetric differences of halfspaces reduces to halfspaces might lead one to believe that the class of intersections of two halfspaces reduces to halfspaces.
Reference: [PW90] <author> PITT, L. and WARMUTH, M.K. </author> <year> (1990), </year> <title> Prediction Preserving Reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41 (3) : 430-467. </volume>
Reference-contexts: A more formal definition of the model (which was introduced in [HLW88] and <ref> [PW90] </ref>) will be given in the following section. <p> To address the question of the predictability of this class when resources are allowed to grow with the length of the representation of the hidden concept, we must choose a representation language for the class of convex polytopes. As in <ref> [PW90] </ref> we are only interested in representation classes and their associated prediction problems for which the following question can be answered in polynomial time: given a point and a representation, is the point in the concept defined by the representation. <p> Thus if vertex-represented convex polytopes are predictable, then so is every prediction problem in B P . However any one-way function that is hard on its iterates [GKL88] [Lev87] leads to a problem in B P that is not predictable <ref> [PW90] </ref>. Thus modulo the minimal cryptographic assumption that such functions exist, any prediction complete problem for B P (including vertex-represented convex polytopes) is not predictable. <p> The examples do not consist of points labeled according to whether they are in the hidden polytope. Instead they consist of encodings of hyper-cubes labeled according to whether they intersect the hidden polytope. In <ref> [PW90] </ref> this prediction problem was proven to be prediction complete for B P . We give a dual transformation from a problem related to the above to the prediction problem for vertex-represented convex polytopes (see the beginning of Section 3 for a high-level discussion of this reduction). <p> Another algorithm for learning k-term DNF in terms of DNF was discovered in parallel [BS90]. In Section 5, we summarize the paper and give a number of open problems. 2 Preliminary Definitions We begin by formalizing the definition of predictability discussed in the introduction [HLW88] <ref> [PW90] </ref>. Let and be finite alphabets. If is a string, let jj denote the length of . If n is a natural number, let [n] be the set of all strings over of length at most n. A concept is any subset of fl . <p> For all w 2 fl , f is computable in time t (jwj). 3. For all r 2 R 0 , jg (r)j q (jrj). This notion of reducibility is more restrictive than that introduced in <ref> [PW90] </ref>, but is all that is required for the reductions of this paper. The fact that fi is transitive and preserves predictability was proven in [PW90]. <p> For all r 2 R 0 , jg (r)j q (jrj). This notion of reducibility is more restrictive than that introduced in <ref> [PW90] </ref>, but is all that is required for the reductions of this paper. The fact that fi is transitive and preserves predictability was proven in [PW90]. <p> We say a prediction problem B 5 This set is called R P in <ref> [PW90] </ref>. 5 is prediction complete for B P if for every B 0 2 B P , B 0 reduces to B using the more general definition of reduction given in [PW90]. Many examples are given in [PW90]. <p> We say a prediction problem B 5 This set is called R P in <ref> [PW90] </ref>. 5 is prediction complete for B P if for every B 0 2 B P , B 0 reduces to B using the more general definition of reduction given in [PW90]. Many examples are given in [PW90]. Our proof that B CHULL is prediction complete for B P consists of a reduction from the following prediction problem, which was shown to be prediction complete for B P in [PW90]: B CIRC = (R; c), where R = fr : r encodes <p> say a prediction problem B 5 This set is called R P in <ref> [PW90] </ref>. 5 is prediction complete for B P if for every B 0 2 B P , B 0 reduces to B using the more general definition of reduction given in [PW90]. Many examples are given in [PW90]. Our proof that B CHULL is prediction complete for B P consists of a reduction from the following prediction problem, which was shown to be prediction complete for B P in [PW90]: B CIRC = (R; c), where R = fr : r encodes an acyclic boolean circuit with AND, <p> reduces to B using the more general definition of reduction given in <ref> [PW90] </ref>. Many examples are given in [PW90]. Our proof that B CHULL is prediction complete for B P consists of a reduction from the following prediction problem, which was shown to be prediction complete for B P in [PW90]: B CIRC = (R; c), where R = fr : r encodes an acyclic boolean circuit with AND, NOT and OR gatesg, and if r has n inputs, c (r) is the set of boolean strings of length n which are accepted by the circuit encoded by r. <p> Since B HY P=P OLY is known to be prediction complete for B P <ref> [PW90] </ref>, we hoped to construct a prediction preserving reduction from B HY P=P OLY to B CHULL based on duality. <p> Proof: Follows from Corollary 8, together with the fact that fi preserves predictability <ref> [PW90] </ref> and B F LAT is predictable [Shv88] [HSW91]. 2 Note that there is a trivial prediction preserving reduction to U k (B F LAT ) from the corresponding prediction problem in which the flats are not restricted to be homogeneous, so our result extends to unions of arbitrary flats. <p> The question of whether the same concept class encoded by listing the facets is predictable remains open. The associated membership evaluation problem for the latter problem is in N C 1 <ref> [PW90] </ref> which suggests that this problem might be easier, and that it is unlikely to be prediction complete for P .
Reference: [Shv88] <author> SHVAYSTER, H. </author> <year> (1988), </year> <title> Linear Manifolds are learnable from positive examples. </title> <type> Manuscript. </type>
Reference-contexts: Flats are translations of subspaces of Euclidian space. Our proof of the predictability of fixed finite unions of flats consists of reducing this prediction problem to that of predicting flats. The class of flats was shown to be predictable in <ref> [Shv88] </ref>. In [VW89] and independently in [Blu89], a similar technique was applied to show that the class of "border augmented symmetric differences of halfspaces" 4 is predictable. Our result for predicting a fixed number of flats holds even if the dimension varies with the target concept. <p> As our first positive result, we show that U k (B F LAT ) is predictable by reducing this prediction problem to B F LAT . The fact that B F LAT is predictable was proved in <ref> [Shv88] </ref> and [HSW91]. Our reduction is similar to that of [Blu89][VW89] which showed that the class of border augmented symmetric differences of halfspaces reduces to the class of halfspaces. Theorem 7 U 2 (B F LAT ) fi B F LAT . <p> Proof: Follows from Corollary 8, together with the fact that fi preserves predictability [PW90] and B F LAT is predictable <ref> [Shv88] </ref> [HSW91]. 2 Note that there is a trivial prediction preserving reduction to U k (B F LAT ) from the corresponding prediction problem in which the flats are not restricted to be homogeneous, so our result extends to unions of arbitrary flats.
Reference: [Val84] <author> VALIANT, L.G. </author> <year> (1984), </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142. </pages>
Reference-contexts: The model of polynomial predictability used here is related to and in some sense equivalent to the PAC model introduced by Valiant 1 <ref> [Val84] </ref>. The prediction algorithm assumes that the target concept is chosen by an adversary from some class of subsets of Euclidian space (called the target concept class), and we ask the question of whether this assumption is strong enough to admit acceptable performance.
Reference: [Vap82] <author> VAPNIK, V.N. </author> <year> (1982), </year> <title> Estimation of Dependencies Based on Empirical Data. </title> <publisher> Springer Verlag. </publisher> <address> New York. </address>
Reference-contexts: The notion of polynomial predictability is equivalent to that of PAC learnability in terms of any "reasonable" hypothesis class [HKLW91]. 2 Called capacity in <ref> [Vap82] </ref>. 3 dimension. The problem of whether the dependence on the domain dimension can be made polynomial as well remains open. 3 Consider the following more complex prediction problem associated with a hidden convex polytope (contained in the unit cube) represented by a conjunction of halfspaces.
Reference: [VC71] <author> VAPNIK, V.N. and CHERVONENKIS A.Y. </author> <year> (1971), </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theoretical Probability and its Applications. </journal> <volume> 16, 2, </volume> <pages> 264-280. </pages>
Reference-contexts: A more formal definition of the model (which was introduced in [HLW88] and [PW90]) will be given in the following section. Since any set of points on a sphere can be shattered by the class of convex polytopes, this class has infinite Vapnik-Chervonenkis (VC) dimension 2 <ref> [VC71] </ref>, and therefore, if we do not allow the algorithm's resources to grow with the complexity of the target concept, this class is not predictable [BEHW89].
Reference: [VW89] <author> VALIANT, L.G. and WARMUTH, M.K. </author> <year> (1989), </year> <title> The border-augmented symmetric difference of halfspaces is learnable. </title> <month> June, </month> <year> 1989. </year> <type> Manuscript. </type>
Reference-contexts: Flats are translations of subspaces of Euclidian space. Our proof of the predictability of fixed finite unions of flats consists of reducing this prediction problem to that of predicting flats. The class of flats was shown to be predictable in [Shv88]. In <ref> [VW89] </ref> and independently in [Blu89], a similar technique was applied to show that the class of "border augmented symmetric differences of halfspaces" 4 is predictable. Our result for predicting a fixed number of flats holds even if the dimension varies with the target concept.
Reference: [War89] <author> WARMUTH, M.K. </author> <year> (1989), </year> <title> Towards Representation Independence in PAC Learning. </title> <booktitle> Analogical and Inductive Inference : International Workshop AII 1989. </booktitle> <publisher> Springer-Verlag. </publisher> <pages> 19 </pages>
Reference-contexts: The fact that the class of border augmented symmetric differences of halfspaces reduces to halfspaces might lead one to believe that the class of intersections of two halfspaces reduces to halfspaces. In <ref> [War89] </ref>, it was conjectured that no such reduction exists, and if the instance transformation is restricted to be continuous, there is provably 17 no such reduction [HP89]. Still, the question of whether there is a reduction with a discontinuous instance transformation remains open.
References-found: 27

