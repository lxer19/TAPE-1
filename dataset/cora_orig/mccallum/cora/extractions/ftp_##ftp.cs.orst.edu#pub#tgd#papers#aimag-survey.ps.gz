URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/aimag-survey.ps.gz
Refering-URL: http://www.cs.wisc.edu/~shavlik/cs760.html
Root-URL: 
Title: Machine Learning Research: Four Current Directions  
Author: Thomas G. Dietterich 
Date: May 23, 1997.  
Note: Draft of  
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science Oregon State University  
Abstract: Machine Learning research has been making great progress is many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (a) improving classification accuracy by learning ensembles of classifiers, (b) methods for scaling up supervised learning algorithms, (c) reinforcement learning, and (d) learning complex stochastic models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. </author> <year> (1990). </year> <title> Learning from hints in neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 6, </volume> <pages> 192-198. </pages>
Reference: <author> Ali, K. M., & Pazzani, M. J. </author> <year> (1996). </year> <title> Error reduction through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> 24 (3), </volume> <pages> 173-202. </pages> <note> 55 correspond to cases where TAN gave more accurate results than C4.5. </note>
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72, </volume> <pages> 81-138. </pages>
Reference-contexts: I will summarize these developments here. 4.1 An Introduction to Dynamic Programming The most important insight of the past five years is that reinforcement learning is best analyzed as a form of online, approximate dynamic programming <ref> (Barto, Bradtke, & Singh, 1995) </ref>. I will introduce this insight using the following notation. Consider a robot interacting with an external environment. At each time t, the environment is in some state s t , and the robot has available some set of actions A.
Reference: <author> Barto, A. G., & Sutton, R. </author> <year> (1997). </year> <title> Introduction to Reinforcement Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: The goal of reinforcement learning algorithms is to compute the optimal policy, denoted fl , which maximizes the cumulative discounted reward. Researchers in dynamic programming <ref> (e.g., Bellman, 1957) </ref> found it convenient to define a real-valued function f (s) called the value function of policy . The value function f (s) gives the expected cumulative discounted reward that will be received by starting in state s and executing policy .
Reference: <author> Bertsekas, D. P., & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Blum, A., & Rivest, R. L. </author> <year> (1988). </year> <title> Training a 3-node neural network is NP-Complete (Extended abstract). </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pp. </pages> <address> 9-18 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hence, practical decision tree algorithms employ search heuristics to guide a greedy search for small decision trees. Similarly, finding the weights for the smallest possible neural network consistent with the training examples is also NP-hard <ref> (Blum & Rivest, 1988) </ref>. Neural network algorithms therefore employ local search methods (such as gradient descent) to find locally optimal weights for the network.
Reference: <author> Blum, A. </author> <year> (1997). </year> <title> Empirical support for Winnow and Weighted-Majority algorithms: Results on a calendar scheduling domain. </title> <journal> Machine Learning, </journal> <volume> 26 (1), </volume> <pages> 5-24. </pages>
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Stacked regressions. </title> <journal> Machine Learning, </journal> <volume> 24, </volume> <pages> 49-64. </pages>
Reference: <author> Buntine, W. L. </author> <year> (1990). </year> <title> A theory of learning classification rules. </title> <type> Ph.D. thesis, </type> <institution> University of Technology, School of Computing Science, </institution> <address> Sydney, Australia. </address>
Reference: <author> Buntine, W. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 159-225. </pages> <note> 56 Buntine, </note> <author> W. </author> <year> (1996). </year> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8, </volume> <pages> 195-210. </pages>
Reference: <author> Caruana, R. </author> <year> (1996). </year> <title> Algorithms and applications for multitask learning. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 87-95 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1023-1028 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Castillo, E., Gutierrez, J. M., & Hadi, A. </author> <year> (1997). </year> <title> Expert Systems and Probabilistic Network Models. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Kodratoff, Y. (Ed.), </editor> <booktitle> Proceedings of the European Working Session on Learning, </booktitle> <pages> pp. </pages> <address> 164-178 Berlin. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Chan, P. K., & Stolfo, S. J. </author> <year> (1995). </year> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pp. </pages> <address> 39-44 Menlo Park, CA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A third approach to large datasets is to take advantage of ensembles of decision trees <ref> (Chan & Stolfo, 1995) </ref>. The training data can be randomly partitioned into N disjoint subsets. A separate decision tree can be grown from each subset in parallel. The trees can then vote to make classification decisions.
Reference: <author> Cheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, D., & Stutz, J. </author> <year> (1988). </year> <title> Bayesian classification. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 607-611 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: A well-known application of the EM algorithm in unsupervised clustering is the Autoclass program <ref> (Cheeseman, Self, Kelly, Taylor, Freeman, & Stutz, 1988) </ref>. In addition to discrete variables (of the kind I have been discussing), Autoclass can handle continuous variables. <p> This is easily accomplished by a minor modification of EM. One of the most interesting applications of Autoclass was to the problem of analyzing the infrared spectra of stars. Autoclass discovered a new class of star, and this discovery was subsequently accepted by astronomers <ref> (Cheeseman et al., 1988) </ref>. 5.3.3 Gibbs Sampling The final algorithm that I will discuss is a Monte Carlo technique called Gibbs sampling (Geman & Geman, 1984).
Reference: <author> Cherkauer, K. J. </author> <year> (1996). </year> <title> Human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks. </title> <editor> In Chan, P. (Ed.), </editor> <booktitle> Working Notes of the AAAI Workshop on Integrating Multiple Learned Models, </booktitle> <pages> pp. 15-21. </pages> <note> Available from http://www.cs.fit.edu/~imlm/. </note>
Reference: <author> Chipman, H., George, E., & McCulloch, R. </author> <year> (1996). </year> <title> Bayesian CART. </title> <type> Tech. rep., </type> <institution> Department of Statistics, University of Chicago. </institution> <note> Available as http://gsbrem.uchicago.edu/Papers/cart.ps. </note>
Reference: <author> Chow, C., & Liu, C. </author> <year> (1968). </year> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14, </volume> <pages> 462-467. </pages>
Reference: <author> Clemen, R. T. </author> <year> (1989). </year> <title> Combining forcasts: A review and annotated bibliography. </title> <journal> International Journal of Forecasting, </journal> <volume> 5, </volume> <pages> 559-583. </pages>
Reference-contexts: The simplest approach is to take an unweighted vote as is done in bagging, ECOC, and many other methods. While it may appear that more intelligent voting schemes should do better, the experience in the forecasting literature has been that simple, unweighted voting is very robust <ref> (Clemen, 1989) </ref>. One refinement on simple majority vote is appropriate when each classifier h ` can produce class probability estimates rather than a simple classification decision.
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <booktitle> In The Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 115-123 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Both of these methods should scale well with the number of classes. Error-correcting output coding has been tested on problems with up to 126 classes, but tests on very large problems with thousands of classes have not yet been performed. 24 Table 3: The Ripper algorithm <ref> (Cohen, 1995) </ref> procedure BuildRuleSet (P ,N) P = positive examples N = negative examples RuleSet = fg DL = DescriptionLength (RuleSet; P; N) while P 6= fg // Grow and prune a new rule split (P; N) into (GrowP os; GrowNeg) and (P runeP os; P runeNeg) Rule := GrowRule (GrowP
Reference: <author> Cooper, G. F., & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference: <author> Craven, M. W., & Shavlik, J. W. </author> <year> (1996). </year> <title> Extracting tree-structured representations from trained networks. </title> <editor> In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. </pages> <address> 24-30 Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: A single decision tree can often be interpreted by human users, but an ensemble of 200 voted decision trees is much more difficult to understand. Can methods be found for obtaining explanations (at least locally) from ensembles? One example of work on this question is Craven's TREPAN algorithm <ref> (Craven & Shavlik, 1996) </ref>. 14 3 Scaling Up Machine Learning Algorithms A second major research area has explored techniques for scaling up learning algorithms so that they can apply to problems with millions of training examples, thousands of features, and hundreds of classes.
Reference: <author> Crites, R. H., & Barto, A. G. </author> <year> (1995). </year> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, Vol. </booktitle> <address> 8 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher> <address> 57 D'Ambrosio, </address> <publisher> B. </publisher> <year> (1993). </year> <title> Incremental probabilistic inference. </title> <editor> In Heckerman, D., & Mamdani, A. (Eds.), </editor> <booktitle> Ninth Annual Conference on Uncertainty on AI, </booktitle> <pages> pp. 301-308. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P., & Hinton, G. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 5, </volume> <pages> pp. 271-278. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Dean, T., & Kanazawa, K. </author> <year> (1989). </year> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5 (3), </volume> <pages> 142-150. </pages>
Reference-contexts: Then it makes sense to represent the hidden state by two separate state variables: robot location and camera direction. Figure 19 shows the resulting stochastic model, which is variously called a dynamic probabilistic network (DPN, Kanazawa, Koller, & Russell, 1995), a dynamic belief network <ref> (DBN, Dean & Kanazawa, 1989) </ref>, and a factorial HMM (Ghahramani & Jordan, 1996). Unfortunately, inference and learning with DPN's is computationally challenging. The overall approach of applying the EM algorithm or Gibbs sampling is still sound.
Reference: <author> Dempster, A. P., Laird, N. M., & Rubin, D. B. </author> <year> (1976). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Proceedings of the Royal Statistical Society, B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: It suffices to constrain and renormalize them after each step of gradient descent. Russell et al have tested this algorithm on a wide variety of probabilistic network structures. 5.3.2 The Expectation Maximization Algorithm The second algorithm I will discuss is the Expectation Maximization (EM) algorithm <ref> (Dempster, Laird, & Rubin, 1976) </ref>. EM can be applied to probabilistic networks if the node probability distributions belong to the exponential family of distributions (which includes the binomial, multinomial, exponential, poisson, and normal distributions, and many others).
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1995). </year> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference: <author> Dietterich, T. G., & Kong, E. B. </author> <year> (1995). </year> <title> Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon. </institution> <note> Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz. </note>
Reference: <author> Domingos, P., & Pazzani, M. </author> <year> (1996). </year> <title> Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 105-112 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Figure 14 compares the performance of C4.5 to the naive Bayes classifier on 28 benchmark tasks <ref> (Domingos & Pazzani, 1996) </ref>. The results show that except for a few domains where naive Bayes performs very badly, it is typically competitive with or superior to C4.5.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference-contexts: In the worst case, these algorithms require exponential time, but if the probabilistic network is sparsely connected, the running time is quite reasonable. 5.2 The Naive Bayes Classifier A very simple approach to stochastic modeling for classification problems is the so-called "naive" Bayes classifier <ref> (Duda & Hart, 1973) </ref>. In this approach, the training examples are assumed to be produced by the probabilistic network shown in Figure 13, where the class variable is y, and the features are x 1 ; : : : ; x n .
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1022-1027 San Francisco. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Flexner, S. B. (Ed.). </author> <year> (1983). </year> <title> Random House Unabridged Dictionary, 2nd edition. Random House, </title> <address> New York. </address>
Reference-contexts: Golding and Roth (1996) describe an application of Winnow to context-sensitive spelling correction. This is the task of identifying spelling errors where one legal word is substituted for another, such as It's not to late, where to is substituted for too. The Random House dictionary <ref> (Flexner, 1983) </ref> lists many sets of commonly-confused words, and Golding and Roth developed a separate Winnow classifier for each of the listed sets (e.g., fto, too, twog). Winnow's task is to decide whether each occurrance of these words is correct or incorrect based on its context.
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <type> Tech. rep., </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ. </address>
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 148-156 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, N., & Goldszmidt, M. </author> <year> (1996). </year> <title> Building classifiers using Bayesian networks. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1277-1284 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Furnkranz, J., & Widmer, G. </author> <year> (1994). </year> <title> Incremental reduced error pruning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 70-77 San Francisco, CA. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Geman, S., & Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6, </volume> <pages> 721-741. </pages>
Reference-contexts: Autoclass discovered a new class of star, and this discovery was subsequently accepted by astronomers (Cheeseman et al., 1988). 5.3.3 Gibbs Sampling The final algorithm that I will discuss is a Monte Carlo technique called Gibbs sampling <ref> (Geman & Geman, 1984) </ref>. Gibbs sampling is a method for generating random samples from a joint probability distribution P (A 1 ; : : : ; A n ) when sampling directly from the joint distribution is difficult.
Reference: <author> Ghahramani, Z., & Jordan, M. I. </author> <year> (1996). </year> <title> Factorial hidden Markov models. </title> <editor> In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. </pages> <address> 472-478 Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Figure 19 shows the resulting stochastic model, which is variously called a dynamic probabilistic network (DPN, Kanazawa, Koller, & Russell, 1995), a dynamic belief network (DBN, Dean & Kanazawa, 1989), and a factorial HMM <ref> (Ghahramani & Jordan, 1996) </ref>. Unfortunately, inference and learning with DPN's is computationally challenging. The overall approach of applying the EM algorithm or Gibbs sampling is still sound. However, the E-step of 50 location (L t ) and camera direction (S t ). computing the augmented training examples is itself difficult.
Reference: <author> Gilks, W., Thomas, A., & Spiegelhalter, D. </author> <year> (1993). </year> <title> A language and program for complex Bayesian modelling. </title> <journal> The Statistician, </journal> <volume> 43, </volume> <pages> 169-178. </pages>
Reference-contexts: Gibbs sampling is a very general method; it can often be applied in situations where the EM algorithm cannot. The generality of Gibbs sampling has made it possible to construct a general-purpose programming environment, called BUGS, for learning stochastic models <ref> (Gilks, Thomas, & Spiegelhalter, 1993) </ref>. In this environment, the user specifies the graph structure of the stochastic model, the form of the probability distribution at each node, and prior distributions for each parameter. The system then develops a Gibbs sampling algorithm for fitting this model to the training data.
Reference: <author> Golding, A. R., & Roth, D. </author> <year> (1996). </year> <title> Applying Winnow to context-sensitive spelling correction. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 182-190 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gordon, G. J. </author> <year> (1995). </year> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 261-268 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Initial research suggests that value function approximators should be "local averagers" that compute the value of a new state by interpolating among the values of previously visited states <ref> (Gordon, 1995) </ref>. A second key problem is to develop reinforcement methods for hierarchical problem solving. For very large search spaces, where the distance to the goal and the branching factor are big, no search method can work well.
Reference: <author> Hansen, L., & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell., </journal> <volume> 12, </volume> <pages> 993-1001. </pages>
Reference-contexts: The main discovery is that ensembles are often much more accurate than the individual classifiers that make them up. An ensemble can be more accurate than its component classifiers only if the individual classifiers disagree with one another <ref> (Hansen & Salamon, 1990) </ref>. To see why, imagine that we have an ensemble of three classifiers: fh 1 ; h 2 ; h 3 g and consider a new case x.
Reference: <author> Hashem, S. </author> <year> (1993). </year> <title> Optimal linear combinations of neural networks. </title> <type> Ph.D. thesis, </type> <institution> Purdue University, School of Industrial Engineering, Lafayette, </institution> <note> IN. </note>
Reference: <author> Heckerman, D. </author> <year> (1996). </year> <title> A tutorial on learning with Bayesian networks. </title> <type> Tech. rep. </type> <institution> MSR-TR-95-06, Microsoft Research, Advanced Technology Division, </institution> <address> Redmond, WA. </address>
Reference: <author> Heckerman, D., Geiger, D., & Chickering, D. M. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <pages> 197-243. </pages>
Reference: <author> Hyafil, L., & Rivest, R. L. </author> <year> (1976). </year> <title> Constructing optimal binary decision trees is NP-Complete. </title> <journal> Information Processing Letters, </journal> <volume> 5 (1), </volume> <pages> 15-17. </pages>
Reference-contexts: A second "cause" of the need for ensembles is that our learning algorithms may not be able to solve the difficult search problems that we pose. For example, the problem of finding the smallest decision tree that is consistent with a set of training examples is NP-hard <ref> (Hyafil & Rivest, 1976) </ref>. Hence, practical decision tree algorithms employ search heuristics to guide a greedy search for small decision trees. Similarly, finding the weights for the smallest possible neural network consistent with the training examples is also NP-hard (Blum & Rivest, 1988).
Reference: <author> Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. </author> <year> (1990). </year> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statistical Quarterly, </journal> <volume> 4, </volume> <pages> 269-282. </pages>
Reference-contexts: A better approach is to wait until the values of the variables have been observed, and then compute the single corresponding row of the class probability table. This inference problem has been studied intensively, and a very general and elegant algorithm| the junction tree algorithm|has been developed <ref> (Jensen, Lauritzen, & Olesen, 1990) </ref>. In addition, efficient online algorithms have been discovered (e.g., D'Ambrosio, 1993).
Reference: <author> Jensen, F. </author> <year> (1996). </year> <title> An Introduction to Bayesian Networks. </title> <publisher> Springer, </publisher> <address> New York. </address>
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 121-129 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jordan, M. I., & Jacobs, R. A. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 (2), </volume> <pages> 181-214. </pages>
Reference-contexts: the naive model from Figure 13, these models can be applied to a wide number of problems without performing the kind of detailed modeling of causal connections that we performed in the diabetes example from Figure 12. 5.4.1 The Hierarchical Mixture of Experts The Hierarchical Mixture of Experts (HME) model <ref> (Jordan & Jacobs, 1994) </ref> is intended for supervised learning in situations where one believes the training data are being generated by a mixture of separate "experts." For example, in a speech recognition system, we might face the task of distinguishing the spoken words "Bee", "Tree", "Gate", and "Mate".
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical reinforcement learning: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 167-173 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Kanazawa, K., Koller, D., & Russell, S. </author> <year> (1995). </year> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 346-351 San Francisco, CA. </address> <publisher> Morgan Kaufmann. 59 Kira, </publisher> <editor> K., & Rendell, L. A. </editor> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 249-256 San Francisco, CA. </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: At each time step, it chooses to perform exactly one of these two actions. Then it makes sense to represent the hidden state by two separate state variables: robot location and camera direction. Figure 19 shows the resulting stochastic model, which is variously called a dynamic probabilistic network <ref> (DPN, Kanazawa, Koller, & Russell, 1995) </ref>, a dynamic belief network (DBN, Dean & Kanazawa, 1989), and a factorial HMM (Ghahramani & Jordan, 1996). Unfortunately, inference and learning with DPN's is computationally challenging. The overall approach of applying the EM algorithm or Gibbs sampling is still sound.
Reference: <author> Kohavi, R., & Kunz, C. </author> <year> (1997). </year> <title> Option decision trees with majority votes. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning San Francisco, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R., & Sahami, M. </author> <year> (1996). </year> <title> Error-based and entropy-based discretizing of continuous features. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining San Francisco. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kolen, J. F., & Pollack, J. B. </author> <year> (1991). </year> <title> Back propagation is sensitive to initial conditions. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 3, </volume> <pages> pp. </pages> <address> 860-867 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the backpropagation algorithm for training neural networks, the initial weights of the network are set randomly. If the algorithm is applied to the same training examples but with different initial weights, the resulting classifier can be quite different <ref> (Kolen & Pollack, 1991) </ref>. While this is perhaps the most common way of generating ensembles of neural networks, manipulating the training set may be more effective. A study by Parmanto, Munro, and Doyle (1996) compared this technique to bagging and to 10-fold cross-validated committees.
Reference: <author> Kong, E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title>
Reference: <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> The Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 313-321 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of relief. </title> <booktitle> In Proceedings of the 1994 European Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 171-182 Amsterdam. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The mutual information weights of all features will be approximately zero using the formula above. An algorithm that overcomes this problem (and is one of the most successful preprocessing algorithms to date) is the Relief-F algorithm <ref> (Kononenko, 1994) </ref>, which is an extension of an earlier algorithm called Relief (Kira & Rendell, 1992).
Reference: <author> Kononenko, I., Simec, E., & Robnik- Sikonja, M. </author> <year> (1997). </year> <title> Overcoming the myopic of inductive learning algorithms with RELIEFF. </title> <journal> Applied Intelligence, </journal> <note> In Press. </note>
Reference: <author> Kucera, H., & Francis, W. N. </author> <year> (1967). </year> <title> Computational analysis of present-day American English. </title> <publisher> Brown University Press, </publisher> <address> Providence, RI. </address>
Reference-contexts: For example, the sequence "htargeti to VERB" is a collocation feature that checks whether the target word is immediately followed by the word "to" and then a word that can potentially be a verb (according to a dictionary lookup). Based on the 1-million word Brown corpus <ref> (Kucera & Francis, 1967) </ref>, Golding and Roth defined more than 10,000 potentially relevant features. Golding and Roth applied Winnow (with ff = 3=2, fi varying between 0.5 and 0.9, and = 1). They compared its accuracy to the best previous method, which is a modified naive Bayesian algorithm.
Reference: <author> Kwok, S. W., & Carter, C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <editor> In Schachter, R. D., Levitt, T. S., Kannal, L. N., & Lemmer, J. F. (Eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <pages> pp. 327-335. </pages> <publisher> Elsevier Science, Amsterdam. </publisher>
Reference-contexts: They found that cross-validated committees worked best, bagging second best, and multiple random initial weights third best on one synthetic data set and two medical diagnosis data sets. For the C4.5 decision tree algorithm, it is also easy to inject randomness <ref> (Kwok & Carter, 1990) </ref>. The key decision of C4.5 is to choose a feature to test at each internal node in the decision tree. At each internal node, C4.5 applies a criterion known as the information gain ratio to rank-order the various possible feature tests.
Reference: <author> Laird, J. E., Newell, A., & Rosenbloom, P. S. </author> <year> (1987). </year> <title> SOAR: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 (1), </volume> <pages> 1-64. </pages>
Reference-contexts: More intelligent search methods, such as means-ends analysis, need to be integrated into reinforcement learning systems as they have been integrated into other learning architectures such as Soar <ref> (Laird, Newell, & Rosenbloom, 1987) </ref> and Prodigy 37 (Minton, Carbonell, Knoblock, Kuokka, Etzioni, & Gil, 1989). A fourth problem is that optimizing cumulative discounted reward is not always appropriate. In problems where the system needs to operate continuously, a better goal is to maximize the average reward per unit time.
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference-contexts: The resulting classifiers gave excellent results on two challenging benchmark tasks. The last feature weighting algorithm I will discuss is the Winnow algorithm developed by Littlestone <ref> (Littlestone, 1988) </ref>. Winnow is a linear threshold algorithm for 2-class problems with binary (i.e., 0/1-valued) input features. It classifies a new example x into class 2 if X w j x j &gt; and into class 1 otherwise.
Reference: <author> Littman, M. L., Cassandra, A., & Kaelbling, L. P. </author> <year> (1995). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 362-370 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lowe, D. G. </author> <year> (1995). </year> <title> Similarity metric learning for a variable-kernel classifier. </title> <journal> Neural Computation, </journal> <volume> 7 (1), </volume> <pages> 72-85. </pages>
Reference-contexts: Both of them have been shown to work well experimentally, and the second method, called Winnow, works extremely well in problems with thousands of potentially relevant input features. The first algorithm is called the Variable-kernel Similarity Metric or VSM method <ref> (Lowe, 1995) </ref>. VSM is a form of Gaussian radial basis function method. To classify a new data point x t , it defines a multivariate Gaussian probability distribution ' centered on x t with standard deviation .
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4 (3), </volume> <pages> 448-472. </pages>
Reference: <author> Mahadevan, S. </author> <year> (1996). </year> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 159-195. </pages> <note> 60 Mahadevan, </note> <author> S., & Kaelbling, L. P. </author> <year> (1996). </year> <booktitle> The national science foundation workshop on reinforce-ment learning. AI Magazine, </booktitle> <volume> 17 (4), </volume> <pages> 89-97. </pages>
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 387-396 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mehta, M., Agrawal, R., & Rissanen, J. </author> <year> (1996). </year> <title> SLIQ: A fast scalable classifier for data mining. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <pages> pp. </pages> <address> 18-32 New York. </address> <publisher> Springer. </publisher>
Reference: <author> Merz, C. J., & Murphy, P. M. </author> <year> (1996). </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: This makes AdaBoost more stochastic, but experiments have shown that this procedure is still very effective. learning databases <ref> (Merz & Murphy, 1996) </ref>. We can see that most points lie above the line y = x, which indicates that the error rate of AdaBoost is less than the error rate of C4.5. Figure 3 compares the performance of bagging (with C4.5) to C4.5 alone.
Reference: <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset selection in regression. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. </author> <year> (1989). </year> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 63-118. </pages>
Reference-contexts: More intelligent search methods, such as means-ends analysis, need to be integrated into reinforcement learning systems as they have been integrated into other learning architectures such as Soar (Laird, Newell, & Rosenbloom, 1987) and Prodigy 37 <ref> (Minton, Carbonell, Knoblock, Kuokka, Etzioni, & Gil, 1989) </ref>. A fourth problem is that optimizing cumulative discounted reward is not always appropriate. In problems where the system needs to operate continuously, a better goal is to maximize the average reward per unit time.
Reference: <author> Moore, A. W., & Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title>
Reference: <editor> In Cohen, W., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 190-198 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Munro, P., & Parmanto, B. </author> <year> (1997). </year> <title> Competition among networks improves committee performance. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 9, </volume> <editor> p. </editor> <publisher> In Press. </publisher>
Reference: <author> Musick, R., Catlett, J., & Russell, S. </author> <year> (1992). </year> <title> Decision theoretic subsampling for induction on large databases. </title> <editor> In Utgoff, P. E. (Ed.), </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 212-219 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Neal, R. </author> <year> (1993). </year> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Tech. rep. </type> <institution> CRG-TR-93-1, Department of Computer Science, University of Toronto, Toronto, </institution> <address> CA. </address>
Reference-contexts: In practice, this problem often does not arise, but in general, steps must be taken to remove symmetries from the model <ref> (see Neal, 1993) </ref>. Gibbs sampling is a very general method; it can often be applied in situations where the EM algorithm cannot. The generality of Gibbs sampling has made it possible to construct a general-purpose programming environment, called BUGS, for learning stochastic models (Gilks, Thomas, & Spiegelhalter, 1993).
Reference: <author> Ok, D., & Tadepalli, P. </author> <year> (1996). </year> <title> Auto-exploratory average reward reinforcement learning. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 881-887 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Opitz, D. W., & Shavlik, J. W. </author> <year> (1996). </year> <title> Generating accurate and diverse members of a neural-network ensemble. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <pages> pp. </pages> <address> 535-541 Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Parmanto, B., Munro, P. W., & et al, H. R. D. </author> <year> (1994). </year> <title> Neural network classifier for hepatoma detection. </title> <booktitle> In Proceedings of the World Congress on Neural Networks, Vol. </booktitle> <address> 1 Mahwah, NJ. </address> <publisher> Lawrence Earlbaum Associates. </publisher>
Reference: <author> Parmanto, B., Munro, P. W., & Doyle, H. R. </author> <year> (1996). </year> <title> Improving committee diagnosis with resampling techniques. </title> <editor> In Touretzky, D. S., Mozer, M. C., & Hesselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 8, </volume> <pages> pp. </pages> <address> 882-888 Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Then 10 overlapping training sets can be constructed by dropping out a different one of these 10 subsets. This same procedure is employed to construct training sets for 10-fold cross-validation, so ensembles constructed in this way are sometimes called cross-validated committees <ref> (Parmanto, Munro, & Doyle, 1996) </ref>. The third method for manipulating the training set is illustrated by the AdaBoost algorithm, developed by Freund and Schapire (1995, 1996) and shown in Table 1. Like bagging, AdaBoost manipulates the training examples to generate multiple hypotheses.
Reference: <author> Parr, R., & Russell, S. </author> <year> (1995). </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1088-1094 San Francisco, CA. </address> <publisher> Morgan Kaufmann. 61 Perrone, </publisher> <editor> M. P., & Cooper, L. N. </editor> <year> (1993). </year> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In Mammone, R. J. (Ed.), </editor> <title> Neural networks for speech and image processing. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Press, W. H., Flannery, B. P., Teukolsky, S. A., & Verrerling, W. T. </author> <year> (1992). </year> <title> Numerical recipes in C : The art of scientific computing, 2nd Edition. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: It then computes the gradient and performs a search in the direction of the gradient to minimize LOOCV error (while keeping this set of nearest neighbors fixed). The search along the direction of the gradient is called a line search, and there are several efficient algorithms available <ref> (Press, Flannery, Teukolsky, & Verrerling, 1992) </ref>. Even though the weights are changing during the line search, the set of nearest neighbors (and the gradient) is not recomputed.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference-contexts: It then iteratively adds tests to the rule until the rule covers no negative examples. Tests are selected via an information gain heuristic developed for Quinlan's FOIL system <ref> (Quinlan, 1990) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: For a training set of size m, the runtime of Ripper scales as O (m (log m) 2 ). This is a major improvement over the rule-learning program C4.5rules <ref> (Quinlan, 1993) </ref>, which scales as O (m 3 ). Table 3 shows pseudo-code for Ripper. Ripper works by building an initial set of rules and optimizing the set of rules k times, where k is a parameter (typically set to 2).
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 725-730 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference-contexts: Minimum description length criteria of this kind have been applied very successfully to rule- and tree-learning algorithms <ref> (e.g., Quinlan & Rivest, 1989) </ref>. Ripper stops adding rules when the description length of the rule set is more than 64 bits larger than the best description length observed so far.
Reference: <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 (2), </volume> <pages> 257-286. </pages>
Reference-contexts: If the HMM makes a transition into this state, it terminates the 49 string being generated. This permits HMM's to model strings of variable length. Hidden Markov models have been widely applied in speech recognition, where the alphabet of letters consists of "frames" of the speech signal <ref> (Rabiner, 1989) </ref>. Each word in the language can be modeled as an HMM. Given a new spoken word, a speech recognition system computes the likelihood that each of the word HMM's generated that spoken word. The recognizer then predicts the most likely word.
Reference: <author> Raviv, Y., & Intrator, N. </author> <year> (1996). </year> <title> Bootstrapping with noise: An effective regularization technique. </title> <journal> Connection Science, </journal> <volume> 8 (3-4), </volume> <pages> 355-372. </pages>
Reference: <author> Revow, M., Williams, C. K. I., & Hinton, G. E. </author> <year> (1996). </year> <title> Using generative models for handwritten digit recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18 (6), </volume> <pages> 592-606. </pages>
Reference: <author> Ricci, F., & Aha, D. W. </author> <year> (1997). </year> <title> Extending local learners with error-correcting output codes. </title> <type> Tech. rep., </type> <institution> Naval Center for Applied Research in Artificial Intelligence. </institution>
Reference: <author> Rosen, B. E. </author> <year> (1996). </year> <title> Ensemble learning using decorrelated neural networks. </title> <journal> Connection Science, </journal> <volume> 8 (3-4), </volume> <pages> 373-384. </pages>
Reference: <author> Russell, S., Binder, J., Koller, D., & Kanazawa, K. </author> <year> (1995). </year> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proc. Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1146-1152 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: At each time step, it chooses to perform exactly one of these two actions. Then it makes sense to represent the hidden state by two separate state variables: robot location and camera direction. Figure 19 shows the resulting stochastic model, which is variously called a dynamic probabilistic network <ref> (DPN, Kanazawa, Koller, & Russell, 1995) </ref>, a dynamic belief network (DBN, Dean & Kanazawa, 1989), and a factorial HMM (Ghahramani & Jordan, 1996). Unfortunately, inference and learning with DPN's is computationally challenging. The overall approach of applying the EM algorithm or Gibbs sampling is still sound.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3, </volume> <pages> 211-229. </pages>
Reference-contexts: This section addresses problems of sequential decision making and control that come under the heading of reinforcement learning. Work in reinforcement learning dates back to the earliest days of artificial intelligence when Arthur Samuel developed his famous checkers program <ref> (Samuel, 1959) </ref>. More recently, there have been several important advances in the practice and theory of reinforcement learning. Perhaps the most famous work is Gerry Tesauro's (1992) TD-gammon program, which has learned to play backgammon better than any other computer program and almost as well as the best human players.
Reference: <author> Schapire, R. E. </author> <year> (1997). </year> <title> Using output codes to boost multiclass learning problems. </title> <type> Tech. rep., </type> <institution> AT&T Research. </institution>
Reference: <author> Schwartz, A. </author> <year> (1993). </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <editor> In Utgoff, P. (Ed.), </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 298-305 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Shafer, J., Agrawal, R., & Mehta, M. </author> <year> (1996). </year> <title> SPRINT: A scalable parallel classifier for data mining. </title> <booktitle> In Proceedings of the Twenty-Second VLDB Conference, </booktitle> <pages> pp. </pages> <address> 544-555 San Francisco, CA. </address> <publisher> Morgan Kaufmann. 62 Singh, </publisher> <editor> S., & Bertsekas, D. </editor> <year> (1997). </year> <title> Reinforcement learning for dynamic channel allocation in cellular telephone systems. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 10. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions to elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 323-340. </pages>
Reference: <author> Smyth, P., Heckerman, D., & Jordan, M. I. </author> <year> (1997). </year> <title> Probabilistic independence networks for hidden Markov probability models. </title> <journal> Neural Computation, </journal> <volume> 9 (2), </volume> <pages> 227-270. </pages>
Reference: <author> Spiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8, </volume> <pages> 219-282. </pages>
Reference-contexts: Gibbs sampling is a very general method; it can often be applied in situations where the EM algorithm cannot. The generality of Gibbs sampling has made it possible to construct a general-purpose programming environment, called BUGS, for learning stochastic models <ref> (Gilks, Thomas, & Spiegelhalter, 1993) </ref>. In this environment, the user specifies the graph structure of the stochastic model, the form of the probability distribution at each node, and prior distributions for each parameter. The system then develops a Gibbs sampling algorithm for fitting this model to the training data.
Reference: <author> Spirtes, P., Glymour, C., & Scheines, R. </author> <year> (1993). </year> <title> Causation, Prediction, and Search. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address> <note> Available online at http://hss.cmu.edu/html/departments/philosophy/TETRAD.BOOK/book.html. </note>
Reference: <author> Spirtes, P., & Meek, C. </author> <year> (1995). </year> <title> Learning Bayesian networks with discrete variables from data. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pp. </pages> <address> 294-299 San Francisco. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stolcke, A., & Omohundro, S. M. </author> <year> (1994). </year> <title> Best-first model merging for hidden Markov model induction. </title> <type> Tech. rep. </type> <institution> TR-94-003, International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 (1), </volume> <pages> 9-44. </pages>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-278. </pages>
Reference-contexts: Hence, T D () can be combined with policy improvement to learn an optimal policy for backgammon. This is what Tesauro did in his famous TD-gammon system <ref> (Tesauro, 1992, 1995) </ref>. TD-gammon employs a neural network representation of the value of a state. The state of the backgammon game is described by a vector of 198 features that encode the locations of the pieces on the board and the values shown on the dice.
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 28 (3), </volume> <pages> 58-68. </pages>
Reference: <author> Tsitsiklis, J. N., & Van Roy, B. </author> <year> (1996). </year> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Tech. rep., </type> <institution> Massachusetts Institute of Technology, Laboratory for Information and Decision Systems, </institution> <address> Cambridge, MA. </address>
Reference: <author> Tumer, K., & Ghosh, J. </author> <year> (1996). </year> <title> Error correlation and error reduction in ensemble classifiers. </title> <journal> Connection Science, </journal> <volume> 8 (3-4), </volume> <pages> 385-404. </pages>
Reference: <author> Verma, T., & Pearl, J. </author> <year> (1990). </year> <title> Equivalence and synthesis of causal models. </title> <booktitle> In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 220-227 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> King's College, Oxford. </institution> <note> (To be reprinted by MIT Press.). </note>
Reference: <author> Watkins, C. J., & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference: <author> Wettschereck, D., Aha, D. W., & Mohri, T. </author> <year> (1997). </year> <title> A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 10, </volume> <pages> 1-37. </pages>
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1995). </year> <title> An experimental comparison of the nearest-neighbor and nearest-hyperrectangle algorithms. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 5-27. </pages> <address> 63 Wolpert, D. </address> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 (2), </volume> <pages> 241-260. </pages>
Reference: <author> Zhang, W., & Dietterich, T. G. </author> <year> (1995). </year> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In 1995 International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1114-1120. </pages> <publisher> AAAI/MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Zhang, X., Mesirov, J. P., & Waltz, D. L. </author> <year> (1992). </year> <title> Hybrid system for protein secondary structure prediction. </title> <journal> Journal of Molecular Biology, </journal> <volume> 225. </volume>
Reference-contexts: Furthermore, there is no guarantee of diversity. Hence, when classifiers from different learning algorithms are combined, they should be checked (e.g., by cross-validation) for accuracy and diversity, and some form of weighted combination should be used. This approach has been shown to be effective in some applications <ref> (e.g., Zhang, Mesirov, & Waltz, 1992) </ref>. 2.2 Methods for Combining Classifiers Given that we have trained an ensemble of classifiers, how should we combine their individual classification decisions? Many methods have been explored. They can be subdivided into unweighted vote, weighted vote, and gating networks.
Reference: <author> Zweben, M., Daun, B., & Deale, M. </author> <year> (1994). </year> <title> Scheduling and rescheduling with iterative repair. </title>
Reference: <editor> In Zweben, M., & Fox, M. S. (Eds.), </editor> <title> Intelligent Scheduling, </title> <journal> chap. </journal> <volume> 8, </volume> <pages> pp. 241-255. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address> <month> 64 </month>
References-found: 121

