URL: http://www.cs.princeton.edu/prism/papers-ps/ocean-locality.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Data Locality and Memory System Performance in the Parallel Simulation of Ocean Eddy Currents  
Author: Jaswinder Pal Singh and John L. Hennessy 
Address: Stanford, CA 94305  
Affiliation: Computer Systems Laboratory, Stanford University,  
Abstract: The regular and predictable data access patterns of many scientific applications make it possible to efficiently access memory in a shared memory multiprocessor. In this paper, we investigate these interactions for a complete scientific application that simulates eddy currents in an ocean basin. We show that the application affords data locality both within and across large computations that are distributed among processors, and that not exploiting this locality can lead to dismal performance in an application that is otherwise highly parallel and load balanced. Partitioning and scheduling for data locality can dramatically improve memory system performance without significantly compromising load balancing in this application. In this study, we first focus on one level of a machine's memory hierarchy: a hardware-coherent cache system. Three partitioning and scheduling schemes that preserve locality are examined, and their interactions with the cache system and organization analyzed. We show that simple computational kernels that access one or two data structures can be reasoned about quite effectively. However, higher level interactions and cache mapping collisions are very difficult to predict in an application with many large and frequently accessed data structures. In fact, we find that even the choice of the best partitioning scheme can be altered purely by the effect of mapping collisions. We also find that longer cache lines help the performance of this application for realistic relationships between problem and machine size, albeit to different extents for different partitioning schemes. We then show that the application can effectively utilize locality in physically distributed shared main memory as well, and present speedup results for different problem and machine size parameters. Finally, we comment on other issues involved in the scalable parallel performance of the application. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.P. Singh and J.L. Hennessy, </author> <title> "Finding and Exploiting Parallelism in an Ocean Simulation Program: Experience, Results and Implications," </title> <note> to appear in Journal of Parallel and Distributed Computing. Also Tech. Report No. </note> <institution> CSL-TR-89-388, Stanford University, </institution> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: 1 INTRODUCTION Numerically intensive scientific programs often afford a lot of parallelism. The process of creating an efficient parallel program can be thought of as comprising two phases: finding parallelism, and implementing it efficiently on an architecture. In a previous paper <ref> [1] </ref>, we described our experience obtaining an efficient parallelization of an ocean simulation program, written in FORTRAN, on a small-scale shared memory multiprocessor. <p> The time-dependent simulation is performed by repeatedly setting up and solving a set of (spatial) elliptic partial differential equations until the eddies and mean ocean flow attain a mutual balance (see <ref> [1] </ref> for details). The generic form of a spatial equation is ffi 2 ffix 2 + ffi 2 ffiy 2 2 = fl; (1) where is a streamfunction we are solving for, is a constant, and fl is the driving function of the equation. <p> is used to solve the equation system, with discretized rectangular grids representing horizontal cross-sections of the ocean basin. 2.1 The Program and the Parallelism Our experience with finding the appropriate parallelism in this application and implementing it for effective speedup on a small-scale bus-based shared memory multiprocessor is described in <ref> [1] </ref>. Every time-step in the program is structured in fully parallel phases separated by barrier synchronization of all processors. This parallel structure is depicted in Figure 1. Every box within a phase represents a task or computational kernel on an entire two-dimensional grid (or grids). <p> The parallelism across tasks is essentially used to reduce the number of barrier synchronizations required and perhaps average out idle times. After experimenting with some equation solvers, we settled upon a non-strict parallel variant of successively over-relaxed (SOR) Gauss-Seidel iteration <ref> [1] </ref>, that works well for our equations with the grid resolution we use. Processors begin computing their assigned grid points in an iteration simultaneously, violating the true SOR ordering at interpartition boundaries. <p> The only significant difference between the Symmetry results and those on the simulated multiprocessor is in the impact of our lack of consideration to 2 A column-based partitioning of grid tasks is used for simplicity and historical reasons <ref> [1] </ref>. 3 The self-relative speedup of a parallel program on a multiprocessor is defined as the ratio of the execution time of the parallel program on a single processor of the machine, to the execution time of the same program on several processors of the same machine. data locality. <p> Fortunately, the number of barriers is independent of the problem or machine size, and the absolute cost of a barrier scales at worst linearly with the number of processors (being independent of the problem size). Besides, as discussed in <ref> [1] </ref>, barriers can be replaced by more specific interprocessor synchronization between only adjacent processors in almost all cases in this application. 11 Summary We investigated the impact of data locality and alternative partitioning schemes on the multiprocessor cache and memory performance of a complete scientific application.
Reference: [2] <author> W.R. Holland, </author> <title> "The Role of Mesoscale Eddies in the General Circulation of the Ocean | Numerical Experiments using a Wind-Driven Quasi-Geostrophic Model," </title> <journal> Journal of Physical Oceanography, </journal> <volume> Vol. 8, </volume> <pages> pp. 363-392, </pages> <month> May </month> <year> 1978. </year>
Reference-contexts: Finally, Section 10 comments on some other issues relevant to the scalability of this application, and Section 11 summarizes the paper. 2 THE APPLICATION The application studies the role of mesoscale eddies and boundary currents in influencing ocean flow <ref> [2] </ref>. A cuboidal ocean basin is simulated, using a discretized quasi-geostrophic 1 circulation model. Wind stress from atmospheric effects provides the forcing function, and the effects of friction with the ocean walls are included.
Reference: [3] <author> R. Sweet. </author> <title> A Cyclic Reduction Method for the Solution of Block Tridiagonal Systems of Equations, </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 14, No. 4, </volume> <month> September </month> <year> 1977, </year> <pages> pp. 706-720. </pages>
Reference: [4] <editor> E.L. Lusk and R.A. Overbeek, </editor> <title> "Use of Monitors in FORTRAN: A Tutorial on the Barrier, Self-scheduling DO-Loop, and Askfor Monitors," </title> <type> Tech. Report No. ANL-84-51, Rev. 1, </type> <institution> Argonne National Laboratory, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Processors begin computing their assigned grid points in an iteration simultaneously, violating the true SOR ordering at interpartition boundaries. For this application, this does not cause a serious degradation in algorithm performance if the partitions are chosen judiciously. Parallelism is implemented by augmenting the FORTRAN program with parmacs macros <ref> [4] </ref>. 3 THE MULTIPROCESSORS USED We use a small-scale multiprocessor in the first set of results we present in this study, to demonstrate the difference in dynamically scheduled performance between it and a high-performance, scalable machine.
Reference: [5] <author> H. Davis, S. Goldschmidt and J.L. Hennessy, </author> <title> "Tango: a Multiprocessor Simulation and Tracing System," </title> <type> Tech. Report No. </type> <institution> CSL-TR-90-439, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: Since there are virtually no such machines available today, and since we are interested in tracking memory system performance, we use a multiprocessor simulator. There are two parts to this simulator: the Tango reference generator <ref> [5] </ref> which runs the application and produces a parallel memory reference stream, and a memory system simulator which processes these references and feeds timing information back to the reference generator.
Reference: [6] <author> P.H. Worley, </author> <title> "Information Requirements and the Implications for Parallel Computation," </title> <type> Tech. Report No. </type> <institution> STAN-CS-88-1212, Stanford University, </institution> <year> 1988. </year>
Reference-contexts: However, scaling the problem size likely implies refining the grid spacing and the tolerance of the spatial equation solver for more accuracy <ref> [6] </ref>. This will cause relatively more time to be spent in the solver, which exhibits the most frequent communication. Synchronization: Barrier synchronization can become a bottleneck if the problem size is kept fixed while adding processors.
References-found: 6

