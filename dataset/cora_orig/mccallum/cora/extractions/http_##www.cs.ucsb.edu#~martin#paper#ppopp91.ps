URL: http://www.cs.ucsb.edu/~martin/paper/ppopp91.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/paper/index.html
Root-URL: http://www.cs.ucsb.edu
Title: Coarse-Grain Parallel Programming in Jade  
Author: Monica S. Lam and Martin C. Rinard 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: This paper presents Jade, a language which allows a programmer to easily express dynamic coarse-grain parallelism. Starting with a sequential program, a programmer augments those sections of code to be parallelized with abstract data usage information. The compiler and run-time system use this information to concurrently execute the program while respecting the program's data dependence constraints. Using Jade can significantly reduce the time and effort required to develop and maintain a parallel version of an imperative application with serial semantics. The paper introduces the basic principles of the language, compares Jade with other existing languages, and presents the performance of a sparse matrix Cholesky factorization algorithm implemented in Jade. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. G. Babb II, L. Storc, and W. C. Ragsdale. </author> <title> A large-grain data flow scheduler for parallel processing on cyberplus. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: SCHEDULE allows programmers to give the system a set of tasks and an explicit specification of the task dependence graph [6]. SCHEDULE then executes the tasks while obeying the given dependence constraints. Programmers using coarse-grain dataflow languages such as LGDF <ref> [1] </ref> and TDFL [21] express concurrency and synchronization with dataflow graphs. Execution of the dataflow graph provides synchronized concurrency. A Strand programmer expresses his program's global con-currency structure in the committed-choice concurrent logic programming paradigm [8]. Suspension on unbound logic variables provides synchronization, while simultaneous goal satisfaction provides concurrency.
Reference: [2] <author> M. Berry and et al. </author> <title> The perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: This implementation runs on an Encore Multi-max and a Silicon Graphics IRIS 4D/240S. Implemented applications include a sparse Cholesky factorization algorithm due to Rothberg and Gupta [19], the Perfect Club benchmark MDG <ref> [2] </ref>, LocusRoute, a VLSI routing system due to Rose [18], a parallel Make program, and cyclic reduction, a column-oriented matrix algorithm. To illustrate Jade with a more realistic example, we now show how Rothberg and Gupta's sparse Cholesky factorization algorithm is implemented in Jade.
Reference: [3] <author> N. Carriero and D. Gelernter. </author> <title> Applications Experience with Linda. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Programming, </booktitle> <pages> pages 173-187, </pages> <address> New Haven, Conn., </address> <month> July </month> <year> 1988. </year>
Reference-contexts: As in the task queue model, programmers implementing applications with such general dependence constraints must directly encode the program's global synchronization pattern using the provided synchronization mechanisms as low-level primitives. For example, the Linda sparse Cholesky factorization application directly implements the task graph's synchronization pattern using counting semaphores <ref> [3] </ref>. 6.1.3 Global Control Languages Another approach is to use a control language to directly express an application's global concurrency pattern.
Reference: [4] <author> N. Carriero and D. Gelernter. </author> <title> How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 323-357, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: One major problem with this approach is that producers and consumers must agree on the order and relative time of data transfer. Linda supports a less tightly coupled programming style by providing a global tuple space with asynchronous operations to insert, read and remove data <ref> [4] </ref>. Tuple spaces support mutual exclusion and asynchronous producer/consumer synchronization based on the presence or absence of data. Tuple spaces also support some less frequently used synchronization mechanisms such as counting semaphores.
Reference: [5] <author> R. Chandra, A. Gupta, and J. L. Hennessy. </author> <title> COOL: </title>
Reference-contexts: For example, Multilisp futures enforce the producer/consumer sequence constraint between a function creating data and its caller consuming the return value [9]. Because this mechanism works well for synchronizing returns from asynchronously invoked functions or methods, concurrent object-oriented languages such as COOL <ref> [5] </ref> and ConcurrentSmalltalk [22] also provide the future synchronization mechanism. Futures, however, are not designed to synchronize the multiple updates to mutable shared data that are a central feature of object-oriented programming.
References-found: 5

