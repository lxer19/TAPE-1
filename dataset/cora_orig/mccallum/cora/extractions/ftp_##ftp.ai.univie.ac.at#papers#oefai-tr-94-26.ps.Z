URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-94-26.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+94-26
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: Pruning Methods for Rule Learning Algorithms  
Author: Johannes Furnkranz 
Address: Schottengasse 3 A-1010 Vienna Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: In this paper we will shortly review several pruning methods for relational learning algorithms and show how they are related to each other. We then report some experiments in several natural domains and try to analyse the performance of the algorithms in these domains in terms of run-time and accuracy. While some algorithms are clearly faster than others, no safe recommendation for achieving high accuracy can be given.
Abstract-found: 1
Intro-found: 1
Reference: [Breiman et al., 1984] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: The implementation of TDP made use of several optimizations, so that finding this theory is often cheaper than fitting the noise. A more detailed description of this process can be found in [Furnkranz, 1994c]. 1 This method is inspired by the approach taken in CART <ref> [Breiman et al., 1984] </ref> where the most general decision tree within this standard error margin is selected as a final theory. procedure TDP (Examples, SplitRatio) Cutoff = 1:0 BestProgram = ; BestAccuracy = 0:0 SplitExamples (SplitRatio, Examples, GrowingSet, PruningSet) repeat NewProgram = Fossil (GrowingSet,Cutoff) NewAccuracy = Accuracy (NewProgram,PruningSet) if NewAccuracy &gt;
Reference: [Brunk and Pazzani, 1991] <author> Clifford A. Brunk and Michael J. Pazzani. </author> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 389-393, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Lately several pruning methods for noise handling in relational rule learning algorithms have been proposed. The classic approaches to pruning are based on pre-pruning (Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994b]) and post-pruning (Reduced Error Pruning (REP) <ref> [Brunk and Pazzani, 1991] </ref> and Grow [Cohen, 1993]). More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) [Furnkranz, 1994c]) and integrate (Incremental Reduced Error Pruning (I-REP) [Furnkranz and Widmer, 1994]) these two basic methods. <p> length restriction, because the latter is dependent on the size of the training set, so that the size of the learned concepts (and thus the amount of overfitting) may increase with training set size [Furnkranz, 1994b]. 2.2 Post-Pruning Post-pruning was introduced to relational learning algorithms with Reduced Error Pruning (REP) <ref> [Brunk and Pazzani, 1991] </ref> based on previous work by [Quinlan, 1987] and [Pagallo and Haussler, 1990].
Reference: [Cameron-Jones and Quinlan, 1993] <author> R.M. Cameron-Jones and J.R. Quinlan. </author> <title> First order learning, zeroth order data. </title> <booktitle> In Proceedings of the 6th Australian Joint Conference on AI. World Scientific, </booktitle> <year> 1993. </year>
Reference-contexts: A side effect of using propositional data is that we can compare propositional and relational learning algorithms and confirm that the quality of the concepts learned by the latter are not below those learned by the former (see <ref> [Cameron-Jones and Quinlan, 1993] </ref> for more experiments along these lines). The Appendix of [Holte, 1993] gives a summary of the results achieved by various algorithms on some of the most commonly used data sets of the UCI repository and a short description of these sets.
Reference: [Cestnik et al., 1987] <author> Bojan Cestnik, Igor Kononenko, and Ivan Bratko. </author> <title> ASSISTANT 86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In Ivan Bratko and Nada Lavrac, editors, </editor> <booktitle> Progress in Machine Learning, </booktitle> <pages> pages 31-45, </pages> <address> Wilmslow, England, 1987. </address> <publisher> Sigma Press. </publisher>
Reference-contexts: This problem is known as overfitting the noise. Pruning is a standard way of dealing with the problem of overfitting (see e.g. [Mingers, 1989, Esposito et al., 1993]). There are two fundamentally different approaches <ref> [Cestnik et al., 1987] </ref>, Pre-Pruning and Post-Pruning. In sections 2.1 and 2.2 we will review classical pruning methods that have been adopted for relational concept learning systems.
Reference: [Clark and Niblett, 1989] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: It prevents overfitting the noise by learning only as long as the costs of encoding the current clause are less than the costs of encoding the tuples covered by it. * Significance Testing was first used in <ref> [Clark and Niblett, 1989] </ref> and later on in the ILP system mFoil [Dzeroski and Bratko, 1992].
Reference: [Cohen, 1993] <author> William W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 988-994, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Lately several pruning methods for noise handling in relational rule learning algorithms have been proposed. The classic approaches to pruning are based on pre-pruning (Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994b]) and post-pruning (Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] and Grow <ref> [Cohen, 1993] </ref>). More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) [Furnkranz, 1994c]) and integrate (Incremental Reduced Error Pruning (I-REP) [Furnkranz and Widmer, 1994]) these two basic methods. <p> The classic approaches to pruning are based on pre-pruning (Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994b]) and post-pruning (Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] and Grow <ref> [Cohen, 1993] </ref>). More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) [Furnkranz, 1994c]) and integrate (Incremental Reduced Error Pruning (I-REP) [Furnkranz and Widmer, 1994]) these two basic methods. We will present and discuss a variety of these pruning algorithms in section 2 and in particular show how they are related to each other. <p> The concept description that has been learned from the growing set is then simplified by greedily deleting conditions and rules from the theory until any further deletion would result in a decrease of predictive accuracy measured on the pruning set. However, this approach has several disadvantages, most notably efficiency. <ref> [Cohen, 1993] </ref> has shown that REP has a time complexity of (n 4 ) on purely random data. Therefore [Cohen, 1993] proposed Grow, a new pruning algorithm based on a technique used in the Grove learning system [Pagallo and Haussler, 1990]. <p> However, this approach has several disadvantages, most notably efficiency. <ref> [Cohen, 1993] </ref> has shown that REP has a time complexity of (n 4 ) on purely random data. Therefore [Cohen, 1993] proposed Grow, a new pruning algorithm based on a technique used in the Grove learning system [Pagallo and Haussler, 1990]. Like REP, Grow first finds a theory that over-fits the data. <p> A logical improvement would therefore be to limit the amount of overfitting by replacing the call to SeparateAndConquer with a call to PrePruning in the algorithm of figure 3. <ref> [Cohen, 1993] </ref> does this with the Grow algorithm using two weak MDL-based stopping criteria.
Reference: [Dzeroski and Bratko, 1992] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Lately several pruning methods for noise handling in relational rule learning algorithms have been proposed. The classic approaches to pruning are based on pre-pruning (Foil [Quinlan, 1990], mFoil <ref> [Dzeroski and Bratko, 1992] </ref>, or Fossil [Furnkranz, 1994b]) and post-pruning (Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] and Grow [Cohen, 1993]). <p> It prevents overfitting the noise by learning only as long as the costs of encoding the current clause are less than the costs of encoding the tuples covered by it. * Significance Testing was first used in [Clark and Niblett, 1989] and later on in the ILP system mFoil <ref> [Dzeroski and Bratko, 1992] </ref>. It tests for significant differences between the distribution of positive and negative examples covered by a rule and the overall distribution of positive and negative examples by comparing the likelihood ratio statistic to a 2 distribution with 1 degree of freedom at the desired significance level. <p> A value of Cutoff 0:3 has proved to be very robust with respect to varying noise levels and training set sizes [Furnkranz, 1994b]. mFoil's significance testing along with the m-estimate and a powerful beam search have been very successful in learning concepts in noisy domains <ref> [Dzeroski and Bratko, 1992] </ref>. Similar results have been obtained for the very efficient cutoff criterion.
Reference: [Esposito et al., 1993] <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> Decision tree pruning as a search in the state space. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 165-184, </pages> <address> Vienna, Austria, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This problem is known as overfitting the noise. Pruning is a standard way of dealing with the problem of overfitting (see e.g. <ref> [Mingers, 1989, Esposito et al., 1993] </ref>). There are two fundamentally different approaches [Cestnik et al., 1987], Pre-Pruning and Post-Pruning. In sections 2.1 and 2.2 we will review classical pruning methods that have been adopted for relational concept learning systems.
Reference: [Furnkranz and Widmer, 1994] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental Reduced Error Pruning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pages 70-77, </pages> <year> 1994. </year>
Reference-contexts: More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) [Furnkranz, 1994c]) and integrate (Incremental Reduced Error Pruning (I-REP) <ref> [Furnkranz and Widmer, 1994] </ref>) these two basic methods. We will present and discuss a variety of these pruning algorithms in section 2 and in particular show how they are related to each other. <p> LowerBound = BestAccuracy StandardError (BestAccuracy,PruningSet) Cutoff = MaximumPrunedCorrelation (NewProgram) until (NewAccuracy &lt; LowerBound) or (Cutoff = 0:0) loop NewProgram = SimplifyProgram (Program,PruningSet) if Accuracy (NewProgram,PruningSet) &lt; Accuracy (Program,PruningSet) exit loop Program = NewProgram return (Program) 2.4 Integrating Pre-and Post-Pruning There are several problems with pruning in relational concept learning (see <ref> [Furnkranz and Widmer, 1994] </ref>). Not all of them are attacked by the algorithms in the previous sections. In particular, the separate-and-conquer strategy (see figure 1) may cause problems. <p> The version of REP described in section 2.2 not only allows to prune any (instead of only the last) node, but also to prune the conditions of the rules associated with each node. Incremental Reduced Error Pruning (I-REP) <ref> [Furnkranz and Widmer, 1994] </ref> is an efficient solution to this problem by means of integrating pre-pruning and post-pruning: Each clause is learned until it covers no more negative examples. <p> The accuracy of the learned concepts was estimated on noise-free example sets of size 5000. Although these results seem to be confirmed by similar experiments in the natural domain of finite element mesh design <ref> [Furnkranz and Widmer, 1994, Furnkranz, 1994c] </ref>, we wanted to compare these algorithms on a variety of natural data sets in order to get more information about their applicability to real-world problems. <p> However, its strong bias for simple theories leads to over-generalization with decreasing noise levels. The clear superiority that I-REP exhibited in the relational domains of KRK endgames (figure 6) and finite element mesh design <ref> [Furnkranz and Widmer, 1994] </ref> could not be confirmed in propostional domains. Maybe the problem with rule interactions in the separate-and-conquer strategy (see section 2.4) is less detrimental in propostional domains.
Reference: [Furnkranz, 1994a] <author> Johannes Furnkranz. </author> <title> A comparison of pruning methods for relational concept learning. </title> <booktitle> In Proceedings of the AAAI-94 Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pages 371-382, </pages> <year> 1994. </year>
Reference-contexts: Thus the accuracy of a clause on the pruning set also serves as a stopping criterion, i.e. post-pruning methods are used as a pre-pruning heuristic. It can be easily seen that I-REP integrated pre-pruning (figure 2) and post-pruning (figure 3) into a single algorithm. 3 Experiments In previous research <ref> [Furnkranz, 1994a] </ref> we have compared a variety of these algorithms in the well-known noisy KRK endgame classification task [Muggleton et al., 1989] at several different training set sizes. The experiments showed that the post-pruning algorithms are very inefficient, while pre-pruning is fast, but less accurate.
Reference: [Furnkranz, 1994b] <author> Johannes Furnkranz. Fossil: </author> <title> A robust relational learner. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 122-137, </pages> <address> Catania, Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Lately several pruning methods for noise handling in relational rule learning algorithms have been proposed. The classic approaches to pruning are based on pre-pruning (Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], or Fossil <ref> [Furnkranz, 1994b] </ref>) and post-pruning (Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] and Grow [Cohen, 1993]). More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) [Furnkranz, 1994c]) and integrate (Incremental Reduced Error Pruning (I-REP) [Furnkranz and Widmer, 1994]) these two basic methods. <p> Insignificant rules are rejected. * Cutoff Stopping Criterion: This simple method used in Fossil <ref> [Furnkranz, 1994b] </ref> only allows to add conditions to a rule when their heuristic values are above a predefined threshold. Thus this cutoff parameter allows the user to directly control the amount of overfitting. <p> A value of Cutoff 0:3 has proved to be very robust with respect to varying noise levels and training set sizes <ref> [Furnkranz, 1994b] </ref>. mFoil's significance testing along with the m-estimate and a powerful beam search have been very successful in learning concepts in noisy domains [Dzeroski and Bratko, 1992]. Similar results have been obtained for the very efficient cutoff criterion. <p> Both have been shown to be superior to the encoding length restriction, because the latter is dependent on the size of the training set, so that the size of the learned concepts (and thus the amount of overfitting) may increase with training set size <ref> [Furnkranz, 1994b] </ref>. 2.2 Post-Pruning Post-pruning was introduced to relational learning algorithms with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on previous work by [Quinlan, 1987] and [Pagallo and Haussler, 1990].
Reference: [Furnkranz, 1994c] <author> Johannes Furnkranz. </author> <title> Top-down pruning in relational learning. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence, </booktitle> <pages> pages 453-457, </pages> <address> Amsterdam, The Netherlands, </address> <year> 1994. </year>
Reference-contexts: More recently approaches have been proposed that combine (MDL-Grow [Cohen, 1993] and Top Down Pruning (TDP) <ref> [Furnkranz, 1994c] </ref>) and integrate (Incremental Reduced Error Pruning (I-REP) [Furnkranz and Widmer, 1994]) these two basic methods. We will present and discuss a variety of these pruning algorithms in section 2 and in particular show how they are related to each other. <p> However, there is always the danger that a predefined stopping criterion will over-generalize the theory. To avoid this <ref> [Furnkranz, 1994c] </ref> have developed an algorithm called Top-Down Pruning (TDP) (see figure 4). <p> The implementation of TDP made use of several optimizations, so that finding this theory is often cheaper than fitting the noise. A more detailed description of this process can be found in <ref> [Furnkranz, 1994c] </ref>. 1 This method is inspired by the approach taken in CART [Breiman et al., 1984] where the most general decision tree within this standard error margin is selected as a final theory. procedure TDP (Examples, SplitRatio) Cutoff = 1:0 BestProgram = ; BestAccuracy = 0:0 SplitExamples (SplitRatio, Examples, GrowingSet, <p> The accuracy of the learned concepts was estimated on noise-free example sets of size 5000. Although these results seem to be confirmed by similar experiments in the natural domain of finite element mesh design <ref> [Furnkranz and Widmer, 1994, Furnkranz, 1994c] </ref>, we wanted to compare these algorithms on a variety of natural data sets in order to get more information about their applicability to real-world problems.
Reference: [Holte, 1993] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: The Appendix of <ref> [Holte, 1993] </ref> gives a summary of the results achieved by various algorithms on some of the most commonly used data sets of the UCI repository and a short description of these sets. We selected 9 of them for our experiments. <p> In the Lymphograpy data set we removed the 6 examples for the classes "normal find" and "fibrosis" in order to get a 2-class problem. All other data were used as described in <ref> [Holte, 1993] </ref>. For all data sets the task was to learn a definition for the minority class. All algorithms tested in this study were implemented by the author in Sicstus PROLOG and had major parts of their implementations in common. <p> Run-times for all datasets were measured in CPU seconds for SUN SPARCstations ELC except for the Mushroom and KRKPa7 datasets which are quite big and thus had to be run on a considerably faster SPARCstation S10. All experiments followed the setup used in <ref> [Holte, 1993] </ref>, i.e. the algorithms were trained on 2=3 of the data and tested on the remaining 1=3. However, only 10 runs were performed for each algorithm on each data set. The results can be found in tables 1, 2, and 3. <p> 0.00 | | Fossil 99.96 0.03 0.11 3538.19 No Pruning 100.00 0.01 0.04 1878.51 REP 99.97 0.05 0.15 1931.75 Grow 99.57 0.66 1.56 2088.81 No Pruning (TDP) 100.00 0.01 0.04 4595.23 TDP 99.97 0.05 0.15 4656.31 Table 3: Results of the Chess (KRKPa7), Lymphography, and Mushroom do mains. performed in <ref> [Holte, 1993] </ref> and are meant as an indicator of the performance of state-of-the-art decision tree learning algorithms on these data sets.
Reference: [Mingers, 1989] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: This problem is known as overfitting the noise. Pruning is a standard way of dealing with the problem of overfitting (see e.g. <ref> [Mingers, 1989, Esposito et al., 1993] </ref>). There are two fundamentally different approaches [Cestnik et al., 1987], Pre-Pruning and Post-Pruning. In sections 2.1 and 2.2 we will review classical pruning methods that have been adopted for relational concept learning systems.
Reference: [Mittenecker, 1977] <editor> Erich Mittenecker. Planung und statistische Auswertung von Experi-menten. Verlag Franz Deuticke, </editor> <address> Vienna, Austria, </address> <note> 8th edition, 1977. In German. </note>
Reference-contexts: Therefore we suspect that our grouping of the domain somewhat corresponds to the amount of noise contained in the data. 2 We have used a range test which can be used to quickly determine significant differences between medium values for small (N &lt; 20) sample sizes <ref> [Mittenecker, 1977] </ref>. For N = 10 the value of L = 1 2 R 1 +R 2 has to be &gt; 0:152 of a significance level of 5% and &gt; 0:210 for a significance level of 1%. ( i are medium values and R i are ranges.
Reference: [Muggleton et al., 1989] <author> Stephen Muggleton, Michael Bain, Jean Hayes-Michie, and Donald Michie. </author> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 113-118, </pages> <year> 1989. </year>
Reference-contexts: It can be easily seen that I-REP integrated pre-pruning (figure 2) and post-pruning (figure 3) into a single algorithm. 3 Experiments In previous research [Furnkranz, 1994a] we have compared a variety of these algorithms in the well-known noisy KRK endgame classification task <ref> [Muggleton et al., 1989] </ref> at several different training set sizes. The experiments showed that the post-pruning algorithms are very inefficient, while pre-pruning is fast, but less accurate. I-REP, the integration of pre- and post-pruning, seems to unite the advantages of both methods.
Reference: [Pagallo and Haussler, 1990] <author> Giulia Pagallo and David Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: training set, so that the size of the learned concepts (and thus the amount of overfitting) may increase with training set size [Furnkranz, 1994b]. 2.2 Post-Pruning Post-pruning was introduced to relational learning algorithms with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on previous work by [Quinlan, 1987] and <ref> [Pagallo and Haussler, 1990] </ref>. The basic idea is that in a first pass, no attention is paid to the noise in the data and a concept description that explains all of the positive and none of the negative examples is learned. <p> However, this approach has several disadvantages, most notably efficiency. [Cohen, 1993] has shown that REP has a time complexity of (n 4 ) on purely random data. Therefore [Cohen, 1993] proposed Grow, a new pruning algorithm based on a technique used in the Grove learning system <ref> [Pagallo and Haussler, 1990] </ref>. Like REP, Grow first finds a theory that over-fits the data. <p> Some of the issues discussed here might also be relevant for decision list learning algorithms such as those suggested in <ref> [Pagallo and Haussler, 1990] </ref>.
Reference: [Quinlan, 1987] <author> John Ross Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: size of the training set, so that the size of the learned concepts (and thus the amount of overfitting) may increase with training set size [Furnkranz, 1994b]. 2.2 Post-Pruning Post-pruning was introduced to relational learning algorithms with Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] based on previous work by <ref> [Quinlan, 1987] </ref> and [Pagallo and Haussler, 1990]. The basic idea is that in a first pass, no attention is paid to the noise in the data and a concept description that explains all of the positive and none of the negative examples is learned.
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Lately several pruning methods for noise handling in relational rule learning algorithms have been proposed. The classic approaches to pruning are based on pre-pruning (Foil <ref> [Quinlan, 1990] </ref>, mFoil [Dzeroski and Bratko, 1992], or Fossil [Furnkranz, 1994b]) and post-pruning (Reduced Error Pruning (REP) [Brunk and Pazzani, 1991] and Grow [Cohen, 1993]). <p> Note that if this clause is empty, no positive (and no negative) examples are covered and the outer loop will terminate as well. The most commonly used stopping criteria are * Encoding Length Restriction: This heuristic used in the classic ILP system Foil <ref> [Quinlan, 1990] </ref> is based on the Minimum Description Length principle [Rissanen, 1978].
Reference: [Quinlan, 1993] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The results can be found in tables 1, 2, and 3. Each line shows the average accuracy on the 10 sets, its standard deviation and range (difference between the maximum and the minimum accuracy encountered), and the run-time of the algorithm. The results of C4.5 <ref> [Quinlan, 1993] </ref> are taken from the experiments Breast Cancer Accuracy Stnd. Dev.
Reference: [Rissanen, 1978] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: The most commonly used stopping criteria are * Encoding Length Restriction: This heuristic used in the classic ILP system Foil [Quinlan, 1990] is based on the Minimum Description Length principle <ref> [Rissanen, 1978] </ref>. <p> One reason for Fossil's stability might be that it was the only algorithm that was able to learn from all of the training data. We are currently thinking of improving the other pruning algorithms by evaluating theories based on Rissanen's Minimum Description Length Principle <ref> [Rissanen, 1978] </ref> instead of separating part of the training data for estimating accuracy. This problem could also be avoided by using cross-validation. In particular for the efficient I-REP the additional computational costs caused by this method might still be bearable.
References-found: 21

