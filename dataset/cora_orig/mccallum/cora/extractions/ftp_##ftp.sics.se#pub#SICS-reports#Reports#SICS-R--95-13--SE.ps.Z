URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--95-13--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Title: Convergence and new operations in SDM new method for converging in the SDM memory, utilizing
Author: Gunnar Sjodin 
Keyword: Sparse Distributed Memory, SDM, Associative memory,Recurrent memory, Convergence, Separation of data.  
Note: A  is  
Address: Box 1263, S-164 28 Kista, Sweden  
Affiliation: RWCP 1 Neuro SICS 2 Laboratory  
Date: December 1995  
Web: introduced.  
Abstract: Report R95:13 ISRN : SICS-R--95/13-SE ISSN : 0283-3638 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gunnar Sjodin. </author> <title> Improving the Capacity of SDM, </title> <note> 1995. SICS Research Report, to appear. </note>
Reference-contexts: The memory is addressed within a binary address space of dimension N, where M t 2 N . A picture of the general structure is given in figure 1. For relevant literature on SDM cf. [3], [4], [5], [2], [7], <ref> [1] </ref>. When a U -dimensional binary vector W is stored "at" an address X an activation mechanism activates a set of the locations and W is added to the contents of these after first transforming 0:s to 1:s. <p> This reading procedure can be considerably improved, as is demonstrated in Sjodin <ref> [1] </ref>. The involved methods are independent of the activation mechanism. A major concern of this paper is the problem of converging. <p> -4 9 3 C M1;u -45 X ADRESS REGISTER W WORD-IN REGISTER Some method for reading based on the contents of the activated locations Z WORD-OUT REGISTER Some activation mechanism ? ? @ @ @ A A A A A A 5 1.2 Notations The notations follow closely those of <ref> [1] </ref>. An address is a binary vector of 0:s and 1:s. Its indices are called coordinates. A datum is a vector of -1:s and 1:s. Its indices are called positions. A mask is a subset of the coordinates in the address vectors. <p> locations activated by X H = the number of locations in H This is actually a fixed size parameter in the Karlsson model h = the locations activated by both X and X t 0 h = the number of locations in h h Q = the, by methods in <ref> [1] </ref>, estimated value of h 1.2.2 The parameter values for cerebellum Recall that in the SDM-interpretation of the cerebellum M = the number of locations = 10 1011 = number of granule cells N = the length of an address = 3 10 7 = number of mossy fibers = M <p> 10 7 = number of Purkinje cells = M 0:6 M 0:55 or M 0:74 M 0:67 respectively 7 2 Good reading and convergence For the "good" reading the procedure is: 1 Use the address to activate the locations H. 2 Find the locations h as in 6 in Sjodin <ref> [1] </ref>. 3 Use only these locations to read the data with some method from 2 in [1]. The general procedure for convergence towards the correct address is: 1 Use the address to activate the locations H. 2 Find the locations h as in as in 6 in [1]. 3 If the <p> 0:67 respectively 7 2 Good reading and convergence For the "good" reading the procedure is: 1 Use the address to activate the locations H. 2 Find the locations h as in 6 in Sjodin <ref> [1] </ref>. 3 Use only these locations to read the data with some method from 2 in [1]. The general procedure for convergence towards the correct address is: 1 Use the address to activate the locations H. 2 Find the locations h as in as in 6 in [1]. 3 If the "stopping criterion" (cf. section 2.1) is satisfied, use h to read the data with some method <p> 6 in Sjodin <ref> [1] </ref>. 3 Use only these locations to read the data with some method from 2 in [1]. The general procedure for convergence towards the correct address is: 1 Use the address to activate the locations H. 2 Find the locations h as in as in 6 in [1]. 3 If the "stopping criterion" (cf. section 2.1) is satisfied, use h to read the data with some method from 2 in [1]. <p> for convergence towards the correct address is: 1 Use the address to activate the locations H. 2 Find the locations h as in as in 6 in <ref> [1] </ref>. 3 If the "stopping criterion" (cf. section 2.1) is satisfied, use h to read the data with some method from 2 in [1]. <p> Below, we will describe these principles in more detail. 2.3 Masking out the faulty part of the address In the following, for a set of locations A <ref> [1; M ] </ref>, let coord (A) = [ mask (m) [1; N ] Now, assume that m 2 H h Then at least one of the coordinates in mask (m) must have different bit-values in X and X t 0 . <p> Below, we will describe these principles in more detail. 2.3 Masking out the faulty part of the address In the following, for a set of locations A [1; M ], let coord (A) = [ mask (m) <ref> [1; N ] </ref> Now, assume that m 2 H h Then at least one of the coordinates in mask (m) must have different bit-values in X and X t 0 . <p> Or, in other words, is the number of misses of the bits, in the part 9 of the addresses that coincide. Then, if &lt; d, flipping all the X-bits with coordinates in <ref> [1; N ] </ref> coord (h) will bring X closer to X t 0 . Of course, we will not know whether the inequality is satisfied or not. However, as we will see, given h Q it is possible to state this with a certain confidence. <p> One should really only flip the relevant bits with coordinates in coord (H). However, in the Karlsson design the set coord (H) is independent of X and thus only this set is relevant anyway. In fact, it may be arranged that this constant set equals <ref> [1; N ] </ref>. Let us make this a little more precise. Let " = d N , i.e. the proportional error in the address. Now, replace the expected number of common locations h, given c H = H, by h. <p> Let us make this a little more precise. Let " = d N , i.e. the proportional error in the address. Now, replace the expected number of common locations h, given c H = H, by h. Then we get the following rough estimate (cf. equation 37 in <ref> [1] </ref>) and hence " K H (4) Given b h = h we get that the expected value of is = (N d)(1 N d (N d)e Kh and hence &lt; d is roughly transformed to (1 ") log ( " KH (7) when d &gt; 0. <p> In the cerebellum, interpreted as an SDM (cf. section 1.2.2), the memory is not fully connected, the K:s are small and N = M 0:7 . The investigation of how the convergence method and other methods in this paper and in Sjodin <ref> [1] </ref> should be made to work for the "thin" SDM-structure of cerebellum is an interesting issue. 11 3 New operations on the memory 3.1 Avoiding multiple storings When storing: We can figure out whether we are "intruding" upon another storage area by calculating the "h Q " as in Sjodin [1]. <p> <ref> [1] </ref> should be made to work for the "thin" SDM-structure of cerebellum is an interesting issue. 11 3 New operations on the memory 3.1 Avoiding multiple storings When storing: We can figure out whether we are "intruding" upon another storage area by calculating the "h Q " as in Sjodin [1]. Set limits "lim1" and "lim2" and proceed as follows: If, case 1: h Q &lt; lim1 then store in the usual way. If, case 2: lim1 h Q &lt; lim2 then change the address and store in this new address (possibly recursively). <p> This can of course be done recursively and also result in a number of choices for the read data. These procedures spreads the stored data and thus gives the memory better storing capacity. It furthermore gets rid of the problems with multiply stored data (cf. section 5 in <ref> [1] </ref>). 3.2 (Partially) erasing a stored value Given a set of locations A [1; M ], we may use these to read the memory to obtain a datum f W and then "erase" this by adding f W to the data in in the A-locations. <p> These procedures spreads the stored data and thus gives the memory better storing capacity. It furthermore gets rid of the problems with multiply stored data (cf. section 5 in [1]). 3.2 (Partially) erasing a stored value Given a set of locations A <ref> [1; M ] </ref>, we may use these to read the memory to obtain a datum f W and then "erase" this by adding f W to the data in in the A-locations. Note that we do not set the values at these locations to zero. <p> One reason for a state to be a spurious state could be that it is a double/multiple reading. If this is the case it could be possible to sort out the 13 different addresses by the methods described in this paper (cf. section 5 in <ref> [1] </ref>).
Reference: [2] <author> L. A. Jaeckel. </author> <title> An alternative Design for a Sparse Distributed Memory. </title> <type> Technical Report TR-89.28, </type> <institution> RIACS, </institution> <year> 1989. </year>
Reference-contexts: The memory is addressed within a binary address space of dimension N, where M t 2 N . A picture of the general structure is given in figure 1. For relevant literature on SDM cf. [3], [4], [5], <ref> [2] </ref>, [7], [1]. When a U -dimensional binary vector W is stored "at" an address X an activation mechanism activates a set of the locations and W is added to the contents of these after first transforming 0:s to 1:s.
Reference: [3] <author> Pentti Kanerva. </author> <title> Sparse Distributed Memory. </title> <publisher> The MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: The memory is addressed within a binary address space of dimension N, where M t 2 N . A picture of the general structure is given in figure 1. For relevant literature on SDM cf. <ref> [3] </ref>, [4], [5], [2], [7], [1]. When a U -dimensional binary vector W is stored "at" an address X an activation mechanism activates a set of the locations and W is added to the contents of these after first transforming 0:s to 1:s.
Reference: [4] <author> Pentti Kanerva. </author> <title> Sparse distributed memory and related models. </title> <editor> In Mo-hamad H. Hassoun, editor, </editor> <title> Associative Neural Memories, </title> <booktitle> chapter 3, </booktitle> <pages> pages 50-76. </pages> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: The memory is addressed within a binary address space of dimension N, where M t 2 N . A picture of the general structure is given in figure 1. For relevant literature on SDM cf. [3], <ref> [4] </ref>, [5], [2], [7], [1]. When a U -dimensional binary vector W is stored "at" an address X an activation mechanism activates a set of the locations and W is added to the contents of these after first transforming 0:s to 1:s. <p> Usually we will assume that the activation mechanism 6 is such that = (2M 2 ) 1 (cf. e.g. <ref> [4] </ref> and [6]) In particular, in the Jaeckel/Karlsson model, we will have 2 K = = (2M 2 ) 1 X t 0 = the address used at "time t 0 " X = the address close to X t 0 at which we read the memory H = the locations
Reference: [5] <author> Roland Karlsson. </author> <title> A Fast Activation Mechanism for the Kanerva SDM Memory, 1995. </title> <institution> SICS Research Report R95:10, Swedish Institute of Computer Science. </institution>
Reference-contexts: The memory is addressed within a binary address space of dimension N, where M t 2 N . A picture of the general structure is given in figure 1. For relevant literature on SDM cf. [3], [4], <ref> [5] </ref>, [2], [7], [1]. When a U -dimensional binary vector W is stored "at" an address X an activation mechanism activates a set of the locations and W is added to the contents of these after first transforming 0:s to 1:s.
Reference: [6] <author> Jan Kristoferson. </author> <title> Best Probabilities of Activation and Performance Comparisons for Several Designs of Kanerva's SDM (Sparse Distributed Memory), 1995. </title> <institution> SICS Research Report R95:09, Swedish Institute of Computer Science. </institution>
Reference-contexts: Usually we will assume that the activation mechanism 6 is such that = (2M 2 ) 1 (cf. e.g. [4] and <ref> [6] </ref>) In particular, in the Jaeckel/Karlsson model, we will have 2 K = = (2M 2 ) 1 X t 0 = the address used at "time t 0 " X = the address close to X t 0 at which we read the memory H = the locations activated by
Reference: [7] <author> Jan Kristoferson. </author> <title> Some comments on the Information Stored in Sparse Distributed Memory, </title> <note> 1995. SICS Research Report, to appear. 15 </note>
Reference-contexts: The memory is addressed within a binary address space of dimension N, where M t 2 N . A picture of the general structure is given in figure 1. For relevant literature on SDM cf. [3], [4], [5], [2], <ref> [7] </ref>, [1]. When a U -dimensional binary vector W is stored "at" an address X an activation mechanism activates a set of the locations and W is added to the contents of these after first transforming 0:s to 1:s.
References-found: 7

