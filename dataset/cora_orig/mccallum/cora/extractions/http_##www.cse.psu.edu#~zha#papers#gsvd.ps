URL: http://www.cse.psu.edu/~zha/papers/gsvd.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: COMPUTING THE GENERALIZED SINGULAR VALUES/VECTORS OF LARGE SPARSE OR STRUCTURED MATRIX PAIRS  
Author: HONGYUAN ZHA 
Keyword: Key words. generalized singular value decomposition, CS decomposition, Lanczos process, bidiagonal matrix, linear least squares problem  
Note: AMS(MOS) subject classifications. primary 65F15  
Abstract: We present a numerical algorithm for computing a few extreme generalized singular values and corresponding vectors of a sparse or structured matrix pair fA; Bg. The algorithm is based on the CS decomposition and the Lanczos bidiagonalization process. At each iteration step of the Lanczos process, the solution to a linear least squares problem with (A T ; B T ) T as the coefficient matrix is approximately computed, and this consists the only interface of the algorithm with the matrix pair fA; Bg. Numerical results are also given to demonstrate the feasibility and efficiency of the algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Z. Bai and H. Zha. </author> <title> A new preprocessing algorithm for computing the generalized singular value decomposition. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 1007-1012, </pages> <year> 1993. </year>
Reference-contexts: 2 HONGYUAN ZHA For a matrix pair fA; Bg where A and B are dense, there are basically two classes of algorithms for computing the GSVD: one is based on the CS decomposition which will be detailed in Section 2 [24, 29]; the other is based on implicit Jacobi rotations <ref> [14, 1] </ref>. These algorithms are geared to the computation of the complete decomposition and they do not take into account of the sparsity or structure of the matrix pair fA; Bg. <p> ); c i = (N i + 1)=2N; S = p We also choose another diagonal matrix D D = diag (d i ); d (25 (i 1) + 1 : 25i) = i + rand (1); i = 1; 2; 3; 4; where rand (1) is uniformly distributed on <ref> [0; 1] </ref>. A typical distribution of the eigen-values of D is depicted in Figure 1. Then set A = CD; B = SD: The size of the condition number of Z is of order O (1). <p> The convergence history of singular values (left) and levels of orthogonality (right), tol N = 25 D = diag (d i ); d (25 (i 1) + 1 : 25i) = i + 0:1rand (1); i = 1; 2; 3; 4; where rand (1) is uniformly distributed on <ref> [0; 1] </ref>.Then set A = CD; B = SD: We choose the block size p = 3. We first choose tol N = 40 and the residual res:1 O (10 15 ). Semi-orthogonality is maintained among the block Lanczos vectors.
Reference: [2] <author> A. Bjorck. </author> <title> Least squares methods. </title> <type> Working paper, </type> <institution> Department of Mathematics, Linkoping University, </institution> <year> 1987. </year> <title> Also in Handbook of Numerical Analysis, V.1: Solution of Equations in R n , P. </title> <editor> G. Ciarlet and J. L. Lions editors, </editor> <publisher> Elsevier, North Holland, </publisher> <year> 1991. </year>
Reference-contexts: solving the following least squares problem: 4 min fl fl B x fl fl = fl fl ~u j A fl fl 2 ; then QQ T ~u j = B x j :(7) There are various ways of computing the least squares solution, a detailed account is given in <ref> [2] </ref>. In some of the applications, the matrix B might not be readily available, instead it is implicitly given in G = B T B. We can compute the Cholesky factorization of G which might destroy the sparsity or structure of G. <p> First we investigate the effect of the inaccuracy of computing the projection QQ T ~u j on the convergence rate and accuracy of the computed generalized singular values and corresponding vectors. The least squares problems are solved using LSCG <ref> [2] </ref>, and the Lanczos bidiagonalization is run 50 steps. <p> Then set A = CD; B = SD: The size of the condition number of Z is of order O (1). The linear least squares problem with coefficient matrix Z = (A T ; B T ) T is solved using the LSCG method given in <ref> [2] </ref>.
Reference: [3] <author> R. Chan, J. Nagy, and B. Plemmons. </author> <title> Circulant preconditioned toeplitz least squares iteration. </title> <type> Technical Report 957, IMA, </type> <institution> University of Minnesota, </institution> <year> 1992. </year>
Reference-contexts: B = toeplitz (c 2 ; r 2 ). The least squares problems are solved using the method in <ref> [3] </ref>, where T. Chan's optimal circulant preconditioner is COMPUTING THE GENERALIZED SVD 15 used.
Reference: [4] <author> J. Cullum, R. A. Willoughby, and M. </author> <title> Lake. A Lanczos algorithm for computing singular values and vectors of large matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4 </volume> <pages> 197-215, </pages> <year> 1983. </year>
Reference-contexts: Approximations of the extreme singular values of Q 1 can be obtained by comput ing the singular values of the bidiagonal matrix J k = B B @ . . . . . . fi k1 1 C C for some suitably chosen k <ref> [18, 6, 4] </ref>. To obtain the generalized singular vectors, we proceed as follows. Let v be an approximation of a right singular vector of Q 1 . It takes the form ~v = Qv in the Modified Lanczos Algorithm. <p> Numerical Examples and Concluding Remarks. In this section, we present some preliminary numerical results for the Modified Lanczos Algorithm. Detailed analyses and numerical experiments on the Lanczos bidiagonalization process for computing the SVD have been presented in <ref> [12, 6, 4] </ref>. Generally speaking, the computational costs of the bidiagonalization process applying to a matrix A depends on, among other things, 1) costs of matrix-vector multiplication Au and A T v; 2) the relative gaps of the desired singular values, i.e., the distribution of the singular values of A.
Reference: [5] <author> J. Demmel and K. Veselic. </author> <title> Jacobi's method is more accurate than QR. </title> <type> Technical Report 468, </type> <institution> Computer Science Department, </institution> <address> New York University, </address> <year> 1989. </year>
Reference-contexts: A. Some Perturbation Bounds. In this appendix, we discuss the relation between the GSVD and the set of canonical angles between two subspaces, then we rederive and strengthen some perturbation bounds for the generalized singular values in <ref> [26, 13, 5] </ref>. Again, we assume that Z = (A T ; B T ) T is of full column rank. In [8], it is proved the following version of the CS decomposition (cf. [23, 24]). Theorem A.1. <p> Then maxfk A ~ A k 2 ; k B ~ B k 2 g * p Denote the generalized singular values of fA; Bg by oe i = tan i . Using Lemma 2.1 in <ref> [5] </ref> and the matrix eigenvalue problems in Section 2, we have the following results concerning the relative perturbation bounds. Theorem A.3. Let Z 2 R (m+p)fin be of full column rank. <p> Therefore fA; Bg and f ~ A; ~ Bg have the same numbers of zero and infinite generalized singular values. The rest of the results follows from kZxk 2 maxfj A ; j B gkZxk 2 and Lemma 2.15 in <ref> [5] </ref>. 2 How to derive componentwise perturbation bounds for the generalized singular vectors remains to be done.
Reference: [6] <author> G. H. Golub, F.T. Luk, </author> <title> and M.L. Overton. A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 </volume> <pages> 149-169, </pages> <year> 1981. </year>
Reference-contexts: Approximations of the extreme singular values of Q 1 can be obtained by comput ing the singular values of the bidiagonal matrix J k = B B @ . . . . . . fi k1 1 C C for some suitably chosen k <ref> [18, 6, 4] </ref>. To obtain the generalized singular vectors, we proceed as follows. Let v be an approximation of a right singular vector of Q 1 . It takes the form ~v = Qv in the Modified Lanczos Algorithm. <p> Numerical Examples and Concluding Remarks. In this section, we present some preliminary numerical results for the Modified Lanczos Algorithm. Detailed analyses and numerical experiments on the Lanczos bidiagonalization process for computing the SVD have been presented in <ref> [12, 6, 4] </ref>. Generally speaking, the computational costs of the bidiagonalization process applying to a matrix A depends on, among other things, 1) costs of matrix-vector multiplication Au and A T v; 2) the relative gaps of the desired singular values, i.e., the distribution of the singular values of A.
Reference: [7] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: The above connections also suggest the possibility of solving a generalized eigenvalue problem with two nonnegative matrices by applying the GSVD to their Cholesky factors. This idea was used in <ref> [7, 22, p. 473] </ref> and further pursued in [31, 10]. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces [23, 7, 8]. <p> This idea was used in [7, 22, p. 473] and further pursued in [31, 10]. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces <ref> [23, 7, 8] </ref>. <p> Although we do not want to explicitly compute Q 1 or Q 2 , let us first just apply the bidiagonalization process to Q 1 and later on we will see how to avoid this explicit computation. Using the standard formulation, we have the following algorithm <ref> [7, Chapter 9] </ref>.
Reference: [8] <author> G. H. Golub and H. Zha. </author> <title> The canonical correlations and their numerical computation. </title> <editor> In A. Bojanczky and G. Cybenko, editors, </editor> <title> Linear Algebra for Signa Processing, </title> <address> Berlin, </address> <year> 1992. </year> <note> Springer-Verlag. to appear. </note>
Reference-contexts: At first glance, it seems that we need to explicitly compute Q in order to apply the Lanczos process. However, we will show that by a change of basis, a trick used in <ref> [8, 32] </ref>, the explicit computation can be avoided. In the remaining part of this section, we make some remarks on the relations between the GSVD and some other useful decompositions. <p> This idea was used in [7, 22, p. 473] and further pursued in [31, 10]. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces <ref> [23, 7, 8] </ref>. <p> Again, we assume that Z = (A T ; B T ) T is of full column rank. In <ref> [8] </ref>, it is proved the following version of the CS decomposition (cf. [23, 24]). Theorem A.1. <p> This condition number is obviously independent of the column scaling of A, i.e., S (AD) = S (A) for any positive definite diagonal matrix D. Using Theorem 3.11 in <ref> [8] </ref>, we have Theorem A.2. Let Z 2 R (m+p)fin be of full column rank, ~ Z = Z + Z with jZj *G Z jZj be such that ~ Z is also of full column rank, where G Z is a matrix with nonnegative elements.
Reference: [9] <author> R.G. Grimes, J.G. Lewis, and H.D. Simon. </author> <title> A shifted block Lanczos algorithm for solving sparse symmetric generalized eigenproblems. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 15 </volume> <pages> 228-272, </pages> <year> 1994. </year>
Reference-contexts: A block version can handle the case of multiple eigenvalues or singular values as long as the multiplicity does not exceed the block size. To keep Lanczos algorithm from making spurious copies of converged eigen-values or singular values, we use the partial re-orthogonalization scheme discussed in <ref> [19, 20, 9] </ref>. <p> The level of orthogonality of the block Lanczos vectors is monitored by the following recurrence that simulates the loss of orthogonality of the computed Lanczos vectors <ref> [20, 9] </ref>. 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : ! j+1;j1 = ~ fi j+1 (2fi j * s + (ff j + ff j1 )* s + fi j1 ! j;j2 + j+1;j1 +fi j ! j1;k + (ff j + ff k )! j;k + j+1;k (12)
Reference: [10] <author> L. Kaufman. </author> <title> An algorithm for the banded symmetric generalized matrix eigenvalue problem. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 14 </volume> <pages> 372-389, </pages> <year> 1993. </year>
Reference-contexts: The above connections also suggest the possibility of solving a generalized eigenvalue problem with two nonnegative matrices by applying the GSVD to their Cholesky factors. This idea was used in [7, 22, p. 473] and further pursued in <ref> [31, 10] </ref>. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces [23, 7, 8].
Reference: [11] <author> B. De Moor and H. Zha. </author> <title> A tree of generalizations of the ordinary singular value decomposition. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 147 </volume> <pages> 469-500, </pages> <year> 1991. </year>
Reference-contexts: It is a generalization of the familiar singular value decomposition (SVD) to the case of matrix pairs. (For other generalizations and a unified treatment, the reader is referred to <ref> [11] </ref>.) The GSVD was first proposed by Van Loan [28] under the name B-SVD, and later on, Paige and Saunders provided improved formulation more suitable for numerical computations and used the designation GSVD [15]. There are various ways of introducing and deriving the GSVD.
Reference: [12] <author> C. C. Paige. </author> <title> Bidiagonalization of matrices and solution of linear systems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 11 </volume> <pages> 197-209, </pages> <year> 1974. </year>
Reference-contexts: Numerical Examples and Concluding Remarks. In this section, we present some preliminary numerical results for the Modified Lanczos Algorithm. Detailed analyses and numerical experiments on the Lanczos bidiagonalization process for computing the SVD have been presented in <ref> [12, 6, 4] </ref>. Generally speaking, the computational costs of the bidiagonalization process applying to a matrix A depends on, among other things, 1) costs of matrix-vector multiplication Au and A T v; 2) the relative gaps of the desired singular values, i.e., the distribution of the singular values of A.
Reference: [13] <author> C. C. Paige. </author> <title> A note on a result of Sun Ji-guang: Sensitivity of the cs and gsv decomposition. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 21 </volume> <pages> 186-191, </pages> <year> 1984. </year> <title> COMPUTING THE GENERALIZED SVD 21 </title>
Reference-contexts: A. Some Perturbation Bounds. In this appendix, we discuss the relation between the GSVD and the set of canonical angles between two subspaces, then we rederive and strengthen some perturbation bounds for the generalized singular values in <ref> [26, 13, 5] </ref>. Again, we assume that Z = (A T ; B T ) T is of full column rank. In [8], it is proved the following version of the CS decomposition (cf. [23, 24]). Theorem A.1.
Reference: [14] <author> C. C. Paige. </author> <title> Computing the generalized singular value decomposition. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 1126-1146, </pages> <year> 1986. </year>
Reference-contexts: 2 HONGYUAN ZHA For a matrix pair fA; Bg where A and B are dense, there are basically two classes of algorithms for computing the GSVD: one is based on the CS decomposition which will be detailed in Section 2 [24, 29]; the other is based on implicit Jacobi rotations <ref> [14, 1] </ref>. These algorithms are geared to the computation of the complete decomposition and they do not take into account of the sparsity or structure of the matrix pair fA; Bg. <p> y , where B y is the Moore-Penrose inverse of B, coincide with the generalized singular values of fA; Bg if B is of full column rank. 3 If B is not of full column rank, this might not be the case, an important fact that was already noted in <ref> [14] </ref>. For numerical computations, resorting to the SVD of AB y is not recommanded especially when B is close to column rank-deficient as is pointed in [14]. <p> rank. 3 If B is not of full column rank, this might not be the case, an important fact that was already noted in <ref> [14] </ref>. For numerical computations, resorting to the SVD of AB y is not recommanded especially when B is close to column rank-deficient as is pointed in [14]. The above connections also suggest the possibility of solving a generalized eigenvalue problem with two nonnegative matrices by applying the GSVD to their Cholesky factors. This idea was used in [7, 22, p. 473] and further pursued in [31, 10]. <p> We then apply the Lanczos bidiagonalization process to AB y . In the singular case, fA; Bg has a infinite generalized singular value while all the singular values of AB y are finite. This problem was also pointed out in <ref> [22, 14, 30] </ref>. Example. 5. The construction of the matrix pair fA; Bg is the same as in Example 4 with t = 12, i.e., the condition number of Z 10 12 .
Reference: [15] <author> C. C. Paige and M. A. Saunders. </author> <title> Toward a generalized singular value decomposition. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 18 </volume> <pages> 398-405, </pages> <year> 1981. </year>
Reference-contexts: the case of matrix pairs. (For other generalizations and a unified treatment, the reader is referred to [11].) The GSVD was first proposed by Van Loan [28] under the name B-SVD, and later on, Paige and Saunders provided improved formulation more suitable for numerical computations and used the designation GSVD <ref> [15] </ref>. There are various ways of introducing and deriving the GSVD. We find the following one through the SVD of a product of matrices more natural and illuminating. It proceeds as follows. <p> The GSVD in its most general form (cf. Theorem 2.1 in the next section) provides the right decomposition that deals with these situations in an elegant and unified fashion <ref> [15] </ref>. fl Department of Computer Science and Engineering, The Pennsylvania State University, University Park, PA 16802, zha@cse.psu.edu. <p> We consider the case where in the matrix pair fA; Bg, A and B have the same number of columns, a dual decomposition also exits for A and B having the same number of rows. Theorem 2.1 (Paige & Saunders <ref> [15] </ref>).
Reference: [16] <author> C. C. Paige and M. A. Saunders. </author> <title> LSQR: an algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Transaction on Mathematical Software, </journal> <volume> 8 </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: Comparison with the "exact" method: singular value (left), singular vector (right) where f; g; h 2 R 100 are random unit vectors, then A = U CDW; B = V SDW: Similar test matrices were also used in <ref> [16] </ref>. Figure 5 shows the convergence history. For ill-conditioned Z the convergence curve for LSCG is rather erratic. Instead of using tol N , we use tol, the amount of the reduction in the residual.
Reference: [17] <author> B. N. Parlett and D. Scott. </author> <title> The Lanczos algorithm with selective orthogonalization. </title> <journal> Mathematics of Computation, </journal> <volume> 22 </volume> <pages> 217-238, </pages> <year> 1979. </year>
Reference-contexts: In our applications where several of the singular values are needed, this will make it very difficult to distinguish between a multiple singular value and a copy. Fortunately, this problem has been addressed in the literature with several re-orthogonalization schemes proposed <ref> [17, 18, 19] </ref>. In the following discussion, we will ignore this problem altogether and instead concentrate on those that are particular to our problem.
Reference: [18] <author> B. N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1980. </year>
Reference-contexts: In our applications where several of the singular values are needed, this will make it very difficult to distinguish between a multiple singular value and a copy. Fortunately, this problem has been addressed in the literature with several re-orthogonalization schemes proposed <ref> [17, 18, 19] </ref>. In the following discussion, we will ignore this problem altogether and instead concentrate on those that are particular to our problem. <p> COMPUTING THE GENERALIZED SVD 5 a future work. Therefore we will only use the simple Lanczos process without any re-orthogonalization in the following discussions with the understanding that a re-orthogonalization scheme such as selective or partial re-orthogonalization <ref> [18, 19, 20] </ref> should be incorporated in an actual implementation. However, some preliminary test results obtained using a block version of the symmetric Lanczos algorithm together with the partial re-orthogonalization will be presented in Example 7 in Section 5. Now we are ready to derive our modified algorithm. <p> Approximations of the extreme singular values of Q 1 can be obtained by comput ing the singular values of the bidiagonal matrix J k = B B @ . . . . . . fi k1 1 C C for some suitably chosen k <ref> [18, 6, 4] </ref>. To obtain the generalized singular vectors, we proceed as follows. Let v be an approximation of a right singular vector of Q 1 . It takes the form ~v = Qv in the Modified Lanczos Algorithm. <p> Notice that e T j w i is the last ele ment of w (j) j w i j can be used in the stopping criterion for the Lanczos bidiagonalization process <ref> [18] </ref>. 5. Numerical Examples and Concluding Remarks. In this section, we present some preliminary numerical results for the Modified Lanczos Algorithm. Detailed analyses and numerical experiments on the Lanczos bidiagonalization process for computing the SVD have been presented in [12, 6, 4].
Reference: [19] <author> H. D. Simon. </author> <title> Analysis of the symmetric Lanczos algorithm with reorthogonalization methods. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 61 </volume> <pages> 103-131, </pages> <year> 1984. </year>
Reference-contexts: In our applications where several of the singular values are needed, this will make it very difficult to distinguish between a multiple singular value and a copy. Fortunately, this problem has been addressed in the literature with several re-orthogonalization schemes proposed <ref> [17, 18, 19] </ref>. In the following discussion, we will ignore this problem altogether and instead concentrate on those that are particular to our problem. <p> COMPUTING THE GENERALIZED SVD 5 a future work. Therefore we will only use the simple Lanczos process without any re-orthogonalization in the following discussions with the understanding that a re-orthogonalization scheme such as selective or partial re-orthogonalization <ref> [18, 19, 20] </ref> should be incorporated in an actual implementation. However, some preliminary test results obtained using a block version of the symmetric Lanczos algorithm together with the partial re-orthogonalization will be presented in Example 7 in Section 5. Now we are ready to derive our modified algorithm. <p> A block version can handle the case of multiple eigenvalues or singular values as long as the multiplicity does not exceed the block size. To keep Lanczos algorithm from making spurious copies of converged eigen-values or singular values, we use the partial re-orthogonalization scheme discussed in <ref> [19, 20, 9] </ref>. <p> Once the desired level of orthogonality is lost as is indicated by max 1kj ! j+1;k , the current two blocks Q j and Q j+1 are re-orthogonalized against all previous blocks. How much orthogonality should one maintain? In <ref> [19] </ref> it is proved that if the orthogonality level is about p * M , the so-called semi-orthogonality, then T j is, up to roundoff, the Rayleigh-Ritz projection of H.
Reference: [20] <author> H. D. Simon. </author> <title> The Lanczos algorithm with partial reorthogonalization. </title> <journal> Mathematics of Computation, </journal> <volume> 42 </volume> <pages> 115-142, </pages> <year> 1984. </year>
Reference-contexts: COMPUTING THE GENERALIZED SVD 5 a future work. Therefore we will only use the simple Lanczos process without any re-orthogonalization in the following discussions with the understanding that a re-orthogonalization scheme such as selective or partial re-orthogonalization <ref> [18, 19, 20] </ref> should be incorporated in an actual implementation. However, some preliminary test results obtained using a block version of the symmetric Lanczos algorithm together with the partial re-orthogonalization will be presented in Example 7 in Section 5. Now we are ready to derive our modified algorithm. <p> A block version can handle the case of multiple eigenvalues or singular values as long as the multiplicity does not exceed the block size. To keep Lanczos algorithm from making spurious copies of converged eigen-values or singular values, we use the partial re-orthogonalization scheme discussed in <ref> [19, 20, 9] </ref>. <p> The level of orthogonality of the block Lanczos vectors is monitored by the following recurrence that simulates the loss of orthogonality of the computed Lanczos vectors <ref> [20, 9] </ref>. 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : ! j+1;j1 = ~ fi j+1 (2fi j * s + (ff j + ff j1 )* s + fi j1 ! j;j2 + j+1;j1 +fi j ! j1;k + (ff j + ff k )! j;k + j+1;k (12)
Reference: [21] <author> J. Speiser. </author> <title> Linear algebra problems arising from signal processing. Invited Presentation 3, </title> <booktitle> SIAM Annual Meeting, </booktitle> <year> 1989. </year>
Reference-contexts: 1. Introduction. The generalized singular value decomposition (GSVD) is one of the essential numerical linear algebra tools in signal processing and system identification <ref> [21, 22] </ref>.
Reference: [22] <author> J. Speiser and C. Van Loan. </author> <title> Signal processing using the generalized singular value decomposition. </title> <booktitle> SPIE, Real Time Signal Processing VII, </booktitle> <volume> 495 </volume> <pages> 47-55, </pages> <year> 1984. </year>
Reference-contexts: 1. Introduction. The generalized singular value decomposition (GSVD) is one of the essential numerical linear algebra tools in signal processing and system identification <ref> [21, 22] </ref>. <p> These algorithms are geared to the computation of the complete decomposition and they do not take into account of the sparsity or structure of the matrix pair fA; Bg. In some applications, for example, the generalized total least squares problem [27] and direction-of-arrival estimation with colored noises <ref> [22] </ref>, the complete decomposition is not necessary: only a few of the generalized singular vectors corresponding to the smallest or largest generalized singular values are needed. In those applications, the matrix pair fA; Bg can be sparse or structured, for example, A and B are toeplitz matrices. <p> The above connections also suggest the possibility of solving a generalized eigenvalue problem with two nonnegative matrices by applying the GSVD to their Cholesky factors. This idea was used in <ref> [7, 22, p. 473] </ref> and further pursued in [31, 10]. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces [23, 7, 8]. <p> We then apply the Lanczos bidiagonalization process to AB y . In the singular case, fA; Bg has a infinite generalized singular value while all the singular values of AB y are finite. This problem was also pointed out in <ref> [22, 14, 30] </ref>. Example. 5. The construction of the matrix pair fA; Bg is the same as in Example 4 with t = 12, i.e., the condition number of Z 10 12 .
Reference: [23] <author> G. W. Stewart. </author> <title> On the perturbation of pseudo-inverses, projections, and linear least squares problems. </title> <journal> SIAM Review, </journal> <volume> 19 </volume> <pages> 634-662, </pages> <year> 1977. </year>
Reference-contexts: The CS Decomposition and the GSVD. The GSVD can be derived using the CS decomposition [24, 25, 29]. The CS decomposition was first introduced by Stewart and is of its own importance <ref> [23] </ref>. The first part of this section is a brief review of these results. Throughout the paper, we use the notation Z = (A T ; B T ) T . To ease the presentation, let us first assume that Z is of full column rank. <p> q ); q = minfp; ng: The quantities c i 's and s i 's can be arranged so that 0 c 1 c q c q+1 = = c n = 1; i + s 2 This is the so-called CS decomposition of fQ 1 ; Q 2 g <ref> [23] </ref>. An outline of the derivation is given here for completeness and the ease of the discussions that follow. To further simplify the derivation, we assume that both Q 1 and Q 2 are square matrices and Q 2 is nonsingular. <p> This idea was used in [7, 22, p. 473] and further pursued in [31, 10]. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces <ref> [23, 7, 8] </ref>. <p> Again, we assume that Z = (A T ; B T ) T is of full column rank. In [8], it is proved the following version of the CS decomposition (cf. <ref> [23, 24] </ref>). Theorem A.1.
Reference: [24] <author> G. W. Stewart. </author> <title> Computing the CS decomposition of a partitioned orthogonal matrix. </title> <journal> Nu-merische Mathematik, </journal> <volume> 40 </volume> <pages> 297-306, </pages> <year> 1982. </year>
Reference-contexts: this author was supported in part by NSF grant CCR-9308399. 2 HONGYUAN ZHA For a matrix pair fA; Bg where A and B are dense, there are basically two classes of algorithms for computing the GSVD: one is based on the CS decomposition which will be detailed in Section 2 <ref> [24, 29] </ref>; the other is based on implicit Jacobi rotations [14, 1]. These algorithms are geared to the computation of the complete decomposition and they do not take into account of the sparsity or structure of the matrix pair fA; Bg. <p> The appendix at the end of the paper further develops some topics that are related to the analysis of the GSVD. 2. The CS Decomposition and the GSVD. The GSVD can be derived using the CS decomposition <ref> [24, 25, 29] </ref>. The CS decomposition was first introduced by Stewart and is of its own importance [23]. The first part of this section is a brief review of these results. Throughout the paper, we use the notation Z = (A T ; B T ) T . <p> Again, we assume that Z = (A T ; B T ) T is of full column rank. In [8], it is proved the following version of the CS decomposition (cf. <ref> [23, 24] </ref>). Theorem A.1.
Reference: [25] <author> G. W. Stewart. </author> <title> A method for computing the generalized singular value decomposition. </title> <editor> In B. Kagstrom and A. Ruhe, editors, </editor> <booktitle> Matrix Pencils, </booktitle> <pages> pages 207-220, </pages> <address> New York, 1983. </address> <publisher> Springer. </publisher>
Reference-contexts: The appendix at the end of the paper further develops some topics that are related to the analysis of the GSVD. 2. The CS Decomposition and the GSVD. The GSVD can be derived using the CS decomposition <ref> [24, 25, 29] </ref>. The CS decomposition was first introduced by Stewart and is of its own importance [23]. The first part of this section is a brief review of these results. Throughout the paper, we use the notation Z = (A T ; B T ) T . <p> of A T Ax = (A T A + B T B)x f1; ; 1 i i+1 ; ; c 2 | -z - g; f0; ; 0 k 1 A quasi-diagonal matrix is block diagonal with each diagonal block a diagonal matrix. 2 This is the approach taken in <ref> [25, 29] </ref>. However, certain extra steps need to be performed to ensure the stability of of the algorithm if the complete decomposition is desired [25, 29]. 4 HONGYUAN ZHA and the generalized eigenvalues of B T Bx = (A T A + B T B)x f0; ; 0 i i+1 ; <p> g; f0; ; 0 k 1 A quasi-diagonal matrix is block diagonal with each diagonal block a diagonal matrix. 2 This is the approach taken in <ref> [25, 29] </ref>. However, certain extra steps need to be performed to ensure the stability of of the algorithm if the complete decomposition is desired [25, 29]. 4 HONGYUAN ZHA and the generalized eigenvalues of B T Bx = (A T A + B T B)x f0; ; 0 i i+1 ; ; s 2 | -z - g; f1; ; 1 k From these it follows that the singular values of AB y , where
Reference: [26] <author> J.-G. Sun. </author> <title> Perturbation analysis for the generalized singular value decomposition. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 611-625, </pages> <year> 1983. </year>
Reference-contexts: A. Some Perturbation Bounds. In this appendix, we discuss the relation between the GSVD and the set of canonical angles between two subspaces, then we rederive and strengthen some perturbation bounds for the generalized singular values in <ref> [26, 13, 5] </ref>. Again, we assume that Z = (A T ; B T ) T is of full column rank. In [8], it is proved the following version of the CS decomposition (cf. [23, 24]). Theorem A.1.
Reference: [27] <author> S. Van Huffel and J. Vandewalle. </author> <title> Analysis and properties of the generalized total least squares problem AX B when some or all columns of A are subject to errors. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 10 </volume> <pages> 294-315, </pages> <year> 1989. </year>
Reference-contexts: These algorithms are geared to the computation of the complete decomposition and they do not take into account of the sparsity or structure of the matrix pair fA; Bg. In some applications, for example, the generalized total least squares problem <ref> [27] </ref> and direction-of-arrival estimation with colored noises [22], the complete decomposition is not necessary: only a few of the generalized singular vectors corresponding to the smallest or largest generalized singular values are needed.
Reference: [28] <author> C. F. Van Loan. </author> <title> Generalizing the singular value decomposition. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 13 </volume> <pages> 76-83, </pages> <year> 1976. </year>
Reference-contexts: It is a generalization of the familiar singular value decomposition (SVD) to the case of matrix pairs. (For other generalizations and a unified treatment, the reader is referred to [11].) The GSVD was first proposed by Van Loan <ref> [28] </ref> under the name B-SVD, and later on, Paige and Saunders provided improved formulation more suitable for numerical computations and used the designation GSVD [15]. There are various ways of introducing and deriving the GSVD.
Reference: [29] <author> C. F. Van Loan. </author> <title> Computing the CS and the generalized singular value decompositions. </title> <journal> Nu-merische Mathematik, </journal> <volume> 46 </volume> <pages> 479-491, </pages> <year> 1985. </year>
Reference-contexts: this author was supported in part by NSF grant CCR-9308399. 2 HONGYUAN ZHA For a matrix pair fA; Bg where A and B are dense, there are basically two classes of algorithms for computing the GSVD: one is based on the CS decomposition which will be detailed in Section 2 <ref> [24, 29] </ref>; the other is based on implicit Jacobi rotations [14, 1]. These algorithms are geared to the computation of the complete decomposition and they do not take into account of the sparsity or structure of the matrix pair fA; Bg. <p> The appendix at the end of the paper further develops some topics that are related to the analysis of the GSVD. 2. The CS Decomposition and the GSVD. The GSVD can be derived using the CS decomposition <ref> [24, 25, 29] </ref>. The CS decomposition was first introduced by Stewart and is of its own importance [23]. The first part of this section is a brief review of these results. Throughout the paper, we use the notation Z = (A T ; B T ) T . <p> of A T Ax = (A T A + B T B)x f1; ; 1 i i+1 ; ; c 2 | -z - g; f0; ; 0 k 1 A quasi-diagonal matrix is block diagonal with each diagonal block a diagonal matrix. 2 This is the approach taken in <ref> [25, 29] </ref>. However, certain extra steps need to be performed to ensure the stability of of the algorithm if the complete decomposition is desired [25, 29]. 4 HONGYUAN ZHA and the generalized eigenvalues of B T Bx = (A T A + B T B)x f0; ; 0 i i+1 ; <p> g; f0; ; 0 k 1 A quasi-diagonal matrix is block diagonal with each diagonal block a diagonal matrix. 2 This is the approach taken in <ref> [25, 29] </ref>. However, certain extra steps need to be performed to ensure the stability of of the algorithm if the complete decomposition is desired [25, 29]. 4 HONGYUAN ZHA and the generalized eigenvalues of B T Bx = (A T A + B T B)x f0; ; 0 i i+1 ; ; s 2 | -z - g; f1; ; 1 k From these it follows that the singular values of AB y , where
Reference: [30] <author> C. F. Van Loan. </author> <title> A unitary method for the ESPRIT direction-of-arrival estimation algorithm. </title> <booktitle> SPIE Advanced Algorithms and Architectures for Signal Processing, </booktitle> <volume> 826 </volume> <pages> 170-176, </pages> <year> 1987. </year>
Reference-contexts: We then apply the Lanczos bidiagonalization process to AB y . In the singular case, fA; Bg has a infinite generalized singular value while all the singular values of AB y are finite. This problem was also pointed out in <ref> [22, 14, 30] </ref>. Example. 5. The construction of the matrix pair fA; Bg is the same as in Example 4 with t = 12, i.e., the condition number of Z 10 12 .
Reference: [31] <author> S. Wang and S. Zhao. </author> <title> An algorithm for Ax = Bx with positive definite A and B. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 12 </volume> <pages> 654-660, </pages> <year> 1991. </year>
Reference-contexts: The above connections also suggest the possibility of solving a generalized eigenvalue problem with two nonnegative matrices by applying the GSVD to their Cholesky factors. This idea was used in [7, 22, p. 473] and further pursued in <ref> [31, 10] </ref>. Another intersting connection is the relation with the concept of the canonical angles (principal angles) between two subspaces [23, 7, 8].
Reference: [32] <author> H. Zha. </author> <title> The Singular Value Decompositions: Theory, Algorithms and Applications. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: At first glance, it seems that we need to explicitly compute Q in order to apply the Lanczos process. However, we will show that by a change of basis, a trick used in <ref> [8, 32] </ref>, the explicit computation can be avoided. In the remaining part of this section, we make some remarks on the relations between the GSVD and some other useful decompositions.
Reference: [33] <author> H. Zha. </author> <title> An Inexact Alternating Projection Method for Computing the Canonical Correlations. </title> <note> (Manuscript in preparation) </note>
Reference-contexts: The Modified Lanczos Process. In this section, we will adapt the Lanc-zos bidiagonalization process to the computation of the GSVD of fA; Bg. Another approach based on alternating projections is presented in <ref> [33] </ref>. In Section 2, we discussed the relation between the GSVD and the CS decomposition. The generalized singular values can be obtained by computing the singular values of either Q 1 or Q 2 . <p> This aspect needs further investigation. Some observations can be made from the above numerical experiments. We are currently working on some error analyses that hopefully will explain some of the phenomena we observed, and some of the results will be presented in a forthcoming paper <ref> [33] </ref>. * The inaccuracy in forming the product QQ T u does limit the final accuracy of the computed generalized singular values and vectors. * The rate of convergence of the computed generalized singular values and vectors seems not to be affected very much by the inaccuracy in the product QQ
References-found: 33

