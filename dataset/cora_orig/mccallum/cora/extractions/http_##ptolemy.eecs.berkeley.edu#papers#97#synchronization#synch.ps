URL: http://ptolemy.eecs.berkeley.edu/papers/97/synchronization/synch.ps
Refering-URL: http://ptolemy.eecs.berkeley.edu/papers/97/synchronization/
Root-URL: 
Title: OPTIMIZING SYNCHRONIZATION IN MULTIPROCESSOR DSP SYSTEMS  
Author: Shuvra S. Bhattacharyya, Sundararajan Sriram, and Edward A. Lee 
Note: IEEE Transactions on Signal Processing, Vol. 45, No.  
Date: February 11, 1997  6, June 1997  
Abstract: This paper is concerned with multiprocessor implementations of embedded applications specified as iterative dataow programs, in which synchronization overhead can be significant. We develop techniques to alleviate this overhead by determining a minimal set of processor synchronizations that are essential for correct execution. Our study is based in the context of self-timed execution of iterative data-ow programs. An iterative dataow program consists of a dataow representation of the body of a loop that is to be iterated an indefinite number of times; dataow programming in this form has been studied and applied extensively, particularly in the context of signal processing software. Self-timed execution refers to a combined compile-time/run-time scheduling strategy in which processors synchronize with one another only based on inter-processor communication requirements, and thus, synchronization of processors at the end of each loop iteration does not generally occur. We introduce a new graph-theoretic framework, based on a data structure called the synchronization graph, for analyzing and optimizing synchronization overhead in self-timed, iterative dataow programs. We show that the comprehensive techniques that have been developed for removing redundant synchronizations in non-iterative programs can be extended in this framework to optimally remove redundant synchronizations in our context. We also present an optimization that converts a feedforward dataow graph into a strongly connected graph in such a way as to reduce synchronization overhead without slowing down execution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Banerjee, D. Picker, D. Fellman, P. M. Chau, </author> <title> Improved Scheduling of Signal Flow Graphs onto Multiprocessor Systems Through an Accurate Network Modelling Technique, VLSI Signal Processing VII, </title> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: Related Work Numerous research efforts have focused on constructing efficient parallel schedules for DFGs. For example in [5, 20], techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling <ref> [1, 18, 23, 27] </ref>, while not explicitly addressing overlapped execution. Similarly, in [9], techniques are developed to simultaneously maximize throughput, possibly using overlapped execution, and minimize buffer memory requirements under the assumption of zero IPC cost.
Reference: [2] <author> A. Benveniste, G. Berry, </author> <title> The Synchronous Approach to Reactive and Real-Time Systems, </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> September, </month> <year> 1991. </year> <title> G ipc G s e G ipc B fb e( ) G s 37 </title>
Reference-contexts: requirement that the number of data values produced (consumed) by each actor onto (from) each of its output (input) edges is a fixed value for each firing of that actor, and is known at compile time [16] and should not be confused with the use of synchronous in synchronous languages <ref> [2] </ref>. The techniques developed in this paper assume that the input SDF graph is homogeneous, which means that the numbers of data values produced or consumed are identically unity.
Reference: [3] <author> S. S. Bhattacharyya, S. Sriram, E. A. Lee, </author> <title> Optimizing Synchronization in Multiprocessor Implementations of Iterative Dataow Programs, </title> <note> Memorandum No. UCB/ERL 95/2, </note> <institution> University of California at Berkeley, </institution> <month> January, </month> <year> 1995. </year> <note> WWW URL: http://ptolemy.eecs.berkeley.edu/~ptdesign/Ptolemy/papers/synch_optimization.ps.Z. </note>
Reference-contexts: T T 6 a hardware barrier mechanism; they assume that tight bounds on task execution times are available; they do not address iterative, self-timed execution, in which the execution of successive iterations of the DFG can overlap; and because even for non-iterative execution, there appears to be no obvious correspondence <ref> [3] </ref> between an optimal solution that uses barrier synchronizations and an optimal solution that employs decoupled synchronization checks at the sender and receiver end (directed synchronization). In [26], Shaffer presents an algorithm that minimizes the number of directed synchronizations in the self-timed execution of a DFG. <p> The same properties hold for it, and we state some of the relevant properties here. See [24] for proofs of Lemmas 1 and 3, and <ref> [3] </ref> for a proof of Lemma 2. Lemma 1: [24] Every cycle in the IPC graph has a path delay of at least one if and only if the static schedule it is constructed from is free of deadlock. That is, for each cycle , . Lemma 2: [3] The number <p> 3, and <ref> [3] </ref> for a proof of Lemma 2. Lemma 1: [24] Every cycle in the IPC graph has a path delay of at least one if and only if the static schedule it is constructed from is free of deadlock. That is, for each cycle , . Lemma 2: [3] The number of tokens in any cycle of the IPC graph is always conserved over all possible valid firings of actors in the graph, and is equal to the path delay of that cycle. <p> It is easily verified that this check is equivalent to checking whether or not is redundant <ref> [3] </ref>. From the definition of a redundant synchronization edge, it is easily verified that given a redundant synchronization edge in , and two arbitrary vertices , if we let , then . <p> Thus, none of the minimum-delay path values computed in Step 1 need to be recalculated after removing a redundant synchronization edge in Step 3. In <ref> [3] </ref>, it is shown that RemoveRedundantSynchs attains a time complexity of if we use a modification of Dijkstras algorithm described in [6] for Step 1. 8. <p> For elaboration on the derivation of this DFG from the original SDF graph see <ref> [3, 16] </ref>. The synchronization graph that corresponds to Figs. 5 (a&b) is shown in Fig. 5 (c). The dashed edges are synchronization edges. <p> Some fundamental difficulties in deriving such an extension are explained in <ref> [3] </ref>. However, DetermineDelays can be extended to yield heuristics for the general case in which the original synchronization graph contains more than one source SCC and more than one sink SCC.
Reference: [4] <author> J. T. Buck, S. Ha, E. A. Lee, D. G. Messerschmitt, Ptolemy: </author> <title> A Framework for Simulating and Prototyping Heterogeneous Systems, </title> <booktitle> Intl. Jo. of Computer Simulation, </booktitle> <year> 1994. </year>
Reference: [5] <author> L-F. Chao, E. H-M. Sha, </author> <title> Static Scheduling for Synthesis of DSP Algorithms on Various Models, </title> <type> technical report, </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1993. </year>
Reference-contexts: Thus an optimal schedule is one that minimizes . 2. Related Work Numerous research efforts have focused on constructing efficient parallel schedules for DFGs. For example in <ref> [5, 20] </ref>, techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling [1, 18, 23, 27], while not explicitly addressing overlapped execution.
Reference: [6] <author> T. H. Cormen, C. E. Leiserson, R. L. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: Since the ini tial number of tokens on is , the size of the buffer corresponding to is bounded above by . Q. E. D. The quantities can be computed using Dijkstras algorithm <ref> [6] </ref> to solve the all-pairs shortest path problem on the synchronization graph in time . Thus the values can be computed in time. 6. Problem Statement We refer to each access of the shared memory synchronization variable by and as a synchronization access 1 to shared memory. <p> This computation is equivalent to solving an instance of the well known all points shortest paths problem <ref> [6] </ref>. Then, we examine each synchronization edge in some arbitrary sequence and determine whether or not there is a path from some successor of (other than ) to that has a path delay that does not exceed . <p> Thus, none of the minimum-delay path values computed in Step 1 need to be recalculated after removing a redundant synchronization edge in Step 3. In [3], it is shown that RemoveRedundantSynchs attains a time complexity of if we use a modification of Dijkstras algorithm described in <ref> [6] </ref> for Step 1. 8. Comparison with Shaffers Approach In [26], Shaffer presents an algorithm that minimizes the number of directed synchronizations in the self-timed execution of a DFG under the (implicit) assumption that the execution of successive iterations of the DFG are not allowed to overlap. <p> C 1 v i C i t *( ) C i w 1 D 1 t *( ) D 1 w i D i t *( ) D i d 0 w m v 1 ,( ) G G 28 Proof: Recall that in a connected graph , must exceed <ref> [6] </ref>. Thus, the number of feedforward edges must satisfy , where is the number of SCCs.
Reference: [7] <author> H. G. Dietz, A. Zaafrani, M. T. Okeefe, </author> <title> Static Scheduling for Barrier MIMD Architectures, Jo. </title> <booktitle> of Supercomputing, </booktitle> <month> February, </month> <year> 1992. </year>
Reference-contexts: Our work can be used as a post-processing step to improve the performance of implementations that use any of these scheduling techniques. Among the prior work that is most relevant to this paper is the barrier-MIMD concept, discussed in <ref> [7] </ref>. <p> Perhaps the most significant direction for further work is the incorporation of timing guarantees for example, hard upper and lower execution time bounds, as Dietz, Zaafrani, and Okeefe use in <ref> [7] </ref>; and handling of a mix of actors some of which have guaranteed execution time bounds, and some that have no such guarantees, as Filo, Ku, Coelho Jr.,and De Micheli, do in [8].
Reference: [8] <author> D. Filo, D. C. Ku, C. N. Coelho Jr., G. De Micheli, </author> <title> Interface Optimization for Concurrent Systems Under Timing Constraints, </title> <journal> IEEE Trans. on VLSI Systems, </journal> <month> September, </month> <year> 1993. </year>
Reference-contexts: timing guarantees for example, hard upper and lower execution time bounds, as Dietz, Zaafrani, and Okeefe use in [7]; and handling of a mix of actors some of which have guaranteed execution time bounds, and some that have no such guarantees, as Filo, Ku, Coelho Jr.,and De Micheli, do in <ref> [8] </ref>.
Reference: [9] <author> R. Govindarajan, G. R. Gao, P. Desai, </author> <title> Minimizing Memory Requirements in Rate-Optimal Schedules, </title> <booktitle> Proc. of the Intl. Conf. on Application Specific Array Processors, </booktitle> <month> August, </month> <year> 1994. </year>
Reference-contexts: For example in [5, 20], techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling [1, 18, 23, 27], while not explicitly addressing overlapped execution. Similarly, in <ref> [9] </ref>, techniques are developed to simultaneously maximize throughput, possibly using overlapped execution, and minimize buffer memory requirements under the assumption of zero IPC cost. Our work can be used as a post-processing step to improve the performance of implementations that use any of these scheduling techniques.
Reference: [10] <author> J. A. Huisken et al., </author> <title> Synthesis of Synchronous Communication Hardware in a Multiprocessor Architecture, Jo. </title> <booktitle> of VLSI Signal Processing, </booktitle> <month> December, </month> <year> 1993. </year>
Reference-contexts: each kind of platform, every IPC that requires such synchronization checks costs performance, and sometimes extra hardware complexity: semaphore checks cost execution time on the processors, synchronization instructions that make use of synchronization hardware also cost execution time, and blocking interfaces in hardware/software implementations require more hardware than non-blocking interfaces <ref> [10] </ref>. The main goal of this paper is to present techniques that reduce the rate at which processors must access shared memory for the purpose of synchronization in embedded, shared-memory multiprocessor implementations of iterative dataow programs.
Reference: [11] <author> A. Kalavade, E. A. Lee, </author> <title> A Hardware/Software Codesign Methodology for DSP Applications, </title> <booktitle> IEEE Design and Test, </booktitle> <month> September </month> <year> 1993. </year>
Reference: [12] <author> W. Koh, </author> <title> A Reconfigurable Multiprocessor System for DSP Behavioral Simulation, </title> <type> Ph.D. Thesis, </type> <note> Memorandum No. </note> <institution> UCB/ERL M90/53, Electronics Research Laboratory, University of California at Berkeley, </institution> <month> June, </month> <year> 1990. </year>
Reference: [13] <author> S. Y. Kung, P. S. Lewis, S. C. Lo, </author> <title> Performance Analysis and Optimization of VLSI Dataow Arrays, Jo. </title> <booktitle> of Parallel and Distributed Computing, </booktitle> <month> December, </month> <year> 1987. </year>
Reference-contexts: Sizing buffers optimally such that the maximum cycle mean remains unchanged has been studied by Kung, Lewis and Lo in <ref> [13] </ref>, where the authors propose an integer linear programming formulation of the problem, with the number of constraints equal to the number of fundamental cycles in the DFG (potentially an exponential number of constraints).
Reference: [14] <author> R. Lauwereins, M. Engels, J.A. Peperstraete, E. Steegmans, J. Van Ginderdeuren, </author> <title> GRAPE: A CASE Tool for Digital Signal Parallel Processing, </title> <journal> IEEE ASSP Magazine, </journal> <month> April, </month> <year> 1990. </year>
Reference-contexts: SDF and closely related models have been used widely in DSP design environments, such as those described in <ref> [14, 19, 22, 25] </ref>. In SDF, a program is represented as a directed graph in which the vertices, called actors, represent computations, and the edges specify FIFO channels for communication between actors.
Reference: [15] <author> E. Lawler, </author> <title> Combinatorial Optimization: Networks and Matroids, </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1976. </year> <month> 38 </month>
Reference-contexts: We have not included IPC costs in this calculation, but these can be included in a straightforward manner by adding the send and receive costs to the corresponding actors performing these operations. The maximum cycle mean can be calculated in time , where and are such that and <ref> [15] </ref>. 4.2 Execution Time Estimates If we only have execution time estimates available instead of exact values, and we set in the C G ipc G ipc max cycle C in G t v ( ) C ( )Delay - = 1 - B E,( ) E I,( ) I G,( <p> Fig. 8 outlines the restricted version of our algorithm that applies when the synchronization graph has exactly one source SCC. Here, BellmanFord is assumed to be an algorithm that takes a synchroni zation graph as input, and applies the Bellman-Ford algorithm discussed in pp. 94-97 of <ref> [15] </ref> to return the cycle mean of the critical cycle in ; if one or more cycles exist that have zero path delay, then Bell G s G s G s G V E,( )= G e 0 D 0 fi e n 1 D n 1 fi, ,[ ] e
Reference: [16] <author> E. A. Lee, D. G. Messerschmitt, </author> <title> Static Scheduling of Synchronous Dataow Programs for Digital Signal Processing, </title> <journal> IEEE Trans. on Computers, </journal> <month> February, </month> <year> 1987. </year>
Reference-contexts: The term synchronous refers to the requirement that the number of data values produced (consumed) by each actor onto (from) each of its output (input) edges is a fixed value for each firing of that actor, and is known at compile time <ref> [16] </ref> and should not be confused with the use of synchronous in synchronous languages [2]. The techniques developed in this paper assume that the input SDF graph is homogeneous, which means that the numbers of data values produced or consumed are identically unity. <p> The techniques developed in this paper assume that the input SDF graph is homogeneous, which means that the numbers of data values produced or consumed are identically unity. However, since efficient techniques have been developed to convert general SDF graphs into equivalent (for our purposes) homogeneous SDF graphs <ref> [16] </ref>, our techniques apply equally to general SDF graphs. In the remainder of this paper, when we refer to a dataow graph (DFG) we imply a homogeneous SDF graph. Delays on DFG edges represent initial tokens, and specify dependencies between iterations of the actors in iterative execution. <p> For elaboration on the derivation of this DFG from the original SDF graph see <ref> [3, 16] </ref>. The synchronization graph that corresponds to Figs. 5 (a&b) is shown in Fig. 5 (c). The dashed edges are synchronization edges.
Reference: [17] <author> E. A. Lee, S. Ha, </author> <title> Scheduling Strategies for Multiprocessor Real-Time DSP, </title> <booktitle> Globecom, </booktitle> <month> November, </month> <year> 1989. </year>
Reference-contexts: By scheduling we collectively refer to the tasks of assigning actors in the DFG to processors, ordering execution of these actors on each processor, and determining when each actor fires (begins execution) such that all data precedence constraints are met. In <ref> [17] </ref> the authors propose a scheduling taxonomy based on which of these tasks are performed at compile time (static strategy) and which at run time (dynamic strategy); in this paper we will use the same terminology that was introduced there. In the fully-static scheduling strategy of [17], all three scheduling tasks <p> In <ref> [17] </ref> the authors propose a scheduling taxonomy based on which of these tasks are performed at compile time (static strategy) and which at run time (dynamic strategy); in this paper we will use the same terminology that was introduced there. In the fully-static scheduling strategy of [17], all three scheduling tasks are performed at compile time. This strategy involves the least possible runtime overhead. All processors run in lock step and no explicit synchronization is required when they exchange data. However, this strategy assumes that exact execution times of actors are known. <p> Under such an assumption on timing, it is best to discard the exact timing information from the fully static schedule, but still retain the processor assignment and actor ordering specified by the fully static schedule. This results in the self-timed scheduling strategy of <ref> [17] </ref>. Each processor executes the actors assigned to it in the order specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available. Thus in self-timed scheduling, processors are required to perform run-time synchronization when they communicate data.
Reference: [18] <author> G. Liao, G. R. Gao, E. Altman, V. K. Agarwal, </author> <title> A Comparative Study of DSP Multiprocessor List Scheduling Heuristics, </title> <type> technical report, </type> <institution> School of Computer Science, McGill University. </institution>
Reference-contexts: Related Work Numerous research efforts have focused on constructing efficient parallel schedules for DFGs. For example in [5, 20], techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling <ref> [1, 18, 23, 27] </ref>, while not explicitly addressing overlapped execution. Similarly, in [9], techniques are developed to simultaneously maximize throughput, possibly using overlapped execution, and minimize buffer memory requirements under the assumption of zero IPC cost.
Reference: [19] <author> D. R. OHallaron, </author> <title> The Assign Parallel Program Generator, </title> <institution> Memorandum CMU-CS-91-141, School of Computer Science, Carnegie Mellon University, </institution> <month> May, </month> <year> 1991. </year>
Reference-contexts: SDF and closely related models have been used widely in DSP design environments, such as those described in <ref> [14, 19, 22, 25] </ref>. In SDF, a program is represented as a directed graph in which the vertices, called actors, represent computations, and the edges specify FIFO channels for communication between actors.
Reference: [20] <author> K. K. Parhi, D. G. Messerschmitt, </author> <title> Static Rate-Optimal Scheduling of Iterative Data-Flow Programs via Optimum Unfolding, </title> <journal> IEEE Trans. on Computers, </journal> <month> February, </month> <year> 1991. </year>
Reference-contexts: Thus an optimal schedule is one that minimizes . 2. Related Work Numerous research efforts have focused on constructing efficient parallel schedules for DFGs. For example in <ref> [5, 20] </ref>, techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling [1, 18, 23, 27], while not explicitly addressing overlapped execution.
Reference: [21] <author> J. L. Peterson, </author> <title> Petri Net Theory and the Modelling of Systems, </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1981. </year>
Reference-contexts: Such an as soon as possible (ASAP) firing pattern implies: . (3) The IPC graph can also be looked upon as a timed marked graph <ref> [21] </ref> or Reiters computation graph [24]. The same properties hold for it, and we state some of the relevant properties here. See [24] for proofs of Lemmas 1 and 3, and [3] for a proof of Lemma 2.
Reference: [22] <author> J. Pino, S. Ha, E. A. Lee, J. T. Buck, </author> <title> Software Synthesis for DSP Using Ptolemy, Jo. </title> <booktitle> of VLSI Signal Processing, </booktitle> <month> January, </month> <year> 1995. </year>
Reference-contexts: SDF and closely related models have been used widely in DSP design environments, such as those described in <ref> [14, 19, 22, 25] </ref>. In SDF, a program is represented as a directed graph in which the vertices, called actors, represent computations, and the edges specify FIFO channels for communication between actors.
Reference: [23] <author> H. Printz, </author> <title> Automatic Mapping of Large Signal Processing Systems to a Parallel Machine, </title> <type> Ph.D. thesis, </type> <institution> Memorandum CMU-CS-91-101, School of Computer Science, Carnegie Mellon University, </institution> <month> May, </month> <year> 1991. </year>
Reference-contexts: Related Work Numerous research efforts have focused on constructing efficient parallel schedules for DFGs. For example in [5, 20], techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling <ref> [1, 18, 23, 27] </ref>, while not explicitly addressing overlapped execution. Similarly, in [9], techniques are developed to simultaneously maximize throughput, possibly using overlapped execution, and minimize buffer memory requirements under the assumption of zero IPC cost.
Reference: [24] <author> R. Reiter, </author> <title> Scheduling Parallel Computations, </title> <type> Jo. </type> <institution> of the Association for Computing Machinery, </institution> <month> October </month> <year> 1968. </year>
Reference-contexts: Such an as soon as possible (ASAP) firing pattern implies: . (3) The IPC graph can also be looked upon as a timed marked graph [21] or Reiters computation graph <ref> [24] </ref>. The same properties hold for it, and we state some of the relevant properties here. See [24] for proofs of Lemmas 1 and 3, and [3] for a proof of Lemma 2. Lemma 1: [24] Every cycle in the IPC graph has a path delay of at least one if <p> Such an as soon as possible (ASAP) firing pattern implies: . (3) The IPC graph can also be looked upon as a timed marked graph [21] or Reiters computation graph <ref> [24] </ref>. The same properties hold for it, and we state some of the relevant properties here. See [24] for proofs of Lemmas 1 and 3, and [3] for a proof of Lemma 2. Lemma 1: [24] Every cycle in the IPC graph has a path delay of at least one if and only if the static schedule it is constructed from is free of deadlock. <p> can also be looked upon as a timed marked graph [21] or Reiters computation graph <ref> [24] </ref>. The same properties hold for it, and we state some of the relevant properties here. See [24] for proofs of Lemmas 1 and 3, and [3] for a proof of Lemma 2. Lemma 1: [24] Every cycle in the IPC graph has a path delay of at least one if and only if the static schedule it is constructed from is free of deadlock. That is, for each cycle , . <p> Lemma 3: The asymptotic iteration period for a strongly connected IPC graph when actors execute as soon as data is available at all inputs is given by <ref> [24] </ref>: . (4) Note that from Lemma 1. t v ( ) t v ( ) t v ( ) end v j k,( ) start v j k,( ) t v j ( )+= start v i k,( ) start v j k v j v i ,( )( )delay,(
Reference: [25] <author> S. Ritz, M. Pankert, H. Meyr, </author> <title> High Level Software Synthesis for Signal Processing Systems, </title> <booktitle> Proc. of the Intl. Conf. on Application Specific Array Processors, </booktitle> <month> August, </month> <year> 1992. </year>
Reference-contexts: SDF and closely related models have been used widely in DSP design environments, such as those described in <ref> [14, 19, 22, 25] </ref>. In SDF, a program is represented as a directed graph in which the vertices, called actors, represent computations, and the edges specify FIFO channels for communication between actors.
Reference: [26] <author> P. L. Shaffer, </author> <title> Minimization of Interprocessor Synchronization in Multiprocessors with Shared and Private Memory, </title> <booktitle> Intl. Conf. on Parallel Processing, </booktitle> <year> 1989. </year>
Reference-contexts: In <ref> [26] </ref>, Shaffer presents an algorithm that minimizes the number of directed synchronizations in the self-timed execution of a DFG. However, this work, like that of Dietz et al., does not allow the execution of successive iterations of the DFG to overlap. <p> In [3], it is shown that RemoveRedundantSynchs attains a time complexity of if we use a modification of Dijkstras algorithm described in [6] for Step 1. 8. Comparison with Shaffers Approach In <ref> [26] </ref>, Shaffer presents an algorithm that minimizes the number of directed synchronizations in the self-timed execution of a DFG under the (implicit) assumption that the execution of successive iterations of the DFG are not allowed to overlap.
Reference: [27] <author> G. C. Sih, E. A. Lee, </author> <title> Scheduling to Account for Interprocessor Communication Within Interconnection-Constrained Processor Networks, </title> <booktitle> Intl. Conf. on Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: Related Work Numerous research efforts have focused on constructing efficient parallel schedules for DFGs. For example in [5, 20], techniques are developed for exploiting overlapped execution to optimize throughput, assuming zero cost for IPC. Other work has focused on taking IPC costs into account during scheduling <ref> [1, 18, 23, 27] </ref>, while not explicitly addressing overlapped execution. Similarly, in [9], techniques are developed to simultaneously maximize throughput, possibly using overlapped execution, and minimize buffer memory requirements under the assumption of zero IPC cost.
Reference: [28] <author> S. Sriram, E. A. Lee, </author> <title> Statically Scheduling Communication Resources in Multiprocessor DSP architectures, </title> <booktitle> Proc. of the Asilomar Conf. on Signals, Systems, and Computers, </booktitle> <month> November, </month> <year> 1994. </year>
Reference: [29] <author> S. Sriram, E. A. Lee, </author> <title> Design and Implementation of an Ordered Memory Access Architecture, </title> <booktitle> Proc. of the Intl. Conf. on Acoustics Speech and Signal Processing, </booktitle> <month> April, </month> <year> 1993. </year> <month> 39 </month>
Reference: [30] <author> V. Zivojnovic, H. Koerner, H. Meyr, </author> <title> Multiprocessor Scheduling with A-priori Node Assignment, VLSI Signal Processing VII, </title> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: This technique is similar in spirit to the one in <ref> [30] </ref>, where the concept of converting a DFG that contains feedforward edges into a strongly connected graph has been studied in the context of retiming. Fig. 6 presents our algorithm for transforming a synchronization graph that is not strongly connected into a strongly connected graph.
References-found: 30

