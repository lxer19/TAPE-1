URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-307.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Facial Expression Recognition using a Dynamic Model and Motion Energy  
Author: Irfan A. Essa and Alex P. Pentland 
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: Perceptual Computing Group, The Media Laboratory, Massachusetts Institute of Technology  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 307 Appears: International Conference on Computer Vision '95, Cambridge, MA, June 20-23, 1995 Abstract Previous efforts at facial expression recognition have been based on the Facial Action Coding System (FACS), a representation developed in order to allow human psychologists to code expression from static facial mugshots. In this paper we develop new, more accurate representations for facial expression by building a video database of facial expressions and then probabilistically characterizing the facial muscle activation associated with each expression using a detailed physical model of the skin and muscles. This produces a muscle-based representation of facial motion, which is then used to recognize facial expressions in two different ways. The first method uses the physics-based model directly, by recognizing expressions through comparison of estimated muscle activations. The second method uses the physics-based model to generate spatio-temporal motion-energy templates of the whole face for each different expression. These simple, biologically-plausible motion energy templates are then used for recognition. Both methods show substantially greater accuracy at expression recognition than has been previously achieved. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. N. Bassili. </author> <title> Facial motion in the perception of faces and of emotional expression. </title> <journal> Journal of Experimental Psyschol-ogy, </journal> <volume> 4 </volume> <pages> 373-379, </pages> <year> 1978. </year>
Reference-contexts: In contradiction to this view, there is now a growing body of psychological research that argues that it is the dynamics of the expression, rather than detailed spatial deformations, that is important in expression recognition <ref> [1, 3] </ref>. Indeed several famous researchers have claimed that the timing of expressions, something that is completely missing from FACS, is a critical parameter in recognizing emotions [4, 10].

Reference: [3] <author> V. Bruce. </author> <title> Recognising Faces. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1988. </year>
Reference-contexts: In contradiction to this view, there is now a growing body of psychological research that argues that it is the dynamics of the expression, rather than detailed spatial deformations, that is important in expression recognition <ref> [1, 3] </ref>. Indeed several famous researchers have claimed that the timing of expressions, something that is completely missing from FACS, is a critical parameter in recognizing emotions [4, 10].
Reference: [4] <author> C. Darwin. </author> <title> The expression of the emotions in man and animals. </title> <publisher> University of Chicago Press, </publisher> <year> 1965. </year> <note> (Original work published in 1872). </note>
Reference-contexts: Indeed several famous researchers have claimed that the timing of expressions, something that is completely missing from FACS, is a critical parameter in recognizing emotions <ref> [4, 10] </ref>.
Reference: [5] <author> P. Ekman and W. V. Friesen. </author> <title> Facial Action Coding System. </title> <publisher> Consulting Psychologists Press Inc., </publisher> <address> 577 College Avenue, Palo Alto, California 94306, </address> <year> 1978. </year>
Reference-contexts: Unlike previous approaches at facial expression recognition, our method does not to rely on a heuristic dictionary of facial motion developed for emotion coding by human psychologists (e.g., the Facial Action Coding System (FACS) <ref> [5] </ref>). Instead, we objectively quantify facial movement during various facial expressions using computer vision techniques. This provides us with a more accurate model of facial expression, allowing us to more efficiently utilize the optical flow information and to achieve greater recognition accuracy. <p> One interesting aspect of this work is that we demonstrate that extremely simple, biologically-plausible motion energy detectors can be extracted to accurately recognize human expressions. 1.1 Recognizing Facial Motion To categorize facial motion we need first to determine the expressions from facial movements. Ekman and Friesen <ref> [5] </ref> have produced a system for describing all visually distinguishable facial movements, called the Facial Action Cod ing System or FACS. It is based on the enumeration of all action units (AU s) of a face that cause facial movements.
Reference: [6] <author> I. Essa, T. Darrell, and A. Pentland. </author> <title> Tracking facial motion. </title> <booktitle> In Proceedings of the Workshop on Motion of Nonrigid and Articulated Objects, </booktitle> <pages> pages 36-42. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Faces and facial expressions are an important aspect of human interaction, both in the context of interpersonal communication and man-machine interfaces. In recent years several researchers have attempted automatic recognition and tracking of facial expressions <ref> [19, 2, 8, 12, 16, 6] </ref>. In this paper we present two new methods for recognizing facial expressions, both of which attempt to improve over previous approaches by using a better model of facial motion and by using facial optical flow more efficiently.
Reference: [7] <author> I. Essa and A. Pentland. </author> <title> A vision system for observing and extracting facial action parameters. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 76-83. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: By combining this physical model with registered optical flow measurements from human faces, we have been able to reliably estimate muscle actuations within the human face (see <ref> [7] </ref>). Our first method for expression recognition builds on our ability to estimate facial muscle actuations from optical flow data. <p> a set of registered features on the face image. 2.2 Visually extracted Facial Expressions After the initial registering of the model to the image the coarse-to-fine flow computation methods presented by Simoncelli [15] and Wang [17] are used to compute the flow within an control-theoretic approach of Essa and Pent-land <ref> [7] </ref>. The model on the face image tracks the motion of the head and the face correctly as long as there is not an excessive amount of rigid motion of the face during an expression. The method of Essa and Pentland [7] provides us with a detailed physical model and also <p> flow within an control-theoretic approach of Essa and Pent-land <ref> [7] </ref>. The model on the face image tracks the motion of the head and the face correctly as long as there is not an excessive amount of rigid motion of the face during an expression. The method of Essa and Pentland [7] provides us with a detailed physical model and also a way of observing and extracting the action units of a face using video sequences as input. <p> image with muscles (black lines), and nodes (dots). (c) and (d) show expressions of smile and surprise, (e) and (f) show a 3D model with surprise and smile expressions resynthesized from extracted muscle actuations, and (g) and (h) show the spa-tio-temporal motion energy representation of facial motion for these expressions <ref> [7] </ref>. * The action units are purely local spatial patterns. Real facial motion is almost never completely localized; Ekman himself has described some of these action units as an unnatural type of facial movement. De (a) (b) (b) Smile expressions. <p> Although the muscle-based models used in computer graphics have alleviated some of these problems [18], they are still too simple to accurately describe real facial motion. Consequently, our earlier method <ref> [7] </ref> lets us characterize the functional form of the actuation profile, and lets us determine a basis set of action units that better describes the spatial properties of real facial motion.
Reference: [8] <author> K. Mase. </author> <title> Recognition of facial expressions for optical flow. </title> <journal> IEICE Transactions, Special Issue on Computer Vision and its Applications, </journal> <volume> E 74(10), </volume> <year> 1991. </year>
Reference-contexts: 1 Introduction Faces and facial expressions are an important aspect of human interaction, both in the context of interpersonal communication and man-machine interfaces. In recent years several researchers have attempted automatic recognition and tracking of facial expressions <ref> [19, 2, 8, 12, 16, 6] </ref>. In this paper we present two new methods for recognizing facial expressions, both of which attempt to improve over previous approaches by using a better model of facial motion and by using facial optical flow more efficiently. <p> Recognition of facial expressions can be achieved by categorizing a set of such predetermined facial motions as in FACS, rather than determining the motion of each facial point independently. This is the approach taken by Ya-coob and Davis [19, 13], Black and Yacoob [2] and Mase and Pentland <ref> [8, 9] </ref> for their recognition systems. Yacoob and Davis [19], who extend the work of Mase, detect motion (only in eight directions) in six predefined and hand initialized rectangular regions on a face and then use simplifications of the FACS rules for the six universal expressions for recognition. <p> Black and Yacoob [2] extend this method, using local parameterized models of image motion to deal with large-scale head motions. These methods show a 86% overall accuracy (92% if false positives are excluded) in correctly recognizing expressions over their database of 105 expressions. Mase <ref> [8] </ref> on a smaller set of data (30 test cases) obtained an accuracy of 80%. In many ways these are impressive results, considering the complexity of the FACS model and the difficulty in measuring facial motion within small windowed regions of the face. <p> All the results discussed in this paper are based on expressions performed by 8 subjects with a total of 52 expressions. This database is substantially larger than that used by Mase <ref> [8] </ref> in his pioneering work on recognizing facial expressions.
Reference: [9] <author> K. Mase and A. Pentland. </author> <title> Lipreading by optical flow. </title> <journal> Systems and Computers, </journal> <volume> 22(6) </volume> <pages> 67-76, </pages> <year> 1991. </year>
Reference-contexts: Recognition of facial expressions can be achieved by categorizing a set of such predetermined facial motions as in FACS, rather than determining the motion of each facial point independently. This is the approach taken by Ya-coob and Davis [19, 13], Black and Yacoob [2] and Mase and Pentland <ref> [8, 9] </ref> for their recognition systems. Yacoob and Davis [19], who extend the work of Mase, detect motion (only in eight directions) in six predefined and hand initialized rectangular regions on a face and then use simplifications of the FACS rules for the six universal expressions for recognition.
Reference: [10] <author> M. Minsky. </author> <title> The Society of Mind. A Touchstone Book, </title> <publisher> Simon and Schuster Inc., </publisher> <year> 1985. </year>
Reference-contexts: Indeed several famous researchers have claimed that the timing of expressions, something that is completely missing from FACS, is a critical parameter in recognizing emotions <ref> [4, 10] </ref>.
Reference: [11] <author> B. Moghaddam and A. Pentland. </author> <title> Face recognition using view-based and modular eigenspaces. In Automatic Systems for the Identification and Inspection of Humans, </title> <booktitle> volume 2277. SPIE, </booktitle> <year> 1994. </year>
Reference-contexts: To us this strongly suggests moving away from a static, dissect-every-change analysis of expression (which is how the FACS model was developed), towards a whole-face analysis of facial dynamics in motion sequences. 1 (a) (b) by Pentland et al. <ref> [11, 12] </ref>, using a canonical model of a face. 1.2 Two Approaches We have previously developed a detailed physical model of the human face and its musculature. <p> Initially we started our estimation process by manually translating, rotating and deforming our 3-D facial model to fit each face as has been done in all previous research studies. To automate this process we are now using the Modular Eigenspace methods of Pentland and Moghaddam <ref> [11, 12] </ref>. This method allows us to extract the positions of the eyes, nose and lips in an image as shown in Figure 1 (a). From these feature positions a canonical mesh is generated and then the image is (affine) warped to the mesh and then masked (Figure 1 (c)).
Reference: [12] <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 84-91. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction Faces and facial expressions are an important aspect of human interaction, both in the context of interpersonal communication and man-machine interfaces. In recent years several researchers have attempted automatic recognition and tracking of facial expressions <ref> [19, 2, 8, 12, 16, 6] </ref>. In this paper we present two new methods for recognizing facial expressions, both of which attempt to improve over previous approaches by using a better model of facial motion and by using facial optical flow more efficiently. <p> To us this strongly suggests moving away from a static, dissect-every-change analysis of expression (which is how the FACS model was developed), towards a whole-face analysis of facial dynamics in motion sequences. 1 (a) (b) by Pentland et al. <ref> [11, 12] </ref>, using a canonical model of a face. 1.2 Two Approaches We have previously developed a detailed physical model of the human face and its musculature. <p> Initially we started our estimation process by manually translating, rotating and deforming our 3-D facial model to fit each face as has been done in all previous research studies. To automate this process we are now using the Modular Eigenspace methods of Pentland and Moghaddam <ref> [11, 12] </ref>. This method allows us to extract the positions of the eyes, nose and lips in an image as shown in Figure 1 (a). From these feature positions a canonical mesh is generated and then the image is (affine) warped to the mesh and then masked (Figure 1 (c)).
Reference: [13] <author> M. Rosenblum, Y. Yacoob, and L. Davis. </author> <title> Human emotion recognition from motion using a radial basis function network architecture. </title> <booktitle> In The Workshop on Motion of Nonrigid and Articulated Objects, </booktitle> <pages> pages 43-49. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: Recognition of facial expressions can be achieved by categorizing a set of such predetermined facial motions as in FACS, rather than determining the motion of each facial point independently. This is the approach taken by Ya-coob and Davis <ref> [19, 13] </ref>, Black and Yacoob [2] and Mase and Pentland [8, 9] for their recognition systems.
Reference: [14] <author> M. Rydfalk. CANDIDE: </author> <title> A Parameterized Face. </title> <type> PhD thesis, </type> <institution> Linkoping University, Department of Electrical Engineering, </institution> <month> Oct </month> <year> 1987. </year>
Reference-contexts: This plot was constructed by mapping the motion onto the face model (a CANDIDE model which is a computer graphics model for implementing FACS motions <ref> [14] </ref>) measuring the motion of the control points.
Reference: [15] <author> E. P. Simoncelli. </author> <title> Distributed Representation and Analysis of Visual Motion. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: can then extract canonical feature points on the image that correspond to our mesh (Figure 1 (d)), producing a set of registered features on the face image. 2.2 Visually extracted Facial Expressions After the initial registering of the model to the image the coarse-to-fine flow computation methods presented by Simoncelli <ref> [15] </ref> and Wang [17] are used to compute the flow within an control-theoretic approach of Essa and Pent-land [7].
Reference: [16] <author> D. Terzopoulus and K. Waters. </author> <title> Analysis and synthesis of facial image sequences using physical and anatomical models. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 15(6) </volume> <pages> 569-579, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Faces and facial expressions are an important aspect of human interaction, both in the context of interpersonal communication and man-machine interfaces. In recent years several researchers have attempted automatic recognition and tracking of facial expressions <ref> [19, 2, 8, 12, 16, 6] </ref>. In this paper we present two new methods for recognizing facial expressions, both of which attempt to improve over previous approaches by using a better model of facial motion and by using facial optical flow more efficiently.
Reference: [17] <author> J. Y. A. Wang and E. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <year> 1993. </year>
Reference-contexts: canonical feature points on the image that correspond to our mesh (Figure 1 (d)), producing a set of registered features on the face image. 2.2 Visually extracted Facial Expressions After the initial registering of the model to the image the coarse-to-fine flow computation methods presented by Simoncelli [15] and Wang <ref> [17] </ref> are used to compute the flow within an control-theoretic approach of Essa and Pent-land [7]. The model on the face image tracks the motion of the head and the face correctly as long as there is not an excessive amount of rigid motion of the face during an expression.
Reference: [18] <author> K. Waters and D. Terzopoulos. </author> <title> Modeling and animating faces using scanned data. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 2 </volume> <pages> 123-128, </pages> <year> 1991. </year>
Reference-contexts: Other limitations of FACS include the inability to describe fine eye and lip motions, and the inability to describe the coarticulation effects found most commonly in speech. Although the muscle-based models used in computer graphics have alleviated some of these problems <ref> [18] </ref>, they are still too simple to accurately describe real facial motion. Consequently, our earlier method [7] lets us characterize the functional form of the actuation profile, and lets us determine a basis set of action units that better describes the spatial properties of real facial motion.
Reference: [19] <author> Y. Yacoob and L. Davis. </author> <title> Computing spatio-temporal representations of human faces. </title> <booktitle> In Proceedings of the Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 70-75. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year> <month> 8 </month>
Reference-contexts: 1 Introduction Faces and facial expressions are an important aspect of human interaction, both in the context of interpersonal communication and man-machine interfaces. In recent years several researchers have attempted automatic recognition and tracking of facial expressions <ref> [19, 2, 8, 12, 16, 6] </ref>. In this paper we present two new methods for recognizing facial expressions, both of which attempt to improve over previous approaches by using a better model of facial motion and by using facial optical flow more efficiently. <p> Recognition of facial expressions can be achieved by categorizing a set of such predetermined facial motions as in FACS, rather than determining the motion of each facial point independently. This is the approach taken by Ya-coob and Davis <ref> [19, 13] </ref>, Black and Yacoob [2] and Mase and Pentland [8, 9] for their recognition systems. <p> This is the approach taken by Ya-coob and Davis [19, 13], Black and Yacoob [2] and Mase and Pentland [8, 9] for their recognition systems. Yacoob and Davis <ref> [19] </ref>, who extend the work of Mase, detect motion (only in eight directions) in six predefined and hand initialized rectangular regions on a face and then use simplifications of the FACS rules for the six universal expressions for recognition. <p> This example illustrates that coarticulation effects can be observed by our system, and that they occur even in quite simple expressions. By using these observed temporal patterns of muscle activation, rather than simple linear ramps, or heuristic approaches of the representing temporal changes (as in <ref> [19] </ref>), our representation of facial motion is more suitable for recognizing facial motion. 3.3 Corrected Motion Fields using Facial Model So far we have concentrated on how we can extract the muscle actuations of an observed expression. <p> This database is substantially larger than that used by Mase [8] in his pioneering work on recognizing facial expressions. Although our database is smaller than that of Yacoob and Davis <ref> [19] </ref>, we believe that it is sufficiently large to demonstrate that we have achieved improved accuracy at facial expression recognition. 5 Recognition of Facial Expressions We will now discuss how we use our representations of facial motion for recognition of facial expressions.
References-found: 18

