URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/maclin.aaai94.ps.Z
Refering-URL: http://www.cs.wisc.edu/~maclin/abstract-ml94-robot-learning.html
Root-URL: 
Email: Email: fmaclin,shavlikg@cs.wisc.edu  
Title: Incorporating Advice into Agents that Learn from Reinforcements  
Author: Richard Maclin Jude W. Shavlik 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Dept., University of Wisconsin  
Note: Appears in the Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94).  
Abstract: Learning from reinforcements is a promising approach for creating intelligent agents. However, reinforcement learning usually requires a large number of training episodes. We present an approach that addresses this shortcoming by allowing a connectionist Q-learner to accept advice given, at any time and in a natural manner, by an external observer. In our approach, the advice-giver watches the learner and occasionally makes suggestions, expressed as instructions in a simple programming language. Based on techniques from knowledge-based neural networks, these programs are inserted directly into the agent's utility function. Subsequent reinforcement learning further integrates and refines the advice. We present empirical evidence that shows our approach leads to statistically-significant gains in expected reward. Importantly, the advice improves the expected reward regardless of the stage of training at which it is given. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P., & Chapman, D. </author> <year> 1987. </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> AAAI-87, </booktitle> <pages> 268-272. </pages>
Reference: <author> Barto, A., Sutton, R., & Watkins, C. </author> <year> 1990. </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M., & Moore, J., eds., </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press. </publisher>
Reference: <author> Berenji, H., & Khedkar, P. </author> <year> 1992. </year> <title> Learning and tuning fuzzy logic controllers through reinforcements. </title> <journal> IEEE Trans. on Neural Networks 3 </journal> <pages> 724-740. </pages>
Reference: <author> Clouse, J., & Utgoff, P. </author> <year> 1992. </year> <title> A teaching method for reinforcement learning. </title> <booktitle> Proc. 9th Intl. ML Conf., </booktitle> <pages> 92-101. </pages>
Reference: <author> Cohen, P., & Feigenbaum, E. </author> <year> 1982. </year> <booktitle> The Handbook of Artificial Intelligence, </booktitle> <volume> Vol. 3. </volume> <publisher> William Kaufmann. </publisher>
Reference-contexts: In connectionist Q-learning, the utility function is implemented as a neural network, whose inputs describe the current state and whose outputs are the utility of each action. We now return to the task of advice-taking. Hayes-Roth, Klahr, and Mostow (1981) <ref> (also see pg. 345-349 of Cohen & Feigenbaum 1982) </ref> described the steps involved in taking advice. In the following subsections, we state their steps and discuss how we propose each should be achieved in the context of RL. Step 1. Request the advice.
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> 1994. </year> <title> A multistrategy learning scheme for agent knowledge acquisition. </title> <booktitle> Infor-matica 17 </booktitle> <pages> 331-346. </pages>
Reference: <author> Hayes-Roth, F., Klahr, P., & Mostow, D. J. </author> <year> 1981. </year> <title> Advice-taking and knowledge refinement: An iterative view of skill acquisition. </title> <editor> In Anderson, J., ed., </editor> <title> Cognitive Skills and their Acquisition. </title> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Lin, L. </author> <year> 1992. </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 293-321. </pages>
Reference-contexts: We also intend to evaluate the use of "replay" (i.e., periodic retraining on remembered pairs of states and reinforcements), a method that has been shown to greatly reduce the number of training examples needed to learn a policy function <ref> (Lin 1992) </ref>. There are a number of research efforts that are related to our work. Clouse and Utgoff (1992), Lin (1992), and Whitehead (1991) developed methods in which an advisor provides feedback to the learner the advisor evaluates the chosen action or suggests an appropriate action.
Reference: <author> Lin, L. </author> <year> 1993. </year> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> Proc. 10th Intl. ML Conf., </booktitle> <pages> 182-189. </pages>
Reference-contexts: Our work, which extends knowledge-based neural networks to a new task and shows that "domain theories" can be supplied piecemeal, is similar to our earlier work with the fskbann system <ref> (Maclin & Shavlik 1993) </ref>. Fskbann extended kbann to deal with state units, but it does not create new state units. Gordon and Subramanian (1994) developed a system similar to ours. Their agent accepts high-level advice of the form if conditions then achieve goal.
Reference: <author> Maclin, R., & Shavlik, J. </author> <year> 1993. </year> <title> Using knowledge-based neural networks to improve algorithms. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 195-215. </pages>
Reference-contexts: Our work, which extends knowledge-based neural networks to a new task and shows that "domain theories" can be supplied piecemeal, is similar to our earlier work with the fskbann system <ref> (Maclin & Shavlik 1993) </ref>. Fskbann extended kbann to deal with state units, but it does not create new state units. Gordon and Subramanian (1994) developed a system similar to ours. Their agent accepts high-level advice of the form if conditions then achieve goal.
Reference: <author> Maclin, R., & Shavlik, J. </author> <year> 1994. </year> <title> Incorporating advice into agents that learn from reinforcements. </title> <type> Technical Report 1227, </type> <institution> CS Dept., Univ. of Wisconsin-Madison. </institution>
Reference-contexts: We will use it to illustrate the process of integrating advice into a neural network. The left column contains advice in our programming language, and the right shows the effects of the advice. A grammar for our advice language appears elsewhere <ref> (Maclin & Shavlik 1994) </ref>. We have made three extensions to the standard kbann algorithm: (i), we allow advice that contains multi-step plans; (ii), advice can contain loops; (iii), advice can refer to previously defined terms.
Reference: <author> Mahadevan, S., & Connell, J. </author> <year> 1992. </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> Artificial Intelligence 55 </booktitle> <pages> 311-365. </pages>
Reference: <author> McCarthy, J. </author> <year> 1958. </year> <title> Programs with common sense. </title> <booktitle> Proc. Symp. on the Mech. of Thought Processes, </booktitle> <volume> Vol. 1, </volume> <pages> 77-84. </pages>
Reference: <author> Mostow, D. J. </author> <year> 1982. </year> <title> Transforming declarative advice into effective procedures: A heuristic search example. </title> <editor> In Michal-ski, R., Carbonell, J., & Mitchell, T., eds., </editor> <booktitle> Machine Learning: An AI Approach, </booktitle> <volume> Vol. 1. </volume> <publisher> Tioga Press. </publisher>
Reference: <author> Nilsson, N. </author> <year> 1994. </year> <title> Teleo-reactive programs for agent control. </title>
Reference: <author> J. </author> <booktitle> of Artificial Intelligence Research 1 </booktitle> <pages> 139-158. </pages>
Reference: <author> Siegelmann, H. </author> <year> 1994. </year> <booktitle> Neural programming language. AAAI-94, this volume. </booktitle>
Reference: <author> Sutton, R. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference: <author> Sutton, R. </author> <year> 1991. </year> <title> Reinforcement learning architectures for animats. </title> <editor> In Meyer, J., & Wilson, S., eds., </editor> <booktitle> From Animals to Animats. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Step 5. Judge the value of the advice. We currently rely on Q-learning to "wash out" poor advice. One can also envision that in some circumstances - such as a game-learner that can play against itself (Tesauro 1992) or when an agent builds an internal world model <ref> (Sutton 1991) </ref> it would be straightforward to empirically evaluate the new advice.
Reference: <author> Tesauro, G. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning 8 </booktitle> <pages> 257-277. </pages>
Reference-contexts: Step 5. Judge the value of the advice. We currently rely on Q-learning to "wash out" poor advice. One can also envision that in some circumstances - such as a game-learner that can play against itself <ref> (Tesauro 1992) </ref> or when an agent builds an internal world model (Sutton 1991) it would be straightforward to empirically evaluate the new advice.
Reference: <author> Thrun, S., & Mitchell, T. </author> <year> 1993. </year> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> IJCAI-93, </booktitle> <pages> 930-936. </pages>
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> 1990. </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> AAAI-90, </booktitle> <pages> 861-866. </pages>
Reference-contexts: Integrate the reformulated advice into the agent's current knowledge base. We use ideas from knowledge-based neural networks to directly install the operationalized advice into the connectionist representation of the utility function. In one such approach, kbann <ref> (Towell, Shavlik, & Noordewier 1990) </ref>, a set of propositional rules are re-represented as a neural network. Kbann converts a ruleset into a network by mapping the "target concepts" of the ruleset to output units and creating hidden units that represent the intermediate conclusions.
Reference: <author> Watkins, C. </author> <year> 1989. </year> <title> Learning from Delayed Rewards. </title> <type> Ph.D. Dissertation, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference-contexts: In our augmentation, an observer watches the learner and periodically provides advice, which is then incor-porated into the action-choosing module (the advice is refined based on subsequent experience). In Q-learning <ref> (Watkins 1989) </ref> the action-choosing module is a utility function that maps states and actions to a numeric value. The utility value of a particular state and action is the predicted future (discounted) reward that will be achieved if that action is taken by the agent in that state.
Reference: <author> Whitehead, S. </author> <year> 1991. </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> AAAI-91, </booktitle> <pages> 607-613. </pages>
References-found: 24

