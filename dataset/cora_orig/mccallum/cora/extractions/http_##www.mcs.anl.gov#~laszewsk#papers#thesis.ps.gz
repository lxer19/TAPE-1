URL: http://www.mcs.anl.gov/~laszewsk/papers/thesis.ps.gz
Refering-URL: http://www-fp.mcs.anl.gov/~gregor/papers/
Root-URL: http://www.mcs.anl.gov
Email: Diplom Informatiker  
Title: A Parallel Data Assimilation System and its Implications on a Metacomputing Environment  
Author: by Gregor von Laszewski 
Note: October 1996  
Address: Bonn, Germany, 1987 M.S, University of Bonn, Germany, 1990  
Affiliation: B.S, University of  
Abstract: of Dissertation 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> NCCS Science Highlights: </editor> <booktitle> Earth and Space Sciences, Supercomputing and Mass Storage Applications, </booktitle> <year> 1994. </year>
Reference-contexts: Data Analysis in Atmospheric Science mospheric events, like global warming <ref> [1] </ref>. Due to the similar nature of climate and weather forecast models, many overlaps exist between them. Often, weather forecast models are based on a limited and smaller scaled climate model simulation. Unfortunately, the atmosphere is the most variable component of an earth climate system.
Reference: [2] <author> Ahmad, I. </author> <title> Dynamic Load Balancing for Large Distributes and Massively Parallel Mul-ticomputer Systems. </title> <type> PhD thesis, </type> <institution> Computer Science Department at Syracuse University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: If more than one task is left in the region, the work is equally split in half. Similar strategies based on igloo or other decompositions can be derived. Many possible algorithms and strategies for dynamical task scheduling can be found in literature <ref> [2, 33] </ref>. 60 4. <p> The central roll of only one host can be relaxed, while partitioning the set of nodes and providing each set with its own host. This can be done in a hierarchical manner, in order to reduce communication between hosts <ref> [2] </ref> (see Section4.7.1). 3. The elimination of the host processor and the generation of a completely decentralized version of the OI (see Section4.7.1). The decentralization can be achieved by an initial static mapping, as introduced earlier, and by nearest neighbor communication of the workload between the nodes itself.
Reference: [3] <author> Akl, S. G. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> New Jersy, </address> <year> 1989. </year>
Reference-contexts: Thus, it is useful to consider a parallel abstraction model, which is supported on as many machines as possible. The abstraction models for most of the existing parallel machines are either based on the message passing model or the data parallel model <ref> [47, 3] </ref>. In a message passing program, parallelism is expressed explicitly with the help of communicating processes. Each process works independently and can exchange messages with other processes. In a data parallel program, parallelism is expressed implicitly. The data is divided amongst the processors.
Reference: [4] <author> Amdahl, G. M. </author> <title> Validity of the single processor approach to achieving large-scale computing capabilities. </title> <booktitle> In AFIPS Conference Proceedings (1967), </booktitle> <publisher> AFIPS Press. </publisher>
Reference-contexts: It was surprising to find out, that the optimal interpolation algorithm did not consume most of the CPU time, but a report generating routine which is an integral part of the quality control algorithm. Following Amdahls Law, it was important to optimize this routine. Amdahl's law states <ref> [47, 4] </ref>: If an inherently sequential component of a problem takes a fraction ff of the time on a single node, then one can never achieve a speedup factor greater than 1 ff , no matter how many processors are used.
Reference: [5] <editor> AVS 4.0 Developer's Guide and User's Guide, </editor> <month> May </month> <year> 1992. </year>
Reference-contexts: This task is controlled by a global process manager. HeNCE lacks virtual machine management facilities, since its primary goal is to simplify the programming task. AVS The Abstract Visualization System (AVS) is a commercial visualization environment available on a wide variety of compute platforms <ref> [5, 110] </ref> . It is based on the software engineering definition of dataflow. A flow network of autonomous processes is generated visually. Data is passed between input and output ports of the processes connected via edges. Each process module fires as soon as its inputs are present.
Reference: [6] <author> Baker, M., Fox, G. C., and Yau, H. </author> <title> Cluster Computing Review. </title> <type> Tech. Rep. </type> <year> 1995, </year> <note> Center for Research on Parallel Computation, </note> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: One way to solve this problem is to use a global queue and to submit all jobs to the global queue. The decision regarding on which machine is executed, is performed by this queue. The batch operating software CODINE is able to do this <ref> [22, 6] </ref>. If this queuing system is not installed for the machine, as it is in the current setup of the metacomputer, the situation is more complex. Thus, the jobs are simply replicated on each one of the machines. The first job completed causes the other jobs to be terminated.
Reference: [7] <author> Baker, W. E., Bloom, S. C., Woollen, J. S., Nestler, M. S., and Brin, E. </author> <title> Experiments with a Three-Dimensional Statistical Objective Analysis Scheme Using FGGE Data. </title> <journal> Monthly Weather Review 115, </journal> <month> 1 </month> <year> (1987). </year>
Reference-contexts: An exact specification of these complex functions and their derivation can be found in <ref> [88, 26, 7] </ref>. <p> Currently, a method called optimal interpolation algorithm (OI) is used as part of the operational integrated data assimilation system <ref> [7] </ref>. 2.6. The Operational NASA Four Dimensional Data Assimilation System 17 At present, one six hourly analysis incorporates approximately 100,000 observations.
Reference: [8] <author> Baumgardner, J. R., and Frederickson, P. O. </author> <title> Icosahedral Discretization of the Two-Sphere. </title> <note> SIAM Journal of Numerical Analysis 22, 6 (Dec. </note> <year> 1985), </year> <pages> 1107-1115. </pages>
Reference-contexts: This is not necessary in polar coordinates where the difference between two points can be derived while using multiplications and additions. Icosahedral Coordinate System A way to improve the spatial and physical representation is to map the atmospheric fields and the minivolumes into an icosahedral grid <ref> [60, 8, 129] </ref>. Here, the distance between grid points is almost preserved [13]. Minivolumes and model variables are distributed regularly over the physical domain. In the traditional sense (Fortran90D/HPF), this distribution has to be regarded as irregular because the domain is represented by the nearest neighbor graph. <p> An icosahedral grid is constructed from an icosahedron (20 faces and 12 vertices). A grid is obtained by dividing the edges of the icosahedral into equal lengths and create new smaller equilateral triangles in the plane, and then project on the sphere <ref> [8, 60] </ref>. Different schemes are possible, which result in the same number of points on the sphere. There are 2 + 10n 2 nodes and 20n 2 faces in the k th refinement of the triangulation, where n = 2 k .
Reference: [9] <author> Beguelin, A., Dongarra, J., Geist, G. A., Manchek, R., and Sunderam, V. </author> <title> A user's guide to PVM: Parallel virtual machine. </title> <type> Tech. Rep. TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1991. </year> <note> 172 BIBLIOGRAPHY 173 </note>
Reference-contexts: Message Passing: For the intercommunication library, the standard Message Passing Interface (MPI) is used [82]. The routines used for message passing are chosen to be as simple as possible, so that a replacement of the underlying message passing library is easy. Therefore, a port to PVM <ref> [9, 10] </ref> is easily possible while replacing the elementary message passing calls. Due to the design and extensions of the message passing libraries used, the use of heterogeneous computing environments is also possible. MPI will provide an efficient interface to supercomputers.
Reference: [10] <author> Beguelin, A., Dongarra, J., Geist, G. A., Manchek, R., and Sunderam, V. </author> <title> A user's guide to PVM 3.0: Parallel virtual machine. </title> <type> Tech. Rep. TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: Message Passing: For the intercommunication library, the standard Message Passing Interface (MPI) is used [82]. The routines used for message passing are chosen to be as simple as possible, so that a replacement of the underlying message passing library is easy. Therefore, a port to PVM <ref> [9, 10] </ref> is easily possible while replacing the elementary message passing calls. Due to the design and extensions of the message passing libraries used, the use of heterogeneous computing environments is also possible. MPI will provide an efficient interface to supercomputers.
Reference: [11] <author> Bic, L., and Gaudiot, J., Eds. </author> <note> Special Issue: Dataflow Processing (1990), vol. 10 of Journal of Parallel and Distributed Computing. </note>
Reference-contexts: Next, the important concept of dataflow and its use in literature is described in more detail, because it is integral part of the metacomputing environment. 7.6 Dataflow Concept The term dataflow has different meanings depending on the context in which it is used <ref> [68, 11] </ref>. In software engineering, it refers to the flow of information between data processing units. 112 7. Metaproblem, Metacomputing, and Dataflow Concept Already in 1966, the term dataflow has been used in the context of parallel computing [71].
Reference: [12] <author> Bjerknes, V. </author> <title> Dynamic Meteorology and Hydrography. </title> <institution> Carnegie Institute, Gibson Bros., </institution> <address> New York, </address> <year> 1911. </year>
Reference-contexts: The goal is to be as precise as possible. A major step towards an automated forecast system has been achieved by Richardson, in his ground breaking publication [91]. He used a finite difference form to integrate forward in time, obtaining a forecast from an initial state (originally published in <ref> [12] </ref>). The difference scheme is applied on a regular latitude-longitude grid of fixed size. A set of different variables are used to describe the governing equations, e.g. pressure, height, the three components of the wind (often abbreviated as u-, v-, and z-component), and many more.
Reference: [13] <author> Brouwer, A. E., Cohen, A. M., and Neumair, A. </author> <title> Distance-Regular Graphs. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Icosahedral Coordinate System A way to improve the spatial and physical representation is to map the atmospheric fields and the minivolumes into an icosahedral grid [60, 8, 129]. Here, the distance between grid points is almost preserved <ref> [13] </ref>. Minivolumes and model variables are distributed regularly over the physical domain. In the traditional sense (Fortran90D/HPF), this distribution has to be regarded as irregular because the domain is represented by the nearest neighbor graph.
Reference: [14] <author> Browne, J. C., Hyder, S. I., Dongarra, J., Moore, K., and Newton, P. </author> <title> Visual Programming and Debugging for Parallel Computing. </title> <booktitle> IEEE Parallel and Distributed Technology 3, </booktitle> <month> 1 (Spring </month> <year> 1995). </year>
Reference-contexts: Dataflow, a Multiparadigm Program Notation 145 Using a message passing paradigm, makes the program even more complicated, as shown in Program 8.2. Writing the program in a dataflow language makes the program notation even more difficult because firing rules have to be established <ref> [61, 86, 14] </ref>. The power of dataflow computation becomes more obvious if a language is defined which implicitly generates the firing rules. Program 8.3 shows the notation in a language using such implicit semantic rules. <p> Nevertheless, it extends the usage towards a realistic metacomputer while providing a database of performance predictions, which guides the selection and mapping of programming tasks to selectable resources. A more detailed description of visual programming and their applications in parallel computing can be found in literature <ref> [14, 86] </ref>. A short summary of tools which will have an impact on the further improvement of the metacomputing environment are listed next.
Reference: [15] <institution> Center for Analysis and Prediction of Storms. Advanced Regional Prediction System, </institution> <note> version 3.0 ed. </note> <institution> University of Oklahoma, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The different demands in space and time is also the reason for deriving different computational simulation models. Weather forecast models are used for medium scale and length atmospheric events, like the occurrence of tornados <ref> [15] </ref>. The results of the simulation are presented to us daily over numerous TV cable channels. Large scale climate models provide the opportunity to study long term at 6 2.1. <p> In the case of data coupling, it is comparable with the other programs. Unfortunately, there is not sufficient data currently available to compare the external characteristics of the codes. In comparison with the CAPS storm prediction code <ref> [115, 15] </ref>, the DAO code rates far below. The CAPS code documentation is available for each of the routines. The sequential parts of the program have also been redesigned in consideration of the implementation on MIMD and SIMD supercomputers [93].
Reference: [16] <author> Chandy, K. M., and Kesselman, C. </author> <title> Compositional C++: Compositional Parallel Programming. </title> <type> Tech. rep., </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: The overlap is defined by fl G d (p). Besides the specification of these functions, a set of redistribution routines are provided, which automatically extract and redistribute the required observations onto the different processors. For future implementations, more sophisticated redistribution libraries for this task can be found in <ref> [94, 16] </ref>. 4.6.2 Overlap region Now, that a better understanding of the distribution is provided, a closer look at one of the processors will explain the relationship between the regions assigned to different processors (Figure 4.4).
Reference: [17] <author> Chen, M., Cowie, J., Fox, G. C., Furmanski, W., and Rebbi, C. WebWork: </author> <title> Integrated Programming Environment Tools for National Grand Challenges. </title> <type> Tech. Rep. </type> <institution> CRPC-TR95614, Center for Research on Parallel Computation, Rice University, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: While the original prototype of the metacomputing editor was developed in Tcl/Tk, Perl, CGI and Python, the current implementation of the interface, is based on Java [87, 127, 111, 77, 38]. This simplifies the expansion towards the WWW driven usage of the interface, as suggested in the MetaWeb project <ref> [17] </ref>. Using Java also reduces the number of software packages involved in the core implementation of the computing environment, thus making the environment better maintainable. Even though Java is supposed to be platform independent, the usage of threads is currently not. On different platforms, preemptive or non-preemptive scheduling is used.
Reference: [18] <author> Cheng, D. Y. </author> <title> A Survey of Parallel Programming Languages and Tools. </title> <type> Tech. Rep. </type> <institution> RND-93-005, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> Mar. </month> <year> 1993. </year>
Reference: [19] <author> Cheng, G. </author> <title> A Dataflow-based Software Integration Model in Parallel and Distributed Computing and Applications. </title> <type> PhD thesis, </type> <institution> Syracuse University, </institution> <year> 1996. </year>
Reference-contexts: Dataflow Concept 113 this graph is referred to as a dataflow graph. Such a dataflow model can be implemented in several different ways. The easiest abstraction model uses synchronous channels between the processing nodes, as introduced in <ref> [19] </ref>. Nevertheless, we find this abstraction model too restrictive because it guarantees only synchronized flow of data between the processing units. Thus, it is similar to the classical static dataflow model.
Reference: [20] <author> Choi, J., Dongarra, J. J., Pozo, R., and Walker, D. W. </author> <title> Scalapack: A scalable linear algebra library for distributed memory concurrent computers. </title> <booktitle> In Proceeding of the Fourth Symposium on the Frontiers of Massively Parallel Computation (1992), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 120-127. 174 BIBLIOGRAPHY </pages>
Reference-contexts: It is possible to integrate other standard libraries, like ScaLAPACK <ref> [20] </ref>, to enhance the function of the middle layer of the metacomputing editor. While the original prototype of the metacomputing editor was developed in Tcl/Tk, Perl, CGI and Python, the current implementation of the interface, is based on Java [87, 127, 111, 77, 38].
Reference: [21] <institution> The Connection Machine CM-5 Technical Summary, </institution> <month> Oct. </month> <year> 1991. </year>
Reference: [22] <author> CODINE. </author> <note> http://www.genias.de/genias/english/codine/codine.html. </note>
Reference-contexts: One way to solve this problem is to use a global queue and to submit all jobs to the global queue. The decision regarding on which machine is executed, is performed by this queue. The batch operating software CODINE is able to do this <ref> [22, 6] </ref>. If this queuing system is not installed for the machine, as it is in the current setup of the metacomputer, the situation is more complex. Thus, the jobs are simply replicated on each one of the machines. The first job completed causes the other jobs to be terminated.
Reference: [23] <author> Cohn, S. E., Sivakumarun, N., and Todling, R. </author> <title> Experiments with a Three-Dimensional Statistical Objective Analysis Scheme Using FGGE Data. Monthly Weather Review (Dec. </title> <booktitle> 1994), </booktitle> <pages> 2838-2867. </pages>
Reference-contexts: For example, a multivariate optimal interpolation algorithm [95, 26], a global analysis algorithm, called the Physical-space Statistical Analysis System (PSAS)[98, 57], and the Kal-man Filter <ref> [23, 80] </ref>, are among the data assimilation strategies. Currently, a method called optimal interpolation algorithm (OI) is used as part of the operational integrated data assimilation system [7]. 2.6. The Operational NASA Four Dimensional Data Assimilation System 17 At present, one six hourly analysis incorporates approximately 100,000 observations.
Reference: [24] <author> Cooperation, I. </author> <title> IBM Visualization Data Explorer (DX). </title> <address> http://www-i.almaden.ibm.com/dx/. </address>
Reference-contexts: A global control manager coordinates the data transfer mechanism which is internally carried out by an Interprocess Communication (single machine) or Remote Procedure Call mechanism (distributed machine). Other similar systems to AVS, are Explorer from SGI, Data Explorer from IBM, and Khoros <ref> [73, 97, 24] </ref>. 8.7.2 Metacomputing Legion The Legion project is pursued at University of Virginia [76]. It attempts to create system services for wide-area assemblies of workstations and supercomputers which provide the illusion of a single machine. The Legion system is an object-oriented system based on C++.
Reference: [25] <author> Cray. </author> <title> Cray Performance Optimization Manual. man performance on a Cray. </title>
Reference-contexts: The performance data on the Cray is obtained with the help of the hardware performance monitoring (hpm) routine. A detailed description of the values shown in the table can be found in the technical manual <ref> [25] </ref>. Most of the timing numbers depicted in Table 6.2 will not be explained further. We leave those numbers in the table, in order to provide a complete overview. The important numbers are the number of floating point operations and instructions, as executed during the program run.
Reference: [26] <author> Daley, R. </author> <title> Atmospheric Data Analysis. Cambridge Atmospheric and Space Science Series, </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: The state of the atmosphere changes every moment. Deriving hourly, daily, seasonal and long term forecasts, helps to prepare for the changes in the state of the atmosphere. The difference in the space and time scale phenomena of the earth climate system are of large variety (Figure 2.1 <ref> [26] </ref>). On the smallest scale, turbulence is studied, while on the larger scale, changes in the climate and the CO 2 values are examined. The region of interest for weather and climate models ranges from about 10 km to 36,000 km, the circumference of the earth. <p> A schematic closeup is shown on the left side of Figure 2.4. In this Figure, the values at the grid points are unknown. The observations are used to obtain the values. Many possible solutions for obtaining the initial values have been introduced in literature <ref> [26] </ref>. 2.3 Optimal Interpolation One of the most successful and often used objective analysis methods, is a technique called optimal interpolation. This technique was introduced by Eliassen [34] and Gandin [55]. It uses a statistical process based on mean square minimization to obtain the missing values for the computational grid. <p> An exact specification of these complex functions and their derivation can be found in <ref> [88, 26, 7] </ref>. <p> This process is known as multivariate analysis. In the multivariate case, the correlation terms and the weights in the Equation (2.5) become matrices instead of vectors. For a complete derivation of a multivariate optimal interpolation algorithm, we refer to <ref> [26] </ref>. The extensive parameter set, as used in an operational OI algorithm, is described in [88]. 2.4 Quality Control Besides using the optimal interpolation strategy, it is of utmost importance to ensure the quality of the observations which are considered in the actual analysis. <p> Applications of Data Analysis 15 the initial data and the actual predicted atmospheric state, it is necessary to check the input observations and correct or reject erroneous data from the analysis. Certain errors can be corrected quite easily, leading to an overall improved quality of the analysis. In <ref> [26] </ref>, details about the numerous errors of the different instruments can be found. 2.5 Applications of Data Analysis Besides finding values to initialize a climate model, data analysis can be used in a much broader sense. <p> For example, a multivariate optimal interpolation algorithm <ref> [95, 26] </ref>, a global analysis algorithm, called the Physical-space Statistical Analysis System (PSAS)[98, 57], and the Kal-man Filter [23, 80], are among the data assimilation strategies. Currently, a method called optimal interpolation algorithm (OI) is used as part of the operational integrated data assimilation system [7]. 2.6. <p> The incremental update improves the analysis, e.g., in terms of accuracy and noise control. A useful term to describe the time at which observation values are taken, are the so called synoptic times. Surface and radiosond observations are taken regularly at synoptic times <ref> [26] </ref>. They are 00 and 12 GMT (Greenwich mean time) for radiosonds, and 00, 03, ..., 21, 24 28 2. Data Analysis in Atmospheric Science GMT for surface observations. Satellite observations are recorded continuously. They are asynoptic.
Reference: [27] <institution> Data Online Monitoring System (DOLMS). </institution> <address> http://dao.gsfc.nasa.gov/restricted links/monitoring, </address> <month> Sept. </month> <year> 1996. </year> <title> Restricted access for GSFC. </title>
Reference-contexts: Download can be activated via anonymous FTP, incorporated in an HTML page, which is accessible on the World Wide Web (WWW). A step in this direction for the NASA DAS is the planned Data Online-Monitoring System (DOLMS) by DAO <ref> [27] </ref>. Here, the selection of single GIF images are used to distribute results. The amount of data involved for displaying, requires a fast data visualization package. Providing a WWW interface to a sequence of downloadable GIF pictures will not provide the speed required.
Reference: [28] <author> DaSilva, A. </author> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: Fortunately, there is enough memory available in current supercomputers to support this strategy for the desired problem instances. In addition, it is expected that the production version for the next few years will not have more then 150,000 observations. Preprocessing in a separate program will ensure this <ref> [28] </ref>. Storage Pattern (Global Grid, Local Observation) To reduce the memory usage in each processor, only observations necessary for the calculation are stored. Due to the smaller number of observations stored in a processor, the algorithm used to determine observations in the vicinity of a point is significantly faster.
Reference: [29] <institution> World Wide Web Cite of the Data Assimilation Office (DAO), NASA Goddard Space Flight Center. </institution> <address> http://dao.gsfc.nasa.gov/restricted links/monitoring, </address> <month> Sept. </month> <year> 1996. </year> <title> Restricted access for GSFC. </title>
Reference: [30] <author> Dennis, J. B. </author> <title> First Version of a Data Flow Procedure Language. </title> <booktitle> vol. 19 of Lecture Notes in Computer Science, </booktitle> <pages> pp. 362-376. </pages>
Reference-contexts: Metaproblem, Metacomputing, and Dataflow Concept Already in 1966, the term dataflow has been used in the context of parallel computing [71]. Besides the theoretical acceptance of dataflow models in the late 60's and early 70's [70], dataflow had also an impact on computer architecture designs <ref> [30, 31] </ref>. In general, a program is evaluated with the help of a computing model. In the case of a von Neuman machine, the computing model is control-flow oriented. A program is evaluated step by step in a processor. The control is transfered from one command to the next command.
Reference: [31] <author> Dennis, J. B. </author> <title> A preliminary architecture for a basic data-flow Processor. </title> <booktitle> In ACM Proceedings of the Second Annual Symposium on Compute Architecture (Jan. </booktitle> <year> 1975), </year> <pages> pp. 126-132. </pages>
Reference-contexts: Metaproblem, Metacomputing, and Dataflow Concept Already in 1966, the term dataflow has been used in the context of parallel computing [71]. Besides the theoretical acceptance of dataflow models in the late 60's and early 70's [70], dataflow had also an impact on computer architecture designs <ref> [30, 31] </ref>. In general, a program is evaluated with the help of a computing model. In the case of a von Neuman machine, the computing model is control-flow oriented. A program is evaluated step by step in a processor. The control is transfered from one command to the next command.
Reference: [32] <author> Dongara, J. </author> <title> Performance of various computers using standard linear equation software. </title> <type> Tech. Rep. </type> <institution> CS-89-85, Oak Ridge National Laboratory, </institution> <month> Oct. </month> <year> 1985,1996. </year>
Reference-contexts: The main memory currently contains 256 Mbytes. An external disk of 1.0GB is available for each node. The nodes of the SP2 operate in time-sharing mode. The DEC 3000/400 Alphas and the RS6000 are rated individually, as depicted in Table 6.1 <ref> [32] </ref>. Table 6.1: Some Performance characteristics of a single Alpha and SP2 node. DEC Alpha RS6000 Model 3000/400 390 SPECint92 74.7 114.3 SPECfp92 112.5 205.3 LINPACK 1000x1000 90 MFlops/s 181.0 MFlops/s LINPACK 100x100 26 MFlops/s 53.0 MFlops/s 6.1.
Reference: [33] <author> El-Rewini, H., Lewis, T. G., and Ali, H. H. </author> <title> Task Scheduling in Parallel and Distributed Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year> <note> BIBLIOGRAPHY 175 </note>
Reference-contexts: If more than one task is left in the region, the work is equally split in half. Similar strategies based on igloo or other decompositions can be derived. Many possible algorithms and strategies for dynamical task scheduling can be found in literature <ref> [2, 33] </ref>. 60 4.
Reference: [34] <author> Eliassen, A. </author> <title> Provisional Report on Calculation of Spatial Covariance and Autocorrelation of the Pressure Field. </title> <type> Tech. rep., </type> <institution> Videnskaps-Akademiet, Institut for Vaer og Klimaforskning, Oslo, </institution> <year> 1954. </year>
Reference-contexts: The observations are used to obtain the values. Many possible solutions for obtaining the initial values have been introduced in literature [26]. 2.3 Optimal Interpolation One of the most successful and often used objective analysis methods, is a technique called optimal interpolation. This technique was introduced by Eliassen <ref> [34] </ref> and Gandin [55]. It uses a statistical process based on mean square minimization to obtain the missing values for the computational grid. The idea behind optimal interpolation is to take a first guess for the fields and observations.
Reference: [35] <institution> Executive Office of the President, Office of Science and Technology Policy. A Research and Development Strategy for High Performance Computing, </institution> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: The more resources are available to the scientific community, the bigger the problems are to be solved or the faster solutions can be achieved. The development of this new infrastructure has been strongly driven by the scientific research community. One of the problems scientists face is solving grand challenges <ref> [35] </ref>. Grand challenge problems are problems which have to be solved on parallel computers, if at all. Combining several supercomputers available on the WWW will provide more computational resources for researchers involved with grand challenge applications. <p> Not only do climate models contain complicated equations, but also perform calculations on large amounts of data sets. Thus, computers with enormous computational power and storage capacity are needed to calculate a prediction of only small complexity. Climate modeling is classified as one of the grand challenge problems <ref> [35] </ref> because of its scientific value and the amount of computational resources necessary to pursue the calculation (Figure 2.2). The calculation cannot be performed on any single existing computer in sufficient time.
Reference: [36] <author> Fenton, N. E. </author> <title> Software Metrics: A Rigorous Approach. </title> <publisher> Chapman and Hall, </publisher> <year> 1991. </year>
Reference-contexts: Control flow complexity F evaluates the number of binary decisions in a subroutine <ref> [36] </ref>. Naturally, the metrics only indicate a problem when its value is high in contrast to a comparable code. The code in question might be more complex than necessary [62]. Figures 3.1-3.4 contain the values for the codes from ECMWF, CSRIO, and BEST [74].
Reference: [37] <author> Fenton, N. E. </author> <title> Software Assessment: A necessary scientific basis. </title> <journal> Trns. Soft. Eng. </journal> <volume> 3, 20 (1994), </volume> <pages> 199-206. </pages>
Reference-contexts: A simple way of comparing codes is given. Formally, computer software can be characterized by internal and external characteristics <ref> [37] </ref>. The internal characteristics include the size and the control flow complexity. The external characteristics contain measurements for the maintenance of the code by other researchers. It is extremely difficult to evaluate the characteristics of a code and compare them with other existing codes.
Reference: [38] <author> Flanagan, D. </author> <title> Java in a Nutshell. </title> <address> O'Reiley, </address> <year> 1996. </year>
Reference-contexts: It is possible to integrate other standard libraries, like ScaLAPACK [20], to enhance the function of the middle layer of the metacomputing editor. While the original prototype of the metacomputing editor was developed in Tcl/Tk, Perl, CGI and Python, the current implementation of the interface, is based on Java <ref> [87, 127, 111, 77, 38] </ref>. This simplifies the expansion towards the WWW driven usage of the interface, as suggested in the MetaWeb project [17]. Using Java also reduces the number of software packages involved in the core implementation of the computing environment, thus making the environment better maintainable.
Reference: [39] <author> Flynn, M. J. </author> <title> Some Computer Organizations and Their Effectiveness. </title> <journal> IEEE Trans. Computers C-21, </journal> <month> 9 (September </month> <year> 1972), </year> <pages> 948-960. </pages>
Reference-contexts: Metaproblems are a hybrid integration of several subproblems of the other four basic ap plication classes. 7.4 Metacomputer The metacomputer is a natural evolution of existing computing technology. Often, Flynn's well known classification is used to categorize parallel computers according to the instruction and data stream <ref> [39] </ref>. Here, the MIMD Distributed Memory (Multiple Instruction Multiple Data) architectures are of special interest (Figure 7.3). It is important to note that the memory is an integral part of each processing unit. The memory is local to each processing unit. Data is exchanged via message passing.
Reference: [40] <author> Forum, H. P. F. </author> <title> High Performance Fortran Language Specification. </title> <type> Tech. rep., </type> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: They can be sequential programs, as well as, parallel programs and can follow different programming paradigms. It is essential to support the integration of programming languages like Fortran, Fortran90, C, C++, and others, to achieve acceptance in the scientific community. Integrating HPF <ref> [53, 40] </ref> will be of advantage in the future. Furthermore, the metacomputing environment should enforce more strict software engineering techniques, in order to force some programmers to provide a documented code.
Reference: [41] <author> Foster, I., and Chandy, K. M. </author> <title> Fortran M: A Language for Modular Parallel Programming. </title> <type> Tech. Rep. Preprint MCS-P237-0992, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1992, 1992. </year>
Reference-contexts: This is not a new idea and has been incorporated in many programming languages, as well 110 7. Metaproblem, Metacomputing, and Dataflow Concept as, visual programming interfaces. An example for a task parallel programming language can be found in <ref> [63, 41] </ref>. Here, the design of a parallel program is often initiated by deriving the tasks and their dependencies to obtain a task graph. The task graph is then transformed to a textual form by hand.
Reference: [42] <author> Foster, I., and Kesselman, C. Globus: </author> <title> A Metacomputing Infrastructure Toolkit. </title> <type> Tech. rep., </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year> <note> http://www.globus.org. </note>
Reference-contexts: It includes a section for trace data analysis and visualization, which is not yet implemented in WAMM. Globus The Globus project at Argonne National Laboratory, is developing a basic software infrastructure for the computations that integrate geographically distributed computational and information resources <ref> [42, 43] </ref>. The I-WAY project demonstrated several components of Glo-bus. The focus of the Globus project is the development of low-level mechanisms that can be used to implement higher-level services. Furthermore, techniques that allow those services to observe and guide the execution are planned.
Reference: [43] <author> Foster, I., Kesselman, C., and Tuecke, S. </author> <title> The Nexus Approach to Integrating Multithreading and Communication. </title> <type> Tech. rep., </type> <institution> Argonne National Laboratory, </institution> <year> 1996. </year> <note> http://www.globus.org. </note>
Reference-contexts: It includes a section for trace data analysis and visualization, which is not yet implemented in WAMM. Globus The Globus project at Argonne National Laboratory, is developing a basic software infrastructure for the computations that integrate geographically distributed computational and information resources <ref> [42, 43] </ref>. The I-WAY project demonstrated several components of Glo-bus. The focus of the Globus project is the development of low-level mechanisms that can be used to implement higher-level services. Furthermore, techniques that allow those services to observe and guide the execution are planned.
Reference: [44] <author> Fox, G. </author> <title> Parallel Computing in Industry: An Initial Survey. </title> <booktitle> In Proc. of Fifth Aus-tralian Supercomputing Conference (World Congress Centre, </booktitle> <address> Melbourne, Australia, </address> <month> Dec. </month> <year> 1992). </year> <note> 176 BIBLIOGRAPHY </note>
Reference: [45] <author> Fox, G., and Furmanski, W. </author> <title> Towards Web / Java based High Performance Distributed Computing an Evolving Virtual Machine. </title> <booktitle> In IEEE Conference HPDC-5 (Aug. </booktitle> <year> 1996). </year>
Reference-contexts: The interface does not change. Thus, the bottom-up approach of SNAP, and the top-down approach for designing the user interface, are viable additions to each other. The lessons learned form the GUI environment can be used in the forthcoming WebFlow project at NPAC <ref> [46, 45] </ref>. 7.5 Motivation and Requirements for the Metacomputing En vironment The motivation for the development of a metacomputing environment arose from the rather complex problem of parallelizing the NASA Four Dimensional Data Assimilation System.
Reference: [46] <author> Fox, G., Furmanski, W., Haupt, T., and Klasky, S. WebFlow: </author> <title> A Visual Problem Solving Environment for Wide-Area, Heterogeneous, Distributed High Performance Computing. NPAC Proposal to the NSF New Technologies Program, </title> <month> July </month> <year> 1996. </year>
Reference-contexts: The interface does not change. Thus, the bottom-up approach of SNAP, and the top-down approach for designing the user interface, are viable additions to each other. The lessons learned form the GUI environment can be used in the forthcoming WebFlow project at NPAC <ref> [46, 45] </ref>. 7.5 Motivation and Requirements for the Metacomputing En vironment The motivation for the development of a metacomputing environment arose from the rather complex problem of parallelizing the NASA Four Dimensional Data Assimilation System.
Reference: [47] <author> Fox, G., Johnson, M., Lyzenga, G., Otto, S., Salmon, J., and Walker, D. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1988. </year>
Reference-contexts: Thus, it is useful to consider a parallel abstraction model, which is supported on as many machines as possible. The abstraction models for most of the existing parallel machines are either based on the message passing model or the data parallel model <ref> [47, 3] </ref>. In a message passing program, parallelism is expressed explicitly with the help of communicating processes. Each process works independently and can exchange messages with other processes. In a data parallel program, parallelism is expressed implicitly. The data is divided amongst the processors. <p> For the rest of the elements, test if they are 1600km away from a point in the interior region. The first step is performed to avoid the second, more expensive, calculation. While regular overlap regions are well understood in computer science <ref> [47, 116] </ref>, irregular overlap regions are cause for active research [66]. 4.6.3 Memory Considerations The different data distributions allow one (theoretically) to reduce the amount of data to be stored in the different processors. The original serial algorithm assumes that all variables in 4.6. <p> It was surprising to find out, that the optimal interpolation algorithm did not consume most of the CPU time, but a report generating routine which is an integral part of the quality control algorithm. Following Amdahls Law, it was important to optimize this routine. Amdahl's law states <ref> [47, 4] </ref>: If an inherently sequential component of a problem takes a fraction ff of the time on a single node, then one can never achieve a speedup factor greater than 1 ff , no matter how many processors are used.
Reference: [48] <author> Fox, G. C. </author> <title> Parallel Computers and Complex Systems. In Complex Systems '92: From Biology to Computation (1992), </title> <editor> Bossomaier and D. G. Green, Eds., </editor> <booktitle> Inaugural Australian National Conference on Complex Systems. also CRPC-TR92266. </booktitle>
Reference-contexts: Most importantly, large memory and many processing units of high speed are required. Since both resources are of limited availability, compromises in the program design have to be considered. 7.3 Metaproblems From the computational and computer science point of view, a classification of complex problems is introduced in <ref> [54, 48] </ref>. Applications for solving complex problems are distinguished. The applications are classified into 5 classes: 1 Time can also be considered as a resource. 7.4. Metacomputer 103 Synchronous applications tend to be regular and are characterized by algorithms employing simultaneous identical updates to a set of points.
Reference: [49] <author> Fox, G. C., and et al. InfoMall: </author> <title> A Scalable Organization for the Development of HPCC Software and Systems. </title> <type> Tech. Rep. </type> <institution> SCCS-531, NPAC at Syracuse University, </institution> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: It is expected that the available libraries will grow rapidly and an information exchange is implicitly achieved while using the metacomputing environment. This can be of advantage for industrial usage, where predefined solutions are often acquired <ref> [49] </ref> through rapid prototyping. The idea for such a library evolved from the extensive performance measurements for solving a system of linear equations for LU factorization with different strategies in a parallel computer using different numbers of processors.
Reference: [50] <author> Fox, G. C., and et al. InfoVision: </author> <title> Information, Video, Imagery and Simulation on Demand. </title> <type> Tech. Rep. </type> <institution> SCCS-575, NPAC at Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: Their requirements are: 1. Easy access, 2. Correctness, 142 8. The Interface and Software Layer of the Metacomputer 3. Availability of the result. On demand publishing provides the solution to this requirements. Today, WWW browsers are familiar to a wide computer user community <ref> [50] </ref>. Thus, it is natural to incorporate the access to the program solving the grand challenge via a HTML page. This page can be publicized with the help of an accessible HTTP server. Methods are included in the page to start the program, and display the result.
Reference: [51] <author> Fox, G. C., and Furmanski, W. </author> <title> Factoring on the World Wide Web Computing Project. </title> <note> http://www.npac.syr.edu/factoring.html, 1995. </note>
Reference-contexts: The different calculations to obtain the factoring are embarrassingly parallel and can be distributed over a number of machines. The result is collected and the main answer is given. More information about the system can be found in <ref> [51] </ref>. 8.8 Advantages and Problems with Metacomputers The advantages of a functioning metacomputing environment are obvious. A metacomputing environment: 1. utilizes the strength of individual computers and programming paradigms, 2. distributes a computational problem over the available resources, 3. can dynamically modify the application, 4. provides transparency for the user.
Reference: [52] <author> Fox, G. C., and Furmanski, W. </author> <title> SNAP, Crackle, </title> <type> WebWindows! Tech. Rep. </type> <institution> SCCS758, NPAC at Syracuse University, </institution> <year> 1996. </year>
Reference-contexts: Motivation and Requirements for the Metacomputing Environment 109 Since the success of the environment will be dependent on the interfaces provided, a top down approach is followed starting from the graphical user interface. Other projects <ref> [52] </ref> pursue a bottom up approach. The SNAP project pursued at NPAC will provide an infrastructure of low-end, but widely usable, applications and technology. One of the components planned, is the development of an interface to automatically control computations to be distributed on the resources of the WWW.
Reference: [53] <author> Fox, G. C., Hiranadani, S., Kennedy, K., Koelbel, C., Kremer, U., Tseng, C.-W., , and Wu, M.-Y. </author> <title> Fortran D Language Specification. </title> <type> Tech. Rep. </type> <institution> SCCS-42c, NPAC at Syracuse University, 1991. Rice University,TR90-141. </institution>
Reference-contexts: They can be sequential programs, as well as, parallel programs and can follow different programming paradigms. It is essential to support the integration of programming languages like Fortran, Fortran90, C, C++, and others, to achieve acceptance in the scientific community. Integrating HPF <ref> [53, 40] </ref> will be of advantage in the future. Furthermore, the metacomputing environment should enforce more strict software engineering techniques, in order to force some programmers to provide a documented code.
Reference: [54] <author> Fox, G. C., Williams, R. D., and Messina, P. C. </author> <title> Parallel Computing Works. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <note> http://www.npac.syr.edu/copywrite/pcw. </note>
Reference-contexts: Most importantly, large memory and many processing units of high speed are required. Since both resources are of limited availability, compromises in the program design have to be considered. 7.3 Metaproblems From the computational and computer science point of view, a classification of complex problems is introduced in <ref> [54, 48] </ref>. Applications for solving complex problems are distinguished. The applications are classified into 5 classes: 1 Time can also be considered as a resource. 7.4. Metacomputer 103 Synchronous applications tend to be regular and are characterized by algorithms employing simultaneous identical updates to a set of points.
Reference: [55] <author> Gandin, L. </author> <title> Objective Analysis of meteorological fields. </title> <address> Gridoment, Leningrad, </address> <year> 1963. </year> <title> translation to English: Israel Program for Scientific Translation, 1965. BIBLIOGRAPHY 177 </title>
Reference-contexts: Many possible solutions for obtaining the initial values have been introduced in literature [26]. 2.3 Optimal Interpolation One of the most successful and often used objective analysis methods, is a technique called optimal interpolation. This technique was introduced by Eliassen [34] and Gandin <ref> [55] </ref>. It uses a statistical process based on mean square minimization to obtain the missing values for the computational grid. The idea behind optimal interpolation is to take a first guess for the fields and observations.
Reference: [56] <author> Golub, G. H., and Loan, C. F. V. </author> <title> Matrix Computations. </title> <publisher> John Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Data Analysis in Atmospheric Science This is done with the help of a Cholesky factorization <ref> [56] </ref>. 1 As mentioned earlier, the optimal interpolation process can also be used to allow observations of one kind of variable to influence the analysis of another one. This process is known as multivariate analysis.
Reference: [57] <author> Guo, J., and da Silva, A. </author> <title> Computational Aspects of Goddard's Physical-Space Statistical Analysis System (PSAS). </title> <booktitle> In Second UNAM-CRAY Supercomputing Conference on Numerical Simulations in Environmental and Earth Sciences (Mexico City, </booktitle> <address> Mexico, </address> <month> June </month> <year> 1995). </year> <note> updated in Nov. </note> <year> 1995. </year>
Reference-contexts: The lessons learned from the parallelization of the DAO code, and its poor characteristics, motivated a change towards the complete new development of an alternative to the current objective analysis code. The new code will produce better results in comparison to the OI, but it will be slower <ref> [57] </ref>. Table 3.1 summarizes some properties of the optimal interpolation and quality control code, as used in the current objective assimilation scheme. This analysis is typical for many scientific codes. 34 3. <p> One argument that supports the existence of such an algorithm, is that in nature the physical processes happen in parallel and not in sequential order. The PSAS algorithm <ref> [57] </ref> does not distinguish between the in three different modules anymore.
Reference: [58] <author> Haupt, T., Hawick, K., and Macivic, M. </author> <title> Evaluation of HPF Compilers. </title> <type> Personal communication, </type> <institution> 1992/93. </institution>
Reference-contexts: The internal representation of an HPF program would be analog to its message passing version. Unfortunately, at the time of writing, the compilers were not stable enough to consider this solution <ref> [58] </ref>. Stable compilers will be available soon. Extensions for irregularly distributed domains will be especially useful, to describe more complex domain decompositions. Even the implementation of an icosahedral distribution would be possible. An experimental implementation of the quality control algorithm can be found in [81].
Reference: [59] <author> Haupt, T., and Klasky, S. </author> <title> Implementation of the T2 code in HPF. </title> <note> http://www.npac.syr.edu/users/haupt/bbh/HPF/index.html, 1996. </note>
Reference-contexts: Thus, the development of a message passing parallel algorithm is justified. An experimental HPF implementation of the quality control algorithm is described in [81]. Today, HPF compilers are more stable. They can be used for larger projects <ref> [59] </ref> and are a viable alternative to message passing programs for many problems. One of the research goals conducted was to find out if a program representation can be found which supports the message passing, but also, the data parallel programming paradigm.
Reference: [60] <author> Heikes, R. P. </author> <title> The Shallow Water Equations on a Spherical Geodesic Grid. </title> <type> Tech. Rep. 524, </type> <institution> Department of Atmospheric Science Colorado State University, </institution> <year> 1993. </year>
Reference-contexts: This is not necessary in polar coordinates where the difference between two points can be derived while using multiplications and additions. Icosahedral Coordinate System A way to improve the spatial and physical representation is to map the atmospheric fields and the minivolumes into an icosahedral grid <ref> [60, 8, 129] </ref>. Here, the distance between grid points is almost preserved [13]. Minivolumes and model variables are distributed regularly over the physical domain. In the traditional sense (Fortran90D/HPF), this distribution has to be regarded as irregular because the domain is represented by the nearest neighbor graph. <p> An icosahedral grid is constructed from an icosahedron (20 faces and 12 vertices). A grid is obtained by dividing the edges of the icosahedral into equal lengths and create new smaller equilateral triangles in the plane, and then project on the sphere <ref> [8, 60] </ref>. Different schemes are possible, which result in the same number of points on the sphere. There are 2 + 10n 2 nodes and 20n 2 faces in the k th refinement of the triangulation, where n = 2 k . <p> Using a coordinate system based on the location of the minivolumes can simplify the domain decomposition of the calculations associated with a minivolume. Reducing the spatial domain to minivolume based coordinate system, reduces the number of calculations necessary. Nevertheless, the forward integration of the model will lead to inaccuracies <ref> [60, 75] </ref>. 4.4.3 Coordinate Systems based on the Observational Domain Distribu tion The observational domain is significantly different from the model variables and the grid domain. As seen in Figures 2.9-2.11, the observational data is clearly irregularly distributed 4.5. Functional Decomposition 45 over the latitude-longitude grid.
Reference: [61] <editor> PVM and HeNCE Programmers Manual. </editor> <address> http://www.epcc.ed.ac.uk/epcc/- publications/cray/pvm.hence.30/index.html. </address>
Reference-contexts: Dataflow, a Multiparadigm Program Notation 145 Using a message passing paradigm, makes the program even more complicated, as shown in Program 8.2. Writing the program in a dataflow language makes the program notation even more difficult because firing rules have to be established <ref> [61, 86, 14] </ref>. The power of dataflow computation becomes more obvious if a language is defined which implicitly generates the firing rules. Program 8.3 shows the notation in a language using such implicit semantic rules. <p> CODE will produce parallel programs for a variety of architectures because its model is architecture-independent. The CODE system can produce parallel programs for machines running PVM, as well as, for the Sequent Symmetry. 8.7. Related Research 147 HeNCE HeNCE is a metacomputing environment based on PVM <ref> [61] </ref>. Its goal is to greatly simplify the software development cycle. It implements a system for source file distribution and compilation on remote nodes. Source files can be compiled in parallel on several machines. This task is controlled by a global process manager.
Reference: [62] <author> Henderson-Sellers, A., Henderson-Sellers, B., Pollard, D., Verner, J., and Pitman, A. </author> <title> Applying Software Engineering Metrics to Land Surface Parameterization Schemes. </title> <note> Journal of Climate 8 (May 1995). </note>
Reference-contexts: Usually, they are of substantial size and have evolved over a number of years. Many domain specialists are responsible for the program generation and code maintenance. These specialists normally do not apply software engineering or software quality assurance practices. Thus, many codes are very difficult to handle and maintain <ref> [62] </ref>. The assimilation system code is no exception. As the following analysis will show, the code is worse than other codes known from atmospheric science. Consequently, future codes should be developed in a more strict and controlled environment. <p> Nevertheless, a general idea can be obtained for the control flow complexity while using measurements used by other researchers in the atmospheric science community. Such measurements, specifically used for the FORTRAN atmospheric codes <ref> [62] </ref>, are displayed in the Figures 3.1-3.4. <p> Control flow complexity F evaluates the number of binary decisions in a subroutine [36]. Naturally, the metrics only indicate a problem when its value is high in contrast to a comparable code. The code in question might be more complex than necessary <ref> [62] </ref>. Figures 3.1-3.4 contain the values for the codes from ECMWF, CSRIO, and BEST [74]. In addition, the values for the major subroutines of the analysis system codes are marked. These routines are similar in function to routines found in the other codes [101].
Reference: [63] <author> Hoare, C. A. R. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice Hall, </publisher> <year> 1985. </year>
Reference-contexts: This is not a new idea and has been incorporated in many programming languages, as well 110 7. Metaproblem, Metacomputing, and Dataflow Concept as, visual programming interfaces. An example for a task parallel programming language can be found in <ref> [63, 41] </ref>. Here, the design of a parallel program is often initiated by deriving the tasks and their dependencies to obtain a task graph. The task graph is then transformed to a textual form by hand. <p> 2 ; t 1 ; t 2 ; 2 do sequential 3 w VectorShift (VectorAdd (x; y); left); 4 end 5 do parallel 6 r 1 A (w); 7 do sequential 8 do parallel 9 t 1 B (w); 11 end 13 end The first intuitive formulation of a CSP <ref> [63] </ref> like code (Program 8.1) shows that the parallel program is more complicated than the visual representation makes believe. Nested statements are used and temporary variables have to be explicitly used to express the parallelism. 8.6.
Reference: [64] <author> Hudson, A. </author> <title> Reference to Performance Data Automatically Collected on the GSFC Cray. NASA GSFC, Data Assimilation Office, </title> <type> Personal Communication, </type> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The objective analysis algorithm based on the optimal interpolation (Version 2.0) performs inbetween 165-186MFLOPS. The modified unoptimized algorithm as run on a single processor of an MIMD machine performs with 133MFlops on one processor of the Cray <ref> [64] </ref>. Performance of the Sequential Algorithm on RS6000 and Dec Alpha Workstations Table 6.3 shows the performance of the HUV analysis and the HUV quality control, as well as, the routine reading in the observation data. All times are given in seconds.
Reference: [65] <author> Hwang, K., and Briggs, F. A. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <year> 1986. </year> <booktitle> [66] 4th International Symposium on Solving Irregular Structured Problems in Parallel, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Extensions for irregularly distributed domains will be especially useful, to describe more complex domain decompositions. Even the implementation of an icosahedral distribution would be possible. An experimental implementation of the quality control algorithm can be found in [81]. Vectorized Optimal Interpolation Algorithm The parallel algorithm for vector supercomputers <ref> [103, 65] </ref> is analog to the parallel algorithms introduced for the MIMD architectures. The vectorization takes place on the loops over the minivolumes and observations, respectively. Under the assumption that the observations are randomly distributed in memory, the vec-torized program relates to the cyclic decomposition for the MIMD algorithm.
Reference: [67] <author> Isaksen, L. </author> <title> Parallelizing the ECMWF Optimum Interpolation analysis. </title> <booktitle> In Parallel Supercomputing in Atmospheric Science, Proceedings of the Fifth ECMWF Workshop on the Use of Parallel Processors in Meteorology (Nov. </booktitle> <year> 1992). </year> <note> 178 BIBLIOGRAPHY </note>
Reference-contexts: a few number of processors based on a data balanced decomposition for the quality control, can lead to superlinear speedup, as shown in Section 6.3.2. 4.9.4 The ECMWF Box Distribution At the European Center for medium Weather forecast, a different optimal interpolation algorithm has been previously parallelized for MIMD computers <ref> [67] </ref>. The motivating factor for a domain decomposition is the unevenly distributed data. As described before, the availability of the data varies from six hour to six hour period, and from day to day. The method developed at ECMWF uses a box method.
Reference: [68] <author> Jagannathan, R. </author> <title> Parallel and Distributed Computing Handbook. McGrawhill, 95, ch. Dataflow Models. </title>
Reference-contexts: Here, the design of a parallel program is often initiated by deriving the tasks and their dependencies to obtain a task graph. The task graph is then transformed to a textual form by hand. The new idea is to use the concept of dataflow <ref> [68] </ref> in a more general way. With the help of dataflow concepts, many different programming paradigms can be combined into one program. The information about which programming paradigm is used, is hidden from the users, thus enabling a uniform interface for multiple programming paradigms. <p> Next, the important concept of dataflow and its use in literature is described in more detail, because it is integral part of the metacomputing environment. 7.6 Dataflow Concept The term dataflow has different meanings depending on the context in which it is used <ref> [68, 11] </ref>. In software engineering, it refers to the flow of information between data processing units. 112 7. Metaproblem, Metacomputing, and Dataflow Concept Already in 1966, the term dataflow has been used in the context of parallel computing [71]. <p> The main difference between control-flow and dataflow computing models, is the fact that the program execution in 1. control flow programs, "corresponds to the instructions in motion operating on data at rest", 2. dataflow programs, "corresponds to data in motion being processed by operations at rest <ref> [68] </ref>." Several, dataflow models are distinguished in literature [68]. In the data-driven dataflow model, values (tokens) are produced and consumed by the nodes of the dataflow graph. A token is generated and sent to an output edge. <p> control-flow and dataflow computing models, is the fact that the program execution in 1. control flow programs, "corresponds to the instructions in motion operating on data at rest", 2. dataflow programs, "corresponds to data in motion being processed by operations at rest <ref> [68] </ref>." Several, dataflow models are distinguished in literature [68]. In the data-driven dataflow model, values (tokens) are produced and consumed by the nodes of the dataflow graph. A token is generated and sent to an output edge.
Reference: [69] <editor> Java-Linux. </editor> <address> http://substance.blackdown.org/java-linux.html, Oct. </address> <year> 1996. </year>
Reference-contexts: Furthermore, a serious bug while 130 8. The Interface and Software Layer of the Metacomputer launching runtime processes exists in several versions of Java, thus preventing reliable usage on some platforms <ref> [69] </ref>. We expect that the current version of Java (1.0.2) will be changed to overcome these problems.
Reference: [70] <author> Kahn, G. </author> <title> A Semantic of a Simple Language for Parallel Processing. </title> <booktitle> In Proceedings IFIP Congress (Amsterdam, 1974), </booktitle> <publisher> Elsvier North Holland, </publisher> <pages> pp. 471-475. </pages>
Reference-contexts: Metaproblem, Metacomputing, and Dataflow Concept Already in 1966, the term dataflow has been used in the context of parallel computing [71]. Besides the theoretical acceptance of dataflow models in the late 60's and early 70's <ref> [70] </ref>, dataflow had also an impact on computer architecture designs [30, 31]. In general, a program is evaluated with the help of a computing model. In the case of a von Neuman machine, the computing model is control-flow oriented. A program is evaluated step by step in a processor.
Reference: [71] <author> Karp, R. M., and Miller, R. E. </author> <title> Properties of a Model for Parallel Computations: Determinacy, Termination, Queuing. </title> <journal> SIAM Journal of Applied Mathematics 6, </journal> <volume> 14 (1966), </volume> <pages> 1390-1411. </pages>
Reference-contexts: In software engineering, it refers to the flow of information between data processing units. 112 7. Metaproblem, Metacomputing, and Dataflow Concept Already in 1966, the term dataflow has been used in the context of parallel computing <ref> [71] </ref>. Besides the theoretical acceptance of dataflow models in the late 60's and early 70's [70], dataflow had also an impact on computer architecture designs [30, 31]. In general, a program is evaluated with the help of a computing model.
Reference: [72] <editor> Khanna, R., Ed. </editor> <booktitle> Distributed Computing. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1994. </year> <note> [73] http://www.khoros.unm.edu/khoros/khoros2/home.html. </note>
Reference-contexts: The user of the heterogeneous computer is aware of the different machines and has to incorporate the knowledge about the distributed resources in the design of the program. Operating systems are available and under development to provide a better and automated resource management <ref> [105, 72] </ref>. The new development in computational and computer science is to 104 7.
Reference: [74] <author> Kowalczyk, E. A. </author> <title> A soilcanopy scheme for use in a numerical model for the atmosphere - 1D standalone model. </title> <type> Tech. Rep. 23, </type> <institution> CSRIO, DAR, </institution> <year> 1991. </year>
Reference-contexts: Naturally, the metrics only indicate a problem when its value is high in contrast to a comparable code. The code in question might be more complex than necessary [62]. Figures 3.1-3.4 contain the values for the codes from ECMWF, CSRIO, and BEST <ref> [74] </ref>. In addition, the values for the major subroutines of the analysis system codes are marked. These routines are similar in function to routines found in the other codes [101].
Reference: [75] <author> Kurihara, Y. </author> <title> A Finite Difference Scheme by Making Use of Primitive Equations of a Spherical Grid. </title> <journal> Monthly Weather Review 93 (69), </journal> <pages> 399-415. </pages>
Reference-contexts: Using a coordinate system based on the location of the minivolumes can simplify the domain decomposition of the calculations associated with a minivolume. Reducing the spatial domain to minivolume based coordinate system, reduces the number of calculations necessary. Nevertheless, the forward integration of the model will lead to inaccuracies <ref> [60, 75] </ref>. 4.4.3 Coordinate Systems based on the Observational Domain Distribu tion The observational domain is significantly different from the model variables and the grid domain. As seen in Figures 2.9-2.11, the observational data is clearly irregularly distributed 4.5. Functional Decomposition 45 over the latitude-longitude grid.
Reference: [76] <editor> Legion. </editor> <address> http://www.cs.virginia.edu/ legion, </address> <year> 1996. </year>
Reference-contexts: Other similar systems to AVS, are Explorer from SGI, Data Explorer from IBM, and Khoros [73, 97, 24]. 8.7.2 Metacomputing Legion The Legion project is pursued at University of Virginia <ref> [76] </ref>. It attempts to create system services for wide-area assemblies of workstations and supercomputers which provide the illusion of a single machine. The Legion system is an object-oriented system based on C++.
Reference: [77] <author> Lemay, L., and Perkins, C. L. </author> <title> Teach Yourself Java in 21 Days. Sams Net, </title> <year> 1996. </year>
Reference-contexts: It is possible to integrate other standard libraries, like ScaLAPACK [20], to enhance the function of the middle layer of the metacomputing editor. While the original prototype of the metacomputing editor was developed in Tcl/Tk, Perl, CGI and Python, the current implementation of the interface, is based on Java <ref> [87, 127, 111, 77, 38] </ref>. This simplifies the expansion towards the WWW driven usage of the interface, as suggested in the MetaWeb project [17]. Using Java also reduces the number of software packages involved in the core implementation of the computing environment, thus making the environment better maintainable.
Reference: [78] <author> Lippman, S. B. </author> <title> C++ Primer, 2nd ed. </title> <publisher> Addison-Wesley, </publisher> <pages> 93. </pages>
Reference-contexts: C/C++: C/C++ is used for some of the data redistribution tools, because data abstraction and the development of software libraries are easier in an object oriented programming language <ref> [78] </ref>. We believe that the incorporation of mathematical data structures in the language (as suggested by the ANSI C++ committee) will enable easier combination of Fortran and C++. No performance loss is expected for the parallel algorithm. 3.3.
Reference: [79] <author> Lumsdaine, A., Squere, J., and McCandless, B. </author> <title> Object Oriented MPI (OOMPI): </title>
References-found: 77

