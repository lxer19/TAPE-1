URL: http://phobos.cs.ucdavis.edu:8001/papers/gwic.ps.gz
Refering-URL: http://phobos.cs.ucdavis.edu:8001/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mock@cs.ucdavis.edu  vemuri@icdc.llnl.gov  
Title: ADAPTIVE USER MODELS FOR INTELLIGENT INFORMATION FILTERING  
Author: KENRICK J. MOCK V. RAO VEMURI 
Address: 95616  94550  
Affiliation: Department of Computer Science University of California at Davis, California,  Department of Applied Science University of California at Davis, Livermore, California,  
Abstract: As networked systems grow in size, the amount of data available to users has increased dramatically. The result is an information overload for the user. In this project, an intelligent information filtering system reduced the user's search burden by automatically eliminating incoming data predicted to be irrelevant. These predictions are learned by adapting an internal user model which is based upon user interactions. This report describes the information filtering problem and examines three techniques for filtering information: global hill climbing, genetic algorithms, and preliminary work with neural networks using radial basis functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Goldberg, D., Nichols, D., Oki, B., Terry, D. </author> <year> (1992). </year> <title> "Using Collaborative Filtering to Weave an Information Tapestry," </title> <journal> Communications of the ACM, </journal> <volume> 35 (12), </volume> <pages> 61-70. </pages>
Reference-contexts: This is clearly undesirable due to the time requirements for training and the space required to store all messages. Many approaches to the information filtering problem bypass this problem by typically forcing the user to explicitly define what should be filtered, e.g. via a keyword-based database language <ref> [1] </ref>. 2 Previous Work In addition to the keyword approaches, neural networks may also be used to classify incoming articles. Eberts examined the approach of feeding words into a feedforward backprop network [2]. In his tests, the article headers were sufficient to correctly classify articles in most instances. <p> This table indicates that the current user's accepted messages strongly correspond with Michelle's accepted messages, while the current user's rejected messages slightly correspond with Michelle's rejected messages. In this fashion, filtering is possible in a collaborative fashion as in <ref> [1] </ref>.
Reference: [2] <author> Eberts, R. </author> <year> (1991). </year> <title> "Knowledge Acquisition Using Neural Networks for Intelligent Interface Design," </title> <booktitle> Proceedings of the 1991 IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <pages> 1331-1335. </pages>
Reference-contexts: Eberts examined the approach of feeding words into a feedforward backprop network <ref> [2] </ref>. In his tests, the article headers were sufficient to correctly classify articles in most instances. This approach did not address the problem of a dynamically changing data set described in the previous section, but only tested upon a static store of messages.
Reference: [3] <author> Jennings, A. & Higuchi, H. </author> <year> (1992). </year> <title> "A Personal News Service Based on a User Model Neural Network," </title> <journal> IEICE Transactions Inf. & Systems, </journal> <volume> E75-D(2), </volume> <pages> 198-209. </pages>
Reference-contexts: This approach did not address the problem of a dynamically changing data set described in the previous section, but only tested upon a static store of messages. Another neural net implementation was examined by Jennings and Higuchi <ref> [3] </ref> in an associative network which associated weights with other words to arrive at an overall interest "activation" for each article. A hill climbing and genetic algorithm approach through competitive agents for information filtering has also been investigated by Baclace, Sheth and Maes, and Stevens [4,5,6].
Reference: [4] <author> Baclace, P.E. </author> <year> (1992). </year> <title> "Competitive Agents for Information Filtering," </title> <journal> Communications of the ACM, </journal> <volume> 35 (12), </volume> <pages> 50. </pages>
Reference: [5] <author> Stevens, C. </author> <year> (1992). </year> <title> "Automating the Creation of Information Filters," </title> <journal> Communications of the ACM, </journal> <volume> 35 (12), </volume> <pages> 48. </pages>
Reference: [6] <author> Sheth, B. and Maes, P. </author> <year> (1993). </year> <title> "Evolving Agents For Personalized Information Filtering," </title> <booktitle> Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, </booktitle> <year> 1993. </year>
Reference: [7] <author> Holland, J. </author> <year> (1975). </year> <booktitle> Adaptation in natural and artificial systems. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference-contexts: The local genetic hill climbing method employed in this project is similar to the global hill-climbing method except instead of a single dynamic table there is a population of many tables, where each table constitutes an individual, and each individual performs its own hill climbing as well as genetic crossover <ref> [7] </ref>. In this project, the table size was set to 20, allowing each individual to identify combinations up to 20 features. Most articles consist of only 4 or 5 features; consequently, each individual has the capacity to represent many different types of messages. <p> The fitness of each individual is given by the history variable plus the sum of the correct predictions minus the sum of the incorrect predictions. The crossover operation uses Fitness Proportionate Reproduction <ref> [7] </ref> to select two parent individuals from the population. The actual crossover operation is two point crossover; two indices are selected at random from one of the parents, and all features and values within this selection are switched among parents.
Reference: [8] <author> Platt, J.(1992). </author> <title> "Learning by Combining Memorization and Gradient Descent," </title> <booktitle> Neural Processing, </booktitle> <year> 1992. </year>
Reference-contexts: The neural network algorithm, developed by Platt, combines gradient descent with memorization so that new cases may be learned quickly but network size is controlled through gradient descent <ref> [8] </ref>. <p> If the memorization criteria are not met, then standard gradient descent is used to modify the weight values. A more detailed description of the neural network may be found in <ref> [8] </ref>. In this project, d varied from 0.7 down to 0.07 and e was set to 0.05. These are the same parameter values used in the simulations described by Pratt [8]. 7.2 NEURAL NETWORK RESULTS Although still under development, the neural network approach appears promising. <p> A more detailed description of the neural network may be found in <ref> [8] </ref>. In this project, d varied from 0.7 down to 0.07 and e was set to 0.05. These are the same parameter values used in the simulations described by Pratt [8]. 7.2 NEURAL NETWORK RESULTS Although still under development, the neural network approach appears promising.
References-found: 8

