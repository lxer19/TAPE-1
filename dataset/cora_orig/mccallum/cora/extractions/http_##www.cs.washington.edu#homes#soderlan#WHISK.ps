URL: http://www.cs.washington.edu/homes/soderlan/WHISK.ps
Refering-URL: http://www.cs.washington.edu/homes/soderlan/
Root-URL: http://www.cs.washington.edu
Email: soderlan@cs.washington.edu  
Title: Learning Information Extraction Rules for Semi-structured and Free Text designed to handle text styles ranging
Author: STEPHEN SODERLAND Editor: Claire Cardie and Raymond Mooney 
Address: Seattle, WA 98195-2350  
Affiliation: Dept. Computer Science Engineering, University of Washington,  
Note: Machine Learning, 1-44 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  WHISK is  has largely been beyond the scope of previous systems. When used in conjunction  
Abstract: A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ashish, N. and Knoblock, C. </author> <year> (1997). </year> <title> Wrapper generation for semi-structured Internet sources. </title> <booktitle> SIGMOD Record 26(4), </booktitle> <pages> 8-15. </pages>
Reference-contexts: WHISK uses a greedy search to add terms to its rules and may make choices that lead to overly restrictive rules that do not generalize well to new instances. Another system that induces rules for structured text is that of Ashish and Knoblock <ref> (Ashish and Knoblock 1997) </ref>. This system is designed for documents 33 with a recursive structure: a regular pattern of sections divided into subsections. It learns a parser for such hierarchically structured text, using cues such as font size of headings and indentation.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., Stone, C. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> California: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: This is a key assumption in top-down decision tree induction such as C4.5 (Quinlan 1993) and CART <ref> (Breiman et al. 1984) </ref> and in the RIPPER rule induction system (Cohen 1996). 40 For the IE problem that WHISK addresses, it is not possible to tabulate how often a term is associated with correct or incorrect extractions. <p> Another anomaly that is closely related to WHISK's rule representation is the behavior of empty rules. Top-down rule induction (Cohen 1996) and top-down decision tree induction (Quinlan 1993) <ref> (Breiman et al. 1984) </ref> assume that an empty rule covers all possible instances and that adding a term or test reduces the coverage of a rule monotonically. This would be the case if WHISK's rules were true regular expressions.
Reference: <author> Califf, M. E. and Mooney, R. </author> <year> (1997). </year> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> Working Papers of ACL-97 Workshop on Natural Language Learning, </booktitle> <pages> 9-15. </pages>
Reference-contexts: These systems are described in more detail later. Some systems operate only on rigidly structured text such as Wrapper Induction (Kushmerick et al. 1997) (WI in Table 1), or on semi-structured text such as RAPIER <ref> (Califf and Mooney 1997) </ref> or SRV (Freitag 1998). Wrapper Induction learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. <p> Along this recall-precision trade-off are results nearly identical with those reported for RAPIER, which had recall 53% at precision 84% for this domain <ref> (Califf and Mooney 1997) </ref>. Except for a few slots such as ID, Post date, and City that occur in highly predictable contexts, most of the rules WHISK learns for this domain are highly specific rules based on exact tokens. <p> An example of this is a web page with dozens of names, addresses, and phone numbers or item numbers, descriptions, and prices. SRV (Freitag 1998) and RAPIER <ref> (Califf and Mooney 1997) </ref> extends information extraction to semi-structured text, but each of these systems extracts only isolated slots. Related information must be re-assembled into a case frame by later processing. This is adequate in domains where each text has only one case frame, such as seminar announcement postings. <p> To learn a rule with k slots would require training on all combinations of k phrases of suitable length in the document. Applying a rule to unseen text would also involve considering an enormous number of candidate phrases. 6.2.2. RAPIER The RAPIER system <ref> (Califf and Mooney 1997) </ref> also uses a form of logic programming and requires no prior syntactic analysis. The text is considered to have three fields centered around the target phrase: the target phrase itself, a pre-filler of tokens before the target phrase, and a post-filler of tokens after it. <p> The next level of difficulty is semi-structured text, often telegraphic in style with much of the relevant information in a fairly small number of stereotyped contexts. On-line rental ads, seminar announcements, and job listings are examples of this genre. SRV (Freitag 1998), RAPIER <ref> (Califf and Mooney 1997) </ref>, and WHISK have been developed specifically to process semi-structured text. The most challenging text style is free text, such as news stories, where there is wide variation in how relevant information is expressed and where judgments about relevancy often require subtle inferences.
Reference: <author> Cohen, W. </author> <year> (1996). </year> <title> Learning trees and rules with set-valued features. </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 709-716. </pages>
Reference-contexts: This is a key assumption in top-down decision tree induction such as C4.5 (Quinlan 1993) and CART (Breiman et al. 1984) and in the RIPPER rule induction system <ref> (Cohen 1996) </ref>. 40 For the IE problem that WHISK addresses, it is not possible to tabulate how often a term is associated with correct or incorrect extractions. <p> It is not clear whether this is an artifact of WHISK's rule representation or whether this is intrinsic to the information extraction problem. Another anomaly that is closely related to WHISK's rule representation is the behavior of empty rules. Top-down rule induction <ref> (Cohen 1996) </ref> and top-down decision tree induction (Quinlan 1993) (Breiman et al. 1984) assume that an empty rule covers all possible instances and that adding a term or test reduces the coverage of a rule monotonically. This would be the case if WHISK's rules were true regular expressions.
Reference: <author> Cohn, D., Atlas, L., Ladner, R. </author> <year> (1994). </year> <title> Improving generalization with active learning. </title> <journal> Machine Learning, </journal> <volume> 15(2), </volume> <pages> 201-221. </pages>
Reference: <author> Dagan, I. and Engelson, S. </author> <year> (1996). </year> <title> Sample selection in natural language learning. </title> <editor> In Wermter, S., Riloff, E., and Scheller, G. (Eds.), </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Berlin: Springer. </publisher>
Reference: <author> Domingos, P. </author> <year> (1994). </year> <title> The RISE system: Conquering without separating. </title> <booktitle> Proceedings of the Sixth IEEE International Conference on Tools with Artificial Intelligence, </booktitle> <pages> 704-707. </pages>
Reference-contexts: WHISK does not "divide and conquer" or even "separate and conquer" The entire training set is available for testing further rules as in RISE <ref> (Domingos 1994) </ref>. This makes the best use of small training sets. Selection of additional instances to be hand-tagged is interleaved with the learning, to provide new training instances most likely to help increase precision or recall of the existing rule set.
Reference: <author> Fisher, D., Soderland, S., McCarthy, J., Feng, F., Lehnert, W. </author> <year> (1995). </year> <title> Description of the UMass system as used for MUC-6. </title> <booktitle> Proceedings of the Sixth Message Understanding Conference, </booktitle> <pages> 221-236, </pages> <address> San Fransisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Mr. B is PersonOut as chairman and CEO of Y and PersonIn as president of Z. tagging of person names, company names, and corporate posts. This example was processed by the University of Massachusetts BADGER syntactic analyzer <ref> (Fisher et al. 1995) </ref>. The sentence is segmented into subject, verb, prepositional phrase, and an ad hoc field REL V for the relative clause attached to the verb. Verb roots are inserted after each head verb, marked with the prefix "@".
Reference: <author> Freitag, D. </author> <year> (1998). </year> <title> Multistrategy learning for information extraction. </title> <booktitle> Proceedings of the Fifteenth International Machine Learning Conference, </booktitle> <pages> 161-169. </pages>
Reference-contexts: These systems are described in more detail later. Some systems operate only on rigidly structured text such as Wrapper Induction (Kushmerick et al. 1997) (WI in Table 1), or on semi-structured text such as RAPIER (Califf and Mooney 1997) or SRV <ref> (Freitag 1998) </ref>. Wrapper Induction learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. <p> WHISK is given full credit for recall if it identifies a target concept at least once, even if it misses an additional reference. This is in keeping with the scoring methodology used for the SRV system <ref> (Freitag 1998) </ref> on this data set. StartTime and EndTime show strong recall at high precision with post-pruning, 100% recall at 96% precision and 87% recall at 89% precision, respectively. <p> Recall for the Location slot is 36% at precision 94% with post-pruning. Rules for Location were also mainly based on names of buildings on campus, but this has utility on unseen announcements. These results are not as high as those reported for the SRV system <ref> (Freitag 1998) </ref> on the same dataset. SRV finds a speaker that is correct 62% of the time, a location that is correct 75% of the time, start time 99%, and end time 96%. The results for SRV are from a five-fold cross validation on 485 documents. <p> This is important for domains where a text has several case frames and it is critical to associate the extracted slots together properly. An example of this is a web page with dozens of names, addresses, and phone numbers or item numbers, descriptions, and prices. SRV <ref> (Freitag 1998) </ref> and RAPIER (Califf and Mooney 1997) extends information extraction to semi-structured text, but each of these systems extracts only isolated slots. Related information must be re-assembled into a case frame by later processing. <p> Human review of the output guides the system to refine its parsing rules. This system handles a different, but complementary, problem than WHISK. 6.2. Systems for semi-structured text 6.2.1. SRV The SRV system <ref> (Freitag 1998) </ref> adopts a multi-strategy approach and combines evidence from three classifiers 8 . The IE problem is transformed into a classification problem by limiting the system to single-slot extraction. All possible phrases from the text up to a maximum length are considered as instances. <p> The next level of difficulty is semi-structured text, often telegraphic in style with much of the relevant information in a fairly small number of stereotyped contexts. On-line rental ads, seminar announcements, and job listings are examples of this genre. SRV <ref> (Freitag 1998) </ref>, RAPIER (Califf and Mooney 1997), and WHISK have been developed specifically to process semi-structured text. The most challenging text style is free text, such as news stories, where there is wide variation in how relevant information is expressed and where judgments about relevancy often require subtle inferences.
Reference: <author> Huffman, S. </author> <year> (1996). </year> <title> Learning information extraction patterns from examples. </title> <editor> In Wermter, S., Riloff, E., and Scheller, G. (Eds.), </editor> <title> Connectionist, Statistical, and Symbolic approaches to Learning for Natural Language Processing. </title> <publisher> Berlin: Springer. </publisher>
Reference-contexts: The next systems shown in Table 1 can handle free text such as news stories: AutoSlog (Riloff 1993), CRYSTAL (Soderland et al. 1995, Soderland 1997), and LIEP <ref> (Huffman 1996) </ref>. AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. Given preprocessed input, CRYSTAL can be extended to handle semi-structured text (Soderland 1997a). <p> CRYSTAL has also been applied to semi-structured text. but only if supplied with an appropriate syntactic analyzer that allows the text to be treated as if it were grammatical (Soderland 1997a). 6.3.3. LIEP, HASTEN, and PALKA Other systems that learn text extraction rules are LIEP, HASTEN, and PALKA. LIEP <ref> (Huffman 1996) </ref> uses heuristics in a manner similar to AutoSlog, but learns multi-slot rules. In contrast to systems we have previously seen that cannot handle multi-slot extraction, LIEP cannot handle single slot extraction. It finds context for a slot only in terms of its syntactic relationship to other slots.
Reference: <author> Kim, J. and Moldovan, D. </author> <year> (1993). </year> <title> Acquisition of semantic patterns for information extraction from corpora. </title> <booktitle> Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> 171-176. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: To apply the HASTEN classifier, a new sentence is compared to each of the exemplars and a goodness-of-fit metric is multiplied by the learned weight which discounts unreliable exemplars. HASTEN was used by one of the highest scoring systems in the MUC-6 evaluation. PALKA <ref> (Kim and Moldovan 1993) </ref> uses an induction method similar to Mitchell's candidate elimination algorithm. PALKA is computationally intensive and was implemented on a parallel computer.
Reference: <author> Krupka, G. </author> <year> (1995). </year> <title> Description of the SRA system as used for MUC-6. </title> <booktitle> Proceedings of the Sixth Message Understanding Conference, </booktitle> <pages> 221-236. </pages> <address> San Fransisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The same key word filtering is done before applying the rules on new text. There seems to be no mechanism for guarding against rules that overgeneralize, but high precision is reported when the test set has only relevant sentences. HASTEN <ref> (Krupka 1995) </ref> uses a k-nearest neighbor approach with a set of handpicked instances as exemplars. Each exemplar represents a positive example of a multi-slot extraction.
Reference: <author> Kushmerick, N., Weld, D., Doorenbos, R. </author> <year> (1997). </year> <title> Wrapper induction for information extraction. </title> <booktitle> Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 729-737. </pages>
Reference-contexts: The chart shown in Table 1 lists the capabilities of existing systems and compares them with WHISK, the system presented in this paper. These systems are described in more detail later. Some systems operate only on rigidly structured text such as Wrapper Induction <ref> (Kushmerick et al. 1997) </ref> (WI in Table 1), or on semi-structured text such as RAPIER (Califf and Mooney 1997) or SRV (Freitag 1998). Wrapper Induction learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. <p> The previous context " * 'pkey' * 'SIZE' * " is necessary to move past the rather long section that precedes the first entry. This rule is equivalent to the rule learned by the Wrapper Induction system <ref> (Kushmerick et al. 1997) </ref>, except that the rule in Figure 19 specifies that the state must be 'NY', since all its training had that in common. A rule learned from two web pages for different states would correct this. <p> The following gives a brief overview of how WHISK compares to other IE learning systems. These systems are described later in this section as well as related work in machine learning. The Wrapper Induction system <ref> (Kushmerick et al. 1997) </ref> works only on rigidly structured text. Wrapper Induction performs multi-slot extraction that bundles together related information into a single case frame 7 . This is important for domains where a text has several case frames and it is critical to associate the extracted slots together properly. <p> Heterogeneous database applications and software agent technology can be enabled by IE systems that transform a web page into the equivalent of database entries. The Wrapper Induction system <ref> (Kushmerick et al. 1997) </ref> was developed to learn a "wrapper" for web pages that allows software robots to treat the text as if it contained a set of relational tuples. <p> The simplest text genre is structured texts, typically created by text formatting software. The relevant information can be transformed into a table of database tuples. The Wrapper Induction system <ref> (Kushmerick et al. 1997) </ref> learns rules effectively for this type of structured text. The next level of difficulty is semi-structured text, often telegraphic in style with much of the relevant information in a fairly small number of stereotyped contexts.
Reference: <author> Lewis, D. and Gale, W. </author> <year> (1994). </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> Proceedings of ACM-SIGIR Conference on Information Retrieval, </booktitle> <pages> 3-12. </pages> <note> 44 Michalski, </note> <author> R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning, </title> <editor> In Michalski, Carbonell, and Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga Publishing. </publisher> <address> MUC-6. </address> <year> (1995). </year> <booktitle> Proceedings of the Sixth Message Understanding Conference, </booktitle> <address> San Fransisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Fransisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The implementation of many inductive learning systems makes an assumption that a term (or test) operates on an instance independently of other terms, and that the test partitions the instances into disjoint classes. This is a key assumption in top-down decision tree induction such as C4.5 <ref> (Quinlan 1993) </ref> and CART (Breiman et al. 1984) and in the RIPPER rule induction system (Cohen 1996). 40 For the IE problem that WHISK addresses, it is not possible to tabulate how often a term is associated with correct or incorrect extractions. <p> It is not clear whether this is an artifact of WHISK's rule representation or whether this is intrinsic to the information extraction problem. Another anomaly that is closely related to WHISK's rule representation is the behavior of empty rules. Top-down rule induction (Cohen 1996) and top-down decision tree induction <ref> (Quinlan 1993) </ref> (Breiman et al. 1984) assume that an empty rule covers all possible instances and that adding a term or test reduces the coverage of a rule monotonically. This would be the case if WHISK's rules were true regular expressions.
Reference: <author> Quinlan, J.R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3). </volume> <pages> 239-266. </pages>
Reference-contexts: The second is a naive Bayes classifier that computes an estimated probability that the tokens in the phrase are found in a correct slot filler. The third classifier is a relational rule learner that does a top down induction similar to FOIL <ref> (Quinlan 1990) </ref>. The relational learner induces a set of constraints such as the length of the phrase and features of particular words in or near the phrase. These features can specify an exact token or derived features such as whether the token is numeric or capitalized. <p> Related machine learning algorithms WHISK belongs to the family of machine learning algorithms know as covering algorithms (Michalski 1983) and shares a general methodology with algorithms that learn classification rules by top-down induction <ref> (Quinlan 1990) </ref>. WHISK begins with an empty rule, then selects terms to add according to a term selection metric. Terms are added to a rule one at a time until the errors are reduced to zero or a pre-pruning criterion has been satisfied.
Reference: <author> Riloff, E. </author> <year> (1993). </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816. </pages>
Reference-contexts: Wrapper Induction learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. The next systems shown in Table 1 can handle free text such as news stories: AutoSlog <ref> (Riloff 1993) </ref>, CRYSTAL (Soderland et al. 1995, Soderland 1997), and LIEP (Huffman 1996). AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. <p> Systems that learn text extraction rules for free text are rare. The systems most closely related to WHISK are AutoSlog and CRYSTAL. These are discussed as well as LIEP, HASTEN, and PALKA. 6.3.1. AutoSlog AutoSlog <ref> (Riloff 1993) </ref> was the first system to learn text extraction rules from training examples. AutoSlog handles only single-slot extraction and uses heuristics to create a rule from relevant examples that extracts the correct information from that example. <p> The other systems require hand-crafted rules, hand-selected key word lists, or hand-selected examples to begin the rule induction or require human review of the proposed rules. WHISK operates at a finer granularity than systems such as CRYSTAL and AutoSlog <ref> (Riloff 1993) </ref>. Those systems identify the syntactic field, such as subject or direct object, that contains the target phrase, but do not identify the target phrase itself. WHISK learns the exact delimiters of the target phrase and requires no later processing to trim away extraneous words.
Reference: <author> Soderland, S., Fisher, D., Aseltine, J., Lehnert, W. </author> <year> (1995). </year> <title> CRYSTAL: Inducing a conceptual dictionary. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1321. </pages>
Reference-contexts: Wrapper Induction learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. The next systems shown in Table 1 can handle free text such as news stories: AutoSlog (Riloff 1993), CRYSTAL <ref> (Soderland et al. 1995, Soderland 1997) </ref>, and LIEP (Huffman 1996). AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. Given preprocessed input, CRYSTAL can be extended to handle semi-structured text (Soderland 1997a). <p> This is with AutoSlog used in conjunction with the same syntactic analyzer, semantic tagger, and discourse processing routines, as the hand-crafted rules. 6.3.2. CRYSTAL A second system to learn text extraction rules is CRYSTAL <ref> (Soderland et al. 1995, Soderland 1997) </ref>, which like AutoSlog, takes input that has been processed by a syntactic analyzer and a semantic tagger. CRYSTAL uses a bottom-up covering algorithm that begins with the most specific rule to cover a seed instance, then generalizes the rule by merging with similar rules. <p> Neither requires syntactic preprocessing for this genre of text. Each of the systems that learns extraction rules for free text has a variety of strengths and limitations. All of them, including WHISK, must operate in con 42 junction with a syntactic analyzer and a semantic tagger. WHISK and CRYSTAL <ref> (Soderland et al. 1995, Soderland 1997) </ref> have a more expressive representation than the other systems and are more fully automated. The other systems require hand-crafted rules, hand-selected key word lists, or hand-selected examples to begin the rule induction or require human review of the proposed rules.
Reference: <author> Soderland, S. </author> <year> (1997). </year> <title> Learning text analysis rules for domain-specific natural language processing. </title> <type> Ph.D. thesis, </type> <institution> technical report UM-CS-1996-087 University of Massachusetts, Amherst. </institution>
Reference-contexts: For extraction from free text, performance of machine learning systems still trails behind that of systems with hand-coded rules. WHISK can handle free text as well as structured or semi-structured when it is provided with syntactically analyzed input, and has performance comparable to CRYSTAL <ref> (Soderland 1997) </ref> a system designed specifically for free text IE. 6.1. IE systems for structured text The driving force behind research on information extraction from structured and semi-structured text is the World Wide Web.
Reference: <author> Soderland, S. </author> <year> (1997a). </year> <title> Learning to extract text-based information from the World Wide Web. </title> <booktitle> Proceedings of the Third International Conference on Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. Given preprocessed input, CRYSTAL can be extended to handle semi-structured text <ref> (Soderland 1997a) </ref>. LIEP identifies the desired phrase exactly, while AutoSlog and CRYSTAL do not identify the target phrase directly, but identify the syntactic field that contains the target phrase. Because of this AutoSlog and CRYSTAL require later processing to trim extraneous words from the extracted phrase. <p> CRYSTAL has also been applied to semi-structured text. but only if supplied with an appropriate syntactic analyzer that allows the text to be treated as if it were grammatical <ref> (Soderland 1997a) </ref>. 6.3.3. LIEP, HASTEN, and PALKA Other systems that learn text extraction rules are LIEP, HASTEN, and PALKA. LIEP (Huffman 1996) uses heuristics in a manner similar to AutoSlog, but learns multi-slot rules.
Reference: <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11), </volume> <pages> 1134-1142. </pages>
Reference-contexts: When to stop tagging How can the user gauge when to stop adding to the training data? An informal analogy to PAC analysis may be used <ref> (Valiant 1984) </ref>. The PAC formalism calculates how many randomly sampled training examples must be examined so that the probability is less than ffi that the error rate of a rule set is greater than *.
References-found: 21

