URL: http://www.ma.ic.ac.uk/statistics/links/ralinks/dgtd.link/rbf.ps.gz
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Bayesian Radial Basis Functions of Unknown Dimension  
Author: C C Holmes and B K Mallick 
Keyword: Radial basis functions, variable architecture selection, Bayesian neural networks, reversible jump Markov chain Monte Carlo.  
Date: March 1997  
Address: London SW7 2BZ  
Affiliation: Department of Mathematics, Imperial College 180 Queen's Gate  
Abstract: A Bayesian framework for the analysis of radial basis functions (RBF) is proposed which readily accommodates uncertainty in the dimension of the model. A distribution is defined over the space of all RBF models of a given basis function and posteriors are computed using reversible jump Markov chain Monte Carlo samplers (Green, 1995). This alleviates the need to select one particular architecture during the modeling process. We show that the resulting models are relatively free from user set design parameters and that they exhibit good performance characteristics on a number of benchmark test series. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bernardo, J. and Smith, A. </author> <title> (1994) Bayesian Theory. </title> <publisher> Wiley Besag, </publisher> <editor> J. Green, P., Higdon, D. and Mengersen, K. </editor> <title> (1996) Bayesian Computation and Stochastic Systems. </title> <journal> Stat. Sci., </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 3-66. </pages>
Reference: <author> Bishop, C. </author> <title> (1995) Neural Networks for Pattern Recognition. </title> <publisher> Oxford. </publisher>
Reference: <author> Broomhead, D. S. and Lowe, D. </author> <year> (1988). </year> <title> Multivariate functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 321-355. </pages>
Reference: <author> Denison, D., Mallick, B., Smith, A. F. M. </author> <title> (1996) Bayesian M.A.R.S. </title> <type> Tech. Report. </type> <institution> Department of Mathematics, Imperial College, </institution> <address> London. </address>
Reference: <author> Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. </author> <title> (1987) Hybrid Monte Carlo. </title> <journal> Physics Letters B, </journal> <volume> vol. 195, </volume> <pages> pp. 216-222. </pages>
Reference-contexts: The true relationship is given by: y 1 = 2:0 cos (x 1 ) + 1:3 cos (x 1 + x 2 ) + * 1 (22) where * i ~ N (0; 2 ), = 0:05. Neal (1996) compares his Bayesian multilayer perceptron, computed using a Hybrid MCMC <ref> (Duane et al 1987) </ref>, with 3 http://wol.ra.phy.cam.ac.uk/mackay/ 20 Average Squared Error Gaussian Approximation method of MacKay Solution with highest evidence 0.00573 Solution with lowest test error 0.00557 Hybrid MCMC of Neal with 150 super-transitions Best over three runs 0.00554 Neal's method with 30 super-transitions Best over three runs 0.00557 Bayesian Radial
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <booktitle> (1990) The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems, </booktitle> <editor> Touretzky, D. S. ed. </editor> <volume> vol. 2, </volume> <pages> pp. 524-532. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fan, J and Gijbels, I. </author> <year> (1996). </year> <title> Local Polynomial Modelling and Its Applications. </title> <publisher> Chapman & Hall. </publisher>
Reference: <author> Franke, R. </author> <title> (1982) Scattered data interpolation: Tests of some methods. </title> <journal> Mathematics of Computation, </journal> <volume> vol. 38, no. 157, </volume> <pages> pp. 181-200. </pages>
Reference: <author> Friedman, J. H. </author> <title> (1991) Multivariate adaptive regression splines (with discussion). </title> <journal> Annals of Statistics. </journal> <volume> vol. 19, </volume> <pages> pp. 1-141. </pages>
Reference: <author> Gelfand, A. and Smith, A. F. M. </author> <title> (1990) Sampling-based approaches to calculating marginal densities. </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 85, </volume> <pages> pp. 398-409. </pages>
Reference-contexts: During the simulation we will update the value of using Gibbs sampling <ref> (Gelfand and Smith, 1990) </ref>. The knot locations are drawn randomly without replacement from the set of data points x.
Reference: <author> Genest, C. and Zidek, J. V. </author> <title> (1986) Combining Probability Distributions: A critique and annotated bibliography. </title> <journal> Statistical Science, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 114-148. </pages>
Reference: <author> Geyer, C. J. and Thompson, E. A. </author> <title> (1995) Annealing Markov chain Monte Carlo with applications to ancestral inference. </title> <journal> Journal of the American Statistical Association. </journal> <volume> vol. 90, </volume> <pages> pp. 909-920. </pages>
Reference-contexts: Note that adding basis functions will never result in a decrease in the likelihood. The authors are currently looking at ways to overcome this problem in MLPs. One promising technique is to use a method known as simulated tempering <ref> (Geyer and Thompson, 1995) </ref> where the probability density is `heated' and then cooled giving unlikely jumps a better chance of being accepted. The presence of strong parameter interaction in MLPs has also caused problems for traditional MCMC techniques based on models of fixed dimension.
Reference: <author> Girosi, F. </author> <title> (1994) Regularization Theory, Radial Basis Functions and Networks. In From Statistics to Neural Networks, Cherkassky, </title> <editor> V., Fried-man, F. and Wechsler, H. eds. </editor> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The theory of radial basis functions specifies many different permissible forms that the function () can take. Each of the acceptable basis types corresponds to an a priori assumption on the smoothness of the true regression function being approximated <ref> (Girosi 1994) </ref>.
Reference: <author> Girosi, F., Jones, M. and Poggio, T. </author> <title> (1995) Regularization Theory and Neural Networks. </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 219-269 Green, </pages> <editor> P. J. </editor> <title> (1995) Reversible Jump Markov chain Monte Carlo computation and Bayesian model determination. </title> <journal> Biometrika, </journal> <volume> vol. 82, </volume> <pages> pp. 711-732. </pages>
Reference: <author> Green, P. J. and Silverman, B. W. </author> <year> (1994). </year> <title> Nonparametric Regression and Generalised Linear Models. </title> <publisher> London: Capman & Hall. </publisher>
Reference: <author> Hastie, T. J. and Tibshirani, R. J. </author> <title> (1990) Generalised Additive Models. </title> <publisher> London: Chapman & Hall. </publisher>
Reference: <author> Hastings, W. K. </author> <title> (1970) Monte Carlo sampling methods using Markov chains and their applications. </title> <journal> Biometrica, </journal> <volume> vol. 57, </volume> <pages> pp. 97-109. </pages>
Reference: <author> Jacobs, R. A. </author> <year> (1995). </year> <title> Methods for combining experts probability assessments. </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 867-888. </pages>
Reference: <author> Jefferys, W. H. and Berger, J. O. </author> <title> Ockham's Razor and Bayesian Analysis. </title> <journal> American Scientist, </journal> <volume> vol. 80, </volume> <pages> pp 64-72. </pages>
Reference: <author> Key, J. </author> <year> (1996). </year> <title> Studies of a Simulation Approach to Bayesian Model Comparison Unpublished PhD Thesis. </title> <institution> Department of Mathematics, Imperial College, </institution> <address> London. </address>
Reference-contexts: What we desire is a 1 A number of Bayesian model choice criteria exist, analogous to those given in section 1, to assist in model selection <ref> (Key, 1996) </ref>. 5 method whereby uncertainty in the dimension of the model space can also be included in our computations and a method that lets the data and any prior knowledge we posses determine the complexity of the solution.
Reference: <author> Le Cun, Y. Denker, J. S. and Solla, S. A. </author> <title> (1990) Optimal brain damage. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <editor> Touretzky, D. S. ed. </editor> <volume> vol. 2, </volume> <pages> pp. 598-605. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lowe, D. </author> <title> (1995) On the use of nonlocal and non positive definite basis functions in radial basis function networks. </title> <booktitle> In Proc. 4th Int. Conf. 26 Artificial Neural Networks , IEE Conference Publication Number 409, </booktitle> <pages> pp. 206-210. </pages>
Reference: <author> Mackay, D. </author> <title> (1992a) Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 415-447. </pages>
Reference-contexts: and Yule's sunspot data given in Weigend et al (1992), we present empirical evidence to support our claim that models computed with this method show good predictive performance. 2 Bayesian Neural Networks The application of Bayesian statistics has done much to remove ad hockery from the field of neural networks <ref> (MacKay, 1992a, 1992b) </ref>, (Neal, 1996). MacKay illustrated how Bayesian analysis provides significant benefits over conventional methods.
Reference: <author> Mackay, D. </author> <title> (1992b) A practical Bayesian framework for backpropogation networks. </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 448-472. </pages>
Reference: <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and Teller, E. </author> <title> (1953) Equations of state calculations by fast computing machines. </title> <journal> J. Chem. Pys., </journal> <volume> vol. 21, </volume> <pages> pp. 1087-1091. </pages>
Reference-contexts: The aim is therefore to generate samples of w in direct correspondence to their posterior probability p (w j D). The Metropolis-Hastings algorithm provides an elegant technique to achieve this goal <ref> (Metropolis et al 1953) </ref>, Hastings (1970). An initial sample point w 1 is first drawn. This could be taken from, but is not necessarily from, the prior p (w).
Reference: <author> Min, C. and Zellner, A. </author> <title> Bayesian and non-Bayesian methods for combining forecasts with applications to forecasting international growth rates. </title> <journal> J. Econometrics. </journal> <volume> vol 56, </volume> <pages> pp. 89-118. </pages>
Reference: <author> Neal, R. </author> <title> (1996) Bayesian Learning for Neural Networks. </title> <publisher> Springer. </publisher>
Reference-contexts: data given in Weigend et al (1992), we present empirical evidence to support our claim that models computed with this method show good predictive performance. 2 Bayesian Neural Networks The application of Bayesian statistics has done much to remove ad hockery from the field of neural networks (MacKay, 1992a, 1992b), <ref> (Neal, 1996) </ref>. MacKay illustrated how Bayesian analysis provides significant benefits over conventional methods.
Reference: <author> Platt, J. </author> <title> (1991) A resource-allocating network for function interpolation. </title> <journal> Neural Computation, </journal> <volume> vol. 3, </volume> <pages> pp. 213-225. </pages>
Reference: <author> Powell, M. J. D. </author> <title> (1987) Radial basis functions for multivariate interpolation: a review. In Algorithms of Approximation, </title> <editor> Mason, J. C. and Cox, </editor> <publisher> M. </publisher>
Reference-contexts: In the analysis that follows we shall consider radial basis functions <ref> (Powell 1987) </ref> although the methods we adopt are generic and readily applicable to other types of networks including MLPs. 3 The Bayesian RBF Modal 3.1 Radial Basis Functions In our model we will use the class of radial basis functions.
Reference: <author> G. </author> <booktitle> eds. </booktitle> <pages> pp. 143-167. </pages> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Richardson, S. and Green, P. J. </author> <title> (1997) On Bayesian Analysis of Mixtures with an Unknown Number of Components. </title> <journal> J. R. Statist. Soc. </journal> <note> B to appear. </note>
Reference: <author> Ripley, B. </author> <title> (1996) Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference: <author> Roberts, S. and Tarassenko, L. </author> <title> (1994) A Probabilistic Resource Allocating Network for Novelty Detection. </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 270-284. </pages>
Reference: <author> Tierney, L. </author> <title> (1994) Markov chains for exploring posterior distributions. </title> <journal> Ann. Statist., </journal> <volume> vol. 22, </volume> <pages> pp. 1701-1762. </pages>
Reference-contexts: Independent samples appearing with probability p (w j D) (the distribution of interest) are obtained by first discarding an initial portion of the chain, to ensure the chain has converged, and then taking every n th sample to remove correlations <ref> (Tierney 1994) </ref>. Until recently Markov chain Monte Carlo methods were mainly restricted to densities of fixed dimension. However, Green (1995) developed a MCMC method that samples from p (w), where w is of unknown dimension.
Reference: <author> Titterington, D. M., Smith, A. F. M. and Makov, U. E. </author> <title> (1985) Statistical Analysis of Finite Mixture Distributions. </title> <address> Chichester: </address> <publisher> Wiley. 27 Tong, </publisher> <editor> H. and Lim, K. S. </editor> <title> (1980) Threshold Autoregression, Limit Cycles and Cyclical Data. </title> <journal> J. R. Statist. Soc. B, </journal> <volume> vol. 42, </volume> <pages> pp. 245-292. </pages>
Reference: <author> Weigend, A., Huberman, B.A. and Rumelhart, D.E. </author> <title> (1992) Predicting Sunspots and Exchange Rates with Connectionist Networks. In Nonlinear Mod-elling and Forecasting, </title> <editor> Casdagli, C. and Eubank, S. eds. </editor> <publisher> Addison Wes-ley. </publisher>
Reference: <author> Williams, P. M. </author> <title> (1995) Bayesian regularization and pruning using a Laplace prior. </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 117-143. </pages>
Reference: <author> Wolpert, D. H. </author> <title> (1992) Stacked Generalization. </title> <journal> Neural Computation, </journal> <volume> vol. 5, </volume> <pages> pp. 241-259. </pages>
References-found: 38

