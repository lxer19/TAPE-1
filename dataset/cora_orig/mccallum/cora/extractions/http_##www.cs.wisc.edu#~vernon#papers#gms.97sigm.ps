URL: http://www.cs.wisc.edu/~vernon/papers/gms.97sigm.ps
Refering-URL: http://www.cs.wisc.edu/~vernon/papers.html
Root-URL: 
Title: Managing Server Load in Global Memory Systems  
Author: Geoffrey M. Voelker, Herve A. Jamrozik Mary K. Vernon Henry M. Levy, and Edward D. Lazowska 
Address: Wisconsin Madison  
Affiliation: Department of Computer Science and Engineering University of Washington Computer Sciences Department University of  
Abstract: New high-speed switched networks have reduced the latency of network page transfers significantly below that of local disk. This trend has led to the development of systems that use network-wide memory, or global memory, as a cache for virtual memory pages or file blocks. A crucial issue in the implementation of these global memory systems is the selection of the target nodes to receive replaced pages. Current systems use various forms of an approximate global LRU algorithm for making these selections. However, using age information alone can lead to suboptimal performance in two ways. First, workload characteristics can lead to uneven distributions of old pages across servers, causing increased contention delays. Second, the global memory traffic imposed on a node can degrade the performance of local jobs on that node. This paper studies the potential benefit and the potential harm of using load information, in addition to age information, in global memory replacement policies. Using an analytic queueing network model, we show the extent to which server load can degrade remote memory latency and how load balancing solves this problem. Load balancing requests can cause the system to deviate from the global LRU replacement policy, however. Using trace-driven simulation, we study the impact on application performance of deviating from the LRU replacement policy. We find that deviating from strict LRU, even significantly for some applications, does not affect application performance. Based upon these results, we conclude that global memory systems can gain substantial benefit from load balancing requests with little harm from suboptimal replacement decisions. Finally, we illustrate the use of the intuition gained from the model and simulation experiments by proposing a new family of algorithms that incorporate load considerations as well as age information in global memory replacement decisions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Carey, D. J. Dewitt, and J. F. Naughton. </author> <title> The 007 benchmark. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: We chose this ratio of local to global memory as a reasonable approximation to the resources applications will likely find available in real global memory systems. These parameters were all derived from simulation experiments, except for the 007 benchmark <ref> [1] </ref> whose parameters were obtained from the GMS prototype.
Reference: [2] <author> Douglas Comer and James Griffioen. </author> <title> A new design for distributed systems: The remote memory model. </title> <booktitle> In Proceedings of the USENIX Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: This research was supported in part by grants from the National Science Foundation (EHR-95-50429, CCR-9024144, CCR-9200832, CCR-9632769, and MIP-9632977), from Digital Equipment Corporation, and from Intel Corporation. in the efficiency of storage access has led to the development of systems that use network-wide memory, or global memory <ref> [2, 3-9] </ref>. By managing memory globally in the network, active nodes can benefit from idle memory available on other nodes; in effect, the memories of idle or lightly-loaded machines are used as a cache for replaced memory pages or file blocks of active nodes.
Reference: [3] <author> Michael D. Dahlin, Randolph Y. Wang, Thomas E. Anderson, and David A. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the USENIX Conference on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: A crucial issue in the implementation of global memory systems is the selection of the target nodes to receive evicted (i.e., paged-out) pages. For example, in the xFS cooperative caching system <ref> [3] </ref>, pages displaced from one node are sent randomly to other nodes, using an algorithm called N-chance to recover from bad selections. <p> Finally, Section 7 summarizes our results and conclusions. 2 Global Memory Systems Our study is based on global memory systems such as the Global Memory Service (GMS) described and implemented in [4] or the recently proposed cooperative caching systems for file I/O <ref> [3, 11] </ref>. The use of global memory is transparent to applications and is managed entirely by the operating system. GMS, for example, implements global memory at the lowest level in the operating system; thus global memory is available for virtual memory backing store, mapped files, and the file buffer cache. <p> The page replacement algorithms in current global memory systems implement some form of approximate LRU replacement <ref> [3, 4, 11] </ref>: when they have to make a page replacement decision, they strive to replace the oldest page in the system. However, based on the MVA results, there is potential benefit to modifying the replacement decision to balance the request load across servers.
Reference: [4] <author> Michael J. Feeley, William E. Morgan, Frederic H. Pighin, Anna R. Karlin, Henry M. Levy, and Chandramohan A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: For example, in the xFS cooperative caching system [3], pages displaced from one node are sent randomly to other nodes, using an algorithm called N-chance to recover from bad selections. In the GMS global memory system <ref> [4] </ref>, the system maintains global information describing the ages of pages on all cluster nodes; using this information, GMS attempts to implement an approximation to network-wide global LRU replacement. Studies of GMS show that the use of global information improves performance relative to a random algorithm. <p> Finally, Section 7 summarizes our results and conclusions. 2 Global Memory Systems Our study is based on global memory systems such as the Global Memory Service (GMS) described and implemented in <ref> [4] </ref> or the recently proposed cooperative caching systems for file I/O [3, 11]. The use of global memory is transparent to applications and is managed entirely by the operating system. <p> Pages in global memory can then be shared among nodes without any locking. In practice, the code pages of an application are shared by multiple instances of the application executing on different nodes, while the data pages are private. Current global memory systems <ref> [4, 11] </ref> maintain approximate global age information for both local and global pages, and use this age information to manage the memory in the cluster as a single resource. When a node needs to replace a page, it locally chooses a target node based on its global age information. <p> Note that both getpage and putpage requests share these probabilities, and that the sum of both sets of probabilities equals 1. These probabilities are also inputs. 3.2 Model Parameterization and Validation We parameterize the model using workload parameters both from an implementation of the Global Memory Service <ref> [4] </ref> and from results from our simulator, which is described subsequently in Section 3.3. Based upon measured values reported in [4], we set the service demands of both idle and local service centers to 194s. <p> These probabilities are also inputs. 3.2 Model Parameterization and Validation We parameterize the model using workload parameters both from an implementation of the Global Memory Service <ref> [4] </ref> and from results from our simulator, which is described subsequently in Section 3.3. Based upon measured values reported in [4], we set the service demands of both idle and local service centers to 194s. Since we assume putpage and getpage requests have the same demand, and the page size never changes, this service demand never changes for any of our experiments. <p> The other inputs, such as the number of each kind of nodes, are set specifically for each experiment with the model. Using these inputs and the average request rate for the 007 benchmark, we can now perform an experiment similar to the one performed in <ref> [4] </ref> and compare the results. In the experiment, we simply vary the number of global nodes (N global ) making requests to a single idle node (N idle = 1). <p> This graph plots the utilization of the idle node and response time of a getpage request versus the number of global nodes in the system. To put the getpage response times in perspective, the fastest disk page fault time of 3600s reported in <ref> [4] </ref> is shown on the response time graph. As with the prototype, the model exhibits linear growth in idle node utilization when there are fewer than 8 global nodes. <p> The page replacement algorithms in current global memory systems implement some form of approximate LRU replacement <ref> [3, 4, 11] </ref>: when they have to make a page replacement decision, they strive to replace the oldest page in the system. However, based on the MVA results, there is potential benefit to modifying the replacement decision to balance the request load across servers. <p> Using local nodes as memory servers can benefit global jobs in two ways: by making more global memory available, and by providing more memory servers to help distribute load. Previous work has quantified the benefit of the increased global memory <ref> [4] </ref>, so we concentrate here on the benefit of having more servers with which to distribute and balance the remote memory load. Using the analytic model, we perform an experiment on a system composed of 10 idle nodes, 10 local nodes, and a variable number of global nodes.
Reference: [5] <author> Edward W. Felten and John Zahorjan. </author> <title> Issues in the implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1991. </year>
Reference: [6] <author> Michael J. Franklin, Michael J. Carey, and Miron Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference: [7] <author> Gideon Glass and Pei Cao. </author> <title> Adaptive Page Replacement Based on Memory Reference Behavior. </title> <booktitle> In Proceedings of ACM SIGMETRICS 1997, </booktitle> <month> June, </month> <year> 1997. </year>
Reference-contexts: For the LRU experiments, we measure the behavior of an additional set of applications to broaden the scope of the results. The additional applications are from the study in <ref> [7] </ref>, and are summarized in Table 2. For each application, the table gives the length of the traced application (in millions of instructions) and the amount of total memory used by the application (in number of 4K pages). <p> The format of the page reference traces, as well as the constraints imposed on simulations using the traces, are fully described in <ref> [7] </ref>. We describe how these constraints interact with our LRU simulator when we describe the results of our experiments. 4.3.1 The LRU stack To quantify the relative importance of a page to an application, we simulated the individual execution of each application on a single node. <p> So that clarity is not entirely sacrificed, only three of the compressed trace applications, applu, m88ksim, and perl, are shown in the figure. These three applications belong to three different types of applications reported in <ref> [7] </ref>, respectively. Interestingly, for each compressed trace application not shown, its curve would overlie the curve shown for the application in its class. The GMS applications are clustered on the left of the graph due to very low probability of accessing pages in the bottom 70% of the stack.
Reference: [8] <author> Herve A. Jamrozik, Michael J. Feeley, Geoffrey M. Voelker, James Evans II, Anna R. Karlin, Henry M. Levy, and Mary K. Vernon. </author> <title> Reducing network latency using subpages in a global memory environment. </title> <booktitle> In Proceedings of the Seventh ACM Conference on Architectural Supportfor ProgrammingLanguagesand Operating Systems, </booktitle> <pages> pp. 258-267, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: More details on the simulator, including the capability to simulate subpage fetching policies not used in this paper, can be found in <ref> [8] </ref>. 4 Issues in Load Balancing Memory Requests This section studies the potential benefits and also the potential harm that can result when global memory requests are load balanced across a given set of memory servers.
Reference: [9] <author> P. J. Leach, P. H. Levine, B. P. Douros, J. A. Hamilton, D. L. Nelson, and B. L. Stumpf. </author> <title> The architecture of an integrated local network. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 1(5) </volume> <pages> 842-857, </pages> <month> November </month> <year> 1983. </year>
Reference: [10] <author> Edward D. Lazowska, John Zahorjan, G. Scott Graham, and Ken-neth C. Sevcik. </author> <title> Quantitative System Performance. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1984. </year>
Reference-contexts: Note that the above bound is simpler and somewhat more aggressive than the bound that would be derived by setting estimated memory server capacity (N* N g ) and using standard asymptotic bounds techniques to estimate N fl. <ref> [10] </ref> For each memory server, if the applicable upper bound on selection frequency is lower than the selection frequency originally computed from the target and age weights, then the selection frequency Symbol Meaning M i total memory available at node i, in pages G i number of pages at node i
Reference: [11] <author> Prasenjit Sarkar and John H. Hartman. </author> <title> Efficient cooperative caching using hints. </title> <booktitle> In Proceedings of the 2nd Symposium on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Studies of GMS show that the use of global information improves performance relative to a random algorithm. Recently, Sarkar and Hartman <ref> [11] </ref> have proposed a fully distributed information exchange (piggybacked on global page forwarding operations) to gather less complete but perhaps more up-to-date page age information, leading to a different approximation of global LRU replacement. <p> Finally, Section 7 summarizes our results and conclusions. 2 Global Memory Systems Our study is based on global memory systems such as the Global Memory Service (GMS) described and implemented in [4] or the recently proposed cooperative caching systems for file I/O <ref> [3, 11] </ref>. The use of global memory is transparent to applications and is managed entirely by the operating system. GMS, for example, implements global memory at the lowest level in the operating system; thus global memory is available for virtual memory backing store, mapped files, and the file buffer cache. <p> Pages in global memory can then be shared among nodes without any locking. In practice, the code pages of an application are shared by multiple instances of the application executing on different nodes, while the data pages are private. Current global memory systems <ref> [4, 11] </ref> maintain approximate global age information for both local and global pages, and use this age information to manage the memory in the cluster as a single resource. When a node needs to replace a page, it locally chooses a target node based on its global age information. <p> The page replacement algorithms in current global memory systems implement some form of approximate LRU replacement <ref> [3, 4, 11] </ref>: when they have to make a page replacement decision, they strive to replace the oldest page in the system. However, based on the MVA results, there is potential benefit to modifying the replacement decision to balance the request load across servers. <p> Alternatively, in the global page replacement algorithm proposed by Dahlin and Sarkar <ref> [11] </ref>, each memory server piggybacks the age of its oldest page on its responses to global memory requests. In that algorithm, each global node maintains a list of the most recent page age information it has received from each server, sorted in order of decreasing age.
Reference: [12] <author> B. N. Schilit and D. Duchamp. </author> <title> Adaptive remote paging. </title> <type> Technical Report CUCS-004091, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> February </month> <year> 1991. </year>
Reference: [13] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <institution> DEC Western Research Lab Technical Report 94/2, </institution> <month> March, </month> <year> 1994. </year>
Reference-contexts: It takes as input memory reference traces generated from applications instrumented by Atom <ref> [13] </ref>. It then simulates these memory references and models the effect of network with 2 idle nodes; with probabilities P idle;0 and P idle;1 , requests go to one idle node or the other.
References-found: 13

