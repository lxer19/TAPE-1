URL: http://robotics.stanford.edu/users/sahami/papers-dir/GNN.ecml95.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: Email: sahami@CS.Stanford.EDU  
Title: Generating Neural Networks Through the Induction of Threshold Logic Unit Trees (Extended Abstract)  
Phone: 1995.  
Author: Mehran Sahami 
Address: Heraklion, Crete, Greece,  Stanford, CA 94305, USA  
Affiliation: Learning,  Computer Science Department, Stanford University,  
Note: In ECML-95: Proceedings of the Eighth European Conference on Machine  
Abstract: We investigate the generation of neural networks through the induction of binary trees of threshold logic units (TLUs). Initially, we describe the framework for our tree construction algorithm and how such trees can be transformed into an isomorphic neural network topology. Several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees (which use single feature tests at each node). Our TLU trees, however, are smaller and thus easier to understand. Moreover, we show that it is possible to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are given. 
Abstract-found: 1
Intro-found: 1
Reference: [BR, 1990] <author> Brent, R.P. </author> <year> 1990. </year> <title> Fast training algorithms for multilayer neural nets. Numerical Analysis Project Manuscript NA-90-03, </title> <institution> Computer Sci. Dept, Stanford. </institution>
Reference-contexts: Such multivariate decision trees have only very recently begun to attract the attention of researchers in the machine learning community [BU, 1992; 1994]. Furthermore, we show how any such TLU trees can be mechanically transformed into a three-layer neural network as first suggested by Brent <ref> [BR, 1990] </ref> and developed by Sahami [SA, 1993].
Reference: [BU, 1992] <author> Brodley, C.E., and Utgoff, P.E. </author> <year> 1992. </year> <title> Multivariate Versus Univariate Decision Trees. </title> <type> COINS Technical Report 92-8, </type> <institution> Computer Science Dept., UMass. </institution>
Reference-contexts: Such multivariate decision trees have only very recently begun to attract the attention of researchers in the machine learning community <ref> [BU, 1992; 1994] </ref>. Furthermore, we show how any such TLU trees can be mechanically transformed into a three-layer neural network as first suggested by Brent [BR, 1990] and developed by Sahami [SA, 1993].
Reference: [BU, 1994] <author> Brodley, C.E., and Utgoff, P.E. </author> <year> 1994. </year> <title> Multivariate Decision Trees. </title> <note> To appear in Machine Learning. </note>
Reference: [GA, 1986] <author> Gallant, S.I. </author> <year> 1986. </year> <title> Optimal Linear Discriminants. </title> <booktitle> In Eighth International Conference on Pattern Recognition, </booktitle> <pages> 849-852. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: Comparatively, we examine a number of adaptive techniques for finding hyperplanes: (i) the Perceptron error-correction rule [NI, 1965] employing the Pocket algorithm <ref> [GA, 1986] </ref>, (ii) the Least Mean Square (LMS) algorithm [WW, 1988] with an annealled learning rate, and (iii) Backpropagation [RHW, 1986] applied to one neuron and then hard-thresholded after learning. 4.2 Learning Simple Boolean Functions 3 bit - 1 corner: X -0,1-3 class 1 if x i i = 1...3 =
Reference: [NI, 1965] <author> Nilsson, N.J. </author> <year> 1965. </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The second method uses a naive Bayesian classifier to find an optimal separating hyperplane that minimizes the probability of error at each node assuming the features of the instance are statistically independent. Comparatively, we examine a number of adaptive techniques for finding hyperplanes: (i) the Perceptron error-correction rule <ref> [NI, 1965] </ref> employing the Pocket algorithm [GA, 1986], (ii) the Least Mean Square (LMS) algorithm [WW, 1988] with an annealled learning rate, and (iii) Backpropagation [RHW, 1986] applied to one neuron and then hard-thresholded after learning. 4.2 Learning Simple Boolean Functions 3 bit - 1 corner: X -0,1-3 class 1 if
Reference: [QU, 1986] <author> Quinlan, J.R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: 1 Introduction We present a non-incremental algorithm that learns binary classification tasks by producing decision trees of threshold logic units (TLU trees). While similar to the decision trees produced by algorithms such as ID3 <ref> [QU, 1986] </ref>, TLU trees promise more generality as each node in our tree implements a linear discriminant function as opposed to testing only one feature of the instance vector.
Reference: [RHW, 1986] <author> Rumelhart, D.E.; Hinton, G. E.; and Williams, R.J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <editor> eds. D. E. Rumelhart and J. L. McClelland, </editor> <address> 318-62. Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Comparatively, we examine a number of adaptive techniques for finding hyperplanes: (i) the Perceptron error-correction rule [NI, 1965] employing the Pocket algorithm [GA, 1986], (ii) the Least Mean Square (LMS) algorithm [WW, 1988] with an annealled learning rate, and (iii) Backpropagation <ref> [RHW, 1986] </ref> applied to one neuron and then hard-thresholded after learning. 4.2 Learning Simple Boolean Functions 3 bit - 1 corner: X -0,1-3 class 1 if x i i = 1...3 = 3 , else class 0 3 bit - 2 corners: X -0,1-3 class 1 if x i i =
Reference: [SA, 1993] <author> Sahami, M. </author> <year> 1993. </year> <title> Learning NonLinearly Separable Boolean Functions With Linear Threshold Unit Trees and Madaline-Style Networks. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence, </booktitle> <pages> 335-41. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Furthermore, we show how any such TLU trees can be mechanically transformed into a three-layer neural network as first suggested by Brent [BR, 1990] and developed by Sahami <ref> [SA, 1993] </ref>.
Reference: [TH, 1991] <author> Thrun, </author> <title> S.B., </title> <booktitle> and 23 coauthors. </booktitle> <year> 1991. </year> <title> The monks problems: a performance comparison of different learning algorithms. </title> <type> TR CMU-CS-91-197, </type> <institution> Carnegie Mellon. </institution>
Reference-contexts: We can first generate the appropriate TLU tree, transform it into a network with informed initial weights and then train the network using any network training method we wish. 4.3 The Monks Problems For further learning experiments, we use a standard testbed for learning tasks known as the Monks Problems <ref> [TH, 1991] </ref>. This set of three learning tasks has been well studied on a number of different machine learning algorithms and includes standard training and test sets to help make fair comparisons between learning methods.
Reference: [WW, 1988] <author> Widrow, B., and Winter, R.G. </author> <year> 1988. </year> <title> Neural Nets for Adaptive Filtering and Adaptive Pattern Recognition. </title> <publisher> IEEE Computer, March:25-39. </publisher>
Reference-contexts: Comparatively, we examine a number of adaptive techniques for finding hyperplanes: (i) the Perceptron error-correction rule [NI, 1965] employing the Pocket algorithm [GA, 1986], (ii) the Least Mean Square (LMS) algorithm <ref> [WW, 1988] </ref> with an annealled learning rate, and (iii) Backpropagation [RHW, 1986] applied to one neuron and then hard-thresholded after learning. 4.2 Learning Simple Boolean Functions 3 bit - 1 corner: X -0,1-3 class 1 if x i i = 1...3 = 3 , else class 0 3 bit - 2
References-found: 10

