URL: http://www.umiacs.umd.edu/users/yaser/iccv-track.ps.Z
Refering-URL: http://www.umiacs.umd.edu/users/yaser/publications.html
Root-URL: 
Email: yaser/lsd@umiacs.umd.edu  
Title: Learned Temporal Models of Image Motion  
Author: Yaser Yacoob and Larry Davis 
Address: College Park, MD 20742  
Affiliation: Computer Vision Laboratory, University of Maryland,  
Note: To Appear in ICCV-98, Mumbai-India, Subject to IEEE Copy-Rights  
Abstract: An approach for learning and estimating temporal-flow models from image sequences is proposed. The temporal-flow models are represented as a set of orthogonal temporal-flow bases that are learned using principal component analysis of instantaneous flow measurements. Spatial constraints on the temporal-flow are also developed for modeling the motion of regions in rigid and coordinated motion. The performance of these models is demonstrated on several long image sequences of rigid and articulated bodies in motion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adiv G. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE PAMI, </journal> <volume> Vol. 7(4), </volume> <year> 1985, </year> <pages> 384-401. </pages>
Reference-contexts: Recall that the flow constraint given in Equation (4) assumes constant flow over a small neighborhood around the point (x; y). Over larger neighborhoods, a more accurate model of the image flow is provided by low-order polynomials <ref> [1] </ref>. For example, the planar motion model [1] is an approximation to the flow generated by a plane moving in 3-D under perspective projection. <p> Recall that the flow constraint given in Equation (4) assumes constant flow over a small neighborhood around the point (x; y). Over larger neighborhoods, a more accurate model of the image flow is provided by low-order polynomials <ref> [1] </ref>. For example, the planar motion model [1] is an approximation to the flow generated by a plane moving in 3-D under perspective projection.
Reference: [2] <author> J.R. Bergen, P. Anandan, K.J. Hanna and R. Hin-gorani. </author> <title> Hierarchical model-based motion estimation. </title> <editor> In G. Sandini, editor, ECCV-92, </editor> <volume> Vol. </volume> <booktitle> 588 of LNCS-Series, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992, </year> <month> 237-252. </month> <title> (2075) (2095) (2115) (2135) a subject marching with the temporal-flow tracking of a subject's marching for both the visible and occluded parts. </title>
Reference-contexts: Afterwords, iteratively, the estimates are refined by decreasing w . This multi-temporal procedure is accompanied by a spatial coarse-to-fine strategy <ref> [2] </ref> that constructs a pyramid of the spatially filtered and sub-sampled images (for more information see [4]) and computes the flow initially at the coarsest level and then propagates the results to finer levels. 3 Learning Temporal-Flow Models Temporal-flow models are constructed by applying principal component analysis to exemplar flow sequences.
Reference: [3] <author> C. Bregler and S. Omohundro, </author> <title> Nonlinear Manifold Learning for Visual Speech Recognition, </title> <booktitle> ICCV 95, </booktitle> <pages> 494-499. </pages>
Reference-contexts: Examples fl The support of the Defense Advanced Research Projects Agency (ARPA Order No. C635), the Office of Naval Research (grant N000149510521) is gratefully acknowledged. of this approach include <ref> [3, 9] </ref>. The main challenges to such appearance-based methods are viewpoint dependence, dealing with appearance variability (due to changes in clothing, shadowing, body size and proportions between individuals), recognition in the presence of occlusion, etc.
Reference: [4] <author> M. Black and P. Anandan. </author> <title> The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 63(1), </volume> <year> 1996, </year> <pages> 75-104. </pages>
Reference-contexts: Afterwords, iteratively, the estimates are refined by decreasing w . This multi-temporal procedure is accompanied by a spatial coarse-to-fine strategy [2] that constructs a pyramid of the spatially filtered and sub-sampled images (for more information see <ref> [4] </ref>) and computes the flow initially at the coarsest level and then propagates the results to finer levels. 3 Learning Temporal-Flow Models Temporal-flow models are constructed by applying principal component analysis to exemplar flow sequences. <p> The former leads to a computation based on a weighted combination of spatio-temporal derivatives while the latter leads to a weighted combination of parametric models. Once a choice for the weighting function has been made, the computation of the parameters of the model follows the approach proposed in <ref> [4] </ref>. In the examples in this paper we adopt the weighted combination of parametric models.
Reference: [5] <author> M. Black and Y. Yacoob. </author> <title> Tracking and Recognizing Rigid and Non-rigid Facial Motions Using Local Parametric Models of Image Motions. </title> <address> ICCV, Boston, MA, </address> <year> 1995, </year> <pages> 374-381. </pages>
Reference: [6] <author> M. Black, Y. Yacoob, A. Jepson and D. </author> <title> Fleet, Learning Parameterized Models of Image Motion, </title> <journal> IEEE CVPR, </journal> <year> 1997, </year> <pages> 561-567. </pages>
Reference-contexts: Equation (10) essentially describes how image motion of a point (x; y) changes over time under the constraint of a temporal flow basis-set. Purely spatial constraints on image motions were recently proposed by Black et al. <ref> [6] </ref>. There, a low dimensional representation of the spatial distribution of image motions in a region was learned and used in recovering motion in image sequences. This spatial model provides only an instantaneous constraint on flow. <p> The above treatment of polynomial flow is also applicable to the orthogonal-basis modeling of spatial flow recently proposed in <ref> [6] </ref>. The coefficients used in the linear combination replace the parameters a i in the above equations. 5 A Rigid Motion Example The use of a temporally parameterized motion model that explicitly accounts for image acceleration and is computed directly from image intensity variations simultaneously was discussed in [10].
Reference: [7] <author> A. Bobick and J. Davis. </author> <title> An appearance-based representation of action. </title> <address> ICPR, </address> <year> 1996, </year> <pages> 307-312. </pages>
Reference-contexts: We present some experimental evidence that suggests that the number of viewpoint-dependent appearance models that one would need to model a given motion is not overwhelming (see also the discussion in <ref> [7] </ref>), and also show how these models can be employed even in conditions when there is partial/full occlusion of some of the body parts (specifically, we demonstrate an ability to track both legs in motion from viewpoints in which one leg occludes part of the other).
Reference: [8] <author> S. X. Ju, M. Black, and Y. Yacoob. </author> <title> Cardboard people: A parameterized model of articulated image motion. </title> <booktitle> in Proc. Int. Conference on Face and Gesture, </booktitle> <address> Vermont, </address> <year> 1996, </year> <pages> 561-567. </pages>
Reference-contexts: The appearance models are created by applying a standard principal components analysis to time sequences of parametric models of body part motion. These observations are obtained using the "cardboard body" model introduced in <ref> [8] </ref> which employs the simple constraint that the motion of body parts must agree at the joints where those parts meet. <p> Equation (18) minimizes the error within a subspace (of a single basis vector, in this case) in which the linear combinations of one line lead to parallel lines. 6 Learned Models of Articulated Human Motion The cardboard <ref> [8] </ref> model for tracking five-part human movement (arm, torso, thigh, calf and foot) involves recovering 40 motion parameters; this requires substantial computation. Furthermore, due to the chain-like structure of the tracking, any error in the computation in an early part (in the chain structure) propagates to the succeeding parts. <p> Dynamic information includes exploiting knowledge about the motion of body parts during activity performance (e.g., the feet are not moving, etc.). a subject walking with the cardboard tracking <ref> [8] </ref>. * A single activity, such as "walking," is learned and tracked. The stage of the "walking" cycle of the first frame of the sequence is known. Learning of a the "walking" cycle temporal-flow model is performed by first employing the algorithm of Ju et al. [8] to compute each region's <p> with the cardboard tracking <ref> [8] </ref>. * A single activity, such as "walking," is learned and tracked. The stage of the "walking" cycle of the first frame of the sequence is known. Learning of a the "walking" cycle temporal-flow model is performed by first employing the algorithm of Ju et al. [8] to compute each region's instantaneous motion parameters during the observed cycle of the activity. Then, the motion paramters of the activity cycles of several people are used to derive the basis-set of temporal-flows of the activity. <p> The five parts are tracked using Equation (18), the body parts are considered as a single object with individual motion parameters for each part coordinated through the principal components model. from the training set of one subject with the five-part body tracking as in <ref> [8] </ref>. Notice that the tracking accumulates errors, some of which also appear in the temporal-flow tracking. In learning the model from ten people's gait 4 , the first basis vector accounts for about 67% of the variations and reflects very clearly the "walking" cycle. <p> Learning plays a critical role in the accuracy of flow estimation. In our experiments on articulated motion, we observed that the inaccuracies of the cardboard model from <ref> [8] </ref> used to generate the training set for (1140) (1100) (1065) (1035) of a subject walking with the computed temporal-flow tracking. the learning algorithm lead to similar inaccuracies in the temporal-flow estimation.
Reference: [9] <author> D. Reynard, A. Wildenberg, A. Blake and J. Marchant, </author> <title> Learning Dynamics of Complex Motions from Image Sequences. </title> <booktitle> ECCV 96, </booktitle> <pages> 357-368. </pages>
Reference-contexts: Examples fl The support of the Defense Advanced Research Projects Agency (ARPA Order No. C635), the Office of Naval Research (grant N000149510521) is gratefully acknowledged. of this approach include <ref> [3, 9] </ref>. The main challenges to such appearance-based methods are viewpoint dependence, dealing with appearance variability (due to changes in clothing, shadowing, body size and proportions between individuals), recognition in the presence of occlusion, etc.
Reference: [10] <author> Y. Yacoob and L. Davis, </author> <title> Temporal Multi-scale Models for Flow and Acceleration. </title> <booktitle> In IEEE CVPR 97, </booktitle> <pages> 921-927. </pages>
Reference-contexts: These observations are obtained using the "cardboard body" model introduced in [8] which employs the simple constraint that the motion of body parts must agree at the joints where those parts meet. Much of the analysis is carried out in a multi-temporal optical flow framework described in <ref> [10] </ref>, which is crucial for analyzing time-varying images of humans since the instantaneous motions of body parts can span a broad spectrum of magnitudes, from sub-pixel to many pixels per frame. The flow models employed there were based on either constant flow or constant acceleration within the temporal integration window. <p> Let (u; v) = (u (0); v (0)) denote the instantaneous flow at time t. The special cases where (u (j); v (j)) are constant for all j or satisfy a constant acceleration model relative to t were considered in <ref> [10] </ref>: u (j) = x 0 + x 1 j (2) throughout the period nffit (where n is the number of time instants and ffit is the time increment-usually ffit = 1). Such flow models are unlikely to hold over long intervals nffit. <p> The weighting function W was designed <ref> [10] </ref> to satisfy the following constraints: * Take on values in the range [0::c], c typically chosen as 1:0 for computational convenience. * For a large w , W approaches 1.0 regardless of (u; v) and s. * Given w , larger estimated flow (u; v) at point (x; y) leads <p> The following Gaussian function, proposed in <ref> [10] </ref>, satisfies the above requirements W (u; v; s; w ) = e (ffjj (u;v)jj 2 +1:0) ) 2 =2 w 2 (8) where jj (u; v)jj 2 is the squared magnitude of the current flow estimate at (x; y), and ff is a constant. <p> The estimation of (u i (s); v i (s)) can be carried out either using the multi-scale approach proposed in <ref> [10] </ref> or by direct two-frame flow estimation technique. Let e i be the vector obtained by concatenating u i (s) for s = 1; :::; n and v i (s) for s = 1; :::; n. <p> The coefficients used in the linear combination replace the parameters a i in the above equations. 5 A Rigid Motion Example The use of a temporally parameterized motion model that explicitly accounts for image acceleration and is computed directly from image intensity variations simultaneously was discussed in <ref> [10] </ref>. Here, we demonstrate how a learned temporal-flow model can capture image acceleration by watching a book "falling" in an image sequence.
References-found: 10

