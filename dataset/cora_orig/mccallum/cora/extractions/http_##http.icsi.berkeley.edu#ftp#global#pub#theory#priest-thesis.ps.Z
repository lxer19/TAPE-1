URL: http://http.icsi.berkeley.edu/ftp/global/pub/theory/priest-thesis.ps.Z
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/theory/
Root-URL: http://http.icsi.berkeley.edu
Title: On Properties of Floating Point Arithmetics: Numerical Stability and the Cost of Accurate Computations Copyright
Author: by Douglas M. Priest 
Note: This document may not be reproduced or distributed in any form without the prior written permission of the author. All copies so authorized must bear this notice. Draft date: 9 November 1992.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adams, D., </author> <title> A Stopping Criterion for Polynomial Root Finding, </title> <booktitle> Comm. ACM 10 (1967), </booktitle> <pages> 655-658. </pages>
Reference-contexts: Another estimate due to Kahan (see Adams <ref> [1] </ref>) shows that the same algorithm is stable for bounded coefficients and arguments with respect to a forward absolute error criterion: specifically, jy P js i x i j where s i = P n j=i a j x ji . (The s i are simply the intermediate quantities in the
Reference: [2] <author> Bailey, D., </author> <title> A Portable High Performance Multiprecision Package, </title> <type> RNR Technical Report RNR-90-022, </type> <institution> NASA Ames Research Center, </institution> <year> 1992. </year>
Reference-contexts: In addition to our model, we consider a model which is partly 42 implicit in the MPFUN multiple precision software package of Bailey <ref> [2] </ref> as well as one suggested by Kulisch and Miranker [41]. As part of his MPFUN multiple precision arithmetic package [2], Bailey provides a program translator, or precompiler, which accepts a standard Fortran program augmented with special comments and produces another Fortran program which calls MPFUN subroutines to perform computations to <p> In addition to our model, we consider a model which is partly 42 implicit in the MPFUN multiple precision software package of Bailey <ref> [2] </ref> as well as one suggested by Kulisch and Miranker [41]. As part of his MPFUN multiple precision arithmetic package [2], Bailey provides a program translator, or precompiler, which accepts a standard Fortran program augmented with special comments and produces another Fortran program which calls MPFUN subroutines to perform computations to a variable level of precision. (Although the maximum precision which can be used is fixed when the original program is <p> since in practice a long computation may involve only a few intermediate results which require high accuracy.) Finally, in chapter 5 we will examine more closely the relationship between arbitrary precision arithmetic with expansions and multiple precision floating point arithmetic, such as that provided by a number of software packages <ref> [2, 10, 59] </ref>. <p> Moreover, on many modern systems designed for high-performance scientific computation, integer arithmetic operations are not as greatly optimized as floating point arithmetic, so intensive computation with integers is not as efficient as a similar floating point computation. Bailey <ref> [2] </ref> addresses this latter problem in his MPFUN package: he represents multiple precision floating point numbers as arrays of floating point numbers in the machine's normal format and uses floating point operations to manipulate multi 95 ple precision numbers.
Reference: [3] <author> Bareiss, E., </author> <title> Sylvester's Identity and Multistep Integer-Preserving Gaussian Elimination, </title> <journal> Math. Comp. </journal> <volume> 22 (1968), </volume> <pages> 565-578. </pages>
Reference-contexts: in several of the examples in chapter 4, we certainly do not preclude the possibility of reducing the size of intermediate results and hence the cost of subsequent computations either by performing exact divisions at an earlier stage, 44 as is done in integer-preserving Gaussian elimination, for example (see Bareiss <ref> [3] </ref>), or by rounding off intermediate results; we simply suggest that these steps be done only when they can be proved not to contaminate the result. Consider how these different paradigms impact the relationship between the cost of solving a problem and the accuracy desired of the solution.
Reference: [4] <author> Bauer, F., </author> <title> Computational Graphs and Rounding Error, </title> <journal> SIAM J. Num. Anal. </journal> <volume> 11 (1974), </volume> <pages> 87-96. </pages>
Reference-contexts: In short, the preceding theorem says that any problem which can be solved exactly by a real number machine (with exact rational arithmetic operations) can be solved approximately by a numerically stable floating point algorithm. The theorem comes close to answering a question raised by Bauer <ref> [4] </ref>: "A constructive derivation of a benign process [for an arbitrary problem] would be a great success|but it is not even clear whether to every problem that is defined by some process, a benign process exists that solves the problem." The theorem doesn't completely answer Bauer's question because his term "benign" <p> For example, if an algorithm admits a backward roundoff error analysis, it is numerically stable with respect to a backward error criterion; in particular, any algorithm whose computation graph is a tree (i.e., each input and intermediate result is used only once; see <ref> [4] </ref>) admits a backward error analysis and is thus numerically stable. Likewise, since numerical instability arises as a result of close approach to a singularity, any algorithm which avoids such a singularity must be numerically stable.
Reference: [5] <author> Ben-Or, M., </author> <title> Lower Bounds for Algebraic Computation Trees, </title> <booktitle> in Proc. 15th ACM Symposium on the Theory of Computing, </booktitle> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Our formal model of floating point computation parallels the Blum-Shub-Smale (BSS) model of computation over a ring; see [6]. One could instead define a model which resembles the real RAM model of Preparata and Shamos [55] or the algebraic computation tree of Ben-Or <ref> [5] </ref>, but for our purposes these models are roughly equivalent to the BSS model. One could similarly define floating point algorithms literally in terms of Fortran programs, albeit with the understanding that such programs are not allowed to employ tricks such as accessing floating point numbers as integers.
Reference: [6] <author> Blum, L., M. Shub, and S. Smale, </author> <title> On a Theory of Computation and Complexity over the Real Numbers: N P -completeness, Recursive Functions, and Universal Machines, </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 21 (1989), </volume> <pages> 1-46. </pages>
Reference-contexts: Our formal model of floating point computation parallels the Blum-Shub-Smale (BSS) model of computation over a ring; see <ref> [6] </ref>. One could instead define a model which resembles the real RAM model of Preparata and Shamos [55] or the algebraic computation tree of Ben-Or [5], but for our purposes these models are roughly equivalent to the BSS model.
Reference: [7] <author> Bohlender, G., </author> <title> Floating-Point Computation of Functions with Maximum Accuracy, </title> <journal> IEEE Trans. Comput. </journal> <note> C-26 (1977), </note> <month> 621-632. </month> <title> [8] ||, What Do We Need Beyond IEEE Arithmetic?, </title> <editor> in C. Ullrich, Ed., </editor> <title> Computer Arithmetic and Self-Validating Numerical Methods, </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: of passes one obtains the first component of the final expansion, which is all he wished to find. (Of course, if the summands are presorted, the doubly compensated summation method given above produces the first component at a cost comparable to 66 two or three passes of Pichat's algorithm.) Bohlender <ref> [7] </ref> later showed that Pichat's algorithm produces the entire expansion for the sum in at most n 1 passes, and he added a stopping criterion for the case where only a given number of leading components are desired.
Reference: [9] <author> Bohm, H., </author> <title> Evaluation of Arithmetic Expressions with Maximum Accuracy, </title> <editor> in U. Kulisch and W. Miranker, Eds., </editor> <title> A New Approach to Scientific Computation, </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1983. </year>
Reference-contexts: We note that a similar bound is claimed by Bohm <ref> [9] </ref> for another polynomial evaluation algorithm.
Reference: [10] <author> Brent, R., </author> <title> A Fortran Multiple Precision Arithmetic Package, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 4 (1978), </volume> <pages> 57-70. </pages>
Reference-contexts: since in practice a long computation may involve only a few intermediate results which require high accuracy.) Finally, in chapter 5 we will examine more closely the relationship between arbitrary precision arithmetic with expansions and multiple precision floating point arithmetic, such as that provided by a number of software packages <ref> [2, 10, 59] </ref>. <p> The most common method of performing multiple precision arithmetic is to use a software package which implements floating point arithmetic of a user-definable precision; the most well known example is Brent's MP package <ref> [10] </ref>. Smith [59] recently published a similar package which features more careful rounding, improved exception handling, and simpler routines for elementary functions (which are faster than Brent's for moderate precision computations).
Reference: [11] <author> Brown, W. S., </author> <title> A Simple but Realistic Model of Floating-Point Computation, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 7 (1981), </volume> <pages> 445-480. </pages>
Reference-contexts: More importantly, the goal of an axiomatic approach is the unification of hypotheses, and Dekker's distinction between different families of properties contributes little to that goal. The Brown model <ref> [11] </ref>, subsequently expressed as a formal specification of floating point arithmetic by Wichmann [63], also specifically includes a definition of float 27 ing point arithmetic.
Reference: [12] <author> Carter, R., </author> <title> Y-MP Floating Point and Cholesky Factorization, </title> <journal> Intl. J. High Speed Comput. </journal> <volume> 3 (1991), </volume> <pages> 215-222. </pages>
Reference-contexts: By these and other counterexamples, we do not intend to suggest that the Cray 2 is less accurate than the Cray X/Y-MP; in fact, Carter <ref> [12] </ref> presents an analysis 25 that suggests the opposite: because the Cray 2 sometimes errs in its subtraction by subtracting too little and other times errs by subtracting too much, the errors tend to cancel one another more frequently than those of the Cray X/Y-MP, which always errs by subtracting too
Reference: [13] <author> Clemmesen, M., </author> <title> Interval Arithmetic Implementations Using Floating Point Arithmetic, </title> <booktitle> SIGNUM Newsletter 19 (1984), </booktitle> <pages> 2-8. 106 </pages>
Reference-contexts: Even directed roundings can be simulated in any faithful arithmetic that does not provide them, albeit at a cost that would not be tolerable for practical purposes (see Clemmesen <ref> [13] </ref>). The primary difference between these three models lies in the paradigms for numerical programming that each suggests.
Reference: [14] <author> Cody, W., </author> <title> Algorithm 665. MACHAR: A Subroutine to Dynamically Determine Machine Parameters, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 14 (1988), </volume> <pages> 303-311. </pages>
Reference-contexts: One consequence of this restriction is that with the exception of the input and output functions, the form of a floating point machine is independent of t, the precision of the arithmetic. (Such a machine can compute the precision t and radix fi, however; see <ref> [14, 22, 44, 47] </ref>, for example.) As in appendix A, we consider the extension of floating point arithmetic to arbitrary precision by representing each number as an unevaluated sum of fixed precision floating point numbers. <p> Their advice appears to be too often ignored, however, judging from Cody's account <ref> [14] </ref> of the prestidigitation required to make his MACHAR program produce correct results on a variety of machines.) The only remedy for problems such as these seems to be to define the notion of the 1 For example, in IEEE 754 arithmetic rounded first to 64 and then to 53 significant
Reference: [15] <author> Crary, F. D., </author> <title> A Versatile Precompiler for Nonstandard Arithmetics, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 5 (1979), </volume> <pages> 204-217. </pages>
Reference-contexts: His translator then substitutes the appropriate calls to an MPFUN subroutine for each arithmetic operation involving multiple precision variables. (Crary <ref> [15] </ref> describes a similar translator which can be used with other multiple precision packages such as Brent's MP package.) Note that in this paradigm, each different precision of arithmetic functions as a different data type with implicit conversions introduced when numbers of different precisions are mixed.
Reference: [16] <author> Dekker, T., </author> <title> A Floating-Point Technique for Extending the Available Precision, </title> <journal> Numer. Math. </journal> <month> 18 </month> <year> (1971), </year> <month> 224-242. </month> <title> [17] ||, Correctness Proof and Machine Arithmetic, </title> <editor> in Fosdick, L., ed., </editor> <title> Performance Evaluation of Numerical Software, </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1979. </year>
Reference-contexts: The following proposition is perhaps the most striking example of the strength of faithfulness as compared with the (1 + *) property: it states that we can compute exactly the roundoff error incurred in the floating point addition of two numbers. As observed by Moller [51], Dekker <ref> [16] </ref>, Linnainmaa [44], and others, this technique can be used to effectively extend the precision of a floating point computation, in some cases to the point of computing an exact representation of the result (see appendix A). <p> We first prove the most general form of the proposition; subsequently, we show that with a few additional hypotheses, we can reduce the cost of the algorithm by nearly half, thereby obtaining the forms proved correct for the various arithmetics considered in <ref> [16, 33, 40, 43, 51, 54] </ref>. Proposition: Let a and b be floating point numbers. <p> Proof: Returning to the proof of the proposition, we are assuming that r is always a t-digit number; hence the test in line 8 never succeeds. Dekker <ref> [16] </ref> and Linnainmaa [43] consider a different variation, which also appears in Pichat [54]. Again, we state a slightly more general result which follows easily from the preceding proposition.
Reference: [18] <author> Demmel, J., </author> <title> Underflow and the Reliability of Numerical Software, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 5 (1984), </volume> <pages> 887-919. </pages>
Reference: [19] <author> Dobkin, D., and D. Silver, </author> <title> Recipes for Geometry and Numerical Analysis, </title> <booktitle> in Proc. Fourth Annual Symposium on Computational Geometry, Association for Computing Machinery, </booktitle> <address> New York, New York, </address> <year> 1988. </year>
Reference-contexts: here we could exercise greater care and reuse some of the quantities already computed in version 2, so that the overall cost will be slightly less than the sum of the costs of each version alone. (In this respect, our approach is similar to that suggested by Dobkin and Silver <ref> [19] </ref>.) Moreover, the most pessimistic quantities in the cost estimates arise from the summation procedure, which we have taken to be doubly compensated summation with an initial O (n 2 ) sort; hence, the costs could be further improved by using a faster sort, a quadruple precision accumulator (if quadruple precision
Reference: [20] <author> Farnum, C., </author> <title> Compiler Support for Floating-Point Computation, </title> <booktitle> Software: Practice and Experience 18 (1988), </booktitle> <pages> 701-709. </pages>
Reference-contexts: Programs such as ours could not possibly be proved correct in a programming environment that unfathomable, since there is no way to know whether the value substituted for a variable is the same as that last assigned to it. (Farnum <ref> [20] </ref> and Goldberg [24] note this and other potential problems which compiler writers ideally should avoid.
Reference: [21] <author> GAO/IMTEC-92-26 Report, </author> <title> Patriot Missile Defense: Software Problem Led to System Failure at Dhahran, </title> <type> Saudi Arabia. </type>
Reference-contexts: The price we might pay for lack of such an understanding is more difficult to estimate. As a start, we might consider the recent report <ref> [21] </ref> on a Scud missile attack on Dhahran, Saudi Arabia, during the 1991 Persian Gulf war, in which a simple floating point roundoff error led to the failure of a Patriot missile defense system to correctly track a Scud missile (which subsequently hit an Army barracks, killing 28 people).
Reference: [22] <author> Gentleman, W., and S. Marovich, </author> <title> More on Algorithms That Reveal Properties of Floating Point Arithmetic Units, </title> <booktitle> Comm. ACM 17 (1974), </booktitle> <pages> 276-277. </pages>
Reference-contexts: One consequence of this restriction is that with the exception of the input and output functions, the form of a floating point machine is independent of t, the precision of the arithmetic. (Such a machine can compute the precision t and radix fi, however; see <ref> [14, 22, 44, 47] </ref>, for example.) As in appendix A, we consider the extension of floating point arithmetic to arbitrary precision by representing each number as an unevaluated sum of fixed precision floating point numbers.
Reference: [23] <author> Goldberg, D., </author> <title> Computer Arithmetic, </title> <editor> Appendix A in D. Patterson and J. Hen-nessy, </editor> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, Calif., </address> <year> 1990. </year> <title> [24] ||, What Every Computer Scientist Should Know About Floating-Point Arithmetic, </title> <booktitle> ACM Computing Surveys 23 (1991), </booktitle> <pages> 5-48. </pages>
Reference-contexts: One can prove, though, that a single rational operation or square root with n-bit operands rounded first to p bits and subsequently to n bits will yield the same value as if the operation were rounded to n bits directly provided p 2n + 1 (see Goldberg <ref> [23] </ref>). 103 "destination" of an arithmetic operation within each programming language standard so as to close the loophole mentioned above.
Reference: [25] <author> Gu, M., and S. Eisenstat, </author> <title> A Stable and Efficient Algorithm for the Rank-one Modification of the Symmetric Eigenproblem, </title> <institution> Yale University Research Report YALEU/DCS/RR-916, </institution> <year> 1992. </year>
Reference-contexts: If such careful computation cannot be per 92 formed, perhaps because the properties upon which it depends are not possessed by the computer we wish to use, then we must resort to a clever approach suggested by Gu and Eisenstat <ref> [25] </ref>. They avoid the use of multiple precision computation by evaluating the relevant function to working precision, yielding a possibly inaccurate result for the given data, then perturbing the original problem to a new problem whose solution is precisely the computed solution.
Reference: [26] <author> Guting, R., </author> <title> Polynomials with Multiple Zeroes, </title> <booktitle> Mathematika 14 (1967), </booktitle> <pages> 181-196. </pages>
Reference-contexts: Unfortunately, the bounds upon which this approach and others like it are based tend to be so large (such as Guting's <ref> [26] </ref> bounds, which are exponential in d, for example) that the resulting precision estimates will almost always yield a computation which is more expensive than simply evaluating the polynomial exactly anyway. Whether better bounds can be found (and computed inexpensively) remains to be seen.
Reference: [27] <author> Henrici, P., </author> <title> Elements of Numerical Analysis, </title> <publisher> Wiley and Sons, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: As Henrici notes <ref> [27, p. 11] </ref>, "the reader should not form the impression that stability is an invariant property which an algorithm either has or does not have.
Reference: [28] <author> Higham, N., </author> <title> The Accuracy of Floating Point Summation, Numerical Analysis Report, </title> <institution> University of Manchester, </institution> <address> England, </address> <year> 1991. </year>
Reference-contexts: Section 1 considers a number of variations on the problem of summing a set of numbers, augmenting the repertoire of methods studied by Higham <ref> [28] </ref> with an algorithm which produces a sum correct to working precision and another which produces the exact sum in the form of an expansion. In section 2, we apply the techniques in appendix A to several simple geometric problems. <p> guarantees that the faithful subtraction of two nearly equal numbers is computed exactly, a preexisting error which is small compared with either number alone can appear much larger in relation to the difference.) Consequently, the problem of computing a sum of several numbers accurately has received much attention; see Higham <ref> [28] </ref> for a survey of a number of methods and a general error analysis for the class of algorithms based on rearranging the order of terms. <p> 2* + O ((n i)* 2 ); in other words, this method produces a sum with a small backward relative error. (Here * denotes a bound on the roundoff of a single operation; take * = fi 1t =2 for correctly rounding arithmetics, * = fi 1t otherwise.) 58 Higham <ref> [28, p. 18] </ref> considers using simply compensated summation with the sum-mands sorted in order of decreasing magnitude and asks whether this method can then produce a sum with a small forward relative error, at least in the case n = 3.
Reference: [29] <author> Holm, J., </author> <title> Floating-Point Arithmetic and Program Correctness Proofs, </title> <type> Ph.D. thesis, </type> <institution> Cornell University, </institution> <year> 1980. </year> <month> 107 </month>
Reference-contexts: Consequently, while his axioms do not apply to programs running under unfaithful floating point arithmetics, they do apply to programs running under other types of arithmetics which could not possibly possess properties analogous to properties S1-S3, for example. The same deficiency a*icts the axioms suggested by Holm <ref> [29] </ref> and Wirth [66], although both specify an otherwise faithful, monotonic, commutative, and anti-symmetric arithmetic. Holm uses only fifteen axioms, compared with van Wijngaarden's 32, by explicitly referring to two rounding functions which relate the computed result of an arithmetic operation to its exact result.
Reference: [30] <institution> IEEE 754-1985 Standard for Binary Floating-Point Arithmetic, Institute of Elec--trical and Electronics Engineers, </institution> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Note that all arithmetics known to the author have monotonic subtraction. Furthermore, subtraction can fail to be anti-symmetric in IEEE standard arithmetic only when a rounding mode other than the default is used (see <ref> [30, 31] </ref>) or in arithmetics such as those of the GE/Honeywell 600 series or the HP 2000 series which round a result halfway between two representable numbers to the next larger number, regardless of its sign. (Strictly speaking, in arithmetics with signed zeroes, the computed value fl (a a) can differ <p> All faithful binary arithmetics and all arithmetics with either properly truncating or correctly chopping addition satisfy property A2. In particular, binary correctly rounding arithmetics, such as those which conform to the IEEE 754 standard <ref> [30] </ref> as well as DEC VAX arithmetics, satisfy both properties. <p> For the purpose of this section, we assume an arithmetic conforming to the IEEE 754 standard <ref> [30] </ref>. <p> In practice, the diversity of experience, understanding, and opinion concerning floating point computation renders standards difficult to agree upon, much less rely upon. The IEEE 754-1985 Standard for Binary Floating Point Arithmetic <ref> [30] </ref> provides an excellent basis for the standardization of floating point computations: it specifies binary, correctly rounding arithmetic (at least in the default rounding mode), which therefore satisfies all of the properties we have mentioned, and it is being widely adopted by the manufacturers of many of the computers most commonly <p> The compiler may be excused on the grounds that "Some languages place the results of intermediate calculations in destinations beyond the user's control. Nonetheless, this standard defines the result of an operation in terms of that destination's format and the operands' values." <ref> [30, p. 7] </ref> Since results are initially left on the floating point stack in the internal format, 102 the standard specifies that they be rounded to that format. <p> Moreover, although some processors can be set to round intermediate results to double precision instead, this setting would no longer comply with the requirement that "An implementation should not provide operations that combine [higher precision] operands to produce a [lower precision] result : : : , with only one rounding." <ref> [30, footnote, p. 10] </ref> That is, since the first fetch operation converts its operand to internal precision, any subsequent arithmetic operation must also round its result to internal precision.
Reference: [31] <institution> IEEE 854-1987 Standard for Radix-Independent Floating-Point Arithmetic, Institute of Electrical and Electronics Engineers, </institution> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Note that all arithmetics known to the author have monotonic subtraction. Furthermore, subtraction can fail to be anti-symmetric in IEEE standard arithmetic only when a rounding mode other than the default is used (see <ref> [30, 31] </ref>) or in arithmetics such as those of the GE/Honeywell 600 series or the HP 2000 series which round a result halfway between two representable numbers to the next larger number, regardless of its sign. (Strictly speaking, in arithmetics with signed zeroes, the computed value fl (a a) can differ
Reference: [32] <institution> ISO/IEC CD 10967-1:1992 Information Technology Language Independent Arithmetic Part 1: </institution> <note> Integer and Floating Point Arithmetic, Committee Draft Version 4.0, ISO/IEC JTC1/SC22/WG11, </note> <year> 1992. </year>
Reference-contexts: Another axiom system appears in the proposed Language Independent Arithmetic Standard, or LIAS <ref> [32] </ref>. <p> (Apparently even Cray Research, Inc., has announced that it intends to begin producing machines which "more or less" conform to the standard.) Of course, not all machines conform to the IEEE 754 standard, and that fact has prompted the recent proposal of another standard, the Language Independent Arithmetic Standard (LIAS) <ref> [32] </ref>, which purports to specify all that a programmer need assume about floating point arithmetic to write portable numerical software. Of course, since the LIAS specifies a faithful arithmetic, it must exclude current Crays, and hence cannot be entirely as helpful as its authors intend.
Reference: [33] <author> Kahan, W., </author> <title> Further Remarks on Reducing Truncation Errors, </title> <journal> Comm. </journal> <note> ACM 8 (1965), </note> <month> 40. </month> <title> [34] ||, If At First You Don't Succeed, Try, Try, Try Again : : : How Hard?, unpublished notes. [35] ||, A Survey of Error Analysis, </title> <editor> in C. V. Freeman, Ed., </editor> <booktitle> Proc. IFIP Cong. 1971, </booktitle> <volume> vol. 2, </volume> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1972. </year> <title> [36] ||, Paradoxes in Concepts of Accuracy, </title> <booktitle> lecture notes from Joint Seminar on Issues and Directions in Scientific Computation, </booktitle> <address> U. C. Berkeley, </address> <year> 1989. </year> <title> [37] ||, Analysis and Refutation of the LCAS, ACM SIGNUM Newsletter 26 (1991), </title> <type> 2-15. [38] ||, oral communication. </type>
Reference-contexts: We first prove the most general form of the proposition; subsequently, we show that with a few additional hypotheses, we can reduce the cost of the algorithm by nearly half, thereby obtaining the forms proved correct for the various arithmetics considered in <ref> [16, 33, 40, 43, 51, 54] </ref>. Proposition: Let a and b be floating point numbers.
Reference: [39] <author> Kahan, W., and B. N. Parlett, </author> <title> Can You Count on Your Calculator?, </title> <institution> Electronics Research Laboratory Technical Report UCB/ERL M77/21, </institution> <year> 1977. </year>
Reference-contexts: Nevertheless, a number of indispensable programs exist whose accuracy cannot be proved from the (1 + *) property alone. One simple example, first analyzed by Kahan and Parlett <ref> [39] </ref>, is a program which evaluates the function f N (x) := &lt; x N 1 N otherwise for a positive integer N and real x.
Reference: [40] <author> Knuth, D., </author> <booktitle> The Art of Computer Programming, vol. 2 (2nd ed.), </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1981. </year>
Reference-contexts: We first prove the most general form of the proposition; subsequently, we show that with a few additional hypotheses, we can reduce the cost of the algorithm by nearly half, thereby obtaining the forms proved correct for the various arithmetics considered in <ref> [16, 33, 40, 43, 51, 54] </ref>. Proposition: Let a and b be floating point numbers. <p> For example, the following simplification was established by Knuth <ref> [40] </ref> for correctly rounding arithmetic; here we prove a slightly more general result which 17 includes properly truncating arithmetics. (Moller [51] obtained a similar, weaker result using still different hypotheses.) Corollary 1: If the arithmetic satisfies property A1, then lines 8 and 9 may be eliminated from the sum-and-error algorithm. <p> The second statement follows by combining the first with the previous corollary. The simplest algorithm described in corollary 2 requires a comparison of absolute values followed by three floating point additions to compute a sum and its roundoff error. Knuth <ref> [40] </ref> proved that an alternative form of the algorithm, requiring six additions and no branching, computes a sum and its roundoff in correctly rounding binary arithmetics; using the preceding properties, we can show that Knuth's algorithm also works for the other arithmetics to which the simplifications of corollary 2 apply. <p> If the error is recombined by immediately adding it to the next term in the sum, we obtain simply compensated summation. Kahan [36] and Knuth <ref> [40] </ref> both give error bounds for this method which show that in a faithful arithmetic, the result of a sum i=1 x i computed by simply compensated summation can be written as P n i where jx 0 i x i j=jx i j 2* + O ((n i)* 2 );
Reference: [41] <author> Kulisch, U., and W. Miranker, </author> <title> Computer Arithmetic in Theory and Practice, </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1981. </year>
Reference-contexts: In addition to our model, we consider a model which is partly 42 implicit in the MPFUN multiple precision software package of Bailey [2] as well as one suggested by Kulisch and Miranker <ref> [41] </ref>. <p> Another model for accurate floating point computation has been suggested by Kulisch and Miranker <ref> [41] </ref>. Their model assumes a fixed precision floating point arithmetic with one extra primitive operation, namely an inner product of two vectors. <p> Unfortunately, many of these techniques rely on special features, such as an extra wide fixed point accumulator, directed roundings, or interval arithmetic, which must be implemented in a combination of hardware and low-level software to be efficient. Other techniques, such as one proposed by Kulisch and Miranker <ref> [41] </ref>, require direct access to the exponent and significand fields of floating point numbers, and obtaining these quantities, even when the programming environment provides a convenient way to do so, is much too time-consuming compared to the highly optimized and streamlined floating point additions and comparisons of either Pichat's algorithm or <p> MPFUN routines provide still more examples of algorithms which rely on stronger properties of floating point arithmetic. (Faithfulness would be a sufficient property to prove their correctness, for example.) Along a somewhat different line, we note the methods for accurate computation proposed by Kulisch and Miranker and their colleagues (see <ref> [41] </ref> and references therein), who show that with directed roundings and a means of evaluating an inner product to working precision, one can solve a wide variety of numerical problems to guaranteed accuracy.
Reference: [42] <author> Leuprecht, H., and W. Oberaigner, </author> <title> Parallel Algorithms for the Rounding-Exact Summation of Floating-Point Numbers, </title> <booktitle> Computing 28 (1982), </booktitle> <pages> 89-104. </pages>
Reference-contexts: Our method, a straightforward divide-and-conquer approach using the addition algorithm in appendix A, recursively adds successive pairs of partially distilled sums in a binary tree-like reduction. A similar approach was proposed for some parallel computer architectures by Leuprecht and Oberaigner <ref> [42] </ref>. 67 Proposition: Given floating point numbers x 1 ; x 2 ; : : : ; x n . If the floating point arithmetic is faithful, the following algorithm computes an expansion y = P m with m n such that y = P x i .
Reference: [43] <author> Linnainmaa, S., </author> <title> Analysis of Some Known Methods of Improving the Accuracy of Floating-Point Sums, BIT 14 (1974), 167-202. [44] ||, Software for Doubled-Precision Floating-Point Computations, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 7 (1981), </volume> <pages> 272-283. </pages>
Reference-contexts: We first prove the most general form of the proposition; subsequently, we show that with a few additional hypotheses, we can reduce the cost of the algorithm by nearly half, thereby obtaining the forms proved correct for the various arithmetics considered in <ref> [16, 33, 40, 43, 51, 54] </ref>. Proposition: Let a and b be floating point numbers. <p> Proof: Returning to the proof of the proposition, we are assuming that r is always a t-digit number; hence the test in line 8 never succeeds. Dekker [16] and Linnainmaa <ref> [43] </ref> consider a different variation, which also appears in Pichat [54]. Again, we state a slightly more general result which follows easily from the preceding proposition.
Reference: [45] <author> Lohner, R., </author> <title> Precise Evaluation of Polynomials in Several Variables, </title> <editor> in U. Kulisch and H. Stetter, Eds., </editor> <title> Scientific Computation with Automatic Result Verification, </title> <publisher> Springer-Verlag, Wien, </publisher> <year> 1988. </year> <month> 108 </month>
Reference-contexts: less clear in the paradigm suggested by Kulisch and Miranker than in either of the other two, partly because the algorithms they use are intended to compute only to working precision (although intermediate results often need to be computed to higher precision, as in Lohner's algorithm for evaluating multivariate polynomials <ref> [45] </ref>), but primarily because they require iterative refinement to ensure the accuracy of the result. Thus, their cost estimates must always be based on estimates of the rate of convergence of the refinement process, and often these estimates will be too pessimistic. <p> the case that the argument x and the coefficients of f are all single floating point numbers, each refinement pass costs O (d), so his method will have the same asymptotic cost as ours. (Of course, this case is one for which exact evaluation is relatively inexpensive as well.) Lohner <ref> [45] </ref> extends Bohm's algorithm to include the case where the coefficients of f are given as expansions. In Lohner's version, the initial approximation to f (x) is again computed using working precision, but each subsequent refinement step computes backward error bounds using the full expansions of the coefficients.
Reference: [46] <author> Malajovich, G., </author> <title> Computability in Dynamical Systems, </title> <type> preprint. </type>
Reference-contexts: Our definition likewise suggests that increasing the precision decreases the roundoff error, though not necessarily by the same amount; we have simply generalized the relationship between precision and accuracy. (In this respect, our definition and the theorem which follows have a similar spirit to the work of Malajovich <ref> [46] </ref> on dynamical systems.) The preceding definition differs from the traditional one in that we have specifically defined numerical stability for floating point implementations of real number algorithms, and consequently we may call algorithms such as those of chapter 2 numerically stable despite the fact that their stability depends on specific
Reference: [47] <author> Malcolm, M., </author> <title> Algorithms to Reveal Properties of Floating Point Arithmetic, </title> <booktitle> Comm. ACM 15 (1972), </booktitle> <pages> 949-951. </pages>
Reference-contexts: One consequence of this restriction is that with the exception of the input and output functions, the form of a floating point machine is independent of t, the precision of the arithmetic. (Such a machine can compute the precision t and radix fi, however; see <ref> [14, 22, 44, 47] </ref>, for example.) As in appendix A, we consider the extension of floating point arithmetic to arbitrary precision by representing each number as an unevaluated sum of fixed precision floating point numbers.
Reference: [48] <author> Mesztenyi, C., and C. Witzgall, </author> <title> Stable Evaluation of Polynomials, </title> <institution> J. Res. Nat. Bur. Stds. </institution> <month> 71B </month> <year> (1967), </year> <pages> 11-17. </pages>
Reference-contexts: Finally we note another approach to accurate polynomial evaluation considered by Mesztenyi and Witzgall <ref> [48] </ref>.
Reference: [49] <author> Milenkovic, V., </author> <title> Verifiable Implementations of Geometric Algorithms Using Finite Precision Arithmetic, </title> <booktitle> Artificial Intelligence 37 (1988), </booktitle> <month> 377-401. </month> <title> [50] ||, Double Precision Geometry: A General Technique for Calculating Line and Segment Intersections Using Rounded Arithmetic, </title> <booktitle> in Proc. Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> Calif., </address> <year> 1989. </year>
Reference-contexts: In order to determine the topology correctly, then, we must introduce artificial perturbations into the geometry and manipulate certain derived objects symbolically without actually computing them, as suggested by the data normalization and hidden variable methods of Milenkovic <ref> [49] </ref>. Needless to say, in order to maintain consistency, this approach requires great care and expense. Again, we propose a simpler solution: by computing the vertices of the clipped line segments correct to working precision, we can guarantee that we do not lose key topological relationships when computing the geometry.
Reference: [51] <author> Moller, O., </author> <title> Quasi Double Precision in Floating-Point Addition, </title> <booktitle> BIT 5 (1965), </booktitle> <pages> 37-50. </pages>
Reference-contexts: The following proposition is perhaps the most striking example of the strength of faithfulness as compared with the (1 + *) property: it states that we can compute exactly the roundoff error incurred in the floating point addition of two numbers. As observed by Moller <ref> [51] </ref>, Dekker [16], Linnainmaa [44], and others, this technique can be used to effectively extend the precision of a floating point computation, in some cases to the point of computing an exact representation of the result (see appendix A). <p> We first prove the most general form of the proposition; subsequently, we show that with a few additional hypotheses, we can reduce the cost of the algorithm by nearly half, thereby obtaining the forms proved correct for the various arithmetics considered in <ref> [16, 33, 40, 43, 51, 54] </ref>. Proposition: Let a and b be floating point numbers. <p> For example, the following simplification was established by Knuth [40] for correctly rounding arithmetic; here we prove a slightly more general result which 17 includes properly truncating arithmetics. (Moller <ref> [51] </ref> obtained a similar, weaker result using still different hypotheses.) Corollary 1: If the arithmetic satisfies property A1, then lines 8 and 9 may be eliminated from the sum-and-error algorithm.
Reference: [52] <author> Ottman, T., G. Thiemt, and C. Ullrich, </author> <title> Numerical Stability of Simple Geometric Algorithms in the Plane, </title> <editor> in E. Borger, Ed., </editor> <booktitle> Computation Theory and Logic, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year> <title> [53] ||, On Arithmetical Problems of Geometric Computations in the Plane, </title> <editor> in U. Kulisch and H. Stetter, Eds., </editor> <title> Scientific Computation with Automatic Result Verification, </title> <publisher> Springer-Verlag, Wien, </publisher> <year> 1988. </year>
Reference-contexts: Therefore, we suggest that the preceding algorithm is not only provably accurate and robust, but could easily be made very efficient as well. We remark that Ottman, et al , <ref> [52, 53] </ref> show how to compute intersection points to full accuracy using single precision arithmetic, a means of computing inner products to full precision, and interval arithmetic methods based on directed roundings.
Reference: [54] <author> Pichat, M., </author> <title> Correction d'une Somme en Arithmetique a Virgule Flottante, </title> <journal> Nu-mer. Math. </journal> <volume> 19 (1972), </volume> <pages> 400-406. </pages>
Reference-contexts: We first prove the most general form of the proposition; subsequently, we show that with a few additional hypotheses, we can reduce the cost of the algorithm by nearly half, thereby obtaining the forms proved correct for the various arithmetics considered in <ref> [16, 33, 40, 43, 51, 54] </ref>. Proposition: Let a and b be floating point numbers. <p> Proof: Returning to the proof of the proposition, we are assuming that r is always a t-digit number; hence the test in line 8 never succeeds. Dekker [16] and Linnainmaa [43] consider a different variation, which also appears in Pichat <ref> [54] </ref>. Again, we state a slightly more general result which follows easily from the preceding proposition. Corollary 2: If the arithmetic satisfies property A2, then line 6 may be eliminated and b substituted for f in lines 7 and 8 of the above algorithm. <p> Pichat <ref> [54] </ref> presented the first complete distillation algorithm: his method consists of repeatedly passing over a partially distilled array (which is initialized with the original summands) applying the basic sum-and-roundoff computation to adjacent pairs of terms.
Reference: [55] <author> Preparata, F., and M. Shamos, </author> <title> Computational Geometry: an Introduction, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Our formal model of floating point computation parallels the Blum-Shub-Smale (BSS) model of computation over a ring; see [6]. One could instead define a model which resembles the real RAM model of Preparata and Shamos <ref> [55] </ref> or the algebraic computation tree of Ben-Or [5], but for our purposes these models are roughly equivalent to the BSS model.
Reference: [56] <author> Priest, D., </author> <title> Algorithms for Arbitrary Precision Floating Point Arithmetic, </title> <editor> in P. Kornerup and D. Matula, Eds., </editor> <booktitle> Proc. 10th Symposium on Computer Arithmetic, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> Calif., </address> <year> 1991. </year>
Reference: [57] <author> Rice, J., </author> <title> Experiment on Gram-Schmidt Orthogonalization, </title> <journal> Math. Comp. </journal> <volume> 20 (1966), </volume> <pages> 325-328. </pages>
Reference-contexts: We caution, however, that such vagueness may lead to some confusion, since subtle variations can produce very different algorithms. Consider the following example, 48 evidently first discovered by Rice <ref> [57] </ref>.
Reference: [58] <author> Smale, S., </author> <title> Some Remarks on the Foundations of Numerical Analysis, </title> <booktitle> SIAM Review 32 (1990), </booktitle> <pages> 211-220. </pages>
Reference-contexts: We begin by defining a model of real number computation and considering the problems which can be solved in such a model. Here we follow some of the development in Smale <ref> [58] </ref>. The most intuitive notion of a problem is a mapping which assigns to each instance of the problem, or input, the corresponding solution, or output. <p> Such a requirement would be in the spirit of stipulating a polynomial bound on the cost of a computation, including the cost of multiple precision arithmetic; Smale <ref> [58] </ref> has expressed a similar idea. We have not explicitly required such a bound since we will prove that taking T j log fi *j, possibly with a constant factor, is essentially sufficient for all piecewise rational computations which would be exact in the absence of roundoff.
Reference: [59] <author> Smith, D., </author> <title> Algorithm 693: A FORTRAN Package for Floating-Point Multiple-Precision Arithmetic, </title> <journal> ACM Trans. Math. Soft. </journal> <volume> 17 (1991), </volume> <pages> 273-283. 109 </pages>
Reference-contexts: since in practice a long computation may involve only a few intermediate results which require high accuracy.) Finally, in chapter 5 we will examine more closely the relationship between arbitrary precision arithmetic with expansions and multiple precision floating point arithmetic, such as that provided by a number of software packages <ref> [2, 10, 59] </ref>. <p> The most common method of performing multiple precision arithmetic is to use a software package which implements floating point arithmetic of a user-definable precision; the most well known example is Brent's MP package [10]. Smith <ref> [59] </ref> recently published a similar package which features more careful rounding, improved exception handling, and simpler routines for elementary functions (which are faster than Brent's for moderate precision computations).
Reference: [60] <author> Sorensen, D., and P. Tang, </author> <title> On the Orthogonality of Eigenvectors Computed by Divide-and-Conquer Techniques, </title> <journal> SIAM J. Num. Anal. </journal> <volume> 28 (1991), </volume> <pages> 1752-1775. </pages>
Reference-contexts: If expm1 is not available, a more subtle program may be needed; see [36].) A more relevant example is provided by Sorensen and Tang <ref> [60] </ref>. They show that by evaluating a certain function to at least twice working precision, a well-known parallel algorithm for computing eigensystems of real symmetric matrices can be guaranteed to give eigenvectors which are orthogonal to working accuracy.
Reference: [61] <author> Sterbenz, P., </author> <title> Floating-Point Computation, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year> <title> [62] van Wijngaarden, A., Numerical Analysis as an Independent Science, </title> <booktitle> BIT 6 (1966), </booktitle> <pages> 68-81. </pages>
Reference-contexts: Nevertheless, faithfulness is a much stronger property, as we will demonstrate in the following results. We begin with two simple lemmas. Lemma 1: (Sterbenz <ref> [61] </ref>) If a and b are t-digit, radix fi floating point numbers such that 1=2 a=b 2, then a b is also a t-digit, radix fi floating point number. 12 In particular, in a faithful arithmetic, fl (a b) = a b exactly.
Reference: [63] <author> Wichmann, B., </author> <title> Towards a Formal Specification of Floating Point, </title> <journal> Computer J. </journal> <volume> 32 (1989), </volume> <pages> 432-436. </pages>
Reference-contexts: More importantly, the goal of an axiomatic approach is the unification of hypotheses, and Dekker's distinction between different families of properties contributes little to that goal. The Brown model [11], subsequently expressed as a formal specification of floating point arithmetic by Wichmann <ref> [63] </ref>, also specifically includes a definition of float 27 ing point arithmetic. Brown defines "model numbers" to be floating point numbers within a given exponent range, although he allows an arithmetic to possess other representable numbers between consecutive model numbers.
Reference: [64] <author> Wilkinson, J., </author> <title> Rounding Errors in Algebraic Processes, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1964. </year> <title> [65] ||, Modern Error Analysis, </title> <booktitle> SIAM Review 13 (1971), </booktitle> <pages> 548-568. </pages>
Reference-contexts: Nearly all well known theoretical results on numerical computation are based solely on a simple property we refer to as the "(1 + *)" property, which first appeared in the earliest floating point roundoff error analyses of Givens, Wilkinson, and their colleagues (see the comments in <ref> [64] </ref>). Considering the weakness of the (1 + *) property, these error analyses achieve remarkably strong results by relying on the elegant technique of backward error analysis, and the error estimates thus obtained are still widely accepted as the best possible for floating point arithmetic. <p> a reader might be tempted to infer that idea from the observation that "it seems highly improbable that any method of solving any problem which is restricted to the use of t-digit arithmetic will, in general, do better than give the solution to a t-digit approximation to the original problem." <ref> [64, p. 32] </ref> Nevertheless, there are useful computations which in fact do much better than give the solution to an approximation to the original problem. <p> Despite its weakness, the (1 + *) property has proved attractive to error analysts thanks to its simple form and to the surprising power of the method of backward error analysis as illustrated in the famous work of Wilkinson <ref> [64] </ref>. As a result, the (1 + *) property has formed the basis for nearly every numerical error analysis known to date. <p> Of course, any numerical programmer knows that the synthetic division method has minimal cost over all branchless real number algorithms, and Wilkinson <ref> [64] </ref> showed that a straightforward implementation of synthetic division in a faithful floating point arithmetic is stable for polynomials of bounded degree with respect to a backward relative error criterion; i.e., for the polynomial P d i=0 a i x i , the computed value y can be written y =
Reference: [66] <author> Wirth, N., </author> <title> Systematic Programming: An Introduction, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1973. </year> <month> 110 </month>
Reference-contexts: The same deficiency a*icts the axioms suggested by Holm [29] and Wirth <ref> [66] </ref>, although both specify an otherwise faithful, monotonic, commutative, and anti-symmetric arithmetic. Holm uses only fifteen axioms, compared with van Wijngaarden's 32, by explicitly referring to two rounding functions which relate the computed result of an arithmetic operation to its exact result.
References-found: 53

