URL: http://www.ececs.uc.edu/~sweaver/postscript/proposal.ps
Refering-URL: http://www.ececs.uc.edu/~sweaver/
Root-URL: 
Title: A Theoretical Framework for Local Adaptive Networks  
Author: Scott E. Weaver 
Degree: A Ph.D. Dissertation Proposal submitted by  in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Department of Electrical and Computer Engineering and Computer Science of the College of Engineering  Committee Chair: Dr. Marios  
Note: M. Polycarpou  
Date: November 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. Abu-Mostafa. </author> <title> The vapnik-chervonenkis dimension: Information versus complexity in learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 312-317, </pages> <year> 1989. </year>
Reference-contexts: CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 8 A concept that would help predict how well f (x; w) implements f fl (x) would clearly be useful. In fact, much work has been accomplished in generalization theory under the assumption that X has a finite number of elements. Generalization ability <ref> [1, 23] </ref> is the probability that an element randomly chosen from the set C will also exist in A. We would expect this probability to grow as f (x; w) learns from B. <p> = i=1 (3.9) where w = [a 1 a 8 b 1 b 8 c 1 c 8 ] T , and a i = 1, b i = 1, c i = i=7, for i 2 f0; 7g: The centers are equally spaced along the input domain X = <ref> [0; 1] </ref>. <p> With an appropriate choice of w k and N k , we show the following two statements: 1. I f k ;w k ;H (x; x 0 ) is bounded for all k 2 N; x; x 0 2 <ref> [0; 1] </ref> n . 2. The limit of I f k ;w k ;H (x; x 0 ) as k approaches infinity is zero almost everywhere. <p> by 1 + n, we see interference has an upper bound that is not a function of x or k, that is, I f k ;w k ;H (x; x 0 ) ^xkz k (x; w k )k 2 B 1 = B (3.31) for all x; x 0 2 <ref> [0; 1] </ref> n . In similar fashion one can show interference is bounded below by B. CHAPTER 3. A FRAMEWORK FOR LOCALIZATION AND INTERFERENCE 27 Now we show that lim k!1 I f k ;w k ;H (x; x 0 ) = 0 almost everywhere. <p> At this point we have met the conditions of the Lebesgue Dominated Convergence Theorem : I f k ;w k ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (3.35) is an integrable function, we see lim E <p> At this point we have met the conditions of the Lebesgue Dominated Convergence Theorem : I f k ;w k ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (3.35) is an integrable function, we see lim E [I f k ;w <p> ;H (x; x 0 ) is a sequence of integrable functions on <ref> [0; 1] </ref> n fi [0; 1] n . Because there exists a bound B &gt; 0 such that jI f k ;w k ;H (x; x 0 )j B for all k 2 N; x; x 0 2 [0; 1] n and (3.35) is an integrable function, we see lim E [I f k ;w k ;H (x; x 0 ) 2 ] = E [ lim I f k ;w k ;H (x; x 0 ) 2 ] = 0 (3.36) for X = [0; 1] n . <p> x 0 2 <ref> [0; 1] </ref> n and (3.35) is an integrable function, we see lim E [I f k ;w k ;H (x; x 0 ) 2 ] = E [ lim I f k ;w k ;H (x; x 0 ) 2 ] = 0 (3.36) for X = [0; 1] n . Equation (3.36) implies that there exists a k such that E [I f k ;w k ;H (x; x 0 ) 2 ] &lt; * for arbitrary * &gt; 0 and hence L f k ;w k ;H;X can be made arbitrarily large. <p> To reduce the effects of interference as well the approximation error, J 1 (x; y; ) is augmented as J 2 (x; x 0 ; x 00 ; y; ) = 2 1 I f;;H 1 (x 0 ; x 00 ) 2 (4.6) where 2 <ref> [0; 1] </ref> controls the relative weighting of the two terms and I f;;H 1 (x 0 ; x 00 ) is the definition of interference given in (3.6) based on algorithm (4.3). <p> In this way learning proper response when on land will tend not to interfere with previous training over water and vice-versa. * Full Supervised Learning data is available ( 2 <ref> [0; 1] </ref>) In this case we are no longer restricted to = 0 and we can train on the supervised learning data. Any of the cases mentioned above can be used here as well. Learning would simultaneously reduce approximation error and interference using to weight the competing terms. <p> The network has the form f (x; ) = i=1 2 where = [a 1 a 10 b 1 b 10 c 1 c 10 ] T , and each element of is initially chosen randomly from a uniform probability distribution in the domain X = <ref> [0; 1] </ref>. A consequence of this is that the basis centers are in the proper range, but the network is not initially local.
Reference: [2] <author> J. Albus. </author> <title> A new approach to manipulator control: The cerebellar model articulation controller. Journal of Dynamic Systems, </title> <booktitle> Measurement, and Control, </booktitle> <pages> pages 220-227, </pages> <year> 1975. </year>
Reference-contexts: Some of the more generic network structures used are multi-layer perceptron (MLP) networks with sigmoidal activation functions (often associated with the error-backpropagation algorithm), Radial Basis Function (RBF) networks (whose bases form Gaussian bumps), Cerebellar Model Ar 5 CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 6 ticulation Controller <ref> [2] </ref> (CMAC) (which typically implements a non-continuous function), and wavelet networks (which are usually chosen for their orthonormal properties).
Reference: [3] <author> K. J. Astrom and B. Wittenmark. </author> <title> Adaptive Control. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: These theoretical tradeoffs made when choosing from this continuum are present in local modeling which has been used on real problems such as gain scheduling <ref> [3] </ref>. 2.8 Local Modeling When faced with a complex modeling or control problem it is natural to use the divide and conquer approach.
Reference: [4] <author> W. Baker and J. Farrell. </author> <title> An introduction to connectionist learning control systems. </title> <editor> In D. White and D. Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pages 35-63, </pages> <address> New York, NY, 1992. </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [41, 19, 39, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> The meaning of local used in this proposal, however, refers to learning that affects the approximator's input/output map in only a (spatially) localized region in the input space. Probably the most complete description of local learning networks offered in the neural network literature is given by Baker and Farrell <ref> [4] </ref> who say "Learning is facilitated in situations where a clear association can be made between a subset of the adjustable elements of the learning system and a localized region of the input-space." Baker and Farrell offer an analysis for local network properties. <p> As width weights are decreased one can see how an RBF network becomes more local as shown in Example 1. 1 Baker and Farrell <ref> [4] </ref> use the term "coverage" for this condition. CHAPTER 3. A FRAMEWORK FOR LOCALIZATION AND INTERFERENCE 29 P N with respect to an amplitude a i , inverted width b i , and center c i . CHAPTER 3.
Reference: [5] <author> R. Bartle. </author> <title> The Elements of Real Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1976. </year>
Reference-contexts: To show kz k ()k is bounded from above we use the ratio test (Bartle <ref> [5] </ref> p. 296) and see that lim g (i + 1) = lim e 2 e i + 1 4 therefore P 1 i=0 g (i) is convergent and less then some value B 2 and therefore kz k ()k 2 &lt; 2B 2 .
Reference: [6] <author> A. G. Barto. </author> <title> Connectionist learning for control. </title> <editor> In I. W. Thomas Miller, R. S. Sutton, and P. J. Werbos, editors, </editor> <booktitle> Neural Networks for Control, </booktitle> <pages> pages 5-58, </pages> <institution> Massachusetts Institute of Technology, </institution> <address> MA, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Although local networks, in general, lessen the problem of interference, there are tradeoffs to consider (see Barto <ref> [6] </ref> for a nice summary). For example, look-up tables can be thought of as the most local of approximation structures because there is a one-to-one relationship between a point in the input space and an adjustable parameter. <p> This makes a strong case for having networks that are not too local and not too distributed. The need for this balance is made repeatedly in the neural network literature <ref> [18, 6, 19] </ref>. Chapter 3 A Framework for Localization and Interference This chapter develops analytical tools necessary to measure the localization properties of a network.
Reference: [7] <author> R. Battiti. </author> <title> First- and second-order methods for learning: Between steepest descent and newton's method. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: Because of the large number of weights, standard neural network algorithms based on second-order models are usually designed to avoid computation of the Hessian. For example, variants of Newton's method do not require knowledge of the Hessian <ref> [7] </ref>. Similarly, in the conjugate gradient method, which computes a conjugate direction and then performs a line search, the conjugate direction can be computed much faster (because of its algebraic form) than the computation of the Hessian matrix [40].
Reference: [8] <author> S. Becker. </author> <title> Unsupervised learning procedures for neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2 </volume> <pages> 17-33, </pages> <year> 1991. </year>
Reference-contexts: For example, the network might learn to transform itself using the input data in useful ways so that when the desired outputs are provided, the network is able to learn more quickly. This hybrid use of unsupervised and supervised ideas have been pursued by many researchers <ref> [27, 30, 28, 34, 8] </ref>. 2.4 Generalization To the lay-person, the term generalization invokes the idea that novel situations can be handled correctly by using extrapolations from solutions to learned situations. The neural network literature sometimes uses the term generalization in a slightly different and more restricted sense.
Reference: [9] <author> R. E. Bellman. </author> <title> Adaptive control processes: A guided tour. </title> <publisher> Princeton University Press, Princeton University, </publisher> <address> NJ, </address> <year> 1961. </year> <note> 56 BIBLIOGRAPHY 57 </note>
Reference-contexts: However, look-up tables are obviously inappropriate when the dimension of the problem grows large because the curse of dimensionality <ref> [9] </ref> causes memory requirements to become prohibitive; furthermore, look-up tables provide no generalization of untrained points. Finding the correct balance between avoiding interference problems, reducing memory requirements, and enhancing generalization, that is, finding a balance between local versus non-local networks, is a key problem in network learning.
Reference: [10] <author> P. Billingsley. </author> <title> Probability and Measure. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: Condition 3 imposes the conditions for ff n . Take Q () = E [ 2 1 I f;;H 1 (X 0 ; X 00 ) 2 ] CHAPTER 4. AN ALGORITHM FOR LOCALIZING NETWORKS 40 and by the localized version of theorem 16.8 (ii) of Billingsley <ref> [10] </ref>, for each 2 R l we have rQ () = E [e (x; y; )rf (X n ; ) + (1 )I f;;H 1 (X 0 ; X 00 )rI f;;H 1 (X 0 ; X 00 )]; given conditions 1 and 2. <p> Since we know ~ n ! fi wp1, then 2 R l such that P r [ ~ n ! S * fl ] &gt; 0 where S * fl = f : k fl k &lt; *g: By the localized version of theorem 16.8 (ii) of Billingsley <ref> [10] </ref>, M () is continuously differentiable for all , given Conditions 1 and 2. The uniform boundedness of (Y n ; X n ) ensures finiteness of J fl which has been assumed to be positive definite.
Reference: [11] <author> G. A. Carpenter and S. Grossberg. </author> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37 </volume> <pages> 54-115, </pages> <year> 1987. </year>
Reference-contexts: The trade-offs involved in local learning systems are closely related to the well-known stability-plasticity dilemma [12], namely how to design a learning system that is "plastic" enough to learn new patterns, and yet is stable enough to remember old learned patterns. CHAPTER 1. INTRODUCTION 3 Carpenter and Grossberg <ref> [11] </ref> developed an architectural solution to this question using their adaptive resonance theory (ART), which overcomes the stability-plasticity dilemma by adapting the stored pattern of a category only when the input is sufficiently similar to it.
Reference: [12] <author> M. A. Cohen and S. Grossberg. </author> <title> Absolute stability of global pattern formation and parallel memory storage by competitive neural networks. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 5 </volume> <pages> 815-826, </pages> <year> 1983. </year>
Reference-contexts: Finding the correct balance between avoiding interference problems, reducing memory requirements, and enhancing generalization, that is, finding a balance between local versus non-local networks, is a key problem in network learning. The trade-offs involved in local learning systems are closely related to the well-known stability-plasticity dilemma <ref> [12] </ref>, namely how to design a learning system that is "plastic" enough to learn new patterns, and yet is stable enough to remember old learned patterns. CHAPTER 1.
Reference: [13] <author> Y. L. Cun, J. S. Denker, S. A. Solla, and L. D. Jackel. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: In this way the network's extra degrees-of-freedom do not get in the way of finding a good generalization. Other additional objectives include second-order Hessian information such as LeCun's work on Optimal Brain Damage <ref> [13] </ref> which uses the sensitivity of network performance to remove weights. Ideas similar to this are found in principal component analysis (PCA), but here the functions are linear combinations of bases that form a linear space.
Reference: [14] <author> G. Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> 2 </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference-contexts: CHAPTER 3. A FRAMEWORK FOR LOCALIZATION AND INTERFERENCE 32 the universal-localization theorem of Section 3.2 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example, <ref> [20, 14, 26] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X .
Reference: [15] <author> S. Fahlman. </author> <title> An empirical study of learning speed in back-propagation networks. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <year> 1988. </year>
Reference-contexts: Along with an efficient implementation of gradient descent called error-backpropagation [38], a number of variants have made improvements by incorporating second order information, such as back-propagation with momentum, Quick Prop <ref> [15] </ref>, and conjugate gradient [37], to name a few. Sometimes an additional term is added to the standard quadratic error resulting in a multi-objective cost function that not only attempts to reduce the approximation error but CHAPTER 2. <p> Similarly, in the conjugate gradient method, which computes a conjugate direction and then performs a line search, the conjugate direction can be computed much faster (because of its algebraic form) than the computation of the Hessian matrix [40]. Other methods called quasi second-order methods such as Quick Prop <ref> [15] </ref> have been used to gain speeds comparable to second order methods using slight heuristic modifications to well-established first-order learning methods. 51 CHAPTER 5.
Reference: [16] <author> S. E. Fahlman and C. Lebiere. </author> <title> The Cascade-Correlation learning architecture. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 524-532. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: This is one of the motivations for algorithms that increase or decrease the network size in order to find an optimal size for the problem finding a sufficient balance between good generalization and approximation capabilities. Network growing algorithms, such as cascade correlation <ref> [16] </ref>, start with a small network and as the training data is introduced, the network grows in accordance with the functionality requirement [16]. <p> Network growing algorithms, such as cascade correlation <ref> [16] </ref>, start with a small network and as the training data is introduced, the network grows in accordance with the functionality requirement [16]. Another method for finding an appropriate network size is through the use of network reduction algorithms that typically start with a large network and reduce its size by a variety of means.
Reference: [17] <author> J. A. Farrell. </author> <title> Approximators characteristics and their effect on training misbehavior in passive learning control. </title> <booktitle> In Proceedings of the 1996 IEEE International Symposium on Intelligent Control, </booktitle> <pages> pages 181-187, </pages> <year> 1996. </year>
Reference-contexts: The difficulty that this presents in non-local networks is that during this fixation period, approximation errors are reduced in the fixated region, but approximation error is sacrificed in other regions of the domain due to unlearning or interference. Farrell <ref> [17] </ref> emphasizes this in the context of an adaptive controller whose output error, the difference between an approximation model and the real system, is to be reduced. <p> Indeed, it is often not adequate to monitor the instantaneous approximation error since this may provide a myopic view of the more global function approximation error over the entire domain of possible inputs. As discussed and illustrated (via a simple example) CHAPTER 5. PROPOSED RESEARCH 53 by Farrell <ref> [17] </ref>, the instantaneous approximation error is only an indicator of the function approximator's fidelity at the current place in state space, and reducing this instantaneous error may not result in a useful system.
Reference: [18] <author> R. </author> <title> French. Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference. </title> <booktitle> In Proceedings of the 16th Annual Cognitive Science Society Conference, </booktitle> <volume> volume 5, </volume> <pages> pages 207-220, </pages> <year> 1994. </year>
Reference-contexts: INTRODUCTION 2 input space affects the input/output (I/O) map in an undesirable way in other areas of the input space. The general problem of interference has been uncovered in various forms by researchers in many areas <ref> [18, 41] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input space is reached). Suppose that without noise, a network function approximator learns the system dynamics, reduces the approximation error, and then ceases learning. <p> Another variant of the interference problem is in the classification literature: "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns <ref> [18] </ref>. These and other interference problems may appear different when embedded in their particular applications but the root of these problems is the same; learning tends to interfere with previous learning elsewhere in the input space. <p> Using local networks in a passive learning situation can reduce the potential for interference or unlearning. The use of local networks, however, is not limited to control and can be advantageous in other applications such as pattern recognition where the same interference problems occur. 2.11 Catastrophic Interference Catastrophic Interference <ref> [18] </ref> occurs when the learning of a new pattern causes complete unlearning of originally trained patterns. 1 Catastrophic Interference is common enough that there is much research aimed at understanding and preventing this problem [18, 24, 32]. <p> applications such as pattern recognition where the same interference problems occur. 2.11 Catastrophic Interference Catastrophic Interference [18] occurs when the learning of a new pattern causes complete unlearning of originally trained patterns. 1 Catastrophic Interference is common enough that there is much research aimed at understanding and preventing this problem <ref> [18, 24, 32] </ref>. The solutions found in the pattern recognition literature focus on the symptoms, poor training. To handle the poor training French [18] introduced an algorithm called activation sharpening for MLPs. <p> The solutions found in the pattern recognition literature focus on the symptoms, poor training. To handle the poor training French <ref> [18] </ref> introduced an algorithm called activation sharpening for MLPs. This algorithm attempts to make a specific input pattern be associated with a specific hidden node, using an unsupervised-learning, winner-take-all strategy in addition to the standard supervised backpropagation step. <p> This makes a strong case for having networks that are not too local and not too distributed. The need for this balance is made repeatedly in the neural network literature <ref> [18, 6, 19] </ref>. Chapter 3 A Framework for Localization and Interference This chapter develops analytical tools necessary to measure the localization properties of a network.
Reference: [19] <author> J. H. Friedman. </author> <title> Local learning based on recursive covering. </title> <journal> submitted to The Annals of Statistics, </journal> <month> Aug. </month> <year> 1996. </year> <note> BIBLIOGRAPHY 58 </note>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [41, 19, 39, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> This makes a strong case for having networks that are not too local and not too distributed. The need for this balance is made repeatedly in the neural network literature <ref> [18, 6, 19] </ref>. Chapter 3 A Framework for Localization and Interference This chapter develops analytical tools necessary to measure the localization properties of a network.
Reference: [20] <author> K.-I. Funahashi. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference-contexts: CHAPTER 3. A FRAMEWORK FOR LOCALIZATION AND INTERFERENCE 32 the universal-localization theorem of Section 3.2 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example, <ref> [20, 14, 26] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X . <p> 1; :::; n); such that h (x; w) = P N P n 1 satisfies max jh (x; w) g fl (x)j &lt; * (3.39) L h;w;H;X &gt; M: (3.40) Proof of Theorem 2: This proof is a combination of the proof of Theorem 1 and a universal-approximation theorem in <ref> [20] </ref>, which says that given a real-valued continuous function g fl (x) on X , then for arbitrary * &gt; 0 there exist a number of nodes N , and weights w, such that g (x; w) = P N P n j=1 b ij x j ) 1 satisfies max <p> It is known that given a large enough single-hidden-layer CHAPTER 4. AN ALGORITHM FOR LOCALIZING NETWORKS 37 (1HL), multi-layer perceptron (MLP) network, any continuous function on a compact domain can be approximated to any degree desired <ref> [20] </ref>, that is, the global minimum of the error surface E [J 1 (X; Y; )] (4.5) approaches zero as the network size increases, where the expectation operator E [] is over the input/desired-output pair, (X,Y), which is chosen from some pdf. <p> As mentioned above, for an arbitrarily large 1HL MLP network, (4.5) has an arbitrarily low global minimum <ref> [20] </ref>. Theorem 2 proves an identical result for the quantity E [J 2 (X; X 0 ; X 00 ; Y; )] (4.7) where the expected value is over the random variable set X; X 0 ; X 00 ; Y . This fact gives us CHAPTER 4.
Reference: [21] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: The new term could be designed such that its reduction would lead to a network with limited degrees-of-freedom. Examples of this type of work are ridge regression <ref> [21] </ref> and related weight decay [25], which control the degrees-of-freedom of function approximators. The idea of a multi-objective cost function is quite useful since it might be easier to specify an additional objective for the cost function rather than a new algorithm that would perform the equivalent function.
Reference: [22] <author> S. Haykin. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan College Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: Still, neural networks are used in a variety of ways, for example, pattern recognition where a neural network may be used to classify images, neurocontrol where a network is used as a controller, and reinforcement learning where a network stores a model of its perceived environment. Haykin <ref> [22] </ref> and Hertz, Krogh, and Palmer [23] are fairly exhaustive, and popular texts on the subject. It is not uncommon to see the same problems recur during the training of neural network approximators. <p> In this way input vectors can be approximated CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 10 by a reduced number of principal components in the feature space <ref> [22, 23] </ref>. PCA can be performed by neural networks Oja [35]. PCA is mentioned here because its nonlinear variant deals directly with issues raised earlier. <p> Theoretical conditions for guaranteeing that (4.3) will find a sufficiently low local minimum have proved elusive in the general case, however, it has been shown through simulations that the local minimum to which (4.3) converges are sufficiently close to zero for many applications <ref> [22] </ref>.
Reference: [23] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: Haykin [22] and Hertz, Krogh, and Palmer <ref> [23] </ref> are fairly exhaustive, and popular texts on the subject. It is not uncommon to see the same problems recur during the training of neural network approximators. For certain types of applications, (most notably control applications) the interference problem causes learning to be slow or prevents learning outright. <p> CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 8 A concept that would help predict how well f (x; w) implements f fl (x) would clearly be useful. In fact, much work has been accomplished in generalization theory under the assumption that X has a finite number of elements. Generalization ability <ref> [1, 23] </ref> is the probability that an element randomly chosen from the set C will also exist in A. We would expect this probability to grow as f (x; w) learns from B. <p> In this way input vectors can be approximated CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 10 by a reduced number of principal components in the feature space <ref> [22, 23] </ref>. PCA can be performed by neural networks Oja [35]. PCA is mentioned here because its nonlinear variant deals directly with issues raised earlier.
Reference: [24] <author> P. Hetherington and M. </author> <title> Seidenberg. </title> <booktitle> Is there 'catastrophic interference' in connectionist networks? In Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 26-33, </pages> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: applications such as pattern recognition where the same interference problems occur. 2.11 Catastrophic Interference Catastrophic Interference [18] occurs when the learning of a new pattern causes complete unlearning of originally trained patterns. 1 Catastrophic Interference is common enough that there is much research aimed at understanding and preventing this problem <ref> [18, 24, 32] </ref>. The solutions found in the pattern recognition literature focus on the symptoms, poor training. To handle the poor training French [18] introduced an algorithm called activation sharpening for MLPs.
Reference: [25] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: Another method for finding an appropriate network size is through the use of network reduction algorithms that typically start with a large network and reduce its size by a variety of means. Pruning <ref> [25] </ref> is one method whereby unimportant connections are severed or, more drastically, entire nodes can be removed if they are found not to be useful. CHAPTER 2. <p> The new term could be designed such that its reduction would lead to a network with limited degrees-of-freedom. Examples of this type of work are ridge regression [21] and related weight decay <ref> [25] </ref>, which control the degrees-of-freedom of function approximators. The idea of a multi-objective cost function is quite useful since it might be easier to specify an additional objective for the cost function rather than a new algorithm that would perform the equivalent function.
Reference: [26] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: CHAPTER 3. A FRAMEWORK FOR LOCALIZATION AND INTERFERENCE 32 the universal-localization theorem of Section 3.2 and well-known, universal-approximation results of single-hidden-layer sigmoidal MLPs. (See, for example, <ref> [20, 14, 26] </ref>). Theorem 2 Let X be a compact subset of R n , H = er w h (x; w), and g fl (x) be a real valued continuous function on X .
Reference: [27] <author> R. A. Jacobs and M. I. Jordan. </author> <title> Learning piecewise control strategies in a modular neural network architecture. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(2) </volume> <pages> 337-345, </pages> <address> Mar.-Apr. </address> <year> 1993. </year>
Reference-contexts: For example, the network might learn to transform itself using the input data in useful ways so that when the desired outputs are provided, the network is able to learn more quickly. This hybrid use of unsupervised and supervised ideas have been pursued by many researchers <ref> [27, 30, 28, 34, 8] </ref>. 2.4 Generalization To the lay-person, the term generalization invokes the idea that novel situations can be handled correctly by using extrapolations from solutions to learned situations. The neural network literature sometimes uses the term generalization in a slightly different and more restricted sense.
Reference: [28] <author> R. A. Jacobs, M. I. Jordan, and A. G. Barto. </author> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision task. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 219-250, </pages> <year> 1991. </year>
Reference-contexts: For example, the network might learn to transform itself using the input data in useful ways so that when the desired outputs are provided, the network is able to learn more quickly. This hybrid use of unsupervised and supervised ideas have been pursued by many researchers <ref> [27, 30, 28, 34, 8] </ref>. 2.4 Generalization To the lay-person, the term generalization invokes the idea that novel situations can be handled correctly by using extrapolations from solutions to learned situations. The neural network literature sometimes uses the term generalization in a slightly different and more restricted sense. <p> To automate the process, centers and widths became adjustable parameters, putting the task of "where to partition" in the hands of the learning algorithm. CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 12 Another automated partitioning schemes was developed by Jacobs et.al. <ref> [28] </ref>. The network they developed was a gated combination of subnets that attempted to learn a pattern, the most successful subnet had its weights modified and gating relevance increased.
Reference: [29] <author> T. A. Johansen and R. Murray-Smith. </author> <title> The operating regime approach to nonliner modelling and control. </title> <editor> In R. Murray-Smith and T. A. Johansen, editors, </editor> <title> Multiple BIBLIOGRAPHY 59 Model Approaches to Modelling and Control, </title> <address> pages 3-72, Bristol, PA, 1997. </address> <publisher> Taylor and Francis. </publisher>
Reference-contexts: When such a scheme is used there is a sequence of four steps that are followed <ref> [29] </ref>. 1. partition the problem into subproblems for 2) and 3) to solve 2. find a good model (network architecture) for each subproblem 3. find an algorithm to adjust the network parameters 4. integrate the solutions of each subproblem to obtain a global solution Notice that steps 2) and 3) are
Reference: [30] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: For example, the network might learn to transform itself using the input data in useful ways so that when the desired outputs are provided, the network is able to learn more quickly. This hybrid use of unsupervised and supervised ideas have been pursued by many researchers <ref> [27, 30, 28, 34, 8] </ref>. 2.4 Generalization To the lay-person, the term generalization invokes the idea that novel situations can be handled correctly by using extrapolations from solutions to learned situations. The neural network literature sometimes uses the term generalization in a slightly different and more restricted sense.
Reference: [31] <author> L. Ljung. </author> <title> Analysis of recursive stochastic algorithms. </title> <journal> IEEE Trans. on Autom. Contr., </journal> <year> 1977. </year>
Reference-contexts: See White [44] for conditions on when convergence to a local minimum is guaranteed for a 1HL MLP. Ljung <ref> [31] </ref> provides a more general theory for learning algorithms using stochastic approximation.
Reference: [32] <author> K. McRae and P. Hetherington. </author> <title> Catastrophic interference is eliminated in pretrained networks. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference-contexts: applications such as pattern recognition where the same interference problems occur. 2.11 Catastrophic Interference Catastrophic Interference [18] occurs when the learning of a new pattern causes complete unlearning of originally trained patterns. 1 Catastrophic Interference is common enough that there is much research aimed at understanding and preventing this problem <ref> [18, 24, 32] </ref>. The solutions found in the pattern recognition literature focus on the symptoms, poor training. To handle the poor training French [18] introduced an algorithm called activation sharpening for MLPs.
Reference: [33] <author> S. Nowlan and G. E. Hinton. </author> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493, </pages> <year> 1992. </year>
Reference-contexts: LITERATURE SURVEY: A MOTIVATION 7 also attempts to reduce some other cost term designed to help the system learn the function more efficiently or accurately <ref> [33] </ref>. If this additional term in the objective function does not require knowledge of the desired output, it can be considered as an unsupervised term. If used alone, it can be used to precondition the network off-line, to have certain characteristics before the desired output data is available.
Reference: [34] <author> S. J. Nowlan and G. E. Hinton. </author> <title> Evaluation of adaptive mixtures of competing experts. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 774-780. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1991. </year>
Reference-contexts: For example, the network might learn to transform itself using the input data in useful ways so that when the desired outputs are provided, the network is able to learn more quickly. This hybrid use of unsupervised and supervised ideas have been pursued by many researchers <ref> [27, 30, 28, 34, 8] </ref>. 2.4 Generalization To the lay-person, the term generalization invokes the idea that novel situations can be handled correctly by using extrapolations from solutions to learned situations. The neural network literature sometimes uses the term generalization in a slightly different and more restricted sense.
Reference: [35] <author> E. Oja. </author> <title> Neural networks, principal components, and subspaces. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 61-68, </pages> <year> 1989. </year>
Reference-contexts: In this way input vectors can be approximated CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 10 by a reduced number of principal components in the feature space [22, 23]. PCA can be performed by neural networks Oja <ref> [35] </ref>. PCA is mentioned here because its nonlinear variant deals directly with issues raised earlier. Nonlinear PCA [36] analyzes the input data and finds an alternate coordinate representation that helps the supervised learning algorithm find a good approximation so that the weights are used more efficiently.
Reference: [36] <author> E. Oja, J. Karhunen, L. Wang, and R. Vigario. </author> <title> Principal and independent components in neural networks recent developments. </title> <booktitle> In Proc. VII Italian Workshop on Neural Nets, </booktitle> <year> 1995. </year>
Reference-contexts: LITERATURE SURVEY: A MOTIVATION 10 by a reduced number of principal components in the feature space [22, 23]. PCA can be performed by neural networks Oja [35]. PCA is mentioned here because its nonlinear variant deals directly with issues raised earlier. Nonlinear PCA <ref> [36] </ref> analyzes the input data and finds an alternate coordinate representation that helps the supervised learning algorithm find a good approximation so that the weights are used more efficiently.
Reference: [37] <author> C. M. Reeves. </author> <title> Conjugate gradient method. </title> <journal> Commun. ACM, </journal> <volume> 7(8):481, </volume> <month> Aug. </month> <year> 1964. </year>
Reference-contexts: Along with an efficient implementation of gradient descent called error-backpropagation [38], a number of variants have made improvements by incorporating second order information, such as back-propagation with momentum, Quick Prop [15], and conjugate gradient <ref> [37] </ref>, to name a few. Sometimes an additional term is added to the standard quadratic error resulting in a multi-objective cost function that not only attempts to reduce the approximation error but CHAPTER 2.
Reference: [38] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, and the PDP research group., editors, </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year> <note> BIBLIOGRAPHY 60 </note>
Reference-contexts: The learning algorithm uses the output error to compute weight changes that, on average, cause the function approximator's I/O map to look more like that of the desired function. Along with an efficient implementation of gradient descent called error-backpropagation <ref> [38] </ref>, a number of variants have made improvements by incorporating second order information, such as back-propagation with momentum, Quick Prop [15], and conjugate gradient [37], to name a few.
Reference: [39] <author> J. Sjoberg, Q. Zhang, L. Ljung, A. Benveniste, B. Deylon, P.-Y. Glorennec, H. Hjal-marsson, and A. Juditsky. </author> <title> Nonlinear black-box modeling in system identification: A unified overview. </title> <journal> Automatica, </journal> <volume> 31 </volume> <pages> 1691-1724, </pages> <year> 1995. </year>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [41, 19, 39, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> The functions shown in Figure 3.5 (b) and 3.5 (c), however, vanish rapidly at positive and negative infinity and therefore exhibit local properties (see Sjoberg et al. <ref> [39] </ref>). The network architecture and weights help determine the interference that occurs and hence play an important role in determining network localization.
Reference: [40] <author> P. P. V. D. Smagt. </author> <title> Minimisation methods for training feedforward neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7 </volume> <pages> 1-11, </pages> <year> 1994. </year>
Reference-contexts: Similarly, in the conjugate gradient method, which computes a conjugate direction and then performs a line search, the conjugate direction can be computed much faster (because of its algebraic form) than the computation of the Hessian matrix <ref> [40] </ref>. Other methods called quasi second-order methods such as Quick Prop [15] have been used to gain speeds comparable to second order methods using slight heuristic modifications to well-established first-order learning methods. 51 CHAPTER 5.
Reference: [41] <author> D. Sofge and D. White. </author> <title> Applied learning: optimal control for manufacturing. </title> <editor> In D. White and D. Sofge, editors, </editor> <booktitle> Handbook of Intelligent Control Neural, Fuzzy, and Adaptive Approaches, </booktitle> <pages> pages 259-281, </pages> <address> New York, NY, 1992. </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: Typically, a description of a local learning system is simply based on the characteristics of the particular network structure, rather than some fundamental definition of localization <ref> [41, 19, 39, 4] </ref>. The literature does not provide a universally accepted description of local learning systems nor does it provide any method for measuring the localization properties of a learning system. <p> INTRODUCTION 2 input space affects the input/output (I/O) map in an undesirable way in other areas of the input space. The general problem of interference has been uncovered in various forms by researchers in many areas <ref> [18, 41] </ref>. For example, consider a dynamical system after it settles into a desired trajectory (where only a small portion of the input space is reached). Suppose that without noise, a network function approximator learns the system dynamics, reduces the approximation error, and then ceases learning. <p> remains active and continually memorizes the system dynamics along the trajectory (because the error never goes to zero) even though there is no need to do so. "Global Network Collapse" results, as the other areas of the input space (those areas not on the trajectory) gradually unlearn due to interference <ref> [41] </ref>. Another variant of the interference problem is in the classification literature: "Catastrophic Interference" occurs when the training of a new pattern causes the unlearning of originally trained patterns [18]. <p> Hence learning will remain in one region for some time before gradually moving on to a different region. In the case of nonlocal networks, focusing on one region, results in an accumulation of interference side-effects and a loss of previously learned information in other areas of the input space <ref> [41] </ref>. These problems are accentuated in non-local networks because the approximation errors in one region of the domain will be reduced at the expense of other regions.
Reference: [42] <author> A. Vemuri. </author> <title> Learning methodologies for non-linear fault diagnosis and accommodation. </title> <type> PhD thesis, </type> <institution> University of Cincinnati, </institution> <year> 1996. </year>
Reference-contexts: learned to be local, learning that focuses on one portion of the state space will not cause interference problems allowing the approximation error to be reduced faster. 5.4 Apply a Localizing Algorithm for Fault Diagnosis and Accommodation (FDA) An application where neural networks have been used successfully is Fault Diagnosis <ref> [42] </ref>. In a typical setup, an on-line approximator monitors a system for any off-nominal behavior due to faults. If faults are present the network is employed to model that portion of the dynamical system that accounts for the fault.
Reference: [43] <author> D. Wang and B. Yuwono. </author> <title> Incremental learning of complex temporal patterns. </title> <type> Technical Report OSU-CISRC-6/95-TR30, </type> <institution> The Ohio State University, Columbus, OH, </institution> <year> 1995. </year>
Reference-contexts: There is some degree of interference but it does not cause catastrophic problems like what we see in artificial networks <ref> [43] </ref>. CHAPTER 2. LITERATURE SURVEY: A MOTIVATION 15 found that the capacity of the network was severely limited by the sparseness restrictions on the hidden layer.
Reference: [44] <author> H. White. </author> <title> Some asymptotic results for learning in single hidden layer feedforward network models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(408) </volume> <pages> 1003-1013, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: See White <ref> [44] </ref> for conditions on when convergence to a local minimum is guaranteed for a 1HL MLP. Ljung [31] provides a more general theory for learning algorithms using stochastic approximation. <p> The two terms will compete for the network's degrees-of-freedom. The expected value of (4.8) is in the direction of negative gradient of (4.7). Similar to White's claim <ref> [44] </ref>, given above, that shows that (4.3) finds a (hopefully low) local minimum of (4.5), we show that (4.8) finds a local minimum of (4.7). To show this we use the same procedure and notation in [44] and let n ; X 00 = J 2 (X n ; X 0 <p> Similar to White's claim <ref> [44] </ref>, given above, that shows that (4.3) finds a (hopefully low) local minimum of (4.5), we show that (4.8) finds a local minimum of (4.7). To show this we use the same procedure and notation in [44] and let n ; X 00 = J 2 (X n ; X 0 n ; Y n ; ) 1 e (X n ; Y n ; ) 2 + (1 ) 2 n ; X 00 then rq n () = H 2 (X; X 0 ; X <p> Proof of Theorem 3: We apply Proposition 3.1 of White's paper <ref> [44] </ref>. Condition 1 ensures that fZ n g is iid and uniformly bounded. <p> Consequently, rQ () = M () T , leading to rQ ()M () = M () T M () 0 for all 2 R l . The conditions of Proposition 3.1 (a) <ref> [44] </ref> thus hold, proving the first result. To prove the second result, we are done if ~ n ! 1. Suppose not. The conditions of Proposition 3.1 (a) [44] have been verified, and M () = rQ () T . <p> The conditions of Proposition 3.1 (a) <ref> [44] </ref> thus hold, proving the first result. To prove the second result, we are done if ~ n ! 1. Suppose not. The conditions of Proposition 3.1 (a) [44] have been verified, and M () = rQ () T . We assume that Q () has isolated stationary points, so it remains to verify the conditions of Proposition 3.1 (b) [44] for each fl 2 fi fl . <p> Suppose not. The conditions of Proposition 3.1 (a) <ref> [44] </ref> have been verified, and M () = rQ () T . We assume that Q () has isolated stationary points, so it remains to verify the conditions of Proposition 3.1 (b) [44] for each fl 2 fi fl . <p> The uniform boundedness of (Y n ; X n ) ensures finiteness of J fl which has been assumed to be positive definite. Thus the conditions of Proposition 3.1 (b) hold, and by Proposition 3.1 (c) <ref> [44] </ref>, ~ n tends to a local minimum of Q ().
References-found: 44

