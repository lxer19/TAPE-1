URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3460/3460.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fomlinc, gilesg@research.nj.nec.com  
Title: Constructing Deterministic Finite-State Automata in Recurrent Neural Networks  
Author: Christian W. Omlin a C. Lee Giles a;b 
Date: Revised February, 1996  
Address: 4 Independence Way, Princeton, NJ 08540  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b UMIACS, U. of Maryland,  U. of Maryland TR  
Pubnum: UMIACS-TR-95-50 and CS-TR-3460  
Abstract: Recurrent neural networks that are trained to behave like deterministic finite-state automata (DFAs) can show deteriorating performance when tested on long strings. This deteriorating performance can be attributed to the instability of the internal representation of the learned DFA states. The use of a sigmoidal discriminant function together with the recurrent structure contribute to this instability. We prove that a simple algorithm can construct second-order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal DFA state representations are stable, i.e. the constructed network correctly classifies strings of arbitrary length. The algorithm is based on encoding strengths of weights directly into the neural network. We derive a relationship between the weight strength and the number of DFA states for robust string classification. For a DFA with n states and m input alphabet symbols, the constructive algorithm generates a "programmed" neural network with O(n) neurons and O(mn) weights. We compare our algorithm to other methods proposed in the literature.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, A. Dewdney, and T. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 495-514, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This is important when a network is used for domain theory revision [19, 27, 30], where the prior knowledge is not only incomplete, but may also be incorrect [13, 22]. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed <ref> [1, 18, 21] </ref>. This paper is concerned with neural network implementations of DFAs where continuous sigmoidal discriminant functions are used. <p> conjecture holds true: Conjecture 5.10.1 If any one of the conditions is violated, then the languages accepted by the constructed network and the given DFA are not identical for an arbitrary distribution of the randomly initialized weights in the interval [W; W ]. 5.11 Comparison with other Methods Different methods <ref> [1, 7, 5, 18, 21] </ref> for encoding DFAs with n states and m input symbols in recurrent networks are summarized in table 1. <p> The methods differ in the choice of the discriminant function (hard-limiting, sigmoidal, radial basis function), the size of the constructed network and the restrictions that are imposed on the weight alphabet, the neuron fan-in and fan-out. The results in [18] improve the upper and lower bounds reported in <ref> [1] </ref> for DFAs with only two input symbols. Those bounds can be generalized to DFAs with m input symbols [17]. <p> Among the methods which use continuous discriminant functions, our algorithm uses no more neurons than the best of all methods, and consistently uses fewer weights and smaller fan-out size than all methods. 5.12 Open Problems One of the theoretical results in <ref> [1] </ref> gives a lower bound of ( p nlogn) on the number of hard-limiting neurons needed to implement a DFA with n states when the weight alphabet and the neuron fan-in are limited.
Reference: [2] <author> M. </author> <title> Barnsley, Fractals Everywhere. </title> <address> San Diego, CA: </address> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: First, we define the concept of fixed points of a function <ref> [2] </ref>: Definition 5.3.1 Let f : X ! X be a mapping on a metric space (X, d). A point x f 2 X such that f (x f ) = x f is called a fixed point of the mapping.
Reference: [3] <author> M. Casey, </author> <title> "The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 6, </volume> <year> 1996. </year> <note> In Press. </note>
Reference-contexts: H denotes the rule strength. The operation S i I k is shown as . that the internal DFAstate representation becomes unstable with increasing string length due to the network's dynamical nature and the sigmoidal discriminant function. This phenomenon has also been observed by others <ref> [3, 29, 32] </ref>. We encoded a randomly generated, minimized 100-state DFA with alphabet = f0; 1g into a recurrent network with 101 state neurons.
Reference: [4] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks.
Reference: [5] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Representation of finite state automata in recurrent radial basis function networks," </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year> <note> In press. </note>
Reference-contexts: conjecture holds true: Conjecture 5.10.1 If any one of the conditions is violated, then the languages accepted by the constructed network and the given DFA are not identical for an arbitrary distribution of the randomly initialized weights in the interval [W; W ]. 5.11 Comparison with other Methods Different methods <ref> [1, 7, 5, 18, 21] </ref> for encoding DFAs with n states and m input symbols in recurrent networks are summarized in table 1.
Reference: [6] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "A unified approach for integrating explicit knowledge and learning by example in recurrent networks," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> vol. 1, </volume> <editor> p. </editor> <volume> 811, </volume> <publisher> IEEE 91CH3049-4, </publisher> <year> 1991. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks. <p> Our proof of stability of an internal DFA state representation establishes such a result for a special case of discrete-time recurrent networks. Our method is an alternative to an algorithm for constructing DFAs in recurrent networks with first-order weights proposed by Frasconi et al. <ref> [6, 7] </ref>. A short introduction to finite-state automata will be followed by a review of the method by Frasconi et al. <p> Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 3 FIRST-ORDER NETWORKS This section summarizes work done by Frasconi et al. on implementing DFAs in recurrent neural networks. For details of the algorithms and the proofs see <ref> [6, 7] </ref>. The work in [7] is an extension of [6] which restricted the class of automata that could be encoded into recurrent networks to DFAs without cycles (except self 3 loops). The authors were focusing on automatic speech recognition as an application of implementing DFAs in recurrent neural networks. <p> For details of the algorithms and the proofs see [6, 7]. The work in [7] is an extension of <ref> [6] </ref> which restricted the class of automata that could be encoded into recurrent networks to DFAs without cycles (except self 3 loops). The authors were focusing on automatic speech recognition as an application of implementing DFAs in recurrent neural networks.
Reference: [7] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda, "Injecting nondeterministic finite state automata into recurrent networks," </title> <type> tech. rep., </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy, </institution> <year> 1993. </year>
Reference-contexts: Our proof of stability of an internal DFA state representation establishes such a result for a special case of discrete-time recurrent networks. Our method is an alternative to an algorithm for constructing DFAs in recurrent networks with first-order weights proposed by Frasconi et al. <ref> [6, 7] </ref>. A short introduction to finite-state automata will be followed by a review of the method by Frasconi et al. <p> Alternatively, a DFA M can also be considered a generator which generates the regular language L (M ). 3 FIRST-ORDER NETWORKS This section summarizes work done by Frasconi et al. on implementing DFAs in recurrent neural networks. For details of the algorithms and the proofs see <ref> [6, 7] </ref>. The work in [7] is an extension of [6] which restricted the class of automata that could be encoded into recurrent networks to DFAs without cycles (except self 3 loops). The authors were focusing on automatic speech recognition as an application of implementing DFAs in recurrent neural networks. <p> For details of the algorithms and the proofs see [6, 7]. The work in <ref> [7] </ref> is an extension of [6] which restricted the class of automata that could be encoded into recurrent networks to DFAs without cycles (except self 3 loops). The authors were focusing on automatic speech recognition as an application of implementing DFAs in recurrent neural networks. <p> Proof: The lemma is proven by defining an appropriate Lyapunov function P (.) and showing that P (:) decreases toward the minima h and + h under the iteration h p (x; H) <ref> [7] </ref>: Let P (x i ) = (x i + h ) 2 . It follows that P decreases only if x i approaches the fixed point + h . <p> conjecture holds true: Conjecture 5.10.1 If any one of the conditions is violated, then the languages accepted by the constructed network and the given DFA are not identical for an arbitrary distribution of the randomly initialized weights in the interval [W; W ]. 5.11 Comparison with other Methods Different methods <ref> [1, 7, 5, 18, 21] </ref> for encoding DFAs with n states and m input symbols in recurrent networks are summarized in table 1. <p> The method proposed in <ref> [7] </ref> implements DFAs using linear programming and explicit implementation of state transitions implementing boolean-like functions with sigmoidal neurons. The authors give rigorous proofs about their neural network implementation of DFAs. An interesting characteristic of their approach is that state transitions usually take several time steps to complete. <p> Our algorithm for constructing DFAs in recurrent neural networks is more straightforward compared to the method proposed in <ref> [7] </ref>. By using second-order weights, we have adjusted the network architecture so that DFA state transitions are naturally mapped into network state transitions. Our networks need fewer nodes and weights than the implementation reported in [7]. <p> constructing DFAs in recurrent neural networks is more straightforward compared to the method proposed in <ref> [7] </ref>. By using second-order weights, we have adjusted the network architecture so that DFA state transitions are naturally mapped into network state transitions. Our networks need fewer nodes and weights than the implementation reported in [7]. The network model has not lost any of its computational capabilities by the introduction of second-order weights.
Reference: [8] <author> S. Geman, E. Bienenstock, and R. Dourstat, </author> <title> "Neural networks and the bias/variance dilemma," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 1-58, </pages> <year> 1992. </year>
Reference-contexts: For enhanced performance, some of these neural network algorithms are mapped directly into VLSI designs [20, 28]. Neural networks readily enhance their performance by having a priori knowledge about the problem to be solved encoded or used in the neural network <ref> [8, 27] </ref>. This work discusses how a priori finite state automata rules can be encoded into a recurrent neural network with sigmoid activation neurons in such a way that arbitrary long string sequences are always correctly recognized a stable encoding of rules.
Reference: [9] <author> C. Giles, D. Chen, C. Miller, H. Chen, G. Sun, and Y. Lee, </author> <title> "Second-order recurrent neural networks for grammatical inference," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <volume> vol. II, </volume> <pages> pp. 273-281, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks.
Reference: [10] <author> C. Giles, G. Kuhn, and R. Williams, </author> <title> "Dynamic recurrent neural networks: </title> <journal> Theory and applications," IEEE Transactions on Neural Networks, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 153-156, </pages> <year> 1994. </year>
Reference-contexts: This feedback property enables such neural nets 1 to be used in problems and applications which require state representation: speech processing, plant control, adaptive signal processing, time series prediction, engine diagnostics etc. (for example see the recent special issue on dynamically-driven recurrent neural networks <ref> [10] </ref>). For enhanced performance, some of these neural network algorithms are mapped directly into VLSI designs [20, 28]. Neural networks readily enhance their performance by having a priori knowledge about the problem to be solved encoded or used in the neural network [8, 27].
Reference: [11] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1992. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks.
Reference: [12] <author> C. Giles and C. Omlin, </author> <title> "Inserting rules into recurrent neural networks," in Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of The 1992 IEEE Workshop (S. </booktitle> <editor> Kung, F. Fallside, J. A. Sorenson, and C. Kamm, </editor> <booktitle> eds.), </booktitle> <pages> pp. 13-22, </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Recently, we have developed a simple method for encoding partial DFAs (state transitions) into recurrent neural networks <ref> [12, 24] </ref>. The goal was to demonstrate that prior knowledge can decrease the learning time significantly compared to learning without any prior knowledge. The training time improvement was `proportional' to the amount of prior knowledge with which a network was initialized. <p> (denoted by ), are fed through a sigmoidal discriminant function t () to compute the next network state S t+1 i . 4 SECOND-ORDER NETWORKS The algorithm used here to construct DFAs in networks with second-order weights has also been used to encode partial prior knowledge to improve convergence time <ref> [12, 24] </ref>, and to perform rule correction [13, 22]. 4.1 Network Construction We use discrete-time, recurrent networks with weights W ijk to implement DFAs.
Reference: [13] <author> C. Giles and C. Omlin, </author> <title> "Rule refinement with recurrent neural networks," </title> <booktitle> in Proceedings IEEE International Conference on Neural Networks (ICNN'93), </booktitle> <volume> vol. II, </volume> <pages> pp. 801-806, </pages> <year> 1993. </year> <month> 38 </month>
Reference-contexts: This is important when a network is used for domain theory revision [19, 27, 30], where the prior knowledge is not only incomplete, but may also be incorrect <ref> [13, 22] </ref>. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed [1, 18, 21]. This paper is concerned with neural network implementations of DFAs where continuous sigmoidal discriminant functions are used. <p> sigmoidal discriminant function t () to compute the next network state S t+1 i . 4 SECOND-ORDER NETWORKS The algorithm used here to construct DFAs in networks with second-order weights has also been used to encode partial prior knowledge to improve convergence time [12, 24], and to perform rule correction <ref> [13, 22] </ref>. 4.1 Network Construction We use discrete-time, recurrent networks with weights W ijk to implement DFAs.
Reference: [14] <author> M. Hirsch, </author> <title> "Convergent activation dynamics in continuous-time neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 331-351, </pages> <year> 1989. </year>
Reference-contexts: Stability of an internal DFA state representation implies that the output of the sigmoidal state neurons assigned to DFA states saturate at high gain; a constructed discrete-time network thus has stable periodic 2 orbits. A saturation result has previously been proven for continuous-time networks <ref> [14] </ref>; for sufficiently high gain, the output along a stable limit cycle is saturated almost all the time. There is no known analog of this for stable periodic orbits of discrete-time networks.
Reference: [15] <author> M. Hirsch, </author> <title> "Saturation at high gain in discrete time recurrent networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 3, </volume> <pages> pp. 449-453, </pages> <year> 1994. </year>
Reference-contexts: There is no known analog of this for stable periodic orbits of discrete-time networks. The only known stability result asserts that for a broad class of discrete-time networks where all output neurons are either self-inhibiting or self-exciting, outputs at stable fixed points saturate at high gain <ref> [15] </ref>. Our proof of stability of an internal DFA state representation establishes such a result for a special case of discrete-time recurrent networks. Our method is an alternative to an algorithm for constructing DFAs in recurrent networks with first-order weights proposed by Frasconi et al. [6, 7].
Reference: [16] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference-contexts: Finally, we will compare DFA encoding algorithm with other methods proposed in the literature. 2 FINITE STATE AUTOMATA Regular languages represent the smallest class of formal languages in the Chomsky hierarchy <ref> [16] </ref>. Regular languages are generated by regular grammars.
Reference: [17] <author> B. </author> <title> Horne. </title> <type> Personal Communication. </type>
Reference-contexts: The results in [18] improve the upper and lower bounds reported in [1] for DFAs with only two input symbols. Those bounds can be generalized to DFAs with m input symbols <ref> [17] </ref>.
Reference: [18] <author> B. Horne and D. Hush, </author> <title> "Bounds on the complexity of recurrent neural network implementations of finite state machines," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 9, no. 2, </volume> <pages> pp. 243-252, </pages> <year> 1996. </year>
Reference-contexts: This is important when a network is used for domain theory revision [19, 27, 30], where the prior knowledge is not only incomplete, but may also be incorrect [13, 22]. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed <ref> [1, 18, 21] </ref>. This paper is concerned with neural network implementations of DFAs where continuous sigmoidal discriminant functions are used. <p> conjecture holds true: Conjecture 5.10.1 If any one of the conditions is violated, then the languages accepted by the constructed network and the given DFA are not identical for an arbitrary distribution of the randomly initialized weights in the interval [W; W ]. 5.11 Comparison with other Methods Different methods <ref> [1, 7, 5, 18, 21] </ref> for encoding DFAs with n states and m input symbols in recurrent networks are summarized in table 1. <p> The methods differ in the choice of the discriminant function (hard-limiting, sigmoidal, radial basis function), the size of the constructed network and the restrictions that are imposed on the weight alphabet, the neuron fan-in and fan-out. The results in <ref> [18] </ref> improve the upper and lower bounds reported in [1] for DFAs with only two input symbols. Those bounds can be generalized to DFAs with m input symbols [17].
Reference: [19] <author> R. Maclin and J. Shavlik, </author> <title> "Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman Algorithm for Protein Folding," </title> <journal> Machine Learning, </journal> <volume> vol. 11, </volume> <pages> pp. 195-215, </pages> <year> 1993. </year>
Reference-contexts: When partial symbolic knowledge is encoded into a network in order to improve training, programming as few weights as possible is desirable because it leaves the network with many unbiased adaptable weights. This is important when a network is used for domain theory revision <ref> [19, 27, 30] </ref>, where the prior knowledge is not only incomplete, but may also be incorrect [13, 22]. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed [1, 18, 21].
Reference: [20] <author> C. Mead, </author> <title> Analog VLSI and Neural Systems. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: For enhanced performance, some of these neural network algorithms are mapped directly into VLSI designs <ref> [20, 28] </ref>. Neural networks readily enhance their performance by having a priori knowledge about the problem to be solved encoded or used in the neural network [8, 27].
Reference: [21] <author> M. Minsky, </author> <title> Computation: Finite and Infinite Machines, </title> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 32-66. </pages> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1967. </year>
Reference-contexts: This is important when a network is used for domain theory revision [19, 27, 30], where the prior knowledge is not only incomplete, but may also be incorrect [13, 22]. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed <ref> [1, 18, 21] </ref>. This paper is concerned with neural network implementations of DFAs where continuous sigmoidal discriminant functions are used. <p> conjecture holds true: Conjecture 5.10.1 If any one of the conditions is violated, then the languages accepted by the constructed network and the given DFA are not identical for an arbitrary distribution of the randomly initialized weights in the interval [W; W ]. 5.11 Comparison with other Methods Different methods <ref> [1, 7, 5, 18, 21] </ref> for encoding DFAs with n states and m input symbols in recurrent networks are summarized in table 1.
Reference: [22] <author> C. Omlin and C. Giles, </author> <title> "Rule revision with recurrent neural networks," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 183-188, </pages> <year> 1996. </year>
Reference-contexts: This is important when a network is used for domain theory revision [19, 27, 30], where the prior knowledge is not only incomplete, but may also be incorrect <ref> [13, 22] </ref>. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed [1, 18, 21]. This paper is concerned with neural network implementations of DFAs where continuous sigmoidal discriminant functions are used. <p> sigmoidal discriminant function t () to compute the next network state S t+1 i . 4 SECOND-ORDER NETWORKS The algorithm used here to construct DFAs in networks with second-order weights has also been used to encode partial prior knowledge to improve convergence time [12, 24], and to perform rule correction <ref> [13, 22] </ref>. 4.1 Network Construction We use discrete-time, recurrent networks with weights W ijk to implement DFAs.
Reference: [23] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 4, </volume> <year> 1996. </year> <note> In Press. </note>
Reference-contexts: The question of how H scales with network size is important. The empirical results in <ref> [23] </ref> indicate that H 6 for randomly generated DFAs independent of the size of the DFA.
Reference: [24] <author> C. Omlin and C. Giles, </author> <title> "Training second-order recurrent neural networks using hints," </title> <booktitle> in Proceedings of the Ninth International Conference on Machine Learning (D. </booktitle> <editor> Sleeman and P. Edwards, eds.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 363-368, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Recently, we have developed a simple method for encoding partial DFAs (state transitions) into recurrent neural networks <ref> [12, 24] </ref>. The goal was to demonstrate that prior knowledge can decrease the learning time significantly compared to learning without any prior knowledge. The training time improvement was `proportional' to the amount of prior knowledge with which a network was initialized. <p> (denoted by ), are fed through a sigmoidal discriminant function t () to compute the next network state S t+1 i . 4 SECOND-ORDER NETWORKS The algorithm used here to construct DFAs in networks with second-order weights has also been used to encode partial prior knowledge to improve convergence time <ref> [12, 24] </ref>, and to perform rule correction [13, 22]. 4.1 Network Construction We use discrete-time, recurrent networks with weights W ijk to implement DFAs.
Reference: [25] <author> J. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks.
Reference: [26] <author> D. Servan-Schreiber, A. Cleeremans, and J. McClelland, </author> <title> "Graded state machine: The representation of temporal contingencies in simple recurrent networks," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <editor> p. </editor> <volume> 161, </volume> <year> 1991. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks.
Reference: [27] <author> J. Shavlik, </author> <title> "Combining symbolic and neural learning," </title> <journal> Machine Learning, </journal> <volume> vol. 14, no. 3, </volume> <pages> pp. 321-331, </pages> <year> 1994. </year>
Reference-contexts: For enhanced performance, some of these neural network algorithms are mapped directly into VLSI designs [20, 28]. Neural networks readily enhance their performance by having a priori knowledge about the problem to be solved encoded or used in the neural network <ref> [8, 27] </ref>. This work discusses how a priori finite state automata rules can be encoded into a recurrent neural network with sigmoid activation neurons in such a way that arbitrary long string sequences are always correctly recognized a stable encoding of rules. <p> When partial symbolic knowledge is encoded into a network in order to improve training, programming as few weights as possible is desirable because it leaves the network with many unbiased adaptable weights. This is important when a network is used for domain theory revision <ref> [19, 27, 30] </ref>, where the prior knowledge is not only incomplete, but may also be incorrect [13, 22]. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed [1, 18, 21].
Reference: [28] <author> B. J. Sheu, </author> <booktitle> Neural Information Processing and VLSI. </booktitle> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: For enhanced performance, some of these neural network algorithms are mapped directly into VLSI designs <ref> [20, 28] </ref>. Neural networks readily enhance their performance by having a priori knowledge about the problem to be solved encoded or used in the neural network [8, 27].
Reference: [29] <author> P. Tino, B. Horne, and C. Giles, </author> <title> "Fixed points in two-neuron discrete time recurrent networks: Stability and bifurcation considerations," </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-95-51, Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, </institution> <year> 1995. </year> <month> 39 </month>
Reference-contexts: H denotes the rule strength. The operation S i I k is shown as . that the internal DFAstate representation becomes unstable with increasing string length due to the network's dynamical nature and the sigmoidal discriminant function. This phenomenon has also been observed by others <ref> [3, 29, 32] </ref>. We encoded a randomly generated, minimized 100-state DFA with alphabet = f0; 1g into a recurrent network with 101 state neurons.
Reference: [30] <author> G. Towell, J. Shavlik, and M. Noordewier, </author> <title> "Refinement of approximately correct domain theories by knowledge-based neural networks," </title> <booktitle> in Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> (San Mateo, CA), p. 861, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: When partial symbolic knowledge is encoded into a network in order to improve training, programming as few weights as possible is desirable because it leaves the network with many unbiased adaptable weights. This is important when a network is used for domain theory revision <ref> [19, 27, 30] </ref>, where the prior knowledge is not only incomplete, but may also be incorrect [13, 22]. Methods for constructing DFAs in recurrent networks where neurons have hard-limiting discriminant functions have been proposed [1, 18, 21].
Reference: [31] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite-state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 406, </volume> <year> 1992. </year>
Reference-contexts: Thus, this encoding methodology would permit rules to be mapped into neural network VLSI chips, offering the potential of greatly increasing the versatility of neural network implementations. 1.2 Background Recurrent neural networks can be trained to behave like deterministic finite-state automata (DFAs) <ref> [4, 6, 9, 11, 25, 26, 31] </ref>. The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings [32]; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks.
Reference: [32] <author> Z. Zeng, R. Goodman, and P. Smyth, </author> <title> "Learning finite state machines with self-clustering recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 976-990, </pages> <year> 1993. </year> <month> 40 </month>
Reference-contexts: The dynamical nature of recurrent networks can cause the internal representation of learned DFA states to deteriorate for long strings <ref> [32] </ref>; therefore, it can be difficult to make predictions about the generalization performance of trained recurrent networks. Recently, we have developed a simple method for encoding partial DFAs (state transitions) into recurrent neural networks [12, 24]. <p> H denotes the rule strength. The operation S i I k is shown as . that the internal DFAstate representation becomes unstable with increasing string length due to the network's dynamical nature and the sigmoidal discriminant function. This phenomenon has also been observed by others <ref> [3, 29, 32] </ref>. We encoded a randomly generated, minimized 100-state DFA with alphabet = f0; 1g into a recurrent network with 101 state neurons.
References-found: 32

