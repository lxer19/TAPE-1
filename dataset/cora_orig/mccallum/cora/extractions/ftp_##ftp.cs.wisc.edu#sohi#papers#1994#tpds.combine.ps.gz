URL: ftp://ftp.cs.wisc.edu/sohi/papers/1994/tpds.combine.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Email: &lt;alvy,sohi&gt;@cs.wisc.edu  
Title: Request Combining in Multiprocessors with Arbitrary Interconnection Networks  
Author: Alvin R. Lebeck and Gurindar S. Sohi 
Note: To appear in IEEE Transactions on Parallel and Distributed Systems  
Address: 1210 W. Dayton street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Several techniques have been proposed to allow parallel access to a shared memory location by combining requests; they have one or more of the following attributes: requirement for a priori knowledge of the requests to combine, restrictions on the routing of messages in the network, or the use of sophisticated interconnection network nodes. We present a new method of combining requests that does not have the above requirements. We obtain this new method for request combining by developing a classification scheme for the existing methods of request combining. This classification scheme is facilitated by separating the request combining process into a two part operation: (i) determining the combining set which is the set of requests that participate in a combined access, and (ii) distributing the results of the combined access to the members of the combining set. The classification of combining strategies is based upon which system component, processor elements or interconnection network, performs each of these tasks. Our approach, which uses the interconnection network to establish the combining set, and the processor elements to distribute the results, lies in an unexplored area of the design space. We also present simulation results to assess the benefits of the proposed approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almasi, G. S. and Gottlieb, A., </author> <title> Highly Parallel Computing. </title> <address> Redwood City, CA: </address> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1989. </year>
Reference-contexts: This implies that the return path must be identical to the forward path for decombining to take place, or the return path must have at least one node in common with the forward paththe node where the combining state is stored. Almasi and Gottlieb <ref> [1] </ref> - 2 - give several examples of how such hardware combining can eliminate serial bottlenecks. Several alternative proposals for request combining have appeared in the literature [8, 11, 19, 21, 29, 32]. The primary focus of these efforts is on reducing the cost of the combining network.
Reference: [2] <author> Arvind, and Iannucci, R. A., </author> <title> ``A Critique of Multiprocessing von Neumann Style,'' </title> <booktitle> in Proc. 10th International Symposium on Computer Architecture, Stockholm, </booktitle> <pages> pp. 426-436, </pages> <year> 1983. </year>
Reference-contexts: 1. Introduction Arvind and Iannucci state that the design of a large-scale, shared memory multiprocessor must address two basic issues <ref> [2] </ref>: (1) it must tolerate long latencies for memory requests, (2) it must achieve unconstrained, yet synchronized, access to shared data.
Reference: [3] <author> Blelloch, G. E., </author> <title> ``Scans as Primitive Parallel Operations,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C38, 11, </volume> <pages> pp. 1526-1538, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We call this set of requests the combining set. The second task is to distribute the results of the combined access to the appropriate processors by performing a prefix computation on the combining set. A prefix computation network, such as the one proposed for scan primitives by Blelloch <ref> [3] </ref>, can be used to distribute the results of the combined access. In such a network, state is saved on the forward trip through the prefix computation network, and the results are distributed on the return trip, very similar to the Ultracomputer approach towards combining. <p> As with schemes that perform IIC, schemes performing PIC use the interconnection network to distribute the results of combined requests. However, the processor elements, and not the interconnection network, determine the combining set. Blelloch's prefix computation network <ref> [3] </ref>, discussed in Section 2.1, and the control network of the Thinking Machines CM-5 [17] fall into this category. Another form of PIC in a SIMD paradigm is proposed by Lipovski and Vaughan [19].
Reference: [4] <author> Freudenthal, E. and Gottlieb, A., </author> <title> ``Process Coordination with Fetch-and-Increment,'' </title> <booktitle> Proceedings ASPLOS-IV, </booktitle> <pages> pp. 260-268, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The ease with which combining can be carried out (in special cases), prompted Freudenthal and Gottlieb to investigate the use of the Fetch&Increment operation in place of the more general F&A operation <ref> [4] </ref>. When broadcast is not an option, some other method must be used to determine the combining set and to carry out the prefix operation on the combining set.
Reference: [5] <author> Goodman, J. R., Vernon, M. K., and Woest, P. J., </author> <title> ``A Set of Efficient Synchronization Primitives for a Large-Scale Shared-Memory Multiprocessor,'' </title> <booktitle> in Proceedings ASPLOS-III, </booktitle> <address> Boston, MA, </address> <pages> pp. 64-73, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The nodes of a combining tree are realized by the implicit storage in the interconnect nodes, i.e., wait buffers. Another alternative is to use explicit storage in memory to construct the combining tree. This method of request combining called software combining in the literature <ref> [5, 31, 32] </ref>, is classified as PPC since the processors bear full responsibility for the combining of requests: the processors establish the combining set and distribute the results and there are no demands of the network at all. <p> However, the L locations (nodes of the combining tree) must be distributed across the memory modules in order to alleviate excessive contention for a single memory module. Yew, Tzeng, and Lawrie show how software combining can be used for barrier operations [32]. Goodman, Vernon and Woest <ref> [5] </ref> and Johnson [12] extend the work of Yew, Tzeng, and Lawrie to carry out arbitrary Fetch& F operations with a software combining tree.
Reference: [6] <author> Gottlieb, A., Grishman, R., Kruskal, C. P., McAuliffe, K. P., Rudolph, L., and Snir, M., </author> <title> ``The NYU Ultra-computer -- Designing a MIMD, Shared Memory Parallel Machine,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, </volume> <pages> pp. 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: When the response of the read request arrives at the node where combining took place, two responses are sent back toward the processors. The idea of combining read requests, or read combining in CHoPP was extended in the NYU Ultracomputer to allow several types of requests to combine <ref> [6] </ref>. The Ultracomputer uses the Fetch& F primitive, where F is any associative and commutative operator. An enhanced interconnection network with the topology of an Omega network is proposed to perform combining on the Fetch& F primitive. The Ultracomputer style of request combining is illustrated in Figure 1. <p> Correct operation is guaranteed if the combining of requests satisfies the serialization principle: the final state of the system must be consistent with the servicing of all requests in some (unspecified) serial order <ref> [6] </ref>. There are three distinct features of the Ultracomputer style of request combining: (1) Requests are combined on the forward trip through the network. (2) State is saved in the network when requests are combined. (3) Requests are decombined on the return trip through the network. <p> In the following subsections we discuss existing methods of request combining according to which region of the design space they belong to. 2.2.1. Interconnect-Interconnect Combining (IIC) The CHoPP [28] and the NYU Ultracomputer <ref> [6] </ref> methods of request combining are instances of IIC: the interconnection network determines the combining set and distributes the results. The IBM RP3 [21] researchers proposed the basic ideas of the Ultracomputer method of combining for their implementation.
Reference: [7] <author> Hagersten, E., Landin, A., and Haridi, S., </author> <title> ``DDM A Cache-Only Memory Architecture,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: An interesting example of IIC read combining can be found in schemes with hierarchical cache/memory structures, such as Cache-Only Memory Architecture (COMA) machines <ref> [7, 27] </ref>, or Non-Uniform Memory Access (NUMA) machines with hierarchical caches [18, 23, 30]. Here read combining can be implemented by using a technique similar to the CHoPP method of read combining.
Reference: [8] <author> Harrison, M. C., </author> <title> ``Synchronous Combining of Fetch-And-Add Operations,'' Ultracomputer Note #71, </title> <month> April </month> <year> 1984. </year>
Reference-contexts: Almasi and Gottlieb [1] - 2 - give several examples of how such hardware combining can eliminate serial bottlenecks. Several alternative proposals for request combining have appeared in the literature <ref> [8, 11, 19, 21, 29, 32] </ref>. The primary focus of these efforts is on reducing the cost of the combining network. This is accomplished either by altering the topology of the combining network or by requiring the system software to reduce the amount of contention for shared data. <p> The prefix computation network may be extended for operation in a Multiple Instruction Multiple Data (MIMD) paradigm, though the authors do not explicitly state how this might be done. An alternative technique for combining requests in a MIMD paradigm, proposed by Harrison <ref> [8] </ref>, uses a synchronous prefix computation network. All requests at the same stage in the network combine. Therefore, the entire combining set must be inserted into the network in the same time slot. This is accomplished by broadcasting information about the combinable locations to the processors.
Reference: [9] <author> Heidelberger, P., Rathi, B. D., and Stone, H. S., </author> <title> ``A Device for Performing Efficient Task Distribution with a Bus Connection,'' </title> <journal> IBM Research, Technical Disclosure YO889-0053, </journal> <volume> vol. 20, </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: Interconnect-Processor Combining We now consider implementations of request combining that fall into the unexplored region of the design space, Interconnect-Processor Combining (IPC). We initially consider two flavors of combinable operations: a restricted form of Fetch&Add (F&A), or Fetch&Increment <ref> [9, 26] </ref>, and the general F&A operation. In Fetch&Increment, or simply F&I, all participants add the same, constant value. In the general F&A, each participant could be adding a different value. <p> The following example illustrates this point. processor is assigned one channel in the ``bus'' (the channel could be a wire in an electronic bus [26] or a specific frequency in an optical bus <ref> [9] </ref> ). A given processor can read all channels, but can write to only its channel. A processor generates a combinable request and broadcasts its intentions on the bus, by putting a "1" on its channel. <p> The method described above was proposed independently for an electrical bus by Sohi, Smith, and Goodman [26] and for an optical bus by Hiedelberger, Rathi, and Stone <ref> [9] </ref>. The ease with which combining can be carried out (in special cases), prompted Freudenthal and Gottlieb to investigate the use of the Fetch&Increment operation in place of the more general F&A operation [4].
Reference: [10] <author> Hillis, W. D. and Steele, G. L., </author> <title> ``Data Parallel Algorithms,'' </title> <journal> CACM, </journal> <pages> pp. 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: To eliminate this serialization for the distribution of results we turn to the literature on parallel applications to see how the prefix operation on the combining set can be carried out in parallel. Several algorithms for performing a parallel prefix computation on a linked list exist in the literature <ref> [10, 13, 15, 20] </ref>. Most of the algorithms are concerned with the case of having more nodes in the linked list than processors available. In our case the number of nodes in the linked list is equivalent to the number of processors that are participating in the combined access. <p> In our case the number of nodes in the linked list is equivalent to the number of processors that are participating in the combined access. Therefore, we use the all partial sums algorithm given by Hillis and Steele <ref> [10] </ref> and shown in Figure 7. The algorithm uses recursive doubling: each iteration of the loop performs half as many operations as the previous iteration until the entire computation is complete.
Reference: [11] <author> Hsu, W. T. and Yew, P.-C., </author> <title> ``An Effective Synchronization Network for Hot-Spot Accesses,'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 10, </volume> <pages> pp. 167-189, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Almasi and Gottlieb [1] - 2 - give several examples of how such hardware combining can eliminate serial bottlenecks. Several alternative proposals for request combining have appeared in the literature <ref> [8, 11, 19, 21, 29, 32] </ref>. The primary focus of these efforts is on reducing the cost of the combining network. This is accomplished either by altering the topology of the combining network or by requiring the system software to reduce the amount of contention for shared data. <p> A distinction is made between non-combinable and potentially combinable requests (typically synchronization requests), and the interconnect dynamically determines the combining set of the potentially combinable requests. Two alternative techniques for IIC are presented by Tzeng [29] and Hsu and Yew <ref> [11] </ref>. Tzeng separates the interconnect into a routing section and a combining section. It is assumed that requests which may combine are distinguished from non-combining requests by examination of the opcode. Such requests are directed to the combining section of the network.
Reference: [12] <author> Johnson, R. E., </author> <title> ``Extending the Scalable Coherent Interface for Large-Scale Shared-Memory Multiprocessors,'' </title> <type> Ph. D. Thesis, </type> <institution> Department of Computer Science (Technical Report #1136), University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: However, the L locations (nodes of the combining tree) must be distributed across the memory modules in order to alleviate excessive contention for a single memory module. Yew, Tzeng, and Lawrie show how software combining can be used for barrier operations [32]. Goodman, Vernon and Woest [5] and Johnson <ref> [12] </ref> extend the work of Yew, Tzeng, and Lawrie to carry out arbitrary Fetch& F operations with a software combining tree. <p> to maintain the combining set, as well as the algorithm used to carry out the prefix operation on the combining set. (One step in this direction is a recent thesis by Johnson where he investigates the use of IPC to build a tree to implement a scalable cache coherence scheme <ref> [12] </ref>. ) More direct comparisons between the different forms of combining, using real application workloads, and different network topologies, also need to be done so that we can get a better picture of the cost-performance benefits of the various techniques for request combining. - 18 -
Reference: [13] <author> Kogge, P. M. and Stone, H. S., </author> <title> ``A Parallel Algorithm for the Efficient Solution of a General Class of Recurrence Equations,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-22, </volume> <pages> pp. 786-793, </pages> <month> August </month> <year> 1973. </year>
Reference-contexts: To eliminate this serialization for the distribution of results we turn to the literature on parallel applications to see how the prefix operation on the combining set can be carried out in parallel. Several algorithms for performing a parallel prefix computation on a linked list exist in the literature <ref> [10, 13, 15, 20] </ref>. Most of the algorithms are concerned with the case of having more nodes in the linked list than processors available. In our case the number of nodes in the linked list is equivalent to the number of processors that are participating in the combined access.
Reference: [14] <author> Kruskal, C. P., Rudolph, L., and Snir, M, </author> <title> ``Efficeint Synchronization on Multiprocessors with Shared Memory,'' </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 10, 4, </volume> <pages> pp. 579-601, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The potential of the new combining scheme is evaluated in section 4. Section 5 summarizes the paper and suggests directions for further research. 2. A Taxonomy for Request Combining 2.1. Parallel Prefix Computation and Request Combining Kruskal, Rudolph, and Snir <ref> [14] </ref> observed that request combining is very similar to the problem of parallel prefix computation [15]. Given the elements x 1 ,x 2 ,...,x n a prefix computation produces the results: I L r 1 = x 1 M O where F is any associative operator.
Reference: [15] <author> Ladner, R. E. and Fischer, M. J., </author> <title> ``Parallel Prefix Computation,'' </title> <journal> JACM, </journal> <volume> vol. 27, </volume> <pages> pp. 831-838, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: Section 5 summarizes the paper and suggests directions for further research. 2. A Taxonomy for Request Combining 2.1. Parallel Prefix Computation and Request Combining Kruskal, Rudolph, and Snir [14] observed that request combining is very similar to the problem of parallel prefix computation <ref> [15] </ref>. Given the elements x 1 ,x 2 ,...,x n a prefix computation produces the results: I L r 1 = x 1 M O where F is any associative operator. Computing the results in parallel is termed a parallel prefix computation [15]. <p> similar to the problem of parallel prefix computation <ref> [15] </ref>. Given the elements x 1 ,x 2 ,...,x n a prefix computation produces the results: I L r 1 = x 1 M O where F is any associative operator. Computing the results in parallel is termed a parallel prefix computation [15]. To examine the similarity between request combining and parallel prefix consider an example in which four processors add a constant, C, to a shared variable X and receive the previous value of X. Assume the processors simultaneously execute the atomic operation Fetch&Add (X,C). <p> To eliminate this serialization for the distribution of results we turn to the literature on parallel applications to see how the prefix operation on the combining set can be carried out in parallel. Several algorithms for performing a parallel prefix computation on a linked list exist in the literature <ref> [10, 13, 15, 20] </ref>. Most of the algorithms are concerned with the case of having more nodes in the linked list than processors available. In our case the number of nodes in the linked list is equivalent to the number of processors that are participating in the combined access.
Reference: [16] <author> Lebeck, A. R., </author> <title> ``Request Combining in Multiprocessors with Arbitrary Interconnection Networks,'' M. S. </title> <type> Thesis, </type> <institution> Dept. of Computer Sciences, University of Wisconsin-Madison, Madison, WI, </institution> <year> 1991. </year>
Reference-contexts: It is important to note that the recursive doubling algorithm is defined for a SIMD machine, and therefore appropriate synchronization must be added for MIMD operation. Figure 9 shows the MIMD version for result distribution in IPC, the reader is referred to <ref> [16] </ref> for further discussion of the transformation from SIMD to MIMD. Processors also require a limited amount of memory (O (log S)) for storing the pointers to neighboring processors in the combining set.
Reference: [17] <author> Leiserson, C. E., </author> <title> ``The Network Architecture of the Connection Machine CM-5,'' </title> <booktitle> Proc. ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: However, the processor elements, and not the interconnection network, determine the combining set. Blelloch's prefix computation network [3], discussed in Section 2.1, and the control network of the Thinking Machines CM-5 <ref> [17] </ref> fall into this category. Another form of PIC in a SIMD paradigm is proposed by Lipovski and Vaughan [19]. This implementation uses a modified carry-lookahead circuit to implement a prefix computation network which distributes the results. The combining set is determined by which processing elements are currently active.
Reference: [18] <author> Lenoski, D., Laudon, J., Gharachorloo, K., Gupta, A., and Hennessy, J., </author> <title> ``The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor,'' </title> <booktitle> Proceedings 17th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: An interesting example of IIC read combining can be found in schemes with hierarchical cache/memory structures, such as Cache-Only Memory Architecture (COMA) machines [7, 27], or Non-Uniform Memory Access (NUMA) machines with hierarchical caches <ref> [18, 23, 30] </ref>. Here read combining can be implemented by using a technique similar to the CHoPP method of read combining. A read miss of a cache block at one level of the hierarchy causes a request to be propagated to the next higher level in the hierarchy.
Reference: [19] <author> Lipovski, G. J. and Vaughan, P., </author> <title> ``A Fetch-and-Op Implementation for Parallel Computers,'' </title> <booktitle> in Proc. 15th Annual Symposium on Computer Architecture, </booktitle> <address> Honolulu, HI, </address> <pages> pp. 384-392, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Almasi and Gottlieb [1] - 2 - give several examples of how such hardware combining can eliminate serial bottlenecks. Several alternative proposals for request combining have appeared in the literature <ref> [8, 11, 19, 21, 29, 32] </ref>. The primary focus of these efforts is on reducing the cost of the combining network. This is accomplished either by altering the topology of the combining network or by requiring the system software to reduce the amount of contention for shared data. <p> Blelloch's prefix computation network [3], discussed in Section 2.1, and the control network of the Thinking Machines CM-5 [17] fall into this category. Another form of PIC in a SIMD paradigm is proposed by Lipovski and Vaughan <ref> [19] </ref>. This implementation uses a modified carry-lookahead circuit to implement a prefix computation network which distributes the results. The combining set is determined by which processing elements are currently active.
Reference: [20] <author> Lubachevsky, B. D. and Greenberg, A. G., </author> <title> ``Simple, Efficient Asynchronous Parallel Prefix Algorithms,'' </title> <booktitle> Proc. 1987 International Conference on Parallel Processing, </booktitle> <pages> pp. 66-69, </pages> <month> August </month> <year> 1987. </year> <month> - 19 </month> - 
Reference-contexts: To eliminate this serialization for the distribution of results we turn to the literature on parallel applications to see how the prefix operation on the combining set can be carried out in parallel. Several algorithms for performing a parallel prefix computation on a linked list exist in the literature <ref> [10, 13, 15, 20] </ref>. Most of the algorithms are concerned with the case of having more nodes in the linked list than processors available. In our case the number of nodes in the linked list is equivalent to the number of processors that are participating in the combined access.
Reference: [21] <author> Pfister, G. F., Brantley, W. C., George, D. A., Harvey, S. L., Kleinfelder, W. J., McAuliffe, K. P., Melton, E. A., Norton, V. A., and Weiss, J., </author> <title> ``The IBM Research Parallel Processor Prototype (RP3): introduction and architecture,'' </title> <booktitle> Proceedings 1985 International Conference on Parallel Processing, </booktitle> <pages> pp. 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Almasi and Gottlieb [1] - 2 - give several examples of how such hardware combining can eliminate serial bottlenecks. Several alternative proposals for request combining have appeared in the literature <ref> [8, 11, 19, 21, 29, 32] </ref>. The primary focus of these efforts is on reducing the cost of the combining network. This is accomplished either by altering the topology of the combining network or by requiring the system software to reduce the amount of contention for shared data. <p> Interconnect-Interconnect Combining (IIC) The CHoPP [28] and the NYU Ultracomputer [6] methods of request combining are instances of IIC: the interconnection network determines the combining set and distributes the results. The IBM RP3 <ref> [21] </ref> researchers proposed the basic ideas of the Ultracomputer method of combining for their implementation. However, the RP3 has two interconnection networks, one network which combines requests and one that services non-combining requests.
Reference: [22] <author> Pfister, G. F. and Norton, V. A., </author> <title> ``'Hot-Spot' Contention and Combining in Multistage Interconnection Networks,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-34, </volume> <pages> pp. 943-948, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Processors generate a memory reference each cycle with probability r, providing the network can accept the request. Of these memory references, h percent are directed at a single hot memory location <ref> [22, 32] </ref>. Each processor may have only one outstanding F&A request, but unlimited outstanding uniform requests. However, a processor does not generate any requests when there is an outstanding F&A. Yew et. al. [32] call this the limited-variable access pattern. <p> We simulated a system with 256 processors connected to 256 memory modules. In all of our simulations we vary h from 0-32 percent and r from 20-100 percent. Figure 10 shows the average latency and the maximum bandwidth when no combining is performed. As previously shown in <ref> [22] </ref> and [32], there is a point of saturation after which bandwidth ceases to increase and latency increases. In our first experiment, we implemented IPC, without ALUs in the network nodes, and with locking the memory until the prefix operation on the current combining set is complete. <p> Overall, the proposed method for IPC is not as effective as the Ultracomputer style of combining (equivalent results for the Ultracomputer-style of combining can be found in the paper by Pfister and Norton <ref> [22] </ref> ), however our results suggest that it is an option worth considering. 5. Summary and Conclusions Unconstrained yet synchronized access to shared memory locations is achieved by combining requests.
Reference: [23] <author> Scott, S. L., </author> <title> ``Toward the Design of Large-Scale Shared-Memory Multiprocessors,'' </title> <type> Ph. D. Thesis, </type> <institution> Department of Computer Science (Technical Report #1100), University of Wisconsin-Madison, Madison, WI 53706, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: An interesting example of IIC read combining can be found in schemes with hierarchical cache/memory structures, such as Cache-Only Memory Architecture (COMA) machines [7, 27], or Non-Uniform Memory Access (NUMA) machines with hierarchical caches <ref> [18, 23, 30] </ref>. Here read combining can be implemented by using a technique similar to the CHoPP method of read combining. A read miss of a cache block at one level of the hierarchy causes a request to be propagated to the next higher level in the hierarchy.
Reference: [24] <author> Smith, A. J., </author> <title> ``Cache Memories,'' </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 14, </volume> <pages> pp. 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: 1. Introduction Arvind and Iannucci state that the design of a large-scale, shared memory multiprocessor must address two basic issues [2]: (1) it must tolerate long latencies for memory requests, (2) it must achieve unconstrained, yet synchronized, access to shared data. While several techniques, for example caches and prefetching <ref> [24] </ref>, and low level context switching [25], have been proposed to tolerate the latency of memory requests, heretofore the only known methods of allowing unconstrained, yet synchronized, access to shared data are implementations of request combining.
Reference: [25] <author> Smith, B., </author> <title> ``Architecture and Applications of the HEP Multiprocessor Computer System,'' </title> <booktitle> Proceedings of the Int. </booktitle> <publisher> Soc. for Opt. Engr, </publisher> <pages> pp. 241-248, </pages> <year> 1982. </year>
Reference-contexts: While several techniques, for example caches and prefetching [24], and low level context switching <ref> [25] </ref>, have been proposed to tolerate the latency of memory requests, heretofore the only known methods of allowing unconstrained, yet synchronized, access to shared data are implementations of request combining.
Reference: [26] <author> Sohi, G. S., Smith, J. E., and Goodman, J. R., </author> <title> ``Restricted Fetch&F Operations for Parallel Processing,'' </title> <booktitle> in Proc. 3rd International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <pages> pp. 410-416, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Interconnect-Processor Combining We now consider implementations of request combining that fall into the unexplored region of the design space, Interconnect-Processor Combining (IPC). We initially consider two flavors of combinable operations: a restricted form of Fetch&Add (F&A), or Fetch&Increment <ref> [9, 26] </ref>, and the general F&A operation. In Fetch&Increment, or simply F&I, all participants add the same, constant value. In the general F&A, each participant could be adding a different value. <p> The following example illustrates this point. processor is assigned one channel in the ``bus'' (the channel could be a wire in an electronic bus <ref> [26] </ref> or a specific frequency in an optical bus [9] ). A given processor can read all channels, but can write to only its channel. A processor generates a combinable request and broadcasts its intentions on the bus, by putting a "1" on its channel. <p> One processor (or the memory controller - 10 - who could also be monitoring the bus) takes responsibility for computing X+4C and updating memory. The method described above was proposed independently for an electrical bus by Sohi, Smith, and Goodman <ref> [26] </ref> and for an optical bus by Hiedelberger, Rathi, and Stone [9]. The ease with which combining can be carried out (in special cases), prompted Freudenthal and Gottlieb to investigate the use of the Fetch&Increment operation in place of the more general F&A operation [4].
Reference: [27] <author> Stenstrom, Per, Joe, Truman, and Gupta, Anoop, </author> <title> ``Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures,'' </title> <booktitle> in Proc. 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <pages> pp. 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: An interesting example of IIC read combining can be found in schemes with hierarchical cache/memory structures, such as Cache-Only Memory Architecture (COMA) machines <ref> [7, 27] </ref>, or Non-Uniform Memory Access (NUMA) machines with hierarchical caches [18, 23, 30]. Here read combining can be implemented by using a technique similar to the CHoPP method of read combining.
Reference: [28] <author> Sullivan, H. and Cohn, L., </author> <title> ``Shared Memory Computer Method and Apparatus,'' </title> <type> U.S. Patent 4,707,781, </type> <month> No-vember </month> <year> 1987. </year>
Reference-contexts: The earliest published proposal for request combining was in the CHoPP system <ref> [28] </ref>, where several read requests to a common memory location are combined in the interconnection network and are satisfied with only a single access of the memory location. <p> Finally, Interconnect-Processor Combining (IPC) indicates that the interconnection network determines the combining set, while the processors distribute the results. In the following subsections we discuss existing methods of request combining according to which region of the design space they belong to. 2.2.1. Interconnect-Interconnect Combining (IIC) The CHoPP <ref> [28] </ref> and the NYU Ultracomputer [6] methods of request combining are instances of IIC: the interconnection network determines the combining set and distributes the results. The IBM RP3 [21] researchers proposed the basic ideas of the Ultracomputer method of combining for their implementation.
Reference: [29] <author> Tzeng, N. F., </author> <title> ``Design of a Novel Combining Structure for Shared-Memory Multiprocessors,'' </title> <booktitle> Proc. 1989 International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <note> I-1 to I-8, </note> <month> August </month> <year> 1989. </year>
Reference-contexts: Almasi and Gottlieb [1] - 2 - give several examples of how such hardware combining can eliminate serial bottlenecks. Several alternative proposals for request combining have appeared in the literature <ref> [8, 11, 19, 21, 29, 32] </ref>. The primary focus of these efforts is on reducing the cost of the combining network. This is accomplished either by altering the topology of the combining network or by requiring the system software to reduce the amount of contention for shared data. <p> A distinction is made between non-combinable and potentially combinable requests (typically synchronization requests), and the interconnect dynamically determines the combining set of the potentially combinable requests. Two alternative techniques for IIC are presented by Tzeng <ref> [29] </ref> and Hsu and Yew [11]. Tzeng separates the interconnect into a routing section and a combining section. It is assumed that requests which may combine are distinguished from non-combining requests by examination of the opcode. Such requests are directed to the combining section of the network.
Reference: [30] <author> Wilson, A. W., </author> <title> ``Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors,'' </title> <booktitle> in Proc. 14th Annual Symposium on Computer Architecture, </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 244-252, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: An interesting example of IIC read combining can be found in schemes with hierarchical cache/memory structures, such as Cache-Only Memory Architecture (COMA) machines [7, 27], or Non-Uniform Memory Access (NUMA) machines with hierarchical caches <ref> [18, 23, 30] </ref>. Here read combining can be implemented by using a technique similar to the CHoPP method of read combining. A read miss of a cache block at one level of the hierarchy causes a request to be propagated to the next higher level in the hierarchy.
Reference: [31] <author> Yew, P.-C. and Tang, P., </author> <title> ``Software Combining Algorithms for Distributing Hot-Spot Addressing,'' </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> October </month> <year> 1990. </year>
Reference-contexts: The nodes of a combining tree are realized by the implicit storage in the interconnect nodes, i.e., wait buffers. Another alternative is to use explicit storage in memory to construct the combining tree. This method of request combining called software combining in the literature <ref> [5, 31, 32] </ref>, is classified as PPC since the processors bear full responsibility for the combining of requests: the processors establish the combining set and distribute the results and there are no demands of the network at all. <p> Tang and Yew also provide several algorithms for traversing a combining tree where the type of memory access determines which algorithm is chosen (e.g., barrier synchronization, semaphore, read combining) <ref> [31] </ref>. A consequence of implementing the combining tree with explicit memory locations is flexibility in the type of memory access. In addition to variable types of memory accesses, software combining permits the use of networks with arbitrary topologies and relatively unsophisticated nodes. <p> Furthermore, if the latency of the combined access is to be minimized, the combining tree must be balanced; this requires a priori knowledge of the number of requests that may combine <ref> [31] </ref>. Moreover, since the combining tree is created based on the maximum number of requests that may combine, the latency to complete the combining operation is influenced by this maximum number: if only one request is accessing the shared location, it must traverse the entire combining tree. <p> Table 1 summarizes the following discussion. - 8 - 2.3.1. Determining the Combining Set The processor elements require a priori knowledge of the combinable locations in order to establish the combining set. For example, the nodes of a software combining tree <ref> [31] </ref> are defined during algorithm design. In contrast the interconnect determines the combining set dynamically by comparing destination addresses of messages.

References-found: 31

