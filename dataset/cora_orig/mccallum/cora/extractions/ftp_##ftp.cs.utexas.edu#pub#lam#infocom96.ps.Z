URL: ftp://ftp.cs.utexas.edu/pub/lam/infocom96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/lam/NRL/video_services.html
Root-URL: 
Email: flam,xieg@cs.utexas.edu  
Title: Group Priority Scheduling  
Author: Simon S. Lam and Geoffrey G. Xie 
Address: Austin, Texas 78712-1188  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: For many applications, the end-to-end delay of an application-specific data unit is a more important performance measure than the end-to-end delays of individual packets within a network. From this observation, we propose the idea of group scheduling. Specifically, consecutive packet arrivals in a flow are partitioned into groups, and the same deadline (called group priority) is assigned to every packet in a group. In this paper, we first present an end-to-end delay guarantee theorem for a network of guaranteed-deadline (GD) servers. The theorem can be instantiated to obtain end-to-end delay bounds for a variety of source control mechanisms and GD servers. We then specialize the delay guarantee theorem to group scheduling for a subclass of GD servers. We work out a detailed example to demonstrate how to use group scheduling in a particular class of networks. The advantages of group scheduling are discussed and illustrated with empirical results from simulation experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alan Demers, Srinivasan Keshav, and Scott Shenker. </author> <title> Analysis and simulation of a fair queuing algorithm. </title> <booktitle> In Proceedings of ACM SIGCOMM '89, </booktitle> <pages> pages 3-12, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: At a GD server, each packet arrival from a guaranteed flow is given a deadline (also called priority), and the server ensures that the packet departs by the deadline. Many GD service disciplines have been proposed <ref> [1, 2, 3, 4, 9, 13] </ref>.
Reference: [2] <author> Domenico Ferrari and Dinesh Verma. </author> <title> A scheme for real-time channel establishment in wide-area networks. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <pages> pages 368-379, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: At a GD server, each packet arrival from a guaranteed flow is given a deadline (also called priority), and the server ensures that the packet departs by the deadline. Many GD service disciplines have been proposed <ref> [1, 2, 3, 4, 9, 13] </ref>. <p> shown in [5] that if the server allocates a minimum rate of f to every packet of flow f , such that v f (j) = s f (j)= f , then the following holds for all j, P k (j) V k (j) 0 (9) For a Delay-EDD server <ref> [2] </ref>, the P values of packets are computed as follows [12], for all j 1, P k (j) = maxfA f f f where P f f k is a local delay bound for every packet in flow f , and v f (j) = v f = s f = <p> If certain schedulablity conditions are met, then a Delay-EDD server provides the guaranteed deadline in (1) with fi k = 0 <ref> [2] </ref>.
Reference: [3] <author> Norival R. Figueira and Joseph Pasquale. Leave-in-time: </author> <title> A new service discipline for real-time communications in a packet-switching network. </title> <booktitle> In Proceedings of ACM SIG-COMM '95, </booktitle> <pages> pages 207-218, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: At a GD server, each packet arrival from a guaranteed flow is given a deadline (also called priority), and the server ensures that the packet departs by the deadline. Many GD service disciplines have been proposed <ref> [1, 2, 3, 4, 9, 13] </ref>. <p> By induction, it is easy to show that for all j 1 P k (j) V k (j) = d k v f (11) For a leave-in-time server <ref> [3] </ref>, the P values of packets are computed as follows 2 for j 1, P k (j) = maxfA f f f V k (j) = maxfA f f where V f f k (j) is the local delay bound of packet j, and v f (j) = s f (j)= <p> Under certain admission control conditions, a leave-in-time server provides the guaranteed deadline in (1) with 2 The packet arrival time, A f k (j), should be interpreted as the time when packet i becomes eligible in <ref> [3] </ref>. fi k = s max k =C k [3]. Subtracting (13) from (12), we have for all j: P f k (j) = d f For example, suppose every server in the path of flow f is one of the four GD servers described above. <p> Under certain admission control conditions, a leave-in-time server provides the guaranteed deadline in (1) with 2 The packet arrival time, A f k (j), should be interpreted as the time when packet i becomes eligible in <ref> [3] </ref>. fi k = s max k =C k [3]. Subtracting (13) from (12), we have for all j: P f k (j) = d f For example, suppose every server in the path of flow f is one of the four GD servers described above.
Reference: [4] <author> S. Jamaloddin Golestani. </author> <title> A self-clocked fair queueing scheme for high speed applications. </title> <booktitle> In Proceedings of IEEE INFOCOM '94, </booktitle> <pages> pages 636-646, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: At a GD server, each packet arrival from a guaranteed flow is given a deadline (also called priority), and the server ensures that the packet departs by the deadline. Many GD service disciplines have been proposed <ref> [1, 2, 3, 4, 9, 13] </ref>.
Reference: [5] <author> Pawan Goyal, Simon S. Lam, and Harrick M. Vin. </author> <title> Determining end-to-end delay bounds in heterogeneous networks. </title> <booktitle> In Proceedings of NOSSDAV, </booktitle> <address> Durham, New Hampshire, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: A widely used mechanism is leaky bucket con trol. If the source of flow f is controlled by a leaky bucket with token rate and bucket depth , then for all packet i in the flow <ref> [5] </ref>, V f 1 (i) + To obtain an end-to-end upper bound for flow f, V f is instantiated to A f 1 (i) + = in the delay guarantee formula of Theorem 1. <p> Under certain admission control conditions, the PGPS server provides the guaranteed deadline in (1) with fi k = s max k =C k [9]. It is shown in <ref> [5] </ref> that if the server allocates a minimum rate of f to every packet of flow f , such that v f (j) = s f (j)= f , then the following holds for all j, P k (j) V k (j) 0 (9) For a Delay-EDD server [2], the P
Reference: [6] <author> Simon S. Lam, Simon Chow, and David K.Y. Yau. </author> <title> An algorithm for lossless smoothing of MPEG video. </title> <booktitle> In Proceedings of ACM SIGCOMM '94, </booktitle> <pages> pages 281-293, </pages> <address> London, England, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: To illustrate the potential benefit of group scheduling, consider interframe-encoded pictures in a video flow, which have very large size fluctuations, e.g., for MPEG sequences studied in <ref> [6] </ref>, an I picture is up to 30 times the size of a B picture. From (28), g m can be as large as 30 for an I picture.
Reference: [7] <author> Simon S. Lam and Geoffrey G. Xie. </author> <title> Burst Scheduling: Architecture and algorithm for switching packet video. </title> <type> Technical Report TR-94-20, </type> <institution> University of Texas at Austin, Austin, Texas, </institution> <month> July </month> <year> 1994. </year> <note> An abbreviated version in Proceedings of IEEE INFOCOM '95, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: The delay guarantee theorem in Section 2 is then specialized to group scheduling for a subclass of GD servers. In Section 4, we work out a detailed example for a particular class of networks <ref> [7] </ref>. We derive end-to-end delay bounds for messages (called bursts in [7]), and illustrate how to choose group sizes such that the end-to-end delays of messages are unaffected by group scheduling. In Section 5, we present empirical results from simulation experiments for MPEG video flows. <p> The delay guarantee theorem in Section 2 is then specialized to group scheduling for a subclass of GD servers. In Section 4, we work out a detailed example for a particular class of networks <ref> [7] </ref>. We derive end-to-end delay bounds for messages (called bursts in [7]), and illustrate how to choose group sizes such that the end-to-end delays of messages are unaffected by group scheduling. In Section 5, we present empirical results from simulation experiments for MPEG video flows. <p> We note that adaptive throughput allocation on a per packet basis is unrealistic in practice. However, adaptive throughput allocation on a per burst basis has been proposed <ref> [7] </ref>, where each burst consists of a number of packets; see Section 4. Lemma 2. For packet i = 1; 2; : : : in flow f and node k = 1; 2; : : :; K 1 k+1 (i) V f 1ji k (j)V f Theorem 1. <p> Another example of source control is the separation timing constraint between consecutive bursts in a flow <ref> [7] </ref>; see Section 4 for more details. 2.4 Examples of GD servers The GD class of servers is general and includes many service disciplines in the literature. There are differences in their P f k () functions, fi k constants, scheduling algorithms, and admission control conditions. <p> where for 1 j i , v f g (j) = n2h (j) v f (n) and k =C k ) + t k;k+1 . 4 A Detailed Example We next illustrate how to apply group scheduling and the delay guarantee in Corollary 1 to a particular class of networks <ref> [7] </ref>, where a guaranteed flow is generated as a sequence of bursts. We will assume that all packets have a fixed size. End-to-end delay bounds are derived for the path of K +2 nodes in Section 2. <p> This is a rather weak constraint and can be easily satisfied if the packets of a burst are derived from the same application data unit at the source. The jitter timing constraint can be exploited to compute virtual clock values very efficiently for each flow at a server <ref> [7] </ref>; specifically, the main steps of the computation are performed only once per burst. The separation timing constraint is a sufficient condition for a VC priority server to allocate reserved rates to flows adaptively on a per burst basis and provide guaranteed deadlines [10]. <p> Corollary 2. D (m; 1) m 1nm g n g + k=1 Corollary 2 generalizes a theorem in <ref> [7] </ref> for the special case of individual priority (that is, g m = 1 for all m). <p> A tight end-to-end delay lower bound is the fol lowing: D (m; 1) (K 1) m K X ff k (22) Since flow regulators preserve the jitter timing constraint for each burst in a guaranteed flow <ref> [7] </ref>, the delay of packet (m; l) is bounded as follows: D (m; l) D (m; 1) + m The end-to-end delay of burst m, denoted by D m , measured from the time when packet (m; 1) arrives at node 1 to the time when packet (m; b m ) <p> This is because group scheduling offers more flexible deadlines for scheduling packets. Consequently, schedulers can better cope with temporary overloads. 5.1 Network configuration The experiments were conducted using a discrete-event simulator from <ref> [7] </ref>. The network simulated is illustrated in Figure 1. There are six switches labeled SW. Each switch has a buffer pool for 1200 packets, which is shared by all video flows. <p> For L2 and L3, 0.2 of the channel capacity C was allocated to ABR traffic by assigning virtual clock values to ABR packets as priorities <ref> [7] </ref>. Whenever there 5 See [8] for a more detailed description of the experiments and empirical results. was nothing to send from the video flow queues, the entire channel capacity was available to ABR traffic. We ran each experiment for 10 seconds of simulated time. <p> We proved a relaxed deadline theorem for the priority subclass of GD servers. The delay guarantee theorem is then specialized to group scheduling for a subclass of GD servers. We worked out a detailed example for a particular class of networks <ref> [7] </ref>. We derived end-to-end delay bounds for bursts (messages), and illustrated how to choose group sizes such that the end-to-end delays of bursts are unaffected by group scheduling. Group scheduling has two advantages.
Reference: [8] <author> Simon S. Lam and Geoffrey G. Xie. </author> <title> Group priority scheduling. </title> <type> Technical Report TR-95-28, </type> <institution> University of Texas at Austin, Austin, Texas, </institution> <month> July </month> <year> 1995; </year> <month> revised, January </month> <year> 1996. </year>
Reference-contexts: For L2 and L3, 0.2 of the channel capacity C was allocated to ABR traffic by assigning virtual clock values to ABR packets as priorities [7]. Whenever there 5 See <ref> [8] </ref> for a more detailed description of the experiments and empirical results. was nothing to send from the video flow queues, the entire channel capacity was available to ABR traffic. We ran each experiment for 10 seconds of simulated time. About 300 pictures were delivered for each video flow. <p> However, group priority with g min equal to 2 and 4 performed better than individual priority only when the network was heavily loaded. At 164% overbooking, all three cases of group priority had better performance than individual priority (see <ref> [8] </ref>).
Reference: [9] <author> Abhay K. Parekh and Robert G. Gallager. </author> <title> A generalized processor sharing approach to flow control in integrated services networks: The single node case. </title> <journal> IEEE/ACM Trans. on Networking, </journal> <pages> pages 344-357, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: At a GD server, each packet arrival from a guaranteed flow is given a deadline (also called priority), and the server ensures that the packet departs by the deadline. Many GD service disciplines have been proposed <ref> [1, 2, 3, 4, 9, 13] </ref>. <p> Under certain admission control conditions, the PGPS server provides the guaranteed deadline in (1) with fi k = s max k =C k <ref> [9] </ref>.
Reference: [10] <author> Geoffrey G. Xie and Simon S. Lam. </author> <title> Delay guarantee of Virtual Clock server. </title> <type> Technical Report TR-94-24, </type> <institution> University of Texas at Austin, Austin, Texas, </institution> <month> October </month> <year> 1994. </year> <note> To appear in IEEE/ACM Trans. Networking, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: Under certain admission control conditions, the VC server provides the guaranteed deadline in (1) with fi k = s max k =C k <ref> [10] </ref>. From (3) and (7), it is trivial to show that, for all j, P k (j) V k (j) = 0 (8) For a PGPS server, P f k (j) is the virtual-time finishing time of packet j. <p> More specifically, the virtual clock values of a flow f are computed assuming that packet i in the flow is allocated a thoughput of f (i) bits/second at each server on the path, and that some admission control mechanism ensures that the capacity of each server is not exceeded <ref> [10] </ref>. From Section 2.4, we see that the virtual clock value of packet i in flow f at server k is equal to its reference clock value V f k (i). <p> The separation timing constraint is a sufficient condition for a VC priority server to allocate reserved rates to flows adaptively on a per burst basis and provide guaranteed deadlines <ref> [10] </ref>. The constraint also ensures that each active flow contains at most one active burst|a server can make use of this information to check that its capacity has not been depleted by allocations to flows. <p> Note that Corollary 1 is applicable because the server at node k (excluding the flow regulator) is work conserving. 4.1 Delay bounds If the channel capacity, for every channel on the path, is not exceeded by the aggregate reserved rate of active flows <ref> [10] </ref>, a tight end-to-end delay upper bound for the first packet of every burst can be derived as a special case of Corollary 1. Corollary 2. <p> It is based upon the assumption that the capacity of every channel in the path has not been exceeded by the aggregate reserved rate allocated to active flows <ref> [10] </ref>. For many multimedia applications, however, statistical guarantees are acceptable, such as: 99% of the pictures in a video sequence are delivered with delays less than the upper bound.
Reference: [11] <author> Geoffrey G. Xie and Simon S. Lam. </author> <title> An efficient scheduler for real-time traffic. </title> <type> Technical Report TR-95-29, </type> <institution> University of Texas at Austin, Austin, Texas, </institution> <month> July </month> <year> 1995. </year> <note> Available from http://net.cs.utexas.edu/users/~lam/NRL/. </note>
Reference-contexts: Hence, the channel scheduler's work in updating its priority data structure (e.g., a heap) would be much reduced for large groups. (An empirical investigation quantifying this reduction can be found in <ref> [11] </ref>.) Second, group scheduling offers more flexible deadlines; consequently, channel schedulers can better cope with temporary overloads. The balance of this paper is organized as follows. In Section 2, we introduce the class of GD servers and prove an end-to-end delay guarantee theorem. <p> First, within a network node the channel scheduler's work is much reduced. This is because the scheduler needs to update its priority data structure whenever the priority of a flow changes. With group scheduling, a flow's priority changes only once per group rather than once per packet; see <ref> [11] </ref>. Second, we discovered that the flexibility of relaxed deadlines results in better statistical performance (i.e., delay, queue size, and loss probability) for networks where some channels are heavily utilized. <p> Hence, the channel scheduler's work in updating its priority data structure (e.g., a heap) would be much reduced for large groups. (An empirical investigation quantifying this reduction can be found in <ref> [11] </ref>.) Second, group scheduling offers more flexible deadlines; consequently, channel schedulers can better cope with temporary overloads.
Reference: [12] <author> Hui Zhang and Srinivasan Keshav. </author> <title> Comparison of rate-based service disciplines. </title> <booktitle> In Proceedings of ACM SIG-COMM '91, </booktitle> <pages> pages 113-121, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: rate of f to every packet of flow f , such that v f (j) = s f (j)= f , then the following holds for all j, P k (j) V k (j) 0 (9) For a Delay-EDD server [2], the P values of packets are computed as follows <ref> [12] </ref>, for all j 1, P k (j) = maxfA f f f where P f f k is a local delay bound for every packet in flow f , and v f (j) = v f = s f = f , with s f and f being the same
Reference: [13] <author> Lixia Zhang. VirtualClock: </author> <title> A new traffic control algorithm for packet switching networks. </title> <booktitle> In Proceedings of ACM SIG-COMM '90, </booktitle> <pages> pages 19-29, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: At a GD server, each packet arrival from a guaranteed flow is given a deadline (also called priority), and the server ensures that the packet departs by the deadline. Many GD service disciplines have been proposed <ref> [1, 2, 3, 4, 9, 13] </ref>. <p> There are differences in their P f k () functions, fi k constants, scheduling algorithms, and admission control conditions. We next discuss four well-known examples. For a VC server, the P values are virtual clock values computed as follows <ref> [13] </ref>, for all j 1, P k (j) = maxfP f f where P f k (0) = 0, and v f (j) is equal to s f (j)= f (j). <p> First, we need to specify the service discipline at each network node on the path. In the balance of this paper, we will consider servers in the priority subclass that use virtual clock values as priorities <ref> [13] </ref>.
References-found: 13

