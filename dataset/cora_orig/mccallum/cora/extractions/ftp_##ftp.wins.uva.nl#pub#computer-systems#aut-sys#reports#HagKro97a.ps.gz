URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/HagKro97a.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email:stephanh@fwi.uva.nl  
Title: Generalizing in TD() learning  
Author: S.H.G. ten Hagen B.J.A. Krose 
Address: Kruislaan 403,1098 SJ Amsterdam (NL)  
Affiliation: Faculty of Mathematics, Computer Science, Physics and Astronomy University of Amsterdam  
Note: Proc. of Third Joint Conf. of Information Sciences, volume 2, pp 319-322, 1997.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Peter Dayan. </author> <title> The Convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: old + k=0 k X ki r w ^ J i : (2) At time step k = n, the old weights w old become w new . ^ J i is the output of the approxima-tor at time step i, ff &gt; 0 is the learning rate and 2 <ref> [0; 1] </ref> is the discount factor for previous gradients. <p> In fact the function approxi-mator calculates the output by multiplying the state vector with a weight vector ( ^ J = X T w if X represents the present state). Under the same circumstances the convergence proofs for batch TD () <ref> [1] </ref> and on-line TD () [2] are valid. 3 Requirements for control applications The batch TD () learning rule is not well suited for the use in real world control applications. For this there are three reasons: 1. There are many runs necessary to train the function approximation.
Reference: [2] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1185-1201, </pages> <year> 1994. </year>
Reference-contexts: In fact the function approxi-mator calculates the output by multiplying the state vector with a weight vector ( ^ J = X T w if X represents the present state). Under the same circumstances the convergence proofs for batch TD () [1] and on-line TD () <ref> [2] </ref> are valid. 3 Requirements for control applications The batch TD () learning rule is not well suited for the use in real world control applications. For this there are three reasons: 1. There are many runs necessary to train the function approximation. <p> Also the convergence proof of the look-up-table entries, indicates the assumption that eventually all states are visited often enough. This leads to an extra constrain for the distribution of the func tion approximators inputs. * The learning rate should be decreasing in time <ref> [2] </ref>. Implying that eventually the learning has to stop. This means that it can not cope with a changing environment. From the problems mentioned above, the first two can be solved when using a function approx-imator that is capable of generalizing.
Reference: [3] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year> <month> 4 </month>
Reference-contexts: The convergence of the TD () method using the RBF will be investigated, without making any assumptions about the environment. fl This research is supported by the Technology Foundation (STW). 2 TD () learning The TD () learning method, introduced in <ref> [3] </ref>, is an algorithm to improve the estimation of the future cost. The future cost is given by the discounted sum of future evaluations ": J k = i=k Where J k represents the future cost at time step k, and fl 2]0; 1 [ is the discount factor. <p> The k represents the temporal difference (TD) between two successive estimations of the future cost and is defined by: k = " k+1 + fl ^ J k+1 ^ J k : (3) In <ref> [3] </ref> also an on-line version of the TD learn ing method was introduced: w k+1 = w k + ff k i=0 with, k = " k+1 + fl ^ J (s k+1 ; w k ) ^ J (s k ; w k ): (5) The idea behind this is <p> In (3) this assumption is not necessary, because (2) already makes k independent of the weight change. The convergence of the update rule is essential when the results are used to improve the control strategy. The first convergence proof was given in <ref> [3] </ref> for batch TD (0). This convergence proof is valid if the environment can be described by an absorbing Markov model, where all states are represented by linear independent vectors and where the function approximator is a look-up-table.
References-found: 3

