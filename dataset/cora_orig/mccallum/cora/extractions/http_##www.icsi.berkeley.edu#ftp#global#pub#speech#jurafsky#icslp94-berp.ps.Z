URL: http://www.icsi.berkeley.edu/ftp/global/pub/speech/jurafsky/icslp94-berp.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/speech/jurafsky/
Root-URL: http://www.icsi.berkeley.edu
Title: In ICSLP-94, 2139-2142 THE BERKELEY RESTAURANT PROJECT  
Author: Daniel Jurafsky, Chuck Wooters Gary Tajchman, Jonathan Segal, Andreas Stolcke, Eric Fosler, and Nelson Morgan 
Address: 1947 Center Street, Suite 600 Berkeley, CA 94704, USA  Berkeley  
Affiliation: International Computer Science Institute  University of California at  
Abstract: This paper describes the architecture and performance of the Berkeley Restaurant Project (BeRP), a medium-vocabulary, speaker-independent, spontaneous continuous speech understanding system currently under development at ICSI. BeRP serves as a testbed for a number of our speech-related research projects, including robust feature extraction, connectionist phonetic likelihood estimation, automatic induction of multiple-pronunciation lexicons, foreign accent detection and modeling, advanced language models, and lip-reading. In addition, it has proved quite usable in its function as a database frontend, even though many of our subjects are non-native speakers of English. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Boite, H. Bourlard, B. D'hoore, and M. Haesen. </author> <title> A new approach towards keyword spotting. </title> <booktitle> In Proceedings of Eurospeech 93, </booktitle> <year> 1993. </year>
Reference-contexts: The idea is to add a new word model, the garbage word, which is designed to match unknown words and word fragments. However, instead of generating garbage likelihoods through some trained model, we use the method of <ref> [1] </ref> to generate virtual likelihoods. At each frame, the phone probabilities for the garbage word are computed by averaging the top N phone probabilities from the MLP. Thus at each frame, the local distance of the garbage word will be worse than any word which matches the best few phones.
Reference: [2] <author> H. Bourlard and N. Morgan. </author> <title> Merging multilayer perceptrons & Hidden Markov Models: Some experiments in continuous speech recognition. </title> <editor> In E. Gelenbe, editor, </editor> <booktitle> Artificial Neural Networks: Advances and Applications. </booktitle> <publisher> North Holland Press, </publisher> <year> 1991. </year>
Reference-contexts: restaurants Bigram Perplexity 10.7 with 77% coverage Grammar 1389 handwritten SCFG rules Implementation 18,000 lines of C++ Performance Recognition 32.1% error Parsing 63% training 61% test Understanding 34% error 3 PROBABILITY ESTIMATION The BeRP system uses a discriminatively-trained Multi-Layer Perceptron (MLP) in an iterative Viterbi procedure to estimate emission probabilities. <ref> [2] </ref> and [10] show that with a few assumptions, an MLP may be viewed as estimating the probability P (qjx) where q is a subword model (or a state of a subword model) and x is the input acoustic speech data.
Reference: [3] <author> Chris Bregler, Stephen Omohundro, Yochai Konig, and Nelson Morgan. </author> <title> Using surface-learning to improve speech recognition with lipreading". </title> <booktitle> In Proc. 28th Annual Asilomar Conf. on Signals, Systems, and Computers, </booktitle> <address> Pacific Grove, CA, </address> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Semantic Error Rate BeRP system 34.1 Backend alone 18.1 Recognizer alone 27.7 Table 5: BeRP semantic performance We are currently extending the BeRP system to use visual data from the speakers' lips, either to aid in recognition, or to help determine when a speaker has started speaking <ref> [3] </ref>. Further details of the BeRP system are presented in [13] and [8]. Acknowledgments Thanks to Emily Bender, Herve Bourlard, Jerry Feldman, Hana Filip, Hynek Hermansky, Ron Kay, Yochai Konig, Robert Moore, Steve Omo-hundro, Patti Price, and Liz Shriberg.
Reference: [4] <author> M. Cohen, H. Franco, N. Morgan, D. Rumelhart, and V. Abrash. </author> <title> Hybrid neural network/Hidden Markov Model continuous speech recognition. </title> <booktitle> In ICSLP-92, 915918, </booktitle> <address> Banff, Canada, </address> <year> 1992. </year>
Reference-contexts: needed by the Viterbi algorithm by dividing by the prior P (q), according to Bayes' rule; we ignore P (x) since it is constant in time-synchronous Viterbi: P (x j q) = P (q) Results with our architecture on the speaker independent DARPA Resource Management database for MLP monophone estimators <ref> [4] </ref> yield competitive performance (4-6% word error with the standard perplexity 60 wordpair grammar). The estimator consists of a simple three-layer feed forward MLP trained with the back-propagation algorithm (see Figure 3). 1 The input layer consists of 9 frames of input speech data.
Reference: [5] <author> M. H. Cohen. </author> <title> Phonological Structures for Speech Recognition. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1989. </year>
Reference-contexts: Next, this initial model is simplified and generalized by repeatedly merging states until we reach a model with (locally) maximum posterior probability. See [14] for more details. Our second method augments this bottom-up approach with top-down information. Following <ref> [5] </ref>, we manually develop phonological rules and automatically apply them to a set of base-form or dictionary pronunciations, to automatically generate multiple pronunciations for any word in the dictionary.
Reference: [6] <author> N. M. Frazer and G. N. Gilbert. </author> <title> Simulating speech systems. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 81-99, </pages> <year> 1991. </year>
Reference-contexts: If two non-overlapping constituents both cover the same number of words, the algorithm chooses the most informative one, (greatest number of semantic predicates) or on further ambiguity the one with the higher probability. 8 DATA COLLECTION The BeRP system was bootstrapped with a Wizard of Oz 3 system <ref> [6] </ref>, allowing us to collect naturalistic data before the first version of the system was built. The test subjects interacted with a windowing system monitored by the hidden operator (wizard) in another room.
Reference: [7] <author> H. Hermansky, N. Morgan, A. Bayya, and P. Kohn. </author> <title> RASTA-PLP speech analysis technique. </title> <booktitle> In IEEE ICASSP-92, </booktitle> <address> I.121-124, San Francisco, CA, </address> <year> 1992. </year>
Reference-contexts: The whole system runs on a SPARCstation, although for speed we usually offload the phonetic likelihood estimation (the MLP forward pass) to special purpose hardware. Figure 1 gives an overview of the architecture. the BeRP system as implemented. 2 FEATURE EXTRACTION BeRP uses RASTA-PLP <ref> [7] </ref> to extract features from digitized acoustic data. RASTA-PLP is a speech analysis technique that is robust to steady-state spectral factors in speech such as those imposed by different communication channels (i.e., different microphones).
Reference: [8] <author> Daniel Jurafsky, Chuck Wooters, Gary Tajchman, Jonathan Se-gal, Andreas Stolcke, and Nelson Morgan. </author> <title> Integrating advanced models of syntax, phonology, and accent/dialect with a speech recognizer. </title> <booktitle> In AAAI Workshop on Integrating Speech and Natural Language Processing, </booktitle> <address> Seattle, </address> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: Use the characteristic bigram of the SCFG, which can be generated in closed form [12]. 3. Use the SCFG directly as the LM for the recognizer, by using the probabilistic parser to compute word transition probabilities directly from the SCFG <ref> [8] </ref>. 4. Mix the SCFG and smoothed bigram probabilities directly to provide word transition probabilities on each frame. Table 2 presents our word error results, showing the results of applying these four methods. Note that the SCFG gave a significant 4.1% improvement in word error over the bigram. <p> Further details of the BeRP system are presented in [13] and <ref> [8] </ref>. Acknowledgments Thanks to Emily Bender, Herve Bourlard, Jerry Feldman, Hana Filip, Hynek Hermansky, Ron Kay, Yochai Konig, Robert Moore, Steve Omo-hundro, Patti Price, and Liz Shriberg. This work was partially funded by ICSI, an SRI subcontract from ARPA contract MDA904-90-C-5253, and ESPRIT project 6487 (The Wernicke project).
Reference: [9] <author> N. Morgan. </author> <title> The ring array processor (RAP): A multiprocessing peripheral for connectionist applications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14:248259, </volume> <year> 1992. </year>
Reference-contexts: In order to perform the phonetic likelihood estimation quickly, we usually offload the computation to special-purpose hardware, the Ring Array Processor (RAP) <ref> [9] </ref>, although we have recently completed a vector-quantized single-layer perceptron version which runs in real-time on the SPARC.
Reference: [10] <author> S. Renals, N. Morgan, H. Bourlard, M. Cohen, H. Franco, C. Woot-ers, and P. Kohn. </author> <title> Connectionist speech recognition: Status and prospects. </title> <booktitle> TR-91-070, ICSI, </booktitle> <address> Berkeley, CA, </address> <year> 1991. </year>
Reference-contexts: Perplexity 10.7 with 77% coverage Grammar 1389 handwritten SCFG rules Implementation 18,000 lines of C++ Performance Recognition 32.1% error Parsing 63% training 61% test Understanding 34% error 3 PROBABILITY ESTIMATION The BeRP system uses a discriminatively-trained Multi-Layer Perceptron (MLP) in an iterative Viterbi procedure to estimate emission probabilities. [2] and <ref> [10] </ref> show that with a few assumptions, an MLP may be viewed as estimating the probability P (qjx) where q is a subword model (or a state of a subword model) and x is the input acoustic speech data.
Reference: [11] <author> Andreas Stolcke and Stephen Omohundro. </author> <title> Best-first model merging for hidden Markov model induction. </title> <booktitle> TR-94-003, ICSI, </booktitle> <address> Berke-ley, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: We generally use a value of N=10 for both local distance and word transition computation. 5 LEXICON The BeRP system uses a multiple-pronunciation HMM lexicon, which we build by combining two lexicon-production methods. In the first method we use the model merging algorithm <ref> [11] </ref> to induce a word model directly from a set of pronunciations for the word culled from TIMIT, various dictionaries, and other sources.
Reference: [12] <author> Andreas Stolcke and Jonathan Segal. </author> <title> Precise n-gram probabilities from stochastic context-free grammars. </title> <booktitle> In Proceedings of the 32nd ACL, </booktitle> <address> Las Cruces, NM, </address> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Smooth the bigram grammar by augmenting the corpus with a pseudo-corpus of sentences generated from the SCFG (extending the method of [15] for training word-pair grammars). 2 2. Use the characteristic bigram of the SCFG, which can be generated in closed form <ref> [12] </ref>. 3. Use the SCFG directly as the LM for the recognizer, by using the probabilistic parser to compute word transition probabilities directly from the SCFG [8]. 4. Mix the SCFG and smoothed bigram probabilities directly to provide word transition probabilities on each frame.
Reference: [13] <author> Charles C. Wooters. </author> <title> Lexical Modeling in a Speaker Independent Speech Understanding System. </title> <type> PhD thesis, </type> <institution> University of Califor-nia, Berkeley, </institution> <address> CA, </address> <year> 1993. </year> <note> available as ICSI TR-92-062. </note>
Reference-contexts: Further details of the BeRP system are presented in <ref> [13] </ref> and [8]. Acknowledgments Thanks to Emily Bender, Herve Bourlard, Jerry Feldman, Hana Filip, Hynek Hermansky, Ron Kay, Yochai Konig, Robert Moore, Steve Omo-hundro, Patti Price, and Liz Shriberg. This work was partially funded by ICSI, an SRI subcontract from ARPA contract MDA904-90-C-5253, and ESPRIT project 6487 (The Wernicke project).
Reference: [14] <author> Chuck Wooters and Andreas Stolcke. </author> <title> Multiple-pronunciation lexical modeling in a speaker-independent speech understanding system. </title> <booktitle> In ICSLP-94, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Next, this initial model is simplified and generalized by repeatedly merging states until we reach a model with (locally) maximum posterior probability. See <ref> [14] </ref> for more details. Our second method augments this bottom-up approach with top-down information. Following [5], we manually develop phonological rules and automatically apply them to a set of base-form or dictionary pronunciations, to automatically generate multiple pronunciations for any word in the dictionary.
Reference: [15] <author> Victor Zue, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneff. </author> <title> Integration of speech recognition and natural language processing in the MIT VOYAGER system. </title> <booktitle> In IEEE ICASSP-91, </booktitle> <address> I.713-716, </address> <year> 1991. </year> <month> 4 </month>
Reference-contexts: As a knowledge consultant, it draws inspiration from earlier consultants like VOYAGER <ref> [15] </ref>. Users ask spoken language questions of BeRP, which directs questions to the user and then queries a database of restaurants and gives advice to the user, based on such use criteria as cost, type of food, and location. <p> We have experimented with a number of ways to use the SCFG information: 1. Smooth the bigram grammar by augmenting the corpus with a pseudo-corpus of sentences generated from the SCFG (extending the method of <ref> [15] </ref> for training word-pair grammars). 2 2. Use the characteristic bigram of the SCFG, which can be generated in closed form [12]. 3. Use the SCFG directly as the LM for the recognizer, by using the probabilistic parser to compute word transition probabilities directly from the SCFG [8]. 4.
References-found: 15

