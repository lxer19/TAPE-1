URL: ftp://ftp.cs.umd.edu/pub/sel/papers/isern03.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/SoftEng/tame/papers/isern.95.03.html
Root-URL: 
Email: email: lbriand@crim.ca  email: kelemam@crim.ca  email: morasca@elet.polimi.it  
Title: 1 Theoretical and Empirical Validation of Software Product Measures  
Author: Lionel Briand Khaled El Emam Sandro Morasca 
Note: validation: theoretical and empirical. The former addresses the question is the measure measuring the attribute it is purporting to measure?, and the latter addresses the question is the measure useful in the sense that it is related to other variables in expected ways?  
Web: http://www.elet.polimi.it/~morasca  
Address: Montral, PQ, H3A 2H4 Canada  Montral, PQ, H3A 2H4 Canada  Piazza L. Da Vinci 32, I-20133, Milano Italy  
Affiliation: Centre de Recherche Informatique de Montral (CRIM) Software Engineering Group 1801 McGill College av.  Centre de Recherche Informatique de Montral (CRIM) Software Engineering Group 1801 McGill College av.  Dipartimento di Elettronica e Informazione Politecnico di Milano  
Abstract: In this paper we present and discuss a concrete method for validating measures of software product internal attributes measurement theory, properties of measures, and the Goal/Question/Metric paradigm (GQM). We identify two types of and provide guidelines for its application. This method integrates much of the relevant previous work, such as
Abstract-found: 1
Intro-found: 1
Reference: [AW91] <author> L. Aiken and S. West: </author> <title> Multiple Regression: Testing and Interpreting Interactions , Sage Publications, </title> <year> 1991. </year>
Reference: [AY79] <author> M. Allen and W. Yen: </author> <title> Introduction to Measurement Theory , Brooks/Cole Publishing Company, </title> <year> 1979. </year>
Reference-contexts: Furthermore, it should be noted that there are alternatives to direct estimation, for example, Thurnstones method of comparative judgements can be used to construct interval scales of say complexity by asking programmers to make order judgements about pairs of programs <ref> [AY79] </ref>. This kind of approach, however, has also not been used in software engineering. 11 Fenton and Kitchenham [FK90] criticise the commonly used empirical validation procedure whereby the analyst correlates the internal product attribute measure with any interesting measure which happened to be available as data.
Reference: [AKD+81] <author> F. Andrews, L. Klem, T. Davidson, P. OMalley, and W. Rodgers: </author> <title> A Guide for Selecting Statistical Techniques for Analyzing Social Science Data , Institute for Social Research, </title> <institution> University of Michigan, </institution> <year> 1981. </year>
Reference-contexts: For instance, they were originally presented by the psychologist Stevens [Ste46], serve as the basis of the classic text of Siegel on nonparametric statistics [SC88], and serve as an integral part of the decision tree developed by Andrews et al. <ref> [AKD+81] </ref> to guide researchers in the selection of the most appropriate statistics.
Reference: [BB81] <author> J. Bailey and V. Basili: </author> <title> A Meta-Model for Software Development Resource Expenditures. </title> <booktitle> In Proceedings of the International Conference on Software Engineering , pages 107-116, </booktitle> <year> 1981. </year>
Reference-contexts: For example, a very common transformation used in software engineering is the logarithmic transformation. This is applied frequently in the construction of effort estimation models using linear regression. As has been noted by Bailey and Basili <ref> [BB81] </ref> and Basili [Bas80], a general form of such models is: E = a L b where: E = effort L = some measure of size (usually LOC) a, b = constants In this particular case, an analyst could use the following estimating equation and thus use estimation procedures for linear
Reference: [Bas80] <author> V. Basili: </author> <title> Resource Models. In Tutorial on Models and Metrics for Software Management and Engineering , IEEE Computer Society Press, </title> <editor> V. Basili (ed.), </editor> <year> 1980. </year>
Reference-contexts: For example, a very common transformation used in software engineering is the logarithmic transformation. This is applied frequently in the construction of effort estimation models using linear regression. As has been noted by Bailey and Basili [BB81] and Basili <ref> [Bas80] </ref>, a general form of such models is: E = a L b where: E = effort L = some measure of size (usually LOC) a, b = constants In this particular case, an analyst could use the following estimating equation and thus use estimation procedures for linear regression models: ln
Reference: [B92] <author> V. Basili: </author> <title> "Software Modeling and Measurement: The Goal/Question/Metric Paradigm." </title> <type> Technical Report , CS-TR-2956, </type> <institution> University of Maryland, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: The four factors mentioned above are four of the dimensions of GQM goals <ref> [B92, BR88] </ref>.
Reference: [BBT94] <author> V. Basili, L. Briand, W. Thomas: </author> <title> "Experience Domain Analysis for Software Reuse", </title> <booktitle> In Proceedings of the 19th NASA-GSFC SEL Software Engineering Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: Unfortunately, there is no easy way to determine what piece of data is relevant or not in the context of a study. For further discussion, refer to <ref> [BBT94] </ref>.
Reference: [BR88] <author> V. Basili and D. Rombach: </author> <title> The TAME Project: Towards Improvement Oriented Software Environments. </title> <journal> In IEEE Transactions on Software Engineering , 14(6) </journal> <pages> 758-773, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In this method, we also integrate many of the relevant existing concepts for validation (e.g., measurement theory [F91][Z91], properties of measures [BMB96], and GQM <ref> [BR88] </ref>). To briefly summarize our method, below are its four main steps: 2 Theories may be more or less straightforward and may look more or less obvious. Some theories may no longer require to be empirically demonstrated. <p> The four factors mentioned above are four of the dimensions of GQM goals <ref> [B92, BR88] </ref>.
Reference: [BBT92] <author> L. Briand, V. Basili, W. Thomas: </author> <title> "A Pattern Recognition Approach for Software Engineering Data Analysis", </title> <journal> In IEEE Transactions on Software Engineering , 18(11) </journal> <month> 931-942 , </month> <year> 1992. </year>
Reference-contexts: Machine learning techniques, such as the ones mentioned above, should be seriously considered when performing multivariate analysis, especially in an exploratory context. For further discussion, refer to <ref> [BBT92] </ref>. 3.4 Interpreting a Lack of Relationship During empirical validation, if there is no empirical evidence supporting the expected relationship between the internal attribute and the external attribute (i.e., the relationship is not statistically significant), then the analyst may be faced with an interpretation difficulty.
Reference: [BBT93] <author> L. Briand, V. Basili and C. Hetmanski: </author> <title> "Developing Interpretable Models with Optimized Set Reduction for Identifying High Risk Software Components," </title> <journal> IEEE Trans. Software Eng. </journal> , <volume> 19 (11), </volume> <month> November, </month> <year> 1993. </year>
Reference-contexts: In addition, interaction terms may be significant and will make the model even more difficult to interpret. This is in part to construct models that are easier to interpret and use that techniques such as classification trees [SP88] and Optimized Set Reduction <ref> [BBT93; J95] </ref> have been developed. Therefore, it is important to note that regression, although the most often used approach to multivariate software engineering model construction, is not the only alternative.
Reference: [BEM96] <author> L. Briand, K. El Emam, and S. Morasca: </author> <title> On the Application of Measurement Theory to Software Engineering. </title> <note> To appear in Empirical Software Engineering: An International Journal , 1(1), </note> <year> 1996. </year>
Reference-contexts: Each unit of an attribute contributing to a valid measure is equivalent. Different entities can have the same attribute value. Consistent with the precepts of measurement theory <ref> [BEM96, F91, R79] </ref>, a valid measure should obey the representation condition (i.e., preserve the intuitive expected behavior of the measured attibute). Even though we acknowledge the importance of all four requirements mentioned above, only the representation condition has been the focus of our discussion in this section. <p> Despite a few available techniques to help the researchers in particular situations (see <ref> [BEM96] </ref>), the answer to those questions is hardly ever straightforward. Therefore, there are many cases where researchers cannot demonstrate that their scales are interval, but they are confident that they are more than only ordinal. By treating them as ordinal, researchers would be discarding a good deal of information. <p> For example, in a given environment, very often programmers know the common causes of errors and their relative impact. Scales may thus be validated with the help of experts. Such an approach is supported by numerous studies (for a more detailed discussion, see <ref> [BEM96] </ref>) which shows that, in general, parametric statistics are robust when scales are not too far from being interval.
Reference: [BMB94] <author> L. Briand, S. Morasca, and V. Basili: </author> <title> Defining and Validating High-Level Design Metrics. </title> <type> Technical Report , CS-TR-3301, </type> <institution> University of Maryland, </institution> <month> November </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: This relationship has been empirically demonstrated so many times that nobody thinks of conducting further experiments on this. However, even though implicitly, this theory has been formulated and validated. 3 As a matter of fact, in <ref> [BMB94] </ref> we have provided such a study of Parnas' design principles. The result of the study basically confirms Parnas' ideas. <p> As a matter of fact, replications are infrequent even where the analyst would not have to collect new data, as in using a holdout sample (however, see <ref> [BMB94] </ref> for an example where two internal replications of an empirical validation of high-level design metrics were reported). One pragmatic reason is the lack of funding for the collection of new data.
Reference: [BMB96] <author> L. Briand, S. Morasca, and V. Basili: </author> <title> Property Based Software Engineering Measurement. </title> <note> In IEEE Transactions on Software Engineering , 22(1) , January 1996. - 22 </note> - 
Reference-contexts: In this method, we also integrate many of the relevant existing concepts for validation (e.g., measurement theory [F91][Z91], properties of measures <ref> [BMB96] </ref>, and GQM [BR88]). To briefly summarize our method, below are its four main steps: 2 Theories may be more or less straightforward and may look more or less obvious. Some theories may no longer require to be empirically demonstrated. <p> However, confusion in terminology and lack of consensus in the software engineering community make it more difficult to formalize empirical relational systems. "Complexity" is a good example of confusion in software engineering <ref> [BMB96] </ref>. We therefore believe it is important that software measurement researchers agree on a basic set of properties that the empirical relational systems of common internal attributes (i.e., complexity, coupling, etc.) should have. <p> The relations of the empirical relational system are translated into relations of the formal relational system. This can be demonstrated by taking size as an example. First, it is necessary to introduce some notation, taken from <ref> [BMB96] </ref>. Let a system S be represented by a pair &lt;E,R&gt;, where E represents the set of elements of S, and R is a binary relation on E (R E x E) representing the relationships between Ss elements. <p> Given a system S=&lt;E,R&gt;, a system m=&lt;E m m &gt; is said to be a module of S iff E m m R. Typical kinds of relations of the formal relational system are (following the examples given in Section 2.1) <ref> [BMB96] </ref>: 1. Equalities. Equivalence relations between entities may be translated into equalities between measures of those entities attributes. <p> Assignment of special values. For instance, the measure of the size of the empty program body may be set to the value zero Thus: (E = ) fi Size (S) = 0 According to the above properties only (which are not complete, please see <ref> [BMB96] </ref> for the remaining properties), several measures introduced in the literature can be classified as size measures. For code measures, for example, these include LOC, #Modules, #Procedures, #Occurrences of Operators, #Unique Operators. <p> This can be done by using a set of properties which characterize all measures of a specified attribute. Instances of such properties can be found in the literature <ref> [W88, L91,TZ92, BMB96] </ref>. Again, such properties are defined based on one's intuition. <p> It is, however, important to note that when talking about complexity, size, coupling, or any internal software product attribute, most of the interesting properties, such as the additivity property for Size attributes (see previous section and <ref> [BMB96] </ref>), imply the existence of at least an interval scale. <p> We proposed such a set of basic and fundamental properties in <ref> [BMB96] </ref>. 3. Empirical Validation Once a measure can be considered valid from a theoretical point of view, the assumptions upon which the empirical relational system is based and the measure itself must be validated empirically. In this section, we will first provide the basic framework for empirical validation (Section 3.1).
Reference: [B84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone: </author> <title> Classification and Regression Trees , Wadsworth, </title> <year> 1984. </year>
Reference-contexts: However, the extent to which this can be achieved in practice remains an empirical question. - 17 - cross validation <ref> [B84] </ref>. If the sample is of size N, then this approach stipulates that the analysis be performed repeatedly for each n, n=1, .., N, with n removed from the sample and using the remaining N-1 observations.
Reference: [BD+94] <author> A. Brooks, J. Daly, J. Miller, M. Roper, and M. Wood: </author> <title> Replications Role in Experimental Computer Science. </title> <type> Technical Report EFoCS-5-94, </type> <institution> Empirical Foundations of Computer Science, Univesity of Strathclyde, </institution> <year> 1994. </year>
Reference-contexts: As members of a scientific discipline, it is responsible of software engineers to replicate empirical validation studies. This will increase our confidence in their application and also help us understand their limitations. For conducting replication validations, there are internal and external replications <ref> [BD+94] </ref>. The latter is conducted by the original analyst, and the former by an external agency. While ideally we would wish that internal replication be practiced more often, in the validation of measures it has not been so frequent.
Reference: [Coh65] <author> J. Cohen: </author> <title> Some Statistical Issues in Psychological Research. In Handbook of Clinical Psychology , B. </title> <editor> Woleman (ed.), </editor> <publisher> McGraw-Hill, </publisher> <year> 1965. </year>
Reference: [Coh88] <author> J. Cohen: </author> <title> Statistical Power Analysis for the Behavioral Sciences , Lawrence Erlbaum Associates, </title> <year> 1988. </year>
Reference-contexts: This is called the asymptotic relative efficiency (ARE) [Gib71]. 6 The values in this table are based on the tables provided in [KT87] and <ref> [Coh88] </ref>. The calculations of sample sizes assume that the assumptions of the tests are met. Where there are analogous tables in [Coh88], the sample size values are only slightly different from [KT87] (approximately -2 difference). - 15 - Power = 90% Power = 80% Corr. <p> This is called the asymptotic relative efficiency (ARE) [Gib71]. 6 The values in this table are based on the tables provided in [KT87] and <ref> [Coh88] </ref>. The calculations of sample sizes assume that the assumptions of the tests are met. Where there are analogous tables in [Coh88], the sample size values are only slightly different from [KT87] (approximately -2 difference). - 15 - Power = 90% Power = 80% Corr.
Reference: [CC83] <author> J. Cohen and P. Cohen: </author> <title> Applied Multiple Regression / Correlation Analysis for the Behavioral Sciences , Lawrence Erlbaum Associates, </title> <year> 1983. </year>
Reference-contexts: All this loss for essentially no gain. Similarly, in the context of multiple regression, Cohen and Cohen <ref> [CC83] </ref> state: The issue of the level of scaling and measurement precision required of quantitative variables in [Multiple Regression/Correlation] is complex and controversial. We take the position that, in practice, almost anything goes.
Reference: [CG93] <author> R. Courtney and D. </author> <title> Gustafson: </title> <journal> Shotgun Correlations in Software Measures , Software Engineering Journal , 8(1) </journal> <pages> 5-13, </pages> <year> 1993. </year>
Reference-contexts: What we propose is basing empirical validation on existing software engineering theories and not on whatever data happens to be available. The exploratory study of a large set of attributes, and, consequently, the definition of a large number of metrics, may have dangerous consequences <ref> [CG93] </ref>. For instance, one may define a measure which turns out to be statistically well correlated to the external attribute of interest only by chance. The result obtained is of difficult and uncertain interpretation, and may lead to incorrect decisions. - 19 - 4.
Reference: [DG84] <author> W. Dillon and M. Goldstein: </author> <title> "Multivariate Analysis: Methods and Applications." </title> <publisher> Wiley & Sons, </publisher> <year> 1984. </year>
Reference: [F91] <author> N. Fenton: </author> <title> Software Metrics: A Rigorous Approach , Chapman & Hall, </title> <year> 1991. </year>
Reference-contexts: In this paper, we will focus on the validation of measures of the internal attributes of software products. Internal attributes, as defined in <ref> [F91] </ref>, can be measured purely in terms of the product itself, independent of how it relates to its environment. Software size, complexity, cohesion, and coupling are examples of internal attributes of software products. <p> Theoretical validation then involves modeling this intuitive understanding of the attribute we want to measure. In the framework of measurement theory <ref> [F91, Z91] </ref>, intuition and empirical knowledge are modeled by empirical relational systems, whose definition is provided below. (This definition and all definition related to the basics of measurement theory are taken from [Z91, pp. 40 - 51], based on [R79].) A relational system A is an ordered tuple (A,R 1 n <p> Each unit of an attribute contributing to a valid measure is equivalent. Different entities can have the same attribute value. Consistent with the precepts of measurement theory <ref> [BEM96, F91, R79] </ref>, a valid measure should obey the representation condition (i.e., preserve the intuitive expected behavior of the measured attibute). Even though we acknowledge the importance of all four requirements mentioned above, only the representation condition has been the focus of our discussion in this section. <p> Several books and papers on the topic of measurement theory are conveying the idea that scale types should be used to proscribe the use of "inappropriate" statistical techniques. For example, a table similar to the one shown in Figure 1 is given in <ref> [F91] </ref>. This table, for instance, proscribes the use of the Pearson product moment correlation for scale types that are either nominal or ordinal. Such proscriptions, of course, are not unique to software engineering.
Reference: [FK90] <author> N. Fenton and B. Kitchenham: </author> <title> Validating Software Measures. In Journal of Software Testing, Verification and Reliability , 1(2) </title> <type> 27-42, </type> <year> 1990. </year>
Reference-contexts: 1. Introduction: When Is a Measure Valid? Recent software engineering literature has reflected a concern for methods to validate measures of attributes of software products (e.g., see [KPF95], [S92], <ref> [FK90] </ref>). This concern is driven, at least partially, by a recognition that: (i) common practices for the validation of software engineering measures are not acceptable on scientific grounds, and (ii) valid measures are essential for effective software project management and sound empirical research. <p> This kind of approach, however, has also not been used in software engineering. 11 Fenton and Kitchenham <ref> [FK90] </ref> criticise the commonly used empirical validation procedure whereby the analyst correlates the internal product attribute measure with any interesting measure which happened to be available as data. We agree with this criticism and our approach does not advocate doing that.
Reference: [Gib71] <author> J. Gibbons: </author> <title> Nonparametric Statistical Inference , McGraw-Hill, </title> <year> 1971. </year>
Reference-contexts: In general, for large samples , to achieve the same power as the Spearman correlation, a test using Pearsons coefficient would require only approximately 91% of the formers sample size [SC88]. This is called the asymptotic relative efficiency (ARE) <ref> [Gib71] </ref>. 6 The values in this table are based on the tables provided in [KT87] and [Coh88]. The calculations of sample sizes assume that the assumptions of the tests are met.
Reference: [GS89] <author> V. Gibson and J. Senn: </author> <title> System Strcuture and Software Maintenance Performance. </title> <booktitle> In Communications of the ACM , 32(3) </booktitle> <pages> 347-358, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: In addition, they may be assessing different attributes without even realizing it. A good example of this difficulty is illustrated in the experiment of Gibson and Senn <ref> [GS89] </ref>. In that article, the authors asked experienced programmers to rank systems that the programmers had modified during the experiment based on their complexity.
Reference: [HL89] <author> D. Hosmer and S. Lemeshow: </author> <title> Applied Logistic Regression. </title> , <publisher> Wiley-Interscience, </publisher> <year> 1989 </year>
Reference-contexts: estimation procedures for linear regression models: ln E = ln a + b ln L When the external attribute measure cannot be considered interval (or even close to be interval) and is, for example, considered as an ordinal or a dichotomous response variable, then classification techniques such as logistic regression <ref> [HL89] </ref> can be used effectively.
Reference: [J95] <author> M. Jrgensen: </author> <title> "Experience with the Accuracy of Software Maintenance Task Effort Prediction Models," </title> <journal> IEEE Trans. Software Eng. </journal> , <volume> 21 (8), </volume> <month> August, </month> <year> 1995. </year>
Reference-contexts: In addition, interaction terms may be significant and will make the model even more difficult to interpret. This is in part to construct models that are easier to interpret and use that techniques such as classification trees [SP88] and Optimized Set Reduction <ref> [BBT93; J95] </ref> have been developed. Therefore, it is important to note that regression, although the most often used approach to multivariate software engineering model construction, is not the only alternative.
Reference: [KPF95] <author> B. Kitchenham, S. Pfleeger, and N. Fenton: </author> <title> Towards a Framework for Software Measurement Validation. </title> <note> In IEEE Transactions on Software Engineering , 21(12) , December 1995. </note>
Reference-contexts: 1. Introduction: When Is a Measure Valid? Recent software engineering literature has reflected a concern for methods to validate measures of attributes of software products (e.g., see <ref> [KPF95] </ref>, [S92], [FK90]). This concern is driven, at least partially, by a recognition that: (i) common practices for the validation of software engineering measures are not acceptable on scientific grounds, and (ii) valid measures are essential for effective software project management and sound empirical research. <p> This concern is driven, at least partially, by a recognition that: (i) common practices for the validation of software engineering measures are not acceptable on scientific grounds, and (ii) valid measures are essential for effective software project management and sound empirical research. For example, in a recent paper <ref> [KPF95] </ref>, the authors write: "Unless the software measurement community can agree on a valid, consistent, and comprehensive theory of measurement validation, we have no scientific basis for the discipline of software measurement, a situation potentially disasterous for both practice and research." It is therefore crucial for the software engineering community to <p> At this point, s/he may choose to either: revise the empirical relational system s/he defined, or accept that measure as a function that measures something other than data flow complexity. 2.3 Comparison with Recent Work in the Literature In <ref> [KPF95] </ref>, the term "theoretical validation" is used in a much broader sense. <p> Even though we acknowledge the importance of all four requirements mentioned above, only the representation condition has been the focus of our discussion in this section. This is because it is very likely the most difficult requirement to satisfy in the definition of a valid measure. In addition, <ref> [KPF95] </ref> states that "any definition of an attribute that implies a particular measurement scale is invalid. [..] any property of an attribute that is asserted to be general property but implies a specific measurement scale must also be invalid." In general, then, this criterion means that any property that implies a <p> Given this state of affairs, we would therefore encourage analysts to conduct such replications if we are to gain confidence in the empirical validity of internal product measures. 3.6 Comparison with Recent Work in the Literature In <ref> [KPF95] </ref>, the term "empirical validation" includes two kinds of validations. <p> Therefore, this paper focuses exclusively on this particular form of empirical validation since this is the one most likely to be used in practice 11 . However, we do not wish to broadly exclude validation procedures such as those suggested by <ref> [KPF95] </ref>, only that we as a community do not yet know if they will produce meaningful results. 9 These results do not justify, however, the broad negative statements made by the authors of that article about subjective metrics in general.
Reference: [KT87] <author> H. Kraemer and S. Thiemann: </author> <title> How Many Subjects? Statistical Power Analysis in Research , Sage Publications, </title> <year> 1987. </year>
Reference-contexts: This is called the asymptotic relative efficiency (ARE) [Gib71]. 6 The values in this table are based on the tables provided in <ref> [KT87] </ref> and [Coh88]. The calculations of sample sizes assume that the assumptions of the tests are met. Where there are analogous tables in [Coh88], the sample size values are only slightly different from [KT87] (approximately -2 difference). - 15 - Power = 90% Power = 80% Corr. <p> relative efficiency (ARE) [Gib71]. 6 The values in this table are based on the tables provided in <ref> [KT87] </ref> and [Coh88]. The calculations of sample sizes assume that the assumptions of the tests are met. Where there are analogous tables in [Coh88], the sample size values are only slightly different from [KT87] (approximately -2 difference). - 15 - Power = 90% Power = 80% Corr.
Reference: [L91] <author> K. B. Lakshmanan, S. Jayaprakash, and P. K. Sinha: </author> <title> "Properties of Control-Flow Complexity Measures," </title> <journal> In IEEE Transactions on Software Engineering , vol. </journal> <volume> 17, no. 12, </volume> <pages> pp. 1289-1295, </pages> <month> Dec. </month> <year> 1991. </year>
Reference: [LE93] <author> R. Lindsay and A. Ehrenberg: </author> <title> The Design of Replicated Studies. </title> <booktitle> In The American Statistician , 47(3) </booktitle> <pages> 217-228, </pages> <year> 1993. </year>
Reference-contexts: The former maximizes confirmatory power, while the latter maximizes generalizability. These are (this is based on the discussion in <ref> [LE93] </ref>): a) Holdout sample. This can be considered as the most basic form of replicating an empirical validation of a measure. Following this kind of approach, the analyst simply divides his/her sample into a test sample and a holdout sample. The fraction of the holdout sample can vary.
Reference: [MT77] <author> F. Mosteller and J. Tukey: </author> <title> Data Analysis and Regression , Addison-Wesley, </title> <year> 1977. </year>
Reference: [NWK90] <author> J. Neter, W. Wasserman, and M. Kunter: </author> <title> Applied Linear Statistical Models , IRWIN, </title> <year> 1990. </year>
Reference-contexts: When ordinary least squares regression is utlized for validation, for example, the PRESS (prediction sum of squares) criterion can be used to interpret the results of this approach <ref> [NWK90] </ref>. b) Close replication. With close replication an analyst would attempt to repeat the actual empirical validation (i.e., collect a new set of data from another system or set of systems).
Reference: [Nun78] <editor> J. Nunnally: </editor> <publisher> Psychometric Theory , McGraw-Hill, </publisher> <year> 1978. </year> <month> - 23 </month> - 
Reference: [P72] <author> D. Parnas: </author> <title> "On the Criteria to be Used in Decomposing Systems into Modules.", </title> <journal> Communications of the ACM , 14(1): </journal> <pages> 221-227, </pages> <year> 1972. </year>
Reference-contexts: A theory defines relationships amongst product, process, and resource attributes. For example, a simple theory could define a negative association between code complexity and maintainability (i.e., the more complex code is less maintainable). As another example, consider Parnas' theory about design <ref> [P72] </ref>, which is commonly accepted amongst researchers and practitioners.
Reference: [R79] <author> F. Roberts: </author> <title> Measurement Theory with Applications to Decisionmaking, Utility, </title> <publisher> and the Social Sciences , Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: In the framework of measurement theory [F91, Z91], intuition and empirical knowledge are modeled by empirical relational systems, whose definition is provided below. (This definition and all definition related to the basics of measurement theory are taken from [Z91, pp. 40 - 51], based on <ref> [R79] </ref>.) A relational system A is an ordered tuple (A,R 1 n 1 m ) where A is a nonempty set of objects, the R i ki-ary relations on A and the o j , j=1,,m are closed binary operations. Empirical Relational System: A = (A,R ,,R ,o ,o ). <p> Each unit of an attribute contributing to a valid measure is equivalent. Different entities can have the same attribute value. Consistent with the precepts of measurement theory <ref> [BEM96, F91, R79] </ref>, a valid measure should obey the representation condition (i.e., preserve the intuitive expected behavior of the measured attibute). Even though we acknowledge the importance of all four requirements mentioned above, only the representation condition has been the focus of our discussion in this section.
Reference: [S92] <author> N. Schneidewind: </author> <title> Methodology for Validating Software Metrics. </title> <journal> In IEEE Transactions on Software Engineering , 18(5) </journal> <pages> 410-422, </pages> <year> 1992. </year>
Reference-contexts: 1. Introduction: When Is a Measure Valid? Recent software engineering literature has reflected a concern for methods to validate measures of attributes of software products (e.g., see [KPF95], <ref> [S92] </ref>, [FK90]). This concern is driven, at least partially, by a recognition that: (i) common practices for the validation of software engineering measures are not acceptable on scientific grounds, and (ii) valid measures are essential for effective software project management and sound empirical research. <p> This is, of course, the most basic form of validation, and a prerequisite to demonstrating the usefulness of a measure (which is empirical validation), since one cannot properly test theories if one is not sure that s/he is measuring the attributes in those theories. b) Empirical validation (e.g., see <ref> [S92] </ref>) is carried out to demonstrate that a measure is useful in the sense that it is related to other variables in expected ways (as defined in the theories).
Reference: [SP88] <author> R. Selby and A. Porter: </author> <title> "Learning from Examples: Generation and Evaluation of Decision Trees for Software Resource Analysis", </title> <journal> IEEE Trans. Software Eng ., 14 (12), </journal> <month> December, </month> <year> 1988. </year>
Reference-contexts: In addition, interaction terms may be significant and will make the model even more difficult to interpret. This is in part to construct models that are easier to interpret and use that techniques such as classification trees <ref> [SP88] </ref> and Optimized Set Reduction [BBT93; J95] have been developed. Therefore, it is important to note that regression, although the most often used approach to multivariate software engineering model construction, is not the only alternative.
Reference: [SC88] <author> S. Siegel and J. Castellan: </author> <title> Nonparametric Statistics for the Behavioral Sciences , McGraw Hill, </title> <year> 1988. </year>
Reference-contexts: Such proscriptions, of course, are not unique to software engineering. For instance, they were originally presented by the psychologist Stevens [Ste46], serve as the basis of the classic text of Siegel on nonparametric statistics <ref> [SC88] </ref>, and serve as an integral part of the decision tree developed by Andrews et al. [AKD+81] to guide researchers in the selection of the most appropriate statistics. <p> In general, for large samples , to achieve the same power as the Spearman correlation, a test using Pearsons coefficient would require only approximately 91% of the formers sample size <ref> [SC88] </ref>. This is called the asymptotic relative efficiency (ARE) [Gib71]. 6 The values in this table are based on the tables provided in [KT87] and [Coh88]. The calculations of sample sizes assume that the assumptions of the tests are met.
Reference: [Ste46] <author> S. Stevens: </author> <booktitle> On the Theory of Scales of Measurement. In Science , 103(2684) </booktitle> <pages> 677-680, </pages> <month> June </month> <year> 1946. </year>
Reference-contexts: This table, for instance, proscribes the use of the Pearson product moment correlation for scale types that are either nominal or ordinal. Such proscriptions, of course, are not unique to software engineering. For instance, they were originally presented by the psychologist Stevens <ref> [Ste46] </ref>, serve as the basis of the classic text of Siegel on nonparametric statistics [SC88], and serve as an integral part of the decision tree developed by Andrews et al. [AKD+81] to guide researchers in the selection of the most appropriate statistics. <p> Meeting this demand would rule out the use of all psychological tests, sociological indices, rating scales, and interview responses ... this eliminates virtually all kinds of quantitative variables on which the behavioral sciences depend. Even Stevens himself, with respect to ordinal scales, concedes that <ref> [Ste46] </ref>: "In the strictest propriety the ordinary statistics involving means and standard deviations ought not to be used with these scales, for these statistics imply a knowledge of something more than relative rank-order of data.
Reference: [TZ92] <author> J. Tian and M. V. Zelkowitz: </author> <title> "A Formal Program Complexity Model and Its Application," </title> <journal> J. Syst. </journal> <volume> Software , vol. 17, </volume> <pages> pp. 253-266, </pages> <year> 1992. </year>
Reference: [Tuk86a] <author> J. Tukey: </author> <title> Data Analysis and Behavioral Science or Learning to Bear the Quantitative Mans Burden by Shunning Badmandments. In The Collected Works of John W. Tukey , Vol. III, </title> <publisher> Wadsworth, </publisher> <year> 1986. </year>
Reference-contexts: Therefore, there are many cases where researchers cannot demonstrate that their scales are interval, but they are confident that they are more than only ordinal. By treating them as ordinal, researchers would be discarding a good deal of information. Therefore, as Tukey <ref> [Tuk86a] </ref> notes The question must be If a scale is not an interval scale, must it be merely ordinal? Is it realistic to answer questions about scale type with absolute certainty, since their answers always rely on intuition and are therefore subjective? Can we know for sure the scale types of <p> It is informative to note that much of the recent progress in the social sciences would not have been possible if the use of "approximate" measurement scales had been strictly proscribed. For example, Tukey <ref> [Tuk86a] </ref> states after summarizing Stevens proscriptions This view thus summarized is a dangerous one. If generally adopted it would not only lead to inefficient analysis of data, but it would also lead to failure to give any answer at all to questions whose answers are perfectly good, though slightly approximate.
Reference: [Tuk86b] <author> J. Tukey: </author> <title> The Future of Data Analysis. In The Collected Works of John W. Tukey , Vol. III, </title> <publisher> Wadsworth, </publisher> <year> 1986. </year>
Reference-contexts: And in those cases, should we just discard our practical questionswhose answers may have a real impact on the software process because we are not 100% positive about the scale types of the measures we are using? To paraphrase Tukey <ref> [Tuk86b] </ref>, "Science is not mathematics" and we are not looking for perfection and absolute proofs but for evidence that our theories match reality as closely as possible. The other alternative, i.e., reject approximate theories, would have catastrophic consequences on most sciences, and in particular, on software engineering.
Reference: [W88] <author> E. Weyuker: </author> <title> Evaluating Software Complexity Measures. </title> <journal> In IEEE Transactions on Software Engineering , 14(9) </journal> <pages> 1357-1365, </pages> <year> 1988. </year>
Reference-contexts: Therefore, for example, we cannot study the relationship between code complexity and maintainability unless we can really measure the attributes of complexity and maintainability with a sufficient degree of accuracy. To this end, the two following kinds of validation are necessary in software engineering. a) Theoretical validation (e.g., see <ref> [W88] </ref>) is carried out to show that a measure is really measuring the attribute it is purporting to measure. <p> Usually, the relations contained in empirical relational systems are of different kinds. We will provide here three instances. We will assume that we want to build an empirical relational system for the size of program bodies, i.e., executable sections of programs, as defined in <ref> [W88] </ref>. A first set of relations defines partial orders . <p> This can be done by using a set of properties which characterize all measures of a specified attribute. Instances of such properties can be found in the literature <ref> [W88, L91,TZ92, BMB96] </ref>. Again, such properties are defined based on one's intuition. <p> Theoretical validation as presented in that article includes validation of measurement instruments, validation of data collection procedures (i.e., referred to as protocols in the paper), and satisfaction of some general requirements about measures similar to the ones mentioned by Weyuker <ref> [W88] </ref>: An attribute is measurable if it allows different entities to be distinguished from one another. Each unit of an attribute contributing to a valid measure is equivalent. Different entities can have the same attribute value.
Reference: [Z91] <author> H. Zuse: </author> <title> Software Complexity: Measures and Methods , de Gruyter, </title> <year> 1991. </year>
Reference-contexts: Theoretical validation then involves modeling this intuitive understanding of the attribute we want to measure. In the framework of measurement theory <ref> [F91, Z91] </ref>, intuition and empirical knowledge are modeled by empirical relational systems, whose definition is provided below. (This definition and all definition related to the basics of measurement theory are taken from [Z91, pp. 40 - 51], based on [R79].) A relational system A is an ordered tuple (A,R 1 n <p> In the framework of measurement theory [F91, Z91], intuition and empirical knowledge are modeled by empirical relational systems, whose definition is provided below. (This definition and all definition related to the basics of measurement theory are taken from <ref> [Z91, pp. 40 - 51] </ref>, based on [R79].) A relational system A is an ordered tuple (A,R 1 n 1 m ) where A is a nonempty set of objects, the R i ki-ary relations on A and the o j , j=1,,m are closed binary operations. <p> Of course, this mapping may not be arbitrary. This leads to the following definition of a scale. Definition 2.2 (Scale): 4 4 Definition 2.2 was slightly changed as compared to the one in <ref> [Z91] </ref>. - 6 - 1 n 1 m ) be an empirical relational system and B = (B,S 1 n 1 m ) a formal relational system and m a measure.
References-found: 44

