URL: http://www.cs.ucsd.edu/users/ckrintz/papers/ASPLOS-98-CCDP.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/ckrintz/
Root-URL: http://www.cs.ucsd.edu
Email: fcalder,ckrintz,sjohng@cs.ucsd.edu taustin@ichips.intel.com  
Title: Cache-Conscious Data Placement  
Author: Brad Calder Chandra Krintz Simmi John Todd Austin 
Affiliation: Dept. of Computer Science and Engineering Microcomputer Research Labs University of California, San Diego Intel Corporation  
Abstract: In this paper we present a general framework for Cache Conscious Data Placement. This is a compiler directed approach that creates an address placement for the stack (local variables), global variables, heap objects, and constants in order to reduce data cache misses. The placement of data objects is guided by a temporal relationship graph between objects generated via profiling. Our results show that profile driven data placement significantly reduces the data miss rate by 24% on average. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and S. D. Pudar. </author> <title> Column-associative caches: A technique for reducing the miss rate of direct-mapped caches. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <address> 21(2):179190, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [21], column-associative caches <ref> [1] </ref>, stride tolerant address mappings [10], page coloring [20], and program restructuring to improve data [4] or instruction cache performance [28]. In this paper, a novel software-based data placement optimization, called Cache-Conscious Data Placement (CCDP), is introduced as a technique for reducing the frequency of data cache misses.
Reference: [2] <author> Todd M. Austin. </author> <title> Hardware and Software Mechanisms for Reducing Load Latency. </title> <type> TR 1311, </type> <institution> Computer Sciences Department, UW Madison, Madison, WI, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: We used the approach presented in [11] for procedure placement as the basis for our cache-conscious data placement algorithm. The data placement mechanisms were adapted from <ref> [2] </ref>. A report by Seidl and Zorn [30] examined the issues dealing with naming heap allocated objects for potential data placement. Their study examined several different prediction mechanisms used to name heap objects. Their techniques focussed on the XOR naming scheme described earlier in this paper.
Reference: [3] <author> D. A. Barrett and B. G. Zorn. </author> <title> Using lifetime predictors to improve memory allocation performance. </title> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> 28(6):187196, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Generating names for heap variables is a more challenging task. Heap object addresses can change with different inputs to the program, making their address an unsuitable name. The approach implemented in this work is based on the naming scheme of Barrett and Zorn <ref> [3] </ref>. Heap variables are named when they are created (e.g., at calls to malloc ()) using the address of the call site to malloc () combined (with XOR-folding) with a few return addresses from the stack. Similar heap naming schemes were also employed by Lebeck and Wood [22].
Reference: [4] <author> S. Carr, K. S. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> 28(5):252262, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [21], column-associative caches [1], stride tolerant address mappings [10], page coloring [20], and program restructuring to improve data <ref> [4] </ref> or instruction cache performance [28]. In this paper, a novel software-based data placement optimization, called Cache-Conscious Data Placement (CCDP), is introduced as a technique for reducing the frequency of data cache misses. Copyright (c) 1998 by the Association for Computing Machinery, Inc.
Reference: [5] <author> C.-L. Chen and C.-K. Liao. </author> <title> Analysis of vector access performance on skewed interleaved memory. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <address> 17(3):387394, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Page coloring [23, 20] techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [8] have been proposed for similar benefits. Compiler-directed array dimension extension <ref> [5] </ref> and array variable padding [29] work to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself. Placement optimizations have been used to reduce false sharing in shared memory multiprocessors [16].
Reference: [6] <author> T. Chilimbi and J. Larus. </author> <title> Using generational garbage collection to implement cache-conscious data placement. </title> <note> Submitted for publication., </note> <year> 1998. </year>
Reference-contexts: Concurrently, Chilimbi et al. developed similar techniques for optimizing heap data placement. In [7] they describe a data placement optimization for tree-like structures. Their approach is semiautomatic, permitting more aggressive optimizations, such as the splitting of structure variables. In <ref> [6] </ref> they extend this approach to support on-line profiling and data placement for Cecil, an object-oriented language with generational garbage collection. 7 Conclusions Cache-conscious data placement is introduced as a software-based technique to improve data cache performance by relocating variables in the virtual memory space.
Reference: [7] <author> T. Chilimbi, J. Larus, and M. Hill. </author> <title> Improving pointer-based codes through cache-conscious data placement. </title> <note> Submitted for publication., </note> <year> 1998. </year>
Reference-contexts: Our results show that a holistic approach must be taken to data placement, accurately placing the stack, global, heap, and constants. Concurrently, Chilimbi et al. developed similar techniques for optimizing heap data placement. In <ref> [7] </ref> they describe a data placement optimization for tree-like structures. Their approach is semiautomatic, permitting more aggressive optimizations, such as the splitting of structure variables.
Reference: [8] <author> F. Dahlgren and P. Stenstrom. </author> <title> On reconfigurable on-chip data caches. </title> <booktitle> Proceedings of the 24th Annual International Symposium on Microar-chitecture, </booktitle> <address> 22(1):189198, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Page coloring [23, 20] techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings <ref> [8] </ref> have been proposed for similar benefits. Compiler-directed array dimension extension [5] and array variable padding [29] work to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself.
Reference: [9] <author> K. I. Farkas and N. P. Jouppi. </author> <title> Complexity/performance tradeoffs with non-blocking loads. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> 22(2):211222, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling [19], or run-time techniques including out-of-order issue, decoupled execution [31], or non-blocking loads <ref> [9] </ref>. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [18], victim caches [17], and prefetching [26].
Reference: [10] <author> Q. S. Gao. </author> <title> The chinese remainder theorem and the prime memory system. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <address> 21(2):337340, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [21], column-associative caches [1], stride tolerant address mappings <ref> [10] </ref>, page coloring [20], and program restructuring to improve data [4] or instruction cache performance [28]. In this paper, a novel software-based data placement optimization, called Cache-Conscious Data Placement (CCDP), is introduced as a technique for reducing the frequency of data cache misses.
Reference: [11] <author> N. Gloy, T. Blockwell, M.D. Smith, and B. Calder. </author> <title> Procedure placement using temporal ordering information. </title> <booktitle> In 30th International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: This metric should estimate the number of cache misses that would be caused by placing a group of overlapped objects into the same cache line. To create this metric we use the Temporal Relationship Graph (TRG) from previous procedure placement work by Gloy et al. <ref> [11] </ref>. The TRG contains weighted edges between objects, which represents their degree of temporal locality. <p> The TRGplace graph used for data placement is slightly modified from the above description to keep track of relationships on a smaller granularity than objects. One result from the procedure placement study <ref> [11] </ref> was that it is hard to place large procedures especially if they are larger than the cache. To effectively place large procedures the temporal information needs to be kept track of on a smaller granularity. We found this same result applies to placing data. <p> For the results presented in this paper we used a chunk size of 256 bytes. This size was large enough to keep the TRG within a manageable size, and small enough to allow large objects to be placed. See <ref> [11] </ref> for the complete details and tradeoffs for building a TRG. 3.3 Data Placement Algorithm The cache-conscious data placement algorithm uses the profiled TRG, the size of the objects, and the structure of the target cache (i.e., cache size and block size) to eliminate cache conflicts and increase cache line utilization. <p> Recent work on procedure placement for improve instruction cache performance has shown that further improvements in performance can be achieved by keeping track of which cache lines procedures are placed to eliminate conflict misses, and by using temporal information to guide the placement algorithm <ref> [13, 11] </ref>. This research showed that taking into consideration the cache attributes when performing the placement and the temporal relation ships between procedures significantly reduces the cache miss rate. We used the approach presented in [11] for procedure placement as the basis for our cache-conscious data placement algorithm. <p> This research showed that taking into consideration the cache attributes when performing the placement and the temporal relation ships between procedures significantly reduces the cache miss rate. We used the approach presented in <ref> [11] </ref> for procedure placement as the basis for our cache-conscious data placement algorithm. The data placement mechanisms were adapted from [2]. A report by Seidl and Zorn [30] examined the issues dealing with naming heap allocated objects for potential data placement. <p> Adapted from previous work on text layout optimization <ref> [11] </ref>, we demonstrate that the algorithm finds placement solutions that improve data cache performance, with a 24% miss rate reduction on average. Moreover, it consistently improves data cache performance across all experiments, even when profiling inputs different from analyzed inputs. Future work entails implementing CCDP to examine execution performance.
Reference: [12] <author> D. Grunwald, B. G. Zorn, and R. Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> 28(6):177186, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The heap allocator we model is similar to previously proposed heap allocators that map objects of similar sizes to the same pages of memory during allocation <ref> [12] </ref>. The difference is we use data placement to guide heap objects into allocation bins. In the CCDP custom allocator, there are several free lists each with an associated bin tag. <p> The main reason for an increase in page usage for the heap program is the allocation algorithm. The original placement uses a single heap bin with a first-fit heap allocator <ref> [12] </ref>. Whereas our heap placement algorithm uses a temporal-fit heap allocation algo rithm, and several different allocation heap bins might be used as specified by the placement algorithm.
Reference: [13] <author> A.H. Hashemi, D.R. Kaeli, and B. Calder. </author> <title> Efficient procedure mapping using cache lince coloring. </title> <booktitle> In Proceedings of the SIGPLANS'97 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 171182, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Recent work on procedure placement for improve instruction cache performance has shown that further improvements in performance can be achieved by keeping track of which cache lines procedures are placed to eliminate conflict misses, and by using temporal information to guide the placement algorithm <ref> [13, 11] </ref>. This research showed that taking into consideration the cache attributes when performing the placement and the temporal relation ships between procedures significantly reduces the cache miss rate. We used the approach presented in [11] for procedure placement as the basis for our cache-conscious data placement algorithm.
Reference: [14] <author> M. D. Hill and A. J. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12):16121630, </volume> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: With data placement to control the contents and location of data cache blocks, it becomes possible to influence the performance of the data cache. Consider how changing a variables placement affects a data cache miss from each of the three miss classes <ref> [14] </ref>: Conflict Misses: Conflict misses occur when the number of frequently referenced blocks of memory map to the same cache set is greater than the associativity of the cache. Blocks that do not fit into the cache set will displace other blocks each time they are referenced.
Reference: [15] <author> W. W. Hwu and P. P. Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler. </title> <booktitle> Proceedings of the16th International Symposium on Computer Architecture, </booktitle> <pages> pages 242251, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering <ref> [15, 28] </ref>, function grouping [34, 15, 28], reordering based on control structure [24], and reordering of system code [33] have all been shown to significantly improve instruction cache performance. <p> The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering [15, 28], function grouping <ref> [34, 15, 28] </ref>, reordering based on control structure [24], and reordering of system code [33] have all been shown to significantly improve instruction cache performance.
Reference: [16] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile-time data transformations. </title> <booktitle> Proceedings of the Symposium on Principals and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Compiler-directed array dimension extension [5] and array variable padding [29] work to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself. Placement optimizations have been used to reduce false sharing in shared memory multiprocessors <ref> [16] </ref>. Compiler-directed variable partitioning has been proposed as an approach to reduce inter-variable interactions [27] for the purpose of improving the predictability of cache access latencies in real-time systems. The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks.
Reference: [17] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> 18(2):364373, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [18], victim caches <ref> [17] </ref>, and prefetching [26].
Reference: [18] <author> N. P. Jouppi and S. J.E. Wilton. </author> <title> Tradeoffs in two-level on-chip caching. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> 22(2):3445, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling [19], or run-time techniques including out-of-order issue, decoupled execution [31], or non-blocking loads [9]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches <ref> [18] </ref>, victim caches [17], and prefetching [26].
Reference: [19] <author> D. R. Kerns and S. J. Eggers. </author> <title> Balanced scheduling: Instruction scheduling when memory latency is unknown. </title> <booktitle> Proceedings of the 1992 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> 28(6):278289, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling <ref> [19] </ref>, or run-time techniques including out-of-order issue, decoupled execution [31], or non-blocking loads [9]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [18], victim caches [17], and prefetching [26].
Reference: [20] <author> R. E. Kessler. </author> <title> Analysis of Multi-Megabyte Secondary CPU Cache Memories. </title> <type> TR 1032, </type> <institution> Computer Sciences Department, UWMadison, Madison, WI, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [21], column-associative caches [1], stride tolerant address mappings [10], page coloring <ref> [20] </ref>, and program restructuring to improve data [4] or instruction cache performance [28]. In this paper, a novel software-based data placement optimization, called Cache-Conscious Data Placement (CCDP), is introduced as a technique for reducing the frequency of data cache misses. Copyright (c) 1998 by the Association for Computing Machinery, Inc. <p> Alternatively, the TRG graph for a direct mapped cache may provide enough information to achieve most of the potential from data placement for associative caches. 6 Related Work A number of peripheral works employ data relocation to improve data cache performance. Page coloring <ref> [23, 20] </ref> techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [8] have been proposed for similar benefits.
Reference: [21] <author> R. E. Kessler, R. Jooss, A. Lebeck, and M. D. Hill. </author> <title> Inexpensive implementations of set-associativity. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <address> 17(3):131139, </address> <year> 1989. </year>
Reference-contexts: It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [18], victim caches [17], and prefetching [26]. Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches <ref> [21] </ref>, column-associative caches [1], stride tolerant address mappings [10], page coloring [20], and program restructuring to improve data [4] or instruction cache performance [28].
Reference: [22] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache profiling and the spec benchmarks: A case study. </title> <journal> IEEE Computer, </journal> <volume> 27(10):1526, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Heap variables are named when they are created (e.g., at calls to malloc ()) using the address of the call site to malloc () combined (with XOR-folding) with a few return addresses from the stack. Similar heap naming schemes were also employed by Lebeck and Wood <ref> [22] </ref>. This naming approach does a reasonably good job of satisfying the constraints listed above. Since the addresses of call sites and function returns do not change between runs of a program (provided the program is not recompiled), heap variable names do not change between runs.
Reference: [23] <author> W. L. Lynch, B. K. Bray, and M. J. Flynn. </author> <title> The effect of page allocation on caches. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <address> 23(1):222225, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Alternatively, the TRG graph for a direct mapped cache may provide enough information to achieve most of the potential from data placement for associative caches. 6 Related Work A number of peripheral works employ data relocation to improve data cache performance. Page coloring <ref> [23, 20] </ref> techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [8] have been proposed for similar benefits.
Reference: [24] <author> S. McFarling. </author> <title> Procedure merging with instruction caches. </title> <booktitle> Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> 26(6):7179, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering [15, 28], function grouping [34, 15, 28], reordering based on control structure <ref> [24] </ref>, and reordering of system code [33] have all been shown to significantly improve instruction cache performance. Like this work, the approaches usually rely on profile information to guide heuristic algorithms in placing instructions to minimize instruction cache conflicts, and maximize cache line utilization and block prefetch.
Reference: [25] <author> A. B. Montz, D. Mosberger, S. W. O'Malley, L. L. Peterson, T. A. Proebsting, and J. H. Hartman. </author> <title> Scout: A communications-oriented operating system. </title> <type> Technical Report TR 9420, </type> <institution> Department of Computer Science, University of Arizona, </institution> <address> Tucson, AZ, </address> <month> June </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Placement optimizations have been used to reduce false sharing in shared memory multiprocessors [16]. Compiler-directed variable partitioning has been proposed as an approach to reduce inter-variable interactions [27] for the purpose of improving the predictability of cache access latencies in real-time systems. The Scout operating system <ref> [25] </ref> employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance.
Reference: [26] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Conference Proceedings of the Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> 27(9):6273, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [18], victim caches [17], and prefetching <ref> [26] </ref>. Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [21], column-associative caches [1], stride tolerant address mappings [10], page coloring [20], and program restructuring to improve data [4] or instruction cache performance [28].
Reference: [27] <author> F. Mueller. </author> <title> Compiler support for software-based cache partitioning. </title> <booktitle> ACM SIGPLAN Workshop on Languages, Compilers, and Tools for Real-Time Systems, </booktitle> <address> 30(11):125133, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Placement optimizations have been used to reduce false sharing in shared memory multiprocessors [16]. Compiler-directed variable partitioning has been proposed as an approach to reduce inter-variable interactions <ref> [27] </ref> for the purpose of improving the predictability of cache access latencies in real-time systems. The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance.
Reference: [28] <author> K. Pettis and R. C. Hansen. </author> <title> Profile guided code positioning. </title> <booktitle> Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> 25(6):1627, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Reducing the frequency of cache misses also works to reduce the performance impact of cache misses; approaches along these lines include set-associative caches [21], column-associative caches [1], stride tolerant address mappings [10], page coloring [20], and program restructuring to improve data [4] or instruction cache performance <ref> [28] </ref>. In this paper, a novel software-based data placement optimization, called Cache-Conscious Data Placement (CCDP), is introduced as a technique for reducing the frequency of data cache misses. Copyright (c) 1998 by the Association for Computing Machinery, Inc. <p> The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering <ref> [15, 28] </ref>, function grouping [34, 15, 28], reordering based on control structure [24], and reordering of system code [33] have all been shown to significantly improve instruction cache performance. <p> The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering [15, 28], function grouping <ref> [34, 15, 28] </ref>, reordering based on control structure [24], and reordering of system code [33] have all been shown to significantly improve instruction cache performance.
Reference: [29] <author> G. Rivera and C.-W. Tseng. </author> <title> Data transformations for eliminating conflict misses. </title> <booktitle> In Proceedings of the SIGPLAN'98 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Page coloring [23, 20] techniques have leveraged the memory mapping capability of virtual memory to reduce conflicts in physically indexed caches. User-programmable cache set mappings [8] have been proposed for similar benefits. Compiler-directed array dimension extension [5] and array variable padding <ref> [29] </ref> work to relocate data within large arrays, giving opportunity to improve data cache performance when a large array conflicts with itself. Placement optimizations have been used to reduce false sharing in shared memory multiprocessors [16].
Reference: [30] <author> M. Seidl and B. Zorn. </author> <title> Predicting references to dynamically allocated objects. </title> <institution> CU-CS-826-97, University of Colorado at Boulder, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: The modified malloc first computes the heap allocation name, an integer value, by XOR-folding N return addresses from the stack. For the results in this paper we used a name depth of 4, which other researchers have also found to have reasonable results <ref> [30] </ref>. The heap allocator we model is similar to previously proposed heap allocators that map objects of similar sizes to the same pages of memory during allocation [12]. The difference is we use data placement to guide heap objects into allocation bins. <p> We used the approach presented in [11] for procedure placement as the basis for our cache-conscious data placement algorithm. The data placement mechanisms were adapted from [2]. A report by Seidl and Zorn <ref> [30] </ref> examined the issues dealing with naming heap allocated objects for potential data placement. Their study examined several different prediction mechanisms used to name heap objects. Their techniques focussed on the XOR naming scheme described earlier in this paper.
Reference: [31] <author> J. E. Smith. </author> <title> Decoupled access/execute architectures. </title> <booktitle> Proceedings of the 9th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 112119, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: 1 Introduction Much effort has been invested in reducing the impact of cache misses on program performance. As with any other latency, cache miss latency can be tolerated using compile-time techniques such as instruction scheduling [19], or run-time techniques including out-of-order issue, decoupled execution <ref> [31] </ref>, or non-blocking loads [9]. It is also possible to reduce the latency of cache misses using techniques that include multi-level caches [18], victim caches [17], and prefetching [26].
Reference: [32] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation, pages 196205. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: The C++ programs were compiled with GCC. We compiled the SPEC benchmark suite under OSF/1 V4.0 operating system using full compiler optimization (-O4 -ifo). For the results in this paper we used ATOM <ref> [32] </ref> to instrument the programs, gather the Name and TRG profiles, perform the data placement optimization, and finally gather the data cache simulation miss rate results.
Reference: [33] <author> J. Torrellas, C. Xia, and R. Daigle. </author> <title> Optimizing instruction cache performance for operating system intensive workloads. </title> <booktitle> In Proceedings of the First International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360369, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering [15, 28], function grouping [34, 15, 28], reordering based on control structure [24], and reordering of system code <ref> [33] </ref> have all been shown to significantly improve instruction cache performance. Like this work, the approaches usually rely on profile information to guide heuristic algorithms in placing instructions to minimize instruction cache conflicts, and maximize cache line utilization and block prefetch.
Reference: [34] <author> Y. Wu. </author> <title> Ordering functions for improving memory reference locality in a shared memory multiprocessor system. </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <address> 23(1):218221, </address> <month> December </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: The Scout operating system [25] employs data placement to reduce data cache conflict between active protocol stacks. Many parallels to this work can be found in software techniques developed for improving instruction cache performance. Techniques such as basic block re-ordering [15, 28], function grouping <ref> [34, 15, 28] </ref>, reordering based on control structure [24], and reordering of system code [33] have all been shown to significantly improve instruction cache performance.
References-found: 34

