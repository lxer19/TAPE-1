URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P473.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Tensor Methods for Large Sparse Systems of Nonlinear Equations  
Author: Ali Bouaricha and Robert B. Schnabel 
Keyword: Key words. tensor methods, nonlinear equations, sparse problems, rank-deficient matrices.  
Note: Research supported by AFOSR Grants No. AFOSR-90-0109 and F49620-94-1-0101, ARO Grants No. DAAL03-91-G-0151 and DAAH04-94-G-0228, and NSF Grant No. CCR-9101795.  
Address: Argonne, IL 60439, USA  Boulder, CO 80309-0430, USA  
Affiliation: MCS Division, Argonne National Laboratory,  Department of Computer Science, University of Colorado,  
Abstract: This paper introduces tensor methods for solving large sparse systems of nonlinear equations. Tensor methods for nonlinear equations were developed in the context of solving small to medium-sized dense problems. They base each iteration on a quadratic model of the nonlinear equations, where the second-order term is selected so that the model requires no more derivative or function information per iteration than standard linear model-based methods, and hardly more storage or arithmetic operations per iteration. Computational experiments on small to medium-sized problems have shown tensor methods to be considerably more efficient than standard Newton-based methods, with a particularly large advantage on singular problems. This paper considers the extension of this approach to solve large sparse problems. The key issue considered is how to make efficient use of sparsity in forming and solving the tensor model problem at each iteration. Accomplishing this turns out to require an entirely new way of solving the tensor model that successfully exploits the sparsity of the Jacobian, whether the Jacobian is nonsingular or singular. We develop such an approach and, based upon it, an efficient tensor method for solving large sparse systems of nonlinear equations. Test results indicate that this tensor method is significantly more efficient and robust than an efficient sparse Newton-based method, in terms of iterations, function evaluations, and execution time. fl Work supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38, by the National Aerospace Agency under Purchase Order L25935D, and by the National Science Foundation, through the Center for Research on Parallel Computation, under Cooperative Agreement No. CCR-9120008. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> A. Bjorck and I. S. Duff. </author> <title> A direct method for the solution of sparse linear least squares problems. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 34 </volume> <pages> 43-67, </pages> <year> 1980. </year>
Reference-contexts: Therefore, we would like to solve the least squares problem min d2 R n jjJd + F jj 2 : (4:5) The method that we use to solve the problem (4.5) is an extension of the method of Peters and Wilkinson [19] that was suggested by Bjorck and Duff <ref> [2] </ref>. This approach usually produces a better solution to (4.5) than the one obtained by using the MA28 package, which sets the last r components of the solution z in (4.4) to 0, where r is the rank deficiency of J. <p> The remainder of this section reviews the method of Bjorck and Duff. The first step in the method of Bjorck and Duff <ref> [2] </ref> is to compute an LU factorization of the Jacobian matrix J, using Gaussian elimination with both row and column interchanges. <p> Else: Step 3.1 Calculate the Newton step d n from the LU factorization of J by the Bjorck and Duff <ref> [2] </ref> method to find some solution to min d2R n jjJd + F jj 2 . Step 3.2 Select the next iterate x + by using the line search algorithm 5.2 outlined below, where d n is the search direction, and go to Step 5.
Reference: [3] <author> A. Bouaricha. </author> <title> Solving large sparse systems of nonlinear equations and nonlinear least squares problems using tensor methods on sequential and parallel computers. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: We have also developed tensor methods for solving large, sparse nonlinear least squares problems. The issues involved are considerably different because of the different large sparse linear algebraic computations that are required. This work is described in <ref> [3] </ref> and in a forthcoming paper. Finally, we continue to develop variants of tensor methods for solving very large systems of nonlinear equations that are based on iterative linear solvers such as Krylov subspace methods.
Reference: [4] <author> A. Bouaricha and R. B. Schnabel. TENSOLVE: </author> <title> A software package for solving systems of nonlinear equations and nonlinear least squares problems using tensor methods. </title> <type> Technical Report CU-CS-735-94, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1994. </year>
Reference-contexts: Tensor methods for small to medium-sized dense systems of nonlinear equations were introduced by Schnabel and Frank [20], and a software package implementing them is described in <ref> [4] </ref>. <p> In tests reported in <ref> [4] </ref> for both nonsingular and singular problems, the tensor method virtually never is less efficient than a standard method based upon a linear (Newton) model, and usually is more efficient. <p> An iteration of the tensor method is summarized in Algorithm 2.1 below. For more details on tensor methods, including the global strategy used in Step 5 of Algorithm 2.1, see Schnabel 5 and Frank [20] and Bouaricha and Schnabel <ref> [4] </ref>. Algorithm 2.1. An Iteration of the Tensor Method for Dense Nonlinear Equations Given n, current iterate x c , F (x c ) Step 0 Calculate F 0 (x c ), and decide whether to stop. <p> Solving the Newton Model Along with the Sparse Tensor Model As in the dense case <ref> [20, 4] </ref>, the global strategy that is used in our tensor method for sparse nonlinear equations sometimes utilizes the Newton step rather than the tensor step (see Section 5). In the dense case, the Newton step can be computed inexpensively as a by-product of computing the tensor step. <p> Some parts of Algorithm 5.1 are still stated in terms of arbitrary p, for generality. The global strategy that is used in our implementation is a standard line search. In <ref> [4] </ref>, both line search and two-dimensional trust region strategies were used in tensor methods for small, dense systems of nonlinear equations. In the tests in that paper, both methods appeared to be 12 equally robust, with the trust region method possibly having a small advantage in efficiency. <p> We have used the line search in the sparse code, however, because of its greater simplicity and because the two-dimensional trust region method requires two additional matrix-vector multiplications involving the Jacobian matrix. The line search strategy that we use is identical to that developed and used in [20] and <ref> [4] </ref>, so we review it only very briefly here. If the full tensor step provides sufficient decrease in jjF (x)jj, it is taken. <p> Based on our previous experience with trust region and line search methods <ref> [4] </ref>, we believe that a trust region strategy often would have helped on these cases. We examined our test results to obtain an experimental indication of the local convergence behavior of the tensor method and Newton's method on problems where rank (F 0 (x fl )) = n 1.
Reference: [5] <author> A. E. Bryson and A. E. Ho. </author> <title> Applied Optimal Control, </title> <journal> Chap. </journal> <volume> 2. </volume> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: First we tested them on three sparse problems provided to us from Boeing Computer Services (BCS) and used as test problems in [13]. These problems are described as follows: 1. LTS : This problem discretizes the differential equations for the Linear Tangent Steering problem in Bryson and Ho <ref> [5] </ref>. This is the search problem formulation where the adjoint differential equations are also discretized and an optimality condition is imposed. 2. GRST : This problem discretizes the differential equations for a coast about a spherical earth.
Reference: [6] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Fortran subroutines for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 </volume> <pages> 346-347, </pages> <year> 1984. </year>
Reference-contexts: Algorithm 5.1. An Iteration of the Tensor Method for Sparse Nonlinear Equations Given current iterate x c ; F (x c ) Step 0 Calculate J = F 0 (x c ) analytically or by finite-difference approximations <ref> [6, 7] </ref>, and decide whether to stop. If not: Step 1 Form the second-order term of the tensor model, T c , so that the tensor model interpo lates F (x) at the most recent past point (i.e., p = 1). <p> As in Algorithm 5.1, the Jacobian matrix is factored at each iteration by using the MA28 package, and if the Jacobian is singular, the search direction is calculated by the method of Bjorck and Duff. In all our experiments, we calculate the Jacobian matrix by finite-difference approximations <ref> [6, 7] </ref>. We tested these algorithms on a variety of nonsingular and singular problems. First we tested them on three sparse problems provided to us from Boeing Computer Services (BCS) and used as test problems in [13]. These problems are described as follows: 1.
Reference: [7] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Software for estimating sparse Jacobian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 </volume> <pages> 329-345, </pages> <year> 1984. </year>
Reference-contexts: Algorithm 5.1. An Iteration of the Tensor Method for Sparse Nonlinear Equations Given current iterate x c ; F (x c ) Step 0 Calculate J = F 0 (x c ) analytically or by finite-difference approximations <ref> [6, 7] </ref>, and decide whether to stop. If not: Step 1 Form the second-order term of the tensor model, T c , so that the tensor model interpo lates F (x) at the most recent past point (i.e., p = 1). <p> As in Algorithm 5.1, the Jacobian matrix is factored at each iteration by using the MA28 package, and if the Jacobian is singular, the search direction is calculated by the method of Bjorck and Duff. In all our experiments, we calculate the Jacobian matrix by finite-difference approximations <ref> [6, 7] </ref>. We tested these algorithms on a variety of nonsingular and singular problems. First we tested them on three sparse problems provided to us from Boeing Computer Services (BCS) and used as test problems in [13]. These problems are described as follows: 1.
Reference: [8] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 187-207, </pages> <year> 1983. </year>
Reference-contexts: While this is not always the case for small problems|quasi-Newton approximations to the Jacobian sometimes being used instead|it is almost always the case in methods that are used for solving large sparse systems of nonlinear equations. The derivatives usually come from efficient sparse finite differences (see, e.g., <ref> [8] </ref>), from user-supplied analytic derivatives, or recently through automatic differentiation (see, e.g., [14, 15]). Hence, this requirement is not a problem and indeed fits this approach well.
Reference: [9] <author> D. W. Decker and C. T. Kelly. </author> <title> Newton's method at singular points I. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 </volume> <pages> 66-70, </pages> <year> 1980. </year>
Reference-contexts: The ratios for a typical problem are given in Table 6.5. In almost all cases the standard method exhibits local linear convergence with constant near 0.5, which is consistent with the theoretical analysis (see, e.g., <ref> [9, 10] </ref>). The local convergence rate of the tensor method is faster, with a typical final ratio of around 0.01. This final ratio might be smaller if analytic Jacobians were used in combination with tighter stopping tolerances.
Reference: [10] <author> D. W. Decker and C. T. Kelly. </author> <title> Newton's method at singular points II. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 </volume> <pages> 465-471, </pages> <year> 1980. </year>
Reference-contexts: The ratios for a typical problem are given in Table 6.5. In almost all cases the standard method exhibits local linear convergence with constant near 0.5, which is consistent with the theoretical analysis (see, e.g., <ref> [9, 10] </ref>). The local convergence rate of the tensor method is faster, with a typical final ratio of around 0.01. This final ratio might be smaller if analytic Jacobians were used in combination with tighter stopping tolerances.
Reference: [11] <author> I. S. Duff. MA28: </author> <title> A set of Fortran subroutines for for sparse unsymmetric linear equations. </title> <type> Technical Report R-8730, </type> <institution> AERE Harwell Laboratory, </institution> <year> 1977. </year>
Reference-contexts: Then we solve U z = y (4:4) for z, where z = P 2 T d. Finally d = P 2 z. Our algorithm uses the MA28 package <ref> [11] </ref> to perform the sparse matrix factorization and triangular solves. If the matrix J is singular, then (4.1) has either zero or an infinite number of solutions. <p> The description includes some more details about the sparse matrix factorization than were given in preceding sections, and a description of the global strategy. We present test results for this implementation in Section 6. As stated previously, the sparse linear equation solutions in our implementation use the MA28 package <ref> [11] </ref>, a widely used package for solving large, sparse, unsymmetric systems of linear equations. <p> If not: Step 1 Form the second-order term of the tensor model, T c , so that the tensor model interpo lates F (x) at the most recent past point (i.e., p = 1). Step 2 Factorize J by using the MA28 software package <ref> [11] </ref>.
Reference: [12] <author> D. Feng, P. Frank, and R. B. Schnabel. </author> <title> Local convergence analysis of tensor methods for nonlinear equations. </title> <journal> Math. Prog., </journal> <volume> 62 </volume> <pages> 427-459, </pages> <year> 1993. </year>
Reference-contexts: Theoretically, the methods converge at least as quickly as Newton's method on nonsingular problems and have been shown to have 3-step Q-order 1.5 convergence on problems where the Jacobian has rank n 1 at the solution, whereas Newton's method is linearly convergent with constant 1/2 on such problems <ref> [12] </ref>. In tests reported in [4] for both nonsingular and singular problems, the tensor method virtually never is less efficient than a standard method based upon a linear (Newton) model, and usually is more efficient. <p> The local convergence rate of the tensor method is faster, with a typical final ratio of around 0.01. This final ratio might be smaller if analytic Jacobians were used in combination with tighter stopping tolerances. As is anticipated in <ref> [12] </ref>, the convergence usually seems to be one-step superlinear, although only a three-step Q-order 3 2 result can be proven.
Reference: [13] <author> P. D. Frank and W. P. Huffman. </author> <title> Parallel solution of large and sparse nonlinear systems. </title> <type> Technical Report ECA-TR-128, </type> <institution> Boeing Computer Services, </institution> <year> 1989. </year>
Reference-contexts: In all our experiments, we calculate the Jacobian matrix by finite-difference approximations [6, 7]. We tested these algorithms on a variety of nonsingular and singular problems. First we tested them on three sparse problems provided to us from Boeing Computer Services (BCS) and used as test problems in <ref> [13] </ref>. These problems are described as follows: 1. LTS : This problem discretizes the differential equations for the Linear Tangent Steering problem in Bryson and Ho [5]. This is the search problem formulation where the adjoint differential equations are also discretized and an optimality condition is imposed. 2.
Reference: [14] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical programming: Recent developments and applications, </booktitle> <pages> pages 83-108, </pages> <address> Amsterdam, 1989. </address> <publisher> Kluwer Academic Publishers, Amsterdam. </publisher>
Reference-contexts: The derivatives usually come from efficient sparse finite differences (see, e.g., [8]), from user-supplied analytic derivatives, or recently through automatic differentiation (see, e.g., <ref> [14, 15] </ref>). Hence, this requirement is not a problem and indeed fits this approach well. Second, the methods for forming and solving the tensor model must make efficient use of the sparsity of the Jacobian matrix and not involve any dense linear algebra using n fi n matrices.
Reference: [15] <author> Andreas Griewank and George F. Corliss, </author> <title> editors. Automatic differentiation of algorithms: Theory, implementation, and application. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991. </year>
Reference-contexts: The derivatives usually come from efficient sparse finite differences (see, e.g., [8]), from user-supplied analytic derivatives, or recently through automatic differentiation (see, e.g., <ref> [14, 15] </ref>). Hence, this requirement is not a problem and indeed fits this approach well. Second, the methods for forming and solving the tensor model must make efficient use of the sparsity of the Jacobian matrix and not involve any dense linear algebra using n fi n matrices.
Reference: [16] <author> J. J. </author> <title> More. A collection of nonlinear model problems. </title> <editor> In E. L. Allgower and K. Georg, editors, </editor> <title> Computational solution of nonlinear systems of equations, </title> <booktitle> volume 26 of Lecture Notes in Applied Mathematics, </booktitle> <pages> pages 723-762. </pages> <publisher> American Mathematical Society, </publisher> <year> 1990. </year>
Reference-contexts: We also ran our methods on some sparse nonlinear equations problems from More, Garbow, and Hillstrom [17], namely, the Broyden banded, the Broyden tridiagonal, and the variable-dimension test problems, and on the distillation column test problem from <ref> [16] </ref>. Finally, we tested our methods on the MINPACK-2 test problems collection [1], namely, the driven cavity, the flow in a channel, the incompressible elastic rod, and the swirling flow between disks problems.
Reference: [17] <author> J. J. More, B. S. Garbow, and K. E. Hillstrom. </author> <title> Testing unconstrained optimization software. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 </volume> <pages> 17-41, </pages> <year> 1981. </year>
Reference-contexts: For the starting iterate we randomly perturb the components of the solution by adding or subtracting 0.1 from each. We also ran our methods on some sparse nonlinear equations problems from More, Garbow, and Hillstrom <ref> [17] </ref>, namely, the Broyden banded, the Broyden tridiagonal, and the variable-dimension test problems, and on the distillation column test problem from [16].
Reference: [18] <author> N. Munksgaard. NS02: </author> <title> A Fortran subroutine for solving sparse sets of nonlinear equations by Powell's Dog-leg algorithm. </title> <type> Technical Report R-11047, </type> <institution> AERE Harwell Laboratory, </institution> <year> 1938. </year>
Reference-contexts: We then tested our methods on a system of sparse trigonometric equations from <ref> [18] </ref> that have the form n X (a ij sin x j + b ij cos x j ) + j=1 where the matrices fa ij g and fb ij g have the same sparsity pattern as one another, including nonzeros on the diagonal, and fc ij g has a different
Reference: [19] <author> G. Peters and J. H. Wilkinson. </author> <title> The least squares problem and pseudo-inverses. </title> <journal> Computer J., </journal> <volume> 13 </volume> <pages> 309-316, </pages> <year> 1970. </year>
Reference-contexts: Therefore, we would like to solve the least squares problem min d2 R n jjJd + F jj 2 : (4:5) The method that we use to solve the problem (4.5) is an extension of the method of Peters and Wilkinson <ref> [19] </ref> that was suggested by Bjorck and Duff [2]. This approach usually produces a better solution to (4.5) than the one obtained by using the MA28 package, which sets the last r components of the solution z in (4.4) to 0, where r is the rank deficiency of J.
Reference: [20] <author> R. B. Schnabel and P. D. Frank. </author> <title> Tensor methods for nonlinear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 21 </volume> <pages> 815-843, </pages> <year> 1984. </year>
Reference-contexts: Tensor methods for small to medium-sized dense systems of nonlinear equations were introduced by Schnabel and Frank <ref> [20] </ref>, and a software package implementing them is described in [4]. <p> It is possible that no root exists; in this case a least squares solution of the model is found instead. Thus, in general, the problem find d 2 R n that minimizes jj M (x c + d) jj 2 (2:6) is solved. Schnabel and Frank <ref> [20] </ref> show that the solution to (2.6) can be reduced to the solution of q quadratic equations in p unknowns (i.e., a very small system of quadratics), plus the solution of n q linear equations in n p unknowns. <p> An iteration of the tensor method is summarized in Algorithm 2.1 below. For more details on tensor methods, including the global strategy used in Step 5 of Algorithm 2.1, see Schnabel 5 and Frank <ref> [20] </ref> and Bouaricha and Schnabel [4]. Algorithm 2.1. An Iteration of the Tensor Method for Dense Nonlinear Equations Given n, current iterate x c , F (x c ) Step 0 Calculate F 0 (x c ), and decide whether to stop. <p> Solving the Newton Model Along with the Sparse Tensor Model As in the dense case <ref> [20, 4] </ref>, the global strategy that is used in our tensor method for sparse nonlinear equations sometimes utilizes the Newton step rather than the tensor step (see Section 5). In the dense case, the Newton step can be computed inexpensively as a by-product of computing the tensor step. <p> We have used the line search in the sparse code, however, because of its greater simplicity and because the two-dimensional trust region method requires two additional matrix-vector multiplications involving the Jacobian matrix. The line search strategy that we use is identical to that developed and used in <ref> [20] </ref> and [4], so we review it only very briefly here. If the full tensor step provides sufficient decrease in jjF (x)jj, it is taken. <p> In the remainder of this section, we will refer to the trigonometric, the BCS, and the More, Garbow, and Hillstrom test problems as the TBM collection. These problems all have nonsingular Jacobians at the solution. We created singular test problems as proposed in Schnabel and Frank <ref> [20] </ref> by modifying these nonsingular test problems to the form ^ F (x) = F (x) F 0 (x fl )A (A T A) 1 A T (x x fl ); (6:2) where F (x) is the standard nonsingular test function, x fl is its root, and A 2 R nfik <p> substantial gains by the tensor method over Newton's method on the rank n 2 TBM test problems are somewhat surprising because in theory the tensor method may not always achieve faster than linear convergence on problems where the rank of F 0 (x fl ) is n 2 or less <ref> [20] </ref>. Though the tensor and Newton methods are both quadratically convergent on nonsingular problems, the tensor method is clearly the more efficient on the nonsingular TBM test problems. The improvement by the tensor method over the Newton method on the nonsingular TBM test problems, however, is less dramatic.
Reference: [21] <author> R. B. Schnabel, J. E. Koontz, and B. E. Weiss. </author> <title> A modular system of algorithms of unconstrained minimization. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 11 </volume> <pages> 419-440, </pages> <year> 1985. </year> <month> 24 </month>
Reference-contexts: The result is a system of q quadratic equations in the p unknowns, ^ d 2 , plus a system of n q equations in all the variables that is linear in the n p unknowns, ^ d 1 . 3. A nonlinear unconstrained optimization software package, UNCMIN <ref> [21] </ref>, is used to minimize the l 2 norm of the q quadratic equations in the p unknowns, ^ d 2 . (If p = 1, this procedure is done analytically instead.) 4. <p> In the dense case, the arithmetic cost per iteration of the above algorithm is the standard O (n 3 ) cost of a matrix factorization, plus an additional O (n 2 p) ( O (n 2:5 )) operations for the orthogonal transformations, plus the cost of using UNCMIN <ref> [21] </ref> in Step 3 of the algorithm.
References-found: 20

