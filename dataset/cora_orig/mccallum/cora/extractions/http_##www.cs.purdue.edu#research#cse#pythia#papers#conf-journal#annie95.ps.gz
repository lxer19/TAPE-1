URL: http://www.cs.purdue.edu/research/cse/pythia/papers/conf-journal/annie95.ps.gz
Refering-URL: http://www.cs.purdue.edu/research/cse/pythia/pythia-papers.html
Root-URL: http://www.cs.purdue.edu
Title: Neuro-Fuzzy Systems for Intelligent Scientific Computation  
Author: Narendran Ramakrishnan, Anupam Joshi, Sanjiva Weerawarana, Elias N. Houstis and John R. Rice 
Address: West Lafayette, IN 47907, USA  
Affiliation: Department of Computer Sciences Purdue University  
Abstract: Intelligence has been envisioned as a key component of future problem solving environments for scientific computing. This paper describes a computationally intelligent approach to address a major problem in scientific computation, i.e., the efficient solution of partial differential equations (PDEs). This approach is implemented in PYTHIA a system that supports smart parallel PDE solvers. PYTHIA provides advice on what method and parameters to use for the solution of a specific PDE problem. It achieves this by comparing the characteristics of the given PDE with those of previously observed classes of PDEs. An important step in the reasoning mechanism of PYTHIA is the categorization of PDE problems into classes based on their characteristics. Exemplar based reasoning systems and backpropagation style neural networks have been earlier used to this end. In this paper, we describe the use of fuzzy min-max neural networks to realize the same objective. This method converges faster, is more accurate, generalizes very well and provides on-line adaptation. This technique makes certain assumptions about the pattern classes underlying the domain. In applying the fuzzy min-max network to our domain , we improve the method by relaxing these assumptions. This scheme will form a major component of future problem solving environments for scientific computing that are being developed by our group. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Joshi A., Weerawarana S., and Houstis E.N., </author> <title> The use of neural networks to support "intelligent" scientific computing, </title> <booktitle> Proceedings Int. Conf. Neural Networks, World Congress on Computational Intelligence, vol. IV, 1994, </booktitle> <address> (Orlando, Florida), </address> <pages> pp. 411-416. </pages>
Reference-contexts: Joshi et.al. <ref> [1] </ref> describe connectionist schemes for the problem of classifying PDE problems into classes based on properties of their solutions. This paper deals mainly with the PYTHIA system which is an advisory system that supports smart parallel PDE solvers for partial differential equations. <p> Consider the k th ordered pair fA k ; d k g from the training set. Let the desired output for the k th pattern be <ref> [1; 1; 0; 0; :::; 0] </ref>. <p> for each set of labeled data i from training set do for each class j that pattern i belongs to box = identifyexpandablebox (); if (box = NOTAVAILABLE) addnewbox (); else expandbox (box); flag = checkforoverlap (); if (flag = true) contracthyperboxes (); g training outputs - d k1 = <ref> [1; 0; 0; 0; :::; 0] </ref> and d k2 = [0; 1; 0; 0; :::; 0] respectively. In other words, the pattern is associated with both class 1 and class 2. This will cause hyperboxes of both classes 1 and 2 to completely contain the pattern A k . <p> do for each class j that pattern i belongs to box = identifyexpandablebox (); if (box = NOTAVAILABLE) addnewbox (); else expandbox (box); flag = checkforoverlap (); if (flag = true) contracthyperboxes (); g training outputs - d k1 = [1; 0; 0; 0; :::; 0] and d k2 = <ref> [0; 1; 0; 0; :::; 0] </ref> respectively. In other words, the pattern is associated with both class 1 and class 2. This will cause hyperboxes of both classes 1 and 2 to completely contain the pattern A k . <p> Thus, the above procedure results in the pattern having equal degrees of membership in both the hyperboxes but is not completely contained in either of them. Assume that the network is first trained with the desired output as d k1 = <ref> [1; 0; 0; 0; :::; 0] </ref>. This results in the k th pattern A k having complete containment in a hyperbox of class 1 (because the 1st bit is set to 1). <p> This results in the k th pattern A k having complete containment in a hyperbox of class 1 (because the 1st bit is set to 1). Then when we train the same pattern with <ref> [0; 1; 0; 0; :::; 0] </ref>, a hyperbox of class 2 will be created/expanded to include the k th pattern. This will result in hyperbox overlap. <p> very well to mutually non-exclusive classes. 10 dashed line indicates the error on the test set. 11 12 13 the training set while the dashed line represents that on the test set. 6 The modified algorithm #2 However we realize that if a PDE problem has a d h = <ref> [0; 1; 0; 0; 1] </ref> (i.e., it has a solution that is analytic and has mixed boundary conditions), then the modified algorithm #1 will identify the PDE correctly as belonging to the above two classes but it will produce an output of, say, [0; 0:91; 0; 0; 0:91]. <p> But, we can reason intuitively that the actual output should be <ref> [0; 1; 0; 0; 1] </ref>. We inherit this feature from Simpson's original algorithm which does not allow for overlap betwen hyperboxes representing different classes. <p> However, the true benefits of the new approach become evident only when we try it on a set like the PYTHIA PDE data set. Thus instead of an output like [0; 0:91; 0; 0; 0:91], we get <ref> [0; 1; 0; 0; 1] </ref> which reflects the true non-exclusive nature of the pattern classes.
Reference: [2] <author> Wayne R. Dyksen and Carl R. Gritter, </author> <title> Scientific computing and the algorithm selection problem, Expert Systems for Scientific Computing (E. </title> <editor> N. Houstis, J. R. Rice, and R. Vichnevetsky, eds.), </editor> <publisher> North-Holland, </publisher> <year> 1992, </year> <pages> pp. 19-31. </pages>
Reference-contexts: At the other end of the PDE solution process, expert systems have been designed that apply self-validating methods in an economical manner to systems of linear equations. These help to guide the internals of a linear system solver. In <ref> [2] </ref>, Dyksen and Gritter describe an expert system for selecting solution methods for elliptic PDEs based on problem characteristics.
Reference: [3] <author> S. Fahlman and C. Lebiere, </author> <title> The casade-correlation learning architecture, </title> <type> Tech. Report CMU-CS-90-100, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> Feb </month> <year> 1990. </year>
Reference-contexts: To control the formation of hyperboxes, the only parameter that needs to be tuned is the maximum hyperbox size i.e., the size beyond which a hyperbox cannot be expanded. When this value is set to zero, the algorithm described above reverts to the k-nearest-neighbor classifier algorithm <ref> [3] </ref>. Recall in the network consists of calculating the fuzzy union of the membership function values produced from each of the fuzzy set hyperboxes. This can be implemented as a three-layer neural network (Fig. 1). <p> Simpson tabulates the performance of various traditional, fuzzy and neural pattern classifiers in [15]. Below, we reproduce the table and provide the performance of the various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier <ref> [3] </ref> - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron [8] - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 <p> Simpson tabulates the performance of various traditional, fuzzy and neural pattern classifiers in [15]. Below, we reproduce the table and provide the performance of the various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier <ref> [3] </ref> - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron [8] - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified <p> Below, we reproduce the table and provide the performance of the various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier <ref> [3] </ref> - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron [8] - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified algorithm #1 performs at least as well as other classifiers. <p> Below, we reproduce the table and provide the performance of the various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier <ref> [3] </ref> - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron [8] - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified algorithm #1 performs at least as well as other classifiers.
Reference: [4] <author> Carpenter G. and Grossberg S., </author> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine, </title> <booktitle> Computer Vision, Graphics and Image Understanding 37 (1987), </booktitle> <pages> 54-115. </pages>
Reference-contexts: Interestingly, these same restrictions are characteristic of another classical neural network paradigm the fuzzy adaptive resonance theory (fuzzy ART) of Carpenter et.al. [5]. This is an analog pattern clustering system that combines the concepts of fuzzy logic with the original ART networks created in <ref> [4] </ref>. An unsupervised version of Simpson's algorithm has also been proposed that clusters unlabeled pattern data into hyperboxes [16]. Again, there is an assumption that the clusters to be found are mutually exclusive. In the following sections, we describe two enhanced schemes that operate with overlapping and nonexclusive classes.
Reference: [5] <author> Carpenter G., Grossberg S., and Rosen D., </author> <title> Fuzzy ART : An adaptive resonance algorithm for rapid, stable classification of analog patterns, </title> <booktitle> Proceedings 1991 Int. Joint Conf. Neural Networks, vol. II, 1991, (Seattle), </booktitle> <pages> pp. 411-416. 16 </pages>
Reference-contexts: It is not reasonable to assume that one parameter is sufficient to tune the entire system. Moreover, the effect of on classification accuracy is not completely understood. Interestingly, these same restrictions are characteristic of another classical neural network paradigm the fuzzy adaptive resonance theory (fuzzy ART) of Carpenter et.al. <ref> [5] </ref>. This is an analog pattern clustering system that combines the concepts of fuzzy logic with the original ART networks created in [4]. An unsupervised version of Simpson's algorithm has also been proposed that clusters unlabeled pattern data into hyperboxes [16].
Reference: [6] <author> E. Gallopoulos, E. Houstis, and J.R. Rice, </author> <title> Computer as Thinker/Doer: Problem-Solving Environments for Computational Science, </title> <booktitle> IEEE Computational Science and Enginerring 1 (1994), </booktitle> <volume> no. 2, </volume> <pages> 11-23. </pages>
Reference-contexts: 1 Introduction It has been envisioned that future problem solving environments (PSEs) will have some form of intelligence and will provide a natural interface within well defined domains of scientific application <ref> [6] </ref>. In this paper, we address how such intelligence can be achieved by a combination of neural and fuzzy mechanisms.
Reference: [7] <author> Bezdek J., Chuah S., and Leep D., </author> <title> Generalized k-nearest neighbor rules, </title> <booktitle> Fuzzy Sets and Systems 18 (1986), </booktitle> <pages> 237-256. </pages>
Reference-contexts: Below, we reproduce the table and provide the performance of the various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier [3] - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN <ref> [7] </ref> - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron [8] - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified algorithm #1 performs at least as
Reference: [8] <author> Keller J. and Hunt D., </author> <title> Incorporating fuzzy membership functions into the perceptron algorithm, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 7 (1985), </journal> <pages> 693-699. </pages>
Reference-contexts: and provide the performance of the various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier [3] - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron <ref> [8] </ref> - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified algorithm #1 performs at least as well as other classifiers. <p> various classifiers (in terms of the no. of wrong classifications) alongside the techniques. (i) Bayes Classifier [3] - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron <ref> [8] </ref> - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network [15] - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified algorithm #1 performs at least as well as other classifiers.
Reference: [9] <author> Peter K. Moore, Can Ozturan, and Joseph E. Flaherty, </author> <title> Towards the automatic numerical solution of partial differential equations, Intelligent Mathematical Software Systems (E. </title> <editor> N. Houstis, J. R. Rice, and R. Vichnevetsky, eds.), </editor> <publisher> North-Holland, </publisher> <year> 1990, </year> <pages> pp. 15-22. </pages>
Reference-contexts: Numerous systems have been proposed for assisting in various aspects of the PDE solution process. An abstract model for the algorithm selection problem is described by Rice [12]. An experimental methodology implementing this model has also been developed in [13]. In <ref> [9] </ref>, Moore et.al. describe a strategy for determining a geometry discretization that leads to a solution guaranteed to be within a certain prescribed accuracy.
Reference: [10] <author> Fisher R., </author> <title> The use of multiple measurements in taxonomic problems, </title> <journal> Annals of Eugenics 7 (1936), </journal> <volume> no. 2, </volume> <pages> 179-188. </pages>
Reference-contexts: The first data set comprised the Fisher iris data <ref> [10] </ref> . This is a very popular data set and there is a wide range of classification results that can be used to provide a good measure of realtive performance.
Reference: [11] <author> Krishnapuram R., Nasraoui O., and Frigui H., </author> <title> The Fuzzy C Spherical Shells Algorithm : A new approach, </title> <journal> IEEE Transactions on Neural Networks 3 (1992), </journal> <volume> no. 5, </volume> <pages> 663-671. </pages>
Reference-contexts: Also, for data of the form in Fig. 18, the natural pattern classification geometry will be that of a hypersphere. Thus, if we were to use hyperspheres/hyperellipsoids as space-covering primitives, a better classification can be obtained. Some work in this direction has been done in [14] and <ref> [11] </ref>. Our ongoing research addresses the issue of finding an algorithm that covers the pattern space with appropriate primitives. These questions can be related to general problems in n-dimensional computational geometry. Their exact solution is either intractable, or exceedingly expensive computationally.
Reference: [12] <author> J. R. Rice, </author> <title> The algorithm selection problem, </title> <booktitle> Advances in Computers 15 (1976), </booktitle> <month> 65-118. </month> <title> [13] , Methodology for the algorithm selection problem, Performance Evaluation of Numerical Software (L. </title> <editor> D. Fosdick, ed.), </editor> <publisher> North-Holland, </publisher> <year> 1979, </year> <pages> pp. 301-307. </pages>
Reference-contexts: Numerous systems have been proposed for assisting in various aspects of the PDE solution process. An abstract model for the algorithm selection problem is described by Rice <ref> [12] </ref>. An experimental methodology implementing this model has also been developed in [13]. In [9], Moore et.al. describe a strategy for determining a geometry discretization that leads to a solution guaranteed to be within a certain prescribed accuracy.
Reference: [14] <author> Dave R.N. and Bhaswan K., </author> <title> Adaptive Fuzzy c-Shells Clustering and Detection of ellipses, </title> <journal> IEEE Transactions on Neural Networks 3 (1992), </journal> <volume> no. 5, </volume> <pages> 643-662. </pages>
Reference-contexts: Also, for data of the form in Fig. 18, the natural pattern classification geometry will be that of a hypersphere. Thus, if we were to use hyperspheres/hyperellipsoids as space-covering primitives, a better classification can be obtained. Some work in this direction has been done in <ref> [14] </ref> and [11]. Our ongoing research addresses the issue of finding an algorithm that covers the pattern space with appropriate primitives. These questions can be related to general problems in n-dimensional computational geometry. Their exact solution is either intractable, or exceedingly expensive computationally.
Reference: [15] <author> P.K. Simpson, </author> <title> Fuzzy Min-Max Neural Networks-Part 1: Classification, </title> <journal> IEEE Transactions on Neural Networks 3 (1992), </journal> <volume> no. 5, </volume> <month> 776-786. </month> <title> [16] , Fuzzy Min-Max Neural Networks-Part 2: Clustering, </title> <journal> IEEE Transactions on Fuzzy Systems 1 (1993), </journal> <volume> no. 1, </volume> <pages> 32-45. </pages>
Reference-contexts: Section 7 considers interesting variations of the general scheme and suggests application areas where they might be useful. Section 8 has a summary and provides pointers for further research in this field. 2 Fuzzy Min Max Neural Networks Fuzzy Min-Max neural networks were proposed by Simpson <ref> [15] </ref> as a supervised learning paradigm that finds reasonable decision boundaries in pattern space. We now briefly describe Simpson's method. This method uses fuzzy sets to describe pattern classes. Each fuzzy set is the fuzzy union of several n-dimensional hyperboxes. <p> True enough, when the growth parameter was set to 0.0175, the number of hyperboxes formed by Simpson's algorithm and the modified algorithm #1 were both 48. Simpson tabulates the performance of various traditional, fuzzy and neural pattern classifiers in <ref> [15] </ref>. <p> wrong classifications) alongside the techniques. (i) Bayes Classifier [3] - 2 (ii) k-nearest neighbor [3] - 4 (iii) Fuzzy k-NN [7] - 4 (iv) Fisher ratios [3] - 3 (v) Ho-Kashyap [3] - 2 (vi) Perceptron [8] - 3 (vii) Fuzzy Perceptron [8] - 3 (viii) Fuzzy min-max neural network <ref> [15] </ref> - 2 (ix) Modified algorithm #1 - 2 Thus we realize that the modified algorithm #1 performs at least as well as other classifiers.
Reference: [17] <author> S. Weerawarana, </author> <title> Problem solving environments for partial differential equation based systems, </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1994. </year> <month> 17 </month>
Reference-contexts: These help to guide the internals of a linear system solver. In [2], Dyksen and Gritter describe an expert system for selecting solution methods for elliptic PDEs based on problem characteristics. Weerawarana <ref> [17] </ref> argues that using problem characteristics is not sufficient and proposes an exemplar based reasoning system that uses performance fl This work was supported in part by NSF awards ASC 9404859 and CCR 9202536, AFOSR award F49620-92-J-0069 and ARPA ARO award DAAH04-94-G-0010 1 profiles of PDE solvers to determine a solver
References-found: 15

