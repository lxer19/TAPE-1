URL: http://www.ius.cs.cmu.edu/IUS/sourdough_usr0/har/papers/pami-96-05-09/online.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/har/faces.html
Root-URL: 
Title: Rowley, Baluja, and Kanade: Neural Network-Based Face Detection Neural Network-Based Face Detection for creating new
Author: Henry A. Rowley, Shumeet Baluja, and Takeo Kanade 
Keyword: Face detection, Pattern recognition, Computer vision, Artificial neural networks, Machine learning  
Note: Copyright 1998 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or  or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.  
Date: January 1998) 1  
Pubnum: (PAMI,  
Abstract: We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented; showing that our system has comparable performance in terms of detection and false-positive rates.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Shumeet Baluja. </author> <title> Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. </title> <type> Technical Report CMU-CS-94-163, </type> <institution> Carnegie Mellon University, </institution> <year> 1994. </year>
Reference: [2] <author> Shumeet Baluja. </author> <title> Expectation-Based Selective Attention. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University Computer Science Department, </institution> <month> October </month> <year> 1996. </year> <note> Available as CS Technical Report CMU-CS-96-182. </note>
Reference-contexts: using to detect faces, then present the error rates of the system over two large test sets. 3.1 Sensitivity Analysis In order to determine which part of its input image the network uses to decide whether the input is a face, we performed a sensitivity analysis using the method of <ref> [2] </ref>. We collected a positive test set based on the training database of face images, but with different randomized scales, translations, and rotations than were used for training. The negative test set was built from a set of negative examples collected during the training of other networks. <p> When an image sequence is available, temporal coherence can focus attention on particular portions of the images. As a face moves about, its location in one frame is a strong predictor of its location in next frame. Standard tracking methods, as well as expectation-based methods <ref> [2] </ref>, can be applied to focus the detector's attention. Other methods of improving system performance include obtaining more positive examples for training, or applying more sophisticated image preprocessing and Rowley, Baluja, and Kanade: Neural Network-Based Face Detection (PAMI, January 1998) 12 normalization techniques.
Reference: [3] <author> Gilles Burel and Dominique Carel. </author> <title> Detection and localization of faces on digital images. </title> <journal> Pattern Recognition Letters, </journal> <volume> 15:963967, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Many face detection researchers have used the idea that facial images can be characterized directly in terms of pixel intensities. These images can be characterized by probabilistic models of the set of face images [4, 13, 15], or implicitly by neural networks or other mechanisms <ref> [3, 12, 14, 19,21, 23,25, 26] </ref>. The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. A few authors have taken the approach of extracting features and applying either manually or automatically generated rules for evaluating these features [7, 11]. <p> If any other centroid locations represent a face overlapping with the current centroid, they are removed from the output pyramid. All remaining centroid locations constitute the final detection result. In the face detection work described in <ref> [3] </ref>, similar observations about the nature of the outputs were made, resulting in the development of heuristics similar to those described above. 2.2.2 Arbitration among Multiple Networks To further reduce the number of false positives, we can apply multiple networks, and arbitrate between their outputs to produce the final decision.
Reference: [4] <author> Antonio J. Colmenarez and Thomas S. Huang. </author> <title> Face detection with information-based maximum discrimination. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 782787, </pages> <year> 1997. </year>
Reference-contexts: The algorithms and training methods are designed to be general, with little customization for faces. Many face detection researchers have used the idea that facial images can be characterized directly in terms of pixel intensities. These images can be characterized by probabilistic models of the set of face images <ref> [4, 13, 15] </ref>, or implicitly by neural networks or other mechanisms [3, 12, 14, 19,21, 23,25, 26]. The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. <p> It may be possible to make the detectors more robust using the bootstrap training technique described here and in [21]. In recent work, Colmenarez and Huang presented a statistically based method for face detection <ref> [4] </ref>. Their system builds probabilistic models of the sets of faces and nonfaces, and compares how well each input window compares with these two categories.
Reference: [5] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4):705 719, </volume> <year> 1993. </year>
Reference-contexts: We used 120 images of scenery for collecting negative examples in the bootstrap manner described above. A typical training run selects approximately 8000 nonface images from the 146,212,178 subimages that are available at all locations and scales in the training scenery images. A similar training algorithm was described in <ref> [5] </ref>, where at each iteration an entirely new network was trained with the examples on which the previous networks had made mistakes. 2.2 Stage Two: Merging Overlapping Detections and Arbitration The examples in Fig. 3 showed that the raw output from a single network will contain a number of false detections.
Reference: [6] <author> Charles Frankel, Michael J. Swain, and Vassilis Athitsos. WebSeer: </author> <title> An image search engine for the world wide web. </title> <type> Technical Report TR-96-14, </type> <institution> University of Chicago, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Systems utilizing the detector described above allow a user to make requests of the form Show me the people who appear in this video [18, 20] or Which images on the World Wide Web contain faces? <ref> [6] </ref> and to have their queries answered automatically. Acknowledgements The authors would like to thank Kah-Kay Sung and Dr. Tomaso Poggio (at MIT) and Dr. Wood-ward Yang (at Harvard) for providing a series of test images and a mug-shot database for training, respectively.
Reference: [7] <author> Venu Govindaraju. </author> <title> Locating human faces in photographs. </title> <journal> International Journal of Computer Vision, </journal> <volume> 19(2):129146, </volume> <year> 1996. </year>
Reference-contexts: The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. A few authors have taken the approach of extracting features and applying either manually or automatically generated rules for evaluating these features <ref> [7, 11] </ref>. Training a neural network for the face detection task is challenging because of the difficulty in characterizing prototypical nonface images.
Reference: [8] <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: Apply the pre processing steps to each of these images. 2. Train a neural network to produce an output of 1 for the face examples, and -1 for the nonface examples. The training algorithm is standard error backpropogation with momentum <ref> [8] </ref>. On the first iteration of this loop, the network's weights are initialized randomly. After the first iteration, we use the weights computed by training in the previous iteration as the starting point. 3. Run the system on an image of scenery which contains no faces.
Reference: [9] <author> H. Martin Hunke. </author> <title> Locating and tracking of human faces with neural networks. </title> <type> Master's thesis, </type> <institution> University of Karlsruhe, </institution> <year> 1994. </year>
Reference-contexts: By taking a picture of the background scene, one can determine which portions of the picture have changed in a newly acquired image, and analyze only those portions of the image. Similarly, a skin color detector like the one presented in <ref> [9] </ref> can restrict the search region.
Reference: [10] <author> Y. Le Cun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. </author> <title> Backpropogation applied to handwritten zip code recognition. </title> <booktitle> Neural Computation, </booktitle> <address> 1:541551, </address> <year> 1989. </year>
Reference-contexts: For the experiments which are described later, we use networks with two and three sets of these hidden units. Similar input connection patterns are commonly used in speech and character recognition tasks <ref> [10, 24] </ref>. The network has a single, real-valued output, which indicates whether or not the window contains a face. Examples of output from a single network are shown in Fig. 3.
Reference: [11] <author> T. K. Leung, M. C. Burl, and P. Perona. </author> <title> Finding faces in cluttered scenes using random labeled graph matching. </title> <booktitle> In Fifth International Conference on Computer Vision, </booktitle> <pages> pages 637 644, </pages> <address> Cambridge, Massachusetts, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. A few authors have taken the approach of extracting features and applying either manually or automatically generated rules for evaluating these features <ref> [7, 11] </ref>. Training a neural network for the face detection task is challenging because of the difficulty in characterizing prototypical nonface images.
Reference: [12] <author> S. H. Lin, S. Y. Kung, and L. J. Lin. </author> <title> Face recognition/detection by probabilistic decision-based neural network. </title> <journal> IEEE Transactions on Neural Networks, Special Issue on Artificial Neural Networks and Pattern Recognition, </journal> <volume> 8(1), </volume> <month> January </month> <year> 1997. </year>
Reference-contexts: Many face detection researchers have used the idea that facial images can be characterized directly in terms of pixel intensities. These images can be characterized by probabilistic models of the set of face images [4, 13, 15], or implicitly by neural networks or other mechanisms <ref> [3, 12, 14, 19,21, 23,25, 26] </ref>. The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. A few authors have taken the approach of extracting features and applying either manually or automatically generated rules for evaluating these features [7, 11].
Reference: [13] <author> Baback Moghaddam and Alex Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In Fifth International Conference on Computer Vision, </booktitle> <pages> pages 786793, </pages> <address> Cambridge, Mas-sachusetts, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher> <editor> Rowley, Baluja, and Kanade: </editor> <title> Neural Network-Based Face Detection (PAMI, </title> <month> January </month> <year> 1998) </year> <month> 14 </month>
Reference-contexts: The algorithms and training methods are designed to be general, with little customization for faces. Many face detection researchers have used the idea that facial images can be characterized directly in terms of pixel intensities. These images can be characterized by probabilistic models of the set of face images <ref> [4, 13, 15] </ref>, or implicitly by neural networks or other mechanisms [3, 12, 14, 19,21, 23,25, 26]. The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. <p> As with Sung and Poggio's work, Moghoddam and Pentland's approach uses a two component distance measure, but combines the two distances in a principled way based on the assumption that the distribution of each cluster is Gaussian <ref> [13] </ref>. The clusters are used together as a multi-modal Gaussian distribution, giving a probability distribution for all face images. Faces are detected by measuring how well each window of the input image fits the distribution, and setting a threshold.
Reference: [14] <author> Edgar Osuna, Robert Freund, and Federico Girosi. </author> <title> Training support vector machines: an application to face detection. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 130136, </pages> <year> 1997. </year>
Reference-contexts: Many face detection researchers have used the idea that facial images can be characterized directly in terms of pixel intensities. These images can be characterized by probabilistic models of the set of face images [4, 13, 15], or implicitly by neural networks or other mechanisms <ref> [3, 12, 14, 19,21, 23,25, 26] </ref>. The parameters for these models are adjusted either automatically from example images (as in our work) or by hand. A few authors have taken the approach of extracting features and applying either manually or automatically generated rules for evaluating these features [7, 11]. <p> Table 3 shows that for equal numbers of false detections, we can achieve slightly higher detection rates. Osuna, Freund, and Girosi <ref> [14] </ref> have recently investigated face detection using a framework similar to that used in [21] and in our own work. However, they use a support vector machine to classify images, rather than a clustering-based method or a neural network.
Reference: [15] <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 8491, </pages> <year> 1994. </year>
Reference-contexts: The algorithms and training methods are designed to be general, with little customization for faces. Many face detection researchers have used the idea that facial images can be characterized directly in terms of pixel intensities. These images can be characterized by probabilistic models of the set of face images <ref> [4, 13, 15] </ref>, or implicitly by neural networks or other mechanisms [3, 12, 14, 19,21, 23,25, 26]. The parameters for these models are adjusted either automatically from example images (as in our work) or by hand.
Reference: [16] <author> P. Jonathon Phillips, Hyeonjoon Moon, Patrick Rauss, and Syed A. Rizvi. </author> <title> The FERET evaluation methodology for face-recognition algorithms. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 137143, </pages> <year> 1997. </year>
Reference-contexts: The images have a wide variety of complex backgrounds, and are useful in measuring the false alarm rate of the system. Test Set 2 is a subset of the FERET database <ref> [16, 17] </ref>. Each image contains one face, and has (in most cases) a uniform background and good lighting. There are a wide variety of faces in the database, which are taken at a variety of angles. <p> The recognition error rate, averaged over all the tested systems, for frontal photographs taken in the same sitting is less than 2% (see the rank 50 results in Figure 4 of <ref> [16] </ref>). This means that the number of images containing detection errors, either false alarms or missing faces, was less than 2% of all images. Anecdotally, the actual error rate is significantly less than 2%.
Reference: [17] <author> P. Jonathon Phillips, Patrick J. Rauss, and Sandor Z. Der. </author> <title> FERET (face recognition technology) recogition algorithm development and test results. </title> <type> Technical Report ARL-TR-995, </type> <institution> Army Research Laboratory, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: The images have a wide variety of complex backgrounds, and are useful in measuring the false alarm rate of the system. Test Set 2 is a subset of the FERET database <ref> [16, 17] </ref>. Each image contains one face, and has (in most cases) a uniform background and good lighting. There are a wide variety of faces in the database, which are taken at a variety of angles. <p> Wood-ward Yang (at Harvard) for providing a series of test images and a mug-shot database for training, respectively. Michael Smith (at CMU) provided some digitized television images for testing purposes. Test Set 2 consists of facial images from the FERET database, collected under the ARPA/ARL FERET program <ref> [17] </ref>. We also thank Eugene Fink, Xue-Mei Wang, Hao-Chi Wong, Tim Rowley, Kaari Flagstad, and the reviewers for their comments on earlier versions of this paper.
Reference: [18] <author> Shin'ichi Satoh and Takeo Kanade. Name-it: </author> <title> Association of face and name in video. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 368373, </pages> <year> 1997. </year>
Reference-contexts: However, automatic high-level classification of the information content is very limited; this is a bottleneck that prevents media technology from reaching its full potential. Systems utilizing the detector described above allow a user to make requests of the form Show me the people who appear in this video <ref> [18, 20] </ref> or Which images on the World Wide Web contain faces? [6] and to have their queries answered automatically. Acknowledgements The authors would like to thank Kah-Kay Sung and Dr. Tomaso Poggio (at MIT) and Dr.
Reference: [19] <author> Pawan Sinha. </author> <title> Object recognition via image invariants: A case study. </title> <journal> Investigative Opthal-mology and Visual Science, </journal> <volume> 35(4), </volume> <month> march </month> <year> 1994. </year>
Reference: [20] <author> Michael A. Smith and Takeo Kanade. </author> <title> Video skimming and characterization through the combination of image and language understanding techniques. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 775781, </pages> <year> 1997. </year>
Reference-contexts: However, automatic high-level classification of the information content is very limited; this is a bottleneck that prevents media technology from reaching its full potential. Systems utilizing the detector described above allow a user to make requests of the form Show me the people who appear in this video <ref> [18, 20] </ref> or Which images on the World Wide Web contain faces? [6] and to have their queries answered automatically. Acknowledgements The authors would like to thank Kah-Kay Sung and Dr. Tomaso Poggio (at MIT) and Dr.
Reference: [21] <author> Kah-Kay Sung. </author> <title> Learning and Example Selection for Object nad Pattern Detection. </title> <type> PhD thesis, </type> <institution> MIT AI Lab, </institution> <month> January </month> <year> 1996. </year> <note> Available as AI Technical Report 1572. </note>
Reference-contexts: We avoid the problem of using a huge training set for nonfaces by selectively adding images to the Rowley, Baluja, and Kanade: Neural Network-Based Face Detection (PAMI, January 1998) 2 training set as training progresses <ref> [21] </ref>. This bootstrap method reduces the size of the training set needed. The use of arbitration between multiple networks and heuristics to clean up the results significantly improves the accuracy of the detector. <p> For the work presented here, we apply the filter at every pixel position in the image, and scale the image down by a factor of 1.2 for each step in the pyramid. The filtering algorithm is shown in Fig. 1. First, a preprocessing step, adapted from <ref> [21] </ref>, is applied to a window of the image. The window is then passed through a neural network, which decides whether the window contains a face. The preprocessing first attempts to equalize the intensity values in across the window. <p> However, collecting a representative set of nonfaces Rowley, Baluja, and Kanade: Neural Network-Based Face Detection (PAMI, January 1998) 4 is difficult. Instead of collecting the images before training is started, the images are collected during training, in the following manner, adapted from <ref> [21] </ref>: 1. Create an initial set of nonface images by generating 1000 random images. Apply the pre processing steps to each of these images. 2. Train a neural network to produce an output of 1 for the face examples, and -1 for the nonface examples. <p> Test Set 1 consists of a total of 130 images collected at CMU, including images from the World Wide Web, scanned from photographs and newspaper pictures, and digitized from broadcast television 3 . It also includes 23 images used in <ref> [21] </ref> to measure the accuracy of their system. The images contain a total of 507 frontal faces, and require the networks to examine 83,099,211 20x20 pixel windows. The images have a wide variety of complex backgrounds, and are useful in measuring the false alarm rate of the system. <p> together, have proven useful in building an almost real-time version of the system suitable for demonstration purposes, which can process a 320x240 image in 2 to 4 seconds, depending on the image complexity. 5 Comparison to Other Systems Sung and Poggio developed a face detection system based on clustering techniques <ref> [21] </ref>. Their system, like ours, passes a small window over all portions of the image, and determines whether a face exists in each window. Their system uses a supervised clustering method with six face and six nonface clusters. <p> Table 3 shows the accuracy of their system on a set of 23 images (a portion of Test Set 1), along with the results of our system using the heuristics employed by Systems 10, 11, and 12 in Table 1. In <ref> [21] </ref>, 149 faces were labelled in this test set, while we labelled 155. Some of these faces are difficult for either system to detect. <p> Table 3 shows that for equal numbers of false detections, we can achieve slightly higher detection rates. Osuna, Freund, and Girosi [14] have recently investigated face detection using a framework similar to that used in <ref> [21] </ref> and in our own work. However, they use a support vector machine to classify images, rather than a clustering-based method or a neural network. The support vector machine has a number of interesting properties, including the fact that it makes the boundary between face and nonface images more explicit. <p> The support vector machine has a number of interesting properties, including the fact that it makes the boundary between face and nonface images more explicit. The result of their system on the same 23 images used in <ref> [21] </ref> is given in Table 3; the accuracy is currently slightly poorer than the other two systems for this small test set. <p> It may be possible to make the detectors more robust using the bootstrap training technique described here and in <ref> [21] </ref>. In recent work, Colmenarez and Huang presented a statistically based method for face detection [4]. Their system builds probabilistic models of the sets of faces and nonfaces, and compares how well each input window compares with these two categories.
Reference: [22] <author> Tazio Umezaki. </author> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: Recall that the amount of position invariance in the pattern recognition component of our system determines how many windows must be processed. In the related task of license plate detection, this was exploited to decrease the number of windows that must be processed <ref> [22] </ref>. The idea was to make the neural network be invariant to translations of about 25% of the size of the license Rowley, Baluja, and Kanade: Neural Network-Based Face Detection (PAMI, January 1998) 9 plate.
Reference: [23] <author> R. Vaillant, C. Monrocq, and Y. Le Cun. </author> <title> Original approach for the localisation of objects in images. </title> <booktitle> IEE Proceedings on Vision, Image, and Signal Processing, </booktitle> <volume> 141(4), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: The candidate verification process used to speed up our system, described in Section 4, is similar to the detection technique presented in <ref> [23] </ref>. In that work, two networks were used. The first network has a single output, and like our system it is trained to produce a positive value for centered faces, and a negative value for nonfaces.
Reference: [24] <author> Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pages 393404, </pages> <year> 1989. </year>
Reference-contexts: For the experiments which are described later, we use networks with two and three sets of these hidden units. Similar input connection patterns are commonly used in speech and character recognition tasks <ref> [10, 24] </ref>. The network has a single, real-valued output, which indicates whether or not the window contains a face. Examples of output from a single network are shown in Fig. 3.
Reference: [25] <author> Gaungzheng Yang and Thomas S. Huang. </author> <title> Human face detection in a complex background. </title> <journal> Pattern Recognition, </journal> <volume> 27(1):5363, </volume> <year> 1994. </year>

References-found: 25

