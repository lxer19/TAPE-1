URL: ftp://ilk.kub.nl/pub/papers/ecml98.ps.gz
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Phone: 2  3  
Title: Interpretable Neural Networks with BP-SOM  
Author: Ton Weijters Antal van den Bosch and Jaap van den Herik 
Address: Netherlands  Netherlands  Netherlands  
Affiliation: 1 Information Technology, Eindhoven University of Technology, The  ILK Computational Linguistics, Tilburg University, The  Department of Computer Science, Universiteit Maastricht, The  
Abstract: Interpretation of models induced by artificial neural networks is often a difficult task. In this paper we focus on a relatively novel neural network architecture and learning algorithm, bp-som, that offers possibilities to overcome this difficulty. It is shown that networks trained with bp-som show interesting regularities, in that hidden-unit activations become restricted to discrete values, and that the som part can be exploited for automatic rule extraction.
Abstract-found: 1
Intro-found: 1
Reference: [ADT95] <author> Andrews, R., Diederich, J., and Tickle, A. B. </author> <year> (1995). </year> <title> A Survey And Critique of Techniques for Extracting Rules from Trained Artificial Neural Networks. Knowledge Based System, </title> <booktitle> 8:6, </booktitle> <pages> 373-389. </pages>
Reference: [Hin86] <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 1-12. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: In earlier publications [Wei95, WVV97, WVVP97] experimental results were reported in which the generalization performances of bp-som were compared to two other learning algorithms for multi-layer feed-forward networks (mfns), viz. bp and bpwd (bp augmented with weight decay <ref> [Hin86] </ref>). <p> Classification and reliability information from the soms is included when updating the connection weights of the mfn (for more details, cf. [Wei95,WVV97, WVVP97]). 3 Knowledge representations in BP-SOM In this section, knowledge representations of bp-som are compared to two related learning algorithms for mfns, viz. bp [RWH86] and bpwd <ref> [Hin86] </ref> by training the three algorithms on the parity-12 classification task, i.e., to determine whether a bit string of 0's and 1's of length 12 contains an even number of 1's.
Reference: [Koh89] <author> Kohonen, T. </author> <year> (1989). </year> <title> Self-organisation and Associative Memory. </title> <publisher> Berlin: Springer Verlag. </publisher>
Reference-contexts: For details we refer to [Wei95, WVV97, WVVP97]. The aim of the bp-som learning algorithm is to establish a cooperation between bp-learning and som-learning in order to find adequate hidden-layer representations for learning classification tasks. To achieve this aim, the traditional mfn architecture [RHW86] is com-bined with soms <ref> [Koh89] </ref>: each hidden layer of the mfn is associated with one som (see Figure 1). During training of the weights in the mfn, the corresponding som is trained on the hidden-unit activation patterns. Fig. 1. An example bp-som network. <p> The bp learning rate is set to 0.15 and the momentum to 0.4. In all soms a decreasing interaction strength from 0.15 to 0.05, and a decreasing neighbourhood-updating context from a square with maximally 9 units to only 1 unit (the winner) is used <ref> [Koh89] </ref>. The hidden layer of the mfn in all three algorithms contains 20 hidden units (the optimal number for a bp trained network), and the som in bp-som con tained 7 fi 7 elements. The algorithms are run with 10 different random weight initialisations.
Reference: [RHW86] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1: </volume> <pages> Foundations (pp. 318-362). </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: For details we refer to [Wei95, WVV97, WVVP97]. The aim of the bp-som learning algorithm is to establish a cooperation between bp-learning and som-learning in order to find adequate hidden-layer representations for learning classification tasks. To achieve this aim, the traditional mfn architecture <ref> [RHW86] </ref> is com-bined with soms [Koh89]: each hidden layer of the mfn is associated with one som (see Figure 1). During training of the weights in the mfn, the corresponding som is trained on the hidden-unit activation patterns. Fig. 1. An example bp-som network. <p> We can visually distinguish areas in the som: areas containing elements labelled with class A and class B, and areas containing unlabelled elements (no winning class could be found). The self-organisation of the som is used as an addition to the standard bp learning rule <ref> [RHW86] </ref>.
Reference: [Thr91] <author> Thrun, S. B., et. </author> <title> al (1991). The MONK's Problems: a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: The same oscillating phenomenon is present in the activations of the other seven hidden units. 4 Automatic rule extraction based on SOM clustering In this section we focus on automatic rule extraction based on the clustering of the som of a bp-som network trained on the monks-1 tasks <ref> [Thr91] </ref>, and give an interpretation of BP-SOM's method of performing this task. Instances are char-acterised by six attributes a1 : : : a6 which have two, three, or four discrete possible values. An instance is mapped to a class `1' if and only if (a1 = a2)or (a5 = 1).
Reference: [Wei95] <author> Weijters, A. </author> <year> (1995). </year> <title> The bp-som architecture and learning rule. </title> <journal> Neural Processing Letters, </journal> <volume> 2, </volume> <pages> 13-16. </pages>
Reference-contexts: In earlier publications <ref> [Wei95, WVV97, WVVP97] </ref> experimental results were reported in which the generalization performances of bp-som were compared to two other learning algorithms for multi-layer feed-forward networks (mfns), viz. bp and bpwd (bp augmented with weight decay [Hin86]). <p> Furthermore, we illustrate how dividing the learning material into a limited number of homogeneous subsets can be exploited for automatic rule extraction. 2 BP-SOM Below we give a brief characterisation of the functioning of bp-som. For details we refer to <ref> [Wei95, WVV97, WVVP97] </ref>. The aim of the bp-som learning algorithm is to establish a cooperation between bp-learning and som-learning in order to find adequate hidden-layer representations for learning classification tasks.
Reference: [WVV97] <author> Weijters, A., Van den Bosch, A., Van den Herik, H. J. </author> <year> (1997). </year> <title> Behavioural Aspects of Combining Backpropagation Learning and Self-organizing Maps. </title> <journal> Connection Science, </journal> <volume> 9, </volume> <pages> 235-252. </pages>
Reference-contexts: In earlier publications <ref> [Wei95, WVV97, WVVP97] </ref> experimental results were reported in which the generalization performances of bp-som were compared to two other learning algorithms for multi-layer feed-forward networks (mfns), viz. bp and bpwd (bp augmented with weight decay [Hin86]). <p> Furthermore, we illustrate how dividing the learning material into a limited number of homogeneous subsets can be exploited for automatic rule extraction. 2 BP-SOM Below we give a brief characterisation of the functioning of bp-som. For details we refer to <ref> [Wei95, WVV97, WVVP97] </ref>. The aim of the bp-som learning algorithm is to establish a cooperation between bp-learning and som-learning in order to find adequate hidden-layer representations for learning classification tasks.
Reference: [WVVP97] <author> Weijters, A., Van den Herik, H. J., Van den Bosch, A., and Postma, E. O. </author> <year> (1997). </year> <title> Avoiding overfitting with BP-SOM. </title> <booktitle> Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> IJCAI'97, San Fran-cisco, </address> <publisher> Morgan Kaufmann, </publisher> <month> 1140-1145. </month> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: In earlier publications <ref> [Wei95, WVV97, WVVP97] </ref> experimental results were reported in which the generalization performances of bp-som were compared to two other learning algorithms for multi-layer feed-forward networks (mfns), viz. bp and bpwd (bp augmented with weight decay [Hin86]). <p> Furthermore, we illustrate how dividing the learning material into a limited number of homogeneous subsets can be exploited for automatic rule extraction. 2 BP-SOM Below we give a brief characterisation of the functioning of bp-som. For details we refer to <ref> [Wei95, WVV97, WVVP97] </ref>. The aim of the bp-som learning algorithm is to establish a cooperation between bp-learning and som-learning in order to find adequate hidden-layer representations for learning classification tasks. <p> The self-organisation of the som is used as an addition to the standard bp learning rule [RHW86]. Classification and reliability information from the soms is included when updating the connection weights of the mfn (for more details, cf. <ref> [Wei95,WVV97, WVVP97] </ref>). 3 Knowledge representations in BP-SOM In this section, knowledge representations of bp-som are compared to two related learning algorithms for mfns, viz. bp [RWH86] and bpwd [Hin86] by training the three algorithms on the parity-12 classification task, i.e., to determine whether a bit string of 0's and 1's of
References-found: 8

