URL: http://www.cs.umd.edu/~tseng/papers/ijpp96.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: Memory Referencing Behavior in Compiler-Parallelized Applications  
Author: Evan Torrie, Margaret Martonosi, Mary W. Hall, and Chau-Wen Tseng 
Abstract: Compiler-parallelized applications are increasing in importance as moderate-scale multiprocessors become common. This paper evaluates how features of advanced memory systems (e.g., longer cache lines) impact memory system behavior for applications amenable to compiler parallelization. Using full-sized input data sets and applications taken from the SPEC, NAS, PERFECT, and RICEPS benchmark suites, we measure statistics such as speedups, memory costs, causes of cache misses, cache line utilization, and data traffic. This exploration allows us to draw several conclusions. First, we find that larger granularity parallelism often correlates with good memory system behavior, good overall performance, and high speedup in these applications. Second, we show that when long (512 byte) cache lines are used, many of these applications suffer from false sharing and low cache line utilization. Third, we identify some of the common artifacts in compiler-parallelized codes that can lead to false sharing or other types of poor memory system performance, and we suggest methods for improving them. Overall, this study offers both an important snapshot of the behavior of applications compiled by state-of-the-art compilers, as well as an increased understanding of the interplay between cache line size, program granularity, and memory performance in moderate-scale multiprocessors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel 17 machines. </title> <booktitle> In Proc. SIGPLAN '93 Conf. on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: There is much active research on compiler techniques to improve memory performance. In [4], the authors evaluated a suite of compiler techniques for improving data referencing locality in uniprocessor code. Heuristics are being developed to reduce true sharing by improved co-location of data and computation <ref> [1, 3] </ref> and eliminate false sharing by better compiler management of large coherence units [10]. 9 Conclusions In this paper, we demonstrate that good memory system behavior is vital to achieving reasonable speedups on moderate-scale multiprocessors.
Reference: [2] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Blume and Eigenmann <ref> [2] </ref> analyzed the performance of commercial parallelizing compilers on the PERFECT benchmarks, concluding that they detected only limited amounts of parallelism. The SUIF compiler incorporates many of the analyses they deemed vital; as a result, it enjoys much better success in extracting parallelism.
Reference: [3] <author> W. Bolosky and M. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <address> San Diego, CA, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: They also demonstrate that the overall miss rate in a multiprocessor can increase as the cache line size increases, whereas it tends to go down in uniprocessors. Bolosky and Scott <ref> [3] </ref> developed the cost component method to measure false sharing and applied it to four computation kernels. More recently, Dubois et al. [6] introduced a definition of false sharing and used it to measure four hand-parallelized applications. We use their definition for our study. <p> There is much active research on compiler techniques to improve memory performance. In [4], the authors evaluated a suite of compiler techniques for improving data referencing locality in uniprocessor code. Heuristics are being developed to reduce true sharing by improved co-location of data and computation <ref> [1, 3] </ref> and eliminate false sharing by better compiler management of large coherence units [10]. 9 Conclusions In this paper, we demonstrate that good memory system behavior is vital to achieving reasonable speedups on moderate-scale multiprocessors.
Reference: [4] <author> S. Carr, K. S. McKinley, and C.-W. Tseng. </author> <title> Compiler Optimizations for Improving Data Locality. </title> <booktitle> In Proc. Sixth Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 252-262, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: There is much active research on compiler techniques to improve memory performance. In <ref> [4] </ref>, the authors evaluated a suite of compiler techniques for improving data referencing locality in uniprocessor code.
Reference: [5] <author> H. Davis, S. R. Goldschmidt, and J. Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: To compensate for this, we reset statistics after initialization and cold start to avoid skewing results. 4 Experimental Methodology For these experiments, we used an extended version of the MemSpy simulator [19, 20] and the TangoLite simulation and tracing system <ref> [5, 9] </ref>. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. To fully capture potential sharing between processors, we interleave threads after each memory reference. MemSpy supports monitoring cold, replacement, and invalidation cache misses on a procedure and data item basis.
Reference: [6] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, et al. </author> <title> The Detection and Elimination of Useless Misses in Multiprocessors. </title> <booktitle> In Proc. 20th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: MemSpy supports monitoring cold, replacement, and invalidation cache misses on a procedure and data item basis. For our study we have further broken down the category of invalidation misses into true sharing and false sharing misses using the scheme described by Dubois et al. <ref> [6] </ref>. In this definition, a true sharing miss occurs if: during a lifetime of the line in the cache, the processor accesses a word written by a different processor since the last true, cold or replacement miss by the same processor to the same cache line. <p> Bolosky and Scott [3] developed the cost component method to measure false sharing and applied it to four computation kernels. More recently, Dubois et al. <ref> [6] </ref> introduced a definition of false sharing and used it to measure four hand-parallelized applications. We use their definition for our study. Torrellas et al. [24] measured false and true sharing and the number of bytes used per cache line.
Reference: [7] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proc. 1991 Int'l Conf. on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: They find poor spatial locality has a greater impact than false sharing in determining the overall miss rate of their applications. In comparison, the SUIF applications in this study have excellent spatial locality and are limited mostly by false sharing. Both Torrellas et al. [24] and Eggers and Jeremiassen <ref> [7] </ref> suggest program transformations to eliminate false sharing in hand- parallelized programs.
Reference: [8] <author> S. J. Eggers and R. H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Third Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 257-270, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: In a realm where parallel computers are widely available, compiler-parallelized applications are likely to be the workload of choice for most users. However, relatively little is known about their characteristics. In particular, memory system behavior has been shown to have a significant impact on the performance of scalable multiprocessors <ref> [8, 11, 17] </ref>. Because of the increasing disparity between Evan Torrie is with the Computer Systems Lab at Stanford University (torrie@cs.stanford.edu). Margaret Martonosi is with the Dept. of Electrical Engineering at Princeton University ( martonosi@princeton.edu). Mary Hall is with the Dept. of Computer Science at California Institute of Technology (mary@cs.caltech.edu). <p> Second, applications may exhibit less spatial locality when executing in parallel, depending on how computation is partitioned. Finally, longer cache lines may lead to increased data traffic, causing memory contention. Previous research has shown false sharing to be a problem for hand-parallelized applications <ref> [8] </ref>. Our study attempts to evaluate the effect of longer cache lines on applications amenable to compiler parallelization. <p> While our work draws on a significant body of related work in understanding multiprocessor memory behavior, we outline below the most directly relevant studies. Eggers and Katz <ref> [8] </ref> did important early work characterizing application caching behavior of hand-parallelized programs in bus-based multiprocessors. For their applications, they show that the majority of cache misses in a bus-based multiprocessor are due to sharing misses.
Reference: [9] <author> S. R. Goldschmidt. </author> <title> Simulation of Multiprocessors, Speed and Accuracy. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: To compensate for this, we reset statistics after initialization and cold start to avoid skewing results. 4 Experimental Methodology For these experiments, we used an extended version of the MemSpy simulator [19, 20] and the TangoLite simulation and tracing system <ref> [5, 9] </ref>. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. To fully capture potential sharing between processors, we interleave threads after each memory reference. MemSpy supports monitoring cold, replacement, and invalidation cache misses on a procedure and data item basis.
Reference: [10] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proc. 1993 ACM Int'l. Conf. on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In [4], the authors evaluated a suite of compiler techniques for improving data referencing locality in uniprocessor code. Heuristics are being developed to reduce true sharing by improved co-location of data and computation [1, 3] and eliminate false sharing by better compiler management of large coherence units <ref> [10] </ref>. 9 Conclusions In this paper, we demonstrate that good memory system behavior is vital to achieving reasonable speedups on moderate-scale multiprocessors.
Reference: [11] <author> A. Gupta and W.-D. Weber. </author> <title> Cache Invalidation Patterns in Shared-Memory Multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In a realm where parallel computers are widely available, compiler-parallelized applications are likely to be the workload of choice for most users. However, relatively little is known about their characteristics. In particular, memory system behavior has been shown to have a significant impact on the performance of scalable multiprocessors <ref> [8, 11, 17] </ref>. Because of the increasing disparity between Evan Torrie is with the Computer Systems Lab at Stanford University (torrie@cs.stanford.edu). Margaret Martonosi is with the Dept. of Electrical Engineering at Princeton University ( martonosi@princeton.edu). Mary Hall is with the Dept. of Computer Science at California Institute of Technology (mary@cs.caltech.edu).
Reference: [12] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: What our study shows is that finding coarse-grain parallelism can also have a very beneficial effect on memory system behavior. The SUIF compiler employs two techniques that enable detection of more outer parallel loops than current commercial compilers <ref> [12] </ref>. First, array privatization locates arrays used as temporary storage within a loop. By creating private copies of the array for each parallel process, storage-related dependences associated with these arrays are eliminated.
Reference: [13] <author> M. W. Hall, B. R. Murphy, and S. P. Amarasinghe. </author> <title> Interprocedural parallelization analysis: A case study. </title> <booktitle> In Proc. Seventh SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: At 16 processors, speedups for SUIF applications range from 1.8 to 16.0 with an average speedup of 8.4. In comparison, SPLASH applications maintained average speedups of 11.4 on 16 processors. (These simulated speedups correspond well with actual speedups observed for these programs on the DASH and SGI Challenge multiprocessors <ref> [13, 25] </ref>.) These performance results suggest that memory system and synchronization costs are causing a significant drop in performance particularly as the number of processors increases. To quantify the effect of the memory system, we measured the miss cycles per instruction (MCPI) directly. <p> The combination of these techniques enables the compiler to parallelize outer loops containing over a thousand lines of code in some cases. A more detailed discussion of the implementation of these techniques can be found in <ref> [13] </ref>. To illustrate how these techniques can impact memory behavior, consider the performance of two SUIF applications, appbt and flo52.
Reference: [14] <author> T. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Both Torrellas et al. [24] and Eggers and Jeremiassen [7] suggest program transformations to eliminate false sharing in hand- parallelized programs. The latter have implemented their transformations in a compiler, and used them to eliminate false sharing in the SPLASH benchmarks by padding lock variables <ref> [14] </ref>. (In our SPLASH programs lock variables have also been padded to eliminate false sharing.) Only a handful of researchers have looked at the behavior of compiler-parallelized applications.
Reference: [15] <author> J. Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. of the 21st Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The bulk of our results are presented for an advanced memory system that more closely resembles an aggressive next-generation multiprocessor. It has a directory-based cache-coherent non-uniform memory access memory system with a high speed interconnect <ref> [15, 17] </ref>. We choose a 200 MHz processor, a 100 MHz 256-bit local memory bus, and a 200 MHz 16-bit wide mesh network interconnect. Each processor has a single level LRU cache whose size, associativity, and line size we vary.
Reference: [16] <author> R. L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: In uniprocessors, cache miss rates behave predictably with increasing line size, decreasing at first, eventually increasing as cache conflicts start to dominate. Unfortunately, miss rates are not so predictable for multiprocessor caches <ref> [16, 24] </ref>. Longer 6 cache lines may prove problematic for parallel codes for several reasons. First, false sharing may cause cache misses on logically separate data placed on the same cache line. Second, applications may exhibit less spatial locality when executing in parallel, depending on how computation is partitioned.
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. 17th Annual Int'l Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: In a realm where parallel computers are widely available, compiler-parallelized applications are likely to be the workload of choice for most users. However, relatively little is known about their characteristics. In particular, memory system behavior has been shown to have a significant impact on the performance of scalable multiprocessors <ref> [8, 11, 17] </ref>. Because of the increasing disparity between Evan Torrie is with the Computer Systems Lab at Stanford University (torrie@cs.stanford.edu). Margaret Martonosi is with the Dept. of Electrical Engineering at Princeton University ( martonosi@princeton.edu). Mary Hall is with the Dept. of Computer Science at California Institute of Technology (mary@cs.caltech.edu). <p> SUIF programs rely on a run-time system built from ANL macros for thread creation, barriers, and locks. The run-time 2 system has been tuned to eliminate false sharing and minimize true sharing; it has been ported to the Stanford DASH <ref> [17] </ref>, SGI Challenge, and KSR-1 multiprocessors. References Par. Par. Program Suite Description Input Data Simulated Covg. <p> The bulk of our results are presented for an advanced memory system that more closely resembles an aggressive next-generation multiprocessor. It has a directory-based cache-coherent non-uniform memory access memory system with a high speed interconnect <ref> [15, 17] </ref>. We choose a 200 MHz processor, a 100 MHz 256-bit local memory bus, and a 200 MHz 16-bit wide mesh network interconnect. Each processor has a single level LRU cache whose size, associativity, and line size we vary.
Reference: [18] <author> D. J. Lilja. </author> <title> The Impact of Parallel Loop Scheduling Strategies on Prefetching in a Shared-Memory Multiprocessor. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 5(6) </volume> <pages> 573-584, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: They determined that parallelism overhead consumed 10-25% of program execution time and memory contention overhead was over 10%. Our study focused on a more advanced memory system and compiler; we also determine causes of poor memory behavior. Lilja <ref> [18] </ref> examines the impact 16 of prefetching in conjunction with loop scheduling strategies that schedule blocks of consecutive iterations to execute on each processor. Our experiments have led to several observations about the behavior of compiler parallelized codes.
Reference: [19] <author> M. Martonosi, A. Gupta, and T. Anderson. MemSpy: </author> <title> Analyzing Memory System Bottlenecks in Programs. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-12, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Where necessary we have reduced the number of time steps in each application to limit simulation time. To compensate for this, we reset statistics after initialization and cold start to avoid skewing results. 4 Experimental Methodology For these experiments, we used an extended version of the MemSpy simulator <ref> [19, 20] </ref> and the TangoLite simulation and tracing system [5, 9]. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. To fully capture potential sharing between processors, we interleave threads after each memory reference.
Reference: [20] <author> M. R. Martonosi. </author> <title> Analyzing and Tuning Memory Performance in Sequential and Parallel Programs. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> Dec. </month> <year> 1993. </year> <note> Also Stanford CSL Technical Report CSL-TR-94-602. 18 </note>
Reference-contexts: Where necessary we have reduced the number of time steps in each application to limit simulation time. To compensate for this, we reset statistics after initialization and cold start to avoid skewing results. 4 Experimental Methodology For these experiments, we used an extended version of the MemSpy simulator <ref> [19, 20] </ref> and the TangoLite simulation and tracing system [5, 9]. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. To fully capture potential sharing between processors, we interleave threads after each memory reference.
Reference: [21] <author> C. Natarajan, S. Sharma, and R. Iyer. </author> <title> Measurement-based characterization of global memory and network contention, operating system and parallelization overheads: Case study on a shared-memory multiprocessor. </title> <booktitle> In Proc. of the 21st Int'l Symp. on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The SUIF compiler incorporates many of the analyses they deemed vital; as a result, it enjoys much better success in extracting parallelism. More recently, Natarajan et al. <ref> [21] </ref> measured operating system, parallelism, and memory contention overhead for five PERFECT applications on the Cedar multiprocessor. They determined that parallelism overhead consumed 10-25% of program execution time and memory contention overhead was over 10%.
Reference: [22] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. 21st Annual Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: This observation is important, because the bimodal behavior suggests that special optimizations for each type of behavior may be possible. In systems allowing flexible protocols (such as Tempest <ref> [22] </ref>), one could specialize handling for each type of data. Essentially, the protocol could implement smaller coherence units for the previously-invalidated data, while maintaining coherence units equal to the cache line size for the previously replaced data.
Reference: [23] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for SharedMemory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Chau- Wen Tseng is with the Dept. of Computer Science at University of Maryland (tseng@cs.umd.edu). 1 processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs <ref> [23, 26] </ref>. In this paper, we examine the memory system behavior of a new class of applications|those amenable to compiler parallelization. Our goal is to evaluate how these programs are impacted by advanced memory systems. <p> We evaluate these benchmarks on full-sized data sets, measuring the impact of memory behavior on the programs' speedups. For comparison, we also present statistics on a selection of hand-parallelized applications from the SPLASH benchmark suite <ref> [23] </ref>. * Based on these measurements, we show that the granularity of application parallelism is an important determinant of application memory behavior, and ultimately of application performance. <p> As we will show in Section 6.4, for many applications the compiler's ability to exploit larger granularities of parallelism is correlated to good memory performance. In order to provide a basis for comparison, we also include in our study hand-parallelized programs from the SPLASH benchmarks <ref> [23, 26] </ref>. Their granularity was not computed because they use different synchronization mechanisms. For brevity, in the remainder of the paper we shall refer to these two groups of programs as the SUIF and SPLASH applications.
Reference: [24] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 43(6) </volume> <pages> 651-63, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In uniprocessors, cache miss rates behave predictably with increasing line size, decreasing at first, eventually increasing as cache conflicts start to dominate. Unfortunately, miss rates are not so predictable for multiprocessor caches <ref> [16, 24] </ref>. Longer 6 cache lines may prove problematic for parallel codes for several reasons. First, false sharing may cause cache misses on logically separate data placed on the same cache line. Second, applications may exhibit less spatial locality when executing in parallel, depending on how computation is partitioned. <p> In the following section, we investigate this observation further. 6.1.3 Cache Line Utilization and Data Traffic Torrellas et al. have looked at the number of words actually touched in a cache line as a measure of the spatial locality exploited by long cache lines <ref> [24] </ref>. <p> More recently, Dubois et al. [6] introduced a definition of false sharing and used it to measure four hand-parallelized applications. We use their definition for our study. Torrellas et al. <ref> [24] </ref> measured false and true sharing and the number of bytes used per cache line. They find poor spatial locality has a greater impact than false sharing in determining the overall miss rate of their applications. <p> They find poor spatial locality has a greater impact than false sharing in determining the overall miss rate of their applications. In comparison, the SUIF applications in this study have excellent spatial locality and are limited mostly by false sharing. Both Torrellas et al. <ref> [24] </ref> and Eggers and Jeremiassen [7] suggest program transformations to eliminate false sharing in hand- parallelized programs.
Reference: [25] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: In the following sections, we describe the compiler, applications, and simulation methodology used in our experiments. We present our measurements for these programs, then examine the behavior of the compiler in greater detail before concluding. 2 The SUIF Parallelizing Compiler For our study we used the SUIF parallelizing compiler <ref> [25] </ref> to generate parallel versions of our applications. SUIF takes as input sequential Fortran or C programs, producing as output parallel C programs that execute according to a master-worker model. <p> At 16 processors, speedups for SUIF applications range from 1.8 to 16.0 with an average speedup of 8.4. In comparison, SPLASH applications maintained average speedups of 11.4 on 16 processors. (These simulated speedups correspond well with actual speedups observed for these programs on the DASH and SGI Challenge multiprocessors <ref> [13, 25] </ref>.) These performance results suggest that memory system and synchronization costs are causing a significant drop in performance particularly as the number of processors increases. To quantify the effect of the memory system, we measured the miss cycles per instruction (MCPI) directly.
Reference: [26] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> Methodological considerations and charac-terization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proc. of the 22st Int'l Symp. on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: Chau- Wen Tseng is with the Dept. of Computer Science at University of Maryland (tseng@cs.umd.edu). 1 processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs <ref> [23, 26] </ref>. In this paper, we examine the memory system behavior of a new class of applications|those amenable to compiler parallelization. Our goal is to evaluate how these programs are impacted by advanced memory systems. <p> As we will show in Section 6.4, for many applications the compiler's ability to exploit larger granularities of parallelism is correlated to good memory performance. In order to provide a basis for comparison, we also include in our study hand-parallelized programs from the SPLASH benchmarks <ref> [23, 26] </ref>. Their granularity was not computed because they use different synchronization mechanisms. For brevity, in the remainder of the paper we shall refer to these two groups of programs as the SUIF and SPLASH applications.
References-found: 26

