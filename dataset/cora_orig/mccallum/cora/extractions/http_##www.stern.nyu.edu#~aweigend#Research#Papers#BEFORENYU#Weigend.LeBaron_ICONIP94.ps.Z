URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Weigend.LeBaron_ICONIP94.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: andreas@cs.colorado.edu  blebaron@facstaff.wisc.edu  
Title: Evaluating Neural Network Predictors by Bootstrapping  
Author: Andreas S. Weigend Blake LeBaron 
Address: Boulder, CO 80309-0430  1180 Observatory Drive Madison, WI 53713  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado  Department of Economics University of Wisconsin  
Note: CU-CS-725-94 (May 1994). Submitted to ICONIP'94-Seoul.  
Abstract: We present a new method, inspired by the bootstrap, whose goal it is to determine the quality and reliability of a neural network predictor. Our method leads to more robust forecasting along with a large amount of statistical information on forecast performance that we exploit. We exhibit the method in the context of multi-variate time series prediction on financial data from the New York Stock Exchange. It turns out that the variation due to different resamplings (i.e., splits between training, cross-validation, and test sets) is significantly larger than the variation due to different network conditions (such as architecture and initial weights). Furthermore, this method allows us to forecast a probability distribution, as opposed to the traditional case of just a single value at each time step. We demonstrate this on a strictly held-out test set that includes the 1987 stock market crash. We also compare the performance of the class of neural networks to identically bootstrapped linear models.
Abstract-found: 1
Intro-found: 1
Reference: [Bollerslev et al., 1990] <author> T. Bollerslev, R. Y. Chou, N. Jayaraman, and K. F. Kroner. </author> <title> ARCH modeling in finance: A review of the theory and empirical evidence. </title> <journal> Journal of Econometrics, </journal> <volume> 52(1) </volume> <pages> 5-60, </pages> <year> 1990. </year>
Reference-contexts: It is defined recursively as 2 t1 + (1 fi)r 2 t with fi = 0:9; similar to variance estimates from autoregressive conditional heteroskedastic models often used in financial time series and summarized in <ref> [Bollerslev et al., 1990] </ref>. 3 Empirical Results 3.1 Learning Curves and Overfitting and the normalized mean squared error.
Reference: [Efron and Tibshirani, 1993] <author> B. Efron and R. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <year> 1993. </year>
Reference-contexts: This allows us to get a better statistical picture of forecast performance, including the ability to estimate the effect of the randomness of initial conditions vs. the randomness of the splits of the data. All bootstrap methods that have been suggested so far (see <ref> [Efron and Tibshirani, 1993] </ref> for an introduction, and [Tibshirani, 1994] for the application of standard methods to neural networks in a cross-sectional context) do not split the data in order to evaluate the performance of the model.
Reference: [Gallant et al., 1993] <author> A. R. Gallant, P. E. Rossi, and G. Tauchen. </author> <title> Nonlinear dynamic structures. </title> <journal> Econometrica, </journal> <volume> 61 </volume> <pages> 871-908, </pages> <year> 1993. </year>
Reference-contexts: We use daily data from December 3rd, 1962, through September 16th, 1987, corresponding to 6230 days. 6 Volume movements are connected to stock return movements in interesting ways (e.g., <ref> [Gallant et al., 1993] </ref>, [Karpov, 1987], [LeBaron, 1992]). One of these features is that volume is related to stock price volatility, the absolute magnitude of daily price movements. Also, volume tends to be higher in rising markets. For these reasons we chose several lagged returns and volume variables as predictors.
Reference: [Karpov, 1987] <author> J. M. Karpov. </author> <title> The relation between price changes and trading volume: A survey. </title> <journal> Journal of Financial and Quantitative Analysis, </journal> <volume> 22 </volume> <pages> 109-126, </pages> <year> 1987. </year>
Reference-contexts: We use daily data from December 3rd, 1962, through September 16th, 1987, corresponding to 6230 days. 6 Volume movements are connected to stock return movements in interesting ways (e.g., [Gallant et al., 1993], <ref> [Karpov, 1987] </ref>, [LeBaron, 1992]). One of these features is that volume is related to stock price volatility, the absolute magnitude of daily price movements. Also, volume tends to be higher in rising markets. For these reasons we chose several lagged returns and volume variables as predictors.
Reference: [LeBaron, 1992] <author> B. LeBaron. </author> <title> Persistance of the Dow Jones index on rising volume. </title> <type> Technical report, </type> <institution> University of Wisconsin - Madison, Madison, Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: We use daily data from December 3rd, 1962, through September 16th, 1987, corresponding to 6230 days. 6 Volume movements are connected to stock return movements in interesting ways (e.g., [Gallant et al., 1993], [Karpov, 1987], <ref> [LeBaron, 1992] </ref>). One of these features is that volume is related to stock price volatility, the absolute magnitude of daily price movements. Also, volume tends to be higher in rising markets. For these reasons we chose several lagged returns and volume variables as predictors.
Reference: [Srivastava and Weigend, 1994] <author> A. N. Srivastava and A. S. Weigend. </author> <title> Computing the probability density in connectionist regression. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <address> Orlando, FL. ICNN, </address> <year> 1994. </year>
Reference-contexts: They use a maximum likelihood framework to train a network with two output units, one giving the prediction, the other one the error bar. A method to estimate the probability distribution of the next value|a rather data-hungry enterprise|is described by <ref> [Srivastava and Weigend, 1994] </ref>. They use "fractional binning" to estimate the conditional density of the stochastic process.
Reference: [Tibshirani, 1994] <author> R. Tibshirani. </author> <title> A comparison of some error estimates for neural network models. </title> <type> Technical report, </type> <institution> Dept. of Preventive Medicine and Biostatistics, University of Toronto, Toronto, </institution> <address> Ontario, </address> <year> 1994. </year>
Reference-contexts: All bootstrap methods that have been suggested so far (see [Efron and Tibshirani, 1993] for an introduction, and <ref> [Tibshirani, 1994] </ref> for the application of standard methods to neural networks in a cross-sectional context) do not split the data in order to evaluate the performance of the model.
Reference: [Weigend and Nix, 1994] <author> A. S. Weigend and D. A. Nix. </author> <title> Predictions with confidence intervals (local error bars). </title> <type> Technical report, </type> <institution> CU-CS-724-94, Computer Science Department, </institution> <year> 1994. </year>
Reference-contexts: This gives some feel for the reliability of our forecasting models, but it is not a true "confidence region" about where the process is going to go. A method to estimate true confidence regions that depend on the location in input space ("local error bars") is given by <ref> [Weigend and Nix, 1994] </ref>. They use a maximum likelihood framework to train a network with two output units, one giving the prediction, the other one the error bar. A method to estimate the probability distribution of the next value|a rather data-hungry enterprise|is described by [Srivastava and Weigend, 1994].
Reference: [Weigend et al., 1990] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart. </author> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209, </pages> <year> 1990. </year> <title> 8 Other sources, important in nonlinear dynamical systems with low noise, such as the divergence of nearby trajectories, are less important in the present case of noisy financial data. </title> <type> 6 </type>
Reference-contexts: the performance of a model would be to split the data into one training set (used for gradient descent in the parameters), one cross-validation set (used to determine the stopping point to avoid overfitting, and/or used to set additional parameters, such as for weight-elimination), and one or more test sets <ref> [Weigend et al., 1990] </ref>. In our experience, this approach can be very sensitive to the specific sample splitting.
References-found: 9

