URL: http://www.cs.gatech.edu/people/home/aman/papers/icpp95.ps.gz
Refering-URL: http://www.cs.gatech.edu/people/home/aman/beehive/index.html
Root-URL: 
Email: fgautam, aman, ramag@cc.gatech.edu  
Title: The Quest for a Zero Overhead Shared Memory Parallel Machine  
Author: Gautam Shah Aman Singla Umakishore Ramachandran 
Date: August 1995.  
Note: In Proceedings of the 1995 International Conference on Parallel Processing, pages I-194-201,  
Address: Atlanta, GA 30332-0280.  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: In this paper we present a new approach to benchmark the performance of shared memory systems. This approach focuses on recognizing how far off the performance of a given memory system is from a realistic ideal parallel machine. We define such a realistic machine model, called the z-machine, that accounts for the inherent communication costs in an application by tracking the data flow in the application. The z-machine is incorporated into an execution-driven simulation framework and is used as a reference for benchmarking different memory systems. The components of the overheads in these memory systems are identified and quantified for four applications. Using the z-machine performance as the standard to strive for we discuss the implications of the performance results and suggest architectural trends to pursue for realizing a zero overhead shared memory machine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Dahlgren aand M. Dubois and P. Stenstrom. </author> <title> Combined performance gains of simple cache protocol extensions. </title> <booktitle> In The 21st annual international symposium on computer architecture, </booktitle> <pages> pages 187-197, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This inherent communication in an application manifests itself as read-stall times on the z-machine. We should note that our base machine model, the z-machine, is different from base machines or idealized machines usually used in other cache studies <ref> [1, 9] </ref>. The base model in the other studies typically refers to a machine to which enhancements are applied. Many of those studies consider a sequentially consistent machine using the same cache protocol as the base model.
Reference: [2] <author> R. J. Anderson and J. C. Setubal. </author> <title> On the parallel implementation of goldberg's maximum flow algorithm. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 168-77, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The applications we studied include Cholesky and Barnes-Hut from SPLASH suite [12], Integer Sort from the NAS parallel benchmark suite [3], and Maxflow <ref> [2] </ref>. Cholesky performs a factorization of a sparse positive definite matrix. The sparse nature of the matrix results in an algorithm with a data dependent access pattern. Sets of columns having similar non-zero structure are combined into supernodes. <p> This boost simulates the effects of more time steps. is well defined statically. The problem size considered is 32K integers with 1K buckets. The Maxflow application finds the maximum flow from the distinguished source to the sink, in a directed graph with edge capacities. In the implementation <ref> [2] </ref>, each processor accesses a local work queue for tasks to perform. These may in turn generate new tasks which are added to this local work queue. Each task involves read and write accesses to shared data.
Reference: [3] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: The applications we studied include Cholesky and Barnes-Hut from SPLASH suite [12], Integer Sort from the NAS parallel benchmark suite <ref> [3] </ref>, and Maxflow [2]. Cholesky performs a factorization of a sparse positive definite matrix. The sparse nature of the matrix results in an algorithm with a data dependent access pattern. Sets of columns having similar non-zero structure are combined into supernodes.
Reference: [4] <author> M. E. Crovella and T. J. LeBlanc. </author> <title> Parallel Performance Prediction Using Lost Cycles Analysis. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: There have been several recent studies in separating the overheads seen in the execution of an application on a parallel architecture <ref> [4, 13] </ref>. These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category.
Reference: [5] <author> F. Dahlgren and P. Stenstrom. </author> <title> Reducing the write traffic for a hybrid cache protocol. </title> <booktitle> In 1994 International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 166-173, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: To reduce the number of messages on the network we assume an additional merge buffer at each node that combines writes to the same cache line. While it has been shown that the merge buffer is effective in reducing the number of messages <ref> [5] </ref>, it does introduce additional stall time for flushing the merge buffer at synchronization points for guaranteeing the correctness of the protocol. RCcomp: This memory system also uses the RC mem ory model, and a merge buffer as in RCupd. However the cache protocol used is slightly different.
Reference: [6] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In our work we consider the following four memory systems (RCinv, RCupd, RCadapt, RCcomp) that are built on top of the base hardware by specifying a particular coherence protocol along with a memory model. RCinv: The memory system uses the release consistent (RC) memory model <ref> [6] </ref> and a Berkeley-style write-invalidate protocol. In this system, a processor write that misses in the cache is simply recorded in the write-buffer and the processor continues execution without stalling.
Reference: [7] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The motivation for doing this is to eliminate the buffer flush time. One approach would be associating data with synchronization <ref> [7] </ref> in order to carry out smart self-invalidations when needed at the consumer instead of stalling at the producer. 7 Concluding Remarks The goal of several parallel computing research projects is to realize scalable shared memory machines.
Reference: [8] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In general the goal of all such techniques is to make the parallel machine appear as a zero overhead machine from the point of view of an application. It is usually recognized that no one technique is universally applicable for reducing or tolerating the communication overheads in all situations <ref> [8] </ref>. fl This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC. 1 We use the term system to denote an algorithm-architecture pair.
Reference: [9] <author> M. Heinrich, J. Kuskin, D. Ofelt, and et al. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This inherent communication in an application manifests itself as read-stall times on the z-machine. We should note that our base machine model, the z-machine, is different from base machines or idealized machines usually used in other cache studies <ref> [1, 9] </ref>. The base model in the other studies typically refers to a machine to which enhancements are applied. Many of those studies consider a sequentially consistent machine using the same cache protocol as the base model.
Reference: [10] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Logic overhead and performance. </title> <journal> Transactions on parallel and distributed systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Each node has a piece of the shared memory with its associated full-mapped directory information, a private cache, and a write buffer (not unlike the Dash multiprocessor <ref> [10] </ref>). In our work we consider the following four memory systems (RCinv, RCupd, RCadapt, RCcomp) that are built on top of the base hardware by specifying a particular coherence protocol along with a memory model.
Reference: [11] <author> U. Ramachandran, G. Shah, A. Sivasubramaniam, A. Singla, and I. Yanasak. </author> <title> Architectural mechanisms for explicit communication in shared memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: The protocol used in this memory system was developed for software management of coherent caches through the use of explicit communication primitives <ref> [11] </ref>. The directory controller keeps state information for sending updates to the active set of sharers through the selective-write primitive. When the selective-write primitive is used the corresponding memory block enters a special state. The presence bits in the directory represent active set the of sharers of that phase.
Reference: [12] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The applications we studied include Cholesky and Barnes-Hut from SPLASH suite <ref> [12] </ref>, Integer Sort from the NAS parallel benchmark suite [3], and Maxflow [2]. Cholesky performs a factorization of a sparse positive definite matrix. The sparse nature of the matrix results in an algorithm with a data dependent access pattern. Sets of columns having similar non-zero structure are combined into supernodes.
Reference: [13] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: There have been several recent studies in separating the overheads seen in the execution of an application on a parallel architecture <ref> [4, 13] </ref>. These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category. <p> These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category. For example, Siva-subramaniam et al. <ref> [13] </ref> break-down the overheads into algorithmic (i.e. inherent in the application such as serial component), and interaction (i.e. due to the communication and system overheads seen by the application when mapped onto a given architecture). All the techniques for latency reduction and tolerance attempt to shave off this interaction overhead. <p> The quest for a machine model that has zero communication overhead from the point of view of an application is the goal of this work. PRAM [16] has been used quite successfully as a vehicle for parallel algorithm design. In <ref> [13] </ref>, it is shown how PRAM could be used as a vehicle for determining the algorithmic overhead in an application as well by using the PRAM in an execution-driven framework. <p> However, we want to incorporate it into an execution-driven simulator so that we can benchmark application performance on different memory systems with reference to the z-machine. For this purpose, we have simulated the z-machine within the SPASM framework <ref> [13, 14] </ref> an execution-driven parallel architecture simulator. SPASM provides a framework to trap into the simulator on every shared memory read and write. We provide routines that are invoked on these traps based on the machine model we are simulating.
Reference: [14] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 411-426, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: However, we want to incorporate it into an execution-driven simulator so that we can benchmark application performance on different memory systems with reference to the z-machine. For this purpose, we have simulated the z-machine within the SPASM framework <ref> [13, 14] </ref> an execution-driven parallel architecture simulator. SPASM provides a framework to trap into the simulator on every shared memory read and write. We provide routines that are invoked on these traps based on the machine model we are simulating.
Reference: [15] <author> C. P. Thacker and L. C. Stewart. Firefly: </author> <title> A Multiprocessor Workstation. </title> <booktitle> In Proceedings of the First International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-172, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: RCupd: This memory system uses the RC memory model, a simple write-update protocol similar to the one used in the Firefly multiprocessor <ref> [15] </ref>. From the point of view of the processor, writes are handled exactly as in RCinv. However, we expect a higher write stall time for this memory system compared to RCinv due to the larger number of messages warranted by update schemes.
Reference: [16] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1979. </year>
Reference-contexts: The quest for a machine model that has zero communication overhead from the point of view of an application is the goal of this work. PRAM <ref> [16] </ref> has been used quite successfully as a vehicle for parallel algorithm design. In [13], it is shown how PRAM could be used as a vehicle for determining the algorithmic overhead in an application as well by using the PRAM in an execution-driven framework.
References-found: 16

