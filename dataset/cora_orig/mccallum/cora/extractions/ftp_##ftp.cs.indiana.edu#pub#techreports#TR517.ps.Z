URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR517.ps.Z
Refering-URL: http://www.cs.indiana.edu/ftp/techreports/index.html
Root-URL: http://www.cs.indiana.edu
Title: Eliminating Dead Code on Recursive Data  
Author: Yanhong A. Liu and Scott D. Stoller 
Date: October 1998  
Abstract: This paper describes a general and powerful method for dead code analysis and elimination in the presence of recursive data constructions. We represent partially dead recursive data using liveness patterns based on general regular tree grammars extended with the notion of live and dead, and we formulate the analysis as computing liveness patterns at all program points based on program semantics. This analysis yields a most precise liveness pattern for the data at each program point, which is significantly more precise than results from previous methods. The analysis algorithm takes cubic time in terms of the size of the program in the worst case but is extremely efficient in practice, as shown by our prototype implementation. The analysis results are used to identify and eliminate dead code. The general framework for representing and analyzing properties of recursive data structures using general regular tree grammars applies to other analyses as well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Dead computations produce values that never get used <ref> [1] </ref>. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse [35, 1]. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. <p> 1 Introduction Dead computations produce values that never get used [1]. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse <ref> [35, 1] </ref>. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization [31, 30], and compile-time garbage collection [22, 19, 37, 49].
Reference: [2] <author> A. Aiken and B. R. Murphy. </author> <title> Static type inference in a dynamically typed language. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on Principles of Programming Languages. ACM, </booktitle> <address> New York, </address> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [3] <author> K. Arnold and J. Golsing. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1996. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [19, 10, 24, 30, 40, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [45, 13, 32, 3] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is difficult because recursive data structures can be defined by the user, and dead substructures may interleave with live substructures.
Reference: [4] <author> B. Blanchet. </author> <title> Escape analysis: correctness proof, implementation and experimental results. </title> <booktitle> In Conference Record of the 25th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-37. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [34, 11]. Another kind of analysis is escape analysis <ref> [37, 12, 4] </ref>, but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [22] uses necessity patterns that correspond to liveness patterns.
Reference: [5] <author> R. Bodik and R. Gupta. </author> <title> Partial dead code elimination using slicing transformations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '97 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 159-170. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [19, 10, 24, 30, 40, 5] </ref>. <p> Authors' address: Computer Science Department, Indiana University, 215 Lindley Hall, Bloomington, IN 47405. Email: fliu,stollerg@cs.indiana.edu. 1 This is different from partial dead code, code that is dead on some but not all computation paths <ref> [24, 5] </ref>. 1 patterns based on general regular tree grammars extended with the notion of live and dead, and we formulate the analysis as computing liveness patterns at all program points based on program semantics. This analysis yields a most precise liveness pattern for the data at each program point.
Reference: [6] <author> W.-N. Chin. </author> <title> Safe fusion of functional expressions. </title> <booktitle> In LFP 1992 [27], </booktitle> <pages> pages 11-20. </pages>
Reference-contexts: The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion <ref> [47, 6] </ref>, as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs.
Reference: [7] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Commun. ACM, </journal> 20(11) 850-856, Nov. 1977. 
Reference-contexts: The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization <ref> [7, 36, 31, 29] </ref>, caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
Reference: [8] <author> J. Cocke and J. T. Schwartz. </author> <title> Programming languages and their compilers; preliminary notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference: [9] <author> P. Cousot and R. Cousot. </author> <title> Formal language, grammar and set-constraint-based program analysis by abstract interpretation. </title> <booktitle> In Proceedings of the 7th International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 170-181. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> Compared with that work, we also handle more program constructs, namely, binding expressions and user-defined constructors of arbitrary arity. We believe that our treatment is also more rigorous, since we adopt the view that regular-tree-grammar-based program analysis is also abstract interpretation <ref> [9] </ref>. We extend the grammars and handle L and D specially in grammar manipulations. We design program-based finite grammar domains to yield precise and efficient analysis methods. <p> Another standard way to obtain the analysis result is to use a general fixed point computation on potentially infinite grammar domains and use approximation operations to guarantee termination. Approximation operations provide a more general solution and make the analysis framework more modular and flexible <ref> [9] </ref>. In a separate paper [28], we describe three approximation operations that together produce significantly more precise analysis results than previous methods. Each operation is efficient, but due to their generality and interaction, that work does not have an exact characterization of the total number of iterations needed. <p> The finite domains described in this work make a complete analysis possible, yet still give significantly more precise analysis results than previous methods. Regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [17, 18, 9] </ref>, but we do not know any work that treats precise and efficient dead code analysis for recursive data as we do.
Reference: [10] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. M. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [19, 10, 24, 30, 40, 5] </ref>.
Reference: [11] <author> A. De Niel, E. Bevers, and K. De Vlaminck. </author> <title> Program bifurcation for a polymorphically typed functional langauge. </title> <booktitle> In Proceedings of the Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 142-153. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [26]. Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars <ref> [34, 11] </ref>. Another kind of analysis is escape analysis [37, 12, 4], but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [22] uses necessity patterns that correspond to liveness patterns. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [12] <author> A. Deutsch. </author> <title> On the complexity of escape analysis. </title> <booktitle> In Conference Record of the 24th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 358-371. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [34, 11]. Another kind of analysis is escape analysis <ref> [37, 12, 4] </ref>, but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [22] uses necessity patterns that correspond to liveness patterns.
Reference: [13] <author> R. K. Dybvig. </author> <title> The Scheme Programming Language. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1987. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [19, 10, 24, 30, 40, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [45, 13, 32, 3] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is difficult because recursive data structures can be defined by the user, and dead substructures may interleave with live substructures.
Reference: [14] <institution> Proceedings of the 4th International Conference on Functional Programming Languages and Computer Architecture. ACM, </institution> <address> New York, </address> <month> Sept. </month> <year> 1989. </year>
Reference: [15] <author> F. Gecseg and M. Steinb. </author> <title> Tree Automata. </title> <publisher> Akademiai Kiado, </publisher> <address> Budapest, </address> <year> 1984. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars <ref> [15] </ref>, which allow bounded, and often precise, representations of unbounded data [21, 33, 34, 2, 44, 9, 40]. <p> The resulting grammars can be minimized <ref> [15] </ref>, but minimization is complicated and is not needed for identifying dead code. 9 N 84 ! triple (N 0 ; N 0 ; N 0 ); N 84 ! cons (N 0 ; N 0 ); N 84 ! nil (); N 84 ! cons (N 0 ; N 77
Reference: [16] <author> C. A. Gunter. </author> <title> Semantics of Programming Languages. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year> <month> 13 </month>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections <ref> [42, 16] </ref>, which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest [48, 25, 34, 30, 40]. Let X be the domain of all possible values computed by our programs, including ? and values containing .
Reference: [17] <author> N. Heintze. </author> <title> Set-Based Program Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The finite domains described in this work make a complete analysis possible, yet still give significantly more precise analysis results than previous methods. Regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [17, 18, 9] </ref>, but we do not know any work that treats precise and efficient dead code analysis for recursive data as we do.
Reference: [18] <author> N. Heintze. </author> <title> Set-based analysis of ML programs. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 306-317. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This allows our method to handle complete programs that contain higher-order functions. Second, we can directly construct production rules corresponding to function abstraction and application and add rules for simplifying them. This is similar to how Heintze <ref> [18] </ref> handles higher-order functions for analyzing sets of values for ML programs. Our method is described here for an untyped language, but it applies to typed languages as well. <p> The finite domains described in this work make a complete analysis possible, yet still give significantly more precise analysis results than previous methods. Regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis <ref> [17, 18, 9] </ref>, but we do not know any work that treats precise and efficient dead code analysis for recursive data as we do.
Reference: [19] <author> J. Hughes. </author> <title> Compile-time analysis of functional programs. </title> <editor> In D. Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, </booktitle> <pages> pages 117-153. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1990. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization [31, 30], and compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [19, 10, 24, 30, 40, 5] </ref>. <p> Several methods have been studied <ref> [22, 19, 30, 40] </ref>, but all have limitations. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. We represent partially dead recursive data using liveness fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253. <p> Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [22] uses necessity patterns that correspond to liveness patterns. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes <ref> [19] </ref> uses contexts that correspond to liveness patterns. Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same. <p> The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [20] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 244-256. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: However, methods used there for handling unbounded growth of such projections are crude. The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick <ref> [20] </ref>, where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times [33, 34, 2, 11, 50, 44, 40].
Reference: [21] <author> N. D. Jones and S. S. Muchnick. </author> <title> Flow analysis and optimization of LISP-like structures. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 102-131. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> The correctness of this algorithm can be proved in a similar way to when only the selector form is used <ref> [21] </ref>. Nonterminals are associated with program points, so there are O (n) of them. Each step adds a production of the form N ! L, N ! c (N 1 ; :::; N k ), or N ! N 0 .
Reference: [22] <author> S. B. Jones and D. Le Metayer. </author> <title> Compile-time garbage collection by sharing analysis. </title> <booktitle> In FPCA 1989 [14], </booktitle> <pages> pages 54-74. </pages>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization [31, 30], and compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> Several methods have been studied <ref> [22, 19, 30, 40] </ref>, but all have limitations. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. We represent partially dead recursive data using liveness fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253. <p> Another kind of analysis is escape analysis [37, 12, 4], but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer <ref> [22] </ref> uses necessity patterns that correspond to liveness patterns. Necessity patterns specify only heads and tails of list values. The absence analysis by Hughes [19] uses contexts that correspond to liveness patterns. <p> The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [23] <author> K. Kennedy. </author> <title> Use-definition chains with applications. </title> <journal> J. Comput. Lang., </journal> <volume> 3(3) </volume> <pages> 163-179, </pages> <year> 1978. </year>
Reference: [24] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 147-158. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [19, 10, 24, 30, 40, 5] </ref>. <p> Authors' address: Computer Science Department, Indiana University, 215 Lindley Hall, Bloomington, IN 47405. Email: fliu,stollerg@cs.indiana.edu. 1 This is different from partial dead code, code that is dead on some but not all computation paths <ref> [24, 5] </ref>. 1 patterns based on general regular tree grammars extended with the notion of live and dead, and we formulate the analysis as computing liveness patterns at all program points based on program semantics. This analysis yields a most precise liveness pattern for the data at each program point.
Reference: [25] <author> J. Launchbury. </author> <title> Projection Factorisations in Partial Evaluation. </title> <type> PhD thesis, </type> <institution> Department of Computing, University of Glasgow, </institution> <address> Glasgow, Scotland, </address> <year> 1989. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [42, 16], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [48, 25, 34, 30, 40] </ref>. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Wadler and Hughes use projections for strictness analysis [48]. Their analysis is also backward 11 but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation <ref> [25] </ref>. It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [26].
Reference: [26] <author> J. Launchbury. </author> <title> Strictness and binding-time analysis: Two for the price of one. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 80-91. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation [25]. It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well <ref> [26] </ref>. Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [34, 11].
Reference: [27] <institution> Proceedings of the 1992 ACM Conference on LISP and Functional Programming. ACM, </institution> <address> New York, </address> <month> June </month> <year> 1992. </year>
Reference: [28] <author> Y. A. Liu. </author> <title> Dependence analysis for recursive data. </title> <booktitle> In Proceedings of the IEEE 1998 International Conference on Computer Languages, </booktitle> <pages> pages 206-215. </pages> <publisher> IEEE CS Press, Los Alamitos, </publisher> <address> Calif., </address> <month> May </month> <year> 1998. </year>
Reference-contexts: Another standard way to obtain the analysis result is to use a general fixed point computation on potentially infinite grammar domains and use approximation operations to guarantee termination. Approximation operations provide a more general solution and make the analysis framework more modular and flexible [9]. In a separate paper <ref> [28] </ref>, we describe three approximation operations that together produce significantly more precise analysis results than previous methods. Each operation is efficient, but due to their generality and interaction, that work does not have an exact characterization of the total number of iterations needed.
Reference: [29] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 157-170. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization <ref> [7, 36, 31, 29] </ref>, caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
Reference: [30] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Static caching for incremental computation. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 20(3) </volume> <pages> 546-585, </pages> <month> May </month> <year> 1998. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization <ref> [31, 30] </ref>, and compile-time garbage collection [22, 19, 37, 49]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [19, 10, 24, 30, 40, 5] </ref>. <p> Several methods have been studied <ref> [22, 19, 30, 40] </ref>, but all have limitations. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. We represent partially dead recursive data using liveness fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253. <p> For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [42, 16], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [48, 25, 34, 30, 40] </ref>. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> For example, on a few programs of about twelve hundred lines, the analysis takes 2 to 3 milliseconds, including garbage collection time. Our original motivation for studying this general problem was for identifying appropriate intermediate results to cache and use for incremental computation <ref> [30] </ref>. There, we propose a method, called cache-and-prune, that first transforms a program to cache all intermediate results, then reuses them in a computation on incremented input, and finally prunes out cached values that are not used. <p> Even if it is extended for recursive data types, it handles only a finite domain of list contexts where every head context and every tail context is the same. The analysis for pruning by Liu, Stoller, and Teitelbaum <ref> [30] </ref> uses projections to specify specific components of tuple values and thus provide more accurate information. However, methods used there for handling unbounded growth of such projections are crude. <p> The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement <ref> [30] </ref>, deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
Reference: [31] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <journal> Sci. Comput. Program., </journal> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization <ref> [31, 30] </ref>, and compile-time garbage collection [22, 19, 37, 49]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization <ref> [7, 36, 31, 29] </ref>, caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
Reference: [32] <author> R. Milner, M. Tofte, and R. Harper. </author> <title> The definition of Standard ML. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [19, 10, 24, 30, 40, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [45, 13, 32, 3] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is difficult because recursive data structures can be defined by the user, and dead substructures may interleave with live substructures.
Reference: [33] <author> P. Mishra and U. Reddy. </author> <title> Declaration-free type checking. </title> <booktitle> In Conference Record of the 12th Annual ACM Symposium on POPL, </booktitle> <pages> pages 7-21. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [34] <author> T. Mogensen. </author> <title> Separating binding times in language specifications. </title> <booktitle> In FPCA 1989 [14], </booktitle> <pages> pages 12-25. </pages>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [42, 16], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [48, 25, 34, 30, 40] </ref>. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> It is a forward analysis equivalent to strictness analysis and uses a fixed finite abstract domain as well [26]. Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars <ref> [34, 11] </ref>. Another kind of analysis is escape analysis [37, 12, 4], but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [22] uses necessity patterns that correspond to liveness patterns. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [35] <editor> S. S. Muchnick and N. D. Jones, editors. </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: 1 Introduction Dead computations produce values that never get used [1]. While programmers are not likely to write code that performs dead computations, such code appears often as the result of program optimization, modification, and reuse <ref> [35, 1] </ref>. There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization [31, 30], and compile-time garbage collection [22, 19, 37, 49].
Reference: [36] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization <ref> [7, 36, 31, 29] </ref>, caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
Reference: [37] <author> Y. G. Park and B. Goldberg. </author> <title> Escape analysis on lists. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 116-127. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization [31, 30], and compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> Mogensen, DeNiel, and others also use projections, based on grammars in particular, for binding-time analysis and program bifurcation, but they use only a restricted class of regular tree grammars [34, 11]. Another kind of analysis is escape analysis <ref> [37, 12, 4] </ref>, but existing methods can not express as precise information as we do. Several analyses are in the same spirit as ours. The necessity interpretation by Jones and Le Metayer [22] uses necessity patterns that correspond to liveness patterns. <p> The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [38] <author> W. Pugh and E. Rosser. </author> <title> Iteration space slicing and its application to communication optimization. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: This paper discusses techniques for recursion. The basic ideas should extend to loops. A recent work has just started this direction; it extends slicing to symbolically capture particular iterations in a loop <ref> [38] </ref>. Object-oriented programming is used widely, but cross-class optimization heavily depends on inlining, which often causes code blow-up. Grammar-based analysis and transformation can be applied to methods across classes without inlining.
Reference: [39] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year> <month> 14 </month>
Reference-contexts: However, the 10 triple in every even call is indeed dead. One needs to unfold the definition of minmax to remove such dead computations. Implementation. We have implemented the analysis in a prototype system. The implementation uses the Synthesizer Generator <ref> [39] </ref>. The algorithm for simplifying the grammars is written in the Synthesizer Generator Scripting Language, STk, a dialect of Scheme, and consists of about 300 lines of code.
Reference: [40] <author> T. Reps and T. Turnidge. </author> <title> Program specialization via program slicing. </title> <editor> In O. Danvy, R. Gluck, and P. Thiemann, editors, </editor> <booktitle> Proceedings of the Dagstuhl Seminar on Partial Evaluation, volume 1110 of Lecture Notes in Computer Science, </booktitle> <pages> pages 409-429. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing <ref> [51, 40] </ref>, specialization [40], incrementalization [31, 30], and compile-time garbage collection [22, 19, 37, 49]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization <ref> [40] </ref>, incrementalization [31, 30], and compile-time garbage collection [22, 19, 37, 49]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> We call this dead code analysis, bearing in mind that it may be used for many other purposes. In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations <ref> [19, 10, 24, 30, 40, 5] </ref>. <p> Several methods have been studied <ref> [22, 19, 30, 40] </ref>, but all have limitations. This paper describes a general and powerful method for analyzing and eliminating dead computations in the presence of recursive data constructions. We represent partially dead recursive data using liveness fl The authors gratefully acknowledge the support of NSF under grant CCR-9711253. <p> For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [42, 16], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [48, 25, 34, 30, 40] </ref>. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis. <p> It is later used to describe other data flow information such as types and binding times [33, 34, 2, 11, 50, 44, 40]. In particular, the analysis for backward slicing by Reps and Turnidge <ref> [40] </ref> explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis. <p> Regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis [17, 18, 9], but we do not know any work that treats precise and efficient dead code analysis for recursive data as we do. The method and algorithms studied here have many applications: program slicing and specialization <ref> [51, 40] </ref>, strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
Reference: [41] <author> M. Sagiv, T. Reps, and R. Wilhelm. </author> <title> Solving shape-analysis problems in languages with destructive updating. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 20(1) </volume> <pages> 1-50, </pages> <month> Jan. </month> <year> 1998. </year>
Reference-contexts: The ideas of including L and D as terminals, constructing grammars based on program points as well as the semantics of program constructs connecting these points, and doing grammar simplifications are the same. Recent work by Sagiv, Reps, and Wilhelm <ref> [41] </ref> uses graph grammars for shape analysis. We believe we can make use of that result for dead code analysis in the presence of destructive updates. Our method can also be extended to handle higher-order functions in two ways.
Reference: [42] <author> D. S. Scott. </author> <title> Lectures on a mathematical theory of computation. </title> <editor> In M. Broy and G. Schmidt, editors, </editor> <booktitle> Theoretical Foundations of Programming Methodology, </booktitle> <pages> pages 145-292. </pages> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1982. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections <ref> [42, 16] </ref>, which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest [48, 25, 34, 30, 40]. Let X be the domain of all possible values computed by our programs, including ? and values containing .
Reference: [43] <author> O. Shivers. </author> <title> Control flow analysis in scheme. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation. ACM, </booktitle> <address> New York, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: We believe we can make use of that result for dead code analysis in the presence of destructive updates. Our method can also be extended to handle higher-order functions in two ways. First, we can simply apply a control-flow analysis <ref> [43] </ref> before we do dead code analysis. This allows our method to handle complete programs that contain higher-order functions. Second, we can directly construct production rules corresponding to function abstraction and application and add rules for simplifying them.
Reference: [44] <author> M. H. Sorensen. </author> <title> A grammar-based data-flow analysis to stop deforestation. </title> <editor> In S. Tison, editor, CAAP'94: </editor> <booktitle> Proceedings of the 19th International Colloquium on Trees in Algebra and Programming, volume 787 of Lecture Notes in Computer Science, </booktitle> <pages> pages 335-351. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Formally, the grammars we use for describing liveness patterns are regular tree grammars [15], which allow bounded, and often precise, representations of unbounded data <ref> [21, 33, 34, 2, 44, 9, 40] </ref>. <p> The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [45] <author> G. L. Steele. </author> <title> Common Lisp the Language. </title> <publisher> Digital Press, </publisher> <year> 1984. </year>
Reference-contexts: In recent years, dead code analysis has been made more precise so as to be effective in more complicated computations [19, 10, 24, 30, 40, 5]. Since recursive data constructions are used increasingly widely in high-level languages <ref> [45, 13, 32, 3] </ref>, an important problem is to identify partially dead recursive data|that is, recursive data whose dead parts form recursive substructures|and eliminate computations of them. 1 It is difficult because recursive data structures can be defined by the user, and dead substructures may interleave with live substructures.
Reference: [46] <author> F. </author> <title> Tip. A survey of program slicing techniques. </title> <journal> Journal of Programming Languages, </journal> <volume> 3(3) </volume> <pages> 121-189, </pages> <month> Sept. </month> <year> 1995. </year>
Reference: [47] <author> P. Wadler. </author> <title> Deforestation: Transforming programs to eliminate trees. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 73 </volume> <pages> 231-248, </pages> <year> 1990. </year> <booktitle> Special issue of selected papers from the 2nd European Symposium on Programming. </booktitle>
Reference-contexts: The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion <ref> [47, 6] </ref>, as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs.
Reference: [48] <author> P. Wadler and R. J. M. Hughes. </author> <title> Projections for strictness analysis. </title> <booktitle> In Proceedings of the 3rd International Conference on Functional Programming Languages and Computer Architecture, volume 274 of Lecture Notes in Computer Science, </booktitle> <pages> pages 385-407. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: For example, cons (D; cons (L; D)) (cons (0; cons (1; cons (2; nil)))) = cons ( ; cons (1; )): Formally, liveness patterns are domain projections [42, 16], which provide a clean tool for describing substructures of constructed data by projecting out the parts that are of interest <ref> [48, 25, 34, 30, 40] </ref>. Let X be the domain of all possible values computed by our programs, including ? and values containing . <p> Wadler and Hughes use projections for strictness analysis <ref> [48] </ref>. Their analysis is also backward 11 but seeks necessary rather than sufficient information, and it uses a fixed finite abstract domain for all programs. Launchbury uses projections for binding-time analysis of partially static data structures in partial evaluation [25].
Reference: [49] <author> M. Wand and W. D. Clinger. </author> <title> Set constraints for destructive array update optimization. </title> <booktitle> In Proceedings of the IEEE 1998 International Conference on Computer Languages, </booktitle> <pages> pages 184-193. </pages> <publisher> IEEE CS Press, Los Alamitos, </publisher> <address> Calif., </address> <month> May </month> <year> 1998. </year>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing [51, 40], specialization [40], incrementalization [31, 30], and compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> The method and algorithms studied here have many applications: program slicing and specialization [51, 40], strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection <ref> [22, 19, 37, 49] </ref>. The analysis results also provide a kind of type information. The overall goal is to analyze dead data and eliminate computations of them across recursions and loops, possibly interleaved with wrappers like classes in object-oriented programs. This paper discusses techniques for recursion.
Reference: [50] <author> E. Wang and P. M. Hilfinger. </author> <title> Analysis of recursive types in lisp-like languages. </title> <booktitle> In LFP 1992 [27], </booktitle> <pages> pages 216-225. </pages>
Reference-contexts: The idea of using regular tree grammars for program flow analysis is due to Jones and Muchnick [20], where it is used mainly for shape analysis and hence for improving storage allocation. It is later used to describe other data flow information such as types and binding times <ref> [33, 34, 2, 11, 50, 44, 40] </ref>. In particular, the analysis for backward slicing by Reps and Turnidge [40] explicitly adopts regular tree grammars to represent projections. It is closest in goal and scope to our analysis.
Reference: [51] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year> <month> 15 </month>
Reference-contexts: There are also other programming activities that do not explicitly involve live or dead code but rely on similar notions. Examples are program slicing <ref> [51, 40] </ref>, specialization [40], incrementalization [31, 30], and compile-time garbage collection [22, 19, 37, 49]. Analysis for identifying dead code, or code having similar properties, has been studied and used widely [8, 7, 23, 36, 1, 22, 19, 10, 24, 31, 30, 46, 40, 49]. <p> Regular-tree-grammar-based program analysis can be reformulated as set-constraint-based analysis [17, 18, 9], but we do not know any work that treats precise and efficient dead code analysis for recursive data as we do. The method and algorithms studied here have many applications: program slicing and specialization <ref> [51, 40] </ref>, strength reduction, finite differencing, and incrementalization [7, 36, 31, 29], caching intermediate 12 results for program improvement [30], deforestation and fusion [47, 6], as well as compile-time garbage collection [22, 19, 37, 49]. The analysis results also provide a kind of type information.
References-found: 51

