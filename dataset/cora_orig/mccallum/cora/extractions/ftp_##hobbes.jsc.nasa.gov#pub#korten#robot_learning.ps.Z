URL: ftp://hobbes.jsc.nasa.gov/pub/korten/robot_learning.ps.Z
Refering-URL: http://tommy.jsc.nasa.gov/~bonasso/
Root-URL: 
Email: korten@aio.jsc.nasa.gov  
Title: An Intelligent Agent Architecture In Which to Pursue Robot Learning A set of robot specific
Author: R. Peter Bonasso and David Kortenkamp 
Keyword: Background and Motivation  
Note: o  The next section details the  
Address: 1120 NASA Road 1 Houston TX 77058  
Affiliation: The MITRE Corporation  
Abstract: This paper describes a multilayered, intelligent agent software architecture, developed for mobile and undersea robot applications in the defense sector, and to provide tele-autonomy to space-based manipulator robots. The architecture has a deliberative layer which uses a state-based planner, a middle layer for sequencing partially ordered plans using robot skills, and a lower layer repertoire of continuous robot skills. The system has been shown to provide a higher level of human supervision that preserves safety while allowing for task level direction, reaction to out-of-norm parameters, and human intervention at all levels of control. For this workshop, we hypothesize that the architecture is a useful framework in which to explore learning techniques. In particular, we outline techniques appropriate to learning within a given layer, techniques for migrating competences from higher to lower layers, and overall system adaptation from its interaction with the environment. Examples are reinforcement learning for tuning individual skills, case-based techniques to improve the replanning capability of the deliberative layer, and chunking or explanation-based learning to migrate new strategies created by the planner into standard procedures for the sequencing level. Since the late eighties we have investigated ways to combine deliberation and reactivity in robot control architectures [Sanborn et al 1989, Bonasso 91, & Bonasso et al 92], in order to program robots to carry out tasks robustly in field environments. Field environments are those in which events for which the robot has a response can occur unpredictably, and wherein the locations of objects and other agents is usually not known with certainty until the robot is carrying out the required task. A robot control software architecture, developed at MITRE is an outgrowth of several lines of situated reasoning research in robot intelligence [Firby 89, Gat 91, Connell 91, Slack 92, Yu et al 94, Elsaesser & Slack 94], and has proven useful for enabling mobile robots to accomplish tasks in field environments. This architecture separates the general robot intelligence problem into three interacting pieces (see Figure 1, below, Figure 2 is discussed in the section on learning): We have been successful in applying this architecture to mobile land [Bonasso et al 92] and undersea robots [Bonasso & Barrett 93] in the defense sector, and to mobile, two-armed manipulator systems in support of NASA. We believe the architecture is developed to the point where it can be used as a framework for integrating the work of other AI disciplines such as spoken language techniques and machine learning. This paper discusses the architecture from the standpoint of how various learning techniques might be accommodated. We have recently been using the architecture as a framework for controlling a two-armed manipulator robot maintaining a space station from the ground. The idea is that an intelligent ground control station can enable a ground crew to supervise the routine maintenance activities of the robot, and thus allow the on-orbit personnel to concentrate on user missions. We use examples from this domain to illustrate the architecture. 
Abstract-found: 1
Intro-found: 0
Reference: [Anderson 83] <editor> John Anderson. </editor> <publisher> The Architecture of Cognition . Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: It is easy to come up with examples of such learning in people, for example, piano playing takes incredible deliberative attention at first, but becomes automatic with practice. Such learning usually follows a power law where increased repetition leads to increased performance. Several systems, such as Soar and ACT* <ref> [Anderson 83] </ref> exhibit this kind of learning, but they are single-layer architectures. How can we achieve similar results within a multilayer system? There are two possibilities.
Reference: [Bonasso 91] <author> R. Peter Bonasso. </author> <title> Integrating Reaction Plans and Layered Competences Through Synchronous Control. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence. </booktitle> <address> Sydney, Australia. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Background and Motivation Since the late eighties we have investigated ways to combine deliberation and reactivity in robot control architectures <ref> [Sanborn et al 1989, Bonasso 91, & Bonasso et al 92] </ref>, in order to program robots to carry out tasks robustly in field environments.
Reference: [Bonasso et al 92] <author> R. Peter Bonasso, H.J. Antonisse, M.G. Slack. </author> <title> A Reactive Robot System for Find and Fetch Tasks In An Outdoor Environment. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. </booktitle> <address> San Jose, CA. </address> <month> July </month> <year> 1992. </year>
Reference-contexts: We have been successful in applying this architecture to mobile land <ref> [Bonasso et al 92] </ref> and undersea robots [Bonasso & Barrett 93] in the defense sector, and to mobile, two-armed manipulator systems in support of NASA.
Reference: [Bonasso & Barrett 93] <author> R. Peter Bonasso, R.P. & J. Barratt, J. </author> <title> A Reactive Robot System for Find and Visit Tasks in a Dynamic Ocean Environment. </title> <booktitle> In Proceedings of the 8th International Symposium on Unmanned Untethered Submersible Technology, </booktitle> <publisher> IEEE Oceanographic Society, </publisher> <month> Sep </month> <year> 1993. </year>
Reference-contexts: We have been successful in applying this architecture to mobile land [Bonasso et al 92] and undersea robots <ref> [Bonasso & Barrett 93] </ref> in the defense sector, and to mobile, two-armed manipulator systems in support of NASA.
Reference: [Brooks 89] <author> Rodney A. Brooks. </author> <title> A Robot that Walks; Emergent Behaviors from a Carefully Evolved Network, </title> <booktitle> Proceedings IEEE Conference on Robotics and Automation, </booktitle> <year> 1989. </year>
Reference-contexts: To solve the problem, the programmer added an additional method to check for that unique state and forego setting the current arm on standby. There are also many examples of reinforcement learning being used to increase the performance of reactive robot skills, for example <ref> [Brooks 89] </ref>. In particular we are interested in having our skills learn various parameters that control their behavior. For example, our obstacle avoidance skill has several thresholds for determining whether a certain direction is open or closed. Adjusting these thresholds for each new environment is time consuming.
Reference: [Connell 91] <author> J. H. Connell. </author> <title> A hybrid architecture applied to robot navigation. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <month> April </month> <year> 1992. </year>
Reference: [DeJong 86] <author> Gerald Dejong. </author> <title> An Approach to Learning from Observation. In Machine Learning, An Artificial Intelligence Approach,Vol II, R.S. </title> <editor> Michalsky, J.G. Carbonell, and T.M. Mitchell, eds. </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: The feedback that the RAPs system exploits from the low - level skills can be the basis of learning new methods in the sequencing layer, a process of learning by observation (e.g., <ref> [DeJong 86] </ref>. There is a simple but illustrative example from our manipulator work which was "learned" by the programmer.
Reference: [Elsaesser & MacMillan 91] <author> C. </author> <title> Elsaesser . Representation and Algorithms for Multiagent Adversarial Planning, </title> <institution> MTR-91W000207, The MITRE Corporation, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: We use examples from this domain to illustrate the architecture. The Planner The planning system is envisioned to be a state-based, nonlinear hierarchical planner a la SIPE. The planner we use, known as AP <ref> [Elsaesser & MacMillan 91] </ref>, is a multi-agent planner which can reason about metric time for scheduling, monitor the execution of its plans, and replan accordingly.
Reference: [Elsaesser & Slack 94] <author> C. Elsaesser & M.G. Slack. </author> <title> Deliberative Planning in a Robot Architecture. </title> <booktitle> In Proceedings of the AAIA/NASA Conference on Intelligent Robots in Field, Factory, Service and Space, </booktitle> <month> March </month> <year> 1994. </year>
Reference: [Firby 89] <author> James R. Firby. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PHD Diss. </type> <institution> YALEU/CSD/RR #672, Yale University. </institution> <year> 1989. </year>
Reference: [Firby 93] <author> James R. Firby. </author> <title> Interfacing the RAP System to Real-time Control (forthcoming), </title> <institution> University of Chicago. </institution>
Reference-contexts: ?planner ?site-item) (arms-status ?planner unfolded) ) (Operator fix-item-with-arm :purpose (fixed ?arm ?site-item) :arguments ( (?site (get-site-from-memory ?site-item)) ) :preconditions ( (attached-to ?planner ?site) ) :effects ( (fixed ?arm ?site-item) (arms-status ?planner unfolded) ) :task-time duration-of-fix-item-with-arm ) The Sequencer We are using a new version of Firby's Reactive Action Packages (RAPs) <ref> [Firby 93] </ref> as the instance of our sequencing system to encode routine behavior as sequences of situated skills. The RAP interpreter uses a library of RAPs to decompose sequences of behaviors to accomplish a task.
Reference: [Gat 91] <author> Erann Gat. </author> <title> Reliable Goal-Directed Reactive Control of Autonomous Mobile Robots. </title> <type> PhD thesis, </type> <institution> Virginia Polytechnic Institute Department of Computer Science, </institution> <month> April </month> <year> 1991. </year>
Reference: [Knoblock et al 91] <author> Craig Knoblock, Steven Minton and Oren Etzioni. </author> <title> Integration of Abstraction and Explanation-Based Learning in PRODIGY, </title> <booktitle> AAAI-91, </booktitle> <year> 1991. </year>
Reference-contexts: Each of these three areas are explored in the following subsections. Learning within layers Learning within layers can be addressed immediately using known techniques. There are several examples of learning being used to increase a planner's performance, for example, Soar [Laird et al 87] and <ref> [Knoblock et al 91] </ref>. We can envision a case-based learning system for our planner, especially in repairs to the plan; repairs to a plan can be cached using the current robot context available from the RAP memory.
Reference: [Laird et al 87] <author> John Laird , Allen Newell and Paul Rosenbloom. </author> <title> Soar: An Architecture for General Intelligence. </title> <booktitle> Artificial Intelligence 33(1), </booktitle> <year> 1987. </year>
Reference-contexts: Each of these three areas are explored in the following subsections. Learning within layers Learning within layers can be addressed immediately using known techniques. There are several examples of learning being used to increase a planner's performance, for example, Soar <ref> [Laird et al 87] </ref> and [Knoblock et al 91]. We can envision a case-based learning system for our planner, especially in repairs to the plan; repairs to a plan can be cached using the current robot context available from the RAP memory.
Reference: [Sanborn et al 1989] <author> Jim Sanborn, B. Bloom, and D. McGrath. </author> <title> A Situated Reasoning Architecture for Space-based Repair and Replace Tasks, </title> <booktitle> In !989 Goddard Conference on Space Applications of ARtificial Intelligence, </booktitle> <address> Greenbelt, MD. </address> <note> NASA Pub # 3033. [Schoppers 89] 6. </note> <author> Marcel J. Schoppers. </author> <title> In Defense of Reaction Plans As Caches. </title> <journal> AI Magazine, </journal> <volume> 10(4): </volume> <pages> 51-60, </pages> <year> 1989 </year>
Reference-contexts: Background and Motivation Since the late eighties we have investigated ways to combine deliberation and reactivity in robot control architectures <ref> [Sanborn et al 1989, Bonasso 91, & Bonasso et al 92] </ref>, in order to program robots to carry out tasks robustly in field environments.
Reference: [Slack 90] <author> Marc G. Slack. </author> <title> Situationally Driven Local Navigation for Mobile Robots, </title> <publisher> JPL Pub. </publisher> <pages> 90-17, </pages> <year> 1990. </year> <institution> NASA. </institution>
Reference: [Slack 92] <author> M. G. Slack. </author> <title> Sequencing formally defined reactions for robotic activity: </title> <booktitle> Integrating -RAPS- and GAPPS-. In Proceedings of the SPIE Conference on Sensor Fusion, </booktitle> <month> November </month> <year> 1992. </year>
Reference: [Yu et al 94] <author> S. Yu, M. G. Slack, and D. P. Miller. </author> <title> A streamlined software environment for situated skills. </title> <booktitle> In Proceedings of the AAIA/NASA Conference on Intelligent Robots in Field, Factory, Service and Space, </booktitle> <month> March </month> <year> 1994. </year>
References-found: 18

