URL: http://www.cs.washington.edu/homes/karlin/papers/spinlock.ps
Refering-URL: http://www.cs.washington.edu/homes/karlin/papers.html
Root-URL: 
Title: Empirical Studies of Competitive Spinning for A Shared-Memory Multiprocessor  
Author: Anna R. Karlin Kai Li Mark S. Manasse Susan Owicki 
Address: 130 Lytton Ave., Palo Alto, CA 94301.  NJ 08544.  
Affiliation: DEC Systems Research Center,  Dept of Computer Science, Princeton University, Princeton,  
Abstract: A common operation in multiprocessor programs is acquiring a lock to protect access to shared data. Typically, the requesting thread is blocked if the lock it needs is held by another thread. The cost of blocking one thread and activating another can be a substantial part of program execution time. Alternatively, the thread could spin until the lock is free, or spin for a while and then block. This may avoid context-switch overhead, but processor cycles may be wasted in unproductive spinning. This paper studies seven strategies for determining whether and how long to spin before blocking. Of particular interest are competitive strategies, for which the performance can be shown to be no worse than some constant factor times an optimal off-line strategy. The performance of five competitive strategies is compared with that of always blocking, always spinning, or using the optimal off-line algorithm. Measurements of lock-waiting time distributions for five parallel programs were used to compare the cost of synchronization under all the strategies. Additional measurements of elapsed time for some of the programs and strategies allowed assessment of the impact of synchronization strategy on overall program performance. Both types of measurements indicate that the standard blocking strategy performs poorly compared to mixed strategies. Among the mixed strategies studied, adaptive algorithms perform better than non-adaptive ones. 
Abstract-found: 1
Intro-found: 1
Reference: [ABLL90] <author> T.E. Anderson, B.N. Bershad, E.D. La-zowska, and H.M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <institution> University of Washington Technical Report 90-04-02, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Among other things, they consider the cost of contention for a lock variable in shared memory, and describe a number of schemes aimed at minimizing the cost of that contention. Related work <ref> [ABLL90] </ref> proposes operating system support for moving many thread scheduling decisions into the application program. In recent work, B. Lim and A. Agarwal [LA91] have studied algorithms for the producer-consumer type synchronization used by "futures". <p> The thread system used in our measurements is a kernel-level implementation. The cost of a context-switch is larger than it would be with a user-level implementation of threads <ref> [ABLL90] </ref>. It is difficult to draw any conclusions from our measurements about the performance of the algorithms if context-switching is significantly cheaper. In general, though, reduced context-switch time might be expected to decrease the value of competitive algorithms, since the penalty for blocking inappropriately is reduced. <p> Taos provides no mechanism to prevent the preemption of a thread that is holding a lock. It is possible that some of the long waiting times we observed occurred because the lock holder was preempted. On the other hand, user-level thread packages <ref> [ABLL90] </ref> may have enough information to prevent preemption in this case. It is not clear how such cases would affect the performance of the competitive algorithms.
Reference: [ALL89] <author> T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> The Performance Implication of Thread Management Alternatives for Shared-Memory Multiprocessors. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling Computer Systems, </booktitle> <year> 1989. </year>
Reference-contexts: In later work [ZLE89] the same authors evaluate how the overhead of spinning is affected by various scheduling policies. Other studies <ref> [And89, ALL89, GT90] </ref> have compared alternatives for implementing synchronization and thread scheduling, using both modeling and empirical data. Among other things, they consider the cost of contention for a lock variable in shared memory, and describe a number of schemes aimed at minimizing the cost of that contention.
Reference: [And89] <author> Thomas E. Anderson. </author> <title> The Performance Implication of Spin-Waiting Alternatives for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages II:170-174, </pages> <year> 1989. </year>
Reference-contexts: This minimizes the traffic on the memory bus and avoids the high overhead of executing an interlocked instruction <ref> [And89] </ref>. 2.3 The optimal off-line algorithm This algorithm has the best performance that any algorithm can hope to achieve, and therefore we compare all other algorithms to it. <p> In later work [ZLE89] the same authors evaluate how the overhead of spinning is affected by various scheduling policies. Other studies <ref> [And89, ALL89, GT90] </ref> have compared alternatives for implementing synchronization and thread scheduling, using both modeling and empirical data. Among other things, they consider the cost of contention for a lock variable in shared memory, and describe a number of schemes aimed at minimizing the cost of that contention.
Reference: [Dij65] <author> E.W. Dijkstra. </author> <title> Solution of a Problem in Concurrent Programming Control. </title> <journal> Communications of the ACM, </journal> <volume> 8(9):569, </volume> <month> September </month> <year> 1965. </year>
Reference-contexts: Early work was concerned with implementing critical sections when the only atomic operations provided by hardware are memory read and write <ref> [Dij65, Knu66] </ref>. A number of software protocols have been developed, based on these operations and coherent shared memory. The main disadvantage of these approaches is their inefficiency. The most efficient approach [Lam87] still requires five writes and two reads in the absence of contention.
Reference: [GGK fl 83] <author> A. Gottlieb, R. Grishman, C.P. Kruskal, K.P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer Designing an MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> February </month> <year> 1983. </year>
Reference-contexts: Greater efficiency is possible when the hardware provides more powerful atomic operations. Simple atomic operations such as test-and-set and compare-and-swap simplify synchronization implementations and can even allow certain certain concurrent data structures to be implemented without blocking [Her87, Her90]. More powerful atomic operations, such as fetch-and-add <ref> [GGK fl 83] </ref> allow certain common operations to be performed in parallel without critical sections. Other work has evaluated the cost of synchronization, considering such complexities as the hardware cache-coherence algorithm, operating system implementation of threads, and the non-deterministic nature of the workload.
Reference: [GT90] <author> Gary Graunke and Shreekant Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6), </volume> <pages> pp 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In later work [ZLE89] the same authors evaluate how the overhead of spinning is affected by various scheduling policies. Other studies <ref> [And89, ALL89, GT90] </ref> have compared alternatives for implementing synchronization and thread scheduling, using both modeling and empirical data. Among other things, they consider the cost of contention for a lock variable in shared memory, and describe a number of schemes aimed at minimizing the cost of that contention.
Reference: [Her87] <author> Maurice Herlihy. </author> <title> Impossibility and Universality Results for Wait-Free Synchronization. </title> <booktitle> In Seventh ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: Greater efficiency is possible when the hardware provides more powerful atomic operations. Simple atomic operations such as test-and-set and compare-and-swap simplify synchronization implementations and can even allow certain certain concurrent data structures to be implemented without blocking <ref> [Her87, Her90] </ref>. More powerful atomic operations, such as fetch-and-add [GGK fl 83] allow certain common operations to be performed in parallel without critical sections.
Reference: [Her90] <author> Maurice Herlihy. </author> <title> A Methodology for Implementing Highly Concurrent Data Structures. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Greater efficiency is possible when the hardware provides more powerful atomic operations. Simple atomic operations such as test-and-set and compare-and-swap simplify synchronization implementations and can even allow certain certain concurrent data structures to be implemented without blocking <ref> [Her87, Her90] </ref>. More powerful atomic operations, such as fetch-and-add [GGK fl 83] allow certain common operations to be performed in parallel without critical sections.
Reference: [KMMO89] <author> A.R. Karlin, M.S. Manasse, L. McGeoch, and S. Owicki. </author> <title> Competitive Randomized Algorithms for Non-Uniform Problems. </title> <booktitle> In 1st Annual ACM Symposium on Discrete Algorithms, </booktitle> <pages> pages 301-309, </pages> <year> 1989. </year>
Reference-contexts: A compromise between these two methods is to combine spinning and blocking: when a thread fails to acquire a lock, it spins for some time and then blocks. Such a scheme was implemented by Ouster-hout [Oust82] in the Medusa system. More recently, it has been shown <ref> [KMMO89] </ref> that a variant of this method, where the time spent spinning is equal to a context-switch, is competitive. <p> If this inequality holds, we say that c is the competitive ratio of the algorithm. We are interested in two kinds of competitive algorithms: deterministic and randomized. Earlier work <ref> [KMMO89] </ref> has shown that there is a simple deterministic algorithm with a competitive ratio of 2, and that there is no deterministic algorithm that has a competitive ratio smaller than 2. On the other hand, randomized algorithms can achieve strongly competitive ratios approaching e=(e 1) 1:58. <p> Fixed spin Fixed-spin <ref> [KMMO89] </ref> is the simplest competitive algorithm for spinning. The number of spins before blocking is taken as a fixed fraction of the spins required for a context switch. In our analysis, we considered two values for SpinThreshold: C and C/2, where C is the context switch time measured in spins. <p> If we consider the family of algorithms that spin for a time t before blocking, then the best deterministic on-line algorithm is the one which minimizes the expected cost under the distribution P L <ref> [KMMO89] </ref>. In practice, distributions are discrete data points. <p> Let A fl be the deterministic algorithm that minimizes the expected cost on lock-waiting time sequences (P), where (P) is generated according to a time-independent distribution P. It has been shown <ref> [KMMO89] </ref> that EC A fl ((P)) e 1 So, an algorithm that uses sample statistics of lock-waiting times in order to estimate the distribution P converges to e=(e 1) competitive behavior or better. Last three samples This algorithm is a simple but somewhat crude approximation to the optimal online algorithm. <p> They developed analytic models for lock-waiting times and studied the performance of fixed-spin algorithms under these models. Their paper presents both analytic and simulation results. Competitive algorithms were first used for paging and snoopy caches [ST85, KMRS88]. Recent work on competitive spinning strategies <ref> [KMMO89] </ref> found that the fixed spin competitive strategy costs at most twice as much as the optimal off-line algorithm and that a randomized algorithm can achieve strongly competitive ratios approaching e=(e 1) 1:58. 7 Limitations This study demonstrates the performance advantage of competitive spinning in a particular computing environment, the Firefly
Reference: [KMRS88] <author> A.R. Karlin, M.S. Manasse, L. Rudolph, and D.D. Sleator. </author> <title> Competitive Snoopy Caching. </title> <journal> Algorithmica, </journal> <volume> 3(1) </volume> <pages> 79-119, </pages> <year> 1988. </year>
Reference-contexts: Agarwal [LA91] have studied algorithms for the producer-consumer type synchronization used by "futures". They developed analytic models for lock-waiting times and studied the performance of fixed-spin algorithms under these models. Their paper presents both analytic and simulation results. Competitive algorithms were first used for paging and snoopy caches <ref> [ST85, KMRS88] </ref>.
Reference: [Knu66] <author> Donald E. Knuth. </author> <title> Additional Comments on A Problem in Concurrent Program Control. </title> <journal> Communications of the ACM, </journal> <volume> 9(5):321, </volume> <month> May </month> <year> 1966. </year>
Reference-contexts: Early work was concerned with implementing critical sections when the only atomic operations provided by hardware are memory read and write <ref> [Dij65, Knu66] </ref>. A number of software protocols have been developed, based on these operations and coherent shared memory. The main disadvantage of these approaches is their inefficiency. The most efficient approach [Lam87] still requires five writes and two reads in the absence of contention.
Reference: [Lam87] <author> Leslie Lamport. </author> <title> An Efficient Mutual Exclusion Algorithm. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(1), </volume> <year> 1987. </year>
Reference-contexts: A number of software protocols have been developed, based on these operations and coherent shared memory. The main disadvantage of these approaches is their inefficiency. The most efficient approach <ref> [Lam87] </ref> still requires five writes and two reads in the absence of contention. Greater efficiency is possible when the hardware provides more powerful atomic operations.
Reference: [LA91] <author> Beng-Hong Lim and Anant Agarwal, </author> <title> Waiting Algorithms for Synchronization in Large-Scale Multiprocessors. </title> <publisher> MIT VLSI Memo No. </publisher> <pages> 91-632 </pages>
Reference-contexts: Related work [ABLL90] proposes operating system support for moving many thread scheduling decisions into the application program. In recent work, B. Lim and A. Agarwal <ref> [LA91] </ref> have studied algorithms for the producer-consumer type synchronization used by "futures". They developed analytic models for lock-waiting times and studied the performance of fixed-spin algorithms under these models. Their paper presents both analytic and simulation results. Competitive algorithms were first used for paging and snoopy caches [ST85, KMRS88].
Reference: [MB90] <author> Jeffrey C. Mogul and Anita Borg, </author> <title> The Effect of Context Switches on Cache Performance. </title> <booktitle> In Proc. 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 75-84, </pages> <address> Santa Clara, CA April, </address> <year> 1991 </year>
Reference-contexts: That value was obtained by measuring a simple program that executed context switches as rapidly as it could. The work of Mogul and Borg <ref> [MB90] </ref> suggests that the a context switch should also take into account the increase in cache misses and bus utilization. * When there are fewer schedulable threads than processors, our analytic results still assume that spinning is costly, whereas in reality it never slows the program down. <p> We conjecture though, that on faster machines the advantages of competitive strategies will be even greater than those we observed for the following simple reason. Faster machines rely on effective caching. Since context switches reduce cache hit ratio <ref> [MB90] </ref>, it is likely that the performance impact of these algorithms will become even more pronounced, at least when compared to always-block. The thread system used in our measurements is a kernel-level implementation.
Reference: [MS87] <author> P.R. McJones and G.F. Swart. </author> <title> Evolving the UNIX System Interface to Support Multithreaded Programs. </title> <type> Research Report 21, </type> <institution> DEC Systems Research Center, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: In the rest of this section, we present the total lock-waiting distributions for each program. In the next section, we use the distributions for individual locks to compare different spinning strategies. Taos Taos is the operating system for the Firefly <ref> [MS87] </ref>. It has two component address spaces (and their associated threads of control): a "kernel" address space called the Nub and the Taos address space. The Nub resides in the system region of the VAX virtual memory, and is effectively part of every address space.
Reference: [Oust82] <author> John K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> Proc. 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: A compromise between these two methods is to combine spinning and blocking: when a thread fails to acquire a lock, it spins for some time and then blocks. Such a scheme was implemented by Ouster-hout <ref> [Oust82] </ref> in the Medusa system. More recently, it has been shown [KMMO89] that a variant of this method, where the time spent spinning is equal to a context-switch, is competitive. <p> Other work has evaluated the cost of synchronization, considering such complexities as the hardware cache-coherence algorithm, operating system implementation of threads, and the non-deterministic nature of the workload. Ousterhout <ref> [Oust82] </ref> noted that blocking should be avoided when a lock would be available in time less than a context switch. His Medusa system delayed blocking a thread for a fixed time determined by the user.
Reference: [Rov86] <author> Paul Rovner. </author> <title> Extending Modula-2 to Build Large, Integrated Systems. </title> <journal> IEEE Software, </journal> <volume> 3(6), </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: Ivy Ivy is a programmable text editor that runs in a single address space. It makes heavy use of the Trestle window system developed at SRC. Ivy was implemented in Modula-2+ <ref> [Rov86] </ref> and Tinylisp, a lisp dialect that interfaces with Modula-2+ and its run-time library. Ivy was designed for the Firefly, and it makes heavy use of concurrency.
Reference: [ST85] <author> D.D. Sleator and R.E. Tarjan. </author> <title> Amortized Efficiency of List Update and Paging Rules. </title> <journal> Communications of the ACM, </journal> <volume> 28(2) </volume> <pages> 202-208, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: Agarwal [LA91] have studied algorithms for the producer-consumer type synchronization used by "futures". They developed analytic models for lock-waiting times and studied the performance of fixed-spin algorithms under these models. Their paper presents both analytic and simulation results. Competitive algorithms were first used for paging and snoopy caches <ref> [ST85, KMRS88] </ref>.
Reference: [TSS88] <author> Charles Thacker, Lawrence Stewart, and Edwin Satterthwaite. Firefly: </author> <title> A Multiprocessor Workstation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 909-920, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In this section we describe the Firefly computer used for our experiments and give some details on how the waiting-time distribution is obtained. We then de scribe the characteristics of the distributions observed for five system and application programs. The Firefly <ref> [TSS88] </ref> is a shared-memory multiprocessor workstation developed at the Digital Equipment Corporation's Systems Research Center (SRC). The Firefly used for our experiments has seven processors: one MicroVAX 78032 (used exclusively for I/O) and 6 CVAX 78034 processors. Each processor has a floating point accelerator and a cache.
Reference: [ZLE88] <author> John Zahorjan, Edward D. Lazowska, and Derek L. Eager. </author> <title> Spinning Versus Blocking in Parallel Systems with Uncertainty. </title> <booktitle> Proc. Intern. Seminar Performance of Distributed and Parallel Systems, </booktitle> <publisher> North Holland, </publisher> <month> December </month> <year> 1988. </year>
Reference-contexts: Ousterhout [Oust82] noted that blocking should be avoided when a lock would be available in time less than a context switch. His Medusa system delayed blocking a thread for a fixed time determined by the user. Zahorjan, Lazowska, and Eager <ref> [ZLE88] </ref> assume that the decision to spin or block will be made by the user; they examine the extent to which multiprogramming and data-dependencies in an application complicate this decision. In later work [ZLE89] the same authors evaluate how the overhead of spinning is affected by various scheduling policies.
Reference: [ZLE89] <author> John Zahorjan, Edward D. Lazowska, and Derek L. Eager. </author> <title> The Effect of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Processors. </title> <type> Technical Report 89-07-03, </type> <institution> University of Washing-ton, </institution> <month> July </month> <year> 1989. </year> <note> Also IEEE Transactions on Parallel and Distributed Systems, to appear. </note>
Reference-contexts: Zahorjan, Lazowska, and Eager [ZLE88] assume that the decision to spin or block will be made by the user; they examine the extent to which multiprogramming and data-dependencies in an application complicate this decision. In later work <ref> [ZLE89] </ref> the same authors evaluate how the overhead of spinning is affected by various scheduling policies. Other studies [And89, ALL89, GT90] have compared alternatives for implementing synchronization and thread scheduling, using both modeling and empirical data.
References-found: 21

