URL: http://www.cs.cornell.edu/tve/papers/ppopp97.ps.gz
Refering-URL: 
Root-URL: 
Title: Performance Implications of Communication Mechanisms in All-Software Global Address Space Systems  
Author: Beng-Hong Lim ChiChao Chang Grzegorz Czajkowski and Thorsten von Eicken 
Address: Yorktown Heights, NY 10598  Ithaca, NY 14853  
Affiliation: T.J. Watson Research Center IBM Corporation  Department of Computer Science Cornell University  
Abstract: Global addressing of shared data simplifies parallel programming and complements message passing models commonly found in distributed memory machines. A number of programming systems have been designed that synthesize global addressing purely in software on such machines. These systems provide a number of communication mechanisms to mitigate the effect of high communication latencies and overheads. This study compares the mechanisms in two representative all-software systems: CRL and Split-C. CRL uses region-based caching while Split-C uses split-phase and push-based data transfers for optimizing communication performance. Both systems take advantage of bulk data transfers. By implementing a set of parallel applications in both CRL and Split-C, and running them on the IBM SP2, Meiko CS-2 and two simulated architectures, we find that split-phase and push-based bulk data transfers are essential for good performance. Region-based caching benefits applications with irregular structure and with sufficient temporal locality, especially under high communication latencies. However, caching also hurts performance when there is insufficient data reuse or when the size of caching granularity is mismatched with the communication granularity. We find the programming complexity of the communication mechanisms in both languages to be comparable. Based on our results, we recommend that an ideal system intended to support diverse applications on parallel platforms should incorporate the communication mechanisms in CRL and Split-C. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Agerwala, J. L. Martin, J. Mirza, D. Sadler, D. Dias, and M. Snir. </author> <title> SP2 System Architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: CRL caches program-designated memory regions to exploit temporal and spatial locality. Bulk communication occurs implicitly when using large memory regions. Split-C provides routines for bulk communication, split-phase communication and push-based communication. We compare the performance of a set of applications written in both languages on an IBM SP2 <ref> [1] </ref>, a Meiko CS-2 [9], and on two simulated machine architectures. We created the application set by taking applications that were originally written in CRL or Split-C and rewriting them to use communication mechanisms provided by the other language. <p> Meiko Active Messages bypass this interface and uses the network processor directly to optimize performance [17]. It achieves a peak bandwidth of 39 MB/s and an am request/reply round-trip time of 11 s. IBM SP2 The IBM SP2 <ref> [1] </ref> is a multicomputer comprised of RS/6000 workstation-class nodes connected by a custom network. Each node has a 66MHz POWER2 processor and a custom network adapter with a built-in coprocessor and DMA controller.
Reference: [2] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Finally, Section 6 concludes the paper. 2 Overview of CRL and Split-C Programming systems that provide global addressing without relying on hardware support may be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway [21], Orca <ref> [2] </ref> and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays [14], CC++ [4] and High Performance Fortran [8]).
Reference: [3] <author> S. Chandra, J. Larus, and A. Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI). ACM, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Other research investigates the benefits of the individual communication mechanisms in isolation. Previous research clearly shows the benefits of bulk communication. In a simulation study, Chandra and Larus <ref> [3] </ref> find that bulk transfer in message-passing systems yield a significant performance advantage over shared-memory systems. Studies on adding message passing primitives to shared-memory architectures also confirm the benefits of bulk transfers [11, 20].
Reference: [4] <author> K. M. Chandy and C. Kesselman. </author> <title> CC++: A Declarative Concurrent Object-Oriented Programming Notation. In Research Directions in Concurrent Object-Oriented Programming. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway [21], Orca [2] and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays [14], CC++ <ref> [4] </ref> and High Performance Fortran [8]). These all-software global address space programming systems provide mechanisms to tolerate high communication latencies and overheads present in off-the-shelf hardware: caching, bulk communication, split-phase communication and push-based communication. Caching replicates data coherently in order to take advantage of temporal and spatial locality.
Reference: [5] <author> C. Chang, G. Czajkowski, C. Hawblitzel, and T. von Eicken. </author> <title> Low-Latency Communication on the IBM RISC System/6000 SP2. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: The SP2 network adapter provides a user-level interface to a pair of send and receive FIFO queues synthesized by microcode running on the coprocessor on the network adapter. The Active Message layer designed at Cornell University for the SP2 <ref> [5] </ref> interfaces directly to these queues and achieves a peak bandwidth of 34 MB/s and an am request/reply round-trip time of 51 s. RMC1 and RMC2 RMC1 and RMC2 simulate an architecture that supports remote memory access (PUT/GET) and remote queue operations directly in hardware [12].
Reference: [6] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This supports our finding that caching small regions of data in CRL hurts performance. The benefits of split-phase communication for overlapping communication and computation has been demonstrated both analytically and experimentally in the LogP model <ref> [6] </ref> and with Active Messages [19]. Scales and Lam [16] demonstrate the benefits of caching and push-based communication when evaluating the SAM shared 8 bar. object programming system.
Reference: [7] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumeta, and T. von Eicken. </author> <title> Introduction to Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: This paper investigates the performance implications of the major mechanisms in all-software systems to tolerate latencies and overheads: caching, bulk communication, split-phase communication, and push-based (sender-initiated) communication. We use CRL [10] and Split-C <ref> [7] </ref>, two representative systems that provide these mechanisms. CRL caches program-designated memory regions to exploit temporal and spatial locality. Bulk communication occurs implicitly when using large memory regions. Split-C provides routines for bulk communication, split-phase communication and push-based communication. <p> relying on hardware support may be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway [21], Orca [2] and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C <ref> [7] </ref>, Global Arrays [14], CC++ [4] and High Performance Fortran [8]). These all-software global address space programming systems provide mechanisms to tolerate high communication latencies and overheads present in off-the-shelf hardware: caching, bulk communication, split-phase communication and push-based communication.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification Version 1.0, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway [21], Orca [2] and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays [14], CC++ [4] and High Performance Fortran <ref> [8] </ref>). These all-software global address space programming systems provide mechanisms to tolerate high communication latencies and overheads present in off-the-shelf hardware: caching, bulk communication, split-phase communication and push-based communication. Caching replicates data coherently in order to take advantage of temporal and spatial locality.
Reference: [9] <author> M. Homewood and M. McLaren. </author> <title> Meiko CS-2 Interconnect Elan-Elite Design. </title> <booktitle> In Proceedings of Hot Interconnects, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Bulk communication occurs implicitly when using large memory regions. Split-C provides routines for bulk communication, split-phase communication and push-based communication. We compare the performance of a set of applications written in both languages on an IBM SP2 [1], a Meiko CS-2 <ref> [9] </ref>, and on two simulated machine architectures. We created the application set by taking applications that were originally written in CRL or Split-C and rewriting them to use communication mechanisms provided by the other language. <p> This section describes each of the platforms and their Active Message layers, as well as the applications. 3.1 Hardware Platforms Meiko CS-2 The Meiko CS-2 <ref> [9] </ref> is a multicomputer where each node contains a 40 MHz three-way superscalar SuperSparc processor and a custom network adapter. The network adapter contains a special-purpose Elan network processor that is integrated with the network interface and DMA controller.
Reference: [10] <author> K. L. Johnson, M. F. Kaashoek, and D. A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, CO, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Thus, a number of mechanisms have been developed to tolerate high communication latencies and overheads in these systems. This paper investigates the performance implications of the major mechanisms in all-software systems to tolerate latencies and overheads: caching, bulk communication, split-phase communication, and push-based (sender-initiated) communication. We use CRL <ref> [10] </ref> and Split-C [7], two representative systems that provide these mechanisms. CRL caches program-designated memory regions to exploit temporal and spatial locality. Bulk communication occurs implicitly when using large memory regions. Split-C provides routines for bulk communication, split-phase communication and push-based communication. <p> Finally, Section 6 concludes the paper. 2 Overview of CRL and Split-C Programming systems that provide global addressing without relying on hardware support may be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL <ref> [10] </ref>, Midway [21], Orca [2] and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays [14], CC++ [4] and High Performance Fortran [8]).
Reference: [11] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B.- H. Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Previous research clearly shows the benefits of bulk communication. In a simulation study, Chandra and Larus [3] find that bulk transfer in message-passing systems yield a significant performance advantage over shared-memory systems. Studies on adding message passing primitives to shared-memory architectures also confirm the benefits of bulk transfers <ref> [11, 20] </ref>. Lui et al. [13] study the performance of TreadMarks, a page-based mostly-software DSM system, and find that TreadMarks' inability to combine data on different pages into a single bulk transfer impacts performance negatively.
Reference: [12] <author> B.-H. Lim, P. Heidelberger, P. Pattnaik, and M. Snir. </author> <title> Message Proxies for Efficient, Protected Communication on SMP Clusters. </title> <booktitle> In Proceedings of the 3rd International Symposium on High Performance Computer Architecture, </booktitle> <address> San Antonio, TX, </address> <month> February </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: RMC1 and RMC2 RMC1 and RMC2 simulate an architecture that supports remote memory access (PUT/GET) and remote queue operations directly in hardware <ref> [12] </ref>. Remote memory access allows a process to read and write memory in the address space of another, possibly remote, process. Remote queues allow a process to enqueue and dequeue data in the address space of another process.
Reference: [13] <author> H. Lui, S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> Message Passing Versus Distributed Shared Memory on Networks of Workstations. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, 1995. </address> <publisher> ACM. </publisher>
Reference-contexts: In a simulation study, Chandra and Larus [3] find that bulk transfer in message-passing systems yield a significant performance advantage over shared-memory systems. Studies on adding message passing primitives to shared-memory architectures also confirm the benefits of bulk transfers [11, 20]. Lui et al. <ref> [13] </ref> study the performance of TreadMarks, a page-based mostly-software DSM system, and find that TreadMarks' inability to combine data on different pages into a single bulk transfer impacts performance negatively.
Reference: [14] <author> J. Nieplocha, R. J. Harrison, and R. J. Littlefield. </author> <title> Global Arrays: A Portable 'Shared-Memory' Programming Model for Distributed Memory Computers. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 340-349, </pages> <address> Washington, DC, 1994. </address> <publisher> IEEE. </publisher>
Reference-contexts: support may be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway [21], Orca [2] and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays <ref> [14] </ref>, CC++ [4] and High Performance Fortran [8]). These all-software global address space programming systems provide mechanisms to tolerate high communication latencies and overheads present in off-the-shelf hardware: caching, bulk communication, split-phase communication and push-based communication. Caching replicates data coherently in order to take advantage of temporal and spatial locality.
Reference: [15] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working Sets, Cache Sizes and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Blocked LU decomposition This application implements insitu factorization of a dense matrix as described in <ref> [15] </ref>. The communication and computation structure of this application is as follows: The matrix is divided up into blocks distributed among processors. Every step comprises three substeps, between which processors synchronize with a barrier. First, the pivot block (I ; I ) is factored by its owner-processor.
Reference: [16] <author> D. J. Scales and M. S. Lam. </author> <title> The Design and Evaluation of a Shared Object System of Distributed Memory Machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: 6 concludes the paper. 2 Overview of CRL and Split-C Programming systems that provide global addressing without relying on hardware support may be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway [21], Orca [2] and SAM <ref> [16] </ref>) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays [14], CC++ [4] and High Performance Fortran [8]). <p> This supports our finding that caching small regions of data in CRL hurts performance. The benefits of split-phase communication for overlapping communication and computation has been demonstrated both analytically and experimentally in the LogP model [6] and with Active Messages [19]. Scales and Lam <ref> [16] </ref> demonstrate the benefits of caching and push-based communication when evaluating the SAM shared 8 bar. object programming system.
Reference: [17] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The custom network is comprised of two 4-ary fat-trees. Meiko's network processor provides a user-level interface to directly read from and write to the address space of a process on a remote node. Meiko Active Messages bypass this interface and uses the network processor directly to optimize performance <ref> [17] </ref>. It achieves a peak bandwidth of 39 MB/s and an am request/reply round-trip time of 11 s. IBM SP2 The IBM SP2 [1] is a multicomputer comprised of RS/6000 workstation-class nodes connected by a custom network.
Reference: [18] <author> J. P. Singh, A. Gupta, and J. L. Hennessy. </author> <title> Implications of Hierarchical N-Body Techniques for Multiprocessor Architecture. </title> <journal> In ACM Transactions on Computer Systems, </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: The Split-C version (water/sc) issues atomic reads and writes to access and update the remote molecules. Barnes-Hut This application simulates the evolution of a system of bodies under the influence of gravitational forces using the hier 4 archical N body algorithm proposed by Barnes and Hut <ref> [18] </ref>. The computation is highly irregular and the communication is relatively fine-grained: a distributed oct-tree is built up with the bodies at the leaves. Each tree node is less than 150 bytes. The algorithm traverses many pointer chains making the remote access pattern quite irregular.
Reference: [19] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th International Symposium in Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: To study the effectiveness of these mechanisms, our investigation uses CRL and Split-C, two representative programming systems from each of the two groups. Since CRL and Split-C are both extensions of C, compiled with a common compiler (gcc), and depend on a common communication layer (Active Messages <ref> [19] </ref>), performance differences between the two can be isolated to the different communication mechanisms used by each system. 2.1 CRL CRL is an all-software DSM system that relies on the programmer to identify regions as logical units of caching (for example, one may designate a matrix column as a region). <p> This supports our finding that caching small regions of data in CRL hurts performance. The benefits of split-phase communication for overlapping communication and computation has been demonstrated both analytically and experimentally in the LogP model [6] and with Active Messages <ref> [19] </ref>. Scales and Lam [16] demonstrate the benefits of caching and push-based communication when evaluating the SAM shared 8 bar. object programming system.
Reference: [20] <author> S. Woo, J. P. Singh, and J. Hennessy. </author> <title> The Performance Advantages of Integrating Message Passing in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI). ACM, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Previous research clearly shows the benefits of bulk communication. In a simulation study, Chandra and Larus [3] find that bulk transfer in message-passing systems yield a significant performance advantage over shared-memory systems. Studies on adding message passing primitives to shared-memory architectures also confirm the benefits of bulk transfers <ref> [11, 20] </ref>. Lui et al. [13] study the performance of TreadMarks, a page-based mostly-software DSM system, and find that TreadMarks' inability to combine data on different pages into a single bulk transfer impacts performance negatively.
Reference: [21] <author> M. J. Zekauskas, W. A. Sawdon, and B. N. Bershad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Finally, Section 6 concludes the paper. 2 Overview of CRL and Split-C Programming systems that provide global addressing without relying on hardware support may be classified into two groups: all-software distributed shared memory (DSM) systems that provide cache-coherent access to globally shared objects/data (e.g., CRL [10], Midway <ref> [21] </ref>, Orca [2] and SAM [16]) and systems that provide primitives to transfer shared data using global pointers and arrays (e.g., Split-C [7], Global Arrays [14], CC++ [4] and High Performance Fortran [8]).
References-found: 21

