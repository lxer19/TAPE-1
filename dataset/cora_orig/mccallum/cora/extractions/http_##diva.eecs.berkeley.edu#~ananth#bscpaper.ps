URL: http://diva.eecs.berkeley.edu/~ananth/bscpaper.ps
Refering-URL: http://diva.eecs.berkeley.edu/~ananth/
Root-URL: http://www.cs.berkeley.edu
Title: The Common Randomness Capacity of a Pair of Independent Binary Symmetric Channels  
Author: S. Venkatesan V. Anantharam zx 
Address: Berkeley.  Cory Hall,  Berkeley, Berkeley, CA 94720.  
Affiliation: Cornell University and U.C. Berkeley. Univ. of California,  Dept. of EECS, U.C.  
Note: Research supported by NSF IRI 9005849, IRI 9310670, NCR 9422513, and the AT&T Foundation.  Address all correspondence to the second author: 570  
Date: 13 August 1995  
Abstract: We study the following problem: two agents are connected to each other by independent binary symmetric channels of crossover probabilities p and q. They wish to generate common randomness by communicating interactively over the two channels. Neither agent has access to any external random sources, so that any randomness generated must come from the noise on the two channels. We show that it is possible to generate common randomness in this situation at a rate (in bits per step) of R fl (p; q) = min fh(p) + h(q); 2 h(p) h(q)g. We also prove a strong converse which establishes R fl (p; q) as the common randomness "capacity" of this pair of channels. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ahlswede and I. Csiszar. </author> <title> Common randomness in information theory and cryptography part I: Secret sharing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39(No. 4), </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: However, no secrecy constraints are imposed, i.e., the random outputs generated need not be kept secret from any eavesdroppers. The results proved here are not implied by those of <ref> [1] </ref>, because both channels here are noisy and constrained in capacity. Imagine an agent Alice at one terminal, and an agent Bob at the other. Let p be the crossover probability of the channel from Alice to Bob, and q 2 that of the channel from Bob to Alice. <p> log K: (4) &gt;From (3) and (4), it follows that min fH (I); H (J)g (1 2) log K h () (1 ) log (1 + ): Thus, if is small, each agent generates a random output whose distribution is close to uniform on [K]. 1.3 Main result Fix 2 <ref> [0; 1] </ref>. For each n 1, define K p;q (n; ) to be the largest K such that there exists an (n; K; ) deterministic protocol for the BSC (p; q).
Reference: [2] <author> R. Ahlswede and G. Dueck. </author> <title> Identification in the presence of feedback a discovery of new capacity formulas. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference: [3] <author> R. Ahlswede and G. Dueck. </author> <title> Identification via channels. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 35(No. 1), </volume> <month> January </month> <year> 1989. </year>
Reference-contexts: 1 Introduction There are several situations in which common randomness available at distant terminals plays a significant role. For example, in identification theory ([2], <ref> [3] </ref>, [4]), the amount of common randomness available to both transmitter and receiver essentially determines the maximum achievable identification rate. Also, in the theory of communication complexity ([7], [9]), it is known that common randomness available to two communicating agents can significantly reduce the complexity of computing certain functions.
Reference: [4] <author> R. Ahlswede and B. Verboven. </author> <title> On identification via multiway channels with feedback. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 37(No. 5), </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction There are several situations in which common randomness available at distant terminals plays a significant role. For example, in identification theory ([2], [3], <ref> [4] </ref>), the amount of common randomness available to both transmitter and receiver essentially determines the maximum achievable identification rate. Also, in the theory of communication complexity ([7], [9]), it is known that common randomness available to two communicating agents can significantly reduce the complexity of computing certain functions.
Reference: [5] <author> R.G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> John Wiley, </publisher> <year> 1968. </year>
Reference-contexts: Proof: Standard. See, e.g., <ref> [5] </ref>. 2 11 Definition 3.2 Let u 2 f0; 1g t and U f0; 1g t .
Reference: [6] <author> J.H.B. Kemperman. </author> <title> Strong converses for a general memoryless channel with feedback. </title> <journal> Trans. 6th Prague Conf. Information Theory, Stat. </journal> <note> Dec. Fct's and Rand. Proc., </note> <year> 1973. </year>
Reference-contexts: This is a simple generalization, to the "interactive" situation, of the main idea in Kemperman's proof of the strong converse to the coding theorem for DMCs with perfect feedback <ref> [6] </ref>.
Reference: [7] <author> L. Lovasz. </author> <title> Communication complexity: A survey. </title> <editor> In B.H. Korte et al., editors, </editor> <title> Paths, Flows and VLSI layout. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference: [8] <author> U.M. Maurer. </author> <title> Secret key agreement by public discussion from common information. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 39(No. 3), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Also, in the theory of communication complexity ([7], [9]), it is known that common randomness available to two communicating agents can significantly reduce the complexity of computing certain functions. And, in cryptography ([1], <ref> [8] </ref>), if two agents share a random key about which an eavesdropper has no information, they can use it to achieve secure communication between them (through encryption of messages).
Reference: [9] <author> A. Orlitsky and A. El Gamal. </author> <title> Communication complexity. </title> <editor> In Y. Abu-Mostafa, editor, </editor> <booktitle> Complexity in Information Theory. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: For example, in identification theory ([2], [3], [4]), the amount of common randomness available to both transmitter and receiver essentially determines the maximum achievable identification rate. Also, in the theory of communication complexity ([7], <ref> [9] </ref>), it is known that common randomness available to two communicating agents can significantly reduce the complexity of computing certain functions.
References-found: 9

