URL: http://www.aic.nrl.navy.mil/~aha/slides/ml-tutorial-bib.ps
Refering-URL: http://www.aic.nrl.navy.mil/~aha/slides.html
Root-URL: 
Title: Machine Learning: An Annotated Bibliography for the 1995 AI Statistics Tutorial on Machine Learning (Version 1)  
Author: David W. Aha Shavlik, J. W., Dietterich, T. G. 
Keyword: Annotated Bibliography  
Note: As of this writing, several textbooks already exist on the topic of machine learning. However, none of them have received popular acclaim, and the majority of professors teaching courses on this subject seem to use the Readings:  (Eds.) (1990). Readings in machine learning.  Morgan Kaufmann. I divided the tutorial into four sections. I'll do the same here.  
Address: Washington, DC 20375  San Mateo, CA:  
Affiliation: Navy Center for Applied Research on Artificial Intelligence Naval Research Laboratory  
Email: aha@aic.nrl.navy.mil  
Phone: (202) 767-9006  
Web: http://www.aic.nrl.navy.mil/ aha  
Date: January 24, 1995  
Abstract: This is a brief annotated bibliography that I wanted to make available to the attendees of my Machine Learning tutorial at the 1995 AI & Statistics Workshop. These slides are available in my WWW pages under slides. Please contact me if you have any questions. Please also note the date (listed above) on which this was most recently updated. While I plan to make occasional updates to this file, it is bound to be outdated quickly. Also, I apologize for the lack of figures, but my time on this project is limited and the slides should compensate. Finally, this bibliography is, by definition, This book is now out of date. Both Pat Langley and Tom Mitchell are in the process of writing textbooks on this subject, but we're still waiting for them. Until then, I suggest looking at both the Readings and the recent ML conference proceedings (both International and European). There are also a few introductory papers on this subject, though I haven't gotten around to putting them in here yet. However, Pat Langley and Dennis Kibler (1988) have written a good paper on ML as an empirical science, and Pat has written several editorials of use to the ML author (Langley 1986; 1987; 1990). incomplete, and I've left out many other references that may be of some use.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aamodt, A., & Plaza, E. </author> <year> (1994). </year> <title> Case-based reasoning: Foundational issues, methodological variations, and system approaches. </title> <journal> AI Communicationsi, </journal> <volume> 7, </volume> <pages> 39-59. </pages>
Reference: <author> Aha, D. W. </author> <year> (1989). </year> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 387-391). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Incremental constructive induction: An instance-based approach. </title> <booktitle> In Proceed--ings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 117-121). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Such worst-case analyses are useful, but have little practical consequences. Thus, many researchers have restricted the set of probability distributions considered, focussed on a specific algorithm, or both <ref> (e.g., Albert & Aha, 1991) </ref>. Researchers in this field have addressed issues on noise (Angluin & Laird, 1988), feature selection (Littlestone, 1988), learning with equivalence queries (Angluin, 1990), and on a large range of other topics.
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 1-10). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <editor> In D. W. Aha (Ed.) </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1995). </year> <title> A comparative evaluation of sequential feature selection algorithms. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics. </booktitle> <address> Ft. Lauderdale, FL: </address> <note> Unpublished. </note>
Reference: <author> Aha, D. W., & Goldstone, R. L. </author> <year> (1992). </year> <title> Concept learning and flexible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 534-539). </pages> <address> Bloomington, </address> <publisher> IN: Lawrence Erlbaum. </publisher>
Reference: <author> Aha, D. W., & Kibler, D. </author> <year> (1989). </year> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 794-799). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Albert, M. K., & Aha, D. W. </author> <year> (1991). </year> <title> Analyses of instance-based learning algorithms. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 553-558). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Such worst-case analyses are useful, but have little practical consequences. Thus, many researchers have restricted the set of probability distributions considered, focussed on a specific algorithm, or both <ref> (e.g., Albert & Aha, 1991) </ref>. Researchers in this field have addressed issues on noise (Angluin & Laird, 1988), feature selection (Littlestone, 1988), learning with equivalence queries (Angluin, 1990), and on a large range of other topics.
Reference: <author> Angluin, D. </author> <year> (1990). </year> <title> Negative results for equivalence queries. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 121-150. </pages>
Reference-contexts: Thus, many researchers have restricted the set of probability distributions considered, focussed on a specific algorithm, or both (e.g., Albert & Aha, 1991). Researchers in this field have addressed issues on noise (Angluin & Laird, 1988), feature selection (Littlestone, 1988), learning with equivalence queries <ref> (Angluin, 1990) </ref>, and on a large range of other topics.
Reference: <author> Angluin, D., & Laird, P. </author> <year> (1988). </year> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 343-370. </pages>
Reference-contexts: Such worst-case analyses are useful, but have little practical consequences. Thus, many researchers have restricted the set of probability distributions considered, focussed on a specific algorithm, or both (e.g., Albert & Aha, 1991). Researchers in this field have addressed issues on noise <ref> (Angluin & Laird, 1988) </ref>, feature selection (Littlestone, 1988), learning with equivalence queries (Angluin, 1990), and on a large range of other topics.
Reference: <author> Birnbaum, L. A., & Collins, G. C. (Ed.). </author> <year> (1991). </year> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Of course, these are extreme viewpoints. Although some good work on cognitive sciences has appeared in the machine learning literature (e.g., one session in <ref> (Birnbaum & Collins, 1991) </ref>, a recent Machine Learning journal issue (Pazzani, 1994), etc.) and a few papers appear on cognitively plausible machine learning research in each IJCAI, AAAI, and other proceedings, the solid majority of machine learning research is not of this ilk. <p> A sub-workshop was devoted to this at the 1991 IMLW <ref> (Birnbaum & Collins, 1991) </ref> and probably elsewhere since.
Reference: <author> Bradshaw, G. L. </author> <year> (1986). </year> <title> Learning by disjunctive spanning. </title> <editor> In T. M. Mitchell, J. G. Carbonell, & R. S. Michalski (Eds.), </editor> <title> Machine learning: A guide to current research. </title> <address> Boston, MA: </address> <publisher> Kluwer. </publisher>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: To some extent this has been true, although ML research interests are obviously not a subset of those in statistics. Nevertheless, there is a long history in the statistics literature of research on decision trees, predicting numeric values, clustering, and modelling concepts <ref> (e.g., Breiman, Freidman, Olshen, & Stone, 1984) </ref>. These are all of interest to machine learning researchers. However, statisticians typically do not involve themselves with first-order concept description representations, nor pursue interests in cognitive plausibility. ML is firmly under the umbrella of AI; statistics is not.
Reference: <author> Brodley, C. E. </author> <year> (1993). </year> <title> Addressing the selective superiority problem: Automatic algorithm/model class selection. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 17-24). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Brodley, C. E. </author> <year> (1995?). </year> <title> Recursive automatic bias selection for classifier construction. </title> <note> To appear in Machine Learning. </note>
Reference: <author> Buntine, W., & Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-86. </pages>
Reference: <author> Cain, T. </author> <year> (1991). </year> <title> The DUCTOR: A theory revision system for propositional domains. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 485-489). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Most work in ML focuses on induction of concept descriptions or clusters, although explanation-based learning originally focussed on deduction (Mitchell, Keller, & Kedar-Cabelli, 1986). Several researchers have also focussed on abductive methods (e.g., Cohen, 1994). Naturally, some research efforts combine these approaches in some productive manner <ref> (e.g., Cain, 1991) </ref>. One way to distinguish these approaches is based on a simple example. The inputs to the system consist of a universally quantified rule, with an antecedent and consequent, and some facts, which can be used to instantiate either component of the rule.
Reference: <author> Cain, T., Pazzani, M. J., & Silverstein, G. </author> <year> (1991). </year> <title> Using domain knowledge to influence similarity judgement. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 191-202). </pages> <address> Washington, DC: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Most work in ML focuses on induction of concept descriptions or clusters, although explanation-based learning originally focussed on deduction (Mitchell, Keller, & Kedar-Cabelli, 1986). Several researchers have also focussed on abductive methods (e.g., Cohen, 1994). Naturally, some research efforts combine these approaches in some productive manner <ref> (e.g., Cain, 1991) </ref>. One way to distinguish these approaches is based on a simple example. The inputs to the system consist of a universally quantified rule, with an antecedent and consequent, and some facts, which can be used to instantiate either component of the rule.
Reference: <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 28-36). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Cestnik, B., & Bratko, I. </author> <year> (1991). </year> <title> On estimating proabilities in tree pruning. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 138-150). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Cestnik, B., Kononenko, I., & Bratko, I. </author> <year> (1987). </year> <title> ASSISTANT-86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in machine learning. </booktitle> <address> Bled, Yugoslavia: </address> <publisher> Sigma Press. </publisher>
Reference: <author> Carbonell, J. G., Michalski, R. S., & Mitchell, T. M. </author> <year> (1983). </year> <title> An overview of machine learning. </title>
Reference-contexts: Supervised learning from examples involves the induction of concept descriptions from examples. The learning task is not limited to classification, but could also be optimization (i.e., learning a numeric function) or could involve learning with more complicated representations. Early work is described in the first Machine Learning volume <ref> (Carbonell, Michalski, & Mitchell, 1983) </ref>. The Readings (Shavlik & Dietterich, 1990) contains several seminal papers on this subject. Inputs include a set or sequence of annotated examples (i.e., with their concept attribute labels).
Reference: <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <pages> AutoClass: </pages>
References-found: 25

