URL: http://www.cs.utexas.edu/users/lorenzo/corsi/cs395t/96F/papers/493.ps
Refering-URL: http://www.cs.utexas.edu/users/lorenzo/corsi/cs395t/96F/fs.html
Root-URL: 
Title: Implementation and Performance of Integrated Application-Controlled Caching, Prefetching and Disk Scheduling  
Author: Pei Cao Edward W. Felten Anna R. Karlin Kai Li 
Abstract: This paper presents the design and implementation of a file system that integrates application-controlled caching, prefetching and disk scheduling. We use a two-level cache management strategy. The kernel uses the LRU-SP policy [CFL94a] to allocate blocks to processes, and each process uses the controlled-aggressive policy, an algorithm previously shown in a theoretical sense to be near-optimal, for managing its cache. Each process then improves its disk access latency by submitting its prefetches in batches and schedules the requests in each batch to optimize disk access performance. Our measurements show that this combination of techniques greatly improves the performance of the file system. Average running time is reduced by 26% for single-process workloads, and by 46% for multi-process workloads. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [CB92] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref>, to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching.
Reference: [CD85] <author> Hong-Tai Chou and David J. DeWitt. </author> <title> An evaluation of buffer management strategies for relational da tabase systems. </title> <booktitle> In Proceedings of the Eleventh International Conference on Ver y Large Databases, </booktitle> <pages> pages 127-141, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: These papers demonstrated the benefits of prefetch-ing. However they did not address the interaction between caching and prefetching and did not investi 11 gate the combined cache management problem. The database community has long studied access patterns and buffer replacement <ref> [Sto81, CD85, OOW93] </ref> and prefetching [CKV93, PZ91] policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [CFKL95] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proc. 1995 ACM SIGMETRICS, </booktitle> <pages> pages 188-197, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Making the decision earlier may hurt performance because new and possibly better replacement opportunities open up as the program proceeds. In a recent paper, we studied application-controlled file caching and prefetching from a theoretical point of view <ref> [CFKL95] </ref>. We presented four rules that any optimal integrated algorithm must follow and then showed that a simple algorithm derived using these rules has performance that is provably close to optimal. <p> Given some amount of lookahead in the file block reference pattern, what is the optimal combined prefetching and caching strategy? In <ref> [CFKL95] </ref> we derived several theoretical results about this problem. These results are summarized here; readers should refer to [CFKL95] for more details. <p> Given some amount of lookahead in the file block reference pattern, what is the optimal combined prefetching and caching strategy? In <ref> [CFKL95] </ref> we derived several theoretical results about this problem. These results are summarized here; readers should refer to [CFKL95] for more details. We abstracted the real problem to the following theoretical model: * The cache holds K blocks. * The entire sequence of future block references is known in advance. * The program references one block per time unit. <p> In <ref> [CFKL95] </ref>, we observed that any optimal prefetch-ing algorithm must follow the following four rules. We state the rules here without proof. Rule 1: Optimal Prefetching Every prefetch should bring into the cache the next block in the reference stream that is not in the cache. <p> In addition, it always does optimal replacement. We proved in <ref> [CFKL95] </ref> that the running time under controlled-aggressive is never more than 1 + F=K times the optimal running 1 In our previous paper this policy was called aggressive to denote that it is the most aggressive of all reasonable policies. <p> We chose to rename it here because we felt the name aggressive is misleading when taken out of context. 3 time. In our case (i.e. prefetching from disk to buffer cache), F=K is typically less than 0.02 <ref> [CFKL95] </ref>, so controlled-aggressive is very close to optimal. Simulations described in our previous paper [CFKL95] confirm that controlled-aggressive does well for real reference streams. <p> In our case (i.e. prefetching from disk to buffer cache), F=K is typically less than 0.02 <ref> [CFKL95] </ref>, so controlled-aggressive is very close to optimal. Simulations described in our previous paper [CFKL95] confirm that controlled-aggressive does well for real reference streams. Although controlled-aggressive relies on perfect knowledge of future access, it is not hard to see how to adapt it to the case where only limited-lookahead knowledge is available. <p> The combina 3 The exception is din with a 6.4MB cache. This behavior is due to a relatively small fetch cost compared with CPU time, and is predicted in our simulation study in <ref> [CFKL95] </ref>. 10 tions were chosen to represent combinations of ac-cess patterns: sequential large files and small files (cs1+cs3), sequential large files and random large files (cs1+psel2), small files and small files (cs3+gli), and small files and random large files (cs3+psel1). <p> In [CFL94a] we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In [CFL94b] we implemented LRU-SP without prefetching or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. In <ref> [CFKL95] </ref> we presented theoretical results on policies for integrating prefetching with application-controlled caching. We introduced the controlled-aggressive cache management policy, proved that it is theoretically close to optimal, and verified its good performance by simulation. <p> This paper improves on our previous work in several ways. * This paper is the first to present policies for integrated prefetching and caching in the multi-process case | [CFL94a] and [CFL94b] did not consider prefetching, and <ref> [CFKL95] </ref> discussed only the single-process case. * This paper presents the first working implementation of our ideas for integrating prefetching and application-controlled caching | our previous implementation described in [CFL94b] did not address prefetching, and our studies of prefetch-ing in [CFKL95] were based on theoretical anal ysis and simulation only. * <p> case | [CFL94a] and [CFL94b] did not consider prefetching, and <ref> [CFKL95] </ref> discussed only the single-process case. * This paper presents the first working implementation of our ideas for integrating prefetching and application-controlled caching | our previous implementation described in [CFL94b] did not address prefetching, and our studies of prefetch-ing in [CFKL95] were based on theoretical anal ysis and simulation only. * This paper is the first to consider the interaction of disk scheduling with application-controlled caching and prefetching. * The experiments described in this paper are the first real measurements of the interaction between application-controlled caching and prefetching. 7 Conclusions We
Reference: [CFL94a] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Application-controlled file caching policies. </title> <booktitle> In Proc. USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 171-182, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Care must 1 be taken to ensure that performance is not adversely affected by this uncertainty. Moreover, global allocation of cache space to competing processes should be fair. Our implementation uses a variant of the LRU-SP policy (described in <ref> [CFL94a] </ref>) in order to address these concerns. We have measured the file system performance with a suite of I/O intensive applications on a DEC 5000/240 workstation. We considered both sequential and concurrent executions of these applications with various combinations of application-controlled caching, prefetching and disk scheduling. <p> Kernel's List A C B D B After Before block A is at the "least recently used" end of the list. The process who owns A decides to replace block B. The figure shows the list before and after the replacement decision. 3.2 The LRU-SP Allocation Policy In <ref> [CFL94a] </ref> and [CFL94b] we presented the LRU-SP (LRU with Swapping and Placeholders) policy, which solves this problem for the case of application-controlled caching without prefetching and disk scheduling. LRU-SP can easily be extended to incorporate prefetching and disk scheduling; we describe the extended version here. <p> That process is asked to give up one of its blocks; it is up to the process which of its blocks to evict. In order to prevent certain bad behaviors (which are described in more detail in <ref> [CFL94a] </ref>), two modifications to the "LRU" list, swapping and placehold-ers (figure 2), are used. <p> Placeholders are data structures which remember when a process chose to evict a non "LRU" block; if this choice turns out to be foolish, the placeholder can be used to keep the foolish process from benefiting from its bad choice. Further details of LRU-SP appear in <ref> [CFL94a] </ref> and [CFL94b]. 3.3 Summary We now have a complete strategy for file cache management integrating application-controlled caching, prefetching, and disk scheduling. We use a two-level strategy, in which the kernel allocates cache blocks to processes, and each process manages its own blocks. <p> Although they considered more complex policies than we do, they did not consider the question of how to integrate disk scheduling with other performance improvement techniques like application-controlled caching. The work described in this paper builds on our pre vious work. In <ref> [CFL94a] </ref> we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In [CFL94b] we implemented LRU-SP without prefetching or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. <p> In <ref> [CFL94a] </ref> we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In [CFL94b] we implemented LRU-SP without prefetching or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. In [CFKL95] we presented theoretical results on policies for integrating prefetching with application-controlled caching. We introduced the controlled-aggressive cache management policy, proved that it is theoretically close to optimal, and verified its good performance by simulation. <p> This paper improves on our previous work in several ways. * This paper is the first to present policies for integrated prefetching and caching in the multi-process case | <ref> [CFL94a] </ref> and [CFL94b] did not consider prefetching, and [CFKL95] discussed only the single-process case. * This paper presents the first working implementation of our ideas for integrating prefetching and application-controlled caching | our previous implementation described in [CFL94b] did not address prefetching, and our studies of prefetch-ing in [CFKL95] were based
Reference: [CFL94b] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proc. First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 165-178, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The process who owns A decides to replace block B. The figure shows the list before and after the replacement decision. 3.2 The LRU-SP Allocation Policy In [CFL94a] and <ref> [CFL94b] </ref> we presented the LRU-SP (LRU with Swapping and Placeholders) policy, which solves this problem for the case of application-controlled caching without prefetching and disk scheduling. LRU-SP can easily be extended to incorporate prefetching and disk scheduling; we describe the extended version here. <p> Placeholders are data structures which remember when a process chose to evict a non "LRU" block; if this choice turns out to be foolish, the placeholder can be used to keep the foolish process from benefiting from its bad choice. Further details of LRU-SP appear in [CFL94a] and <ref> [CFL94b] </ref>. 3.3 Summary We now have a complete strategy for file cache management integrating application-controlled caching, prefetching, and disk scheduling. We use a two-level strategy, in which the kernel allocates cache blocks to processes, and each process manages its own blocks. <p> ACM implements the application-controlled caching interface calls and acts as a proxy for the user-level managers. ACM and BUF do roughly the same jobs that they did in our system that implemented only application-controlled file caching <ref> [CFL94b] </ref>. In the current implementation, ACM and BUF have been modified in order to integrate with prefetching. The PCM module implements the application prefetching interface and disk scheduling. It interfaces with the BUF, ACM and disk driver modules to optimize prefetching performance. <p> what to prefetch, but too short to give precise information about optimal replacement. (Recall that the optimal replacement decision is to replace that block whose next reference is furthest in the future.) Replacement decisions can be aided by other general cache replacement primitives that applications provide to the ACM module <ref> [CFL94b] </ref>. The PCM does, however, attempt to ensure that the ACM does not replace blocks that will be referenced before a block that is currently being prefetched. <p> The work described in this paper builds on our pre vious work. In [CFL94a] we introduced the LRU-SP kernel allocation policy and simulated its performance on applications. In <ref> [CFL94b] </ref> we implemented LRU-SP without prefetching or disk scheduling, and showed by experiment that it works well in practice, confirming the simulation results of [CFL94a]. In [CFKL95] we presented theoretical results on policies for integrating prefetching with application-controlled caching. <p> This paper improves on our previous work in several ways. * This paper is the first to present policies for integrated prefetching and caching in the multi-process case | [CFL94a] and <ref> [CFL94b] </ref> did not consider prefetching, and [CFKL95] discussed only the single-process case. * This paper presents the first working implementation of our ideas for integrating prefetching and application-controlled caching | our previous implementation described in [CFL94b] did not address prefetching, and our studies of prefetch-ing in [CFKL95] were based on theoretical <p> to present policies for integrated prefetching and caching in the multi-process case | [CFL94a] and <ref> [CFL94b] </ref> did not consider prefetching, and [CFKL95] discussed only the single-process case. * This paper presents the first working implementation of our ideas for integrating prefetching and application-controlled caching | our previous implementation described in [CFL94b] did not address prefetching, and our studies of prefetch-ing in [CFKL95] were based on theoretical anal ysis and simulation only. * This paper is the first to consider the interaction of disk scheduling with application-controlled caching and prefetching. * The experiments described in this paper are the first real measurements
Reference: [Che87] <author> David Cheriton. </author> <title> Effective use of large RAM diskless workstations with the V virtual memory system. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <year> 1987. </year>
Reference-contexts: not improve the elapsed time | the reordering of requests and resultant dis-overlapping of CPU computation and disk access must be taken into account. prefetching, and the changes in context switches and the interleaving of requests. 6 Related Work There have been many studies on caching in file systems (e.g. <ref> [SGN85, Che87, SBK + 85, HKM + 88, NWO88, KS92] </ref>), but these investigations were not primarily concerned with the performance impact of different caching and prefetching policies.
Reference: [CKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref>, to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching.
Reference: [CKV93] <author> Kenneth M. Curewitz, P. Krishnan, and Jef-frey Scott Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proc. 1993 ACM-SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems [PG94, GA94, TD90] to databases <ref> [CKV93, PZ91] </ref> and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. Recently there have been a number of research projects on prefetching in file systems. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93] </ref>. Few of these studies considered the interaction between prefetching and caching. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching [PG94] showed that prefetching using hints from applications is an effective way of exploiting I/O concurrency in disk arrays. <p> These papers demonstrated the benefits of prefetch-ing. However they did not address the interaction between caching and prefetching and did not investi 11 gate the combined cache management problem. The database community has long studied access patterns and buffer replacement [Sto81, CD85, OOW93] and prefetching <ref> [CKV93, PZ91] </ref> policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [EK89] <author> Carla Schlatter Ellis and David Kotz. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <booktitle> In Proc. 1989 Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 306-314, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments. There are a number of papers on prefetching in parallel I/O systems <ref> [EK89, WYT93] </ref>. Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem [WYT93].
Reference: [GA94] <author> Jim Griffioen and Randy Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In Proc. of USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. While it may seem at first glance that this is easy, this is not the case. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems <ref> [PG94, GA94, TD90] </ref> to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. <p> Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching [PG94] showed that prefetching using hints from applications is an effective way of exploiting I/O concurrency in disk arrays. Griffioen and Appleton's work <ref> [GA94] </ref> tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. These papers demonstrated the benefits of prefetch-ing. However they did not address the interaction between caching and prefetching and did not investi 11 gate the combined cache management problem.
Reference: [Gra91] <author> Jim Gray. </author> <title> The Benchmark Handbook. </title> <publisher> Mor-gan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: If there is no index file, the query will scan the relation tuple file sequentially to find the matching tuples. We used a 200,000 tuple relation from a scaled-up Wisconsin benchmark <ref> [Gra91] </ref>. There is an index on attribute unique1, which is uniquely random in the range 1-200,000.
Reference: [Hag87] <author> Robert B. Hagmann. </author> <title> Reimplementing the cedar file system using logging and group commit. </title> <booktitle> In Proceedings of 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 155-162, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Recently several research projects have tried to improve file system performance in a number of other ways, including log-structured file systems <ref> [Hag87, RO91, SBadCS93] </ref>, disk block clustering [MK91, SS95], and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block looka-head or large I/O units as the primary prefetching techniques.
Reference: [HKM + 88] <author> John H. Howard, Michael Kazar, Sherri G. Menees, David A. Nichols, M. Satya-narayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distribuetd file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: not improve the elapsed time | the reordering of requests and resultant dis-overlapping of CPU computation and disk access must be taken into account. prefetching, and the changes in context switches and the interleaving of requests. 6 Related Work There have been many studies on caching in file systems (e.g. <ref> [SGN85, Che87, SBK + 85, HKM + 88, NWO88, KS92] </ref>), but these investigations were not primarily concerned with the performance impact of different caching and prefetching policies.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Text searches in CScope read source files sequentially. Our policy for these searches is to use MRU replacement, and to prefetch files in their order of access. Dinero [din] Dinero is a cache simulator written by Mark Hill and used in Hennessy and Patterson's architecture textbook <ref> [HP90] </ref>. The distribution package for the course material includes the simulator and several program trace files.
Reference: [KS92] <author> James J. Kistler and M. Satyanarayanan. </author> <title> Disconnected operation in the Coda file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 1-25, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: not improve the elapsed time | the reordering of requests and resultant dis-overlapping of CPU computation and disk access must be taken into account. prefetching, and the changes in context switches and the interleaving of requests. 6 Related Work There have been many studies on caching in file systems (e.g. <ref> [SGN85, Che87, SBK + 85, HKM + 88, NWO88, KS92] </ref>), but these investigations were not primarily concerned with the performance impact of different caching and prefetching policies.
Reference: [MK91] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performacne from a UNIX file system. </title> <booktitle> In Proc. of 1991 Winter USENIX Symposium, </booktitle> <pages> pages 33-43, </pages> <year> 1991. </year>
Reference-contexts: Recently several research projects have tried to improve file system performance in a number of other ways, including log-structured file systems [Hag87, RO91, SBadCS93], disk block clustering <ref> [MK91, SS95] </ref>, and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block looka-head or large I/O units as the primary prefetching techniques. They did not address how to integrate prefetching and disk scheduling with application-controlled file caching.
Reference: [Mog94] <author> Jeffrey C. Mogul. </author> <title> A better update policy. </title> <booktitle> In Proc. of 1994 Summer USENIX Technical Conference, </booktitle> <pages> pages 99-111, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Recently several research projects have tried to improve file system performance in a number of other ways, including log-structured file systems [Hag87, RO91, SBadCS93], disk block clustering [MK91, SS95], and delayed writeback <ref> [Mog94] </ref>. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block looka-head or large I/O units as the primary prefetching techniques. They did not address how to integrate prefetching and disk scheduling with application-controlled file caching.
Reference: [MW94] <author> Udi Manber and Sun Wu. GLIMPSE: </author> <title> A tool to search through entire file systems. </title> <booktitle> In Proc. of USENIX Winter 1994 Technical Conference, </booktitle> <pages> pages 23-32, </pages> <month> January </month> <year> 1994. </year> <month> 13 </month>
Reference-contexts: Dinero reads the trace file sequentially for each simulation. Thus, the caching strategy is MRU on the trace file. For prefetching, we simply pass the trace file name to the kernel. 7 Glimpse [gli] Glimpse is a text information retrieval system <ref> [MW94] </ref>. It builds approximate indices for words to allow relatively fast search with small index files. We took a snapshot of news articles in several comp.* newsgroups on May 22, 1994, consisting of about 40 MBytes of text in all.
Reference: [NWO88] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: not improve the elapsed time | the reordering of requests and resultant dis-overlapping of CPU computation and disk access must be taken into account. prefetching, and the changes in context switches and the interleaving of requests. 6 Related Work There have been many studies on caching in file systems (e.g. <ref> [SGN85, Che87, SBK + 85, HKM + 88, NWO88, KS92] </ref>), but these investigations were not primarily concerned with the performance impact of different caching and prefetching policies.
Reference: [OOW93] <author> Elizabeth J. O'Neil, Patrick E. O'Neil, and Gerhard Weikum. </author> <title> The LRU-K page replacement algorithm for database disk buffer ing. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 297-306, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These papers demonstrated the benefits of prefetch-ing. However they did not address the interaction between caching and prefetching and did not investi 11 gate the combined cache management problem. The database community has long studied access patterns and buffer replacement <ref> [Sto81, CD85, OOW93] </ref> and prefetching [CKV93, PZ91] policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [PG94] <author> R. Hugo Patterson and Garth A. Gibson. </author> <title> Exposing I/O concurrency witn informed prefetching. </title> <booktitle> In Proc. Third Intl. Conf. on Parallel and Distributed Information Systems, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. While it may seem at first glance that this is easy, this is not the case. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems <ref> [PG94, GA94, TD90] </ref> to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. <p> There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching <ref> [PG94] </ref> showed that prefetching using hints from applications is an effective way of exploiting I/O concurrency in disk arrays. Griffioen and Appleton's work [GA94] tries to predict future file accesses based on past accesses using "probability graphs," and prefetches accordingly. These papers demonstrated the benefits of prefetch-ing.
Reference: [PZ91] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> Fido: A cache that learns to fetch. </title> <booktitle> In Proc. of 17th Intl. Conf. on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems [PG94, GA94, TD90] to databases <ref> [CKV93, PZ91] </ref> and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. Recently there have been a number of research projects on prefetching in file systems. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93] </ref>. Few of these studies considered the interaction between prefetching and caching. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching [PG94] showed that prefetching using hints from applications is an effective way of exploiting I/O concurrency in disk arrays. <p> These papers demonstrated the benefits of prefetch-ing. However they did not address the interaction between caching and prefetching and did not investi 11 gate the combined cache management problem. The database community has long studied access patterns and buffer replacement [Sto81, CD85, OOW93] and prefetching <ref> [CKV93, PZ91] </ref> policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [RL92] <author> Anne Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 38-50, </pages> <month> Octo-ber </month> <year> 1992. </year>
Reference-contexts: Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref>, to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching.
Reference: [RO91] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-15, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Recently several research projects have tried to improve file system performance in a number of other ways, including log-structured file systems <ref> [Hag87, RO91, SBadCS93] </ref>, disk block clustering [MK91, SS95], and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block looka-head or large I/O units as the primary prefetching techniques.
Reference: [SBadCS93] <author> Margo Seltzer, Keith Bostic, and Marshall Kirk McKusick an d Carl Staelin. </author> <title> An implementation of a log-structured file system for UNIX. </title> <booktitle> In Proceedings of 1993 Winter USENIX, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Recently several research projects have tried to improve file system performance in a number of other ways, including log-structured file systems <ref> [Hag87, RO91, SBadCS93] </ref>, disk block clustering [MK91, SS95], and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block looka-head or large I/O units as the primary prefetching techniques.
Reference: [SBK + 85] <author> R. Sandberg, D. Boldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> In Summer Usenix Conference Proceedings, </booktitle> <pages> pages 119-130, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: not improve the elapsed time | the reordering of requests and resultant dis-overlapping of CPU computation and disk access must be taken into account. prefetching, and the changes in context switches and the interleaving of requests. 6 Related Work There have been many studies on caching in file systems (e.g. <ref> [SGN85, Che87, SBK + 85, HKM + 88, NWO88, KS92] </ref>), but these investigations were not primarily concerned with the performance impact of different caching and prefetching policies.
Reference: [SCO90] <author> Margo Seltzer, Peter Chen, and John Ousterhout. </author> <title> Disk scheduling revisited. </title> <booktitle> In Proc. of USENIX Winter 1990 Technical Conference, </booktitle> <pages> pages 313-324, </pages> <year> 1990. </year>
Reference-contexts: The details will be addressed in section 4. 2.2 Incorporating Disk Scheduling The final technique we use is disk scheduling. Because of the physical attributes of disks, careful scheduling of disk accesses can provide significant improvement in performance <ref> [SCO90] </ref>. Without prefetching, scheduling opportunities only come from asynchronous I/O activities or multiple processes. Prefetching provides new opportunities for disk scheduling, because prefetch requests can be generated in groups. We employ a simple scheduling heuristic: limited batch scheduling. The prefetcher waits until it has B disk requests to issue. <p> They did not address how to integrate prefetching and disk scheduling with application-controlled file caching. Our work is complementary to these approaches: in other words, with application-controlled file caching, these approaches will improve file system performance even more. Several detailed studies of disk scheduling have been done <ref> [SCO90, WGP94] </ref>. These studies typically considered a wide variety of scheduling policies under timesharing Unix workloads. Although they considered more complex policies than we do, they did not consider the question of how to integrate disk scheduling with other performance improvement techniques like application-controlled caching.
Reference: [SGN85] <author> M. D. Schroeder, D. K. Gifford, and R. M. Needham. </author> <title> A caching file system for a programmer's workstation. </title> <booktitle> Operating System Review, </booktitle> <pages> pages 19(5) 35-50, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: not improve the elapsed time | the reordering of requests and resultant dis-overlapping of CPU computation and disk access must be taken into account. prefetching, and the changes in context switches and the interleaving of requests. 6 Related Work There have been many studies on caching in file systems (e.g. <ref> [SGN85, Che87, SBK + 85, HKM + 88, NWO88, KS92] </ref>), but these investigations were not primarily concerned with the performance impact of different caching and prefetching policies.
Reference: [Smi78] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(12) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. While it may seem at first glance that this is easy, this is not the case. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref>, to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching.
Reference: [SS95] <author> Margo Seltzer and Keith A. Smith. </author> <title> File system logging versus clustering: A performance comparision. </title> <booktitle> In Proceedings of 1995 Summer USENIX, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Recently several research projects have tried to improve file system performance in a number of other ways, including log-structured file systems [Hag87, RO91, SBadCS93], disk block clustering <ref> [MK91, SS95] </ref>, and delayed writeback [Mog94]. Most of these papers still assume global LRU as the basic cache replacement policies and sequential one-block looka-head or large I/O units as the primary prefetching techniques. They did not address how to integrate prefetching and disk scheduling with application-controlled file caching.
Reference: [Sto81] <author> Michael Stonebraker. </author> <title> Operating system support for database management. </title> <journal> Communications of the ACM, v. </journal> <volume> 24, no. 7, </volume> <pages> pages 412-418, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: These papers demonstrated the benefits of prefetch-ing. However they did not address the interaction between caching and prefetching and did not investi 11 gate the combined cache management problem. The database community has long studied access patterns and buffer replacement <ref> [Sto81, CD85, OOW93] </ref> and prefetching [CKV93, PZ91] policies. However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments.
Reference: [TD90] <author> Carl D. Tait and Dan Duchamp. </author> <title> Detectino and exploitation of file working sets. </title> <type> Technical Report CUCS-050-90, </type> <institution> Computer Science Dept., Columbia University, </institution> <year> 1990. </year>
Reference-contexts: The next section will describe how to extend this integration to the multiple-process case. We focus on applications that can predict their future access patterns. Research has shown that such predictions are often possible in practice <ref> [PG94, GA94, TD90, Smi78] </ref>. 2.1 Integrating Prefetching with Caching Since both application-controlled caching and prefetching rely on knowledge of future access patterns, it is natural to try to integrate them. While it may seem at first glance that this is easy, this is not the case. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems <ref> [PG94, GA94, TD90] </ref> to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. <p> Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures [RL92, CB92, CKP91, Smi78, TE93], to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses <ref> [TD90, PZ91, CKV93] </ref>. Few of these studies considered the interaction between prefetching and caching. Recently there have been a number of research projects on prefetching in file systems. Patterson's Transparent-Informed Prefetching [PG94] showed that prefetching using hints from applications is an effective way of exploiting I/O concurrency in disk arrays.
Reference: [TE93] <author> Dean M. Tullsen and Susan J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In Proceedings of 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 278-288, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Prefetch-ing has also been studied extensively in various domains, ranging from uni-processor and multiprocessor architectures <ref> [RL92, CB92, CKP91, Smi78, TE93] </ref>, to file systems [PG94, GA94, TD90] to databases [CKV93, PZ91] and beyond. There have also been many studies focusing on how to predict future accesses from past accesses [TD90, PZ91, CKV93]. Few of these studies considered the interaction between prefetching and caching. <p> File systems, on the other hand, can change their cache management algorithms freely and can spare more cycles for calculating a good replacement or prefetching decision, as the potential savings are substantial. On the other hand, Tullsen and Eggers <ref> [TE93] </ref> showed that thrashing is a problem when prefetching in bus-based multiprocessor caches, suggesting that the "Do No Harm" rule applies in those systems as well.
Reference: [WGP94] <author> Bruck L. Worthington, Gregory R. Ganger, and Yale N. Patt. </author> <title> Scheduling for modern disk drives and non-random workloads. </title> <type> Technical Report CSE-TR-194-94, </type> <institution> Dept. of Electrical Engineering and Computer Science, University of Michigan, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Nevertheless, studies show that fetching in order of logical block numbers tends to work well <ref> [WGP94] </ref>. We chose the value of B empirically. In general, if B is too small, disk scheduling will be ineffective because there is not much latitude to reorder fetches. <p> They did not address how to integrate prefetching and disk scheduling with application-controlled file caching. Our work is complementary to these approaches: in other words, with application-controlled file caching, these approaches will improve file system performance even more. Several detailed studies of disk scheduling have been done <ref> [SCO90, WGP94] </ref>. These studies typically considered a wide variety of scheduling policies under timesharing Unix workloads. Although they considered more complex policies than we do, they did not consider the question of how to integrate disk scheduling with other performance improvement techniques like application-controlled caching.
Reference: [WYT93] <author> Kun-Ling Wu, Philip S. Yu, and James Z. Teng. </author> <title> Performance comparison of thrashing control policies for concurrent mergesorts with parallel prefetching. </title> <booktitle> In Proc. of 1993 ACM SIGMETRICS, </booktitle> <pages> pages 171-182, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, most of these studies focus on buffer management and prefetching in database storage management systems rather than in file systems, and they did not address the integration of caching, prefetching and disk scheduling in multi-process environments. There are a number of papers on prefetching in parallel I/O systems <ref> [EK89, WYT93] </ref>. Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem [WYT93]. <p> Although our work focuses on prefetching with a single disk or server, the "Do No Harm" and "First Opportunity" principles apply to prefetching algorithms in the parallel context as well. We believe these principles are important to avoid the thrashing problem <ref> [WYT93] </ref>. Prefetching in uni-processor and multi-processor computer architectures is similar to prefetching in file systems. However, in these systems there is little flexibility in cache management, as the cache is usually direct-mapped or has very limited associativity.
References-found: 36

