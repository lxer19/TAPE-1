URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/main_ml.ps.gz
Refering-URL: http://www-cad.eecs.berkeley.edu/HomePages/aml/publications/index.html
Root-URL: http://www.cs.berkeley.edu
Title: Machine Learning,  Using the Minimum Description Length Principle to Infer Reduced Ordered Decision Graphs  
Author: ARLINDO L. OLIVEIRA IST/INESC, R. Alves Redol , Lisboa, Portugal ALBERTO SANGIOVANNI-VINCENTELLI Editor: Jude Shavlik 
Keyword: Inductive learning, MDL principle, decision trees  
Address: Berkeley, Berkeley CA 94720  
Affiliation: Department of EECS, UC  
Note: c 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 12,  
Email: aml@inesc.pt  alberto@eecs.berkeley.edu  
Date: 1-30 (1995)  
Abstract: We propose an algorithm for the inference of decision graphs from a set of labeled instances. In particular, we propose to infer decision graphs where the variables can only be tested in accordance with a given order and no redundant nodes exist. This type of graphs, reduced ordered decision graphs, can be used as canonical representations of Boolean functions and can be manipulated using algorithms developed for that purpose. This work proposes a local optimization algorithm that generates compact decision graphs by performing local changes in an existing graph until a minimum is reached. The algorithm uses Rissanen's minimum description length principle to control the tradeoff between accuracy in the training set and complexity of the description. Techniques for the selection of the initial decision graph and for the selection of an appropriate ordering of the variables are also presented. Experimental results obtained using this algorithm in two sets of examples are presented and analyzed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. </author> <year> (1986). </year> <title> Classifying learnable geometric concepts with the Vapnik-Chervonenkis dimension. </title> <booktitle> In Proc. 18th Annual ACM Symp. Theory Comput., </booktitle> <address> Berkeley, CA, </address> <pages> pages 273-282. </pages> <publisher> ACM Press. </publisher>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Inform. Proc. Lett., </journal> <volume> 24 </volume> <pages> 377-380. </pages>
Reference: <author> Brace, K., Rudell, R., & Bryant, R. </author> <year> (1989). </year> <title> Efficient implementation of a BDD package. </title> <booktitle> In Proceedings of the Design Automation Conference, </booktitle> <address> Anaheim, CA, </address> <pages> pages 40-45. </pages> <publisher> ACM Press. </publisher>
Reference-contexts: Packages that manipulate RODG's are widely available and have become the most commonly used tool for discrete-function manipulation in the logic synthesis community (Brayton et al., 1990). Some of these packages are restricted to Boolean functions <ref> (Brace et al., 1989) </ref>. In this case, each non-terminal node has exactly two DECISION GRAPHS OF MINIMAL DESCRIPTION LENGTH 7 outgoing edges. Other packages (Kam & Brayton, 1990) can accept multi-valued attributes directly, thereby allowing each non-terminal node to have an arbitrary number of outgoing edges. <p> The algorithms described in Sections 4.1 through 4.3 are combined in a straightforward way as the pseudo-code in Table 3 shows. These algorithms were implemented in a system called smog (Selection of Minimal Ordered Graphs) that uses the CMU RODG package <ref> (Brace et al., 1989) </ref> to perform the standard RODG manipulations. 5. Experiments To evaluate the effectiveness of the approach presented here, we tested the algorithm on a set of problems that have been addressed previously in the machine learning literature and that are either widely available or easily reproducible.
Reference: <author> Brayton, R., Hachtel, G., McMullen, C., & Sangiovanni-Vincentelli, A. </author> <year> (1984). </year> <title> Logic Minimization Algorithms for VLSI Synthesis. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> Hingham, </month> <title> MA. DECISION GRAPHS OF MINIMAL DESCRIPTION LENGTH 29 Brayton, </title> <editor> R. K., Hachtel, G. D., & Vincentelli, A. L. S. </editor> <year> (1990). </year> <title> Multilevel logic synthesis. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 264-300. </pages>
Reference-contexts: OLIVEIRA AND ALBERTO SANGIOVANNI-VINCENTELLI * Minimal consistent DNF: the minimum DNF expression consistent with the training set data is found using a two-level logic minimizer <ref> (Brayton et al., 1984) </ref>, espresso. This expression is then used to classify unseen instances. For these problems, the generalization accuracy was tested on a set of instances that also includes the instances used for training.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA. </address>
Reference: <author> Bryant, R. E. </author> <year> (1986). </year> <title> Graph-based algorithms for Boolean function manipulation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 35(8) </volume> <pages> 677-691. </pages>
Reference-contexts: A decision graph is called reduced if no two nodes exist that branch exactly in the same way, and it is never the case that all outgoing edges of a given node terminate in the same node <ref> (Bryant, 1986) </ref>. A graph that is both reduced and ordered is called a reduced ordered decision graph (RODG). For a given ordering of the variables, reduced ordered decision graphs are canonical representations of Boolean functions, a characteristic that makes them specially useful for the manipulation of this type of functions. <p> Manipulating Discrete Functions Using RODGs For a given ordering of the variables, reduced ordered decision graphs are a canonical representation for functions in that domain <ref> (Bryant, 1986) </ref>. This means that given a function f : X 1 fi X 2 ::: fi X N ! f0; 1g and an ordering of the variables, there is one and only one representation for a function f .
Reference: <author> Casella, G. & Berger, R. L. </author> <year> (1990). </year> <title> Statistical Inference. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, CA. </address>
Reference-contexts: The comparison was performed using the commonly accepted 10-fold cross-validation methodology and the statistical significance of the results was evaluated using a one-tailed paired t-test <ref> (Casella & Berger, 1990) </ref>. All problems with continuously valued attributes were discretized using the entropy-based method of Fayyad and Irani (1993).
Reference: <author> Coudert, O., Berthet, C., & Madre, J. C. </author> <year> (1989). </year> <title> Verification of synchronous sequential machines based on symbolic execution. In Proceedings of the Workshop on Automatic Verification Methods for Finite State Systems, Grenoble, </title> <booktitle> France. Volume 407 of Lecture Notes in Computer Science, </booktitle> <pages> pages 365-373. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: the function implemented by a decision tree derived from the training set data using a standard decision-tree algorithms. * The RODG that realizes the function implemented by a decision tree defined over a new set of variables obtained using constructive-induction techniques. * The RODG obtained by applying the restrict heuristic <ref> (Coudert et al., 1989) </ref> to the function obtained by listing all positive instances in the training set. We now describe in more detail how each one of these techniques can be used to obtain an RODG that serves as the starting point for the local optimization algorithm. 4.1.1. <p> The decision graph for x 7 is the one returned by the procedure. 4.1.3. Initialization Using the Restrict Operator The third way to initialize the algorithm is to use algorithms for RODG reduction like the restrict operator <ref> (Coudert et al., 1989) </ref>. This RODG operator can be used to obtain a more compact RODG representation for an incompletely specified function. The restrict operator belongs to a family of heuristics (Shiple et al., 1994) that generate a small RODG by merging, in a bottom-up fashion, nodes in an RODG.
Reference: <author> Dougherty, J., Kohavi, R., & Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, </address> <pages> pages 194-202. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The comparison was performed using the commonly accepted 10-fold cross-validation methodology and the statistical significance of the results was evaluated using a one-tailed paired t-test (Casella & Berger, 1990). All problems with continuously valued attributes were discretized using the entropy-based method of Fayyad and Irani (1993). Recent results <ref> (Dougherty et al., 1995) </ref> have shown that the performance of induction algorithms and, in particular, of C4.5, does not degrade and may even improve if this discretization method is applied to problems with continuously valued attributes.
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artifical Intelligence, </booktitle> <address> Chambery, France, </address> <pages> pages 1022-1027. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, S. J. & Supowit, K. J. </author> <year> (1990). </year> <title> Finding the optimal variable ordering for binary decision diagrams. </title> <journal> IEEE Trans. Comput., </journal> <volume> 39 </volume> <pages> 710-713. </pages>
Reference-contexts: Regrettably, selecting the optimal ordering for a given function is NP-complete (Tani et al., 1993) and cannot be solved exactly in most cases. For this reason, and because it is a problem of high practical interest in logic synthesis, many heuristic algorithms have been proposed for this problem <ref> (Friedman & Supowit, 1990) </ref>. In our setting, the problem is even more complex because we wish to select the ordering that minimizes the final RODG and this ordering may not be the same as the one that minimizes the RODG obtained after the initialization step.
Reference: <author> Goldman, J. A. </author> <year> (1994). </year> <title> Machine learning: A comparative study of pattern theory and C4.5. </title> <type> Technical Report WL-TR-94-1102, </type> <institution> Wright Laboratory, USAF, WL/AART, WPAFB, OH. </institution>
Reference-contexts: This group tested smog in a benchmark assembled for the purpose of evaluating the effectiveness of a set of learning algorithms <ref> (Goldman, 1994) </ref>. The reader is referred to this reference for a complete description of the methodology adopted and the set of problems addressed. Each one of the problems is defined by a noise-free concept defined over a space of eight Boolean attributes.
Reference: <author> Ishiura, N., Sawada, H., & Yajima, S. </author> <year> (1991). </year> <title> Minimization of binary decision diagrams based on exchanges of variables. </title> <booktitle> In Proceedings of the International Conference on Computer Aided Design, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 472-475. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Kam, T. & Brayton, R. </author> <year> (1990). </year> <title> Multi-valued decision diagrams. </title> <institution> UC Berkeley Tech. Report ERL M90/125, EECS Dept., Berkeley, </institution> <address> CA. </address>
Reference-contexts: Some of these packages are restricted to Boolean functions (Brace et al., 1989). In this case, each non-terminal node has exactly two DECISION GRAPHS OF MINIMAL DESCRIPTION LENGTH 7 outgoing edges. Other packages <ref> (Kam & Brayton, 1990) </ref> can accept multi-valued attributes directly, thereby allowing each non-terminal node to have an arbitrary number of outgoing edges.
Reference: <author> Kohavi, R. </author> <year> (1994). </year> <title> Bottom-up induction of oblivious read-once decision graphs: Strengths and limitations. </title> <booktitle> In Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Tahoe City, CA, </address> <pages> pages 613-618. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Li, M. & Vitanyi, P. M. B. </author> <year> (1994). </year> <title> An Introduction to Kolmogorov Complexity. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address>
Reference-contexts: Minimizing Message Length and Encoding of RODGs The tradeoff between hypothesis simplicity and accuracy in the training data is controlled using the minimum description length (MDL) principle of Rissanen (1978,1986). This very general principle can be derived from algorithmic complexity theory in a simple and elegant way <ref> (Li & Vitanyi, 1994) </ref>. <p> Our experiments have shown that three of them are particularly effective, although the relative size of the RODGs generated by different methods varies strongly from problem to problem <ref> (Oliveira, 1994) </ref>. <p> in problems that have many attributes and where the positive and negative instances can be separated by a small subset of the available attributes this heuristic tends to generate RODGs that depend only on a small subset of the attributes, namely the ones that come first in the ordering selected <ref> (Oliveira, 1994) </ref>. 4.2. Reducing an RODG After creating the initial RODG using one of the methods described above, a local optimization algorithm will be used to reduce its size. The search for an RODG of minimum total description length is performed in steps. <p> The ReplacePair procedure is more expensive and requires O (r 3 m) operations. By using bit-packing techniques the algorithm can be used to reduce RODGs with hundreds or a few thousands of nodes. For very large problems the decision graph obtained from the initialization phase may be too large <ref> (Oliveira, 1994) </ref>. In this case, the local optimization algorithm may take a long time to reduce this graph. For these problems, the algorithm can be run in a fast mode that initializes the graph with a decision tree that is not fully consistent with the training set data. <p> section, the comparison between different algorithms was made using three sets of problems with distinct origins: the set of 8 problems proposed by Pagallo and Haussler (1990), a set of 5 problems selected because they are known to accept small decision graph solutions but require comparably larger decision tree representations <ref> (Oliveira, 1994) </ref>, and a set of 13 problems from the U.C. Irvine repository (Murphy & Aha, 1991). 5.1.1. <p> A filled circle means that the difference observed is statistically significant at a confidence level of 95%. The three groups of problems shown have been proposed in (Pagallo & Haussler, 1990), <ref> (Oliveira, 1994) </ref> and (Murphy & Aha, 1991), respectively. <p> This comparison shows the effectiveness of the algorithms for graph reduction, and of the reordering algorithm in this particular case. The reduction of the description size observed in this problem is typical of the reduction observed in the majority of the cases where smog outperforms C4.5 <ref> (Oliveira, 1994) </ref>. For problems that are defined by sets of continuous attributes, little or no gain was obtained by the use of the decision graphs.
Reference: <author> Mahoney, J. J. & Mooney, R. J. </author> <year> (1991). </year> <title> Initializing ID5R with a domain theory: some negative results. </title> <type> Technical Report 91-154, </type> <institution> CS Dept., University of Texas at Austin, Austin, TX. </institution>
Reference: <author> Meinel, C. </author> <year> (1989). </year> <title> Modified Branching Programs and Their Computational Power. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address>
Reference-contexts: A decision graph is similar to a decision tree except that the underlying graph may have re-convergent paths. Decision graphs are commonly described as Boolean decision diagrams in the logic synthesis literature and as branching programs in computer science theory work <ref> (Meinel, 1989) </ref>. Figure 1 shows a decision tree and a decision graph for the function f : f0; 1g 4 ! f0; 1g defined by f = x 1 x 2 + x 3 x 4 .
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1991). </year> <title> Repository of Machine Learning Databases Machine readable data repository. </title> <institution> University of California, Irvine. </institution>
Reference-contexts: Irvine repository <ref> (Murphy & Aha, 1991) </ref>. 5.1.1. Experimental Setup For each of the problems selected, we compared the performance of C4.5, a popular decision tree induction algorithm (Quinlan, 1993), with smog, the system that implements the algorithms described in Section 4. <p> A filled circle means that the difference observed is statistically significant at a confidence level of 95%. The three groups of problems shown have been proposed in (Pagallo & Haussler, 1990), (Oliveira, 1994) and <ref> (Murphy & Aha, 1991) </ref>, respectively.
Reference: <author> Oliveira, A. L. </author> <year> (1994). </year> <title> Inductive Learning by Selection of Minimal Complexity Representations. </title> <type> PhD thesis, </type> <institution> UC Berkeley, Berkeley, CA. </institution> <note> Also available as UCB/ERL Technical Report M94/97. </note>
Reference-contexts: Our experiments have shown that three of them are particularly effective, although the relative size of the RODGs generated by different methods varies strongly from problem to problem <ref> (Oliveira, 1994) </ref>. <p> in problems that have many attributes and where the positive and negative instances can be separated by a small subset of the available attributes this heuristic tends to generate RODGs that depend only on a small subset of the attributes, namely the ones that come first in the ordering selected <ref> (Oliveira, 1994) </ref>. 4.2. Reducing an RODG After creating the initial RODG using one of the methods described above, a local optimization algorithm will be used to reduce its size. The search for an RODG of minimum total description length is performed in steps. <p> The ReplacePair procedure is more expensive and requires O (r 3 m) operations. By using bit-packing techniques the algorithm can be used to reduce RODGs with hundreds or a few thousands of nodes. For very large problems the decision graph obtained from the initialization phase may be too large <ref> (Oliveira, 1994) </ref>. In this case, the local optimization algorithm may take a long time to reduce this graph. For these problems, the algorithm can be run in a fast mode that initializes the graph with a decision tree that is not fully consistent with the training set data. <p> section, the comparison between different algorithms was made using three sets of problems with distinct origins: the set of 8 problems proposed by Pagallo and Haussler (1990), a set of 5 problems selected because they are known to accept small decision graph solutions but require comparably larger decision tree representations <ref> (Oliveira, 1994) </ref>, and a set of 13 problems from the U.C. Irvine repository (Murphy & Aha, 1991). 5.1.1. <p> A filled circle means that the difference observed is statistically significant at a confidence level of 95%. The three groups of problems shown have been proposed in (Pagallo & Haussler, 1990), <ref> (Oliveira, 1994) </ref> and (Murphy & Aha, 1991), respectively. <p> This comparison shows the effectiveness of the algorithms for graph reduction, and of the reordering algorithm in this particular case. The reduction of the description size observed in this problem is typical of the reduction observed in the majority of the cases where smog outperforms C4.5 <ref> (Oliveira, 1994) </ref>. For problems that are defined by sets of continuous attributes, little or no gain was obtained by the use of the decision graphs.
Reference: <author> Oliveira, A. L. & Vincentelli, A. S. </author> <year> (1993). </year> <title> Learning complex Boolean functions : Algorithms and applications. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> Denver, </address> <publisher> CO, </publisher> <pages> pages 911-918. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Initialization Using a Constructive Induction Algorithm Constructive induction algorithms create new complex attributes by combining existing attributes in ways that make the description of the concept easier. The fulfringe constructive induction algorithm <ref> (Oliveira & Vincentelli, 1993) </ref> identifies patterns near the fringes of the decision tree and uses them to build new attributes. The idea was first proposed by Pagallo and Haussler (1990) and further developed by other authors.
Reference: <author> Oliveira, A. L. & Vincentelli, A. S. </author> <year> (1995). </year> <title> Inferring reduced ordered decision graphs of minimal description length. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA, </address> <pages> pages 421-429. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Oliver, J. J. </author> <year> (1993). </year> <title> Decision graphs an extension of decision trees. </title> <type> Technical Report 92/173, </type> <institution> Monash University, Clayton, Victori, Australia. </institution>
Reference-contexts: the generalization performed by a decision tree induced from data suffers because of two well known problems: the replication of subtrees required to represent some concepts and the rapid fragmentation of the training set data when attributes that can take a high number of values are tested at a node <ref> (Oliver, 1993) </ref>. Reduced ordered decision graphs, the particular representation addressed in this work, are decision graphs that have no redundant nodes and where the tests performed on the variables follow some fixed order for all paths in the graph.
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100. </pages>
Reference-contexts: A circle in a given row marks the algorithm that obtained the lowest average error in the given problem. A filled circle means that the difference observed is statistically significant at a confidence level of 95%. The three groups of problems shown have been proposed in <ref> (Pagallo & Haussler, 1990) </ref>, (Oliveira, 1994) and (Murphy & Aha, 1991), respectively.
Reference: <author> Pearl, J. </author> <year> (1978). </year> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> Journal of General Systems, </journal> <volume> 4 </volume> <pages> 255-264. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: As mentioned, this work draws heavily on techniques developed by other authors in the machine learning and logic synthesis fields. From machine learning, we use many of the techniques developed for the induction of decision trees <ref> (Quinlan, 1986) </ref> as well as the constructive induction algorithms first studied by Pagallo and Haus-sler (1990). <p> Since the attributes are Boolean and we are not concerned with algorithms for pruning the tree, we can use a relatively straightforward DECISION GRAPHS OF MINIMAL DESCRIPTION LENGTH 11 algorithm to generate the decision tree. A simplified version of ID3 <ref> (Quinlan, 1986) </ref> is used to generate the decision tree. The decision tree is built in a recursive way, by selecting, at each point, the attribute to be tested as the one that provides the larger amount of information about the class of the instances that reached that node.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <note> 30 ARLINDO L. </note> <author> OLIVEIRA AND ALBERTO SANGIOVANNI-VINCENTELLI Quinlan, J. R. & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <journal> Inform. Comput., </journal> <volume> 80(3) </volume> <pages> 227-248. </pages>
Reference-contexts: Irvine repository (Murphy & Aha, 1991). 5.1.1. Experimental Setup For each of the problems selected, we compared the performance of C4.5, a popular decision tree induction algorithm <ref> (Quinlan, 1993) </ref>, with smog, the system that implements the algorithms described in Section 4. The comparison was performed using the commonly accepted 10-fold cross-validation methodology and the statistical significance of the results was evaluated using a one-tailed paired t-test (Casella & Berger, 1990).
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Rissanen, J. </author> <year> (1986). </year> <title> Stochastic complexity and modeling. </title> <journal> Annals of Statististics, </journal> <volume> 14(3) </volume> <pages> 1080-1100. </pages>
Reference: <author> Rudell, R. </author> <year> (1993). </year> <title> Dynamic variable ordering for ordered binary decision diagrams. </title> <booktitle> In Pro-ceeddings of the International Conference on Computer Aided Design, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 42-47. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The approach described in this paper is radically different. First, the problem of selecting an appropriate ordering for the variables is solved using one of the highly effective heuristic algorithms for variable reordering proposed in the logic synthesis literature <ref> (Rudell, 1993) </ref>. Second, the algorithm that derives a compact decision graph uses many of the techniques developed in the logic synthesis and machine learning communities in its search for compact decision graphs. <p> In our setting, the problem is even more complex because we wish to select the ordering that minimizes the final RODG and this ordering may not be the same as the one that minimizes the RODG obtained after the initialization step. Our implementation uses the sift algorithm <ref> (Rudell, 1993) </ref> for dynamic RODG ordering.
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ, </address> <pages> pages 259-265. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Shannon, C. E. </author> <year> (1938). </year> <title> A symbolic analysis of relay and switching circuits. </title> <journal> Transactions AIEE, </journal> <volume> 57 </volume> <pages> 713-723. </pages>
Reference-contexts: For example, f = ab is equivalent to f = Ite (a; b; 0) and f = a b is equivalent to f = Ite (a; b; b). Shannon's decomposition theorem <ref> (Shannon, 1938) </ref> states that f = vf v + vf v (A.2) where v is a variable and f v and f v represent f evaluated at v = 1 and v = 0, respectively.
Reference: <author> Shiple, T. R., Hojati, R., Vincentelli, A. L. S., & Brayton, R. K. </author> <year> (1994). </year> <title> Heuristic minimization of BDDs using don't cares. </title> <booktitle> In Proc. Design Automat. Conf., </booktitle> <address> San Diego, CA, </address> <pages> pages 225-231. </pages> <publisher> ACM Press. </publisher>
Reference-contexts: This RODG operator can be used to obtain a more compact RODG representation for an incompletely specified function. The restrict operator belongs to a family of heuristics <ref> (Shiple et al., 1994) </ref> that generate a small RODG by merging, in a bottom-up fashion, nodes in an RODG. The merging of nodes is performed in a way that keeps the RODG consistent with the training set data. 14 ARLINDO L.
Reference: <author> Tani, S., Hamaguchi, K., & Yajima, S. </author> <year> (1993). </year> <title> The complexity of the optimal variable ordering problems of shared binary decision diagrams. </title> <booktitle> In Proceedings of the 4th International Symposium on Algorithms and Computation, Hong Kong, </booktitle> <pages> pages 389-98. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Selecting the Best Ordering The selection of a good ordering for the variables is of critical importance if the goal is to obtain a compact RODG. Regrettably, selecting the optimal ordering for a given function is NP-complete <ref> (Tani et al., 1993) </ref> and cannot be solved exactly in most cases. For this reason, and because it is a problem of high practical interest in logic synthesis, many heuristic algorithms have been proposed for this problem (Friedman & Supowit, 1990).
Reference: <author> Wallace, C. S. & Patrick, J. D. </author> <year> (1993). </year> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 7-22. </pages>
Reference: <author> Yang, D. S., Rendell, L., & Blix, G. </author> <year> (1991). </year> <title> Fringe-like feature construction: A comparative study and a unifying scheme. </title> <booktitle> In Proceedings of the Eight International Conference on Machine Learning, </booktitle> <address> Evanston, IL, </address> <pages> pages 223-227. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The fulfringe constructive induction algorithm (Oliveira & Vincentelli, 1993) identifies patterns near the fringes of the decision tree and uses them to build new attributes. The idea was first proposed by Pagallo and Haussler (1990) and further developed by other authors. A constructive induction algorithm of this family, dcfringe <ref> (Yang et al., 1991) </ref> identifies the patterns shown in the first two rows of Figure 3. Fulfringe identifies all the patterns used by dcfringe but also identifies additional ones that correspond to functions poorly correlated with the input variables.
References-found: 36

