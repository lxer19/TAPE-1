URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/97.Heeman-Allen.Eurospeech97.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/heeman/papers.html
Root-URL: 
Email: heeman@lannion.cnet.fr  james@cs.rochester.edu  
Title: INCORPORATING POS TAGGING INTO LANGUAGE MODELING  
Author: Peter A. Heeman James F. Allen 
Address: Technopole Anticipa 2 Avenue Pierre Marzin 22301 Lannion Cedex, France.  Rochester NY 14627, USA  
Affiliation: France Tlcom CNET  Department of Computer Science University of Rochester  
Abstract: Language models for speech recognition tend to concentrate solely on recognizing the words that were spoken. In this paper, we redetne the speech recognition problem so that its goal is to tnd both the best sequence of words and their syntactic role (partofspeech) in the utterance. This is a necessary trst step towards tightening the interaction between speech recognition and natural language understanding. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. R. Bahl, P. F. Brown, P. V. deSouza, and R. L. Mercer. </author> <title> A treebased statistical language model for natural lan guage speech recognition. </title> <journal> IEEE Transactions on Acous tics, Speech, and Signal Processing, </journal> <volume> 36(7), </volume> <year> 1989. </year>
Reference-contexts: Our approach is to use the decision tree learning algorithm <ref> [1, 2, 3] </ref>, which uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node. <p> After the tree is grown, the heldout dataset is used to smooth the probabilities of each node with its parent <ref> [1] </ref>. 3.1 Word and POS Classitcation Trees To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. [4] into a binary classitcation tree. <p> This means that the false case has been split into two separate nodes, which could cause unnecessary data fragmentation. Unnecessary data fragmentation can be avoided by al lowing composite questions. Bahl et al. <ref> [1] </ref> introduced a simple but effective approach for constructing composite questions.
Reference: [2] <author> E. Black, F. Jelinek, J. Lafferty, R. Mercer, and S. Roukos. </author> <title> Decision tree models applied to the labeling of text with partsofspeech. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 117121, </pages> <year> 1992. </year>
Reference-contexts: Our approach is to use the decision tree learning algorithm <ref> [1, 2, 3] </ref>, which uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node. <p> Unlike other work that uses classitcation trees as the basis for the questions used by a decision tree (e.g. <ref> [2] </ref>), we treat the word identities as a further retnement of the POS tags. This approach has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities will not be viewed as separate sources of information. <p> We grow the classitcation tree by starting with a unique class for each word and each POS tag that 1 A notable exception is the work of Black et al. <ref> [2] </ref>, who use a decision tree to learn the probability distributions for POS tagging.
Reference: [3] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classitcation and Regression Trees. </title> <publisher> Wadsworth & Brooks, Monterrey, </publisher> <address> CA, </address> <year> 1984. </year>
Reference-contexts: Our approach is to use the decision tree learning algorithm <ref> [1, 2, 3] </ref>, which uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node.
Reference: [4] <author> P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer. </author> <title> Classbased ngram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4), </volume> <year> 1992. </year>
Reference-contexts: Due to sparseness of data, one must detne equivalence classes amongst the contexts W 1;i1 , which can be done by limiting the context to an ngram language model [11] and also by grouping words into words classes <ref> [4] </ref>. Several attempts have been made to incorporate shallow syntactic information to give better equivalence classes, where the shallow syntactic information is expressed as partofspeech (POS) tags (e.g. [11], [13]). A POS tag fl This research work was completed while the trst author was at the University of Rochester. <p> dataset is used to smooth the probabilities of each node with its parent [1]. 3.1 Word and POS Classitcation Trees To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. <ref> [4] </ref> into a binary classitcation tree. The algorithm starts with each word (or POS tag) in a separate class, and successively merges classes that result in the smallest lost in mutual information in terms of the cooccurrences of these classes.
Reference: [5] <author> E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. </author> <title> Equations for partofspeech tagging. </title> <booktitle> In Pro ceedings of the National Conference on Artitcial Intelli gence (AAAI '93), </booktitle> <year> 1993. </year>
Reference-contexts: Pr (W 1;N P 1;N ) Y Pr (W i P i jW 1;i1 P 1;i1 ) Y Pr (W i jW 1;i1 P 1;i ) Pr (P i jW 1;i1 P 1;i1 ) The tnal probability distributions are similar to those used for POS tagging of written text <ref> [5, 6, 7] </ref>.
Reference: [6] <author> K. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedingsof the 2nd Con ference on Applied Natural Language Processing, </booktitle> <pages> pages 136143, </pages> <year> 1988. </year>
Reference-contexts: Pr (W 1;N P 1;N ) Y Pr (W i P i jW 1;i1 P 1;i1 ) Y Pr (W i jW 1;i1 P 1;i ) Pr (P i jW 1;i1 P 1;i1 ) The tnal probability distributions are similar to those used for POS tagging of written text <ref> [5, 6, 7] </ref>.
Reference: [7] <author> S. J. DeRose. </author> <title> Grammatical category disambiguation by statistical optimization. </title> <booktitle> Computational Linguistics, </booktitle> <address> 14(1):3139, </address> <year> 1988. </year>
Reference-contexts: Pr (W 1;N P 1;N ) Y Pr (W i P i jW 1;i1 P 1;i1 ) Y Pr (W i jW 1;i1 P 1;i ) Pr (P i jW 1;i1 P 1;i1 ) The tnal probability distributions are similar to those used for POS tagging of written text <ref> [5, 6, 7] </ref>.
Reference: [8] <author> P. A. Heeman. </author> <title> Speech repairs, intonational boundaries and discourse markers: Modeling speakers' utterances in spoken dialog. </title> <type> Doctoral dissertation. </type> <year> 1997. </year>
Reference-contexts: Full details of how we compute the word based perplexity are given in <ref> [8] </ref>. We also measure the error rate in assigning the POS tags. Here, as in measuring the perplexity, we run the language model on the hand transcribed word annotations. 4.1 Effect of Richer Context Table 1 gives the perplexity and POS tagging error rate (expressed as a percent). <p> Currently, we are exploring the effect of this model in reducing the word error rate. Incorporating shallow syntactic information into the speech recognition process is just the trst step. In other work <ref> [8, 10] </ref>, this syntactic information, as well as the tech niques introduced in this paper, are used to help model the occurrence of dysuencies and intonational phrasing in a speech recognition language model.
Reference: [9] <author> P. A. Heeman and J. F. Allen. </author> <title> The Trains spoken dialog corpus. CDROM, </title> <booktitle> Linguistics Data Consortium, </booktitle> <year> 1995. </year>
Reference-contexts: The effect of using composite questions is explored in Section 4.3. 4 RESULTS To demonstrate our model, we have tested it on the Trains corpus <ref> [9] </ref>, a collection of humanhuman taskoriented spoken dialogues consisting of 6 and half hours worth of speech, 34 different speakers, 58,000 words of transcribed speech, with a vocabulary size of 860 words.
Reference: [10] <author> P. A. Heeman and J. F. Allen. </author> <title> Intonational boundaries, speech repairs, and discourse markers: Modeling spoken dialog. </title> <booktitle> In Proceedings of the 35 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: Currently, we are exploring the effect of this model in reducing the word error rate. Incorporating shallow syntactic information into the speech recognition process is just the trst step. In other work <ref> [8, 10] </ref>, this syntactic information, as well as the tech niques introduced in this paper, are used to help model the occurrence of dysuencies and intonational phrasing in a speech recognition language model.
Reference: [11] <author> F. Jelinek. </author> <title> Selforganized language modeling for speech recognition. </title> <type> Technical report, </type> <institution> IBM T.J. Watson Research Center, Continuous Speech Recognition Group, </institution> <year> 1985. </year>
Reference-contexts: Hence, speech recognizers employ a language model that prunes out acoustic alternatives by taking into account the previous words that were recognized. In doing this, the speech recognition problem is viewed as tnding the most likely word sequence ^ W given the acoustic signal <ref> [11] </ref>. ^ W = arg max Pr (W jA) W Pr (A) W The last line involves two probabilities that need to be estimatedthe trst due to the acoustic model Pr (AjW ) and the second due to the language model Pr (W ). <p> Due to sparseness of data, one must detne equivalence classes amongst the contexts W 1;i1 , which can be done by limiting the context to an ngram language model <ref> [11] </ref> and also by grouping words into words classes [4]. Several attempts have been made to incorporate shallow syntactic information to give better equivalence classes, where the shallow syntactic information is expressed as partofspeech (POS) tags (e.g. [11], [13]). <p> which can be done by limiting the context to an ngram language model <ref> [11] </ref> and also by grouping words into words classes [4]. Several attempts have been made to incorporate shallow syntactic information to give better equivalence classes, where the shallow syntactic information is expressed as partofspeech (POS) tags (e.g. [11], [13]). A POS tag fl This research work was completed while the trst author was at the University of Rochester. The authors would like to thank Geraldine Damnati, Kyungho LokenKim, Tsuyoshi Morimoto, Eric Ringger and Ramesh Sarukkai. <p> Only by interpolating in a wordbased model is an improvement seen <ref> [11] </ref>. A more major problem with the above approach is that in a spoken dialogue system, speech recognition is only the trst step in understanding a speaker's contribution.
Reference: [12] <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Pro cessing, </journal> <pages> pages 400401, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Not Used Single 10 POS Error Rate 3.04 3.04 2.97 Perplexity 25.36 24.36 24.17 Table 3: Effect of Composite Questions 4.4 Effect of Larger Context In Table 4, we look at the effect of the size of the context, and compare the results to a wordbased backoff language model <ref> [12] </ref> built using the CMU toolkit [14]. For a bigram model, it has a perplexity of 29.3, in comparison to our word perplexity of 27.4. For a trigram model, the word based model has a perplexity of 26.1, in comparison to our perplexity of 24.2.
Reference: [13] <author> T. R. Niesler and P. C. Woodland. </author> <title> A variablelength categorybased ngram language model. </title> <booktitle> In Proceedings of the International Conference on Audio, Speech and Sig nal Processing, </booktitle> <year> 1996. </year>
Reference-contexts: Several attempts have been made to incorporate shallow syntactic information to give better equivalence classes, where the shallow syntactic information is expressed as partofspeech (POS) tags (e.g. [11], <ref> [13] </ref>). A POS tag fl This research work was completed while the trst author was at the University of Rochester. The authors would like to thank Geraldine Damnati, Kyungho LokenKim, Tsuyoshi Morimoto, Eric Ringger and Ramesh Sarukkai. <p> For instance, Srinivas [15] reports that such a model results in a 24.5% increase in perplexity over a wordbased model on the Wall Street Journal, and Niesler and Woodland <ref> [13] </ref> report an 11.3% increase (but a 22fold decrease in the number of parameters of such a model). Only by interpolating in a wordbased model is an improvement seen [11].
Reference: [14] <author> R. Rosenfeld. </author> <title> The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation. </title> <booktitle> In Proceedings of the ARPA Spoken Language Systems Technology Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: Error Rate 3.04 3.04 2.97 Perplexity 25.36 24.36 24.17 Table 3: Effect of Composite Questions 4.4 Effect of Larger Context In Table 4, we look at the effect of the size of the context, and compare the results to a wordbased backoff language model [12] built using the CMU toolkit <ref> [14] </ref>. For a bigram model, it has a perplexity of 29.3, in comparison to our word perplexity of 27.4. For a trigram model, the word based model has a perplexity of 26.1, in comparison to our perplexity of 24.2.
Reference: [15] <author> B. Srinivas. </author> <title> Almost parsing techniques for language modeling. </title> <booktitle> In Proceedings of the 4rd International Confer ence on Spoken Language Processing, </booktitle> <year> 1996. </year>
Reference-contexts: Pr (W i jP 1;i W 1;i1 ) Pr (W i jP i ) However, this approach does not lead to an improvement in the performance of the speech recognizer. For instance, Srinivas <ref> [15] </ref> reports that such a model results in a 24.5% increase in perplexity over a wordbased model on the Wall Street Journal, and Niesler and Woodland [13] report an 11.3% increase (but a 22fold decrease in the number of parameters of such a model).
References-found: 15

