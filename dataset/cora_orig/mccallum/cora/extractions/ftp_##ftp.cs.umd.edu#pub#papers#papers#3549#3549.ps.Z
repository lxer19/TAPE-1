URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3549/3549.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fhelman, dbader, josephg@umiacs.umd.edu  
Title: A Parallel Sorting Algorithm With an Experimental Study (Preliminary Draft)  
Author: David R. Helman David A. Bader Joseph JaJa 
Keyword: Parallel Algorithms, Generalized Sorting, Integer Sorting, Sample Sort, Parallel Per formance.  
Note: The support by NASA Graduate Student Researcher Fellowship No. NGT-50951 is gratefully acknowledged. Supported in part by NSF grant No. CCR-9103135 and NSF HPCC/GCAG grant No. BIR-9318183.  
Date: December 29, 1995  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies, and Department of Electrical Engineering, University of Maryland,  
Abstract: Previous schemes for sorting on general-purpose parallel machines have had to choose between poor load balancing and irregular communication or multiple rounds of all-to-all personalized communication. In this paper, we introduce a novel variation on sample sort which uses only two rounds of regular all-to-all personalized communication in a scheme that yields very good load balancing with virtually no overhead. This algorithm was implemented in Split-C and run on a variety of platforms, including the Thinking Machines CM-5, the IBM SP-2, and the Cray Research T3D. We ran our code using widely different benchmarks to examine the dependence of our algorithm on the input distribution. Our experimental results are consistent with the theoretical analysis and illustrate the efficiency and scalability of our algorithm across different platforms. In fact, it seems to outperform all similar algorithms known to the authors on these platforms, and its performance is invariant over the set of input distributions unlike previous efficient algorithms. Our results also compare favorably with those reported for the simpler ranking problem posed by the NAS Integer Sorting (IS) Benchmark. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Alexandrov, M. Ionescu, K. Schauser, and C. Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP Model One step closer towards a realistic model for parallel computation. </title> <booktitle> In 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-105, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [16, 34, 1] </ref>) that have 3 recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp as the maximum time it takes a processor to perform all the local computation steps. <p> In Tables IV and V, we compare the performance of our sample sort algorithm with two other sample sort algorithms. In all cases, the code was written in Split-C. In the case of Alexandrov et al. <ref> [1] </ref>, the times were determined by us directly on a 32 node CM-5 using code supplied by the authors which had been optimized for a Meiko CS-2.
Reference: [2] <author> R.H. Arpaci, D.E. Culler, A. Krishnamurthy, S.G. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <publisher> In ACM Press, </publisher> <editor> editor, </editor> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The Meiko CS-2 Computing Facility was acquired through NSF CISE Infrastructure Grant number CDA-9218202, with support from the College of Engineering and the UCSB Office of Research, for research in parallel computing. Arvind Krishnamurthy provided additional help with his port of Split-C to the Cray Research T3D <ref> [2] </ref>. The Jet Propulsion Lab/Caltech 256-node Cray T3D Supercomputer used in this investigation was provided by funding from the NASA Offices of Mission to Planet Earth, Aeronautics, and Space Science.
Reference: [3] <author> D.A. Bader, D.R. Helman, and J. JaJa. </author> <title> Practical Parallel Algorithms for Personalized Communication and Integer Sorting. </title> <institution> CS-TR-3548 and UMIACS-TR-95-101 Technical Report, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> November </month> <year> 1995. </year> <note> Submitted to ACM Journal of Experimental Algorithmics. </note>
Reference-contexts: The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm.
Reference: [4] <author> D.A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <institution> Technical Report CS-TR-3384 and UMIACS-TR-94-133, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1994. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm.
Reference: [5] <author> D.A. Bader and J. JaJa. </author> <title> Parallel Algorithms for Image Histogramming and Connected Components with an Experimental Study. </title> <booktitle> In Fifth ACM SIGPLAN Symposium of Principles and Practice of Parallel Programming, </booktitle> <pages> pages 123-133, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm. <p> The cost of each of the collective communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost (which is an overestimate) can be justified by using our earlier work <ref> [22, 21, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [6] <author> D.A. Bader and J. JaJa. </author> <title> Practical Parallel Algorithms for Dynamic Data Redistribution, Median Finding, and Selection. </title> <institution> Technical Report CS-TR-3494 and UMIACS-TR-95-74, UMIACS and Electrical Engineering, University of Maryland, College Park, MD, </institution> <month> July </month> <year> 1995. </year> <booktitle> To be presented at the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, HI, </address> <month> April 15-19, </month> <year> 1996. </year>
Reference-contexts: The remote read and write typically have both blocking and non--blocking versions. Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in <ref> [6] </ref>, are similar to those of the MPI [27], the IBM POWERparallel [8], and the Cray MPP systems [13] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> The primitives gather and scatter are companion primitives. Scatter divides a single array residing on a processor into equal-sized blocks, each of which is distributed to a unique processor, and gather coalesces these blocks back into a single array at a particular processor. See <ref> [3, 6, 4, 5] </ref> for algorithmic details, performance analyses, and empirical results for these communication primitives. The organization of this paper is as follows. Section 2 presents our computation model for analyzing parallel algorithms. Section 3 describes in detail our improved sample sort algorithm. <p> The cost of each of the collective communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost (which is an overestimate) can be justified by using our earlier work <ref> [22, 21, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and . <p> Step (8) requires O ( n p log p) time if we merge the sorted subsequences in a binary tree fashion. Steps (2), (5), and (7) call the communication primitives transpose, bcast, and transpose, respectively. The analysis of these primitives in <ref> [6] </ref> shows that with high probability these three steps require T comm (n; p) (t + 2 n p 2 (p 1)), T comm (n; p) (t + (p 1)), and T comm (n; p) (t + 2:48 n p 2 (p 1)), respectively.
Reference: [7] <author> D. Bailey, E. Barszcz, J. Barton, D. Browning, R. Carter, L. Dagum, R. Fatoohi, S. Fineberg, P. Frederickson, T. Lasinski, R. Schreiber, H. Simon, V. Venkatakrishnan, and S. Weeratunga. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> Numerical Aerodynamic Simulation Facility, NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Its significant requirements for interprocessor communication bandwidth and the irregular communication patterns that are typically generated have earned its inclusion in several parallel benchmarks such as NAS <ref> [7] </ref> and SPLASH [35]. Moreover, its practical importance has motivated the publication of a number of empirical studies seeking to identify the most efficient sorting routines. Yet, parallel sorting strategies have still generally fallen into one of two groups, each with its respective disadvantages.
Reference: [8] <author> V. Bala, J. Bruck, R. Cypher, P. Elustondo, A. Ho, C.-T. Ho, S. Kipnis, and M. Snir. </author> <title> CCL: A Portable and Tunable Collective Communication Library for Scalable Parallel Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6 </volume> <pages> 154-164, </pages> <year> 1995. </year>
Reference-contexts: Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [6], are similar to those of the MPI [27], the IBM POWERparallel <ref> [8] </ref>, and the Cray MPP systems [13] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> Second, different implementations of the communication primitives were allowed for each machine. Wherever possible, we tried to use the vendor supplied implementations. In fact, IBM does provide all of our communication primitives as part of its machine specific Collective Communication Library (CCL) <ref> [8] </ref>. As one might expect, they were faster than the high level Split-C implementation. The graphs in Figures 1 and 2 display the performance of our sample sort as a function of input distribution for a variety of input sizes.
Reference: [9] <author> K. Batcher. </author> <title> Sorting Networks and Their Applications. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference 32, </booktitle> <pages> pages 307-314, </pages> <address> Reston, VA, </address> <year> 1968. </year> <month> 19 </month>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort <ref> [9] </ref>, column sort [23], rotate sort [26], hyperquicksort [29], flashsort [30], B-flashsort [19], smoothsort [28], and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication.
Reference: [10] <author> G.E. Blelloch, C.E. Leiserson, B.M. Maggs, C.G. Plaxton, S.J. Smith, and M. Zagha. </author> <title> A Com--parison of Sorting Algorithms for the Connection Machine CM-2. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Yet, parallel sorting strategies have still generally fallen into one of two groups, each with its respective disadvantages. The first group, using the classification of Li and Sevcik [24], is the single-step algorithms, so named because data is moved once between processors. Examples of this include sample sort <ref> [20, 10] </ref>, parallel sorting by regular sampling [32, 25], and parallel sorting by overpartitioning [24]. The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. <p> One way to choose the splitters is by randomly sampling the input elements at each processor hence the name sample sort. Previous versions of sample sort <ref> [20, 10, 17, 15] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter. <p> Proof: Establishing a bound on the number of elements received by any processor in Step (7) is equivalent to establishing a bound on the number of elements which fall between any two consecutive splitters in the sorted order. But as Blelloch et al. <ref> [10] </ref> observed, the number of elements which fall between any two consecutive splitters in the sorted order can only be greater than ff 2 n p if in the sorted order there are less than n p 2 samples drawn from the ff 2 n p elements which follow the first
Reference: [11] <author> W.W. Carlson and J.M. Draper. </author> <title> AC for the T3D. </title> <type> Technical Report SRC-TR-95-141, </type> <institution> Supercomputing Research Center, Bowie, MD, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: We also acknowledge William Carlson and Jesse Draper from the Center for Computing Science (formerly Supercomputing Research Center) for writing the parallel compiler AC (version 2.6) <ref> [11] </ref> on which the T3D port of Split-C has been based. This work also utilized the CM-5 at National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, under grant number ASC960008N.
Reference: [12] <author> H. Chernoff. </author> <title> A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the Sum of Observations. </title> <journal> Annals of Math. Stat., </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: Using the following Chernoff bound <ref> [12] </ref> for estimating the tail of a binomial distribution X b (s; r; q) e * 2 rq the probability that a particular bucket will contain at least c 1 n p 2 elements can be bounded by e 3p 2 .
Reference: [13] <institution> Cray Research, Inc. </institution> <note> SHMEM Technical Note for C, October 1994. Revision 2.3. </note>
Reference-contexts: Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [6], are similar to those of the MPI [27], the IBM POWERparallel [8], and the Cray MPP systems <ref> [13] </ref> and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows.
Reference: [14] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In fact, it seems to outperform all similar algorithms known to the authors on these platforms, and its performance is indifferent to the set of input distributions unlike previous efficient algorithms. The high-level language used in our studies is Split-C <ref> [14] </ref>, an extension of C for distributed memory machines. The algorithm makes use of MPI-like communication primitives but does not make any assumptions as to how these primitives are actually implemented. The basic data transport 2 is a read or write operation. <p> Hence, it is very important to perform an empirical evaluation of an algorithm using a wide variety of benchmarks, as we will do next. 4 Performance Evaluation Sample sort was implemented using Split-C <ref> [14] </ref> and run on a variety of machines and processors, including the Thinking Machines CM-5, the IBM SP-2-WN and SP-2-TN2, and the Cray Research T3D.
Reference: [15] <author> D.E. Culler, A.C. Dusseau, R.P. Martin, and K.E. Schauser. </author> <title> Fast Parallel Sorting Under LogP: From Theory to Practice. In Portability and Performance for Parallel Processing, </title> <booktitle> chapter 4, </booktitle> <pages> pages 71-98. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1993. </year>
Reference-contexts: One way to choose the splitters is by randomly sampling the input elements at each processor hence the name sample sort. Previous versions of sample sort <ref> [20, 10, 17, 15] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter.
Reference: [16] <author> D.E. Culler, R.M. Karp, D.A. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [16, 34, 1] </ref>) that have 3 recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp as the maximum time it takes a processor to perform all the local computation steps.
Reference: [17] <author> A.C. Dusseau. </author> <title> Modeling Parallel Sorts with LogP on the CM-5. </title> <type> Technical Report UCB//CSD-94-829, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: One way to choose the splitters is by randomly sampling the input elements at each processor hence the name sample sort. Previous versions of sample sort <ref> [20, 10, 17, 15] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter. <p> In all cases, the code was written in Split-C. In the case of Alexandrov et al. [1], the times were determined by us directly on a 32 node CM-5 using code supplied by the authors which had been optimized for a Meiko CS-2. In the case of Dusseau <ref> [17] </ref>, the times were obtained from the graphed results reported for a 64 node CM-5. [U] [G] [2-G] [B] [S] int./proc.
Reference: [18] <author> T. Hagerup and C. Rub. </author> <title> A Guided Tour of Chernoff Bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 305-308, </pages> <year> 1990. </year>
Reference-contexts: Using the following "Chernoff" type bound <ref> [18] </ref> for estimating the head of a binomial distribution X b (s; r; q) e (1*) 2 rq where s = n p 2 , r = ff 2 p , and q = 1 p , the probability that n p 2 or less samples will be found amongst the
Reference: [19] <author> W.L. Hightower, J.F. Prins, and J.H. Reif. </author> <title> Implementations of Randomized Sorting on Large Parallel Machines. </title> <booktitle> In Proceedings of the 4th Symposium on Parallel Architectures and Algorithms, </booktitle> <pages> pages 158-167, </pages> <address> San Diego, CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort [23], rotate sort [26], hyperquicksort [29], flashsort [30], B-flashsort <ref> [19] </ref>, smoothsort [28], and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication. In this paper, we present a novel variation on the sample sort algorithm which addresses the limitations of previous implementations.
Reference: [20] <author> J.S. Huang and Y.C. Chow. </author> <title> Parallel Sorting and Data Partitioning by Sampling. </title> <booktitle> In Proceedings of the 7th Computer Software and Applications Conference, </booktitle> <pages> pages 627-631, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Yet, parallel sorting strategies have still generally fallen into one of two groups, each with its respective disadvantages. The first group, using the classification of Li and Sevcik [24], is the single-step algorithms, so named because data is moved once between processors. Examples of this include sample sort <ref> [20, 10] </ref>, parallel sorting by regular sampling [32, 25], and parallel sorting by overpartitioning [24]. The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. <p> One way to choose the splitters is by randomly sampling the input elements at each processor hence the name sample sort. Previous versions of sample sort <ref> [20, 10, 17, 15] </ref> have randomly chosen s samples from the n p elements at each processor, routed them to a single processor, sorted them at that processor, and then selected every s th element as a splitter.
Reference: [21] <author> J. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model. </title> <type> Technical Report CS-TR-3207, </type> <institution> Computer Science Department, University of Maryland, College Park, </institution> <month> January </month> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: The cost of each of the collective communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost (which is an overestimate) can be justified by using our earlier work <ref> [22, 21, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [22] <author> J.F. JaJa and K.W. Ryu. </author> <title> The Block Distributed Memory Model for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 752-756, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year> <note> (Extended Abstract). 20 </note>
Reference-contexts: The cost of each of the collective communication primitives will be modeled by O (t + max (m; p)), where m is the maximum amount of data transmitted or received by a processor. Such a cost (which is an overestimate) can be justified by using our earlier work <ref> [22, 21, 6, 5] </ref>. Using this cost model, we can evaluate the communication time T comm (n; p) of an algorithm as a function of the input size n, the number of processors p , and the parameters t and .
Reference: [23] <author> F.T. Leighton. </author> <title> Tight Bounds on the Complexity of Parallel Sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34:344-354, </volume> <year> 1985. </year>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort <ref> [23] </ref>, rotate sort [26], hyperquicksort [29], flashsort [30], B-flashsort [19], smoothsort [28], and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication.
Reference: [24] <author> H. Li and K.C. Sevcik. </author> <title> Parallel Sorting by Overpartitioning. </title> <type> Technical Report CSRI-295, </type> <institution> Computer Systems Research Institute, University of Toronto, Canada, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Yet, parallel sorting strategies have still generally fallen into one of two groups, each with its respective disadvantages. The first group, using the classification of Li and Sevcik <ref> [24] </ref>, is the single-step algorithms, so named because data is moved once between processors. Examples of this include sample sort [20, 10], parallel sorting by regular sampling [32, 25], and parallel sorting by overpartitioning [24]. <p> The first group, using the classification of Li and Sevcik <ref> [24] </ref>, is the single-step algorithms, so named because data is moved once between processors. Examples of this include sample sort [20, 10], parallel sorting by regular sampling [32, 25], and parallel sorting by overpartitioning [24]. The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing.
Reference: [25] <author> X. Li, P. Lu, J. Schaeffer, J. Shillington, P.S. Wong, and H. Shi. </author> <title> On the Versatility of Parallel Sorting by Regular Sampling. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1079-1103, </pages> <year> 1993. </year>
Reference-contexts: The first group, using the classification of Li and Sevcik [24], is the single-step algorithms, so named because data is moved once between processors. Examples of this include sample sort [20, 10], parallel sorting by regular sampling <ref> [32, 25] </ref>, and parallel sorting by overpartitioning [24]. The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing.
Reference: [26] <author> J.M. Marberg and E. Gafni. </author> <title> Sorting in Constant Number of Row and Column Phases on a Mesh. </title> <journal> Algorithmica, </journal> <volume> 3 </volume> <pages> 561-572, </pages> <year> 1988. </year>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort [23], rotate sort <ref> [26] </ref>, hyperquicksort [29], flashsort [30], B-flashsort [19], smoothsort [28], and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication.
Reference: [27] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year> <note> Version 1.1. </note>
Reference-contexts: Second, we obtain predictable, regular communication requirements which are essentially invariant with respect to the input distribution. The importance of utilizing regular communication has become more important with the advent of message passing standards, such as MPI <ref> [27] </ref>, which seek to guarantee the availability of very efficient (often machine specific) implementations of certain basic collective communication routines. Our algorithm was implemented in a high-level language and run on a variety of platforms, including the Thinking Machines CM-5, the IBM SP-2, and the Cray Research T3D. <p> Also, when reading or writing more than a single element, bulk data transports are provided with corresponding bulk read and bulk write primitives. Our collective communication primitives, described in detail in [6], are similar to those of the MPI <ref> [27] </ref>, the IBM POWERparallel [8], and the Cray MPP systems [13] and, for example, include the following: transpose, bcast, gather, and scatter. Brief descriptions of these are as follows. <p> Moreover, such an irregular communication scheme cannot take advantage of the regular communication primitives proposed under the MPI standard <ref> [27] </ref>. In our solution, we incur no overhead in obtaining n p 2 samples from each processor and in sorting these samples to identify the splitters.
Reference: [28] <author> C.G. Plaxton. </author> <title> Efficient Computation on Sparse Interconnection Networks. </title> <type> Technical Report STAN-CS-89-1283, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <month> Septem-ber </month> <year> 1989. </year>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort [23], rotate sort [26], hyperquicksort [29], flashsort [30], B-flashsort [19], smoothsort <ref> [28] </ref>, and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication. In this paper, we present a novel variation on the sample sort algorithm which addresses the limitations of previous implementations.
Reference: [29] <author> M.J. Quinn. </author> <title> Analysis and Benchmarking of Two Parallel Sorting Algorithms: </title> <journal> Hyperquicksort and Quickmerge. BIT, </journal> <volume> 29 </volume> <pages> 239-250, </pages> <year> 1989. </year>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort [23], rotate sort [26], hyperquicksort <ref> [29] </ref>, flashsort [30], B-flashsort [19], smoothsort [28], and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication.
Reference: [30] <author> J.H. Reif and L.G. Valiant. </author> <title> A Logarithmic Time Sort for Linear Sized Networks. </title> <journal> Journal of the ACM, </journal> <volume> 34 </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing. The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort [23], rotate sort [26], hyperquicksort [29], flashsort <ref> [30] </ref>, B-flashsort [19], smoothsort [28], and Tridgell and Brent's sort [33]. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication.
Reference: [31] <author> S. Saini and D.H. Bailey. </author> <title> NAS Parallel Benchmarks Results 12-95. Report NAS-95-021, Numerical Aerodynamic Simulation Facility, </title> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: 16.6 21 12.2 91 10.6 11 Table V: Time required per element (in microseconds) to sample sort 64M integers, comparing our results (HBJ) with those obtained from the graphed results reported by Dusseau (DUS) on a 64 node CM-5. 16 Finally, there are the results for the NAS Parallel Benchmark <ref> [31] </ref> for integer sorting (IS). The name of this benchmark is somewhat misleading. Instead of requiring that the integers be placed in sorted order as we do, the benchmark only requires that they be ranked without any reordering, which is a significantly simpler task.
Reference: [32] <author> H. Shi and J. Schaeffer. </author> <title> Parallel Sorting by Regular Sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 361-372, </pages> <year> 1992. </year>
Reference-contexts: The first group, using the classification of Li and Sevcik [24], is the single-step algorithms, so named because data is moved once between processors. Examples of this include sample sort [20, 10], parallel sorting by regular sampling <ref> [32, 25] </ref>, and parallel sorting by overpartitioning [24]. The price paid by these single-step algorithms is an irregular communication scheme and difficulty with load balancing.
Reference: [33] <author> A. Tridgell and R.P. Brent. </author> <title> An Implementation of a General-Purpose Parallel Sorting Algorithm. </title> <type> Techical Report TR-CS-93-01, </type> <institution> Computer Sciences Laboratory, Australian National University, Canberra, Australia, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The other group of sorting algorithms is the multi-step algorithms, which include bitonic sort [9], column sort [23], rotate sort [26], hyperquicksort [29], flashsort [30], B-flashsort [19], smoothsort [28], and Tridgell and Brent's sort <ref> [33] </ref>. Generally speaking, these algorithms accept multiple rounds of communication in return for better load balancing and, in some cases, regular communication. In this paper, we present a novel variation on the sample sort algorithm which addresses the limitations of previous implementations. <p> Note that while we actually place the integers in sorted order, the benchmark only requires that they be ranked without actually reordering. The only performance studies we are aware of on similar platforms for generalized sorting are those of Tridgell and Brent <ref> [33] </ref>, who report the performance of their algorithm using a 32 node CM-5 on a uniformly distributed random input of signed integers, as described in Table VII.
Reference: [34] <author> L.G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: The coefficient of t gives the total number of times collective communication primitives are used, and the coefficient of gives the maximum total amount of data exchanged between a processor and the remaining processors. This communication model is close to a number of similar models (e.g. <ref> [16, 34, 1] </ref>) that have 3 recently appeared in the literature and seems to be well-suited for designing parallel algorithms on current high performance platforms. We define the computation time T comp as the maximum time it takes a processor to perform all the local computation steps.
Reference: [35] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 21 </month>
Reference-contexts: Its significant requirements for interprocessor communication bandwidth and the irregular communication patterns that are typically generated have earned its inclusion in several parallel benchmarks such as NAS [7] and SPLASH <ref> [35] </ref>. Moreover, its practical importance has motivated the publication of a number of empirical studies seeking to identify the most efficient sorting routines. Yet, parallel sorting strategies have still generally fallen into one of two groups, each with its respective disadvantages.
References-found: 35

