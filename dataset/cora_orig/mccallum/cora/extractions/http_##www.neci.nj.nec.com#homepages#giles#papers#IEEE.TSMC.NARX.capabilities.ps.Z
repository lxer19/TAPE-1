URL: http://www.neci.nj.nec.com/homepages/giles/papers/IEEE.TSMC.NARX.capabilities.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Email: Email: iehava@ie.technion.ac.il  Email: fhorne,gilesg@research.nj.nec.com  
Title: Computational capabilities of recurrent NARX neural networks of what amount of feedback or recurrence is
Author: Hava T. Siegelmann Bill G. Horne and C. Lee Giles 
Date: 1)  
Note: Also with  u(t n u u(t 1); u(t); (t n (t  issue  Accepted for publication in IEEE Transactions on Systems, Man and Cybernetics.  
Address: Haifa 32000, Israel  4 Independence Way Princeton, NJ 08540  College Park, MD 20742  
Affiliation: Department of Information Systems Engineering Faculty of Industrial Engineering and Management Technion (Israel Institute of Technology)  NEC Research Institute  UMIACS University of Maryland  
Abstract: Recently, fully connected recurrent neural networks have been proven to be computationally rich | at least as powerful as Turing machines. This work focuses on another network which is popular in control applications and has been found to be very effective at learning a variety of problems. These networks are based upon Nonlinear AutoRegressive models with eXogenous Inputs (NARX models), and are therefore called NARX networks. As opposed to other recurrent networks, NARX networks have a limited feedback which comes only from the output neuron rather than from hidden states. They are formalized by ; where u(t) and y(t) represent input and output of the network at time t, n u and n y are the input and output order, and the function is the mapping performed by a Multilayer Perceptron. We constructively prove that the NARX networks with a finite number of parameters are computa-tionally as strong as fully connected recurrent networks and thus Turing machines. We conclude that in theory one can use the NARX models, rather than conventional recurrent networks without any computational loss even though their feedback is limited. Furthermore, these results raise the y(t) = 
Abstract-found: 1
Intro-found: 1
References-found: 0

