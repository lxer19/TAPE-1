URL: http://polaris.cs.uiuc.edu/reports/1339.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: email: rwerger@csrd.uiuc.edu  
Phone: telephone: (217) 333-6578, fax: (217) 244-1351.  
Title: Speculative Run-Time Parallelization of Loops  
Author: Lawrence Rauchwerger and David Padua Lawrence Rauchwerger. 
Note: Corresponding Author:  Research supported in part by Army contract #DABT63-92-C-0033. This work is not necessarily representative of the positions or policies of the Army or the Government.  
Address: 1308 W. Main St., Urbana, IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: Current parallelizing compilers cannot identify a significant fraction of fully parallel loops because they have complex or statically insufficiently defined access patterns. Since fully parallel loops arise frequently in practice, we have developed methods to speculatively execute loops concurrently. These methods can be applied to any loop, even if the iteration space of the loop is unknown, as in WHILE Loops or DO Loops with conditional exits. To verify the validity of our speculation, we have devised a fully parallel run-time technique for detecting the presence of cross-iteration data dependences in loops. This technique can also be used to eliminate some memory-related dependences by dynamically privatizing scalars and arrays. We outline a cost/performance analysis that can be performed to decide when the methods should be used. Our conclusion is that they should almost always be applied because, as we show, the expected speedup for fully parallel loops is significant, and the cost of a failed speculation (a not fully parallel loop), is minimal. We present experimental results on loops from the PERFECT Benchmarks which substantiate our conclusion that these techniques can yield significant speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation, 42 Nagog Park, Acton, Massachusetts 01720. FX/Series Architecture Manual, </institution> <year> 1986. </year> <title> Part Number: </title> <publisher> 300-00001-B. </publisher>
Reference-contexts: Step 2 would be omitted, and if the loop was not classified by Step 2 (b) or (c), then there must exist some output dependences between the iterations of the loop (i.e., 1 any returns the "OR" of its vector operand's elements, i.e., any (v [1 : n]) = (v <ref> [1] </ref> _ v [2] _ : : : _ v [n]). 6 tw (A) 6= tm (A)) and the loop is NOT a DOALL. <p> In order to terminate the parallel loop cleanly before all iterations have been executed, a QUIT operation similar to the one on Alliant computers <ref> [1] </ref> could be used. Once a QUIT command is issued by an iteration, all iterations with loop counters less than the that of the issuing iteration will be completed, but no iterations with larger loop counters will be begun. <p> maximize the potential gains attainable from parallel execution, while, at the same time, minimizing the costs incurred by failed speculations, i.e., speculations on loops that are, in fact, not parallel. 6 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 <ref> [1] </ref>) and 14 processors (Alliant FX/2800 [2]) using a Fortran implementation of our methods. It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our run-time methods.
Reference: [2] <institution> Alliant Computers Systems Corporation. Alliant FX/2800 Series System Description, </institution> <year> 1991. </year>
Reference-contexts: be omitted, and if the loop was not classified by Step 2 (b) or (c), then there must exist some output dependences between the iterations of the loop (i.e., 1 any returns the "OR" of its vector operand's elements, i.e., any (v [1 : n]) = (v [1] _ v <ref> [2] </ref> _ : : : _ v [n]). 6 tw (A) 6= tm (A)) and the loop is NOT a DOALL. <p> parallel execution, while, at the same time, minimizing the costs incurred by failed speculations, i.e., speculations on loops that are, in fact, not parallel. 6 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [1]) and 14 processors (Alliant FX/2800 <ref> [2] </ref>) using a Fortran implementation of our methods. It should be pointed out that our results scale with the number of processors and the data size and that they should be extrapolated for MPPs, the actual target of our run-time methods.
Reference: [3] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [18, 11, 3, 29, 32] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [4] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: We considered six DO Loops and two WHILE Loops that could not be parallelized by any compiler available to us; seven loops are from the PERFECT Benchmarks <ref> [4] </ref>, and one of the WHILE Loops is extracted from MCSPARSE, a parallel version of a sparse matrix solver [8]. Our results are summarized in Table 2.
Reference: [5] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [6] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [6, 14, 16, 26, 27] </ref>).
Reference: [7] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar Fortran and its Restructuring Compiler. </title> <editor> In A. Nicolau D. Gelernter, T. Gross and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 1-23. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction During the last two decades, compiler techniques for the automatic detection of parallelism have been studied extensively <ref> [29, 18, 7] </ref>. From this work it has become clear that, for a class of programs, compile-time analysis must be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected.
Reference: [8] <author> K. Gallivan, B. Marsolf, and H. Wijshoff. </author> <title> A large-grain parallel sparse system solver. </title> <booktitle> In Proc. Fourth SIAM Conf. on Parallel Proc. for Scient. Comp., </booktitle> <pages> pages 23-28, </pages> <address> Chicago, IL, </address> <year> 1989. </year>
Reference-contexts: We considered six DO Loops and two WHILE Loops that could not be parallelized by any compiler available to us; seven loops are from the PERFECT Benchmarks [4], and one of the WHILE Loops is extracted from MCSPARSE, a parallel version of a sparse matrix solver <ref> [8] </ref>. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and the potential slowdown that would have been incurred if, after applying the method, the loop had to be re-executed sequentially.
Reference: [9] <author> E. H. Gornish, E. D. Granston, and A. V. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <month> July </month> <year> 1990. </year> <month> 18 </month>
Reference-contexts: For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement. The strategy of speculatively making assumptions about the dynamic control flow or about data locality for prefetching instructions and/or data <ref> [9] </ref> has been used for some time with success. Recently, branch speculation has been used effectively in superscalar compilers [15, 22, 24].
Reference: [10] <author> M. Guzzi, D. Padua, J. Hoeflinger, and D. Lawrie. </author> <title> Cedar fortran and other vector and parallel fortran dialects. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference: [11] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [18, 11, 3, 29, 32] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [12] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: The counting in Step 2 (a) can be done in parallel by giving each processor s=p values to add within its private memory, and then summing the p resulting values in global storage; this takes O (s=p + log p) time <ref> [12] </ref>. The comparisons in Step 2 (b) and 2 (d) of the shadow arrays will take at most O (s=p + log p) time. If s &gt; na, then the complexity can be reduced to O (na=p + log p) by using hash tables.
Reference: [13] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th PPOPP, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [31, 21, 13] </ref>. Even when the access pattern can be analyzed, if the the iteration space is statically unknown (WHILE loops), then compilers have so far not been able to generate parallel code. Run-time techniques have been used practically from the beginning of parallel computing. <p> The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [14] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [6, 14, 16, 26, 27] </ref>).
Reference: [15] <author> S. A. Mahlke, W. Y. Chen, W. W. Hwu, B. R. Rau, and M. S. Schl ansker. </author> <title> Sentinel scheduling for VLIW and superscalar processors. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The strategy of speculatively making assumptions about the dynamic control flow or about data locality for prefetching instructions and/or data [9] has been used for some time with success. Recently, branch speculation has been used effectively in superscalar compilers <ref> [15, 22, 24] </ref>. In this paper we make assumptions about the parallelism of loops, i.e., we assume a loop is fully parallel, speculatively execute it concurrently, and then apply a run-time test to check if there were any cross-iteration dependences.
Reference: [16] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [6, 14, 16, 26, 27] </ref>).
Reference: [17] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [18] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction During the last two decades, compiler techniques for the automatic detection of parallelism have been studied extensively <ref> [29, 18, 7] </ref>. From this work it has become clear that, for a class of programs, compile-time analysis must be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected. <p> In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [18, 11, 3, 29, 32] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [19] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August 12-16, </month> <pages> pages 174-178. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [20] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> The doconsider loop. </title> <booktitle> In Proceedings of the 1989 ACM International Conference on Supercomputing , Crete, </booktitle> <address> Greece, </address> <pages> pages 29-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [21] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [31, 21, 13] </ref>. Even when the access pattern can be analyzed, if the the iteration space is statically unknown (WHILE loops), then compilers have so far not been able to generate parallel code. Run-time techniques have been used practically from the beginning of parallel computing. <p> The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement. <p> This is a simple illustration of the schedule reuse technique, in which a correct execution schedule is determined once, and subsequently reused if all of the defining conditions remain invariant (see, e.g., <ref> [21] </ref>). If it can be determined at compile time that the data access pattern is invariant across different executions of the same loop, then no additional computation is required.
Reference: [22] <author> M. D. Smith, M. S. Lam, and M. A. Horowitz. </author> <title> Boosting beyond static scheduling in a superscalar processor. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 344-354, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The strategy of speculatively making assumptions about the dynamic control flow or about data locality for prefetching instructions and/or data [9] has been used for some time with success. Recently, branch speculation has been used effectively in superscalar compilers <ref> [15, 22, 24] </ref>. In this paper we make assumptions about the parallelism of loops, i.e., we assume a loop is fully parallel, speculatively execute it concurrently, and then apply a run-time test to check if there were any cross-iteration dependences.
Reference: [23] <author> J. E. Thornton. </author> <title> Design of a Computer:The Control Data 6600. </title> <type> Scott, </type> <institution> Foresman, Glenview, Illinois, </institution> <year> 1971. </year>
Reference-contexts: Run-time techniques have been used practically from the beginning of parallel computing. Also, during the 1960s, relatively simple forms of run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 <ref> [23, 25] </ref>. Some of today's parallelizing compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version. The boolean expression in the if statement typically tests the value of a scalar variable.
Reference: [24] <author> P. Tirumalai, M. Lee, and M. Schlansker. </author> <title> Parallelization of loops with exits on pipelined architectures. </title> <booktitle> In Supercomputing, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: The strategy of speculatively making assumptions about the dynamic control flow or about data locality for prefetching instructions and/or data [9] has been used for some time with success. Recently, branch speculation has been used effectively in superscalar compilers <ref> [15, 22, 24] </ref>. In this paper we make assumptions about the parallelism of loops, i.e., we assume a loop is fully parallel, speculatively execute it concurrently, and then apply a run-time test to check if there were any cross-iteration dependences.
Reference: [25] <author> R. M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11 </volume> <pages> 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Run-time techniques have been used practically from the beginning of parallel computing. Also, during the 1960s, relatively simple forms of run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 <ref> [23, 25] </ref>. Some of today's parallelizing compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version. The boolean expression in the if statement typically tests the value of a scalar variable.
Reference: [26] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proceedings 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [6, 14, 16, 26, 27] </ref>).
Reference: [27] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of memory-related dependences a transformation called privatization can be applied to the loop. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [6, 14, 16, 26, 27] </ref>).
Reference: [28] <author> III W. Ludwell Harrison. </author> <title> Compiling Lisp for Evaluation on a Tightly Coupled Multiprocessor. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> Mar. 20, </month> <year> 1986. </year> <journal> CSRD Rpt. </journal> <volume> No. </volume> <pages> 565. </pages>
Reference-contexts: A detailed discussion of how the PD Test can be used to selectively privatize individual array elements can be found in [?]. 4 Speculative Parallel Execution of WHILE Loops WHILE Loops have often been treated by compilers as sequential constructs because their iteration space is unknown <ref> [28] </ref>. A related case which is generally also handled sequentially by compilers is the DO Loop with a conditional exit. In this section we propose techniques that can be used to execute such loops in parallel.
Reference: [29] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction During the last two decades, compiler techniques for the automatic detection of parallelism have been studied extensively <ref> [29, 18, 7] </ref>. From this work it has become clear that, for a class of programs, compile-time analysis must be complemented with run-time techniques if a significant fraction of the implicit parallelism is to be detected. <p> In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [18, 11, 3, 29, 32] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [30] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August 12-16, </month> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [31] <author> C. Zhu and P. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [31, 21, 13] </ref>. Even when the access pattern can be analyzed, if the the iteration space is statically unknown (WHILE loops), then compilers have so far not been able to generate parallel code. Run-time techniques have been used practically from the beginning of parallel computing. <p> The boolean expression in the if statement typically tests the value of a scalar variable. During the last few years, new techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [30, 19, 13, 20, 21, 31, 17, 5] </ref>. The behavior of programs is usually unknown at compile time. For example, incomplete foreknowledge of the outcome of branches and of the memory space access pattern prevent the compiler from adopting transformations such as locality enhancement.
Reference: [32] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <pages> 19 20 21 22 </pages>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [18, 11, 3, 29, 32] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
References-found: 32

