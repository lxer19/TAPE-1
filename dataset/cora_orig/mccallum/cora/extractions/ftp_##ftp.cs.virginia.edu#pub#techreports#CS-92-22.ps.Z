URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-92-22.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Portability and Performance: Mentat Applications on Diverse Architectures  
Author: Padmini Narayan, Sherry Smoot, Ambar Sarkar, Emily West, Andrew Grimshaw, Timothy Strayer 
Note: This work was supported in part by grants from the National Science Foundation, CDA-8922545-01, the National Aeronautics and Space Administration, NAG-1-1181, and the National Laboratory of Medicine LM04969.  
Pubnum: Technical Report No. CS-92-22  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. S. Grimshaw, </author> <title> The Mentat Run-Time System: Support for Medium Grain Parallel Computation, </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pp. 1064-1073, </pages> <address> Charleston, SC., </address> <month> April 9-12, </month> <year> 1990. </year>
Reference-contexts: Consequently, porting the application to a new architecture requires considerable effort. Given the plethora of new architectures and the rapid obsolescence of existing ones, communities are reluctant to commit to a single platform for their applications. Mentat <ref> [1] </ref> has been developed to directly address the difficulty of developing architecture-independent programs. The three primary design objectives of Mentat are to provide easy-to-use parallelism, to achieve high performance via parallel execution, and to facilitate the portability of applications across a wide range of platforms.
Reference: [2] <author> A. S. Grimshaw, E. Loyot Jr., and J. Weissman, </author> <title> Mentat Programming Language (MPL) Reference Manual, </title> <institution> University of Virginia, Computer Science TR 91-32, </institution> <year> 1991. </year>
Reference-contexts: The Mentat approach exploits the object-oriented paradigm to provide high-level abstractions that mask the complex aspects of parallel programming, communication, synchronization, and scheduling, from the programmer <ref> [2, 3] </ref>. Instead of managing these details, the programmer concentrates on the 3 application. The programmer uses application domain knowledge to specify those object classes that are of sufficient computational complexity to warrant parallel execution. The parallelization of these complex tasks is handled by Mentat. <p> Among the applications presented here are: An in-core matrix multiplication implementation A Gaussian elimination method of solving linear systems An image processing application A DNA sequence decomposition application A process pipeline example Ease of implementation and portability are important aspects of the Mentat approach <ref> [2] </ref>, but these benefits are moot if there is no performance advantage as well. Since Mentat provides a framework within which the programmer and compiler cooperate to parallelize applications, performance gains through parallel execution are realized.
Reference: [3] <author> A. S. Grimshaw, </author> <title> An Introduction to Parallel Object-Oriented Programming with Mentat, </title> <institution> TR-91-07, Department of Computer Science, University of Virginia, </institution> <month> April, </month> <year> 1991. </year>
Reference-contexts: The Mentat approach exploits the object-oriented paradigm to provide high-level abstractions that mask the complex aspects of parallel programming, communication, synchronization, and scheduling, from the programmer <ref> [2, 3] </ref>. Instead of managing these details, the programmer concentrates on the 3 application. The programmer uses application domain knowledge to specify those object classes that are of sufficient computational complexity to warrant parallel execution. The parallelization of these complex tasks is handled by Mentat.
Reference: [4] <author> A. S. Grimshaw, V. E. Vivas, </author> <title> FALCON: A Distributed Scheduler for MIMD Architectures, </title> <booktitle> Proceedings of the Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pp. 149-163, </pages> <address> Atlanta, GA, </address> <month> March, </month> <year> 1991. </year>
Reference-contexts: The best times and not averages are used to calculate the speed-ups for each application and architecture. This is done to demonstrate the capabilities of Mentat. Some of the irregularities seen in the graphs are due to the Random algorithm used by the scheduler <ref> [4] </ref>. We observe that the scheduler does not perform well at high loads or when the number of available processors is less than the number of processes resulting from the decomposition of the application. 4.1. In-core Matrix Multiplication This matrix multiplication utility operates on any two matrices already in memory.
Reference: [5] <author> A. S. Grimshaw, E. West and W. R. Pearson, </author> <title> No Pain and Gain! - Experiences with Mentat on a Biological Application, </title> <booktitle> to appear in Proceedings of Symposium on High Performance Distributed Computing, </booktitle> <address> Syracuse, NY, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: A typical library size is in the range of ten to twenty thousand sequences. 4.4.3. Performance The performance speed-ups for this application and observations on them are given in <ref> [5] </ref>. As can be seen in Figure 7., the speed-up is almost linear for Smith-Waterman on both architectures. FASTA on the iPSC/2 and Sparcs suffers from very small computation granularity. This is quite apparent as speed-up ceases to improve with additional processors.
References-found: 5

