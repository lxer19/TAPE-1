URL: ftp://ftp.cs.man.ac.uk/pub/ai/jls/aisb95.ps.Z
Refering-URL: http://www.cs.man.ac.uk/ai/jls/jls.html
Root-URL: http://www.cs.man.ac.uk
Title: Maximum Entropy Analysis of Genetic Algorithm Operators  
Author: Jonathan L. Shapiro and Adam Prugel-Bennett 
Address: Manchester Manchester, M13 9PL, UK 2 NORDITA Blegdamsvej 17, DK-2100 Copenhagen O, Denmark  
Affiliation: 1 Department of Computer Science University of  
Abstract: A maximum entropy approach is used to derive a set of equations describing the evolution of a genetic algorithm involving crossover, mutation and selection. The problem is formulated in terms of cumulants of the fitness distribution. Applying this method to very simple problems, the dynamics of the genetic algorithm can be reduced to a set of nonlinear coupled difference equations which give good results when truncated to four variables. These equations correctly predict the best fitness after convergence as a function of the genetic search parameters. Suggestions concerning annealing rates for time-dependent search parameters are also discussed. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> John H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <institution> University of Michigan Press (Ann Arbor), </institution> <year> 1975. </year>
Reference-contexts: Although a great deal of understanding about how genetic algorithms work has come from empirical observations, genetic algorithms are not well understood theoretically. This may be a controversial view, because many theoretical approaches have been proposed. These include static studies of the fitness landscape, such as hyperplane analysis <ref> [1] </ref> and deception analysis [2], Markov chain analysis [3, 4], and direct methods for small or special problems (e.g. [5]), and others.
Reference: 2. <author> David E. Goldberg. </author> <title> Genetic algorithms and walsh functions: Part II, deception and its analysis. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 153-171, </pages> <year> 1990. </year>
Reference-contexts: This may be a controversial view, because many theoretical approaches have been proposed. These include static studies of the fitness landscape, such as hyperplane analysis [1] and deception analysis <ref> [2] </ref>, Markov chain analysis [3, 4], and direct methods for small or special problems (e.g. [5]), and others.
Reference: 3. <author> A Nix and Michael D. Vose. </author> <title> Modeling genetic algorithms with markov chains. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 79-88, </pages> <year> 1991. </year>
Reference-contexts: This may be a controversial view, because many theoretical approaches have been proposed. These include static studies of the fitness landscape, such as hyperplane analysis [1] and deception analysis [2], Markov chain analysis <ref> [3, 4] </ref>, and direct methods for small or special problems (e.g. [5]), and others.
Reference: 4. <author> H. Muhlenbein. </author> <title> Evolution in space and time the parallel genetic algorithm. </title> <editor> In G. Rawlins., editor, </editor> <booktitle> Foundations of Genetic Algorithms. </booktitle> <address> Morgan-Kaufman, </address> <year> 1991. </year>
Reference-contexts: This may be a controversial view, because many theoretical approaches have been proposed. These include static studies of the fitness landscape, such as hyperplane analysis [1] and deception analysis [2], Markov chain analysis <ref> [3, 4] </ref>, and direct methods for small or special problems (e.g. [5]), and others. <p> If the mutation rate is small enough to find a good solution, the convergence will be very slow. (It has been shown in the ones-counting problem that the optimal mutation rate is also 1=N <ref> [4] </ref>.) Thus, there is a trade-off. Small mutation rate gives slow convergence to a good solution; larger mutation rate gives rapid convergence to a poor solution.
Reference: 5. <author> Darrell Whitley. </author> <title> An executable model of a simple genetic algorithm. </title> <editor> In L. Darrel Whitley, editor, </editor> <booktitle> Foundations of Genetic Algorithms 2. </booktitle> <publisher> Morgan Kaufmann (San Mateo), </publisher> <year> 1993. </year>
Reference-contexts: This may be a controversial view, because many theoretical approaches have been proposed. These include static studies of the fitness landscape, such as hyperplane analysis [1] and deception analysis [2], Markov chain analysis [3, 4], and direct methods for small or special problems (e.g. <ref> [5] </ref>), and others.
Reference: 6. <author> John J. Grefenstette. </author> <title> Deception considered harmful. </title> <editor> In L. Darrel Whitley, editor, </editor> <booktitle> Foundations of Genetic Algorithms 2. </booktitle> <publisher> Morgan Kaufmann (San Mateo), </publisher> <year> 1993. </year>
Reference-contexts: Although these techniques may be useful in providing insight as to how GAs work (or in some cases, it has been argued <ref> [6] </ref>, erroneous insight), it is still difficult to get quantitative results concerning the effectiveness of GA search for different parameter settings, for different types of problems, or for different types of representations. This is an important goal of a theoretical description of genetic algorithms.
Reference: 7. <author> A. Prugel-Bennett and J. L. Shapiro. </author> <title> An analysis of genetic algorithms using statistical mechanics. </title> <journal> Phys. Rev. Letts., </journal> <volume> 72(9) </volume> <pages> 1305-1309, </pages> <year> 1994. </year>
Reference-contexts: This is an important goal of a theoretical description of genetic algorithms. In previous papers <ref> [7, 8] </ref>, we proposed a "statistical mechanics" approach to the study of genetic algorithm dynamics. This approach was fairly crude and approximate, although we found good agreement with simulations in the cases we tested. <p> The change in width of fitness distribution after selection starting from a Gaussian distribution for P = 2 5 , 2 10 and 2 20 . The solid lines are calculated by numerical integration. 4.1 Selection Selection has been described in detail elsewhere <ref> [7, 8] </ref>, so will be considered briefly here.
Reference: 8. <author> J. L. Shapiro, A. Prugel-Bennett, and M. Rattray. </author> <title> A statistical mechanical formulation of the dynamics of genetic algorithms. </title> <editor> In Terence C. Fogarty, editor, </editor> <booktitle> Evolutionary Computing: AISB Workshop, </booktitle> <address> Leeds, U.K., </address> <month> April </month> <year> 1994, </year> <booktitle> Selected Papers, volume 864 of Lecture Notes in Computer Science, </booktitle> <pages> pages 17-27. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: This is an important goal of a theoretical description of genetic algorithms. In previous papers <ref> [7, 8] </ref>, we proposed a "statistical mechanics" approach to the study of genetic algorithm dynamics. This approach was fairly crude and approximate, although we found good agreement with simulations in the cases we tested. <p> The change in width of fitness distribution after selection starting from a Gaussian distribution for P = 2 5 , 2 10 and 2 20 . The solid lines are calculated by numerical integration. 4.1 Selection Selection has been described in detail elsewhere <ref> [7, 8] </ref>, so will be considered briefly here.
Reference: 9. <author> A. Prugel-Bennett and J. L. Shapiro. </author> <title> Dynamics of genetic algorithms for the ising spin-glass chain. </title>
Reference-contexts: In previous papers [7, 8], we proposed a "statistical mechanics" approach to the study of genetic algorithm dynamics. This approach was fairly crude and approximate, although we found good agreement with simulations in the cases we tested. We have since developed a more rigorous formulation <ref> [9] </ref> and better understanding of the assumption which was implicit in that approach; this is basically a maximum entropy assumption. <p> The assumption is likely to be true if the genetic operators are sufficiently mixing to make all populations with given fitness properties equally likely, and if the fitness properties chosen are appropriate in describing the system. Some direct numerical tests appear in <ref> [9] </ref> and show that the assumption is very good (but not perfect). 0 50 100 150 200 Generation 0.0 100.0 F best ff h 2 i Fig. 1. The solid curves show the evolution of the first two cumulants and the fitness of the best member of the population. <p> The latter makes the problem NP-complete, and we have not yet looked at it. The two tasks are analysed in great detail in <ref> [9] </ref>. A third task to which this approach has been applied is the task of training in a binary perceptron [11]. This is a truly hard problem for which there is no known efficient algorithm; simulated annealing and genetic algorithms give the best results.
Reference: 10. <author> Thomas Cover and Joy Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: That is, of all the populations which have the same distribution of fitnesses, the more numerous ones are most likely to describe the actual one (see, for example, <ref> [10] </ref> for a general description of the maximum entropy method). In earlier work, we have called this a statistical mechanics approach to studying GAs. This is in analogy with the way the statistical mechanics infers properties about microstates such as energy distributions from thermodynamics.
Reference: 11. <author> Magnus Rattray. </author> <title> An analysis of a genetic algorithm training the binary perceptron. </title> <type> Master's thesis, </type> <institution> University of Manchester, </institution> <year> 1995. </year>
Reference-contexts: The latter makes the problem NP-complete, and we have not yet looked at it. The two tasks are analysed in great detail in [9]. A third task to which this approach has been applied is the task of training in a binary perceptron <ref> [11] </ref>. This is a truly hard problem for which there is no known efficient algorithm; simulated annealing and genetic algorithms give the best results. There has been progress, but not complete success, toward deriving the dynamical equations for this problem.
Reference: 12. <author> Magnus Rattray. </author> <title> An analysis of a genetic algorithm solving the subset-sum problem. </title> <note> To appear in Complex Systems, </note> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style. </title>
Reference-contexts: There has been progress, but not complete success, toward deriving the dynamical equations for this problem. The techniques have also been applied to the subset sum problem <ref> [12] </ref>, an NP-complete problem which can be solved by pseudo-polynomial algorithms. These results will not be discussed in this paper. 4 Genetic Search Operators We now discuss some of the results which have been found concerning the genetic search operators.
References-found: 12

