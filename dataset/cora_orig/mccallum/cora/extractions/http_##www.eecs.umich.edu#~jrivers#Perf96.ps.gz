URL: http://www.eecs.umich.edu/~jrivers/Perf96.ps.gz
Refering-URL: http://www.eecs.umich.edu/~jrivers/
Root-URL: http://www.cs.umich.edu
Email: fjrivers,davidsong@eecs.umich.edu  
Title: Performance Issues in Integrating Temporality-Based Caching with Prefetching  
Author: Jude A. Rivers and Edward S. Davidson 
Address: Ann Arbor, MI 48109-2122, USA  
Affiliation: Advanced Computer Architecture Laboratory, Department of Electrical Engineering and Computer Science, The University of Michigan,  
Abstract: This work evaluates the performance effectiveness of combining two techniques for improving cache hit rate and reducing memory traffic in small on-chip direct-mapped caches. Temporality-based caching is an efficient technique for reducing unnecessary cache block conflicts in direct-mapped caches, but does not address compulsory misses. Tagged prefetching is a known technique for controlling compulsory misses, but has the potential for introducing high block interference, cache pollution and increased memory traffic in small direct-mapped caches. We propose and evaluate a group of caching strategies that integrate various combinations of temporality-based caching and tagged prefetching. Some combinations show both a remarkable improvement in hit rate and a substantial reduc tion in memory traffic.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.J. Smith, </author> <title> Bibliography and Reading on CPU Cache Memories and Related Topics, Computer Architecture News , Jan 1986. </title>
Reference: [2] <author> A.J. Smith, </author> <title> Second Bibliography on Cache Memories, Computer Architecture News , Jan 1991. </title>
Reference: [3] <author> M. Hill, </author> <title> A Case for Direct-Mapped Caches, </title> <booktitle> IEEE Computer 21 (1988). </booktitle>
Reference: [4] <author> J.M. Mulder, N.T. Quach and M.J. Flynn, </author> <title> An Area Model for On-Chip Memories and its Application, </title> <journal> IEEE Journal of Solid-State Circuits 26 (1991). </journal> <volume> 21 </volume>
Reference: [5] <author> J.A. Rivers and E.S. Davidson, </author> <title> Reducing Conflicts in Direct-Mapped Caches with a Temporality-Based Design, </title> <booktitle> Proc. 1996 International Conference on Parallel Processing , August 1996. </booktitle>
Reference-contexts: However, prefetching can introduce conflicts in the cache, especially for caches with little or no associativity. Such conflicts can be costly if a substantial fraction of the prefetched data tend to be non-temporal <ref> [5] </ref> or superfluous. Non-temporal and/or superfluous data are responsible for unnecessary conflict misses and cache pollution. In [5], we showed the efficiency of temporality-based caching in controlling cache block conflicts and reducing the overall miss ratio for direct-mapped caches. This technique, however, does not handle compulsory misses. <p> Such conflicts can be costly if a substantial fraction of the prefetched data tend to be non-temporal <ref> [5] </ref> or superfluous. Non-temporal and/or superfluous data are responsible for unnecessary conflict misses and cache pollution. In [5], we showed the efficiency of temporality-based caching in controlling cache block conflicts and reducing the overall miss ratio for direct-mapped caches. This technique, however, does not handle compulsory misses. The effectiveness of cache prefetching [6] in reducing compulsory (and some capacity) misses has been well demonstrated. <p> In practice however, a high reference level in this class is not easily attainable because of various factors. On the other hand, references from the Spatial Non-temporal and Non-temporal Non-spatial brackets and, to some extent, the Temporal Non-spatial class, are known to disrupt good cache behavior <ref> [5] </ref>. The Spatial Non-temporal class, mostly dominated by numerical arrays, is often a nuisance for regular caching and yet can be a disaster for cache bypassing 2 , especially in situations where no accommodation is provided for exploiting spatial locality. <p> We will also present a theoretical comparison of the two designs versus both a regular and an optimal direct-mapped cache for some simple but common program reference patterns. The temporality-based caching strategy has been adopted in the Assist Cache [9] and in the proposed Non-Temporal Streaming (NTS) Cache <ref> [5] </ref> to minimize unnecessary block conflicts and to prevent cache pollution. <p> The HP7200 Assist Cache implementation uses a 2KB buffer with 64 entries of 32-byte blocks. 17 loop nest and then used in a subsequent loop nest. APPBT, APPSP, and FFT--PDE are dominated by inter-loop reuse arrays <ref> [5] </ref>. For APPBT and APPSP however, significant inter-loop reuse only becomes obvious as cache size gets larger, i.e. when array self-conflicts and cross-conflicts are substantially reduced permitting temporal reuse.
Reference: [6] <author> A.J. Smith, </author> <title> Cache Memories, </title> <booktitle> ACM Computing Surveys 14 (1982) 473-530. </booktitle>
Reference-contexts: Non-temporal and/or superfluous data are responsible for unnecessary conflict misses and cache pollution. In [5], we showed the efficiency of temporality-based caching in controlling cache block conflicts and reducing the overall miss ratio for direct-mapped caches. This technique, however, does not handle compulsory misses. The effectiveness of cache prefetching <ref> [6] </ref> in reducing compulsory (and some capacity) misses has been well demonstrated. However, cache prefetching has the potential for introducing block interference, cache pollution and a sharp upward surge in cache memory traffic. <p> Hence, prefetching this next sequential line into the cache should reduce the number of cache misses. Common systematic prefetching policies include Always Prefetch, Prefetch On Misses and Tagged Prefetch <ref> [6] </ref>. On an access to cache block B, the Always Prefetch technique always prefetches the next cache block, B+1. The Prefetch On Misses technique prefetches cache block B+1 only if the access to cache block B missed the cache.
Reference: [7] <author> S.G. Abraham, R.A. Sugamar, B.R. Rau and R. Gupta, </author> <title> Predictability of Load/Store Instruction Latencies, </title> <booktitle> Proc. 26th International Symposium on Microarchitectures , Dec 1993, </booktitle> <pages> 139-152. </pages>
Reference-contexts: In a recent study <ref> [7] </ref>, Abraham et al. showed through application code profiling that a very small number of load/store instructions are responsible for causing a disproportionate percentage of cache misses. Their findings provide an important direction for cache management research. Caches derive their efficacy from the exploitation of reference locality in application programs.
Reference: [8] <author> T.P. Shih, </author> <title> Goal-Directed Performance Tuning for Scientific Applications, </title> <type> Ph.D. Thesis, </type> <institution> EECS Dept., The University of Michigan, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: The Spatial Non-temporal class, mostly dominated by numerical arrays, is often a nuisance for regular caching and yet can be a disaster for cache bypassing 2 , especially in situations where no accommodation is provided for exploiting spatial locality. A streaming buffer technique <ref> [8] </ref> is often recommended for such patterns. A reference from the Non-temporal Non-spatial class is typically a candidate for cache bypassing since the data will only be used once in the immediate future. <p> A reference from the Non-temporal Non-spatial class is typically a candidate for cache bypassing since the data will only be used once in the immediate future. The Temporal Non-spatial category can boost the gains of efficient caching if software techniques like copying and spilling <ref> [8] </ref> are easily applicable to the code. Temporal reuse is of utmost importance in cache operation because it increases the overall system performance by reducing unnecessary processor stalls and thus total program execution time.
Reference: [9] <author> G. Kurpanek, K. Chan, J. Zheng, E. DeLano and W. Bryg, PA7200: </author> <title> A PA-RISC Processor with Integrated High Performance MP Bus Interface, </title> <booktitle> COMPCON Digest of Papers , Feb 1994, </booktitle> <pages> 375-382. </pages>
Reference-contexts: We will also present a theoretical comparison of the two designs versus both a regular and an optimal direct-mapped cache for some simple but common program reference patterns. The temporality-based caching strategy has been adopted in the Assist Cache <ref> [9] </ref> and in the proposed Non-Temporal Streaming (NTS) Cache [5] to minimize unnecessary block conflicts and to prevent cache pollution. The NTS Cache is a design approach that reduces conflict misses in direct-mapped caches; and the Assist Cache of [9] shows similar behavior when implemented in the HP7200 microprocessor. 3.1 The <p> The temporality-based caching strategy has been adopted in the Assist Cache <ref> [9] </ref> and in the proposed Non-Temporal Streaming (NTS) Cache [5] to minimize unnecessary block conflicts and to prevent cache pollution. The NTS Cache is a design approach that reduces conflict misses in direct-mapped caches; and the Assist Cache of [9] shows similar behavior when implemented in the HP7200 microprocessor. 3.1 The Assist Cache The HP-7200 Assist Cache [9,10] design provides a methodology for avoiding both block interference and cache pollution before they actually happen.
Reference: [10] <author> E. Rashid, </author> <title> A CMOS RISC CPU with On-Chip Parallel Cache, </title> <booktitle> ISSCC Digest of Papers , Feb 1994, </booktitle> <pages> 210-211. </pages>
Reference-contexts: If the check produces a hit, the data is returned directly to the requesting functional unit. To achieve the rigorous timing requirements needed for the NT buffer lookup, it may be necessary to use aggressive self-timed logic <ref> [10] </ref>. For control purposes, hardware monitoring and bit setting logic combines with the NTDU in recording the reuse behavior of every block loaded into the main cache. Initially, all blocks are untagged. When an untagged block is first referenced, it is loaded into the main cache.
Reference: [11] <author> S. McFarling, </author> <title> Cache Replacement with Dynamic Exclusion, </title> <booktitle> Proc. 5th International Conference on Architectural Support for Programming Languages and Operating System , Oct 1992, </booktitle> <pages> 191-200. </pages>
Reference-contexts: However, if too many such blocks have their NT bit set, there can be a capacity problem in the NT buffer. 8 3.3 Temporality-Based Caching on Common Reference Patterns We briefly examine the temporality-based caching policy's effect on three most common sources of reference conflicts <ref> [11] </ref>. These reference patterns include: 1) conflict between references in two different loops; 2) conflict between a reference inside a loop with a reference outside the loop; and 3) conflict between two references within the same loop.
Reference: [12] <author> N. Drach, </author> <title> Hardware Implementation Issues of Data Prefetching, </title> <booktitle> Proc. 1995 International Conference on Supercomputing , June 1995. </booktitle>
Reference-contexts: For simplicity, we limit our focus in this preliminary work to hardware-controlled prefetching, even though our design could further benefit from the use of software-controlled schemes. Most hardware-controlled prefetching mechanisms are systematic in nature, though more complex hardware schemes have been proposed that rely on stride prediction tables <ref> [12] </ref> to automatically compute the stride of an array reference and prefetch accordingly. Even though such schemes are powerful, they are particularly costly due to table size requirements; furthermore, they must still implement the same basic hardware support required for standard prefetching techniques.
Reference: [13] <author> N.P. Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers, </title> <booktitle> Proc. 17th International Symposium on Computer Architecture , May 1990, </booktitle> <pages> 364-373. </pages>
Reference-contexts: The victim block assumes the LRU position in the Assist buffer. In a sense, the Assist buffer acts as a victim cache <ref> [13] </ref> for blocks being evicted from the direct-mapped unit.
Reference: [14] <author> J. Chatterjee, J. Jin and J. Volakis, </author> <title> Application of Edge-based Finite Elements and ABCs to 3-D Scattering, </title> <journal> IEEE Transactions on Antennas and Propagation, </journal> <month> Feb </month> <year> 1993, </year> <month> 221-226. </month> <title> 22 a) Change in Cache Misses b) Change in Memory Traffic Fig. 6. Normalized Performance Data. All numbers are relative percentage change w.r.t. DM (8KB, 32B blocks). The lower the number, the better is the improvement. 23 a) Cache Misses b) Memory Traffic Fig. 7. Comparative Performance Across The Integrated Caching Techniques 24 </title>
Reference-contexts: Significant among the miscellaneous programs is FEMC, which is a finite element application code that evaluates electromagnetic backscatter from a distant object. This code was developed by the University of Michigan Radiation Laboratory <ref> [14] </ref> and is in production use. The size of the programs range from 200 to 8000 non-comment lines and are all written in FORTRAN, with the exception of EQNTOTT which is in C. Their execution times range from a few seconds to a few hours on the IBM RS/6000.
References-found: 14

