URL: http://www.research.ibm.com/kitchawan/OSDI99-to-be-published.ps
Refering-URL: http://www.research.ibm.com/kitchawan/
Root-URL: http://www.research.ibm.com
Email: fben,jonathan,stummg@eecg.toronto.edu  okrieg@us.ibm.com  
Title: Tornado: Maximizing Locality and Concurrency in a Shared Memory Multiprocessor Operating System  
Author: Ben Gamsa Orran Krieger Jonathan Appavoo Michael Stumm 
Address: Toronto, Toronto, Canada  Yorktown Heights, New York  
Affiliation: Department of Electrical and Computer Engineering University of  IBM T.J. Watson Research Center  
Abstract: We describe the design and implementation of Tornado, a new operating system designed from the ground up specifically for today's shared memory multiprocessors. The need for improved locality in the operating system is growing as multiprocessor hardware evolves, increasing the costs for cache misses and sharing, and adding complications due to NUMAness. Tornado is optimized so that locality and independence in application requests for operating system serviceswhether from multiple sequential applications or a single parallel application are mapped onto locality and independence in the servicing of these requests in the kernel and system servers. By contrast, previous shared memory multiprocessor operating systems all evolved from designs constructed at a time when sharing costs were low, memory latency was low and uniform, and caches were small; for these systems, concurrency was the main performance concern and locality was not an important issue. Tornado achieves this locality by starting with an object-oriented structure, where every virtual and physical resource is represented by an independent object. Locality, as well as concurrency, is further enhanced with the introduction of three key innovations: (i) clustered objects that support the partitioning of contended objects across processors, (ii) a protected procedure call facility that preserves the locality and concurrency of IPC's, and (iii) a new locking strategy that allows all locking to be encapsulated within the objects being protected and greatly simplifies the overall locking protocols. As a result of these techniques, Tornado has far better performance characteristics, particularly for multithreaded applications, than existing commercial operating systems. Tornado has been fully implemented and runs both on Toronto's NUMAchine hardware and on the SimOS simulator. 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> M. Auslander, H. Franke, O. Krieger, B. Gamsa, and M. Stumm. Customization-lite. </author> <booktitle> In 6th Workshop on Hot Topics in Operating Systems (HotOS-VI), </booktitle> <pages> pages 43-48, </pages> <year> 1997. </year>
Reference-contexts: Although our object-oriented structure is not the only way to get the benefits of locality and concurrency, it is a natural structuring technique and provides the foundation for other Tornado features such as the clustered object system [ 23 ] and the building block system <ref> [ 1 ] </ref> . 3 Clustered Objects Although the object-oriented structure of Tornado can help reduce contention and increase locality by mapping independent resources to independent objects, some components, such as a File Cache Manager (FCM) for a shared file, a Process object for a parallel program, or the system DRAM
Reference: [ 2 ] <author> B. Bershad. </author> <title> The increasing irrelevance of IPC performance for microkernel-based operating systems. </title> <booktitle> In Proc. USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 205-212, </pages> <year> 1992. </year>
Reference-contexts: Protected procedure call. The majority of research on performance conscious inter-process communication (IPC) is for uniprocessor systems. Excellent results have been reported for these systems, to the point where it has been argued that the IPC overhead has become largely irrelevant <ref> [ 2 ] </ref> . 20 Although many results have been reported over the years on a number of different platforms, the core cost for a call-return pair (with similar functionality) is usually between 100 and 200 instructions [ 10, 11, 16, 19 ] .
Reference: [ 3 ] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Trans. Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: However, the Tornado PPC facility goes beyond the standard techniques used to optimize IPC performance, by optimizing for the multiprocessor case by eliminating locks and shared data accesses, and by providing concur-rency to the servers. The key previous work done in multiprocessor IPC was by Bershad et al <ref> [ 3 ] </ref> , where excellent results were obtained on the hardware of the time. However, it is interesting that the recent changes in technology lead to design tradeoffs far different from what they used to be.
Reference: [ 4 ] <author> E. Bugnion, S. Devine, K. Govil, and M. Rosenblum. </author> <title> Disco: running commodity operating systems on scalable multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 15(4) </volume> <pages> 412-447, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: However, both groups ran into complexity problems with this approach and both have moved on to other approaches: Disco <ref> [ 4 ] </ref> and Tornado, respectively. Clustered objects. Concepts similar to clustered objects have appeared in a number of distributed systems, most notably in Globe [ 18 ] and SOS [ 20 ] .
Reference: [ 5 ] <author> M. Campbell et al. </author> <title> The parallelization of UNIX system V release 4.0. </title> <booktitle> In Proc. USENIX Technical Conference, </booktitle> <pages> pages 307-324, </pages> <year> 1991. </year>
Reference-contexts: investigate the performance of Tornado when other resources are shared, and study the performance of our system for real applications. 8 Related work A number of papers have been published on performance issues in shared-memory multiprocessor operating systems, mostly in the context of resolving specific problems in a specific system <ref> [ 5, 6, 8, 21, 24, 26 ] </ref> . These systems were mostly uniprocessor or small-scale multiprocessor Unix systems trying to scale up to larger systems. Two process. The bottom row (d-f) depicts the multiprogrammed tests with n processes, each with one thread.
Reference: [ 6 ] <author> J. Chapin, S. A. Herrod, M. Rosenblum, and A. Gupta. </author> <title> Memory system performance of UNIX on CC-NUMA multiprocessors. </title> <booktitle> In Proc. ACM SIGMETRICS Intl. Conf. on Measurement and Modelling of Computer Systems, </booktitle> <year> 1995. </year>
Reference-contexts: investigate the performance of Tornado when other resources are shared, and study the performance of our system for real applications. 8 Related work A number of papers have been published on performance issues in shared-memory multiprocessor operating systems, mostly in the context of resolving specific problems in a specific system <ref> [ 5, 6, 8, 21, 24, 26 ] </ref> . These systems were mostly uniprocessor or small-scale multiprocessor Unix systems trying to scale up to larger systems. Two process. The bottom row (d-f) depicts the multiprogrammed tests with n processes, each with one thread.
Reference: [ 7 ] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teo-dosio, and A. Gupta. Hive: </author> <title> Fault containment for shared-memory multiprocessors. </title> <booktitle> In Proc. of the 15th ACM Symp. on Operating Systems Principles (SOSP), </booktitle> <pages> pages 12-25, </pages> <year> 1995. </year>
Reference-contexts: The systems on which the tests were run are: SGI Origin 2000 running IRIX 6.4, Convex SPP-1600 running SPP-UX 4.2, IBM 7012-G30 PowerPC 604 running AIX 4.2.0.0, Sun 450 UltraSparc II running Solaris 2.5.1, and NUMAchine running Tornado. projects that were aimed explicitly at large-scale multiprocessors were Hive <ref> [ 7 ] </ref> , and the precursor to Tornado, Hurricane [ 28 ] . Both independently chose a clustered approach by connecting multiple small-scale systems to form either, in the case of Hive, a more fault tolerant system, or, in the case of Hurricane, a more scalable system.
Reference: [ 8 ] <author> D. R. Cheriton and K. J. Duda. </author> <title> A caching model of operating system kernel functionality. </title> <booktitle> In Proc. Symp. on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 179-193, </pages> <year> 1994. </year>
Reference-contexts: investigate the performance of Tornado when other resources are shared, and study the performance of our system for real applications. 8 Related work A number of papers have been published on performance issues in shared-memory multiprocessor operating systems, mostly in the context of resolving specific problems in a specific system <ref> [ 5, 6, 8, 21, 24, 26 ] </ref> . These systems were mostly uniprocessor or small-scale multiprocessor Unix systems trying to scale up to larger systems. Two process. The bottom row (d-f) depicts the multiprogrammed tests with n processes, each with one thread.
Reference: [ 9 ] <author> H. Custer. </author> <title> Inside Windows NT. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: For example, without a global page cache, it is difficult to implement global policies like a clock replacement algorithm in its purest form. Memory management in Tornado is based on a working set policy (similar to that employed by NT <ref> [ 9 ] </ref> ), and most decisions can be made local to FCMs. In Tornado, most operating system objects have multiple implementations, and the client or system can choose the best implementation to use at run time.
Reference: [ 10 ] <author> D. R. Engler, M. F. Kaashoek, and Jr. O'Toole J. Exoker-nel: </author> <title> an operating system architecture for application-level resource management. </title> <booktitle> In Proc. 15th ACM Symp. on Operating Systems Principles, </booktitle> <pages> pages 251-266, </pages> <year> 1995. </year>
Reference-contexts: where it has been argued that the IPC overhead has become largely irrelevant [ 2 ] . 20 Although many results have been reported over the years on a number of different platforms, the core cost for a call-return pair (with similar functionality) is usually between 100 and 200 instructions <ref> [ 10, 11, 16, 19 ] </ref> . However, the Tornado PPC facility goes beyond the standard techniques used to optimize IPC performance, by optimizing for the multiprocessor case by eliminating locks and shared data accesses, and by providing concur-rency to the servers.
Reference: [ 11 ] <author> B. Ford and J. Lepreau. </author> <title> Evolving Mach 3.0 to a migrating thread model. </title> <booktitle> In Proc. USENIX Technical Conference, </booktitle> <pages> pages 97-114, </pages> <year> 1994. </year>
Reference-contexts: where it has been argued that the IPC overhead has become largely irrelevant [ 2 ] . 20 Although many results have been reported over the years on a number of different platforms, the core cost for a call-return pair (with similar functionality) is usually between 100 and 200 instructions <ref> [ 10, 11, 16, 19 ] </ref> . However, the Tornado PPC facility goes beyond the standard techniques used to optimize IPC performance, by optimizing for the multiprocessor case by eliminating locks and shared data accesses, and by providing concur-rency to the servers.
Reference: [ 12 ] <author> B. Gamsa, O. Krieger, and M. Stumm. </author> <title> Optimizing IPC performance for shared-memory multiprocessors. </title> <booktitle> In Proc. ICPP, </booktitle> <pages> pages 208-211, </pages> <year> 1994. </year>
Reference-contexts: Concurrency and locality in these communications are crucial to maintaining the high performance that exists within the various Tornado subsystems and servers. Tornado uses a Protected Procedure Call (PPC) model <ref> [ 12 ] </ref> , where a call from a client object to a server object acts like a clustered object call that crosses from the protection domain of the client to that of the server, and then back on completion.
Reference: [ 13 ] <editor> A. Grbic et al. </editor> <booktitle> Design and implementation of the NUMA-chine multiprocessor. In Proceedings of the 35rd DAC, </booktitle> <pages> pages 66-69, </pages> <year> 1998. </year>
Reference-contexts: Although we focus primarily on the Tornado kernel, it is important to note that these components are also used in the implementation of the Tornado system servers. Tornado is fully implemented (in C++), and runs on our 16 processor NUMAchine <ref> [ 13, 29 ] </ref> and on the SimOS simulator [ 25 ] ; it supports most of the facilities (e.g., shells, compilers, editors) and services (pipes, TCP/IP, NFS, file system) one expects. <p> (including 1300 for two remote interrupt exchanges), plus the cost of four cache transfers (a pair for each of the remote PPC call and return exchanges). 17 7 Experimental results The results presented in this paper are based on both hardware tests on a locally developed 16 processor NUMA-chine prototype <ref> [ 13, 29 ] </ref> and the SimOS simulator from Stanford [ 25 ] . The NUMAchine architecture consists of a set of stations (essentially small, bus-based multiprocessors) connected by a hierarchy of rings. It uses a novel selective broadcast mechanism to efficiently handle invalidations, as well as broadcast data.
Reference: [ 14 ] <author> M. Greenwald and D.R. Cheriton. </author> <title> The synergy between non-blocking synchronization and operating system structure. </title> <booktitle> In Symp. on Operating System Design and Implementation, </booktitle> <pages> pages 123-136, </pages> <year> 1996. </year>
Reference-contexts: However, practical algorithms often require additional instructions not currently found on modern processors, and they have their own difficulties in dealing with memory deallocation <ref> [ 14, 17 ] </ref> for read-only objects).
Reference: [ 15 ] <author> D. Grunwald, B. G. Zorn, and R. Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In Proc. Conf. on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 177-186, </pages> <year> 1993. </year>
Reference-contexts: A survey paper by Wilson et al [ 31 ] covers many of the other schemes, but does not address multiprocessor or caching issues. Grunwald et al examined cache performance of allocation schemes <ref> [ 15 ] </ref> and suggest a number of techniques they felt would be most effective in dealing with locality issues. Most of these techniques can be found in the McKenney and Slingwine memory allocator (with a few additions in our own adaptation). Synchronization.
Reference: [ 16 ] <author> G. Hamilton and P. Kougiouris. </author> <title> The Spring nucleus: A microkernel for objects. </title> <booktitle> In Proc. USENIX Summer Technical Conference, </booktitle> <year> 1993. </year>
Reference-contexts: where it has been argued that the IPC overhead has become largely irrelevant [ 2 ] . 20 Although many results have been reported over the years on a number of different platforms, the core cost for a call-return pair (with similar functionality) is usually between 100 and 200 instructions <ref> [ 10, 11, 16, 19 ] </ref> . However, the Tornado PPC facility goes beyond the standard techniques used to optimize IPC performance, by optimizing for the multiprocessor case by eliminating locks and shared data accesses, and by providing concur-rency to the servers.
Reference: [ 17 ] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data objects. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 745-770, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: However, practical algorithms often require additional instructions not currently found on modern processors, and they have their own difficulties in dealing with memory deallocation <ref> [ 14, 17 ] </ref> for read-only objects). <p> Synchronization. The topic of locking and concur-rency control in general has a long history, as does garbage collection [ 30 ] . The relationship between locking and garbage collection is evident in some of the issues surrounding memory management for lock-free approaches <ref> [ 17 ] </ref> . Our garbage collection scheme is in some sense a hack, but works reasonably well in our environment. Although it is somewhat similar to IBM's patent 4809168, their scheme appears to target uniprocessors only and is less general than ours.
Reference: [ 18 ] <author> P. Homburg, L. van Doorn, M. van Steen, A. S. Tanen-baum, and Wi. de Jonge. </author> <title> An object model for flexible distributed systems. </title> <booktitle> In Proc. of the 1st Annual ASCI Conference, </booktitle> <pages> pages 69-78, </pages> <year> 1995. </year>
Reference-contexts: However, both groups ran into complexity problems with this approach and both have moved on to other approaches: Disco [ 4 ] and Tornado, respectively. Clustered objects. Concepts similar to clustered objects have appeared in a number of distributed systems, most notably in Globe <ref> [ 18 ] </ref> and SOS [ 20 ] . In all these cases (including Tornado's clustered objects) the goal is to hide the distributed nature of the objects from the users of the objects while improving performance over a more naive centralized approach.
Reference: [ 19 ] <editor> T. Jaeger et al. </editor> <booktitle> Achieved IPC performance. In 6th Workshop on Hot Topics in Operating Systems (HotOS-VI), </booktitle> <year> 1997. </year>
Reference-contexts: This leaves 167 instructions for the core cost of a PPC call and return, which compares favorably to the corresponding cost of 158 instructions for two one-way calls for one of the fastest uniprocessor IPC systems running on the same (MIPS R4000 based) processor <ref> [ 19 ] </ref> . In addition, up to 8KB of data can be exchanged between client and server (in both directions) through remapping a region for an additional cost of only 82 instructions. <p> where it has been argued that the IPC overhead has become largely irrelevant [ 2 ] . 20 Although many results have been reported over the years on a number of different platforms, the core cost for a call-return pair (with similar functionality) is usually between 100 and 200 instructions <ref> [ 10, 11, 16, 19 ] </ref> . However, the Tornado PPC facility goes beyond the standard techniques used to optimize IPC performance, by optimizing for the multiprocessor case by eliminating locks and shared data accesses, and by providing concur-rency to the servers.
Reference: [ 20 ] <author> M. Makpangou, Y. Gourhant, J.P. Le Narzul, and M. Shapiro. </author> <title> Fragmented objects for distributed abstractions. </title> <editor> In T. L. Casavant and M. Singhal, editors, </editor> <booktitle> Readings in Distributed Computing Systems, </booktitle> <pages> pages 170-186. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: However, both groups ran into complexity problems with this approach and both have moved on to other approaches: Disco [ 4 ] and Tornado, respectively. Clustered objects. Concepts similar to clustered objects have appeared in a number of distributed systems, most notably in Globe [ 18 ] and SOS <ref> [ 20 ] </ref> . In all these cases (including Tornado's clustered objects) the goal is to hide the distributed nature of the objects from the users of the objects while improving performance over a more naive centralized approach.
Reference: [ 21 ] <author> D. McCrocklin. </author> <title> Scaling Solaris for enterprise computing. </title> <booktitle> In Spring 1995 Cray Users Group Meeting, </booktitle> <year> 1995. </year>
Reference-contexts: investigate the performance of Tornado when other resources are shared, and study the performance of our system for real applications. 8 Related work A number of papers have been published on performance issues in shared-memory multiprocessor operating systems, mostly in the context of resolving specific problems in a specific system <ref> [ 5, 6, 8, 21, 24, 26 ] </ref> . These systems were mostly uniprocessor or small-scale multiprocessor Unix systems trying to scale up to larger systems. Two process. The bottom row (d-f) depicts the multiprogrammed tests with n processes, each with one thread.
Reference: [ 22 ] <author> P. E. McKenney and J. Slingwine. </author> <title> Efficient kernel memory allocation on shared-memory multiprocessor. </title> <booktitle> In Proc. USENIX Technical Conference, </booktitle> <pages> pages 295-305, </pages> <year> 1993. </year>
Reference-contexts: The memory allocator must be efficient, highly concurrent, and maximize locality both in its internal design and in the memory it returns on allocation. The Tornado memory allocation facility is based on the work of McKenney and Slingwine <ref> [ 22 ] </ref> , which is one of the few published designs that maximizes concurrency and locality, and is targeted at multiprocessor kernel environments. Although the original design provides many of the features required, the evolution of hardware and software motivated a few key changes. <p> Hence, the Tornado clustered object system is geared more strongly towards maximizing performance and reducing complexity than the other systems. Dynamic memory allocation. Our dynamic memory allocation design borrows heavily from McKenney and Slingwine's design <ref> [ 22 ] </ref> , which is one of the few pub lished works on multiprocessor memory allocation, in particular for kernel environments. A survey paper by Wilson et al [ 31 ] covers many of the other schemes, but does not address multiprocessor or caching issues.
Reference: [ 23 ] <author> E. Parsons, B. Gamsa, O. Krieger, and M. Stumm. </author> <title> (De-)clustering objects for multiprocessor system software. </title> <booktitle> In Proc. Fourth Intl. Workshop on Object Orientation in Operating Systems (IWOOS95), </booktitle> <pages> pages 72-84, </pages> <year> 1995. </year>
Reference-contexts: Although our object-oriented structure is not the only way to get the benefits of locality and concurrency, it is a natural structuring technique and provides the foundation for other Tornado features such as the clustered object system <ref> [ 23 ] </ref> and the building block system [ 1 ] . 3 Clustered Objects Although the object-oriented structure of Tornado can help reduce contention and increase locality by mapping independent resources to independent objects, some components, such as a File Cache Manager (FCM) for a shared file, a Process object <p> The goal of Tornado's clustered object system is to facilitate the application of these techniques as an integral part of a system's overall design <ref> [ 23 ] </ref> . 3.1 Overview A clustered object presents the illusion of a single object, but is actually composed of multiple component objects, each of which handles calls from a specified subset of the processors (see Figure 4).
Reference: [ 24 ] <author> D. L. Presotto. </author> <title> Multiprocessor streams for Plan 9. </title> <booktitle> In Proc. Summer UKUUG Conf., </booktitle> <pages> pages 11-19, </pages> <year> 1990. </year>
Reference-contexts: investigate the performance of Tornado when other resources are shared, and study the performance of our system for real applications. 8 Related work A number of papers have been published on performance issues in shared-memory multiprocessor operating systems, mostly in the context of resolving specific problems in a specific system <ref> [ 5, 6, 8, 21, 24, 26 ] </ref> . These systems were mostly uniprocessor or small-scale multiprocessor Unix systems trying to scale up to larger systems. Two process. The bottom row (d-f) depicts the multiprogrammed tests with n processes, each with one thread.
Reference: [ 25 ] <author> M. Rosenblum, E. Bugnion, S. Devine, and S. A. Her-rod. </author> <title> Using the SimOS machine simulator to study complex computer systems. </title> <journal> ACM Trans. on Modeling and Computer Simulation, </journal> <volume> 7(1) </volume> <pages> 78-103, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Although we focus primarily on the Tornado kernel, it is important to note that these components are also used in the implementation of the Tornado system servers. Tornado is fully implemented (in C++), and runs on our 16 processor NUMAchine [ 13, 29 ] and on the SimOS simulator <ref> [ 25 ] </ref> ; it supports most of the facilities (e.g., shells, compilers, editors) and services (pipes, TCP/IP, NFS, file system) one expects. <p> of four cache transfers (a pair for each of the remote PPC call and return exchanges). 17 7 Experimental results The results presented in this paper are based on both hardware tests on a locally developed 16 processor NUMA-chine prototype [ 13, 29 ] and the SimOS simulator from Stanford <ref> [ 25 ] </ref> . The NUMAchine architecture consists of a set of stations (essentially small, bus-based multiprocessors) connected by a hierarchy of rings. It uses a novel selective broadcast mechanism to efficiently handle invalidations, as well as broadcast data.
Reference: [ 26 ] <author> J. </author> <title> Talbot. Turning the AIX operating system into an MP-capable OS. </title> <booktitle> In Proc. USENIX Technical Conference, </booktitle> <year> 1995. </year>
Reference-contexts: investigate the performance of Tornado when other resources are shared, and study the performance of our system for real applications. 8 Related work A number of papers have been published on performance issues in shared-memory multiprocessor operating systems, mostly in the context of resolving specific problems in a specific system <ref> [ 5, 6, 8, 21, 24, 26 ] </ref> . These systems were mostly uniprocessor or small-scale multiprocessor Unix systems trying to scale up to larger systems. Two process. The bottom row (d-f) depicts the multiprogrammed tests with n processes, each with one thread.
Reference: [ 27 ] <author> C. P. Thacker and L. C. Stewart. Firefly: </author> <title> a multiprocessor workstation. </title> <booktitle> In Proc. 2nd Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 164-172, </pages> <year> 1987. </year>
Reference-contexts: However, it is interesting that the recent changes in technology lead to design tradeoffs far different from what they used to be. The Firefly multiprocessor <ref> [ 27 ] </ref> on which Bershad's IPC work was developed has a smaller ratio of processor to memory speed, has caches that are no faster than main memory (used to reduce bus traffic), and uses an updating cache consistency protocol.
Reference: [ 28 ] <author> R. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <journal> Journal of Supercomputing, </journal> 9(1/2):105-134, 1995. 
Reference-contexts: are: SGI Origin 2000 running IRIX 6.4, Convex SPP-1600 running SPP-UX 4.2, IBM 7012-G30 PowerPC 604 running AIX 4.2.0.0, Sun 450 UltraSparc II running Solaris 2.5.1, and NUMAchine running Tornado. projects that were aimed explicitly at large-scale multiprocessors were Hive [ 7 ] , and the precursor to Tornado, Hurricane <ref> [ 28 ] </ref> . Both independently chose a clustered approach by connecting multiple small-scale systems to form either, in the case of Hive, a more fault tolerant system, or, in the case of Hurricane, a more scalable system. <p> This approach would be prohibitive in today's systems with the high cost of cache misses and invalidations. 9 Concluding Remarks Tornado was built on our experience with the Hurricane operating system <ref> [ 28 ] </ref> . Hurricane was designed for scalability by loosely coupling small tightly-coupled operating systems images (called clusters) to form a large-scale single system image. This approach is now being employed in one form or another by several commercial systems, for example in SGI's Cellular IRIX.
Reference: [ 29 ] <author> Z. Vranesic et al. </author> <title> The NUMAchine multiprocessor. </title> <type> Technical Report CSRI-324, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <year> 1995. </year>
Reference-contexts: Although we focus primarily on the Tornado kernel, it is important to note that these components are also used in the implementation of the Tornado system servers. Tornado is fully implemented (in C++), and runs on our 16 processor NUMAchine <ref> [ 13, 29 ] </ref> and on the SimOS simulator [ 25 ] ; it supports most of the facilities (e.g., shells, compilers, editors) and services (pipes, TCP/IP, NFS, file system) one expects. <p> (including 1300 for two remote interrupt exchanges), plus the cost of four cache transfers (a pair for each of the remote PPC call and return exchanges). 17 7 Experimental results The results presented in this paper are based on both hardware tests on a locally developed 16 processor NUMA-chine prototype <ref> [ 13, 29 ] </ref> and the SimOS simulator from Stanford [ 25 ] . The NUMAchine architecture consists of a set of stations (essentially small, bus-based multiprocessors) connected by a hierarchy of rings. It uses a novel selective broadcast mechanism to efficiently handle invalidations, as well as broadcast data.
Reference: [ 30 ] <author> P. R. Wilson. </author> <title> Uniprocessor garbage collection techniques. </title> <booktitle> In Intl. Workshop on Memory Management. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Most of these techniques can be found in the McKenney and Slingwine memory allocator (with a few additions in our own adaptation). Synchronization. The topic of locking and concur-rency control in general has a long history, as does garbage collection <ref> [ 30 ] </ref> . The relationship between locking and garbage collection is evident in some of the issues surrounding memory management for lock-free approaches [ 17 ] . Our garbage collection scheme is in some sense a hack, but works reasonably well in our environment.
Reference: [ 31 ] <author> P. R. Wilson, M. S. Johnstone, M. Neely, and D. Boles. </author> <title> Dynamic storage allocation: A survey and critical review. </title> <booktitle> In Intl. Workshop on Memory Management. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Dynamic memory allocation. Our dynamic memory allocation design borrows heavily from McKenney and Slingwine's design [ 22 ] , which is one of the few pub lished works on multiprocessor memory allocation, in particular for kernel environments. A survey paper by Wilson et al <ref> [ 31 ] </ref> covers many of the other schemes, but does not address multiprocessor or caching issues. Grunwald et al examined cache performance of allocation schemes [ 15 ] and suggest a number of techniques they felt would be most effective in dealing with locality issues.
References-found: 31

