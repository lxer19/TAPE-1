URL: http://www.cs.indiana.edu/~owaddell/papers/sas-97.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/owaddell.html
Root-URL: http://www.cs.indiana.edu
Title: Fast and Effective Procedure Inlining  
Author: Oscar Waddell and R. Kent Dybvig 
Address: Bloomington, IN 47405, USA  
Affiliation: Indiana University Computer Science Department  
Abstract: The effectiveness of an inlining algorithm is determined not only by its ability to recognize inlining opportunities but also by its discretion in exercising those opportunities. This paper presents a new inlin-ing algorithm for higher-order languages that combines simple analysis techniques with demand-driven online transformation to achieve consistent and often dramatic performance gains in fast linear time. The algorithm is shown to be as effective as and significantly faster than o*ine, analysis-intensive algorithms recently described in the literature.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: We do not CPS-convert input programs; doing so would not materially simplify the algorithm and could exaggerate the benefits obtained by inlining <ref> [1] </ref>. Table 2. Ratio of code size to original code size with different size and effort limits. Numbers less than 1 indicate a decrease in code size. <p> This means that flow analysis must be repeated after inlining. In the SML/NJ compiler, Appel uses an o*ine algorithm in which a data gathering pass is run alternately with a transformation pass that performs several optimizations including constant folding, fi-contraction, uncurrying, and inlining of functions called only once <ref> [1] </ref>. This transformation phase is iterated until the number of contractions found is below some threshold. Next, inline expansion is performed using several heuristics to estimate the cost and benefit of inlining. To improve these estimates, this "round" of optimization is iterated beginning with the contraction phase.
Reference: 2. <author> Andrew W. Appel and Trevor Jim. </author> <title> Shrinking lambda expressions in linear time. </title> <note> To appear in Journal of Functional Programming. </note>
Reference-contexts: Although our algorithm can be iterated, we have found that few programs actually benefit from additional passes. Appel and Jim recently described a linear-time algorithm that performs con-stant folding, dead-variable elimination, and inlining of functions called only once <ref> [2] </ref>. They have implemented an O (n 2 ) variant of the algorithm to replace the contraction phase of the SML/NJ compiler described above.
Reference: 3. <author> J. Michael Ashley. </author> <title> A practical and flexible flow analysis for higher-order languages. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 184-194, </pages> <year> 1996. </year>
Reference-contexts: Their approach is o*ine: all inlining decisions are made prior to any transformation. Our algorithm combines a less accurate analysis (effectively sub-0CFA <ref> [3] </ref>) with online transformation and polyvariant specialization. The time required by their flow analysis varies widely and can be excessive|110 seconds to analyze dynamic|rendering their method impractical for use in a production compiler. Table 4.
Reference: 4. <author> J. Michael Ashley. </author> <title> The effectiveness of flow analysis for inlining. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN International Conference on Functional Programming, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: It appears that we achieve equivalent or better speedups for all benchmarks except boyer and better code size ratios for all benchmarks except splay, yet our algorithm completes both analysis and transformation in far less time than their analysis alone. Ashley <ref> [4] </ref> evaluated the effectiveness of four different flow analyses for in-lining. The analyses range from a fast analysis less accurate than 0CFA to a polyvariant analysis similar to 1CFA. His inlining algorithm is based on that of Jagannathan and Wright but exposes more opportunities for inlining.
Reference: 5. <author> J. Eugene Ball. </author> <title> Predicting the effects of optimization on a procedure body. </title> <journal> SIG-PLAN Notices, </journal> <volume> 14(8) </volume> <pages> 214-220, </pages> <month> August </month> <year> 1979. </year> <booktitle> Proceedings of the ACM SIGPLAN '79 Symposium on Compiler Construction. </booktitle>
Reference-contexts: Their algorithm is not online, polyvariant, or context-sensitive in the sense we have described. Ball describes an analysis that determines which parameters contribute to the value of the expressions in a procedure body <ref> [5] </ref>. When constant parameters are available at a call site he uses the parameter dependency information to guide inlining decisions with an estimate of code savings and performance gain that is based on predictions about the impact of subsequent optimizations.
Reference: 6. <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <type> Technical Report RC 14756, </type> <institution> IBM, </institution> <year> 1991. </year>
Reference-contexts: Wegman and Zadeck [10] present a fast global constant propagation algorithm and show how it can be extended to perform many of the specializations needed when procedures are inlined. Their algorithm propagates constants through the static single-assignment (SSA) graph <ref> [6] </ref> using a demand-driven traversal of the control-flow graph that facilitates elimination of unreachable code which, in turn, improves the accuracy of the information collected.
Reference: 7. <author> Jeffrey Dean and Craig Chambers. </author> <title> Towards better inlining decisions using inlin-ing trials. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 273-282, </pages> <year> 1994. </year>
Reference-contexts: The new algorithm typically performs as many contractions in one pass as their earlier o*ine algorithm performs in three. They do not consider the more difficult problem of inlining functions called more than once. Dean and Chambers <ref> [7] </ref> describe an online approach in which the cost and benefit of inlining at a call site are estimated by examining the code produced when the routine is tentatively inlined and optimized for the call site.
Reference: 8. <author> Suresh Jagannathan and Andrew Wright. </author> <title> Flow-directed inlining. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 193-205, </pages> <year> 1996. </year>
Reference-contexts: The benchmark programs provided by Jagannathan and Wright <ref> [8] </ref> had already been processed by their local simplification pass, which performs many of the same optimizations as our inliner. To facilitate comparison with their results, performance data was collected on a 150 MHz SGI MIPS R4400 workstation. <p> Increasing the effort limit by an order of magnitude typically only doubles the run time of the algorithm. When the size limit is varied, the run time of the algorithm increases roughly in proportion to the increase in code size. 6 Related Work Jagannathan and Wright <ref> [8] </ref> use a polyvariant flow analysis to identify inlining opportunities and to estimate the size of a procedure when specialized for a particular call site. Their approach is o*ine: all inlining decisions are made prior to any transformation.
Reference: 9. <author> Oscar Waddell and R. Kent Dybvig. </author> <title> Fast and effective procedure inlining. </title> <type> Technical Report 484, </type> <institution> Indiana University Computer Science Department, Lindley Hall 101, Bloomington Indiana, </institution> <month> 47405, </month> <year> 1997. </year>
Reference-contexts: This prevents trivial specialization. Specialization is accomplished by processing the procedure body in a call context that supplies operands only for the trivially invariant formal parameters. Additional details are provided in <ref> [9] </ref>. Specializing the call (fold * x 1 zero? (lambda (x) x) (lambda (x) (- x 1))) yields code identical to the earlier definition of f, modulo renaming. 5 Performance We have implemented the inlining algorithm as an additional pass within Chez Scheme, a commercial optimizing compiler for Scheme.
Reference: 10. <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(2) </volume> <pages> 181-210, </pages> <year> 1991. </year>
Reference-contexts: By using inlining trials instead of o*ine scoring estimates, they were able to reduce overall compile time and code size with minimal loss of run-time performance. Our algorithm can be viewed as incorporating a lightweight form of inlining trials. Wegman and Zadeck <ref> [10] </ref> present a fast global constant propagation algorithm and show how it can be extended to perform many of the specializations needed when procedures are inlined.
References-found: 10

