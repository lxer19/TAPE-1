URL: http://www.physik.uni-marburg.de/bio/mitarbei/karim/flexnet.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00037.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mohraz@forwiss.uni-erlangen.de  
Title: FlexNet A Flexible Neural Network Construction Algorithm  
Author: Karim Mohraz Peter Protzel 
Address: Am Weichselgarten 7, 91058 Erlangen, Germany  
Affiliation: Bavarian Research Center for Knowledge-Based Systems (FORWISS)  
Abstract: Dynamic neural network algorithms are used for automatic network design in order to avoid time consuming search for finding an appropriate network topology with trial & error methods. The new FlexNet algorithm, unlike other network construction algorithms, does not underlie any constraints regarding the number of hidden layers and hidden units. In addition different connection strategies are available, together with candidate pool training and the option of freezing weights. Test results on 3 different benchmarks showed higher generalization rates for FlexNet compared to Cascade-Correlation and optimized static MLP networks. Keywords: network construction, generalization, Cascade-Correlation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Frean, M.: </author> <title> The Upstart Algorithm, A Method for Constructing and Training Feedforward Neural Networks, </title> <booktitle> Neural Computation 2, </booktitle> <year> 1990. </year>
Reference-contexts: Dynamic network algorithms can be divided into 3 categories: network construction, pruning, and genetic algorithms for network design. The FlexNet algorithm presented here is a network construction algorithm. A number of network construction algorithms have been developed so far: Upstart algorithm <ref> [1] </ref>, Add&Remove [2], Cascade-Correlation [3], to name a few. However, the drawback with these procedures is that they either have been designed for the use of binary neurons [1] or underlie constraints such as a limited number of layers and hidden units [2], [3]. <p> A number of network construction algorithms have been developed so far: Upstart algorithm <ref> [1] </ref>, Add&Remove [2], Cascade-Correlation [3], to name a few. However, the drawback with these procedures is that they either have been designed for the use of binary neurons [1] or underlie constraints such as a limited number of layers and hidden units [2], [3]. FlexNet, on the other hand, creates networks with as many layers and as many hidden units as are needed to solve a given problem.
Reference: [2] <author> Hirose, Y., Yamashita, K., Hijiya, S.: </author> <title> Back-Propagation Algorithm which varies the Number of hidden Units, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 4, </volume> <pages> pp 61-66, </pages> <year> 1991. </year>
Reference-contexts: Dynamic network algorithms can be divided into 3 categories: network construction, pruning, and genetic algorithms for network design. The FlexNet algorithm presented here is a network construction algorithm. A number of network construction algorithms have been developed so far: Upstart algorithm [1], Add&Remove <ref> [2] </ref>, Cascade-Correlation [3], to name a few. However, the drawback with these procedures is that they either have been designed for the use of binary neurons [1] or underlie constraints such as a limited number of layers and hidden units [2], [3]. <p> algorithms have been developed so far: Upstart algorithm [1], Add&Remove <ref> [2] </ref>, Cascade-Correlation [3], to name a few. However, the drawback with these procedures is that they either have been designed for the use of binary neurons [1] or underlie constraints such as a limited number of layers and hidden units [2], [3]. FlexNet, on the other hand, creates networks with as many layers and as many hidden units as are needed to solve a given problem. In addition, the user is able to choose between different connection strategies and has the option of freezing weights. 2.
Reference: [3] <author> Fahlman, S., Lebiere, C.: </author> <title> The Cascade-Correlation Learning Architecture, </title> <type> Technical Report CMU-CS-90-100, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Dynamic network algorithms can be divided into 3 categories: network construction, pruning, and genetic algorithms for network design. The FlexNet algorithm presented here is a network construction algorithm. A number of network construction algorithms have been developed so far: Upstart algorithm [1], Add&Remove [2], Cascade-Correlation <ref> [3] </ref>, to name a few. However, the drawback with these procedures is that they either have been designed for the use of binary neurons [1] or underlie constraints such as a limited number of layers and hidden units [2], [3]. <p> have been developed so far: Upstart algorithm [1], Add&Remove [2], Cascade-Correlation <ref> [3] </ref>, to name a few. However, the drawback with these procedures is that they either have been designed for the use of binary neurons [1] or underlie constraints such as a limited number of layers and hidden units [2], [3]. FlexNet, on the other hand, creates networks with as many layers and as many hidden units as are needed to solve a given problem. In addition, the user is able to choose between different connection strategies and has the option of freezing weights. 2. <p> By training not only one set of candidates, but a pool of several sets of candidates, the chances to install weak candidate units decrease and weight space is searched more effectively <ref> [3] </ref>. An example of network construction through FlexNet is given in Figure 1 a) - c): FlexNet starts out with the input layer connected to the output layer (a). This linear network is trained until error stagnation is observed. <p> This strategy is a compromise between Full and Adjacent Layers. Weight freezing is also possible in FlexNet. To tackle the moving target problem, input weights of hidden units are frozen (after they are installed in the network) to prevent the unlearning of previously learned features <ref> [3] </ref>. The freezing option is used only with the Medium connection strategy. 3. Benchmarks FlexNet is tested on 3 benchmark problems. The results are compared to static MLPs and CasCor. <p> Weight freezing once again slowed down FlexNet and an average of 57 units had to be installed. CasCor could not converge in 15000 epochs. Only with mixed activation functions <ref> [3] </ref> (candidate units were assigned at random Bessel, Cosine, Sine, Gaussian, and Sigmoid activation functions) could CasCor converge in 8000 epoch with an average of 70 hidden units in 70 cascaded layers! The generalization result is very poor as can be seen in Fig. 3b, especially when compared to the smooth
Reference: [4] <author> Klagges, H., Soegtrop, M.: </author> <title> Limited Fan-in Random Wired Cascade-Correlation, </title> <institution> IBM Research Division, Physics Group, Munich. </institution>
Reference-contexts: The process of network construction by FlexNet. c ready installed hidden units. There are various connection strategies ranging from the standard MLP (without shortcut connections) to the limited fan-in random wired method <ref> [4] </ref>.
Reference: [5] <author> Fahlman, S.: </author> <title> An Empirical Study of Learning Speed in Back-Propagation Networks, </title> <institution> CMU-CS-88-162, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: For a fair comparison to the dynamic networks, an appropriate MLP network, with its architecture and parameters hand-tuned [8], had to be found for each benchmark. Note that the number of runs needed to obtain these optimized MLPs is not reported here. Resilient Propagation (RP) [6] and Quickprop (QP) <ref> [5] </ref> were used as learning paradigms. The neural network simulator used was FAST [7], which contained the MLP and CasCor algorithms. 10 runs with both Rprop and Quickprop were made for each of the MLPs, CasCor, and the FlexNet flavors (named after their connection strategy): FlexAdj, FlexFull, FlexMed, and FlexFreeze.
Reference: [6] <author> Riedmiller, M.: </author> <title> Rprop - Description and Implementation Details, </title> <type> Technical Report, </type> <institution> Institut fr Logik, Komplexitt und Deduktionssyteme, Universitt Karlsruhe, </institution> <year> 1994. </year>
Reference-contexts: For a fair comparison to the dynamic networks, an appropriate MLP network, with its architecture and parameters hand-tuned [8], had to be found for each benchmark. Note that the number of runs needed to obtain these optimized MLPs is not reported here. Resilient Propagation (RP) <ref> [6] </ref> and Quickprop (QP) [5] were used as learning paradigms.
Reference: [7] <author> Mohraz, K., Arras, M.: </author> <title> Forwiss Artificial Neural Network Simulation Toolbox, </title> <note> Bavarian Research Center for Knowledge-Based Systems (FORWISS), Internal Report, Erlangen, </note> <year> 1994. </year>
Reference-contexts: Note that the number of runs needed to obtain these optimized MLPs is not reported here. Resilient Propagation (RP) [6] and Quickprop (QP) [5] were used as learning paradigms. The neural network simulator used was FAST <ref> [7] </ref>, which contained the MLP and CasCor algorithms. 10 runs with both Rprop and Quickprop were made for each of the MLPs, CasCor, and the FlexNet flavors (named after their connection strategy): FlexAdj, FlexFull, FlexMed, and FlexFreeze. <p> FlexFull and FlexMed had to install 45 hidden units, but needed fewer training epochs than the optimized static MLP. Freezing weights with FlexNet did not affect generalization but needed about twice as many epochs to con-verge. An interesting result can be seen when the receptive fields <ref> [7] </ref> of the trained networks are plotted as shown in Fig. 2.
Reference: [8] <author> Arras, M., Protzel, P.: </author> <title> Assessing Generalization by 2-D Receptive Field Visualization, </title> <note> Bavarian Research Center for Knowledge-Based Systems (FORWISS), Internal Report, Erlangen, </note> <year> 1993. </year>
Reference-contexts: The freezing option is used only with the Medium connection strategy. 3. Benchmarks FlexNet is tested on 3 benchmark problems. The results are compared to static MLPs and CasCor. For a fair comparison to the dynamic networks, an appropriate MLP network, with its architecture and parameters hand-tuned <ref> [8] </ref>, had to be found for each benchmark. Note that the number of runs needed to obtain these optimized MLPs is not reported here. Resilient Propagation (RP) [6] and Quickprop (QP) [5] were used as learning paradigms.
Reference: [9] <author> Mohraz, K.: </author> <title> Neuronale Netze mit dynamischer Architektur, </title> <type> Thesis, </type> <note> Research Center for Knowledge-Based Systems (FORWISS), Erlangen, </note> <year> 1994. </year>
Reference-contexts: In addition, training time (epochs) and an estimate of the network size (average number of hidden units) are provided. 3.1 2-Spirals The networks task in this benchmark is to distinguish between two intertwined spirals, which coil around each other three times <ref> [9] </ref>. Each spiral is represented by 96 training points. The test data has the same size as the training set, with each test point lying between two training points on the same spiral. Table 1 shows the results for each network category.
Reference: [10] <author> Lang, K., Witbrock, M.,: </author> <title> Learning to tell two spirals apart, </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, M. </booktitle> <publisher> Kaufmann Publ., </publisher> <year> 1988. </year>
Reference: [11] <author> Prechelt, L.: </author> <title> PROBEN1 - A set of Neural network Benchmark Problems and Benchmarking Rules, </title> <type> Technical Report 21/94, </type> <institution> Universitt Karlsruhe, </institution> <year> 1994. </year>
Reference-contexts: is very poor as can be seen in Fig. 3b, especially when compared to the smooth approximation of the other network types. 3.3 Breast cancer classification The breast cancer database was initially obtained from the University of Wisconsin [12] and arranged as a neural network benchmark in the Proben1 collection <ref> [11] </ref>. A tumor is to be classified as benign or malignant depending on input attributes such as clump thickness, uniformity of cell size and shape, the amount of marginal adhesion, and the frequency of bare nuclei.
Reference: [12] <author> Mangasarian, O., Wolberg, W.: </author> <title> Cancer diagnosis via linear programming, </title> <journal> SIAM News, </journal> <volume> Volume 23, Number 5, </volume> <year> 1990. </year>
Reference-contexts: average of 70 hidden units in 70 cascaded layers! The generalization result is very poor as can be seen in Fig. 3b, especially when compared to the smooth approximation of the other network types. 3.3 Breast cancer classification The breast cancer database was initially obtained from the University of Wisconsin <ref> [12] </ref> and arranged as a neural network benchmark in the Proben1 collection [11]. A tumor is to be classified as benign or malignant depending on input attributes such as clump thickness, uniformity of cell size and shape, the amount of marginal adhesion, and the frequency of bare nuclei.
References-found: 12

