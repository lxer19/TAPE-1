URL: file://ftp.cs.toronto.edu/pub/carl/svarer_design.93.ps.gz
Refering-URL: http://www.cs.toronto.edu/~carl/pub.html
Root-URL: http://www.cs.toronto.edu
Email: emails: claus,lars,jan,ed@eiffel.ei.dth.dk  
Phone: Tel: +45 4593 4207 Fax: +45 4288 0117  
Title: DESIGNER NETWORKS FOR TIME SERIES PROCESSING  
Author: C. Svarer, L. K. Hansen, J. Larsen and C. E. Rasmussen 
Address: B349  DK-2800 Lyngby, Denmark  
Affiliation: CONNECT, Electronics Institute  University of Denmark,  
Pubnum: Technical  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. Akaike: </author> <title> "Fitting Autoregressive Models for Prediction". </title> <journal> Ann. Inst. Stat. Mat. </journal> <volume> vol. 21, </volume> <pages> 243-247, </pages> <year> 1969. </year>
Reference-contexts: Further, we show that the generalization error of the network may be estimated, without extensive cross-validation, using a modification of Akaike's Final Prediction Error (FPE) estimate <ref> [1] </ref>. The minimal FPE constitutes a useful stopping criterion for pruning [12]. TIME SERIES PROCESSING Time series processing is an important application area for neural networks. While long time forecasts can be ruled out for chaotic systems, short time predictions may still be viable. <p> We refrain from operations involving the full Hessian, which scales poorly for large networks. The recipe allows for ranking the weights according to saliency. The question of how many weights it may be possible to delete was answered in [12]. We applied Akaike's FPE estimate <ref> [1, 9] </ref> of the test error in terms of the training error. In its standard form it reads: b E test = p + N E train ; (8) where p is the number of training samples, and N is the number of parameters in the model.
Reference: [2] <author> S.A. Barton: </author> <title> "A Matrix Method for Optimization a Neural Network". </title> <journal> Neural Computation, </journal> <volume> vol. 3, </volume> <pages> 450-459, </pages> <year> 1990. </year>
Reference-contexts: A simulator has been developed based on batch mode, second order local optimization. A direct matrix-inversion method is used for identification of the hidden-to-output weights <ref> [2] </ref>, while a pseudo Gauss-Newton method (diagonal approximation) is used for identification of input-to-hidden weights, see e.g. [5]. To ensure numerical stability and for assisting the pruning procedure we augment the cost-function with a weight decay term.
Reference: [3] <editor> A.P. Day, M.R. Davenport, and D.S. Camporese: </editor> <title> "Dispersive Networks for Nonlinear Adaptive Filters". </title> <booktitle> In Neural Networks For Signal Processing II; Proceedings of the 1992 IEEE-SP Workshop, </booktitle> <editor> (Eds. S.Y. Kung, F. Fallside, J.Aa. Sorensen, </editor> <publisher> and C.A. Kamm), IEEE Service Center, </publisher> <pages> 464-473, </pages> <year> 1992. </year>
Reference-contexts: We conclude that the Designer Networks more accurately identify the underlying dynamics, than straight fully connected networks. Inverse Modeling: Channel Equalization We have also implemented the toy-model studied by Day et al. <ref> [3] </ref>, involving a non-linear channel sandwiched between low-pass filters with identical transfer functions H 1 (z) = 0:2602 0:9298z 1 + 0:2602z 2 and the non-linearity of the channel is given by the simple map v = 5ujuj (1 u 2 ) ffi driving signal is a Gaussian i.i.d. sequence (unit <p> The signal to be predicted is delayed 15 samples in order to ensure causal filtering. The training set comprises 2000 examples, which is a minute fraction of the training set used in <ref> [3] </ref>. <p> Output from the channel is denoted x (k). The neural network is trained to reproduce the channel input z (k) based on a sample of the noisy output x (k). The variance of the channel Gaussian i.i.d. noise j (k) is 30dB below the variance of the channel output. <ref> [3] </ref> the reduction was from 0.0625 to 0.0256 for the back-prop net 1 , which amounts to a reduction by a factor of 0.41. CONCLUSION We have corroborated our earlier results for the sunspot series [12] by new results for chaotic time series prediction and channel equalization. <p> This shows that the pruning scheme effectively captures the relevant connections and removes unimportant hidden units. We succeeded in designing compact neural networks which generalized better than the unpruned networks. A significant gain in generalization abil 1 The errors in <ref> [3] </ref> are defined as the normalized root-mean-square error. Furthermore, it should be noted that the error for the linear network on our data is significantly higher than the error of [3]. This is not due to the different training set sizes used, since the linear model of [3] converged very quickly. <p> A significant gain in generalization abil 1 The errors in <ref> [3] </ref> are defined as the normalized root-mean-square error. Furthermore, it should be noted that the error for the linear network on our data is significantly higher than the error of [3]. This is not due to the different training set sizes used, since the linear model of [3] converged very quickly. Rather, the non-linearity in our data set is more noticeable. <p> The errors in <ref> [3] </ref> are defined as the normalized root-mean-square error. Furthermore, it should be noted that the error for the linear network on our data is significantly higher than the error of [3]. This is not due to the different training set sizes used, since the linear model of [3] converged very quickly. Rather, the non-linearity in our data set is more noticeable.
Reference: [4] <author> J.D. Farmer, and J.J. Sidorowich: </author> <title> "Exploiting Chaos to Predict the Fu--ture and Reduce Noise", </title> <type> Technical Report LA-UR-88, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: In Table 1 we compare the performances of pruned networks with those of fully connected nets, a linear model, and with a K-nearest-neighbor linear model <ref> [4] </ref>. It is interesting to note that the performance of the networks is similar to the nearest neighbor estimate. The latter involves finding the neighbors among the 1000 training examples and computing a regularized linear estimate of the output.
Reference: [5] <author> J. Hertz, A. Krogh and R.G. Palmer: </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: A simulator has been developed based on batch mode, second order local optimization. A direct matrix-inversion method is used for identification of the hidden-to-output weights [2], while a pseudo Gauss-Newton method (diagonal approximation) is used for identification of input-to-hidden weights, see e.g. <ref> [5] </ref>. To ensure numerical stability and for assisting the pruning procedure we augment the cost-function with a weight decay term.
Reference: [6] <author> J. Larsen: </author> <title> Design of Neural Network Filters. </title> <type> Ph. D. Thesis, </type> <institution> Electronics Institute, Technical University of Denmark, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The estimate is based on linearization of the networks as regards the fluctuations in the weights resulting from different training sets. For a discussion of the approximations entering this estimate see <ref> [6] </ref>. The relation expresses the fact that the training and test errors are biased estimates of the noise level because each parameter during training has "absorbed" noise from the training samples. <p> have regularized the training procedure by weight-decay terms ff w ; ff W , hence, suppressed the ability of the (otherwise) ill-determined parameters to model noise, we need to modify the standard FPE estimate by replacing the total number of parameters with the effective number of param- eters see e.g. <ref> [6, 10, 12] </ref>: b E test = p + N eff E train ; (9) N eff = ij ij 2 N W X fl j + 2ff W =p : (10) where the 's are the second derivatives already computed in eq. (6), ij j @ 2 E train ffi
Reference: [7] <author> Y. Le Cun, J.S. Denker, and S.A. Solla: </author> <title> "Optimal Brain Damage". </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <pages> 598-605, </pages> <publisher> Mor-gan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: In this presentation we review and extend recent efforts to demonstrate the power of this strategy within time series processing. We aim at designing compact networks using the so-called Optimal Brain Damage (OBD) method of Le Cun et al. <ref> [7] </ref>. The benefits from compact architectures are three-fold: Their generalization ability is at least comparable mostly better, they carry less computational burden, and they are faster to adapt if the environment changes. <p> Before each step j is initialized to 1 and iteratively diminished by powers of two until the step leads to a decrease in the cost-function. As e.g. in <ref> [7] </ref> we approximate the second derivative by the positive semi-definite expression: @ 2 E train @w 2 p k=1 @F u (x (k)) 2 PRUNING BY OPTIMAL BRAIN DAMAGE The OBD method proposed by Le Cun et al. [7] was successfully applied to reduce large networks for recognition of handwritten digits <p> As e.g. in <ref> [7] </ref> we approximate the second derivative by the positive semi-definite expression: @ 2 E train @w 2 p k=1 @F u (x (k)) 2 PRUNING BY OPTIMAL BRAIN DAMAGE The OBD method proposed by Le Cun et al. [7] was successfully applied to reduce large networks for recognition of handwritten digits [8]. The basic idea is to estimate the increase in the training error when deleting weights.
Reference: [8] <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jakel: </author> <title> "Handwritten Digit Recognition with a BackPropagation Network". </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <pages> 396-404. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: we approximate the second derivative by the positive semi-definite expression: @ 2 E train @w 2 p k=1 @F u (x (k)) 2 PRUNING BY OPTIMAL BRAIN DAMAGE The OBD method proposed by Le Cun et al. [7] was successfully applied to reduce large networks for recognition of handwritten digits <ref> [8] </ref>. The basic idea is to estimate the increase in the training error when deleting weights.
Reference: [9] <author> L. Ljung: </author> <title> System Identification: Theory for the user, </title> <booktitle> Prentice-Hall, Information and System Sciences series, </booktitle> <year> 1987. </year>
Reference-contexts: We refrain from operations involving the full Hessian, which scales poorly for large networks. The recipe allows for ranking the weights according to saliency. The question of how many weights it may be possible to delete was answered in [12]. We applied Akaike's FPE estimate <ref> [1, 9] </ref> of the test error in terms of the training error. In its standard form it reads: b E test = p + N E train ; (8) where p is the number of training samples, and N is the number of parameters in the model.
Reference: [10] <author> J.E. Moody: </author> <title> "Note on Generalization, Regularization and Architecture Selection in Nonlinear Systems". </title> <booktitle> In Neural Networks For Signal Processing; Proceedings of the 1991 IEEE-SP Workshop, </booktitle> <editor> (Eds. B.H. Juang, S.Y. Kung, and C. Kamm), </editor> <publisher> IEEE Service Center, </publisher> <pages> 1-10, </pages> <year> 1991. </year>
Reference-contexts: have regularized the training procedure by weight-decay terms ff w ; ff W , hence, suppressed the ability of the (otherwise) ill-determined parameters to model noise, we need to modify the standard FPE estimate by replacing the total number of parameters with the effective number of param- eters see e.g. <ref> [6, 10, 12] </ref>: b E test = p + N eff E train ; (9) N eff = ij ij 2 N W X fl j + 2ff W =p : (10) where the 's are the second derivatives already computed in eq. (6), ij j @ 2 E train ffi
Reference: [11] <author> M.B. Priestly: </author> <title> Non-linear and Non-stationary Times Series Analysis, </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: The minimal FPE constitutes a useful stopping criterion for pruning [12]. TIME SERIES PROCESSING Time series processing is an important application area for neural networks. While long time forecasts can be ruled out for chaotic systems, short time predictions may still be viable. Recent work by Priestly <ref> [11] </ref>, and Weigend et al. [14] have established the sunspot series as a benchmark for time series prediction algorithms. Recently we showed how pruning by OBD can produce very compact networks for this problem [12].
Reference: [12] <author> C. Svarer, L.K. Hansen, and J. Larsen: </author> <title> "On Design and Evaluation of Tapped Delay Line Networks", </title> <booktitle> In Proceedings of the 1993 IEEE International Conference on Neural Networks, San Francisco, </booktitle> <volume> vol. 1, </volume> <pages> 46-51, </pages> <year> 1993. </year>
Reference-contexts: Further, we show that the generalization error of the network may be estimated, without extensive cross-validation, using a modification of Akaike's Final Prediction Error (FPE) estimate [1]. The minimal FPE constitutes a useful stopping criterion for pruning <ref> [12] </ref>. TIME SERIES PROCESSING Time series processing is an important application area for neural networks. While long time forecasts can be ruled out for chaotic systems, short time predictions may still be viable. <p> Recent work by Priestly [11], and Weigend et al. [14] have established the sunspot series as a benchmark for time series prediction algorithms. Recently we showed how pruning by OBD can produce very compact networks for this problem <ref> [12] </ref>. We obtained networks using around one third of the parameters of the network published by Weigend et al. while having comparable performance. <p> We refrain from operations involving the full Hessian, which scales poorly for large networks. The recipe allows for ranking the weights according to saliency. The question of how many weights it may be possible to delete was answered in <ref> [12] </ref>. We applied Akaike's FPE estimate [1, 9] of the test error in terms of the training error. <p> have regularized the training procedure by weight-decay terms ff w ; ff W , hence, suppressed the ability of the (otherwise) ill-determined parameters to model noise, we need to modify the standard FPE estimate by replacing the total number of parameters with the effective number of param- eters see e.g. <ref> [6, 10, 12] </ref>: b E test = p + N eff E train ; (9) N eff = ij ij 2 N W X fl j + 2ff W =p : (10) where the 's are the second derivatives already computed in eq. (6), ij j @ 2 E train ffi <p> With the above tool we can obtain a generalization error estimate for each pruned network. By selecting the network with the lowest estimated generalization error we have developed the stop criterion sought. EXPERIMENTS In <ref> [12] </ref> we tested the system on the classical sunspot series. In this paper the methods are tested on two other typical signal processing problems. The first is a standard problem of nonlinear dynamics viz. the Mackey-Glass chaotic time series, and the second concerns channel equalization. <p> We have found that weight decay greatly assist pruning and optimization for the networks with superfluous resources; however, for the optimized pruned architectures we remove the weight decay and retrain to fine tune the performance <ref> [12] </ref>. <p> CONCLUSION We have corroborated our earlier results for the sunspot series <ref> [12] </ref> by new results for chaotic time series prediction and channel equalization. We suggested to optimize network architectures using the Optimal Brain Damage pruning scheme combined with our new statistical stopping criterion.
Reference: [13] <author> H. Tong and K. S. Lim: </author> <title> "Threshold autoregression, limit cycles and cyclical data". </title> <journal> Journ. Roy. Stat. Soc. B, </journal> <volume> vol. 42, 245, </volume> <year> 1980. </year>
Reference: [14] <author> A.S. Weigend, B.A. Huberman, and D.E. Rumelhart: </author> <title> "Prediction the future: A Connectionist Approach", </title> <journal> Int. J. of Neural Systems, </journal> <volume> vol. 1, </volume> <pages> 193-209, </pages> <year> 1990. </year>
Reference-contexts: INTRODUCTION The conventional Tapped-Delay Neural Net <ref> [14] </ref> may be analyzed using statistical methods and the results of such analysis can be applied to model optimization. In this presentation we review and extend recent efforts to demonstrate the power of this strategy within time series processing. <p> TIME SERIES PROCESSING Time series processing is an important application area for neural networks. While long time forecasts can be ruled out for chaotic systems, short time predictions may still be viable. Recent work by Priestly [11], and Weigend et al. <ref> [14] </ref> have established the sunspot series as a benchmark for time series prediction algorithms. Recently we showed how pruning by OBD can produce very compact networks for this problem [12].
Reference: [15] <author> N.H. Wulff and J.A. Hertz: </author> <title> "Prediction with Recurrent Networks". </title> <booktitle> In Neural Networks For Signal Processing II; Proceedings of the 1992 IEEE-SP Workshop, </booktitle> <editor> (Eds. S.Y. Kung, F. Fallside, J.Aa. Sorensen, </editor> <publisher> and C.A. Kamm), IEEE Service Center, </publisher> <pages> 464-473, </pages> <year> 1992. </year>
Reference-contexts: 4 3:12 fi 10 5 3:68 fi 10 5 131 Table 1: 1) Linear model is a single linear unit. 2) The initial pre-pruned networks, trained with the same weight decay terms as used during pruning. 3) Pruned network. 4) Pruned networks retrained without weight decay. tractor, as described in <ref> [15] </ref>. The attractor is sampled on a grid of 20 fi 20 points for the correlation estimate. The correlation coefficients for two independent samples of 8850 points of the true attractor is 0.991. <p> The correlation coefficients for two independent samples of 8850 points of the true attractor is 0.991. For the fully connected network trained on 1000 examples it is 0.922, for the pruned network trained on 1000 examples 0.948. In order to compare with <ref> [15] </ref> we also completed a pruning session based on on 500 examples, and for the pruned network we found a correlation coefficient of 0.934 (with weight decay), while the coefficient for a feed-forward network with second order couplings were found to be 0.917 in [15]. <p> In order to compare with <ref> [15] </ref> we also completed a pruning session based on on 500 examples, and for the pruned network we found a correlation coefficient of 0.934 (with weight decay), while the coefficient for a feed-forward network with second order couplings were found to be 0.917 in [15]. For networks trained on 500 examples only, it turns out that, retraining without weight decay deteriorates the performance, indicating that pruning has not identified the optimal architecture. We conclude that the Designer Networks more accurately identify the underlying dynamics, than straight fully connected networks.
References-found: 15

