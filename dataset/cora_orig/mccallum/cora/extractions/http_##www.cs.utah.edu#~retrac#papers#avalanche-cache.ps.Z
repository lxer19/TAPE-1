URL: http://www.cs.utah.edu/~retrac/papers/avalanche-cache.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Reducing Consistency Traffic and Cache Misses in the Avalanche Multiprocessor  
Author: John B. Carter Ravindra Kuramkote Chen-Chi Kuo 
Keyword: Computer architecture, multiprocessor design, coherency protocols, shared memory, performance, communication latency.  
Note: TECHNICAL PAPER  
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science University of Utah  
Pubnum: TECHNICAL PAPER  
Email: Email: retrac@cs.utah.edu  Email: kuramkot@cs.utah.edu  Email: chenchi@cs.utah.edu  
Phone: Phone: 801-585-5474  
Web: WWW: http://www.cs.utah.edu/~retrac  WWW: http://www.cs.utah.edu/~kuramkot  WWW: http://www.cs.utah.edu/~chenchi  
Abstract: For a parallel architecture to scale effectively, communication latency between processors must be avoided. We have found that the source of a large number of avoidable cache misses is the use of hardwired write-invalidate coherency protocols, which often exhibit high cache miss rates due to excessive invalidations and subsequent reloading of shared data. In the Avalanche project at the University of Utah, we are building a 64-node multiprocessor designed to reduce the end-to-end communication latency of both shared memory and message passing programs. As part of our design efforts, we are evaluating the potential performance benefits and implementation complexity of providing hardware support for multiple coherency protocols. Using a detailed architecture simulation of Avalanche, we have found that support for multiple consistency protocols can reduce the time parallel applications spend stalled on memory operations by up to 66% and overall execution time by up to 31%. Most of this reduction in memory stall time is due to a novel release-consistent multiple-writer write-update protocol implemented using a write state buffer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and D. Chaiken et al. </author> <title> The MIT Alewife Machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report Technical Memp 454, </type> <institution> MIT/LCS, </institution> <year> 1991. </year>
Reference-contexts: Myrinet's performance characteristics are discussed in detail in Section 3.2. 2.2 Consistency Management Spurred by scalable shared memory architectures developed in academia <ref> [1, 19] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex, Cray, and IBM). <p> This machine, called FLASH [18], is currently being designed to support both DASH-like shared memory and efficient message passing. However, their plans for exploiting the flexibility of their controller's operation have not been revealed. The MIT Alewife machine <ref> [1, 10] </ref> also uses a directory-based cache design that supports both low latency message passing and shared memory based on an invalidation-based consistency protocol.
Reference: [2] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: We propose to do this by allowing shared data to be maintained using the consistency or synchronization protocol best-suited to the way the programming is accessing the data. For example, data that is being accessed primarily by a single processor would likely be handled by a conventional write-invalidate protocol <ref> [2] </ref>, while data being heavily shared by multiple processes, such as global counters or edge elements in finite differencing codes, would likely be handled using a delayed write-update protocol [5].
Reference: [3] <author> David Beazley, </author> <year> 1994. </year> <title> Member of 1993 Gordon Bell Prize winning team, </title> <type> personal communication. </type>
Reference-contexts: For example, even highly-tuned applications often achieve well under 50% of peak performance on multiprocessors such as the CM-5 [23] and Cray T3D [11] despite their powerful communication fabrics <ref> [3] </ref>. For a parallel architecture to scale effectively into the tera- and peta-op range, high latency cache misses must be avoided. Thus, cache controller designs and consistency protocols that reduce the frequency of cache misses must be developed.
Reference: [4] <author> M. J. Beckerle. </author> <title> An Overview of the START (*T) Computer System. </title> <type> MCRC Technical Report MCRC-TR-28, </type> <institution> Motorola Cambridge Research Center, </institution> <year> 1992. </year>
Reference-contexts: However, it does not provide the tight integration of communication fabric and protocol into a realistic memory hierarchy, nor does it exploit context sensitivity to tune its behavior. The Motorola and MIT *T machine <ref> [4] </ref> has many interesting components that offer excellent support to exploit dataflow style parallelism. The *T architecture provides tight coupling between the processor registers and the interconnect fabric, but isolates the memory hierarchy by placing the 23 CPU between the interconnect fabric and the memory.
Reference: [5] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [5, 14, 24] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others. <p> For example, data that is being accessed primarily by a single processor would likely be handled by a conventional write-invalidate protocol [2], while data being heavily shared by multiple processes, such as global counters or edge elements in finite differencing codes, would likely be handled using a delayed write-update protocol <ref> [5] </ref>. Similarly, locks will be handled using conventional distributed locking protocols, while more complex synchronization operations like barriers and reduction operators for vector sums will be handled using specialized protocols. <p> Furthermore, the use of a write update protocol can significantly reduce the number of read misses that a write-invalidate protocol induces as a side effect of maintaining coherence when the degree of sharing is high <ref> [5] </ref>. For example, if processors a and b are both reading and writing data from a particular cache line, a write invalidate protocol will result in a large number of invalidations and subsequent read misses when the invalidated processor reloads the data that it needs.
Reference: [6] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E.W. Felten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: As such, it currently requires extensive program modification or user effort to achieve scalable performance, although the designers are working on a number of compilation and performance debugging tools to help automate this process. The tradeoffs between the software and hardware approaches are being studied. The SHRIMP Multicomputer <ref> [6] </ref> employs a custom designed network interface to provide both shared memory and low-latency message passing. A virtual memory-mapped interface provides a constrained form of shared memory in which a process can map in pages that are physically located on another node.
Reference: [7] <author> N.J. Boden, D. Cohen, R.E. Felderman, A.E. Kulawik, C.L. Seitz, J.N. Seizovic, and W.-K. Su. </author> <title> Myrinet A gigabit-per-second local-area network. </title> <journal> IEEE MICRO, </journal> <note> 1995. To appear. </note>
Reference-contexts: An important aspect of this flexibility is Avalanche's support for multiple hardware coherency protocols, which we will discuss in detail in the following section. To minimize our design effort and exploit existing commercial technology whenever possible, we are using the Myrinet <ref> [7] </ref> network as our multiprocessor backplane. Myrinet, which derived from the Caltech router project, is a high-speed mesh-connected network fabric designed for use both as a LAN network and as a multiprocessor backplane. <p> Depending on the number of processors and the complexity of the cache controllers being simulated, our simulation runs took between twenty minutes and five hours to complete. 3.2 Network Model To accurately model network delays and contention, we have developed a very detailed, flit-by-flit model of the Myrinet fabric <ref> [7] </ref>. The Myrinet fabric is mesh-connected, with one crossbar at the core of each switching node.
Reference: [8] <author> J.B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: processing elements and provides no mechanism for tuning by the compiler or runtime system. 1 Except in the case of the Cray, which does not cache shared data. 5 These observations have led a number of researchers to propose building cache controllers that can execute a variety of caching protocols <ref> [8, 26] </ref>, support multiple communication models [10, 15], or accept guidance from software [18, 21].
Reference: [9] <author> J.B. Carter and R. Kuramkote. </author> <title> Avalanche: Cache and dsm protocol design. </title> <type> Technical report, </type> <institution> University of Utah, </institution> <month> April </month> <year> 1995. </year> <note> Also available via WWW under http://www.cs.utah.edu/projects/avalanche. </note>
Reference-contexts: Space constraints make it impossible to discuss all of the details of the simulation environment herein amore detailed description of the Avalanche architecture can be found elsewhere <ref> [9] </ref>. We used the following model in our architecture simulations. We modeled a sixteen-node system, where each node was configured as illustrated in Figure 1. <p> Due to space constraints, we have not included a detailed description of the protocols in this paper, but instead refer the interested reader to a more detailed technical report <ref> [9] </ref>. For each application program, we explored the potential of allowing software to specify the coherence protocol to be used to maintain shared data for an application by evaluating the performance of each individual protocol on the application.
Reference: [10] <author> D. Chaiken and A. Agarwal. </author> <title> Software-extended coherent shared memory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: for tuning by the compiler or runtime system. 1 Except in the case of the Cray, which does not cache shared data. 5 These observations have led a number of researchers to propose building cache controllers that can execute a variety of caching protocols [8, 26], support multiple communication models <ref> [10, 15] </ref>, or accept guidance from software [18, 21]. We are investigating cache designs that will implement a variety of caching protocols, support both shared memory and message passing efficiently, accept guidance from software to tune its behavior, and directly support efficient high-level synchronization primitives. <p> This machine, called FLASH [18], is currently being designed to support both DASH-like shared memory and efficient message passing. However, their plans for exploiting the flexibility of their controller's operation have not been revealed. The MIT Alewife machine <ref> [1, 10] </ref> also uses a directory-based cache design that supports both low latency message passing and shared memory based on an invalidation-based consistency protocol.
Reference: [11] <author> Cray Research, Inc. </author> <title> CRAY T3D System Architecture Overview, </title> <address> hr-04033 edition, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Evidence of this situation can be seen in the significant differences between the peak performance of today's fast multiprocessing systems and the achieved performance. For example, even highly-tuned applications often achieve well under 50% of peak performance on multiprocessors such as the CM-5 [23] and Cray T3D <ref> [11] </ref> despite their powerful communication fabrics [3]. For a parallel architecture to scale effectively into the tera- and peta-op range, high latency cache misses must be avoided. Thus, cache controller designs and consistency protocols that reduce the frequency of cache misses must be developed.
Reference: [12] <author> A. L. Davis. Mayfly: </author> <title> A General-Purpose, Scalable, </title> <booktitle> Parallel Processing Architecture. Lisp and Symbolic Computation, </booktitle> 5(1/2):7-47, May 1992. 
Reference-contexts: The MIT M-Machine work [20] contains a context cache similar to previous designs such as the HP Mayfly system <ref> [12] </ref>. This context cache provides dynamic binding of variable names to register contents to permit rapid task switching and promote the interesting processor coupling mechanism of the M-machine.
Reference: [13] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Most of this reduction in memory stall time is due to a novel release-consistent <ref> [13] </ref> multiple-writer write-update protocol implemented using a write state buffer. <p> from a controller's input FIFO 1 cycle Update directory entry 4 cycles Table 2 Delay Characteristics 3.4 Protocols Investigated We evaluated the performance of four basic coherence protocols: (i) a sequentially consistent multiple reader, singler writer, write invalidate protocol (sc-wi), (ii) a no-replicate migratory protocol (mig), (iii) a release consistent <ref> [13] </ref> implementation of a conventional multiple reader, single writer, write invalidate protocol (rc-wi), and (iv) a release consistent multiple reader, multiple writer, write update protocol (rc-wu). We selected these four protocols because they covered a wide spectrum of options available to system designers. <p> This optimization assumes that the program is written using sufficient synchronization to avoid data races, which is most often the case. The details of why this results in correct behavior is beyond the scope of this paper a detailed explanation can be found elsewhere <ref> [13] </ref>. The rc-wu protocol uses the write state buffer in a different way. When a node writes to a word of shared data, it allocates an entry in the write buffer for the associated cache line and marks that word as dirty.
Reference: [14] <author> A. Gupta and W.-D. Weber. </author> <title> Cache invalidation patterns in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [5, 14, 24] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [15] <author> M. Heinrich and J. Kuskin et al. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: for tuning by the compiler or runtime system. 1 Except in the case of the Cray, which does not cache shared data. 5 These observations have led a number of researchers to propose building cache controllers that can execute a variety of caching protocols [8, 26], support multiple communication models <ref> [10, 15] </ref>, or accept guidance from software [18, 21]. We are investigating cache designs that will implement a variety of caching protocols, support both shared memory and message passing efficiently, accept guidance from software to tune its behavior, and directly support efficient high-level synchronization primitives.
Reference: [16] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Thus, the on-chip cache miss penalties discussed earlier have proven problematic in terms of achieving a reasonable percentage of the impressive peak performance of the CM-5 on real applications. Another commercial scalable supercomputer of interest is the Intel Paragon <ref> [16] </ref>. The interconnect is a high performance mesh routing device. The fabric does not support direct DMA into the Paragon's memory hierarchy but utilizes a second i860XP CPU for this purpose on each processing element.
Reference: [17] <author> N.P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Each entry is associated with a local dirty cache line and is used to keep track of which words are dirty in that line. Write state buffer entries are allocated on demand when the local cache writes to a shared cache line. Unlike a conventional write buffer <ref> [17] </ref>, which contains the modified data as well as its address, the write state buffer contains only an indication of what words have been modified. The modified data itself is stored in the cache.
Reference: [18] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Finally, the DC and CCU are connected to local memory through a local bus arbiter. Our project differs from related related research projects <ref> [19, 18, 21] </ref> in a number of important ways. <p> system. 1 Except in the case of the Cray, which does not cache shared data. 5 These observations have led a number of researchers to propose building cache controllers that can execute a variety of caching protocols [8, 26], support multiple communication models [10, 15], or accept guidance from software <ref> [18, 21] </ref>. We are investigating cache designs that will implement a variety of caching protocols, support both shared memory and message passing efficiently, accept guidance from software to tune its behavior, and directly support efficient high-level synchronization primitives. <p> best for every application except mp3d, which is known to have mostly migratory data. rc-wu performed particularly well for barnes and locus, removing over 60% of the cache stall time compared to the conventional sc-wi protocol and over 40% compared to rc-wi protocol used as the base protocol in FLASH <ref> [18] </ref>. For the faster interconnect (Figure 3), the results were more varied. rc-wu continues to perform very well for barnes and locus, while mig continues to perform best for mp3d, but with the use of a faster interconnect, FLASH's rc-wi protocol performs best for cholesky and water. <p> A second generation DASH multiprocessor is being developed that introduces a limited amount of processing power and state at the distributed directories to add flexibility to the consistency implementation. This machine, called FLASH <ref> [18] </ref>, is currently being designed to support both DASH-like shared memory and efficient message passing. However, their plans for exploiting the flexibility of their controller's operation have not been revealed.
Reference: [19] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Finally, the DC and CCU are connected to local memory through a local bus arbiter. Our project differs from related related research projects <ref> [19, 18, 21] </ref> in a number of important ways. <p> Myrinet's performance characteristics are discussed in detail in Section 3.2. 2.2 Consistency Management Spurred by scalable shared memory architectures developed in academia <ref> [1, 19] </ref>, the next generation of massively parallel systems will support shared memory in hardware (e.g., machines by Convex, Cray, and IBM). <p> Our approach differs from the approaches taken in these systems in a number of important aspects, as described below. The Stanford DASH multiprocessor <ref> [19] </ref> uses a novel directory-based cache design to interconnect a collection of 4-processor SGI boards based on the MIPS 3000 RISC processor. The Convex Exemplar employs a similar design based around the HP7100 PA-RISC. Avalanche will employ a similar directory-based cache design.
Reference: [20] <author> P. Nuth and W. J. Dally. </author> <title> A Mechanism for Efficient Context Switching. </title> <booktitle> In Proceedings of the IEEE International Conference on Computer Design, </booktitle> <pages> pages 301-304, </pages> <year> 1991. </year>
Reference-contexts: Alewife incorporates a limited amount of flexibility by allowing the controller to invoke specialized low-level software trap handlers to handle uncommon consistency operations, but currently the Alewife designers are only planning to use this capability to support an arbitrary number of "replica" pointers. The MIT M-Machine work <ref> [20] </ref> contains a context cache similar to previous designs such as the HP Mayfly system [12]. This context cache provides dynamic binding of variable names to register contents to permit rapid task switching and promote the interesting processor coupling mechanism of the M-machine.
Reference: [21] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Finally, the DC and CCU are connected to local memory through a local bus arbiter. Our project differs from related related research projects <ref> [19, 18, 21] </ref> in a number of important ways. <p> system. 1 Except in the case of the Cray, which does not cache shared data. 5 These observations have led a number of researchers to propose building cache controllers that can execute a variety of caching protocols [8, 26], support multiple communication models [10, 15], or accept guidance from software <ref> [18, 21] </ref>. We are investigating cache designs that will implement a variety of caching protocols, support both shared memory and message passing efficiently, accept guidance from software to tune its behavior, and directly support efficient high-level synchronization primitives. <p> The level of primary processor cycle stealing that this implies will seriously impede scalability on conventional style applications based on DSM or message passing that do not exploit the *T's powerful support for data flow languages. Like Avalanche, the user level shared memory in the Tempest and Typhoon systems <ref> [21] </ref> will support cooperation between software and hardware to implement both scalable shared memory and message passing abstractions. Like the Alewife system, will support low level interaction between software and hardware to provide flexibility.
Reference: [22] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: While it is probably not reasonable to assume that this performance is achievable in general, it provides us with some insight into the value of allowing software to specify the coherence protocol at a small grain. 3.5 Benchmark Programs We used five programs from the SPLASH benchmark suite <ref> [22] </ref> in our study, mp3d, water, barnes, LocusRoute, and cholesky. Table 3 contains the inputs for each test program. mp3d is a three-dimensional particle simulator used to simulated rarified hypersonic airflow.
Reference: [23] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 technical summary, </title> <year> 1991. </year>
Reference-contexts: Evidence of this situation can be seen in the significant differences between the peak performance of today's fast multiprocessing systems and the achieved performance. For example, even highly-tuned applications often achieve well under 50% of peak performance on multiprocessors such as the CM-5 <ref> [23] </ref> and Cray T3D [11] despite their powerful communication fabrics [3]. For a parallel architecture to scale effectively into the tera- and peta-op range, high latency cache misses must be avoided. Thus, cache controller designs and consistency protocols that reduce the frequency of cache misses must be developed. <p> In addition, incoming messages are placed into main memory via a DMA engine, using invalidation to maintain consistency, which results in cache misses that would not occur if the network controller was more tightly coupled with the memory system. The Thinking Machines CM-5 <ref> [23] </ref> did not directly support DSM or a multilevel external memory hierarchy, and as such the excellent communication fabric of the CM-5 is not well integrated into the memory architecture.
Reference: [24] <author> J.E. Veenstra and R.J. Fowler. </author> <title> A performance evaluation of optimal hybrid cache coherency protocols. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 149-160, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Using traces of shared memory parallel programs, researchers have found there are a small number of characteristic ways in which shared memory is accessed <ref> [5, 14, 24] </ref>. These characteristic "patterns" are sufficiently different from one another that any protocol designed to optimize one will not perform particularly well for the others.
Reference: [25] <author> J.E. Veenstra and R.J. Fowler. Mint: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In MASCOTS 1994, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Currently our performance results are based on a highly detailed architecture simulation system, but we will validate these results via an actual implementation of a 64-node prototype in due course. 3.1 MINT Multiprocessor Simulator We used the Mint memory hierarchy simulator <ref> [25] </ref> running on Silicon Graphics and Hewlett-Packard workstations to perform our simulations. Mint simulates a collection of processors and provides support for spinlocks, semaphores, barriers, shared memory, and most Unix system calls. We augmented it to support message passing and multiple processes per node.
Reference: [26] <author> A. Wilson and R. LaRowe. </author> <title> Hiding shared memory reference latency on the GalacticaNet distributed shared memory architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: processing elements and provides no mechanism for tuning by the compiler or runtime system. 1 Except in the case of the Cray, which does not cache shared data. 5 These observations have led a number of researchers to propose building cache controllers that can execute a variety of caching protocols <ref> [8, 26] </ref>, support multiple communication models [10, 15], or accept guidance from software [18, 21].
References-found: 26

