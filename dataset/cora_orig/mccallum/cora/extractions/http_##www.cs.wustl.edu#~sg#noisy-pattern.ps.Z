URL: http://www.cs.wustl.edu/~sg/noisy-pattern.ps.Z
Refering-URL: http://www.cs.wustl.edu/~sg/
Root-URL: http://www.cs.wustl.edu
Email: sg@cs.wustl.edu and sds@cs.wustl.edu  
Title: A Theoretical and Empirical Study of a Noise-Tolerant Algorithm to Learn Geometric Patterns (Extended Abstract)  
Author: Sally A. Goldman and Stephen D. Scott 
Keyword: Noise-tolerant PAC-learning, statistical query model, landmark matching problem  
Address: St. Louis, MO 63130  
Affiliation: Department of Computer Science Washington University  
Abstract: Developing the ability to recognize a landmark from a visual image of a robot's current location is a fundamental problem in robotics. We describe a way in which the landmark matching problem can be mapped to that of learning a one-dimensional geometric pattern. We present an efficient noise-tolerant algorithm (designed using the statistical query model) to PAC-learn the class of one-dimensional geometric patterns. Then we report results from an initial empirical study of our algorithm that provides at least some evidence that statistical query algorithms may be valuable for use in practice. 
Abstract-found: 1
Intro-found: 1
Reference: <author> M. Anthony & N. Biggs. </author> <title> Computational Learning Theory. </title> <booktitle> Cambridge Tracts in Theoretical Computer Science (30). </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference: <author> J. A. Aslam & S. E. Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In 34th Ann. Symp. on Found. Comp. Sci., </booktitle> <pages> pp 282-291, </pages> <year> 1993. </year>
Reference-contexts: A key result of this paper is an efficient algorithm that PAC-learns the class of one-dimensional geometric patterns under high noise rates (any rate &lt; 1=2 of classification noise). We obtain such a noise-tolerant algorithm by using the recently developed statistical query model <ref> (Kearns 1993, Aslam and Decatur 1995) </ref>. A second contribution of this work is an empirical study of how well an algorithm designed using the statistical query model works on simulated data.
Reference: <author> J. Aslam & S. Decatur. </author> <title> Specification and simulation of statistical query algorithms for efficiency and noise tolerance. </title> <booktitle> In Proc. of the 8th Ann. ACM Conf. on Computational Learning Theory, </booktitle> <pages> pp 437-446, </pages> <year> 1995. </year>
Reference-contexts: To obtain a noise-tolerant version of our algorithm we use the statistical query model (Aslam and De-catur 1993, Aslam and Decatur 1995, Decatur 1993, Kearns 1993). In this model, rather than sampling labeled examples, the learner requests the value of various statistics. A relative statistical query <ref> (Aslam and Decatur 1995) </ref> takes the form SQ D (O; ; ) where O is a predicate over labeled examples, is a relative error bound, and is a threshold. For target concept C, P O = Pr D [O (X; C (X)) = 1]. <p> The learner may also request unlabeled examples. It is known that all statistical query algorithms are robust against high rates of random classification noise <ref> (Aslam and Decatur 1995, Decatur 1993, Kearns 1993) </ref>. Statistical query algorithms are also known to be robust against small amounts of malicious errors and distributional errors (Decatur 1993). <p> Finally, the bound for m u is derived from the VC-dimension result (Blumer et al. 1989), and the bound for m ` is derived from <ref> (Aslam and Decatur 1995) </ref> so that all statistical queries will have the required accuracy.
Reference: <author> A. Blumer, A. Ehrenfeucht, D. Haussler, & M. Warmuth. </author> <title> Occam's razor. </title> <journal> Inform. Proc. Lett., </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: In contrast, for the union of intervals, the consistency problem is trivial to solve. 5 A COVERING ALGORITHM We now review the PAC-algorithm (Goldberg and Goldman 1994, Goldberg, Goldman and Scott 1995) for learning C k;n . This algorithm is an Occam algorithm <ref> (Blumer et al. 1987, Blumer et al. 1989) </ref>. Namely, it draws a sufficiently large sample of size m (polynomial in k; lg n; 1=*, and lg 1=ffi) and then finds a hypothesis, consistent with all examples, that has size sublinear in the size of the sample.
Reference: <author> A. Blumer, A. Ehrenfeucht, D. Haussler, & M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: If ^ P O *=(6k) then P O ^ P O 6k 6 Further, since = *=(5k) it follows that for any point q added to H + , P O *=(5k). Finally, since S has a point in any interval of weight at least *=(7k) <ref> (by the Blumer et al. 1989 result) </ref>, for each target interval some point q fl considered has P O *=(7k). Thus, for this q fl , ^ P O P O (1 + ) * 6 = *=(6k). <p> From Lemma 2 we know that the final hypothesis has both false positive and false negative error rates of at most *=2 and thus the overall error rate is *. Finally, the bound for m u is derived from the VC-dimension result <ref> (Blumer et al. 1989) </ref>, and the bound for m ` is derived from (Aslam and Decatur 1995) so that all statistical queries will have the required accuracy.
Reference: <author> S. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory, </booktitle> <pages> pp 262-268, </pages> <year> 1993. </year>
Reference-contexts: A key result of this paper is an efficient algorithm that PAC-learns the class of one-dimensional geometric patterns under high noise rates (any rate &lt; 1=2 of classification noise). We obtain such a noise-tolerant algorithm by using the recently developed statistical query model <ref> (Kearns 1993, Aslam and Decatur 1995) </ref>. A second contribution of this work is an empirical study of how well an algorithm designed using the statistical query model works on simulated data. <p> It is known that all statistical query algorithms are robust against high rates of random classification noise (Aslam and Decatur 1995, Decatur 1993, Kearns 1993). Statistical query algorithms are also known to be robust against small amounts of malicious errors and distributional errors <ref> (Decatur 1993) </ref>. While several very nice theoretical results have been proven about the statistical query model, we are not aware 2 Here we consider the version of the PAC-learning model that is often called the non-proper or prediction model.
Reference: <author> P. Goldberg & S. Goldman. </author> <title> Learning one-dimensional geometric patterns under one-sided random misclassification noise. </title> <booktitle> In Proc. of the 7th Ann. ACM Conf. on Computational Learning Theory, </booktitle> <pages> pp 246-255, </pages> <year> 1994. </year>
Reference-contexts: In contrast, for the union of intervals, the consistency problem is trivial to solve. 5 A COVERING ALGORITHM We now review the PAC-algorithm <ref> (Goldberg and Goldman 1994, Goldberg, Goldman and Scott 1995) </ref> for learning C k;n . This algorithm is an Occam algorithm (Blumer et al. 1987, Blumer et al. 1989).
Reference: <author> P. Goldberg, S. Goldman, & S. Scott. </author> <title> Pac-learning one-dimensional geometric patterns. </title> <type> Unpublished Manuscript, </type> <year> 1995. </year>
Reference: <author> P. Goldberg. </author> <title> PAC Learning Geometrical Figures. </title> <type> PhD thesis, </type> <institution> University of Edinburgh, </institution> <year> 1992. </year>
Reference: <author> P. Gruber. </author> <title> Approximation of convex bodies. </title> <editor> In P. Gru-ber and P. Willis, editors, </editor> <title> Convexity and its applications. </title> <publisher> Brikhauser Verlag, </publisher> <year> 1983. </year>
Reference: <author> J. Hong, X. Tan, B. Pinette, R. Weiss, & E. Riseman. </author> <title> Image-based homing. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 12(1) </volume> <pages> 38-45, </pages> <year> 1992. </year>
Reference: <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pp 392-401, </pages> <year> 1993. </year>
Reference-contexts: A key result of this paper is an efficient algorithm that PAC-learns the class of one-dimensional geometric patterns under high noise rates (any rate &lt; 1=2 of classification noise). We obtain such a noise-tolerant algorithm by using the recently developed statistical query model <ref> (Kearns 1993, Aslam and Decatur 1995) </ref>. A second contribution of this work is an empirical study of how well an algorithm designed using the statistical query model works on simulated data.
Reference: <author> M. Kearns & U. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference: <author> T. Levitt & D. Lawton. </author> <title> Qualitative navigation for mobile robots. </title> <journal> Artificial Intelligence, </journal> <volume> 44(3): </volume> <pages> 305-360, </pages> <year> 1990. </year>
Reference: <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: <author> B. Pinette. </author> <title> Image-Based Navigation Through Large-Scaled Environments. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1993. </year>
Reference: <author> H. Suzuki & S. Arimoto. </author> <title> Visual control of autonomous mobile robot based on self-organizing model for pattern learning. </title> <journal> J. of Robotic Systems, </journal> <volume> 5(5) </volume> <pages> 453-470, </pages> <year> 1988. </year>
Reference: <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference: <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proc. of the 9th Int. Joint Conf. on Artificial Intelligence, </booktitle> <volume> vol. 1, </volume> <pages> pp 560-566, </pages> <year> 1985. </year>
Reference: <author> V. N. Vapnik & A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabil ities. </title> <journal> Theory of Probab. and its Applications, </journal> <volume> 16(2):264 280, </volume> <year> 1971. </year>
Reference-contexts: The paper of Blumer et al. (1989) uses a combinatorial parameter of a class of hypotheses, called the Vapnik-Chervonenkis dimension <ref> (Vapnik and Chervo-nenkis 1971) </ref>, to give bounds on how large a sample is required in order to have enough data for accurate generalization.
References-found: 20

