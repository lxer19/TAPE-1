URL: http://www.cis.udel.edu/~case/papers/noisy-rec-txt.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: Email: case@cis.udel.edu  Email: sanjay@comp.nus.edu.sg  
Title: Synthesizing Learners Tolerating Computable Noisy Data Stephan's model of noisy data is employed, in which,
Author: John Case Sanjay Jain 
Note: F.  
Address: 19716, USA  Singapore 119260  
Affiliation: Department of CIS University of Delaware Newark, DE  School of Computing National University of Singapore  
Abstract: Studied, then, is the synthesis from indices for r.e. classes and for indexed families of languages of various kinds of noise-tolerant language-learners for the corresponding classes or families indexed, where the noisy input data sequences are restricted to being computable. Many positive results, as well as some negative results, are presented regarding the existence of such synthesizers. The main positive result is surprisingly more positive than its analog in the case the noisy data is not restricted to being computable: grammars for each indexed family can be learned behaviorally correctly from computable, noisy, positive data! The proof of another positive synthesis result yields, as a pleasant corollary, a strict subset-principle or tell-tale style characterization, for the computable noise-tolerant behaviorally correct learnability of grammars from positive and negative data, of the corresponding families indexed. 
Abstract-found: 1
Intro-found: 1
Reference: [Ang80] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: In fact the computational learning theory community has shown considerable interest (spanning at least from [Gol67] to [ZL95]) in language classes defined by r.e. listings of decision procedures. These classes are called uniformly decidable or indexed families. As is essentially pointed out in <ref> [Ang80] </ref>, all of the formal language style example classes are indexed families. <p> result yielded the surprising characterization [BCJ96]: for indexed families L, L can be Bc-learned from positive data with the learner outputting grammars iff (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from <ref> [Ang80] </ref>, and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data [Ang80, Ber85, ZLK95, KB92, Cas98]. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as <p> (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas98] </ref>. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's [Ste95, CJS98b] noise model. <p> For example, the well-known class of pattern languages <ref> [Ang80] </ref> 6 can be Ex-learned from texts but cannot be Bc-learned from unrestricted noisy texts even if we allow the final grammars each to make finitely many mistakes. <p> These results have consequences for Inductive Logic Programming [MR94, LD94]. 7 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions. See, for example, <ref> [Ang80, Muk92, LZK96, dJK96] </ref>. 2 with the negative result mentioned above from [CJS98a] that even the class of pattern languages is not learnable from unrestricted noisy texts)! Another main positive result of the present paper is Corollary 3 in Section 4.1 below.
Reference: [AS94] <author> H. Arimura and T. Shinohara. </author> <title> Inductive inference of Prolog programs with linear data dependency from positive data. </title> <editor> In H. Jaakkola, H. Kangassalo, T. Kitahashi, and A. Markus, editors, </editor> <booktitle> Proc. Information Modelling and Knowledge Bases V, </booktitle> <pages> pages 365-375. </pages> <publisher> IOS Press, </publisher> <year> 1994. </year>
Reference-contexts: The techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFSs [Shi91]. Investigations of the learnability of subclasses of EFSs are important because they yield corresponding results about the learnability of subclasses of logic programs. <ref> [AS94] </ref> use the insight gained from the learnability of EFSs subclasses to show that a class of linearly covering logic programs with local variables is TxtEx-learnable.
Reference: [ASY92] <author> S. Arikawa, T. Shinohara, and A. Yamamoto. </author> <title> Learning elementary formal systems. </title> <journal> Theoretical Computer Science, </journal> <volume> 95 </volume> <pages> 97-113, </pages> <year> 1992. </year>
Reference-contexts: For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see [SSS + 94, SA95]). Pattern languages and finite unions of pattern languages [Shi83, Wri89] turn out to be subclasses of Smullyan's [Smu61] Elementary Formal Systems (EFSs). <ref> [ASY92] </ref> show that the EFSs can also be treated as a logic programming language over strings. The techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFSs [Shi91].
Reference: [Bar74] <author> J. Barzdi~ns. </author> <title> Two theorems on the limiting synthesis of functions. </title> <booktitle> In Theory of Algorithms and Programs, </booktitle> <volume> vol. 1, </volume> <pages> pages 82-88. </pages> <institution> Latvian State University, </institution> <year> 1974. </year> <note> In Russian. </note>
Reference-contexts: Example more general learners are: Bc-learners, which, when successful on an object input, (by definition) find a final (possibly infinite) sequence of correct programs for that object after at most finitely many trial and error attempts <ref> [Bar74, CS83] </ref>. 4 Of course, if suitable learner-synthesizer algorithm lsyn is fed procedures for listing decision procedures (instead of mere grammars), one also has more success at synthesizing learners. <p> or informant I. * [Gol67, CL82] M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * <ref> [Bar74, CL82] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. * [Bar74, CL82]. <p> L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * <ref> [Bar74, CL82] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. * [Bar74, CL82]. M InfBc a -identifies L from informant I iff (8 1 n)[W M (I [n]) = a L]. (b) Suppose J 2 fTxtEx a ; TxtBc a g. M J-identifies L iff, for all texts T for L, M J-identifies L from T .
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> The learners so synthesized are called enumeration techniques <ref> [BB75, Ful90] </ref>. <p> J = fL j (9M)[L J (M)]g. We often write TxtEx 0 as TxtEx. A similar convention applies to the other learning criteria of this paper. Several proofs in this paper depend on the concept of locking sequence. Definition 2 (Based on <ref> [BB75] </ref>) Suppose a 2 N [ fflg. (a) is said to be a TxtEx a -locking sequence for M on L iff, content () L, W M () = a L, and (8t j content (t ) L)[M ( t ) = M ()]. (b) is said to be a TxtBc <p> Lemma 1 (Based on <ref> [BB75] </ref>) Suppose a; b 2 N [ fflg. Suppose J 2 fTxtEx a ; TxtBc a g. If M J-identifies L then there exists a J-locking sequence for M on L. <p> One can similarly define RecInfEx a , RecTxtBc a ; RecInfBc a , NoisyRecTxtEx a , NoisyRecTxtBc a , NoisyRecInfEx a , NoisyRecInfBc a . RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, TxtEx a = RecTxtEx a <ref> [BB75, Wie77, Cas98] </ref>. In Section 3 below, we indicate the remaining comparisons. 2.2 Recursively Enumerable Classes and Indexed Families This paper is about the synthesis of algorithmic learners for r.e. classes of r.e. languages and of indexed families of recursive languages.
Reference: [BCJ96] <author> G. Baliga, J. Case, and S. Jain. </author> <title> Synthesizing enumeration techniques for language learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 169-180. </pages> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: provided an amazingly negative result: there is no learner-synthesizer algorithm lsyn so that, if lsyn is fed a pair of grammars g 1 ; g 2 for a language class L = fL 1 ; L 2 g, then lsyn outputs an Ex-learner successful, from positive data, on L. 3 <ref> [BCJ96] </ref> showed how to circumvent some of the sting of this [OSW88] result by resorting to more general learners than Ex. <p> These classes are called uniformly decidable or indexed families. As is essentially pointed out in [Ang80], all of the formal language style example classes are indexed families. A sample result from <ref> [BCJ96] </ref> is: there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists decision procedures defining some indexed family L of languages which can be Bc-learned from positive data with the learner outputting grammars, then lsyn outputs a Bc-learner successful, from positive data, on L. <p> The proof of this positive result yielded the surprising characterization <ref> [BCJ96] </ref>: for indexed families L, L can be Bc-learned from positive data with the learner outputting grammars iff (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to <p> He shows that one cannot algorithmically find an Ex-learning machine for Ex-learnable indexed families of recursive languages from an index of the class. This is a bit weaker than a closely related negative result from <ref> [BCJ96] </ref>. 4 Bc is short for behaviorally correct. 5 Less roughly: in the case of noisy informant each false item may occur a finite number of times; in the case of text, it is mathematically more interesting to require, as we do, that the total amount of false information has to
Reference: [Ber85] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas98] </ref>. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's [Ste95, CJS98b] noise model.
Reference: [Blu67] <author> M. Blum. </author> <title> A machine-independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: W i denotes the domain of ' i . W i is considered as the language enumerated by the i-th program in ' system, and we say that i is a grammar or index for W i . denotes a standard Blum complexity measure <ref> [Blu67] </ref> for the programming system '. W i;s = fx &lt; s j i (x) &lt; sg. A text is a mapping from N to N [ f#g.
Reference: [BP73] <author> J. Barzdi~ns and K. Podnieks. </author> <title> The theory of inductive inference. </title> <booktitle> In Second Symposium on Mathematical Foundations of Computer Science, </booktitle> <pages> pages 9-15. </pages> <institution> Math. Inst. of the Slovak Academy of Sciences, </institution> <year> 1973. </year>
Reference-contexts: Then, let s+1 = s t s t 0 s . Note that, for all s such 12 Suppose a; b 2 N <ref> [ fflg. From [Cas98, BP73] </ref> we also have the following criteria intermediate between Ex style and Bc style. M TxtFex a b -identifies L from text T iff (9S j card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. <p> Then, let s+1 = s t s t 0 s . Note that, for all s such 12 Suppose a; b 2 N [ fflg. From <ref> [Cas98, BP73] </ref> we also have the following criteria intermediate between Ex style and Bc style. M TxtFex a b -identifies L from text T iff (9S j card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S].
Reference: [Cas86] <author> J. </author> <title> Case. Learning machines. </title> <editor> In W. Demopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: In this regard we briefly considered limiting recursive texts. One of the surprising results we have here is that TxtBc = LimRecTxtBc and NoisyTxtBc = LimRecNoisyTxtBc. One can also similarly consider texts from natural subrecursive classes [RC94], linear-time computable and above. From <ref> [Gol67, Cas86] </ref>, in that setting, some machine learns E. However, it remains to determine the possible tradeoffs between the complexity of the texts and useful complexity features of the resultant learners. [Cas86] mentions that, in some cases, subrecursiveness of texts forces infinite repetition of data. <p> One can also similarly consider texts from natural subrecursive classes [RC94], linear-time computable and above. From [Gol67, Cas86], in that setting, some machine learns E. However, it remains to determine the possible tradeoffs between the complexity of the texts and useful complexity features of the resultant learners. <ref> [Cas86] </ref> mentions that, in some cases, subrecursiveness of texts forces infinite repetition of data. Can this be connected to complexity tradeoffs? [Cas86] further notes that, if the texts we present to children, contain many repetitions, that would be consistent with a restriction in the world to subrecursive texts. <p> However, it remains to determine the possible tradeoffs between the complexity of the texts and useful complexity features of the resultant learners. <ref> [Cas86] </ref> mentions that, in some cases, subrecursiveness of texts forces infinite repetition of data. Can this be connected to complexity tradeoffs? [Cas86] further notes that, if the texts we present to children, contain many repetitions, that would be consistent with a restriction in the world to subrecursive texts.
Reference: [Cas98] <author> J. </author> <title> Case. The power of vacillation in language learning. </title> <journal> SIAM Journal on Computing, </journal> <note> 1998. To Appear (Preliminary Version Appeared in COLT 88). </note>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas98] </ref>. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's [Ste95, CJS98b] noise model. <p> One can similarly define RecInfEx a , RecTxtBc a ; RecInfBc a , NoisyRecTxtEx a , NoisyRecTxtBc a , NoisyRecInfEx a , NoisyRecInfBc a . RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, TxtEx a = RecTxtEx a <ref> [BB75, Wie77, Cas98] </ref>. In Section 3 below, we indicate the remaining comparisons. 2.2 Recursively Enumerable Classes and Indexed Families This paper is about the synthesis of algorithmic learners for r.e. classes of r.e. languages and of indexed families of recursive languages. <p> Then, let s+1 = s t s t 0 s . Note that, for all s such 12 Suppose a; b 2 N [ fflg. From <ref> [Cas98, BP73] </ref> we also have the following criteria intermediate between Ex style and Bc style. M TxtFex a b -identifies L from text T iff (9S j card (S) b ^ (8i 2 S)[W i = a L])(8 1 n)[M (T [n]) 2 S]. <p> TxtFex a b = fL j (9M)[L TxtFex a b (M)]g. InfFex a b is defined similarly. The definitions of the variants of these learning criteria involving noisy data or computable noisy data are handled similarly to such variants above. By generalizing locking sequence arguments from <ref> [Cas98] </ref> and the present paper, Theorem 8 can be improved to say: NoisyTxtFex a b = NoisyRecTxtFex a b and NoisyInfFex a b = NoisyRecInfFex a b . t s ; t 0 s exist, and each s+1 is well defined. Let T = S s2N s .
Reference: [CJS98a] <author> J. Case, S. Jain, and A. Sharma. </author> <title> Synthesizing noise-tolerant language learners. </title> <note> Theoretical Computer Science A, 1998. Accepted. 21 </note>
Reference-contexts: S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data [Ang80, Ber85, ZLK95, KB92, Cas98]. <ref> [CJS98a] </ref> considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's [Ste95, CJS98b] noise model. <p> The alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets [Ste95] (see also Theorem 14). 1 In the context of <ref> [CJS98a] </ref>, where the noisy data sequences can be uncomputable, the presence of noise plays havoc with the learnability of many concrete classes that can be learned without noise. <p> The class of languages formed by taking the union of two pattern languages can be Ex-learned from texts [Shi83]; however, this class cannot be Bc-learned from unrestricted noisy informants even if we allow the final grammars each to make finitely many mistakes. In <ref> [CJS98a] </ref>, the proofs of most of the positive results providing existence of learner-synthesizers which synthesize noise-tolerant learners also yielded pleasant characterizations which look like strict versions of the subset principle (1). 7 Here is an example. <p> See, for example, [Ang80, Muk92, LZK96, dJK96]. 2 with the negative result mentioned above from <ref> [CJS98a] </ref> that even the class of pattern languages is not learnable from unrestricted noisy texts)! Another main positive result of the present paper is Corollary 3 in Section 4.1 below. <p> 0 g)[L 0 L]: (3) Corollary 2 in the same section is the constructive version of Corollary 3 and says one can algorithmically find such a learner from an index for any indexed family so learnable. (3) is easy to check too and intriguingly differs slightly from the characterization in <ref> [CJS98a] </ref> of the same learning criterion applied to indexed families but with the noisy data sequences unrestricted: (8L 2 L)(9z)(8L 0 2 L j fx z j x 2 Lg = fx z j x 2 L 0 g)[L 0 = L]: (4) Let N denote the set of natural numbers. <p> For n &gt; 0, we do not know about synthesizing learners for U i 2 NoisyRecInfBc n . 4.2 Principal Results on Synthesizing From R.E. Indices Theorem 16 :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i RecTxtBc n (M f (x) )]. Proof. Theorem 17 in <ref> [CJS98a] </ref> showed :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i TxtBc n (M f (x) )]. The proof of this given in [CJS98a] also shows that :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i RecTxtBc n (M f (x) )]. <p> Indices Theorem 16 :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i RecTxtBc n (M f (x) )]. Proof. Theorem 17 in <ref> [CJS98a] </ref> showed :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i TxtBc n (M f (x) )]. The proof of this given in [CJS98a] also shows that :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i RecTxtBc n (M f (x) )]. Corollary 4 :(9f 2 R)(8i j C i 2 NoisyTxtEx " NoisyInfEx)[C i NoisyRecTxtBc n (M f (x) )].
Reference: [CJS98b] <author> J. Case, S. Jain, and F. Stephan. </author> <title> Vacillatory and BC learning on noisy data. </title> <note> Theoretical Computer Science A, 1998. Special Issue for ALT'96, to appear. </note>
Reference-contexts: as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data [Ang80, Ber85, ZLK95, KB92, Cas98]. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's <ref> [Ste95, CJS98b] </ref> noise model. Roughly, in this model correct information about an object occurs infinitely often while incorrect information occurs only finitely often. <p> In the case of informant every false item (x; L (x)) may occur a finite number of times. In the case of text, it is mathematically more interesting to require, as we do, that the total amount of false information has to be finite. 11 Definition 4 <ref> [Ste95, CJS98b] </ref> Suppose a 2 N [ fflg. Suppose J 2 fTxtEx a ; TxtBc a g. Then M NoisyJ-identifies L iff, for all noisy texts T for L, M J-identifies L from T . In this case we write L 2 NoisyJ (M). <p> Definition of locking sequences for learning from noisy texts is similar to that of learning from noise free texts (we just drop the requirement that content () L). However, definition of locking sequence for learning from noisy informant is more involved. Definition 5 <ref> [CJS98b] </ref> Suppose a; b 2 N [ fflg. (a) is said to be a NoisyTxtEx a -locking sequence for M on L iff, W M () = a L, and (8t j content (t ) L)[M ( t ) = M ()]. (b) is said to be a NoisyTxtBc a -locking <p> proof for NoisyEx Ex 0 [K] ]. 11 As we noted in Section 1 above, the alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets [Ste95]. 6 Proposition 1 <ref> [CJS98b] </ref> Suppose a; b 2 N [ fflg. If M learns L from noisy text or informant ac-cording to one of the criteria NoisyTxtEx a , NoisyTxtBc a , NoisyInfEx a , or NoisyInfBc a , then there exists a corresponding locking sequence for M on L. <p> Hence, U i is the indexed family with index i. 2.3 Some Previous Results on Noise Tolerant Learning In this section, we state some results from <ref> [CJS98b] </ref> and some consequences of these results (or related results) which we will apply later in the present paper. We let 2fl def Using Proposition 1 we have the following two theorems, Theorem 1 Suppose a 2 N [ fflg. Suppose L 2 NoisyInfBc a . <p> Then, for all L 2 L, there exists an n such that, (8L 0 2 L j fx 2 L j x ng = fx 2 L 0 j x ng)[L = a L 0 ]. 7 The following two theorems were proved in <ref> [CJS98b] </ref>. Theorem 4 [CJS98b] Suppose a 2 N [ fflg. L 2 NoisyTxtBc a ) [(8L 2 L)(8L 0 2 L j L 0 L)[L = 2a L 0 ]]. Theorem 5 [CJS98b] Suppose a 2 N [ fflg. <p> Then, for all L 2 L, there exists an n such that, (8L 0 2 L j fx 2 L j x ng = fx 2 L 0 j x ng)[L = a L 0 ]. 7 The following two theorems were proved in <ref> [CJS98b] </ref>. Theorem 4 [CJS98b] Suppose a 2 N [ fflg. L 2 NoisyTxtBc a ) [(8L 2 L)(8L 0 2 L j L 0 L)[L = 2a L 0 ]]. Theorem 5 [CJS98b] Suppose a 2 N [ fflg. <p> L 0 j x ng)[L = a L 0 ]. 7 The following two theorems were proved in <ref> [CJS98b] </ref>. Theorem 4 [CJS98b] Suppose a 2 N [ fflg. L 2 NoisyTxtBc a ) [(8L 2 L)(8L 0 2 L j L 0 L)[L = 2a L 0 ]]. Theorem 5 [CJS98b] Suppose a 2 N [ fflg. Then NoisyInfBc a [ NoisyTxtBc a TxtBc a and NoisyInfEx a [ NoisyTxtEx a TxtEx a . The proof of Theorem 5 also shows: Theorem 6 Suppose a 2 N [ fflg.
Reference: [CL82] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <editor> In M. Nielsen and E. M. Schmidt, editors, </editor> <booktitle> Proceedings of the 9th International Colloquium on Automata, Languages and Programming, volume 140 of Lecture Notes in Computer Science, </booktitle> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> Definition 1 Suppose a; b 2 N [ fflg. (a) Below, for each of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CL82] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * [Bar74, CL82]. <p> of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CL82] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * [Bar74, CL82]. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. * [Bar74, CL82]. <p> or informant I. * [Gol67, CL82] M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * <ref> [Bar74, CL82] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. * [Bar74, CL82]. <p> L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * <ref> [Bar74, CL82] </ref>. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. * [Bar74, CL82]. M InfBc a -identifies L from informant I iff (8 1 n)[W M (I [n]) = a L]. (b) Suppose J 2 fTxtEx a ; TxtBc a g. M J-identifies L iff, for all texts T for L, M J-identifies L from T . <p> One can similarly define RecInfEx a , RecTxtBc a ; RecInfBc a , NoisyRecTxtEx a , NoisyRecTxtBc a , NoisyRecInfEx a , NoisyRecInfBc a . RecTxtBc a 6= TxtBc a <ref> [CL82, Fre85] </ref>; however, TxtEx a = RecTxtEx a [BB75, Wie77, Cas98]. In Section 3 below, we indicate the remaining comparisons. 2.2 Recursively Enumerable Classes and Indexed Families This paper is about the synthesis of algorithmic learners for r.e. classes of r.e. languages and of indexed families of recursive languages. <p> The next theorem says that for Bc fl -learning, with computable noise, from either texts or informants, some machine learns grammars for all the r.e. languages. It improves a similar result from <ref> [CL82] </ref> for the noise-free case. Theorem 7 (a) E 2 NoisyRecTxtBc fl . (b) E 2 NoisyRecInfBc fl . Proof. (a) Define M as follows: M (T [n]) = prog (T [n]), where W prog (T [n]) is defined as follows.
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> Example more general learners are: Bc-learners, which, when successful on an object input, (by definition) find a final (possibly infinite) sequence of correct programs for that object after at most finitely many trial and error attempts <ref> [Bar74, CS83] </ref>. 4 Of course, if suitable learner-synthesizer algorithm lsyn is fed procedures for listing decision procedures (instead of mere grammars), one also has more success at synthesizing learners. <p> It follows from the above cases that M does not NoisyRecTxtBc n -identify L. Theorem 10 Suppose n 2 N. (a) NoisyTxtBc n+1 RecInfBc n 6= ;. (b) NoisyInfBc n+1 RecInfBc n 6= ;. Proof. The main idea is to modify the construction of Bc n+1 Bc n in <ref> [CS83] </ref>. (a) Let L = fL 2 REC j card (L) = 1 ^ (8 1 x 2 L)W x = n+1 Lg. Clearly, L 2 NoisyTxtBc n+1 . An easy modification of the proof of Bc n+1 Bc n 6= ; in [CS83] shows that L 62 RecInfBc n . <p> construction of Bc n+1 Bc n in <ref> [CS83] </ref>. (a) Let L = fL 2 REC j card (L) = 1 ^ (8 1 x 2 L)W x = n+1 Lg. Clearly, L 2 NoisyTxtBc n+1 . An easy modification of the proof of Bc n+1 Bc n 6= ; in [CS83] shows that L 62 RecInfBc n . We omit the details. (b) Let L = fL 2 REC j (8x 2 W min (L) )[W x = n+1 L] _ [card (W min (L) ) &lt; 1 ^ W max (W min (L) ) = n+1 L]g. <p> It is easy to verify that L 2 NoisyInf Bc n+1 . An easy modification of the proof of Bc n+1 Bc n 6= ; in <ref> [CS83] </ref> shows that L 62 RecInfBc n . We omit the details. Theorem 11 (a) NoisyRecTxtBc TxtBc fl 6= ;. (b) NoisyRecInfBc TxtBc fl 6= ;. Proof. (a) Corollary 1 below shows that all indexed families are in NoisyRecTxtBc.
Reference: [dJK96] <author> D. de Jongh and M. </author> <title> Kanazawa. Angluin's thoerem for indexed families of r.e. sets and applications. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 193-204. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: These results have consequences for Inductive Logic Programming [MR94, LD94]. 7 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions. See, for example, <ref> [Ang80, Muk92, LZK96, dJK96] </ref>. 2 with the negative result mentioned above from [CJS98a] that even the class of pattern languages is not learnable from unrestricted noisy texts)! Another main positive result of the present paper is Corollary 3 in Section 4.1 below.
Reference: [Fre85] <author> R. Freivalds. </author> <title> Recursiveness of the enumerating functions increases the inferrability of recursively enumerable sets. </title> <journal> Bulletin of the European Association for Theoretical Computer Science, </journal> <volume> 27 </volume> <pages> 35-40, </pages> <year> 1985. </year>
Reference-contexts: One can similarly define RecInfEx a , RecTxtBc a ; RecInfBc a , NoisyRecTxtEx a , NoisyRecTxtBc a , NoisyRecInfEx a , NoisyRecInfBc a . RecTxtBc a 6= TxtBc a <ref> [CL82, Fre85] </ref>; however, TxtEx a = RecTxtEx a [BB75, Wie77, Cas98]. In Section 3 below, we indicate the remaining comparisons. 2.2 Recursively Enumerable Classes and Indexed Families This paper is about the synthesis of algorithmic learners for r.e. classes of r.e. languages and of indexed families of recursive languages.
Reference: [Ful90] <author> M. Fulk. </author> <title> Robust separations in inductive inference. </title> <booktitle> In 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 405-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: The learners so synthesized are called enumeration techniques <ref> [BB75, Ful90] </ref>.
Reference: [Gol67] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction Ex-learners, when successful on an object input, (by definition) find a final correct program for that object after at most finitely many trial and error attempts <ref> [Gol67, BB75, CS83, CL82] </ref>. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S [Gol67]. <p> at most finitely many trial and error attempts [Gol67, BB75, CS83, CL82]. 1 For function learning, there is a learner-synthesizer algorithm lsyn so that, if lsyn is fed any procedure that lists programs for some (possibly infinite) class S of (total) functions, then lsyn outputs an Ex-learner successful on S <ref> [Gol67] </ref>. The learners so synthesized are called enumeration techniques [BB75, Ful90]. <p> In fact the computational learning theory community has shown considerable interest (spanning at least from <ref> [Gol67] </ref> to [ZL95]) in language classes defined by r.e. listings of decision procedures. These classes are called uniformly decidable or indexed families. As is essentially pointed out in [Ang80], all of the formal language style example classes are indexed families. <p> Definition 1 Suppose a; b 2 N [ fflg. (a) Below, for each of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CL82] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * [Bar74, CL82]. <p> of several learning criteria J, we define what it means for a machine M to J-identify a language L from a text T or informant I. * <ref> [Gol67, CL82] </ref> M TxtEx a -identifies L from text T iff (9i j W i = a L)[M (T )# = i]. * [Gol67, CL82] M InfEx a -identifies L from informant I iff (9i j W i = a L)[M (I )# = i]. * [Bar74, CL82]. M TxtBc a -identifies L from text T iff (8 1 n)[W M (T [n]) = a L]. * [Bar74, CL82]. <p> Let L = fL i j i 2 N g. Note that L 62 TxtBc fl (essentially due to <ref> [Gol67] </ref>). Now let z i = i + 1. It is easy to verify that, for all i, for all L 0 2 N, if L 0 " fx j x z i g = L " fx j x z i g, then L 0 L. <p> In this regard we briefly considered limiting recursive texts. One of the surprising results we have here is that TxtBc = LimRecTxtBc and NoisyTxtBc = LimRecNoisyTxtBc. One can also similarly consider texts from natural subrecursive classes [RC94], linear-time computable and above. From <ref> [Gol67, Cas86] </ref>, in that setting, some machine learns E. However, it remains to determine the possible tradeoffs between the complexity of the texts and useful complexity features of the resultant learners. [Cas86] mentions that, in some cases, subrecursiveness of texts forces infinite repetition of data.
Reference: [Jan79a] <author> K. </author> <title> Jantke. Automatic synthesis of programs and inductive inference of functions. </title> <booktitle> In Int. Conf. Fundamentals of Computations Theory, </booktitle> <pages> pages 219-225. </pages> <publisher> Akademie-Verlag, </publisher> <address> Berlin, </address> <year> 1979. </year>
Reference-contexts: Hence, this model has the advantage that noisy data about an object nonetheless uniquely specifies that object. 5 1 Ex is short for explanatory. 2 The reader is referred to Jantke <ref> [Jan79a, Jan79b] </ref> for a discussion of synthesizing learners for classes of computable functions that are not necessarily recursively enumerable. 3 Again for language learning from positive data and with learners outputting grammars, a somewhat related negative result is provided by Kapur [Kap91].
Reference: [Jan79b] <author> K. </author> <title> Jantke. Natural properties of strategies identifying recursive functions. </title> <journal> Electronische Informationverarbeitung und Kybernetik, </journal> <volume> 15 </volume> <pages> 487-496, </pages> <year> 1979. </year>
Reference-contexts: Hence, this model has the advantage that noisy data about an object nonetheless uniquely specifies that object. 5 1 Ex is short for explanatory. 2 The reader is referred to Jantke <ref> [Jan79a, Jan79b] </ref> for a discussion of synthesizing learners for classes of computable functions that are not necessarily recursively enumerable. 3 Again for language learning from positive data and with learners outputting grammars, a somewhat related negative result is provided by Kapur [Kap91].
Reference: [Kap91] <author> S. Kapur. </author> <title> Computational Learning of Languages. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1991. </year>
Reference-contexts: for explanatory. 2 The reader is referred to Jantke [Jan79a, Jan79b] for a discussion of synthesizing learners for classes of computable functions that are not necessarily recursively enumerable. 3 Again for language learning from positive data and with learners outputting grammars, a somewhat related negative result is provided by Kapur <ref> [Kap91] </ref>. He shows that one cannot algorithmically find an Ex-learning machine for Ex-learnable indexed families of recursive languages from an index of the class.
Reference: [KB92] <author> S. Kapur and G. Bilardi. </author> <title> Language learning without overgeneralization. </title> <editor> In A. Finkel and M. Jantzen, editors, </editor> <booktitle> Proceedings of the Ninth Annual Symposium on Theoretical Aspects of Computer Science, volume 577 of Lecture Notes in Computer Science, </booktitle> <pages> pages 245-256. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas98] </ref>. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's [Ste95, CJS98b] noise model.
Reference: [KR88] <author> S. Kurtz and J. Royer. </author> <title> Prudence in language learning. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 143-156. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Since, for every r.e. language, the canonical informant is limit recursive, the notion of learning from limit recursive informant collapses to the notion of learning from arbitrary informants (for Bc a and Ex a style learning criteria). Definition 9 <ref> [KR88] </ref> A text T is normalized, iff for all n, T (n) = #, or T (n) &lt; n. <p> It follows that T 0 = S a limit recursive text which is not TxtBc-identified by M. The proof of Theorem 4.9 in <ref> [KR88] </ref> also showed that Theorem 18 From a given M one can algorithmically generate an M 0 such that, for all L: If every normalized , such that content () L, has a normalized extension 0 , which is a normalized TxtBc-locking sequence for M on L, then M 0 TxtBc-identifies
Reference: [LD94] <author> N. Lavarac and S. Dzeroski. </author> <title> Inductive Logic Programming. </title> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: These results have consequences for Inductive Logic Programming <ref> [MR94, LD94] </ref>. 7 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions.
Reference: [LZK96] <author> S. Lange, T. Zeugmann, and S. Kapur. </author> <title> Monotonic and dual monotonic language learning. </title> <journal> Theoretical Computer Science A, </journal> <volume> 155 </volume> <pages> 365-410, </pages> <year> 1996. </year>
Reference-contexts: However, some learner can succeed on the pattern languages from noise-free informants and on its first guess as to a correct grammar (see <ref> [LZK96] </ref>). The class of languages formed by taking the union of two pattern languages can be Ex-learned from texts [Shi83]; however, this class cannot be Bc-learned from unrestricted noisy informants even if we allow the final grammars each to make finitely many mistakes. <p> These results have consequences for Inductive Logic Programming [MR94, LD94]. 7 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions. See, for example, <ref> [Ang80, Muk92, LZK96, dJK96] </ref>. 2 with the negative result mentioned above from [CJS98a] that even the class of pattern languages is not learnable from unrestricted noisy texts)! Another main positive result of the present paper is Corollary 3 in Section 4.1 below.
Reference: [MR94] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> 19/20:669-679, 1994. 
Reference-contexts: These results have consequences for Inductive Logic Programming <ref> [MR94, LD94] </ref>. 7 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions.
Reference: [Muk92] <author> Y. Mukouchi. </author> <title> Characterization of finite identification. </title> <editor> In K. Jantke, editor, </editor> <title> Analogical and Inductive Inference, </title> <booktitle> Proceedings of the Third International Workshop, </booktitle> <pages> pages 260-267, </pages> <year> 1992. </year> <month> 22 </month>
Reference-contexts: These results have consequences for Inductive Logic Programming [MR94, LD94]. 7 For L either an indexed family or defined by some r.e. listing of grammars, the prior literature has many interesting characterizations of L being Ex-learnable from noise-free positive data, with and without extra restrictions. See, for example, <ref> [Ang80, Muk92, LZK96, dJK96] </ref>. 2 with the negative result mentioned above from [CJS98a] that even the class of pattern languages is not learnable from unrestricted noisy texts)! Another main positive result of the present paper is Corollary 3 in Section 4.1 below.
Reference: [Nix83] <author> R. Nix. </author> <title> Editing by examples. </title> <type> Technical Report 280, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, USA, </address> <year> 1983. </year>
Reference-contexts: but one can algorithmically find a corresponding Bc-learner (of this kind) from an index for any indexed family! As a corollary to Theorem 13 we have that the class of finite unions of pattern languages is Bc-learnable from computable noisy texts, where the machine outputs grammars (this contrasts sharply 6 <ref> [Nix83] </ref> as well as [SA95] outline interesting applications of pattern inference algorithms. For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see [SSS + 94, SA95]).
Reference: [Odi89] <author> P. Odifreddi. </author> <title> Classical Recursion Theory. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: Importantly, the same proof yields this equivalence also in the case of noisy data sequences. Finally Section 6 gives some directions for further research. 2 Preliminaries 2.1 Notation and Identification Criteria The recursion theoretic notions are from the books of Odifreddi <ref> [Odi89] </ref> and Soare [Soa87]. N = f0; 1; 2; : : :g is the set of all natural numbers, and this paper considers r.e. subsets L of N.
Reference: [OSW86] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn: An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: One of the motivations for considering possibly non-computable data sequences is that, in the case of child language learning, the utterances the child hears (as its data) may, in part, be determined by uncomputable random processes <ref> [OSW86] </ref> perhaps external to the utterance generators (e.g., the parents). The limit recursive functions are in between the computable and the arbitrarily uncomputable. Here is the idea.
Reference: [OSW88] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Synthesising inductive expertise. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 138-161, </pages> <year> 1988. </year>
Reference-contexts: The learners so synthesized are called enumeration techniques [BB75, Ful90]. These enumeration techniques yield many positive learnability results, for example, that the class of all functions computable in time polynomial in the length of input is Ex-learnable. 2 For language learning from positive data and with learners outputting grammars, <ref> [OSW88] </ref> provided an amazingly negative result: there is no learner-synthesizer algorithm lsyn so that, if lsyn is fed a pair of grammars g 1 ; g 2 for a language class L = fL 1 ; L 2 g, then lsyn outputs an Ex-learner successful, from positive data, on L. 3 <p> so that, if lsyn is fed a pair of grammars g 1 ; g 2 for a language class L = fL 1 ; L 2 g, then lsyn outputs an Ex-learner successful, from positive data, on L. 3 [BCJ96] showed how to circumvent some of the sting of this <ref> [OSW88] </ref> result by resorting to more general learners than Ex.
Reference: [RC94] <author> J. Royer and J. </author> <title> Case. Subrecursive programming systems: Complexity & succinctness. </title> <publisher> Birkhauser, </publisher> <year> 1994. </year>
Reference-contexts: In this regard we briefly considered limiting recursive texts. One of the surprising results we have here is that TxtBc = LimRecTxtBc and NoisyTxtBc = LimRecNoisyTxtBc. One can also similarly consider texts from natural subrecursive classes <ref> [RC94] </ref>, linear-time computable and above. From [Gol67, Cas86], in that setting, some machine learns E.
Reference: [SA95] <author> T. Shinohara and A. </author> <title> Arikawa. Pattern inference. </title> <editor> In Klaus P. Jantke and Steffen Lange, editors, </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 259-291. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: find a corresponding Bc-learner (of this kind) from an index for any indexed family! As a corollary to Theorem 13 we have that the class of finite unions of pattern languages is Bc-learnable from computable noisy texts, where the machine outputs grammars (this contrasts sharply 6 [Nix83] as well as <ref> [SA95] </ref> outline interesting applications of pattern inference algorithms. For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see [SSS + 94, SA95]). <p> For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see <ref> [SSS + 94, SA95] </ref>). Pattern languages and finite unions of pattern languages [Shi83, Wri89] turn out to be subclasses of Smullyan's [Smu61] Elementary Formal Systems (EFSs). [ASY92] show that the EFSs can also be treated as a logic programming language over strings.
Reference: [Sha71] <author> N. Shapiro. </author> <title> Review of "Limiting recursion" by E.M. Gold and "Trial and error predicates and the solution to a problem of Mostowski" by H. Putnam. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 36:342, </volume> <year> 1971. </year>
Reference-contexts: The limit recursive functions are in between the computable and the arbitrarily uncomputable. Here is the idea. Informally, they are (by definition) the functions computed by limit-programs, programs which do not give correct output until after some unspecified but finite number of trial outputs <ref> [Sha71] </ref>. They "change their minds" finitely many times about each output before getting it right. 9 In Section 5 we consider briefly what would happen if the world provided limit recursive data sequences (instead of computable or unrestricted ones).
Reference: [Shi83] <author> T. Shinohara. </author> <title> Inferring unions of two pattern languages. </title> <journal> Bulletin of Informatics and Cybernetics, </journal> <volume> 20 </volume> <pages> 83-88., </pages> <year> 1983. </year>
Reference-contexts: However, some learner can succeed on the pattern languages from noise-free informants and on its first guess as to a correct grammar (see [LZK96]). The class of languages formed by taking the union of two pattern languages can be Ex-learned from texts <ref> [Shi83] </ref>; however, this class cannot be Bc-learned from unrestricted noisy informants even if we allow the final grammars each to make finitely many mistakes. <p> For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see [SSS + 94, SA95]). Pattern languages and finite unions of pattern languages <ref> [Shi83, Wri89] </ref> turn out to be subclasses of Smullyan's [Smu61] Elementary Formal Systems (EFSs). [ASY92] show that the EFSs can also be treated as a logic programming language over strings.
Reference: [Shi91] <author> T. Shinohara. </author> <title> Inductive inference of monotonic formal systems from positive data. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 371-384, </pages> <year> 1991. </year>
Reference-contexts: The techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFSs <ref> [Shi91] </ref>. Investigations of the learnability of subclasses of EFSs are important because they yield corresponding results about the learnability of subclasses of logic programs. [AS94] use the insight gained from the learnability of EFSs subclasses to show that a class of linearly covering logic programs with local variables is TxtEx-learnable.
Reference: [Smu61] <author> R. </author> <title> Smullyan. </title> <journal> Theory of Formal Systems, Annals of Mathematical Studies, </journal> <volume> No. </volume> <pages> 47. </pages> <address> Princeton, NJ, </address> <year> 1961. </year>
Reference-contexts: For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see [SSS + 94, SA95]). Pattern languages and finite unions of pattern languages [Shi83, Wri89] turn out to be subclasses of Smullyan's <ref> [Smu61] </ref> Elementary Formal Systems (EFSs). [ASY92] show that the EFSs can also be treated as a logic programming language over strings. The techniques for learning finite unions of pattern languages have been extended to show the learnability of various subclasses of EFSs [Shi91].
Reference: [Soa87] <author> R. Soare. </author> <title> Recursively Enumerable Sets and Degrees. </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: Importantly, the same proof yields this equivalence also in the case of noisy data sequences. Finally Section 6 gives some directions for further research. 2 Preliminaries 2.1 Notation and Identification Criteria The recursion theoretic notions are from the books of Odifreddi [Odi89] and Soare <ref> [Soa87] </ref>. N = f0; 1; 2; : : :g is the set of all natural numbers, and this paper considers r.e. subsets L of N.
Reference: [SSS + 94] <author> S. Shimozono, A. Shinohara, T. Shinohara, S. Miyano, S. Kuhara, and S. </author> <title> Arikawa. Knowledge acquisition from amino acid sequences by machine learning system BONSAI. </title> <journal> Trans. Information Processing Society of Japan, </journal> <volume> 35 </volume> <pages> 2009-2018, </pages> <year> 1994. </year>
Reference-contexts: For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see <ref> [SSS + 94, SA95] </ref>). Pattern languages and finite unions of pattern languages [Shi83, Wri89] turn out to be subclasses of Smullyan's [Smu61] Elementary Formal Systems (EFSs). [ASY92] show that the EFSs can also be treated as a logic programming language over strings.
Reference: [Ste95] <author> F. Stephan. </author> <title> Noisy inference and oracles. </title> <editor> In K. Jantke, T. Shinohara, and T. Zeugmann, editors, </editor> <booktitle> Algorithmic Learning Theory: Sixth International Workshop (ALT '95), volume 997 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 185-200. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data [Ang80, Ber85, ZLK95, KB92, Cas98]. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's <ref> [Ste95, CJS98b] </ref> noise model. Roughly, in this model correct information about an object occurs infinitely often while incorrect information occurs only finitely often. <p> The alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets <ref> [Ste95] </ref> (see also Theorem 14). 1 In the context of [CJS98a], where the noisy data sequences can be uncomputable, the presence of noise plays havoc with the learnability of many concrete classes that can be learned without noise. <p> Definition 3 <ref> [Ste95] </ref> An information sequence I is a noisy information sequence (or noisy informant) for L iff (8x) [occur (I ; (x; L (x))) = 1 ^ occur (I ; (x; L (x))) &lt; 1]. <p> In the case of informant every false item (x; L (x)) may occur a finite number of times. In the case of text, it is mathematically more interesting to require, as we do, that the total amount of false information has to be finite. 11 Definition 4 <ref> [Ste95, CJS98b] </ref> Suppose a 2 N [ fflg. Suppose J 2 fTxtEx a ; TxtBc a g. Then M NoisyJ-identifies L iff, for all noisy texts T for L, M J-identifies L from T . In this case we write L 2 NoisyJ (M). <p> For the criteria of noisy inference discussed in this paper one can prove the existence of a locking sequence as was done in <ref> [Ste95, Theorem 2, proof for NoisyEx Ex 0 [K] </ref> ]. 11 As we noted in Section 1 above, the alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets [Ste95]. 6 <p> in [Ste95, Theorem 2, proof for NoisyEx Ex 0 [K] ]. 11 As we noted in Section 1 above, the alternative of allowing each false item in a text to occur finitely often is too restrictive; it would, then, be impossible to learn even the class of all singleton sets <ref> [Ste95] </ref>. 6 Proposition 1 [CJS98b] Suppose a; b 2 N [ fflg.
Reference: [Wie77] <author> R. Wiehagen. </author> <title> Identification of formal languages. </title> <booktitle> In Mathematical Foundations of Computer Science, volume 53 of Lecture Notes in Computer Science, </booktitle> <pages> pages 571-579. </pages> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: One can similarly define RecInfEx a , RecTxtBc a ; RecInfBc a , NoisyRecTxtEx a , NoisyRecTxtBc a , NoisyRecInfEx a , NoisyRecInfBc a . RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, TxtEx a = RecTxtEx a <ref> [BB75, Wie77, Cas98] </ref>. In Section 3 below, we indicate the remaining comparisons. 2.2 Recursively Enumerable Classes and Indexed Families This paper is about the synthesis of algorithmic learners for r.e. classes of r.e. languages and of indexed families of recursive languages.
Reference: [Wri89] <author> K. Wright. </author> <title> Identification of unions of languages drawn from an identifiabl e class. </title> <editor> In R. Rivest, D. Haussler, and M.K. Warmuth, editors, </editor> <booktitle> Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 328-333. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1989. </year> <month> 23 </month>
Reference-contexts: For example, pattern language learning algorithms have been successfully applied for solving problems in molecular biology (see [SSS + 94, SA95]). Pattern languages and finite unions of pattern languages <ref> [Shi83, Wri89] </ref> turn out to be subclasses of Smullyan's [Smu61] Elementary Formal Systems (EFSs). [ASY92] show that the EFSs can also be treated as a logic programming language over strings.
Reference: [ZL95] <author> T. Zeugmann and S. Lange. </author> <title> A guided tour across the boundaries of learning recursive languages. </title> <editor> In K. Jantke and S. Lange, editors, </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 190-258. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: In fact the computational learning theory community has shown considerable interest (spanning at least from [Gol67] to <ref> [ZL95] </ref>) in language classes defined by r.e. listings of decision procedures. These classes are called uniformly decidable or indexed families. As is essentially pointed out in [Ang80], all of the formal language style example classes are indexed families.
Reference: [ZLK95] <author> T. Zeugmann, S. Lange, and S. Kapur. </author> <title> Characterizations of monotonic and dual monotonic language learning. </title> <journal> Information and Computation, </journal> <volume> 120 </volume> <pages> 155-173, </pages> <year> 1995. </year> <month> 24 </month>
Reference-contexts: (8L 2 L)(9S L j S is finite)(8L 0 2 L j S L 0 )[L 0 6 L]: (1) (1) is Angluin's important Condition 2 from [Ang80], and it is referred to as the subset principle, in general a necessary condition for preventing overgeneralization in learning from positive data <ref> [Ang80, Ber85, ZLK95, KB92, Cas98] </ref>. [CJS98a] considered language learning from both noisy texts (only positive data) and from noisy informants (both positive and negative data), and adopted, as does the present paper, Stephan's [Ste95, CJS98b] noise model.
References-found: 45

