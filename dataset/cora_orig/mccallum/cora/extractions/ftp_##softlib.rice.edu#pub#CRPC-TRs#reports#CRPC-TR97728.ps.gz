URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97728.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Comparison of PETSc Library and HPF Implementations of an Archetypal PDE Computation 1  
Author: M. Ehtesham Hayder a David E. Keyes b Piyush Mehrotra c 
Keyword: Parallel languages, parallel libraries, parallel scientific computing, nonlinear ellip tic boundary value problems  
Address: MS  Hampton VA. 23681.  
Affiliation: Computation, Rice University, Houston, TX. b Computer Science Department, Old Dominion University, Norfolk, VA and ICASE. c Institute for Computer Applications in Science and Engineering  NASA Langley Research Center,  
Note: a Center for Research on Parallel  
Email: hayder@cs.rice.edu keyes@cs.odu.edu pm@icase.edu  
Phone: 403,  
Abstract: Two paradigms for distributed-memory parallel computation that free the application programmer from the details of message passing are compared for an archetypal structured scientific computation | a nonlinear, structured-grid partial differential equation boundary value problem | using the same algorithm on the same hardware. Both paradigms, parallel libraries represented by Argonne's PETSc, and parallel languages represented by the Portland Group's HPF, are found to be easy to use for this problem class, and both are reasonably effective in exploiting concurrency after a short learning curve. The level of involvement required by the application programmer under either paradigm includes specification of the data partitioning (corresponding to a geometrically simple decomposition of the domain of the PDE). Programming in SPMD style for the PETSc library requires writing the routines that discretize the PDE and its Jacobian, managing subdomain-to-processor mappings (affine global-to-local index mappings), and interfacing to library solver routines. Programming for HPF requires a complete sequential implementation of the same algorithm, introduction of concurrency through subdomain blocking (an effort similar to the index mapping), and modest experimentation with rewriting loops to elucidate to the compiler the latent concurrency. Correctness and scalability are cross-validated on up to 32 nodes of an IBM SP2. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. More, and G. Xue, </author> <year> 1992, </year> <title> The MINPACK-2 Test Problem Collection, </title> <institution> MCS-P153-0692, Mathematics and Computer Science Division, Argonne National Laboratory. </institution>
Reference-contexts: The model problem is given as r 2 u e u = 0; (1) with u = 0 at the boundary, where u is the temperature and is a constant. The Bratu problem is a part of the MINPACK-2 test problem collection <ref> [1] </ref> and is implemented in a variety of ways in the distribution set of demo drivers for the PETSc library, to illustrate different features of PETSc for nonlinear problems. For our model case, we consider a square domain of unit length and = 6.

Reference: [3] <author> S. Balay, W. D. Gropp, L. C. McInnes, and B. F. Smith, </author> <year> 1997, </year> <title> Efficient Management of Parallelism in Object-Oriented Numerical Software Libraries, in "Modern Software Tools in Scientific Computing", </title> <editor> E. Arge, A. M. Bruaset, and H. P. Langtangen, eds., </editor> <publisher> Birkhauser. </publisher>
Reference-contexts: The tradeoff between cost per iteration and number of iterations is variously resolved in the parallel implicit PDE literature, but our choices are rather common and not far from optimal, in practice. 3 PETSc Implementation Our library implementation employs the "Portable, Extensible Toolkit for Scientific Computing" (PETSc) <ref> [2, 3] </ref>, a freely available software package that attempts to handle through a uniform interface, in a highly efficient way, the low-level details of the distributed memory hierarchy.
Reference: [4] <editor> P. E. Bjorstad, M. Espedal, and D. E. Keyes, eds., </editor> <booktitle> 1997, "Domain Decomposition Methods in Computational Science and Engineering" (Proceedings of the 9th International Conference on Domain Decomposition Methods, Bergen, </booktitle> <year> 1996), </year> <note> Wiley, to appear. </note>
Reference-contexts: Schwarz-type domain decomposi-tion methods have been extensively developed for finite difference/element/volume PDE dis-cretizations over the past decade, as reported in the annual proceedings of the international conferences on domain decomposition methods (see, e.g., <ref> [4] </ref> and the references therein).
Reference: [5] <author> X.-C. Cai, W. D. Gropp, D. E. Keyes, R. E. Melvin, and D. P. Young, </author> <year> 1996, </year> <title> Parallel Newton-Krylov-Schwarz Algorithms for the Transonic Full Potential Equation, </title> <note> SIAM J. Sci. Comp., to appear; see also ICASE TR 96-39. </note>
Reference: [6] <author> B. Chapman, P. Mehrotra, and H. Zima, </author> <year> 1994, </year> <title> Extending HPF for Advanced Data Parallel Applications, </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1994, </year> <pages> pp. 59-70. </pages>
Reference-contexts: We have provided only a brief description of some of the features of HPF. A full description can be found in [13] while a discussion of how to use these features in various applications can be found in <ref> [6, 19, 20] </ref>. Conversion of the Code to HPF The original code for the Bratu problem was a Fortran 77 implementation of the NKS method of Section 2, written by one of us (DEK), which pre-dated the PETSc NKS implementation.
Reference: [7] <author> D. E. Culler, J. P. Singh, and A. Gupta, </author> <year> 1997, </year> <title> "Parallel Computer Architecture", </title> <note> Morgan-Kaufman Press, to appear. </note>
Reference-contexts: Domain-based parallelism is recognized by architects and algorithmicists as the form of data parallelism that most effectively exploits contemporary multi-level memory hierarchy microproces 3 sors <ref> [7, 17] </ref>. Schwarz-type domain decomposi-tion methods have been extensively developed for finite difference/element/volume PDE dis-cretizations over the past decade, as reported in the annual proceedings of the international conferences on domain decomposition methods (see, e.g., [4] and the references therein).
Reference: [8] <author> R. Dembo, S. Eisenstat, and T. Steihaug, </author> <year> 1982, </year> <title> Inexact Newton Methods, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 19 </volume> <pages> 400-408. </pages>
Reference: [9] <author> J. E. Dennis and R. B. Schnabel, </author> <year> 1983, </year> <title> "Numerical Methods for Unconstrained Optimization and Nonlinear Equations", </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Newton-Krylov-Schwarz We solve (2) by an inexact Newton-iterative method with a cubic backtracking line search <ref> [9] </ref>. Typically the RHS of the linear Newton correction equation, which is the negative of the nonlinear residual vector, is evaluated to full precision. The inexactness arises from an incomplete convergence employing the true Jacobian matrix or from the employment of an inexact or a "lagged" Jacobian.
Reference: [10] <author> M. Dryja and O. B. Widlund, </author> <year> 1987, </year> <title> An Additive Variant of the Alternating Method for the Case of Many Subregions, </title> <type> TR 339, </type> <institution> Courant Institute, New York University. </institution>
Reference-contexts: In KS methods, the preconditioning is introduced on a subdomain-by-subdomain basis through convenient concurrently computable approximations to local Jacobians. Such Schwarz-type preconditioning provides good data locality for parallel implementations over a range of parallel granu-larities, allowing significant architectural adaptability [12, 14]. Two-level Additive Schwarz preconditioning <ref> [10] </ref> with modest overlap between the subdo-mains and a coarse grid is optimal for this problem, for sufficiently small . However, for conformity with common practice and simplicity of coding, we employ a "poor man's" Additive Schwarz, namely single-level zero-overlap subdomain block Jacobi.
Reference: [11] <author> W. D. Gropp and D. E. Keyes, </author> <year> 1989, </year> <title> Domain Decomposition on Parallel Computers, Impact of Comp. </title> <journal> in Sci. and Eng. </journal> <volume> 1 </volume> <pages> 421-439. </pages>
Reference-contexts: The algorithmic discussion in 2 the balance of this section is sufficient to under-stand the main computation and communication costs in solving (2), but we defer full parallel complexity studies, including a discussion of optimal parallel granularities, partitioning strategies, and running times to the literature, e.g. <ref> [11, 16] </ref>. Newton-Krylov-Schwarz We solve (2) by an inexact Newton-iterative method with a cubic backtracking line search [9]. Typically the RHS of the linear Newton correction equation, which is the negative of the nonlinear residual vector, is evaluated to full precision.
Reference: [12] <author> W. D. Gropp, D. E. Keyes, L. C. McInnes, and M. D. Tidriri, </author> <year> 1997, </year> <title> Parallel Implicit PDE Computations: </title> <booktitle> Algorithms and Software, in "Parallel Computational Fluid Dynamics '97" (Proceedings of Parallel CFD'97, </booktitle> <address> Manchester, </address> <year> 1997), </year> <editor> A. Ecer, D. Emerson, J. Periaux, and N Satofuka, eds., </editor> <address> Elsevier, </address> <note> to appear. </note>
Reference-contexts: In KS methods, the preconditioning is introduced on a subdomain-by-subdomain basis through convenient concurrently computable approximations to local Jacobians. Such Schwarz-type preconditioning provides good data locality for parallel implementations over a range of parallel granu-larities, allowing significant architectural adaptability <ref> [12, 14] </ref>. Two-level Additive Schwarz preconditioning [10] with modest overlap between the subdo-mains and a coarse grid is optimal for this problem, for sufficiently small . However, for conformity with common practice and simplicity of coding, we employ a "poor man's" Additive Schwarz, namely single-level zero-overlap subdomain block Jacobi.
Reference: [13] <author> High Performance Fortran Forum, </author> <year> 1997, </year> <title> High Performance Fortran Language Specification, </title> <note> Version 2.0; see also http://www.crpc.rice.edu/HPFF/home.html. </note>
Reference-contexts: IBM's own MPI was employed as the communication library. 4 HPF Implementation High Performance Fortran (HPF) is a set of extensions to Fortran, designed to facilitate efficient data parallel programming on a wide range of parallel architectures <ref> [13] </ref>. The basic approach of HPF is to provide directives that allow the programmer to specify the distribution of data across processors, which, in turn, helps the compiler effectively exploit the parallelism. <p> We have provided only a brief description of some of the features of HPF. A full description can be found in <ref> [13] </ref> while a discussion of how to use these features in various applications can be found in [6, 19, 20].
Reference: [14] <author> D. E. Kaushik, D. E. Keyes, and B. F. Smith, </author> <year> 1997, </year> <title> On the Interaction of Architecture and Algorithm in the Domain-based Parallelization of an Unstructured Grid Incompressible Flow Code, </title> <booktitle> in "Proceedings of the 10th International Conference on Domain Decomposition Methods", </booktitle> <editor> C. Farhat, et al., eds., </editor> <address> Wiley, </address> <note> to appear. </note>
Reference-contexts: In KS methods, the preconditioning is introduced on a subdomain-by-subdomain basis through convenient concurrently computable approximations to local Jacobians. Such Schwarz-type preconditioning provides good data locality for parallel implementations over a range of parallel granu-larities, allowing significant architectural adaptability <ref> [12, 14] </ref>. Two-level Additive Schwarz preconditioning [10] with modest overlap between the subdo-mains and a coarse grid is optimal for this problem, for sufficiently small . However, for conformity with common practice and simplicity of coding, we employ a "poor man's" Additive Schwarz, namely single-level zero-overlap subdomain block Jacobi.
Reference: [15] <author> D. E. Keyes, </author> <year> 1995, </year> <title> A Perspective on Data-Parallel Implicit Solvers for Mechanics, </title> <journal> Bulletin of the U. S. Association of Computational Mechanics 8(3) </journal> <pages> 3-7. </pages>
Reference-contexts: The algorithm is a Newton-Krylov method with subdomain-concurrent ILU preconditioning, also known as a Newton-Krylov-Schwarz (NKS) method <ref> [15] </ref>. Its basic components are typical of other algorithms for PDEs: (1) sparse matrix-vector products (together with Jacobian matrix and residual vector evaluations) based on regular multidimensional grid stencil operations, (2) sparse triangular solution recurrences, (3) global reductions, and (4) DAX-PYs.
Reference: [16] <author> D. E. Keyes and M. D. Smooke, </author> <year> 1987, </year> <title> A Par-allelized Elliptic Solver for Reacting Flows, in "Parallel Computations and Their Impact on Mechanics", </title> <editor> A. K. Noor, ed., </editor> <booktitle> ASME, </booktitle> <pages> pp. 375-402. </pages>
Reference-contexts: The algorithmic discussion in 2 the balance of this section is sufficient to under-stand the main computation and communication costs in solving (2), but we defer full parallel complexity studies, including a discussion of optimal parallel granularities, partitioning strategies, and running times to the literature, e.g. <ref> [11, 16] </ref>. Newton-Krylov-Schwarz We solve (2) by an inexact Newton-iterative method with a cubic backtracking line search [9]. Typically the RHS of the linear Newton correction equation, which is the negative of the nonlinear residual vector, is evaluated to full precision.
Reference: [17] <author> D. E. Keyes, D. S. Truhlar, and Y. Saad, eds., </author> <year> 1995, </year> <title> Domain-based Parallelism and Problem Decomposition Methods in Science and Engineering, </title> <publisher> SIAM. </publisher>
Reference-contexts: Domain-based parallelism is recognized by architects and algorithmicists as the form of data parallelism that most effectively exploits contemporary multi-level memory hierarchy microproces 3 sors <ref> [7, 17] </ref>. Schwarz-type domain decomposi-tion methods have been extensively developed for finite difference/element/volume PDE dis-cretizations over the past decade, as reported in the annual proceedings of the international conferences on domain decomposition methods (see, e.g., [4] and the references therein).
Reference: [18] <author> C. H. Koelbel, D. B. Loveman, R. S. Schreiber, G. L. Steele, and M. E. Zosel, </author> <year> 1994, </year> <title> "The High Performance Fortran Handbook", </title> <publisher> MIT Press. </publisher>
Reference-contexts: The burden on the programmer may be reduced if the high-level programming language itself supports parallel constructs, which is the philosophy that underlies the High Performance Fortran <ref> [18] </ref> extensions to Fortran. With varying degrees of hints from programmers, the HPF approach leaves the responsibility of managing concurrency and data communication to the compiler and runtime system.
Reference: [19] <author> P. Mehrotra, J. Van Rosendale, and H. Zima, </author> <year> 1997, </year> <title> High Performance Fortran: History, Status and Future, Parallel Computing, </title> <note> to appear. </note>
Reference-contexts: We have provided only a brief description of some of the features of HPF. A full description can be found in [13] while a discussion of how to use these features in various applications can be found in <ref> [6, 19, 20] </ref>. Conversion of the Code to HPF The original code for the Bratu problem was a Fortran 77 implementation of the NKS method of Section 2, written by one of us (DEK), which pre-dated the PETSc NKS implementation.
Reference: [20] <author> K. P. Roe and P. Mehrotra, </author> <year> 1997, </year> <title> Implementation of a Total Variation Diminishing Scheme for the Shock Tube Problem in High Performance Fortran, </title> <booktitle> Proceedings of the 8th SIAM Conference on Parallel Processing, </booktitle> <address> Minneapo-lis, </address> <publisher> SIAM (CD-ROM). </publisher>
Reference-contexts: We have provided only a brief description of some of the features of HPF. A full description can be found in [13] while a discussion of how to use these features in various applications can be found in <ref> [6, 19, 20] </ref>. Conversion of the Code to HPF The original code for the Bratu problem was a Fortran 77 implementation of the NKS method of Section 2, written by one of us (DEK), which pre-dated the PETSc NKS implementation.
Reference: [21] <author> Y. Saad and M. H. Schultz, </author> <year> 1986, </year> <pages> GMRES: </pages>
Reference-contexts: Periodic nearest-neighbor communication is required to "ghost" the values present in the boundary stencils of one processor but maintained and updated by a neighboring processor. We use the restarted generalized minimum residual (GMRES) <ref> [21] </ref> method for the iterative solution of the linearized equation. We terminate GMRES when the norm of the linear residual first falls below a threshold defined relative its initial value or at which it falls below an absolute threshold.
References-found: 20

