URL: ftp://svr-ftp.eng.cam.ac.uk/pub/reports/kershaw_nips95.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: Email: djk, ajr @eng.cam.ac.uk  
Phone: Tel: [+44] 1223 332800, Fax: [+44] 1223 332662.  
Title: Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System  
Author: Dan Kershaw Tony Robinson Mike Hochberg 
Address: Trumpington Street, Cambridge CB2 1PZ, England.  
Affiliation: Cambridge University Engineering Department,  
Abstract: A method for incorporating context-dependent phone classes in a connectionist-HMM hybrid speech recognition system is introduced. A modular approach is adopted, where single-layer networks discriminate between different context classes given the phone class and the acoustic data. The context networks are combined with a context-independent (CI) network to generate context-dependent (CD) phone probability estimates. Experiments show an average reduction in word error rate of 16% and 13% from the CI system on ARPA 5,000 word and SQALE 20,000 word tasks respectively. Due to improved modelling, the decoding speed of the CD system is more than twice as fast as the CI system.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H. & Morgan, N. </author> <year> (1993), </year> <title> `Continuous Speech Recognition by Connectionist Statistical Methods', </title> <journal> IEEE Transactions on Neural Networks 4(6), </journal> <pages> 893-909. </pages>
Reference: <author> Bourlard, H. & Morgan, N. </author> <year> (1994), </year> <title> Connectionist Speech Recognition: A Hybrid Approach, </title> <publisher> Kluwer Acedemic Publishers. </publisher>
Reference: <author> Cohen, M., Franco, H., Morgan, N., Rumelhart, D. & Abrash, V. </author> <year> (1992), </year> <title> Context-Dependent Multiple Distribution Phonetic Modeling with MLPs, </title> <booktitle> in `NIPS 5'. </booktitle>
Reference: <author> Hochberg, M., Cook, G., Renals, S. & Robinson, A. </author> <year> (1994), </year> <title> Connectionist Model Combination for Large Vocabulary Speech Recognition, in `Neural Networks for Signal Processing', </title> <booktitle> Vol. IV, </booktitle> <pages> pp. 269-278. </pages>
Reference-contexts: The American English context-dependent system (CD527) was extended to include a set of modules trained backwards in time (which were log-merged with the forward context), to augment a four way log-merged context-independent system <ref> (Hochberg, Cook, Renals & Robinson 1994) </ref>.
Reference: <author> Hochberg, M., Cook, G., Renals, S., Robinson, A. & Schechtman, R. </author> <year> (1995), </year> <title> The 1994 ABBOT Hybrid Connectionist-HMM Large-Vocabulary Recognition System, </title> <booktitle> in `Spoken Language Systems Technology Workshop', ARPA, </booktitle> <pages> pp. 170-6. </pages>
Reference-contexts: INTRODUCTION The abbot hybrid connectionist-HMM system performed competitively with many conventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations of speech recognition systems <ref> (Hochberg, Cook, Renals, Robinson & Schechtman 1995) </ref>. This hybrid framework is attractive because it is compact, having far fewer parameters than conventional HMM systems, whilst also providing the discriminative powers of a connectionist architecture. It is well established that particular phones vary acoustically when they occur in different phonetic contexts. <p> Hence, the network outputs are mapped to scaled likelihoods by, p (u (t)jq i (t)) ' Pr (q i ) where the priors Pr (q i ) are estimated from the training data. Decoding uses the noway decoder <ref> (Renals & Hochberg 1995) </ref> to compute the utterance model that is most likely to have generated the observed speech signal. CONTEXT-DEPENDENT PROBABILITY ESTIMATION The approach taken by this work is to augment the CI RNN, in a similar vein to Bourlard & Morgan (1993).
Reference: <author> Jordan, M. & Jacobs, R. </author> <year> (1994), </year> <title> `Hierarchical Mixtures of Experts and the EM Algorithm', </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214. </pages>
Reference-contexts: This system performs well on the DARPA Resource Management Task. The work presented in Zhoa, Schwartz, Sroka & Makhoul (1995) followed along similar work to Cohen et al. (1992). A context-dependent mixture of experts (ME) system <ref> (Jordan & Jacobs 1994) </ref> based on the structure of the context-independent ME was built. For each state, the whole training data was divided into 46 parts according to its left or right context. Then, a separate ME model was built for each context. <p> The RNN posterior probability outputs are multiplied by the module outputs to form context-dependent posterior probability estimates. RELATIONSHIP WITH MIXTURE OF EXPERTS This architecture has similarities with mixture of experts <ref> (Jordan & Jacobs 1994) </ref>. During training, rather than making a "soft" split of the data as in the mixture of experts case, the Viterbi segmentation selects one expert at every exemplar. This means only one expert is responsible for each example in the data.
Reference: <author> Kershaw, D., Hochberg, M. & Robinson, A. </author> <year> (1995), </year> <title> Incorporating Context-Dependent Classes in a Hybrid Recurrent Network-HMM Speech Recognition System, </title> <institution> F-INFENG TR217, Cambridge University Engineering Department. </institution>
Reference: <author> Lee, K.-F. </author> <year> (1989), </year> <title> Automatic Speech Recognition; The Development of the SPHINX System, </title> <publisher> Kluwer Acedemic Publishers. </publisher>
Reference-contexts: Tel: [+1] 415 6148260. handled in HMMs by creating a model for all sufficiently differing phonetic con-texts with enough acoustic evidence. This modelling of phones in their particular phonetic contexts produces sharper probability density functions. This approach vastly improves HMM recognition accuracy over equivalent context-independent systems <ref> (Lee 1989) </ref>. Although the recurrent neural network (RNN) model acoustic context internally (within the state vector), it does not model phonetic context. This paper presents an approach to improving the abbot system through phonetic context-dependent modelling.
Reference: <author> Renals, S. & Hochberg, M. </author> <year> (1995), </year> <title> Efficient Search Using Posterior Phone Probability Estimates, </title> <booktitle> in `ICASSP', </booktitle> <volume> Vol. 1, </volume> <pages> pp. 596-9. </pages>
Reference-contexts: INTRODUCTION The abbot hybrid connectionist-HMM system performed competitively with many conventional hidden Markov model (HMM) systems in the 1994 ARPA evaluations of speech recognition systems <ref> (Hochberg, Cook, Renals, Robinson & Schechtman 1995) </ref>. This hybrid framework is attractive because it is compact, having far fewer parameters than conventional HMM systems, whilst also providing the discriminative powers of a connectionist architecture. It is well established that particular phones vary acoustically when they occur in different phonetic contexts. <p> Hence, the network outputs are mapped to scaled likelihoods by, p (u (t)jq i (t)) ' Pr (q i ) where the priors Pr (q i ) are estimated from the training data. Decoding uses the noway decoder <ref> (Renals & Hochberg 1995) </ref> to compute the utterance model that is most likely to have generated the observed speech signal. CONTEXT-DEPENDENT PROBABILITY ESTIMATION The approach taken by this work is to augment the CI RNN, in a similar vein to Bourlard & Morgan (1993).
Reference: <author> Robinson, A. </author> <year> (1994), </year> <title> `An Application of Recurrent Nets to Phone Probability Estimation.', </title> <journal> IEEE Transactions on Neural Networks 5(2), </journal> <pages> 298-305. </pages>
Reference-contexts: Right (future) acoustic context is given by delaying the posterior probability estimation until four frames of input have been seen by the network. The network is trained using a modified version of error back-propagation through time <ref> (Robinson 1994) </ref>. Decoding with the hybrid connectionist-HMM approach is equivalent to conventional HMM decoding, with the difference being that the RNN models the state observations. Like typical HMM systems, the recognition process is expressed as finding the maximum a posteriori state sequence for the utterance. <p> Each context network is trained on a non-overlapping subset of the state vectors generated from all the Viterbi aligned training data. The context networks were trained using the RProp training procedure <ref> (Robinson 1994) </ref>. The frame-by-frame phonetic context posterior probabilities are required as input to the noway decoder, i.e. all the outputs from the context modules on the right hand side of Figure 1. These posterior probabilities are calculated from the numerator of (7). <p> The American English context-dependent system (CD527) was extended to include a set of modules trained backwards in time (which were log-merged with the forward context), to augment a four way log-merged context-independent system <ref> (Hochberg, Cook, Renals & Robinson 1994) </ref>.
Reference: <author> Young, S., Odell, J. & Woodland, P. </author> <year> (1994), </year> <title> `Tree-Based State Tying for High Accuracy Acoustic Modelling', </title> <booktitle> Spoken Language Systems Technology Workshop. </booktitle>
Reference: <author> Zhoa, Y., Schwartz, R., Sroka, J. & Makhoul, J. </author> <year> (1995), </year> <title> Hierarchical Mixtures of Experts Methodology Applied to Continuous Speech Recognition, </title> <booktitle> in `NIPS 7'. </booktitle>
References-found: 12

