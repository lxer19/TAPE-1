URL: http://www.research.att.com/~wcohen/postscript/ml-90-axaebl.ps
Refering-URL: http://www.research.att.com/~wcohen/
Root-URL: 
Title: Learning Approximate Control Rules Of High Utility  
Author: William W. Cohen 
Address: New Brunswick, NJ 08903  
Affiliation: Computer Science Department Rutgers University  
Abstract: One of the difficult problems in the area of explanation based learning is the utility problem; learning too many rules of low utility can lead to swamping, or degradation of performance. This paper introduces two new techniques for improving the utility of learned rules. The first technique is to combine EBL with inductive learning techniques to learn a better set of control rules; the second technique is to use these inductive techniques to learn approximate control rules. The two techniques are synthesized in an algorithm called approximating abductive explanation based learning (AxA-EBL). AxA-EBL is shown to improve substantially over standard EBL in several domains.
Abstract-found: 1
Intro-found: 1
Reference: [ Chase et al., 1989 ] <author> Melissa Chase, Monte Zweben, Richard Piazza, John Burger, Paul Maglio, and Haym Hirsh. </author> <title> Approximating learned search control knowledge. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, New York, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The search techniques described by these authors improve on Keller's by taking advantage of a partial order on approximate theories; however, they have not been applied to the problem of learning control rules. Approximations to EBL rules have also been investigated in <ref> [ Chase et al., 1989 ] </ref> in the context of the ULS system. The approximations done in ULS are not, however, selected by an inductive learning mechanism; as a consequence, ULS is limited to conservative approximations (e.g., dropping one or two conditions).
Reference: [ Chien, 1989 ] <author> Steve Chien. </author> <title> Using and refining simplifications: Explanation-based learning of plans in intractible domains. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Michigan, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This would be prohibitively expensive in a real-world domain. Ellman [ Ellman, 1988 ] , Tadepalli [ Tadepalli, 1989 ] and Chien <ref> [ Chien, 1989 ] </ref> have also investigated explanation based learning of approximate theories. The search techniques described by these authors improve on Keller's by taking advantage of a partial order on approximate theories; however, they have not been applied to the problem of learning control rules.
Reference: [ Cohen, 1989 ] <author> William W. Cohen. </author> <title> Abductive explanation based learning: A solution to the multiple explanation problem. </title> <type> Technical Report ML-TR-26, </type> <institution> Rutgers University, </institution> <year> 1989. </year>
Reference-contexts: The greedy set cover technique has also been used in Haussler's algorithm for learning disjunctive boolean formulae. A-EBL has been used with some success on inductive learning tasks <ref> [ Cohen, 1989; Cohen, 1990 ] </ref> . Its main advantage over standard EBL techniques for such tasks is that it can be used even on an "abductive" domain theory | one which generates multiple inconsistent explanations. <p> shown to satisfy Valiant's criterion of pac-learnability, and to have 2 Experimental studies [ Shavlik, 1987 ] suggest that this ordering is most beneficial. 3 The size of a rule is defined to be the number of nodes in the explanation structure used to form the rule. near-optimal sample complexity <ref> [ Cohen, 1989 ] </ref> . Approximating abductive EBL (AxA-EBL): Approximating abductive EBL works exactly like A-EBL, except that the candidate pool consists of all k-bounded approximations to a rule generated by applying EBG to some correct control decision.
Reference: [ Cohen, 1990 ] <author> William W. Cohen. </author> <title> Learning from textbook knowledge: A case study. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, Massachusetts, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The greedy set cover technique has also been used in Haussler's algorithm for learning disjunctive boolean formulae. A-EBL has been used with some success on inductive learning tasks <ref> [ Cohen, 1989; Cohen, 1990 ] </ref> . Its main advantage over standard EBL techniques for such tasks is that it can be used even on an "abductive" domain theory | one which generates multiple inconsistent explanations.
Reference: [ Ellman, 1988 ] <author> Tom Ellman. </author> <title> Approximate theory formation: An explanation-based approach. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <address> Saint Paul, Minnesota, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This would be prohibitively expensive in a real-world domain. Ellman <ref> [ Ellman, 1988 ] </ref> , Tadepalli [ Tadepalli, 1989 ] and Chien [ Chien, 1989 ] have also investigated explanation based learning of approximate theories.
Reference: [ Fikes et al., 1972 ] <author> Richard Fikes, Peter Hart, and Nils Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3 </volume> <pages> 251-288, </pages> <year> 1972. </year>
Reference-contexts: RW: The STRIPS robot world of <ref> [ Fikes et al., 1972 ] </ref> , with a means-ends analysis planner.
Reference: [ Flann and Dietterich, 1989 ] <author> Nicholas Flann and Thomas Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4(2), </volume> <year> 1989. </year>
Reference-contexts: Combining EBL with inductive learning. Flann and Dietterich have noted that in the process of learning control rules, many EBL systems make inductive leaps. An example given in <ref> [ Flann and Dietterich, 1989 ] </ref> is a rule learned by LEX2 in the symbolic integration domain: If the current problem matches R cx r dx (r 6= 1) Then use the operator R cf (x)dx ) c f (x)dx Forming this rule is an inductive leap; this is witnessed by <p> Usually, several control rules, interpreted disjunctively, will be needed to determine when such a clause is useful. A-EBL and AxA-EBL differ in these respects from other inductive extensions to EBL, in particular mEBG and IOE <ref> [ Flann and Dietterich, 1989 ] </ref> . 4 Experimental results 4.1 Description of the domains Experimentation has been done with these three learning algorithms in several domains. The domains are summarized in Table 1. LEX: A simplified version of symbolic integration using state-space search with iterative deepening.
Reference: [ Keller, 1987 ] <author> Richard Keller. </author> <title> The role of explicit contextual knowledge in learning concepts to improve performance. </title> <type> Technical Report ML-TR-7, </type> <institution> Rutgers University, </institution> <year> 1987. </year>
Reference-contexts: of domains used in experiments Domain Description Search Number of Training Set Test Set Strategy Operators Sizes Size LEX simplified LEX state-space 36 4,8,12,16,19 13 RW STRIPS robot world means-ends 10 5,15,25,45,80,120 50 BW Nilsson blocks world means-ends 4 5,15,25,45,80,120 100 GRID graph search depth-first 3 5,10,15,25 100 taken from <ref> [ Keller, 1987 ] </ref> . RW: The STRIPS robot world of [ Fikes et al., 1972 ] , with a means-ends analysis planner. <p> In each experiment, first a test set, of the size indicated in table 1, was selected. In the case of BW, RW and GRID, the test problems were randomly selected; in LEX, the test problems were the designated test problems in <ref> [ Keller, 1987 ] </ref> (problem sets AT and BT.) Then a training set was selected, again randomly for BW, RW and GRID, and from the designated training problems (problems sets A and B) for LEX. <p> performance tradeoff: if matching rules is cheap and problem solving is expensive, as in the RW domain, then faster convergence will offset the poorer quality of the rules, and A-EBL will produce faster programs. 5 Related work Use of approximations in learning control knowledge was investigated in the MetaLEX project <ref> [ Keller, 1987 ] </ref> . The techniques used in MetaLEX, however, were specific to state-space search, and required periodic testing of programs including learned control 5 This is obvious in the blocks world domain, since AxA-EBL's control rules outperform A-EBL's control rules even though their coverage is usually worse.
Reference: [ Markovitch and Scott, 1989 ] <author> Shaul Markovitch and Paul Scott. </author> <title> Utilization filtering: A method for reducing the inherit harmfulness of deductively learned knowledge. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Michigan, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning too many rules of low utility can lead to swamping, or degradation of performance: in the worst case, a problem solver can be slower after learning than before learning. Previous research on the utility problem has focused on detecting and discarding rules of low utility <ref> [ Minton, 1988; Markovitch and Scott, 1989 ] </ref> , lowering the match cost of rules using partial evaluation or other simplification techniques [ Prieditis and Mostow, 1987; Minton, 1988; Tambe and Rosenbloom, 1989 ] , and constraining the use of learned rules [ Mooney, 1989 ] .
Reference: [ Minton, 1988 ] <author> Steven Minton. </author> <title> Learning effective search control knowledge: An explanation-based approach. </title> <type> Technical report, </type> <institution> Carnegie-Mellon University Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: Learning too many rules of low utility can lead to swamping, or degradation of performance: in the worst case, a problem solver can be slower after learning than before learning. Previous research on the utility problem has focused on detecting and discarding rules of low utility <ref> [ Minton, 1988; Markovitch and Scott, 1989 ] </ref> , lowering the match cost of rules using partial evaluation or other simplification techniques [ Prieditis and Mostow, 1987; Minton, 1988; Tambe and Rosenbloom, 1989 ] , and constraining the use of learned rules [ Mooney, 1989 ] . <p> Another case in which inductive decisions are made is when only one of several possible control rules is learned; for example, PRODIGY might learn either a macro-operator or a set of operator preference rules from a trace <ref> [ Minton, 1988 ] </ref> . However, the standard implementation of EBL, in which control rules are learned incrementally as needed, is relatively slow as an inductive learner. Combining EBL with more powerful inductive learning techniques can improve the rate at which learning converges to an adequate set of control rules. <p> RW: The STRIPS robot world of [ Fikes et al., 1972 ] , with a means-ends analysis planner. Problems for the training and test set are generated by selecting a floor-plan randomly from the three given in <ref> [ Minton, 1988 ] </ref> , randomly placing 3 blocks and the robot, and then taking a random walk of bounded length in state space. BW: The blocks world of [ Nilsson, 1987 ] , with the identical means-ends analysis planner. <p> The domains are listed roughly in order of their difficulty for standard EBL techniques. For instance, symbolic integration is well suited to use of EBL for operator selection; however, unconstrained use of EBL in the blocks world domain can lead to swamping <ref> [ Minton, 1988; Mooney, 1989 ] </ref> . <p> Graph searching is a very difficult problem for standard EBL techniques because the match cost of learned rules is very high, and the cost of problem solving without any learned rules is low; matching the rules learned by EBL is equivalent to solving the NP-complete subgraph isomorphism problem <ref> [ Minton, 1988 ] </ref> , whereas depth-first search only requires time linear in the size of the graph. 4.2 Comparison of EBL and AxA-EBL A series of experiments were designed to determine how the performance of a learned program varied as a function of the number of training examples used in
Reference: [ Mitchell, 1982 ] <author> Tom Mitchell. </author> <title> Toward combining empirical and analytical methods for inferring heuristics. </title> <booktitle> In Human and Artificial Intelligence. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <year> 1982. </year>
Reference-contexts: This `usefulness theory" is an extension to the problem of Prolog clause selection of the theory used in the LEX/2 system <ref> [ Mitchell, 1982 ] </ref> for operator selection. The second routine is a control rule compiler which, given a set of control rules and an initial program, generates a new Prolog program which incorporates the learned control rules.
Reference: [ Mooney, 1989 ] <author> R. J. Mooney. </author> <title> The effect of rule use on the utility of explanation based learning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Michigan, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: on detecting and discarding rules of low utility [ Minton, 1988; Markovitch and Scott, 1989 ] , lowering the match cost of rules using partial evaluation or other simplification techniques [ Prieditis and Mostow, 1987; Minton, 1988; Tambe and Rosenbloom, 1989 ] , and constraining the use of learned rules <ref> [ Mooney, 1989 ] </ref> . This paper introduces two new techniques for improving the utility of learned rules. The first technique is to combine EBL with inductive learning techniques to learn a better set of control rules. <p> The domains are listed roughly in order of their difficulty for standard EBL techniques. For instance, symbolic integration is well suited to use of EBL for operator selection; however, unconstrained use of EBL in the blocks world domain can lead to swamping <ref> [ Minton, 1988; Mooney, 1989 ] </ref> .
Reference: [ Nilsson, 1987 ] <author> Nils Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: Problems for the training and test set are generated by selecting a floor-plan randomly from the three given in [ Minton, 1988 ] , randomly placing 3 blocks and the robot, and then taking a random walk of bounded length in state space. BW: The blocks world of <ref> [ Nilsson, 1987 ] </ref> , with the identical means-ends analysis planner. Problems are generated by constructing an initial scene with 20 randomly placed blocks, and then taking a random walk of bounded length in state space. GRID: Depth-bounded depth-first search of an artificial graph.
Reference: [ Prieditis and Mostow, 1987 ] <author> Armand Prieditis and Jack Mostow. PROLEARN: </author> <title> Towards a prolog interpreter that learns. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, Washington, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Shavlik, 1987 ] <author> Jude Shavlik. </author> <title> Generalizing number in explanation based learning. </title> <type> Technical Report UILO-ENG-87-2276, </type> <institution> Univ. of Illinois/Champaign, </institution> <year> 1987. </year> <type> (PhD thesis). </type>
Reference-contexts: Its main advantage over standard EBL techniques for such tasks is that it can be used even on an "abductive" domain theory | one which generates multiple inconsistent explanations. A-EBL has been shown to satisfy Valiant's criterion of pac-learnability, and to have 2 Experimental studies <ref> [ Shavlik, 1987 ] </ref> suggest that this ordering is most beneficial. 3 The size of a rule is defined to be the number of nodes in the explanation structure used to form the rule. near-optimal sample complexity [ Cohen, 1989 ] .
Reference: [ Tadepalli, 1989 ] <author> Prasad Tadepalli. </author> <title> Lazy explanation-based learning: A solution to the intractible theory problem. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Michigan, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This would be prohibitively expensive in a real-world domain. Ellman [ Ellman, 1988 ] , Tadepalli <ref> [ Tadepalli, 1989 ] </ref> and Chien [ Chien, 1989 ] have also investigated explanation based learning of approximate theories.
Reference: [ Tambe and Rosenbloom, 1989 ] <author> Milinde Tambe and Paul Rosenbloom. </author> <title> Eliminating expensive chunks by restricting expressiveness. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, Michigan, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: ULS nonetheless has made modest improvements in planning time in the RW domain. The use of k-bounded approximations is one way of bounding the cost of learned rules. An alternative approach is described by Tambe and Rosenbloom in <ref> [ Tambe and Rosenbloom, 1989 ] </ref> ; they describe a syntactic constraint on SOAR programs which ensures that chunks have match cost linear in their length.
References-found: 17

