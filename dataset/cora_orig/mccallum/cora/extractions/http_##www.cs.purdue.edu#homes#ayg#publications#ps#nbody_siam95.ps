URL: http://www.cs.purdue.edu/homes/ayg/publications/ps/nbody_siam95.ps
Refering-URL: http://www.cs.purdue.edu/homes/ayg/publications/work.html
Root-URL: http://www.cs.purdue.edu
Title: n-Body Simulations Using Message Passing Parallel Computers tween nodes and particles are dictated by a
Author: Ananth Y. Grama Vipin Kumar Ahmed Sameh Appel, Barnes and Hut, and Greengard and Rokhlin Singh et al. [] and Warren and Salmon [, ] 
Keyword: 1 n-Body Simulation Techniques  
Address: Minneapolis MN.  Minneapolis MN.  Minneapolis MN.  
Affiliation: Department of Computer Science, University of Minnesota,  Department of Computer Science, University of Minnesota,  Department of Computer Science, University of Minnesota,  
Note: SIAM Conference on Parallel Processing, San Francisco,  This work was supported by IST/BMDO through Army Research Office contract DA/DAAH04-93-G-0080, and by the ARO under contract DAAL03-89-C-0038 with the University of Minnesota Army High Performance Computing Research Center; Access to the nCUBE2 at Sandia National Lab was provided via DeSRA consortium.  in this class include those due to  2 Parallel Formulations of Barnes-Hut  
Email: (ananth@cs.umn.edu)  (kumar@cs.umn.edu)  (sameh@cs.umn.edu)  
Date: 1994  
Abstract: In this paper, we present new parallel formulations of the Barnes-Hut method for n-body simulations on message passing computers. These parallel formulations partition the domain efficiently incurring minimal communication overhead. This is in contrast to existing schemes that are based on sorting a large number of keys or on the use of global data structures. The new formulations are augmented by alternate communication strategies which serve to minimize communication overhead. The impact of these communication strategies is experimentally studied. We report on experimental results obtained from an astrophysical simulation on an nCUBE2 parallel computer. The n-body problem simulates the behavior of n particles, each of which is influenced by every other particle during a time-step. An exact formulation of this problem therefore requires calculation of n 2 interactions between each pair of particles. Many approximate algorithms have been devised that reduce the complexity of this problem. Most of these algorithms are based on a hierarchical representation of the domain using a spatial tree data structure. The leaf nodes consist of aggregates of particles. Each node in the tree contains a series representation of the effect of the particles contained in the subtree rooted at the node. These representations are typically based on Taylor or Legendre polynomials. Interactions be The Barnes-Hut method is one of the most popular methods due to its simplicity. Although its computational complexity of O(n log n) is more than that of the Fast Multipole Method (FMM), which is O(n)[4], the associated constants are smaller for the Barnes-Hut method particularly for simulations in three dimensions. Furthermore, FMM uses a number of complicated data structures that make it difficult to program. However, FMM has proven error bounds unlike the Barnes-Hut method. Warren and Salmon [9] present a variant of the Barnes-Hut method that has a good worst-case error bound. In this paper, we describe parallel formulations of the Barnes-Hut method. The parallel formulations presented in this paper can also be applied to FMM with some modifications because the issues in parallelizing either of these algorithms are very similar. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> J. A. Board, J. W. Causey, J. F. Leathrum, A. Windemuth, and K. Schulten. </author> <title> Accelerated molecular dynamics with the fast multipole algorithm. </title> <institution> Chem. Phys. Let., 198:89, </institution> <year> 1992. </year>
Reference: [2] <author> Ananth Grama, Vipin Kumar, and Ahmad Sameh. </author> <title> Scalable parallel formulations of the barnes-hut method for n-body simulations. </title> <booktitle> In Supercomputing '94 Proceedings, </booktitle> <year> 1994. </year>
Reference: [3] <author> L. Greengard and W. Gropp. </author> <title> A parallel version of the fast multipole method. </title> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 213-222, </pages> <year> 1987. </year>
Reference: [4] <author> L. Greengard and V. Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> J. Comp. Physics, </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year>
Reference: [5] <author> J. F. Leathrum and J. A. </author> <title> Board. Mapping the adaptive fast multipole algorithm into mimd systems. </title> <editor> In P. Mehrotra and J. Saltz, editors, </editor> <title> n-Body Simulations Using Message Passing Parallel Computers 5 Unstructured Scientific Computation on Scalable Multiprocessors. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference: [6] <author> K. E. Schmidt and M. A. Lee. </author> <title> Implementing the fast multipole method in three dimensions. </title> <journal> J. Stat. Phys., </journal> <volume> 63:1120, </volume> <year> 1991. </year>
Reference: [7] <author> J. Singh, C. Holt, T. Totsuka, A. Gupta, and J. Hennessy. </author> <title> Load balancing and data locality in hierarchical n-body methods. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994 (to appear). </note>
Reference: [8] <author> M. Warren and J. Salmon. </author> <title> Astrophysical n-body simulations using hierarchical tree data structures. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <year> 1992. </year>
Reference: [9] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct tree n-body algorithm. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <year> 1993. </year>
Reference: [10] <author> F. Zhao and S. L. Johnsson. </author> <title> The parallel multi-pole method on the connection machine. </title> <journal> SIAM J. of Sci. Stat. Comp., </journal> <volume> 12 </volume> <pages> 1420-1437, </pages> <year> 1991. </year>
References-found: 10

