URL: http://www.cs.umn.edu/Users/dept/users/kumar/frontiers95.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: A High Performance Sparse Cholesky Factorization Algorithm For Scalable Parallel Computers  
Author: GEORGE KARYPIS AND VIPIN KUMAR 
Address: MINNEAPOLIS, MN 55455  
Affiliation: DEPARTMENT OF COMPUTER SCIENCE, UNIVERSITY OF MINNESOTA,  
Note: Appears in Frontiers 95  
Abstract: This paper presents a new parallel algorithm for sparse matrix factorization. This algorithm uses subforest-to-subcube mapping instead of the subtree-to-subcube mapping of another recently introduced scheme by Gupta and Kumar [10]. Asymptotically, both formulations are equally scalable on a wide range of architectures and a wide variety of problems. But the subtree-to-subcube mapping of the earlier formulation causes significant load imbalance among processors, limiting overall efficiency and speedup. The new mapping largely eliminates the load imbalance among processors. Furthermore, the algorithm has a number of enhancements to improve the overall performance substantially. This new algorithm achieves up to 20GFlops on a 1024-processor Cray T3D for moderately large problems. To our knowledge, this is the highest performance ever obtained on an MPP for sparse Cholesky factorization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cleve Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference: [2] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> (9):302-325, 1983. 
Reference-contexts: This reduces the communication overhead and improves the isoefficiency to O. p 1:5 log p/. Gupta and Kumar [10] recently developed a parallel formulation of sparse Cholesky factorization based on the multifrontal method. The multifrontal method <ref> [2, 18] </ref> is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices.
Reference: [3] <author> Kalluri Eswar, Ponnuswamy Sadayappan, and V. Visvanathan. </author> <title> Supernodal Sparse Cholesky factorization on distributed-memory multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 18-22 (vol. 3), </pages> <year> 1993. </year>
Reference: [4] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 17, 20] </ref>.
Reference: [5] <author> D. M. Gay. </author> <title> Electronic Mail Distribution of Linear Programming Test Problems. </title> <journal> &lt;mathematical Programming Society COAL Newsletter, </journal> <month> December </month> <year> 1985. </year>
Reference-contexts: However, very often such local improvements do not result in improving the overall load imbalance. For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB <ref> [5] </ref>, even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% [10, 15]. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that.
Reference: [6] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky Factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: A parallel formulation for sparse matrix factorization can be easily obtained by simply distributing rows to different processors <ref> [6] </ref>. Due to the sparsity of the matrix, communication overhead is a large fraction of the computation for this method, resulting in poor scalability.
Reference: [7] <author> A. George and J. W.-H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [30], therefore a number of heuristic algorithms for ordering have been developed. In particular, the minimum degree ordering <ref> [7, 11] </ref> is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix.
Reference: [8] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication Results for Parallel Sparse Cholesky Factorization on a Hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10(3) </volume> <pages> 287-298, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In a smarter parallel formu fl This work was supported by Army Research Office under contract # DA/DAAH04-93-G-0080 and by the University of Minnesota Army High Performance Computing Research Center under contract # DAAL03-89-C-0038. Related papers are available on World Wide Web accessible via Mosaic URL: http://ftp.cs.umn.edu/users/kumar/papers.html lation <ref> [8] </ref>, the rows of the matrix are allocated to processors using the subtree-to-subcube mapping. This localizes the communication among groups of processors, and thus improves the isoefficiency of the scheme to O. p 3 /. Roth-berg and Gupta [26, 27] used a different method to reduce the communication overhead. <p> Any elimination tree of arbitrary shape can be converted to a binary tree using a simple tree restructuring algorithm described in [10]. In this scheme, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy <ref> [8, 11] </ref> illustrated in Figure 1. With subtree-to-subcube assignment, all p processors in the system cooperate to factor the frontal matrix associated with the root node of the elimination tree. The two subtrees of the root node are assigned to subcubes of p=2 processors each.
Reference: [9] <author> John R. Gilbert and Robert Schreiber. </author> <title> Highly Parallel Sparse Cholesky Factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year>
Reference: [10] <author> Anshul Gupta and Vipin Kumar. </author> <title> A scalable parallel algorithm for sparse matrix factorization. </title> <type> TR 94-19, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> A shorter version appears in Supercomputing '94. TR available in users/kumar/sparse-cholesky.ps at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: Roth-berg and Gupta [26, 27] used a different method to reduce the communication overhead. In their method, the entire sparse matrix is partitioned among processors using a two-dimensional block cyclic mapping. This reduces the communication overhead and improves the isoefficiency to O. p 1:5 log p/. Gupta and Kumar <ref> [10] </ref> recently developed a parallel formulation of sparse Cholesky factorization based on the multifrontal method. The multifrontal method [2, 18] is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices. <p> For larger problems, even higher performance can be achieved). To our knowledge, this is the highest performance ever obtained on an MPP for sparse Cholesky factorization. Our new scheme, like the scheme of Gupta and Kumar <ref> [10] </ref>, has an asymptotic isoefficiency of O. p 1:5 / for matrices arising out of two- and three-dimensional finite element problems on a wide variety of architectures such as hypercube, mesh, fat tree, and three-dimensional torus. 2 Cholesky Factorization Consider a system of linear equations Ax D b where A is <p> For a more detailed description the reader should refer to <ref> [10] </ref>. Consider a p-processor hypercube-connected computer. Let A be the n fi n matrix to be factored, and let T be its supernodal elimination tree. The algorithm requires the elimination tree to be binary for the first log p levels. <p> The algorithm requires the elimination tree to be binary for the first log p levels. Any elimination tree of arbitrary shape can be converted to a binary tree using a simple tree restructuring algorithm described in <ref> [10] </ref>. In this scheme, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy [8, 11] illustrated in Figure 1. With subtree-to-subcube assignment, all p processors in the system cooperate to factor the frontal matrix associated with the root node of the elimination tree. <p> Each processor participates in log p distributed extend-add operations, in which the update matrices from the factorization at level l are redistributed to perform the extend-add operation at level l 1 prior to factoring the frontal matrix. In the algorithm proposed in <ref> [10] </ref>, each processor exchanges data with only one other processor during each one of these log p distributed extend-adds. The above is achieved by a careful embedding of the processor grids on the hypercube, and by carefully mapping rows and columns of each frontal matrix onto this grid. <p> This mapping is described in [16]. 4 The New Algorithm As mentioned in the introduction, the subtree-to-subcube mapping scheme used in <ref> [10] </ref> does not distribute the work equally among the processors. This load imbalance puts an upper bound on the achievable efficiency. For example, consider the supernodal elimination tree shown in Figure 2. This elimination tree is partitioned among 8 processors using the subtree-to-subcube allocation scheme. <p> For elimination trees of general sparse matrices, the load imbalance can be usually decreased by performing some simple elimination tree reorderings described in <ref> [10] </ref>. However, these techniques have two serious limitations. First, they increase the fill-in as they try to balance the elimina (i.e., supernode) is labeled by the range of nodes belonging to it. <p> For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB [5], even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% <ref> [10, 15] </ref>. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that. In the rest of this section we present a modification to the algorithm presented in Section 3 that uses a different scheme for mapping the elimination tree onto the processors.
Reference: [11] <author> M. T. Heath, E. Ng, and B. W. Payton. </author> <title> Parallel Algorithms for Sparse Linear Systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [30], therefore a number of heuristic algorithms for ordering have been developed. In particular, the minimum degree ordering <ref> [7, 11] </ref> is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix. <p> Any elimination tree of arbitrary shape can be converted to a binary tree using a simple tree restructuring algorithm described in [10]. In this scheme, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy <ref> [8, 11] </ref> illustrated in Figure 1. With subtree-to-subcube assignment, all p processors in the system cooperate to factor the frontal matrix associated with the root node of the elimination tree. The two subtrees of the root node are assigned to subcubes of p=2 processors each.
Reference: [12] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [4, 17, 20]. However, despite inherent parallelism in sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [12, 28] </ref>, and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers.
Reference: [13] <author> M. T. Heath and P. Raghavan. </author> <title> A Cartesian nested dissection algorithm. </title> <type> TR UIUCDCS-R-92-1772, </type> <institution> Department of Computer Science, University of Illi-nois, Urbana, </institution> <address> IL 61801, </address> <month> October </month> <year> 1992. </year> <note> to appear in SIMAX. </note>
Reference-contexts: With highly parallel formulation available, the factorization step is no longer the most time consuming step in the solution of sparse systems of equations. Another step that is quite time consuming, and thus needs to be parallelized effectively is that of ordering <ref> [13] </ref>. In our current research we are investigating ordering algorithms that are of high quality and can be implemented fast on parallel computers.
Reference: [14] <author> M. T. Heath and P. Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> TR 93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference: [15] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> A Parallel Formulation of Interior Point Algorithms. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year> <note> TR available in users/kumar/interior-point.ps at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB [5], even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% <ref> [10, 15] </ref>. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that. In the rest of this section we present a modification to the algorithm presented in Section 3 that uses a different scheme for mapping the elimination tree onto the processors.
Reference: [16] <author> George Karypis and Vipin Kumar. </author> <title> A High Performance Sparse Cholesky Factorization Algorithm For Scalabale Parallel Computers. </title> <type> TR 94-41, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> TR available in users/kumar/cholesky-forest.ps at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: Spectral nested dissection [22, 23] has been found to generate orderings that have both low fill-in and good parallelism. For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings on the performance of our algorithm refer to <ref> [16] </ref>. In the multifrontal method for Cholesky factorization, a frontal matrix F k and an update matrix U k is associated with each node k of the elimination tree. The rows and columns of F k corresponds to t C1 indices of L in increasing order. <p> The remaining t fi t matrix is called the update matrix U k and is passed on to the parent of k in the elimination tree. Since matrices are symmetric, only the upper triangular part is stored. For further details on the multifrontal method, the reader should refer to <ref> [16] </ref>, and to the excellent tutorial by Liu [18]. If some consecutively numbered nodes form a chain in the elimination tree, and the corresponding rows of L have identical nonzero structure, then this chain is called a supernode. <p> The above is achieved by a careful embedding of the processor grids on the hypercube, and by carefully mapping rows and columns of each frontal matrix onto this grid. This mapping is described in <ref> [16] </ref>. 4 The New Algorithm As mentioned in the introduction, the subtree-to-subcube mapping scheme used in [10] does not distribute the work equally among the processors. This load imbalance puts an upper bound on the achievable efficiency. For example, consider the supernodal elimination tree shown in Figure 2. <p> In the rest of this section we briefly describe some modifications. For a more detailed description of these and other enhancements the reader should refer to <ref> [16] </ref>. For the factorization of a supernode, we use the pipelined variant of the grid-based dense Cholesky algorithm [17]. In this algorithm, successive rows of the frontal matrix are factored one after the other, and the communication and computation proceeds in a pipelined fashion. <p> The rank-b update can now be implemented using matrix-matrix multiply, leading to a higher computational rate. A number of design issues involved in using block cyclic mapping and ways to further improve the performance are described in <ref> [16] </ref>. 6 Experimental Results We implemented our new parallel sparse multifrontal algorithm on a 1024-processor Cray T3D parallel computer. Each processor on the T3D is a 150Mhz Dec Alpha chip, with peak performance of 150MFlops for 64-bit operations (double precision). <p> This trend is more pronounced for three-dimensional problems, because they tend to have fairly shallow trees. The cost of the distributed extend-add phase decreases almost linearly as the number of processors increases. This is consistent with the analysis presented in <ref> [16] </ref>, since the overhead of distributed extend-add is O..n log p/= p/. Since the figure for the time spent during the extend-add steps also includes the idling due to load imbalance, the almost linear decrease also shows that the load imbalance is quite small.
Reference: [17] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Ben-jamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 17, 20] </ref>. <p> Note that the isoefficiency of the best known parallel formulation of dense matrix factorization is also O. p 1:5 / <ref> [17] </ref>. On a variety of problems, Gupta and Kumar report speedup of up to 364 on a 1024-processor nCUBE 2, which is a major improvement over the previ ously existing algorithms. <p> If each of the matrices is factored by all the processors, then the total communication time for factoring the two matrices is n 2 = p p <ref> [17] </ref>. If A and B are factored concurrently by p=2 processors each, then the communica tion time is n 2 =.2 p p=2/ which is smaller. <p> In the rest of this section we briefly describe some modifications. For a more detailed description of these and other enhancements the reader should refer to [16]. For the factorization of a supernode, we use the pipelined variant of the grid-based dense Cholesky algorithm <ref> [17] </ref>. In this algorithm, successive rows of the frontal matrix are factored one after the other, and the communication and computation proceeds in a pipelined fashion. Even though this scheme is simple, it has two major limitations. <p> Hence, level three BLAS operations can better exploit the multiple functional units, and deep pipelines available in these processors. However, by distributing the frontal matrices using a block cyclic mapping <ref> [17] </ref>, we are able to eliminate both of the above limitations and greatly improve the performance of our algorithm.
Reference: [18] <author> Joseph W. H. Liu. </author> <title> The Multifrontal Method for Sparse Matrix Solution: </title> <journal> Theory and Practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: This reduces the communication overhead and improves the isoefficiency to O. p 1:5 log p/. Gupta and Kumar [10] recently developed a parallel formulation of sparse Cholesky factorization based on the multifrontal method. The multifrontal method <ref> [2, 18] </ref> is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices. <p> Since matrices are symmetric, only the upper triangular part is stored. For further details on the multifrontal method, the reader should refer to [16], and to the excellent tutorial by Liu <ref> [18] </ref>. If some consecutively numbered nodes form a chain in the elimination tree, and the corresponding rows of L have identical nonzero structure, then this chain is called a supernode. The supernodal elimination tree is similar to the elimination tree, but nodes forming a supernode are collapsed together.
Reference: [19] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference: [20] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and Scheduling in Parallel Matrix Factorization. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 275-299, </pages> <year> 1986. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 17, 20] </ref>.
Reference: [21] <author> Christos H. Papadimitriouand Kenneth Steiglitz. CombinatorialOptimization, </author> <title> Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: Fortunately, this is a typical bin-packing problem, and even though, bin-packing is NP complete, a number of good approximate algorithms exist <ref> [21] </ref>. The use of bin-packing makes it possible to balance the computation and to significantly reduce the load imbalance. Acceptable Partitions A partition is acceptable if the percentage difference in the amount of work in the two parts is less than a small constant *.
Reference: [22] <author> A. Pothen and C-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference-contexts: Elimination of rows in different subtrees can proceed concurrently. For a given matrix, elimination trees of smaller height usually have greater concurrency than trees of larger height. A desirable ordering for parallel computers must increase the amount of concurrency without increasing fill-in substantially. Spectral nested dissection <ref> [22, 23] </ref> has been found to generate orderings that have both low fill-in and good parallelism. For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings on the performance of our algorithm refer to [16]. <p> Table 1 shows the load imbalance at the top level of the elimination tree for some matrices from the Boeing-Harwell matrix set. These matrices were ordered using the spectral nested dissection <ref> [22, 23] </ref>. Note that for all matrices the load imbalance in terms of operation count is substantially higher than the relative difference in the number of nodes in the left and right subtrees. <p> COPTER2 comes from a model of a helicopter rotor. CUBE35 is a 35 fi 35 fi 35 regular three-dimensional grid. NUG15 is from a linear programming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection <ref> [22, 23] </ref> to order the matrices. The performance obtained by our multifrontal algorithm in some of these matrices is shown in Table 2.
Reference: [23] <author> Alex Pothen, Horst D. Simon, and Kang-PuLiou. </author> <title> PartitioningSparse Matrices With Eigenvectors of Graphs. </title> <journal> SIAM J. on Matrix Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: Elimination of rows in different subtrees can proceed concurrently. For a given matrix, elimination trees of smaller height usually have greater concurrency than trees of larger height. A desirable ordering for parallel computers must increase the amount of concurrency without increasing fill-in substantially. Spectral nested dissection <ref> [22, 23] </ref> has been found to generate orderings that have both low fill-in and good parallelism. For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings on the performance of our algorithm refer to [16]. <p> Table 1 shows the load imbalance at the top level of the elimination tree for some matrices from the Boeing-Harwell matrix set. These matrices were ordered using the spectral nested dissection <ref> [22, 23] </ref>. Note that for all matrices the load imbalance in terms of operation count is substantially higher than the relative difference in the number of nodes in the left and right subtrees. <p> COPTER2 comes from a model of a helicopter rotor. CUBE35 is a 35 fi 35 fi 35 regular three-dimensional grid. NUG15 is from a linear programming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection <ref> [22, 23] </ref> to order the matrices. The performance obtained by our multifrontal algorithm in some of these matrices is shown in Table 2.
Reference: [24] <author> Alex Pothen and Chunguang Sun. </author> <title> Distributed multifrontal factorization using clique trees. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 34-40, </pages> <year> 1991. </year>
Reference: [25] <author> P. Raghavan. </author> <title> Distributed sparse Gaussian elimination and orthogonal factorization. </title> <type> TR 93-1818, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference: [26] <author> Edward Rothberg. </author> <title> Performance of Panel and Block Approaches to Sparse Cholesky Factorization on the iPSC/860 and Paragon Multicomputers. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: This localizes the communication among groups of processors, and thus improves the isoefficiency of the scheme to O. p 3 /. Roth-berg and Gupta <ref> [26, 27] </ref> used a different method to reduce the communication overhead. In their method, the entire sparse matrix is partitioned among processors using a two-dimensional block cyclic mapping. This reduces the communication overhead and improves the isoefficiency to O. p 1:5 log p/.
Reference: [27] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '93 Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: This localizes the communication among groups of processors, and thus improves the isoefficiency of the scheme to O. p 3 /. Roth-berg and Gupta <ref> [26, 27] </ref> used a different method to reduce the communication overhead. In their method, the entire sparse matrix is partitioned among processors using a two-dimensional block cyclic mapping. This reduces the communication overhead and improves the isoefficiency to O. p 1:5 log p/.
Reference: [28] <author> Robert Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> TR RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [4, 17, 20]. However, despite inherent parallelism in sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [12, 28] </ref>, and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers. <p> However, despite inherent parallelism in sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [12, 28], and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers. In <ref> [28] </ref>, Schreiber concludes that it is not yet clear whether sparse direct solvers can be made competitive at all for highly ( p 256) and massively ( p 4096) parallel computers. A parallel formulation for sparse matrix factorization can be easily obtained by simply distributing rows to different processors [6].
Reference: [29] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing '91 Proceedings, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference: [30] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-complete. </title> <journal> SIAM J. Algebraic Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year>
Reference-contexts: More precisely, we can choose a permutation matrix P such that the Cholesky factors of P A P T have minimal fill-in. The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete <ref> [30] </ref>, therefore a number of heuristic algorithms for ordering have been developed. In particular, the minimum degree ordering [7, 11] is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix.
References-found: 30

