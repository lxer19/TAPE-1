URL: http://www.cs.umn.edu/Research/Agassiz/Paper/choi.isca96.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: lchoi@csrd.uiuc.edu  yew@cs.umn.edu  
Title: Compiler and Hardware Support for Cache Coherence in Large-Scale Multiprocessors: Design Considerations and Performance Study  
Author: Lynn Choi Pen-Chung Yew 
Address: Urbana, IL 61801-1351  Minneapolis, MN 55455-0519  
Affiliation: University of Illinois at Urbana-Champaign Center for Supercomputing R D  University of Minnesota Department of Computer Science  
Abstract: In this paper, we study a hardware-supported, compiler-directed (HSCD) cache coherence scheme, which can be implemented on a large-scale multiprocessor using off-the-shelf microprocessors, such as the Cray T3D. It can be adapted to various cache organizations, including multi-word cache lines and byte-addressable architectures. Several system related issues, including critical sections, inter-thread communication, and task migration have also been addressed. The cost of the required hardware support is small and proportional to the cache size. The necessary compiler algorithms, including intra- and interprocedural array data-flow analysis, have been implemented on the Polaris compiler [17]. From our simulation study using the Perfect Club benchmarks, we found that, in spite of the conservative analysis made by the compiler, the performance of the proposed HSCD scheme can be comparable to that of a full-map hardware directory scheme. With its comparable performance and reduced hardware cost, the scheme can be a viable alternative for large-scale multiprocessors, such as the Cray T3D, that rely on users to maintain data coherence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, V. S. Adve, M. D. Hill, and M. K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298-308, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Lilja [14] compared the performance of the version control scheme [6] with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of the directory schemes. Adve et al <ref> [1] </ref> used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads.
Reference: [2] <author> R. Ballance, A. Maccabe, and K. Ottenstein. </author> <title> The Program Dependence Web: a Representation Supporting Control Data- and Demand-Driven Interpretation of Imperative Languages. </title> <booktitle> Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We use demand-driven symbolic analysis using the GSA form <ref> [2] </ref>. * Interprocedural analysis Previous HSCD schemes invalidate the entire cache at procedure boundaries to avoid side effects caused by procedure calls. We use a complete interprocedural analysis to avoid such invalidations and to exploit locality across procedure boundaries. <p> First, we construct a procedure call graph. Then, based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis transforms the source program into a GSA form <ref> [2] </ref>. Then, we construct a modified flow graph, called the epoch flow graph [9] that contains the epoch boundary information and the control flow of the program.
Reference: [3] <author> L. M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December, </month> <year> 1978. </year>
Reference-contexts: This coherence mechanism is similar to that of the Cray T3D, where the compiler generates noncacheable loads for shared memory references and cacheable loads for pri vate references. 2. Full-Map Directory Scheme (HW) This scheme uses a simple, three-state (invalid, read-shared, write-exclusive) invalidation-based protocol with a full-map directory <ref> [3] </ref> but without broadcasting. It gives a performance comparison to the hardware directory proto cols. Table 3 summarizes the characteristics of the four schemes according to the hardware and compiler support required, the caching strategy for shared data, and their performance limitations.
Reference: [4] <author> Yung-Chin Chen. </author> <title> Cache Design and Performance in a Large-Scale Shared-Memory Multiprocessor System. </title> <type> Technical report, </type> <institution> Univ. of Illinois, Dept. of Elec. Eng ., 1993. </institution> <type> Ph.D. Thesis. </type>
Reference-contexts: Adve et al [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. They concluded that the performance of compiler-directed schemes depends on the characteristics of the workloads. Chen <ref> [4] </ref> showed that a simple invalidation scheme can achieve performance comparable to that of a directory scheme and discusses several different write policies. Most of those studies, however, assumed perfect compile-time memory disambiguation and complete control dependence information. <p> This can be done by using write-through caches. However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor [10], such redundant write traffic can be effectively reduced <ref> [4] </ref>. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, the TPI scheme should force all global writes to be written back to the main memory at synchronization points. <p> A write-through write-allocate policy is used for both the TPI and the SC schemes, while a write-back cache is used for the hardware directory protocol. These write policies are chosen to deliver the best performance for each type of coherence scheme <ref> [4] </ref>. A weak consistency model is used for all the coherence schemes. It is assumed that each processor can handle basic arithmetic and logical operations in one cycle and has synchronization operations to support parallel language constructs. <p> Therefore, redundant writes are not merged. The assumption of the infinite write buffer will decrease the CPU stall times during the simulation compared to a fixed size write buffer. Chen <ref> [4] </ref> studied the issue of write buffer design for the compiler-directed schemes and found that 8 words of write merging write buffers will reduce the traffic significantly, and that the write through with the write merging write buffer is a better choice than a write back cache implementation for compiler-directed schemes. <p> In TRFD, there is a significant number of redundant writes which increases the overall network traffic of TPI substantially compared to HW. This additional write traffic can be effectively eliminated by organizing a write buffer as a cache <ref> [4] </ref>. Similar technique can also be employed to remove redundant write traffic for update-based coherence protocols. The third type of network traffic is for coherence transactions in the directory protocol. This extra traffic is relatively small compared to the read and write traffic for the benchmarks considered. <p> On the other hand, for MDG, QCD, and TRFD, HW has a much higher miss penalty (up to 31.2% 9 Although the compiler-directed schemes can employ write-back at task boundaries, it increases the latency of the invalidation, and results in more bursty traffic <ref> [4] </ref>. 10 This is the network latency vs. traffic tradeoff. Using write-through cache will increase the traffic while the write back cache will will increase the latency for hardware directory schemes.
Reference: [5] <author> H. Cheong. </author> <title> Life Span Strategy A Compiler-Based Approach to Cache Coherence. </title> <booktitle> Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [5, 6, 7, 9, 11, 15] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes. <p> Several compiler-directed cache coherence schemes [5, 6, 7, 9, 11, 15] have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [5, 9] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The guarded execution technique can be used to further optimize the code generation [8]. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [5, 6] </ref>. 3 Hardware Implementation Issues Off-chip secondary cache implementation Since most multiprocessors today use off-the-shelf microprocessors, it is more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors.
Reference: [6] <author> H. Cheong and A. Veidenbaum. </author> <title> Compiler-Directed Cache Management In Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [5, 6, 7, 9, 11, 15] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes. <p> Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [15] compared the performance of a directory scheme and a timestamp-based scheme assuming an infinite cache size and single-word cache lines. Lilja [14] compared the performance of the version control scheme <ref> [6] </ref> with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of the directory schemes. Adve et al [1] used an analytic model to compare the performance of compiler-directed and directory-based techniques. <p> The guarded execution technique can be used to further optimize the code generation [8]. The compiler marking algorithms developed here are general enough to be applicable to other compiler-directed coherence schemes <ref> [5, 6] </ref>. 3 Hardware Implementation Issues Off-chip secondary cache implementation Since most multiprocessors today use off-the-shelf microprocessors, it is more cost effective if the proposed TPI scheme can directly be implemented using existing microprocessors.
Reference: [7] <author> T. Chiueh. </author> <title> A Generational Approach to Software-Controlled Multiprocessor Cache Coherence. </title> <booktitle> Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [5, 6, 7, 9, 11, 15] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes.
Reference: [8] <author> Lynn Choi. </author> <title> Hardware and Compiler Support for Cache Coherence in Large-Scale Multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Dept., University of Illinois, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: The details of the compiler algorithms are beyond the scope of this paper and are described in <ref> [8] </ref>. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris paral-lelizing compiler [17]. Figure 2 shows the flowchart of our reference marking algorithm. First, we construct a procedure call graph. <p> We also calculate more precise offsets. We then transform the program in the GSA form back to the original program with the reference marking information, and generate appropriate cache and memory operations. The guarded execution technique can be used to further optimize the code generation <ref> [8] </ref>. <p> They are first parallelized by the Polaris compiler. In the parallelized code, the parallelism is expressed in terms of DOALL loops. We then mark the Time-Read operations in the parallelized source codes using our compiler algorithms <ref> [8] </ref>, which are also implemented in the Polaris compiler. After the compiler marking, we instrument the benchmarks to generate memory events, which are then used for the simulation. Simulation Execution-driven simulations [18] are used to verify the compiler algorithms and to evaluate the performance of our proposed coherence scheme.
Reference: [9] <author> Lynn Choi and Pen-Chung Yew. </author> <title> A Compiler-Directed Cache Coherence Scheme with Improved Intertask Locality. </title> <booktitle> Proceedings of the ACM/IEEE Supercomputing'94, </booktitle> <pages> pages 773-782, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [5, 6, 7, 9, 11, 15] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes. <p> Several compiler-directed cache coherence schemes [5, 6, 7, 9, 11, 15] have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes. They require a more precise program analysis to maintain coherence on a reference basis <ref> [5, 9] </ref> instead of a program region basis compared to the previous schemes. In addition, these schemes require hardware support to maintain local runtime cache states. In this regard, the terminology software cache coherence is a misnomer. It is a hardware approach with strong compiler support. <p> The issues of synchronization, such as lock variables and critical sections, have also rarely been addressed. In this paper, we address these issues and demonstrate the feasibility and the performance of a HSCD scheme we previously proposed <ref> [9] </ref>. To study the compiler analysis techniques for the proposed scheme, we have implemented the compiler algorithms on the Polaris compiler. <p> Second, the read reference to X (f (i)) in epoch 4 cannot be analyzed precisely at compile time due to the unknown index value. To overcome these limitations, we proposed a hardware scheme that keeps track of the local cache states at runtime <ref> [9] </ref>. 1 This operation can be implemented in the MIPS R10000 processor by a cache block invalidate (Index Write Back Invalidate) followed by a regular load operation. <p> On a cache miss, the timetags of other words in the same cache line are assigned the value (R counter - 1) to handle the implicit RAW (read-after-write) or WAR (write-after-read) dependences between concurrent tasks in the same epoch <ref> [9] </ref> (see section 2.1). Since the timetags are updated from the epoch counter by hardware at run-time, there is no need to store the timetags in main memory. <p> analysis. * Array data-flow analysis For a more precise analysis for arrays, we identify the region of an array that is referenced by an array reference and treat it as a distinct variable. 2 It is called two-phase invalidation due to its hardware reset mech anism on epoch counter overflow. <ref> [9] </ref> * Gated single assignment (GSA) form To perform effective array flow analysis, the symbolic manipulation of expressions is preferred because the computation of array regions often involves equality and comparison tests between symbolic expressions. <p> Then, based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. The per-procedure analysis transforms the source program into a GSA form [2]. Then, we construct a modified flow graph, called the epoch flow graph <ref> [9] </ref> that contains the epoch boundary information and the control flow of the program. Given a source program unit and its epoch flow graph, G, we identify the target references in each epoch to utilize both spatial and temporal reuses inside a task. <p> Therefore, there must be a strategy to recycle the timetag values for a correct operation. A simple strategy is to invalidate the entire cache and reset the epoch counter when it overflows. However, a small timetag may lead to frequent invalidations. In <ref> [9] </ref>, we propose a two-phase hardware re and large (64 bytes) cache line sizes. set mechanism that can simultaneously invalidate only those cache data whose timetags are out of phase. The idea is to use the most significant bit (MSB) of the epoch counter as an overflow flag.
Reference: [10] <author> Digital Equipment Corp. </author> <title> Alpha 21164 Microprocessor: Hardware Reference Manual. </title> <year> 1994. </year>
Reference-contexts: Note that this is true for all the shared-memory multiprocessors. This can be done by using write-through caches. However, this will produce more redundant write traffic. By organizing the write buffer as a cache, as in the DEC Alpha 21164 processor <ref> [10] </ref>, such redundant write traffic can be effectively reduced [4]. Note that ordinary write buffers can help hide latencies but cannot eliminate redundant write traffic. For a write-back cache configuration, the TPI scheme should force all global writes to be written back to the main memory at synchronization points.
Reference: [11] <author> E. Darnell and K. Kennedy. </author> <title> Cache Coherence Using Local Knowledge. </title> <booktitle> Proceedings of the Supercomputing '93, </booktitle> <pages> pages 720-729, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [5, 6, 7, 9, 11, 15] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes.
Reference: [12] <author> C. Dubnicki and T. LeBlanc. </author> <title> Adjustable Block Size Coherent Caches. </title> <booktitle> Proceedings the 19th Annual International Symposium on Computer Archtecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Maintaining sharing information per cache word in directory schemes will increase directory storage significantly since the memory requirement is proportional to the total memory size instead of the total cache size. Optimizations to reduce the storage overhead may result in very complicated hardware protocols <ref> [12] </ref>. However, more fine-grained sharing information can be incorporated in this HSCD scheme more easily because the coherence enforcement is done locally. A cache memory has two components: data storage and address tag storage.
Reference: [13] <author> C. P. Kruskal and M. Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(12):1091-1098, </volume> <month> Sept., </month> <year> 1987. </year>
Reference-contexts: size on-chip 64 KB, direct-mapped on epoch counter overflow two-phase reset mechanism line size 4 32-bit word cache line timetag size 8-bits number of processors 16 Cache and system organization ALU operations 1 CPU cycle cache hit 1 CPU cycle base miss latency 100 CPU cycles network delay analytic model <ref> [13] </ref> memory bus cycle 3 CPU cycles off-chip timetag access 2 CPU cycles off-chip timetag access penalty 8 (3 + 2 + 3) CPU cycles Table 1: Parameters used for typical simulations. <p> The memory system provides a one-cycle cache hit latency and a 100-cycle miss latency assuming no network load. The network delays are simulated using an analytical delay model for indirect multistage networks <ref> [13] </ref>. 7 We use ordinary write buffers for simulations. Therefore, redundant writes are not merged. The assumption of the infinite write buffer will decrease the CPU stall times during the simulation compared to a fixed size write buffer.
Reference: [14] <author> D. J. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer [15] compared the performance of a directory scheme and a timestamp-based scheme assuming an infinite cache size and single-word cache lines. Lilja <ref> [14] </ref> compared the performance of the version control scheme [6] with directory schemes, and analyzed the directory overhead of several implementations. Both studies show that the performance of those HSCD schemes can be comparable to that of the directory schemes.
Reference: [15] <author> S. L. Min and J.-L. Baer. </author> <title> Design and Analysis of a Scalable Cache Coherence Scheme Based on Clocks and Timestamps. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(1) </volume> <pages> 25-44, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Furthermore, data movement instructions are provided so that the programmer can explicitly move data between the cluster and global memories. By using these software mechanisms, coherence can be maintained for globally shared data. Several compiler-directed cache coherence schemes <ref> [5, 6, 7, 9, 11, 15] </ref> have been recently proposed. These schemes give better performance, but demand more hardware and compiler supports than the previous schemes. <p> It is a hardware approach with strong compiler support. We call them hardware-supported compiler-directed (HSCD) coherence schemes, which is distinctly different from a pure hardware directory scheme and a pure software scheme. Several studies have compared the performance of directory schemes and some recent HSCD schemes. Min and Baer <ref> [15] </ref> compared the performance of a directory scheme and a timestamp-based scheme assuming an infinite cache size and single-word cache lines. Lilja [14] compared the performance of the version control scheme [6] with directory schemes, and analyzed the directory overhead of several implementations.
Reference: [16] <author> J. M. Mulder, N. T. Quach, and M. J. Flynn. </author> <title> An area model for on-chip memories and its application. </title> <journal> Journal of Solid State Circuits, </journal> <volume> 26 </volume> <pages> 98-106, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: size of access units since each access unit (such as character, integer, or floating point data) is a distinct variable analyzed by the compiler. 5 Our experimental results in section 4 show that a 4-bit or 8-bit timetag is large enough to achieve very good performance. 6 As noted by <ref> [16, 22] </ref>, the tag overhead in the design of the on-chip caches are significant, occupying area comparable to the data portion, especially for set-associative caches with 64-bit addresses.
Reference: [17] <author> D. A. Padua, R. Eigenmann, J. Hoeflinger, P. Peter-son, P. Tu, S. Weatherford, and K. Faign. </author> <note> Polaris: </note>
Reference-contexts: The details of the compiler algorithms are beyond the scope of this paper and are described in [8]. Compiler implementation All the compiler algorithms explained earlier have been implemented on the Polaris paral-lelizing compiler <ref> [17] </ref>. Figure 2 shows the flowchart of our reference marking algorithm. First, we construct a procedure call graph. Then, based on the bottom-up scan of the call graph, we analyze each procedure and propagate its side effects to its callers. <p> The cost is less than the Scalable Coherence Interface (SCI) because the tag size required is much smaller than that required in SCI. The necessary compiler algorithms, including intra- and inter procedural array data-flow analysis, have been implemented on the Polaris paralleling compiler <ref> [17] </ref>. The result of our simulation study using the Perfect Club Benchmarks shows that both hardware directory schemes and the TPI scheme have comparable amount of unnecessary cache misses.
References-found: 17

