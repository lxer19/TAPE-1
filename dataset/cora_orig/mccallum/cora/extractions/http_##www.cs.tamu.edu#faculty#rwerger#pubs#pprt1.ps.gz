URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/pprt1.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Title: Run-Time Parallelization: It's Time Has Come survey limits itself to the domain of Fortran applications
Author: Lawrence Rauchwerger 
Keyword: Parallelization, Speculative, Run-Time, Inspector/Executor, Compiler, Scheduling, Debugging, Pointer Aliasing, Subsripted Subscripts, Irregular Applications.  
Note: Corresponding Author: Lawrence Rauchwerger  This  is also described.  
Address: College Station, TX 77843  847-8578  
Affiliation: Department of Computer Science Texas A&M University  
Email: email: rwerger@cs.tamu.edu  
Phone: telephone: (409) 845-8872  fax: (409)  
Abstract: Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. This type of loop mostly occurs in irregular, dynamic applications which represent more than 50% of all applications [20]. Making parallel computing succeed has therefore become conditioned by the ability of compilers to analyze and extract the parallelism from irregular applications. In this paper we present a survey of techniques that can complement the current compiler capabilities by performing some form of data dependence analysis during program execution, when all information is available. After describing the problem of loop parallelization and its difficulties, a general overview of the need for techniques of run-time parallelization is given. A survey of the various approaches to parallelizing partially parallel loops and fully parallel loops is presented. Special emphasis is placed on two parallelism enabling transformations, privatization and reduction parallelization, because of their proven efficiency. The technique of speculatively parallelizing doall loops is presented in more detail. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abraham. </author> <title> Private communication, </title> <year> 1994. </year>
Reference-contexts: In order to recognize statically which read references may not affect the data flow at run-time we first define a dynamic dead read reference: 1 any returns the OR of its vector operand's elements, i.e., any (v [1 : n]) = (v <ref> [1] </ref> _ v [2] _ : : : _ v [n]). 12 original shadow arrays PD test 1 2 3 4 5 tw tm A w 0 1 0 1 0 3 2 A np 0 0 0 0 0 A w (:) ^ A np (:) 0 1 0 1 <p> A processor-wise version of the LRPD test. The LRPD Test determines whether a loop has any cross-iteration data dependences. It turns out that essentially the same method can be used to test whether the loop, as executed, has any cross-processor data dependences <ref> [1] </ref>. The only difference is that all checks in the test refer to processors rather than to iterations, i.e., replace iteration by processor in the description of the LRPD test so that all iterations assigned to a processor are considered as one super-iteration by the test.
Reference: [2] <author> T. Allen and D. A. Padua. </author> <title> Debugging fortran on a shared-memory machine. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 721-727, </pages> <address> St. Charles, IL, </address> <year> 1987. </year>
Reference-contexts: In order to recognize statically which read references may not affect the data flow at run-time we first define a dynamic dead read reference: 1 any returns the OR of its vector operand's elements, i.e., any (v [1 : n]) = (v [1] _ v <ref> [2] </ref> _ : : : _ v [n]). 12 original shadow arrays PD test 1 2 3 4 5 tw tm A w 0 1 0 1 0 3 2 A np 0 0 0 0 0 A w (:) ^ A np (:) 0 1 0 1 0 new shadow <p> Generally, access anomaly detection techniques seek to identify the point in the parallel execution at which the access anomaly occurred. Padua et al. <ref> [2, 16] </ref> discuss methods that statically analyze the source program, and methods that analyze an execution trace of the program.
Reference: [3] <institution> Alliant Computer Systems Corporation. FX/Series Architecture Manual, </institution> <year> 1986. </year>
Reference-contexts: In Section 6 we will sketch a general performance driven strategy for run-time parallelization. 5.5 Experimental Results This section describes some experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 <ref> [3] </ref>) and 14 processors (Alliant FX/2800 [4]) using a Fortran implementation of the LRPD test. Four do loops from the PERFECT Benchmarks [6] that could not be parallelized by any available compiler were considered. The results are summarized in Table 3.
Reference: [4] <institution> Alliant Computers Systems Corporation. Alliant FX/2800 Series System Description, </institution> <year> 1991. </year>
Reference-contexts: In Section 6 we will sketch a general performance driven strategy for run-time parallelization. 5.5 Experimental Results This section describes some experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [3]) and 14 processors (Alliant FX/2800 <ref> [4] </ref>) using a Fortran implementation of the LRPD test. Four do loops from the PERFECT Benchmarks [6] that could not be parallelized by any available compiler were considered. The results are summarized in Table 3.
Reference: [5] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: important when static compiler analysis fails, i.e., for irregular, dynamic applications. 2.1 Fully Parallel (Doall) Loops In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [5, 24, 34, 47, 50] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
Reference: [6] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Four do loops from the PERFECT Benchmarks <ref> [6] </ref> that could not be parallelized by any available compiler were considered. The results are summarized in Table 3. For each loop, the type of test applied is noted: doall indicates cross-iteration dependences were checked (Lazy Doall (LD) test), privat indicates privatization was checked (LRPD test).
Reference: [7] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: Thus, if, as would be natural, each processor were assigned one iteration of the outer loop, we would have precisely the situation described above. 4.2 Methods for Loops Without Output Dependences The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [7, 41, 42, 43, 48] </ref>. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences.
Reference: [8] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks TM Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: It is widely assumed that more than 50% of codes [20] are of the irregular type. Thus, in order to realize the full potential of parallel computing it has become clear that static (compile-time) analysis must be augmented with new methods <ref> [8, 12, 15] </ref>. We need techniques that let us access the information necessary to decide if a loop is parallel and perform parallelizing transformations. The only time this data is available is during program execution, at run-time. <p> The more work (iterations) a doall loop executes, the more processors can be employed and a higher speedup can be obtained. Because there is experimental evidence that many important loops are indeed fully parallel <ref> [8] </ref>, we believe that they represent the biggest potential payoff in program parallelization. As mentioned before, exploiting the parallelism in irregular applications requires the application of run-time techniques whose overhead needs to be minimized.
Reference: [9] <author> William Blume, Rudolf Eigenmann, Jay Hoeflinger, David Padua, Paul Petersen, Lawrence Rauchwerger, and Peng Tu. </author> <title> Automatic Detection of Parallelism: A Grand Challenge for High-Performance Computing. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 37-47, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Just as important is the fact that parallel systems don't run only newly written applications. There is an enormous body of existing software that must be ported and perform well on these new systems. One solution is to rewrite the so named 'legacy' programs <ref> [9] </ref>, but this could prove to be prohibitively expensive. The alternative is to automatically transform them for concurrent execution by means of a restructuring or parallelizing compiler. This compiler should be able to safely (without errors) detect the available parallelism and transform the code into an explicitly parallel (machine) language.
Reference: [10] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: be applied to the loop: 4 * privatization * reduction parallelization Privatization can remove certain types of anti and output dependences by creating, whenever allowed, for each processor cooperating on the execution of the loop, private copies of program variables that give rise to anti or output dependences (see, e.g., <ref> [34, 10, 27, 28, 45, 46] </ref>).
Reference: [11] <author> Mark Byler, James Davies, Christopher Huson, Bruce Leasure, and Michael Wolfe. </author> <title> Multiple Version Loops. </title> <booktitle> Proc. of 1987 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <year> 1987. </year>
Reference-contexts: Many of today's parallelizing compilers postpone part of their analysis for the run-time phase by generating two-version loops <ref> [11] </ref>. These consist of an if statement that selects either the original serial loop or its parallel version. The boolean expression in the if statement typically tests the value of a scalar variable.
Reference: [12] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Comm. ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It is widely assumed that more than 50% of codes [20] are of the irregular type. Thus, in order to realize the full potential of parallel computing it has become clear that static (compile-time) analysis must be augmented with new methods <ref> [8, 12, 15] </ref>. We need techniques that let us access the information necessary to decide if a loop is parallel and perform parallelizing transformations. The only time this data is available is during program execution, at run-time.
Reference: [13] <author> D. K. Chen, P. C. Yew, and J. Torrellas. </author> <title> An efficient algorithm for the run-time parallelization of doacross loops. </title> <booktitle> In Proceedings of Supercomputing 1994, </booktitle> <pages> pages 518-527, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) <ref> [13, 26, 35, 41, 43, 49] </ref>. The only method that manages to combine the most advantageous features is that of [37]. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [37] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas <ref> [13] </ref> No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No Rauchwerger/Padua [38, 39] No 6 No No No P,R <p> The loop is executed in parallel using synchronization (full/empty bits) to enforce flow dependences. To our knowledge, this is the only run-time privatization technique except [38, 39]. Chen, Yew, and Torrellas <ref> [13] </ref> proposed an inspector that has a private phase and a merging phase. In the private phase, the loop is chunked and each processor builds a list of all the accesses to each memory location for its assigned iterations; read accesses to a memory location are serialized.
Reference: [14] <author> A. Dinning and E. Schonberg. </author> <title> An empirical comparison of monitoring algorithms for access anomaly detection. </title> <booktitle> In Proc. of 2-nd ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-10, </pages> <year> 1990. </year>
Reference-contexts: Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable <ref> [44, 14, 33] </ref>. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [44] and later by Nudler [33] bears similarities to a simplified version of the LRPD test presented in Section 5.1 (i.e., a version without privatization).
Reference: [15] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: It is widely assumed that more than 50% of codes [20] are of the irregular type. Thus, in order to realize the full potential of parallel computing it has become clear that static (compile-time) analysis must be augmented with new methods <ref> [8, 12, 15] </ref>. We need techniques that let us access the information necessary to decide if a loop is parallel and perform parallelizing transformations. The only time this data is available is during program execution, at run-time. <p> One typical method for the case of commutative reductions is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [15, 50] </ref> a section of code guarded by a lock-unlock operation which allows mutually exclusive operations on the shared variable. Drawbacks of this method are that it is not always scalable and requires synchronizations which can be very expensive in large multiprocessor systems.
Reference: [16] <author> P. A. Emrath, S. Ghosh, and D. A. Padua. </author> <title> Detecting nondeterminacy in parallel programs. </title> <journal> IEEE Soft., </journal> <pages> pages 69-77, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Generally, access anomaly detection techniques seek to identify the point in the parallel execution at which the access anomaly occurred. Padua et al. <ref> [2, 16] </ref> discuss methods that statically analyze the source program, and methods that analyze an execution trace of the program.
Reference: [17] <author> D.M. Gallagher, W. Y. Chen, S. A. Malke, J.G. Gyllenhaal, and Wen mei W. Hwu. </author> <title> Dynamic memory disambiguation using the memory conflict buffer. </title> <booktitle> In Proc. 21st Ann. Int'l Symp. Computer Architecture, </booktitle> <pages> pages 183-195, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [49]. Another significant contribution to this field is the work of Nicolau [32]. Run-time disambiguation has been recently used in optimizing codes for instruction level parallelism <ref> [18, 17] </ref>. Their idea is to speculatively execute code very aggressively (out of order) despite the fact that some memory locations (few) could cause unsatisfied data dependences. The offending addresses which are used out of order are stored until all potential hazards have been cleared.
Reference: [18] <author> A.S. Huang, G. Slavenburg, and J.P. Shen. </author> <title> Speculative disambiguation: A compilation technique for dynamic memory disambiguation. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 200-210, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [49]. Another significant contribution to this field is the work of Nicolau [32]. Run-time disambiguation has been recently used in optimizing codes for instruction level parallelism <ref> [18, 17] </ref>. Their idea is to speculatively execute code very aggressively (out of order) despite the fact that some memory locations (few) could cause unsatisfied data dependences. The offending addresses which are used out of order are stored until all potential hazards have been cleared.
Reference: [19] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <year> 1985. </year>
Reference-contexts: While the method may not be suitable for performance-oriented parallelization of doall loops, it is a clever technique for debugging arbitrary fork-join parallelism constructs. 20 7.2 Optimistic Execution A concept related to the speculative approach described in this paper is virtual time first introduced in <ref> [19] </ref> and defined as " ... paradigm for organizing and synchronizing distributed systems.... [It] provides a flexible abstraction of real time in much the same way that virtual memory provides an abstraction of real memory.
Reference: [20] <author> Ken Kennedy. </author> <title> Compiler technology for machine-independent programming. </title> <journal> Int. J. Paral. Prog., </journal> <volume> 22(1) </volume> <pages> 79-98, </pages> <month> February </month> <year> 1994. </year> <month> 22 </month>
Reference-contexts: Since modeling techniques are becoming more sophisticated we believe that future large scale simulations and computation will be dynamic in nature and will only increase the fraction of statically non-analyzable codes. It is widely assumed that more than 50% of codes <ref> [20] </ref> are of the irregular type. Thus, in order to realize the full potential of parallel computing it has become clear that static (compile-time) analysis must be augmented with new methods [8, 12, 15].
Reference: [21] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> various methods is given in Table 2. obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [37] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan <ref> [21] </ref> No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No <p> An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except [35, 38, 39]) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan <ref> [21] </ref> proposed a run-time scheme for removing anti and output dependences 8 from loops. Their scheme includes a parallel inspector that determines the number of accesses to each memory location using critical sections as in the method of Zhu and Yew (and is thus sensitive to hotspots).
Reference: [22] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: Drawbacks of this method are that it is not always scalable and requires synchronizations which can be very expensive in large multiprocessor systems. A scalable method can be obtained by noting that a reduction operation is an associative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [22, 23, 25] </ref>.
Reference: [23] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 869-876, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Drawbacks of this method are that it is not always scalable and requires synchronizations which can be very expensive in large multiprocessor systems. A scalable method can be obtained by noting that a reduction operation is an associative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [22, 23, 25] </ref>.
Reference: [24] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: important when static compiler analysis fails, i.e., for irregular, dynamic applications. 2.1 Fully Parallel (Doall) Loops In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [5, 24, 34, 47, 50] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
Reference: [25] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Drawbacks of this method are that it is not always scalable and requires synchronizations which can be very expensive in large multiprocessor systems. A scalable method can be obtained by noting that a reduction operation is an associative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [22, 23, 25] </ref>.
Reference: [26] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th PPOPP, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops <ref> [26, 41, 43] </ref>, have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. The only method that manages to combine the most advantageous features is that of [37]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) <ref> [13, 26, 35, 41, 43, 49] </ref>. The only method that manages to combine the most advantageous features is that of [37]. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan <ref> [26] </ref> Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No Rauchwerger/Padua [38, 39] No 6 No No No P,R Table 2: A comparison of run-time parallelization techniques for do loops. <p> Leung and Zahorjan <ref> [26] </ref> have proposed some other methods of parallelizing the inspector of Saltz et al. These techniques are also restricted to loops with no output dependences.
Reference: [27] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: be applied to the loop: 4 * privatization * reduction parallelization Privatization can remove certain types of anti and output dependences by creating, whenever allowed, for each processor cooperating on the execution of the loop, private copies of program variables that give rise to anti or output dependences (see, e.g., <ref> [34, 10, 27, 28, 45, 46] </ref>).
Reference: [28] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: be applied to the loop: 4 * privatization * reduction parallelization Privatization can remove certain types of anti and output dependences by creating, whenever allowed, for each processor cooperating on the execution of the loop, private copies of program variables that give rise to anti or output dependences (see, e.g., <ref> [34, 10, 27, 28, 45, 46] </ref>).
Reference: [29] <author> John Mellor-Crummey. </author> <title> On-the-fly detection of data races for programs with nested fork-join parallelism. </title> <booktitle> In Proceedings of Supercomputing 1991, </booktitle> <pages> pages 24-33, </pages> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Viewed in the framework of the LRPD test, a separate shadow array for each iteration in a loop must be maintained. In 1991 Mellor-Crummey <ref> [29] </ref> improved this technique by dramatically reducing the memory requirements; the maximum accesses history storage is O (N), where N is the maximum level fork-join nesting.
Reference: [30] <author> John Mellor-Crummey. </author> <title> Compile-time support for efficient data race detection in shared-memory parallel programs. </title> <booktitle> In Proc. if the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 129-139, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The execution time overhead is still very high, because every reference monitored has to be logged and checked against the access history in a critical section. In <ref> [30] </ref> an order of magnitude increase in execution time of instrumented codes is reported for experiments on a sequential machine. Even after reducing the shadowed references through compile time analysis, the time expansion factor remains around 5.
Reference: [31] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: In the case of irregular applications, where compiler analysis is insufficient, software pipelining is not possible, because the safe initiation interval unknown and often variable. Statically analyzable doacross loops have been exploited in multiprocessors through the use of hardware or software synchronization primitives <ref> [31, 49] </ref>. The compiler can place post and await instructions around the accesses that cause flow dependences. The speedup obtained with this technique is upper bounded by the size of the loop iterations that may be overlapped and the overhead of synchronizations. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> A high level comparison of the various methods is given in Table 2. obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [37] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua <ref> [31] </ref> Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 <p> Midkiff and Padua <ref> [31] </ref> extended this method to allow concurrent reads from a memory location in multiple iterations. Due to the compare-and-swap synchronizations, this method runs the risk of a severe degradation in performance for access patterns containing hot spots (i.e., many accesses to the same memory location).
Reference: [32] <author> A. Nicolau. </author> <title> Run-time disambiguation: coping with statically unpredictable dependencies. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(5) </volume> <pages> 663-678, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [49]. Another significant contribution to this field is the work of Nicolau <ref> [32] </ref>. Run-time disambiguation has been recently used in optimizing codes for instruction level parallelism [18, 17]. Their idea is to speculatively execute code very aggressively (out of order) despite the fact that some memory locations (few) could cause unsatisfied data dependences.
Reference: [33] <author> I. Nudler and L. Rudolph. </author> <title> Tools for the efficient developement of efficient parallel programs. </title> <booktitle> In Proc. 1st Israeli Conference on Computer System Engineering, </booktitle> <year> 1988. </year>
Reference-contexts: Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable <ref> [44, 14, 33] </ref>. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [44] and later by Nudler [33] bears similarities to a simplified version of the LRPD test presented in Section 5.1 (i.e., a version without privatization). <p> In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [44] and later by Nudler <ref> [33] </ref> bears similarities to a simplified version of the LRPD test presented in Section 5.1 (i.e., a version without privatization).
Reference: [34] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Techniques addressing the issue of data dependence analysis have been studied extensively over the last two decades <ref> [34, 47] </ref> but parallelizing compilers cannot perform a meaningful data dependence analysis and extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically insufficiently defined access pattern. Unfortunately irregular programs, as previously defined, represent a large part of all scientific applications. <p> important when static compiler analysis fails, i.e., for irregular, dynamic applications. 2.1 Fully Parallel (Doall) Loops In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [5, 24, 34, 47, 50] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). <p> be applied to the loop: 4 * privatization * reduction parallelization Privatization can remove certain types of anti and output dependences by creating, whenever allowed, for each processor cooperating on the execution of the loop, private copies of program variables that give rise to anti or output dependences (see, e.g., <ref> [34, 10, 27, 28, 45, 46] </ref>).
Reference: [35] <author> C. Polychronopoulos. </author> <title> Compiler Optimizations for Enhancing Parallelism and Their Imp act on Architecture Design. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-37(8):991-1004, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components <ref> [35, 41, 43] </ref>, and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. The only method that manages to combine the most advantageous features is that of [37]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) <ref> [13, 26, 35, 41, 43, 49] </ref>. The only method that manages to combine the most advantageous features is that of [37]. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous <ref> [35] </ref> No No No No No Rauchwerger/Padua [38, 39] No 6 No No No P,R Table 2: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> However, when there are no hot spots and the critical path length is very small, this method should perform well. An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except <ref> [35, 38, 39] </ref>) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan [21] proposed a run-time scheme for removing anti and output dependences 8 from loops. <p> We should remark however that this technique can be applied only to loops from which a side-effect free inspector can be extracted. 4.4 Other methods In contrast to the above methods which place iterations in the lowest possible wavefront, Polychronopolous <ref> [35] </ref> gives a method where wavefronts are maximal sets of contiguous iterations with no cross-iteration dependences. Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [49].
Reference: [36] <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> Run-time methods for parallelizing partially parallel loops. </title> <booktitle> In Proceedings of the 1995 International Conference on Supercomputing, Barcelona, Spain, </booktitle> <pages> pages 137-146, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: In <ref> [36, 37] </ref> a technique is presented in which all its 3 phases, inspector, scheduler and executor, are parallel and scale well with the number of processors.
Reference: [37] <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> A scalable method for run-time loop parallelization. </title> <journal> IJPP, </journal> <volume> 26(6) </volume> <pages> 537-576, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The only method that manages to combine the most advantageous features is that of <ref> [37] </ref>. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> A high level comparison of the various methods is given in Table 2. obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua <ref> [37] </ref> Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et <p> In <ref> [36, 37] </ref> a technique is presented in which all its 3 phases, inspector, scheduler and executor, are parallel and scale well with the number of processors. <p> From this fact alone we can conclude that the run-time overhead of doall detection will be lower than that of partially parallel loops. All the techniques surveyed in the previous section use some form of the inspector/executor model. Of these, only the method proposed by <ref> [37] </ref> does not make use of critical sections. Unfortunately the distribution of a loop into an inspector and an executor is often not advantageous (or even possible).
Reference: [38] <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array privatization. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No Rauchwerger/Padua <ref> [38, 39] </ref> No 6 No No No P,R Table 2: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> However, when there are no hot spots and the critical path length is very small, this method should perform well. An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except <ref> [35, 38, 39] </ref>) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan [21] proposed a run-time scheme for removing anti and output dependences 8 from loops. <p> The loop is executed in parallel using synchronization (full/empty bits) to enforce flow dependences. To our knowledge, this is the only run-time privatization technique except <ref> [38, 39] </ref>. Chen, Yew, and Torrellas [13] proposed an inspector that has a private phase and a merging phase. <p> Because of the author's opinion that speculative doall parallelization can be the basis of run-time parallelization we present it in more detail the previously described methods. First we describe the LRPD Test <ref> [40, 38] </ref>, an algorithm that can detect fully parallel loops and that can validate two of the most effective parallelism enabling transformations: privatization and reduction parallelization. Then we show how the test can be used both speculatively as well as an inspector for parallelizing loops in real applications.
Reference: [39] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <type> Technical Report 1390, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No Rauchwerger/Padua <ref> [38, 39] </ref> No 6 No No No P,R Table 2: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> However, when there are no hot spots and the critical path length is very small, this method should perform well. An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except <ref> [35, 38, 39] </ref>) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan [21] proposed a run-time scheme for removing anti and output dependences 8 from loops. <p> The loop is executed in parallel using synchronization (full/empty bits) to enforce flow dependences. To our knowledge, this is the only run-time privatization technique except <ref> [38, 39] </ref>. Chen, Yew, and Torrellas [13] proposed an inspector that has a private phase and a merging phase.
Reference: [40] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <pages> pages 218-232, </pages> <month> June </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Because of the author's opinion that speculative doall parallelization can be the basis of run-time parallelization we present it in more detail the previously described methods. First we describe the LRPD Test <ref> [40, 38] </ref>, an algorithm that can detect fully parallel loops and that can validate two of the most effective parallelism enabling transformations: privatization and reduction parallelization. Then we show how the test can be used both speculatively as well as an inspector for parallelizing loops in real applications. <p> Syntactic pattern matching cannot identify all potential reduction variables (e.g., in the presence of subscripted subscripts). Below we describe how potential reductions can be validated at run-time. Techniques for statically finding more potential reductions are described in more detail in <ref> [40] </ref>. A potential reduction statement is assumed to syntactically pattern match the generic reduction template x = x exp; reduction statements that do not meet this criterion are treated in [40]. <p> Techniques for statically finding more potential reductions are described in more detail in <ref> [40] </ref>. A potential reduction statement is assumed to syntactically pattern match the generic reduction template x = x exp; reduction statements that do not meet this criterion are treated in [40]. The reduction can be validated by checking at run-time that the reduction variable x satisfies the definition given in Section 2, i.e., that x is only accessed in the reduction statement, and that it does not appear in exp. <p> The storage requirements become in this case O (na=p) for each processor or O (na) in total and the time complexity of the test becomes O (na=p + log p). A more precise analysis can be found in <ref> [40] </ref>. 5.4 Inspector/Executor vs. Speculative Strategies So far the only run-time test that can be performed in either inspector/executor or speculative mode is the LRPD test which checks exclusively for full parallelism.
Reference: [41] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 174-178. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II - Software. </booktitle>
Reference-contexts: Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops <ref> [26, 41, 43] </ref>, have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. The only method that manages to combine the most advantageous features is that of [37]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components <ref> [35, 41, 43] </ref>, and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. The only method that manages to combine the most advantageous features is that of [37]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) <ref> [13, 26, 35, 41, 43, 49] </ref>. The only method that manages to combine the most advantageous features is that of [37]. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [37] Yes No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney <ref> [41] </ref> No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No Rauchwerger/Padua [38, 39] No 6 No No No P,R Table 2: A comparison of run-time parallelization techniques <p> Next, the lists for each memory location are linked across processors using a global Zhu/Yew algorithm [49]. Their scheduler/executor uses doacross parallelization <ref> [41] </ref>, i.e., iterations are started in a wrapped manner and processors busy wait until their operands are ready. Although this scheme potentially has less communication overhead than [49], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [49]. <p> Thus, if, as would be natural, each processor were assigned one iteration of the outer loop, we would have precisely the situation described above. 4.2 Methods for Loops Without Output Dependences The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [7, 41, 42, 43, 48] </ref>. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences. <p> The inspector computation (the topological sort) can be parallelized somewhat using the DOACROSS parallelization technique of Saltz and Mirchandaney <ref> [41] </ref>, in which processors are assigned iterations in a wrapped manner, and busy-waits are used to ensure that values have been produced before they are used (again, this is only possible if the original loop has no output dependences).
Reference: [42] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> The doconsider loop. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <pages> pages 29-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Thus, if, as would be natural, each processor were assigned one iteration of the outer loop, we would have precisely the situation described above. 4.2 Methods for Loops Without Output Dependences The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [7, 41, 42, 43, 48] </ref>. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences.
Reference: [43] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops <ref> [26, 41, 43] </ref>, have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. The only method that manages to combine the most advantageous features is that of [37]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components <ref> [35, 41, 43] </ref>, and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. The only method that manages to combine the most advantageous features is that of [37]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) <ref> [13, 26, 35, 41, 43, 49] </ref>. The only method that manages to combine the most advantageous features is that of [37]. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> No No No P,R Zhu/Yew [49] No 1 No Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. <ref> [43] </ref> Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [26] Yes No Yes Yes 5 No Polychronopoulous [35] No No No No No Rauchwerger/Padua [38, 39] No 6 No No No P,R Table 2: A comparison of run-time parallelization techniques for do loops. <p> Thus, if, as would be natural, each processor were assigned one iteration of the outer loop, we would have precisely the situation described above. 4.2 Methods for Loops Without Output Dependences The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [7, 41, 42, 43, 48] </ref>. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences. <p> In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences. In <ref> [43] </ref>, the inspector constructs stages that respect the flow dependences by performing a sequential topological sort of the accesses in the loop. The executer enforces any anti-dependences by using old and new versions of each variable.
Reference: [44] <author> E. Schonberg. </author> <title> On-the-fly detection of access anomalies. </title> <booktitle> In Proceedings of the SIGPLAN 1989 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 285-297, </pages> <address> Portland, Oregon, </address> <year> 1989. </year>
Reference-contexts: Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable <ref> [44, 14, 33] </ref>. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg [44] and later by Nudler [33] bears similarities to a simplified version of the LRPD test presented in Section 5.1 (i.e., a version without privatization). <p> Since not all anomalies can be detected statically, and execution traces can require prohibitive amounts of memory, run-time access anomaly detection methods that minimize memory requirements are desirable [44, 14, 33]. In fact, a run-time anomaly detection method proposed by Snir, and optimized by Schonberg <ref> [44] </ref> and later by Nudler [33] bears similarities to a simplified version of the LRPD test presented in Section 5.1 (i.e., a version without privatization).
Reference: [45] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proceedings 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: be applied to the loop: 4 * privatization * reduction parallelization Privatization can remove certain types of anti and output dependences by creating, whenever allowed, for each processor cooperating on the execution of the loop, private copies of program variables that give rise to anti or output dependences (see, e.g., <ref> [34, 10, 27, 28, 45, 46] </ref>).
Reference: [46] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: be applied to the loop: 4 * privatization * reduction parallelization Privatization can remove certain types of anti and output dependences by creating, whenever allowed, for each processor cooperating on the execution of the loop, private copies of program variables that give rise to anti or output dependences (see, e.g., <ref> [34, 10, 27, 28, 45, 46] </ref>).
Reference: [47] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: Techniques addressing the issue of data dependence analysis have been studied extensively over the last two decades <ref> [34, 47] </ref> but parallelizing compilers cannot perform a meaningful data dependence analysis and extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically insufficiently defined access pattern. Unfortunately irregular programs, as previously defined, represent a large part of all scientific applications. <p> important when static compiler analysis fails, i.e., for irregular, dynamic applications. 2.1 Fully Parallel (Doall) Loops In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [5, 24, 34, 47, 50] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write).
Reference: [48] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: Thus, if, as would be natural, each processor were assigned one iteration of the outer loop, we would have precisely the situation described above. 4.2 Methods for Loops Without Output Dependences The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [7, 41, 42, 43, 48] </ref>. In most of these methods, the original source loop is transformed into an inspector, which performs some run-time data dependence analysis and constructs a (preliminary) schedule, and an executor, which performs the scheduled work. The original source loop is assumed to have no output dependences.
Reference: [49] <author> C. Zhu and P. C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: In the case of irregular applications, where compiler analysis is insufficient, software pipelining is not possible, because the safe initiation interval unknown and often variable. Statically analyzable doacross loops have been exploited in multiprocessors through the use of hardware or software synchronization primitives <ref> [31, 49] </ref>. The compiler can place post and await instructions around the accesses that cause flow dependences. The speedup obtained with this technique is upper bounded by the size of the loop iterations that may be overlapped and the overhead of synchronizations. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) <ref> [13, 21, 26, 31, 35, 41, 43, 49] </ref>, are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) [13, 26, 35, 41, 43, 49]. <p> Briefly, run-time methods for parallelizing loops rely heavily on global synchronizations (communication) [13, 21, 26, 31, 35, 41, 43, 49], are applicable only to restricted types of loops [26, 41, 43], have significant sequential components [35, 41, 43], and/or do not extract the maximum available parallelism (they make conservative assumptions) <ref> [13, 26, 35, 41, 43, 49] </ref>. The only method that manages to combine the most advantageous features is that of [37]. It does however rely on the availability of an inspector loop, which is not a generally applicable technique. <p> A high level comparison of the various methods is given in Table 2. obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions Rauchwerger/Amato/Padua [37] Yes No No No P,R Zhu/Yew <ref> [49] </ref> No 1 No Yes 2 No No Midkiff/Padua [31] Yes No Yes 2 No No Krothapalli/Sadayappan [21] No 3 No Yes 2 No P Chen/Yew/Torrellas [13] No 1;3 No Yes No No Saltz/Mirchandaney [41] No 3 No Yes Yes 5 No Saltz et al. [43] Yes Yes 4 Yes Yes <p> the method is only applicable to loops without any output dependences (i.e., each memory location is written at most once); 6, the method only identifies fully parallel loops. 4.1 Methods Utilizing Critical Sections One of the first run-time methods for scheduling partially parallel loops was proposed by Zhu and Yew <ref> [49] </ref>. It computes the wavefronts one after another using the following simple strategy. <p> Next, the lists for each memory location are linked across processors using a global Zhu/Yew algorithm <ref> [49] </ref>. Their scheduler/executor uses doacross parallelization [41], i.e., iterations are started in a wrapped manner and processors busy wait until their operands are ready. Although this scheme potentially has less communication overhead than [49], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves <p> Next, the lists for each memory location are linked across processors using a global Zhu/Yew algorithm <ref> [49] </ref>. Their scheduler/executor uses doacross parallelization [41], i.e., iterations are started in a wrapped manner and processors busy wait until their operands are ready. Although this scheme potentially has less communication overhead than [49], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [49]. <p> Although this scheme potentially has less communication overhead than <ref> [49] </ref>, it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [49]. For example, consider a loop with cpl = p and dependence distance p as well, i.e., in which each processor's iterations access the same set of n=p distinct memory locations. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in <ref> [49] </ref>. Another significant contribution to this field is the work of Nicolau [32]. Run-time disambiguation has been recently used in optimizing codes for instruction level parallelism [18, 17].
Reference: [50] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <month> 24 </month>
Reference-contexts: important when static compiler analysis fails, i.e., for irregular, dynamic applications. 2.1 Fully Parallel (Doall) Loops In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [5, 24, 34, 47, 50] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). <p> One typical method for the case of commutative reductions is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [15, 50] </ref> a section of code guarded by a lock-unlock operation which allows mutually exclusive operations on the shared variable. Drawbacks of this method are that it is not always scalable and requires synchronizations which can be very expensive in large multiprocessor systems. <p> So far the problem of reduction variable recognition has been handled at compile-time by syntactically pattern matching the loop statements with a template of a generic reduction, and then performing a data dependence analysis of the variable under scrutiny to validate it as a reduction variable <ref> [50] </ref>. There are two major shortcomings of such pattern matching identification methods. 1. The data dependence analysis necessary to qualify a statement as a reduction cannot be performed statically in the presence of input-dependent access patterns. 2.
References-found: 50

