URL: http://www.cis.ohio-state.edu/~harrold/webpapers/tse98-studies.ps
Refering-URL: http://www.cis.ohio-state.edu/~harrold/allpapers.html
Root-URL: 
Email: grother@cs.orst.edu  harrold@cis.ohio-state.edu  
Title: Empirical Studies of a Safe Regression Test Selection Technique  
Author: Gregg Rothermel Mary Jean Harrold 
Date: May 14, 1998  
Address: Corvallis, OR 97331  Columbus, OH 43210  
Affiliation: Department of Computer Science Oregon State University  Department of Computer and Information Science The Ohio State University  
Abstract: Regression testing is an expensive testing procedure utilized to validate modified software. Regression test selection techniques attempt to reduce the cost of regression testing by selecting a subset of a program's existing test suite. Safe regression test selection techniques select subsets that, under certain well-defined conditions, exclude no tests (from the original test suite) that if executed would reveal faults in the modified software. Many regression test selection techniques, including several safe techniques, have been proposed, but few have been subjected to empirical validation. This paper reports empirical studies on a particular safe regression test selection technique, in which the technique is compared to the alternative regression testing strategy of running all tests. The results indicate that safe regression test selection can be cost-effective, but that its costs and benefits vary widely based on a number of factors. In particular, test suite design can significantly affect the effectiveness of test selection, and coverage-based test suites may provide test selection results superior to those provided by test suites that are not coverage-based.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Agrawal, J. Horgan, E. Krauser, and S. </author> <title> London. Incremental regression testing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1993, </booktitle> <pages> pages 348-357, </pages> <month> September </month> <year> 1993. </year>
Reference: [2] <author> M. Balcer, W. Hasling, and T. </author> <title> Ostrand. Automatic generation of test scripts from formal test specifications. </title> <booktitle> In Proceedings of the Third Symposium on Software Testing, Analysis, and Verification, </booktitle> <pages> pages 210-218, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: create these test pools, they first created an initial set of black-box tests "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of . . . the code" [19, p. 194], using the category partition method and the Siemens Test Specification Language tool <ref> [2, 27] </ref>.
Reference: [3] <author> T. Ball. </author> <title> On the limit of control flow analysis for regression test selection. </title> <booktitle> In Proceedings of the 1998 International Symposium on Software Testing and Analysis (ISSTA), </booktitle> <month> March </month> <year> 1998. </year>
Reference-contexts: At this point further traversal is unneeded; the algorithm returns test suite ft2,t3g. An analytical evaluation of our test selection algorithms [34] proves that they are safe for controlled regression testing. 3 Several other safe algorithms now exist <ref> [3, 8, 21, 38] </ref>; however, our basic algorithm is among the two most precise of these algorithms. 4 Our analysis also shows that our algorithms are at least as general as existing techniques, whether safe or not. <p> Controlled regression testing is discussed in detail in Reference [33]. 4 Recently, Ball <ref> [3] </ref> presented a family of control-flow-graph based algorithms that, building on our control-flow-graph based approach, achieve greater precision than our basic algorithm. <p> As a result, set operations may require less time due to improved instruction cache effects. Interestingly, a recent paper by Ball <ref> [3] </ref> presents a version of our basic algorithm that, while walking the graphs, simply selects a set of edges, and then performs all set unions at the end; by reducing the number of set unions that may be required this approach improves the worst case time bound on the algorithm from
Reference: [4] <author> S. Bates and S. Horwitz. </author> <title> Incremental program testing using program dependence graphs. </title> <booktitle> In Proceedings of the 20th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference: [5] <author> P. Benedusi, A. Cimitile, and U. De Carlini. </author> <title> Post-maintenance testing based on path change analysis. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1988, </booktitle> <pages> pages 352-361, </pages> <month> October </month> <year> 1988. </year>
Reference: [6] <author> D. Binkely. </author> <title> Semantics guided regression test cost reduction. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 23(8), </volume> <month> August </month> <year> 1997. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34]. <p> Many of the selective retest techniques cited in Section 2 of this paper consider this regression testing adequacy problem in addition to the problem of test selection; however, to our knowledge, with the exception of recent work by Binkley <ref> [6] </ref>, these techniques have not been empirically investigated. Data calculated from the results of Studies 2 and 3 support a hypothesis that for purposes of regression test selection, code-coverage-based test suites may be preferable to non-coverage-based test suites. This support comes in two forms.
Reference: [7] <author> D. Binkley. </author> <title> Reducing the cost of regression testing by semantics guided test case selection. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1995, </booktitle> <month> October </month> <year> 1995. </year> <month> 31 </month>
Reference: [8] <author> Y.F. Chen, D.S. Rosenblum, and K.P. Vo. TestTube: </author> <title> A system for selective regression testing. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 211-222, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34]. <p> At this point further traversal is unneeded; the algorithm returns test suite ft2,t3g. An analytical evaluation of our test selection algorithms [34] proves that they are safe for controlled regression testing. 3 Several other safe algorithms now exist <ref> [3, 8, 21, 38] </ref>; however, our basic algorithm is among the two most precise of these algorithms. 4 Our analysis also shows that our algorithms are at least as general as existing techniques, whether safe or not.
Reference: [9] <author> K.F. Fischer. </author> <title> A test case selection method for the validation of software maintenance modifications. </title> <booktitle> In Proceedings of COMPSAC '77, </booktitle> <pages> pages 421-426, </pages> <month> November </month> <year> 1977. </year>
Reference: [10] <author> K.F. Fischer, F. Raji, and A. Chruscicki. </author> <title> A methodology for retesting modified software. </title> <booktitle> In Proceedings of the National Telecommunications Conference B-6-3, </booktitle> <pages> pages 1-6, </pages> <month> November </month> <year> 1981. </year>
Reference: [11] <author> T.L. Graves, M.J. Harrold, J-M Kim, A. Porter, and G. Rothermel. </author> <title> An empirical study of regression test selection techniques. </title> <booktitle> In The 20th International Conference on Software Engineering, </booktitle> <month> April </month> <year> 1998. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34]. <p> We are currently investigating these issues. In addition to experiments with individual test selection algorithms, comparative experiments of various algorithms are also mandated. To date, only two such experiments have been reported in the literature <ref> [11, 29] </ref>. Such experiments should examine the relative cost-benefits of different families of test selection algorithms: for example, minimization versus safe algorithms. Such experiments should also examine the relative cost-benefits of algorithms within the same family, such as the various safe algorithms, or the various versions of our graph-walk-based algorithms.
Reference: [12] <author> R. Gupta, M.J. Harrold, </author> <title> and M.L. Soffa. An approach to regression testing using slicing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1992, </booktitle> <pages> pages 299-308, </pages> <month> November </month> <year> 1992. </year>
Reference: [13] <author> M. J. Harrold, D. Rosenblum, G. Rothermel, and E. Weyuker. </author> <title> Empirical studies of a prediction model for regression test selection. </title> <type> Technical Report OSU-CISRC-2/98-TR55, </type> <institution> The Ohio State University, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: Their predictors use the average percentage of test cases that execute covered entities|such as statements, branches, or functions|to predict the number of tests that will be selected when a change is made to those entities. Subsequent empirical studies by Harrold, Rosenblum, Rothermel and Weyuker <ref> [13] </ref> show that, although the Rosenblum-Weyuker predictor is relatively effective at predicting the average effectiveness of a regression test selection strategy, it may significantly under- or over-estimate test selection results for particular versions.
Reference: [14] <author> M. J. Harrold and G. Rothermel. Aristotle: </author> <title> A system for research on and development of program-analysis-based tools. </title> <type> Technical Report OSU-CISRC-3/97-TR17, </type> <institution> The Ohio State University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: This approach adds precision to the test selection without adding the full cost of, or implementation effort required to support, complete dataflow analysis. To obtain control flow graphs, variable usage data, and test history information, we utilized the Aristotle program analysis system <ref> [14] </ref>. Given the functionality provided by Aristotle, the DejaVu implementation itself required only 1220 lines of C code. DejaVu and its interface with Aristotle are described in detail in Reference [31].
Reference: [15] <author> M.J. Harrold and M.L. Soffa. </author> <title> An incremental approach to unit testing during maintenance. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1988, </booktitle> <pages> pages 362-367, </pages> <month> October </month> <year> 1988. </year>
Reference: [16] <author> M.J. Harrold and M.L. Soffa. </author> <title> An incremental data flow testing tool. </title> <booktitle> In Proceedings of the Sixth International Conference on Testing Computer Software, </booktitle> <month> May </month> <year> 1989. </year>
Reference: [17] <author> J. Hartmann and D.J. Robson. </author> <title> RETEST development of a selective revalidation prototype environment for use in software maintenance. </title> <booktitle> In Proceedings of the Twenty-Third Hawaii International Conference on System Sciences, </booktitle> <pages> pages 92-101, </pages> <month> January </month> <year> 1990. </year>
Reference: [18] <author> J. Hartmann and D.J. Robson. </author> <title> Techniques for selective revalidation. </title> <journal> IEEE Software, </journal> <volume> 16(1) </volume> <pages> 31-38, </pages> <month> January </month> <year> 1990. </year>
Reference: [19] <author> M. Hutchins, H. Foster, T. Goradia, and T. </author> <title> Ostrand. Experiments on the effectiveness of dataflow- and controlflow-based test adequacy criteria. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 191-200, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Subjects We obtained seven C programs together with a number of modified versions and tests for those programs. These subjects had been used in an earlier study by researchers at Siemens Corporate Research to compare controlflow-based and dataflow-based test adequacy criteria <ref> [19] </ref>. 6 Table 1 lists the subjects. <p> The researchers at Siemens constructed tests for these programs by following a process described in Reference <ref> [19] </ref>; we paraphrase that description here. For each base program they created a large test pool containing possible tests for the program. <p> To create these test pools, they first created an initial set of black-box tests "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of . . . the code" <ref> [19, p. 194] </ref>, using the category partition method and the Siemens Test Specification Language tool [2, 27]. <p> Their goal was to introduce faults that were as realistic as possible, based on their experience with real programs. To obtain meaningful results, the researchers retained only faults that were "neither too easy nor too difficult to detect" <ref> [19, p. 196] </ref>, which they defined as being detectable by at least three and at most 350 tests in the test pool associated with each program. Ten people performed the fault seeding, "mostly without knowledge of each other's work" [19, p. 196]. <p> faults that were "neither too easy nor too difficult to detect" <ref> [19, p. 196] </ref>, which they defined as being detectable by at least three and at most 350 tests in the test pool associated with each program. Ten people performed the fault seeding, "mostly without knowledge of each other's work" [19, p. 196]. For regression testing experiments, we can view the base and faulty modified versions of the subjects in either of two ways. First, we can consider the faulty modified versions of base programs to be ill-fated attempts to create modified versions of the base programs.
Reference: [20] <author> R. Johnson. </author> <title> Elementary Statistics. </title> <publisher> Duxbury Press, </publisher> <address> Belmont, CA, sixth edition, </address> <year> 1992. </year>
Reference-contexts: Results the seven subject programs. Each graph plots percentages of tests selected (vertical axis) against modified versions of the base program (horizontal axis). Results for each version are depicted by a box plot a standard statistical device for representing data sets <ref> [20] </ref>. In each box plot, the dashed crossbar represents the median percentage at which tests were selected over the 1000 test suites of that version. The box shows the range of percentages in which the middle 50% of the test selection results occurred (the interquartile range).
Reference: [21] <author> J. Laski and W. Szermer. </author> <title> Identification of program modifications and its applications in software maintenance. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1992, </booktitle> <pages> pages 282-290, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: At this point further traversal is unneeded; the algorithm returns test suite ft2,t3g. An analytical evaluation of our test selection algorithms [34] proves that they are safe for controlled regression testing. 3 Several other safe algorithms now exist <ref> [3, 8, 21, 38] </ref>; however, our basic algorithm is among the two most precise of these algorithms. 4 Our analysis also shows that our algorithms are at least as general as existing techniques, whether safe or not.
Reference: [22] <author> J.A.N. Lee and X. </author> <title> He. A methodology for test selection. </title> <journal> The Journal of Systems and Software, </journal> <volume> 13(1) </volume> <pages> 177-185, </pages> <month> September </month> <year> 1990. </year>
Reference: [23] <author> H.K.N. Leung and L. White. </author> <title> Insights into regression testing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1989, </booktitle> <pages> pages 60-69, </pages> <month> October </month> <year> 1989. </year>
Reference: [24] <author> H.K.N. Leung and L. White. </author> <title> Insights into testing and regression testing global variables. </title> <journal> Journal of Software Maintenance, </journal> <volume> 2 </volume> <pages> 209-222, </pages> <month> December </month> <year> 1990. </year>
Reference: [25] <author> H.K.N. Leung and L.J. White. </author> <title> A study of integration testing and software regression at the integration level. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1990, </booktitle> <pages> pages 290-300, </pages> <month> November </month> <year> 1990. </year>
Reference: [26] <author> H.K.N. Leung and L.J. White. </author> <title> A cost model to compare regression test strategies. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1991, </booktitle> <pages> pages 201-208, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Cost Models for Regression Test Selection Techniques To evaluate the costs and benefits of regression test selection techniques we require a cost model that accounts for the factors responsible for those costs and benefits. Leung and White <ref> [26] </ref> present such a cost model. Their model considers both test selection and coverage identification, so we adapt it to consider just the 3 Controlled regression testing is the practice of testing a modified version under conditions equivalent to those that were used to test the base program.
Reference: [27] <author> T.J. Ostrand and M.J. Balcer. </author> <title> The category-partition method for specifying and generating functional tests. </title> <journal> Communications of the ACM, </journal> <volume> 31(6), </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: create these test pools, they first created an initial set of black-box tests "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of . . . the code" [19, p. 194], using the category partition method and the Siemens Test Specification Language tool <ref> [2, 27] </ref>.
Reference: [28] <author> T.J. Ostrand and E.J. Weyuker. </author> <title> Using dataflow analysis for regression testing. </title> <booktitle> In Sixth Annual Pacific Northwest Software Quality Conference, </booktitle> <pages> pages 233-247, </pages> <month> September </month> <year> 1988. </year>
Reference: [29] <author> D. Rosenblum and G. Rothermel. </author> <title> An empirical comparison of regression test selection techniques. </title> <booktitle> In Proceedings of the International Workshop for Empirical Studies of Software Maintenance, </booktitle> <pages> pages 89-94, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34]. <p> We are currently investigating these issues. In addition to experiments with individual test selection algorithms, comparative experiments of various algorithms are also mandated. To date, only two such experiments have been reported in the literature <ref> [11, 29] </ref>. Such experiments should examine the relative cost-benefits of different families of test selection algorithms: for example, minimization versus safe algorithms. Such experiments should also examine the relative cost-benefits of algorithms within the same family, such as the various safe algorithms, or the various versions of our graph-walk-based algorithms.
Reference: [30] <author> D.S. Rosenblum and E.J. Weyuker. </author> <title> Using coverage information to predict the cost-effectiveness of regression testing strategies. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 23(3) </volume> <pages> 146-156, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34]. <p> As Leung and White state, this cost model makes simplifying assumptions. The model assumes equivalent test execution costs under regression test selection and retest all, and assumes constant costs for tests. Rosenblum and Weyuker <ref> [30] </ref> describe additional assumptions: the model assumes that costs can be expressed in equivalent units whereas in practice they can include a mixture of other factors such as CPU time, human effort, and equipment expenditures. <p> Given the observed potential for variations in test selection results, we would like to find plausible predictors that can tell us in advance, with a reasonable degree of certainty, whether or not test selection is likely to be effective. Toward this end, Rosenblum and Weyuker <ref> [30] </ref> propose coverage-based predictors for use in predicting the cost-effectiveness of selective regression testing strategies. Their predictors use the average percentage of test cases that execute covered entities|such as statements, branches, or functions|to predict the number of tests that will be selected when a change is made to those entities.
Reference: [31] <author> G. Rothermel. </author> <title> Efficient, effective regression testing using safe test selection techniques. </title> <type> Technical Report 96-101, </type> <institution> Clemson University, </institution> <month> January </month> <year> 1996. </year> <month> 32 </month>
Reference-contexts: Third, the technique must be efficient: its time and space requirements must be reasonable. Finally, the technique must be general: it must be applicable to a wide class of programs and modifications. We developed a family of regression test selection algorithms that traverse graphs to select tests <ref> [31, 32, 34] </ref>. Our most basic algorithm builds control flow graphs 1 for procedure P and modified version P 0 , collects test traces that associate tests in T with edges in the graph for P , and performs synchronous depth-first traversals of the two graphs. <p> To obtain control flow graphs, variable usage data, and test history information, we utilized the Aristotle program analysis system [14]. Given the functionality provided by Aristotle, the DejaVu implementation itself required only 1220 lines of C code. DejaVu and its interface with Aristotle are described in detail in Reference <ref> [31] </ref>. Cost Models for Regression Test Selection Techniques To evaluate the costs and benefits of regression test selection techniques we require a cost model that accounts for the factors responsible for those costs and benefits. Leung and White [26] present such a cost model.
Reference: [32] <author> G. Rothermel and M.J. Harrold. </author> <title> A safe, efficient algorithm for regression test selection. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1993, </booktitle> <pages> pages 358-367, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Third, the technique must be efficient: its time and space requirements must be reasonable. Finally, the technique must be general: it must be applicable to a wide class of programs and modifications. We developed a family of regression test selection algorithms that traverse graphs to select tests <ref> [31, 32, 34] </ref>. Our most basic algorithm builds control flow graphs 1 for procedure P and modified version P 0 , collects test traces that associate tests in T with edges in the graph for P , and performs synchronous depth-first traversals of the two graphs.
Reference: [33] <author> G. Rothermel and M.J. Harrold. </author> <title> Analyzing regression test selection techniques. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 22(8) </volume> <pages> 529-551, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: These techniques have been evaluated and compared analytically <ref> [33] </ref>, but only recently have attempts been made to evaluate or compare them empirically [6, 8, 11, 29, 30, 34, 40]. In previous work, we developed and described a new regression test selection technique [34]. <p> Step 5 addresses the test suite maintenance problem: the problem of updating and storing test information. Although each of these problems is significant we restrict our attention to the regression test selection problem. 2 Regression Test Selection Techniques In earlier work <ref> [33] </ref>, we developed a framework for analyzing regression test selection techniques that consists of four categories: inclusiveness, precision, efficiency, and generality. Inclusiveness measures the extent to which a technique selects tests from T that reveal faults in P 0 : a 100% inclusive technique is safe. <p> Efficiency measures the space and time requirements of a technique. Generality measures the ability of a technique to function in a practical and sufficiently wide range of situations. We used this framework to compare and evaluate existing code-based regression test selection techniques <ref> [33] </ref>. This analysis suggested a need for a regression test selection technique that possesses several qualifications. First, the technique must be safe: it must not exclude tests (from the original test suite) that if executed would reveal a fault in the modified program. <p> Controlled regression testing is discussed in detail in Reference <ref> [33] </ref>. 4 Recently, Ball [3] presented a family of control-flow-graph based algorithms that, building on our control-flow-graph based approach, achieve greater precision than our basic algorithm.
Reference: [34] <author> G. Rothermel and M.J. Harrold. </author> <title> A safe, efficient regression test selection technique. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 6(2) </volume> <pages> 173-210, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34]. <p> These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically [6, 8, 11, 29, 30, 34, 40]. In previous work, we developed and described a new regression test selection technique <ref> [34] </ref>. We proved that under certain well-defined conditions, our test selection algorithms exclude no tests (from the original 1 test suite) that if executed would reveal faults in the modified program. Under these conditions, our algo-rithms are safe, and their fault-detection abilities are equivalent to those of the retest-all approach. <p> Third, the technique must be efficient: its time and space requirements must be reasonable. Finally, the technique must be general: it must be applicable to a wide class of programs and modifications. We developed a family of regression test selection algorithms that traverse graphs to select tests <ref> [31, 32, 34] </ref>. Our most basic algorithm builds control flow graphs 1 for procedure P and modified version P 0 , collects test traces that associate tests in T with edges in the graph for P , and performs synchronous depth-first traversals of the two graphs. <p> Enhanced versions of these algorithms add data dependence information 2 to control flow graphs and use it to select tests more precisely. We further illustrate our basic algorithm by discussing a simple example of its operation; additional details can be found in Reference <ref> [34] </ref>. Figure 1 presents procedure avg, and a modified version of that procedure, avg2, in which statement S7 has erroneously been deleted and statement S5a has been added. <p> At this point further traversal is unneeded; the algorithm returns test suite ft2,t3g. An analytical evaluation of our test selection algorithms <ref> [34] </ref> proves that they are safe for controlled regression testing. 3 Several other safe algorithms now exist [3, 8, 21, 38]; however, our basic algorithm is among the two most precise of these algorithms. 4 Our analysis also shows that our algorithms are at least as general as existing techniques, whether <p> DejaVu analyzes whole programs by individually analyzing pairs of procedures from the old and new versions. (We chose this approach to simplify the implementation effort; it produces test selection results equivalent to those of our interprocedural-control-flow-graph based algorithm, although possibly at greater cost <ref> [34] </ref>.) In cases where variable declarations differ, the tool postpones test selection until it reaches occurrences of those variables, and treats the nodes that contain those occurrences as modified. <p> They then augmented this set with manually-created white-box tests to ensure that each executable statement, edge, and definition-use pair in the base program or its 5 Preliminary versions of Studies 1 and 4 were reported in Reference <ref> [34] </ref>; the versions reported here contain additional details, and employ precise measurements in cases where estimates were employed previously. 6 We modified some of the programs and versions slightly to enable their processing by Aristotle. 7 control flow graph was exercised by at least 30 tests. <p> Unix time command to measure costs, using the time for 3d as a measure of the cost of using retest-all, and summing the times for 1, 2, 3a, 3b, and 3e as a measure of the cost of performing test 12 For an initial version of this study, summarized in <ref> [34] </ref>, we could only estimate control flow graph construction costs, because our control flow graph constructor functioned only on a subset of the code in player.
Reference: [35] <author> B. Sherlund and B. Korel. </author> <title> Modification oriented software testing. </title> <booktitle> In Conference Proceedings: Quality Week 1991, </booktitle> <pages> pages 1-17, </pages> <year> 1991. </year>
Reference: [36] <author> B. Sherlund and B. Korel. </author> <title> Logical modification oriented software testing. </title> <booktitle> In Proceedings: Twelfth International Conference on Testing Computer Software, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [37] <author> A.B. Taha, S.M. Thebaut, and S.S. Liu. </author> <title> An approach to software fault localization and revalidation based on incremental data flow analysis. </title> <booktitle> In Proceedings of the 13th Annual International Computer Software and Applications Conference, </booktitle> <pages> pages 527-534, </pages> <month> September </month> <year> 1989. </year>
Reference: [38] <author> F. Vokolos and P. Frankl. </author> <title> Pythia: A regression test selection tool based on textual differencing. </title> <booktitle> In ENCRESS '97, Third International Conference on Reliability, Quality, and Safety of Software Intensive Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: At this point further traversal is unneeded; the algorithm returns test suite ft2,t3g. An analytical evaluation of our test selection algorithms [34] proves that they are safe for controlled regression testing. 3 Several other safe algorithms now exist <ref> [3, 8, 21, 38] </ref>; however, our basic algorithm is among the two most precise of these algorithms. 4 Our analysis also shows that our algorithms are at least as general as existing techniques, whether safe or not.
Reference: [39] <author> L.J. White and H.K.N. Leung. </author> <title> A firewall concept for both control-flow and data-flow in regression integration testing. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1992, </booktitle> <pages> pages 262-270, </pages> <month> November </month> <year> 1992. </year>
Reference: [40] <author> L.J. White, V. Narayanswamy, T. Friedman, M. Kirschenbaum, P. Piwowarski, and M. Oha. </author> <title> Test Manager: a regression testing tool. </title> <booktitle> In Proceedings of the Conference on Software Maintenance - 1993, </booktitle> <pages> pages 338-347, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: These techniques have been evaluated and compared analytically [33], but only recently have attempts been made to evaluate or compare them empirically <ref> [6, 8, 11, 29, 30, 34, 40] </ref>. In previous work, we developed and described a new regression test selection technique [34].
Reference: [41] <author> S.S. Yau and Z. Kishimoto. </author> <title> A method for revalidating modified programs in the maintenance phase. </title> <booktitle> In COMP-SAC '87: The Eleventh Annual International Computer Software and Applications Conference, </booktitle> <pages> pages 272-277, </pages> <month> October </month> <year> 1987. </year> <month> 33 </month>
References-found: 41

