URL: http://www.cs.umr.edu/techreports/93-05.ps
Refering-URL: http://www.cs.umr.edu/techreports/
Root-URL: 
Phone: 2  
Title: Parallel Algorithm Fundamentals and Analysis  
Author: CSC - BRUCE McMILLIN ? HANAN LUTFIYYA ?? GRACE TSAI and JUN-LIN LIU 
Keyword: Key Words: Algorithm Design, Embeddings, Speedup, Class NC, Reasoning  
Note: supported in part by the National Science Foundation under Grant Numbers MSS-9216479 and CDA-9222827, and, in part, from the Air Force Office of Scientific Research under contract number F49620-92-J-0546. supported in part by the National Sciences and Engineering Research Council of Canada (NSERC) under contract number OGP0138180-S365A2, and in part, from University of Western Ontario NSERC internal funding under contract number Z001A8-S365A1.  
Address: Rolla, MO 65401 USA  Ontario London, ONTARIO N6A 5B7 CANADA  
Affiliation: 1 Department of Computer Science University of Missouri-Rolla  Department of Computer Science University of Western  
Abstract: This paper appears, in its entirety, in the Preliminary Proceedings of the International Summer Institute on Parallel Computer Architectures, Languages, and Algorithms, July 5-10, 1993, Prague, Czech Republic. A revised version, which will appear in a volume published by the IEEE Computer Society Press, appears as Technical Report Number C.Sc. 93-17. available via anonymous ftp from "cs.umr.edu". 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G.M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> AFIPS Conference Proceedings, </booktitle> <volume> 30 </volume> <pages> 483-485, </pages> <year> 1967. </year>
Reference-contexts: Then the speedup is linear, as N grows, the speedup S = N . Between these two extremes, are two measures of what occurs when system bottlenecks, overhead, imperfect parallel decomposition occur. 11 Amdahl's law <ref> [1] </ref> treats every program as consisting of a sequential component s and a parallel component p = 1 s. The crucial observation is that a program's speedup will be limited, severely, by the amount of non-parallelizable code.
Reference: 2. <author> R. Apt and W. Roever. </author> <title> A proof system for communicating sequential processes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(3) </volume> <pages> 359-385, </pages> <year> 1981. </year>
Reference-contexts: The first is the sequential proofs of each individual process that makes assumptions about the effects of the communication commands. The second part is to ensure that the assumptions are "legitimate". This will be discussed later. This approach is taken in <ref> [2] </ref> and [25]. The second approach allows us to prove properties of the individual processes using the axioms and rules of inference applicable to the statements in the individual processes.
Reference: 3. <author> J. Backus. </author> <title> Can programming be liberated from the von neumann style? a functional style and its algebra of programs. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 613-641, </pages> <year> 1979. </year>
Reference-contexts: This is expressed below, in a version of matrix multiplication expressed in FP <ref> [3] </ref>.
Reference: 4. <author> W. Briggs. </author> <title> Multigrid Tutorial. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1987. </year>
Reference-contexts: Multgrid structure for N=16 Processors at the Finest Level Thus, solving the errors at a coarser level increases the speedup of the solution by damping out the errors faster, along with increasing convergence rate due to better guesses. As illustrated in Figure 21 and as described by <ref> [4] </ref>, there are many ways to implement the multi-grid idea. In the figure, level 0 represents the finest array of points, while level 3 is the coarsest. In the V-cycle, level 0 does a set number of iterations of equation 7, then passes its residuals to level 1.
Reference: 5. <author> E. M. Clarke, E. A. Emerson, and A. P. Sistla. </author> <title> Automatic verification of finite state concurrent systems using temporal logic specification. </title> <journal> ACM Transactions on Programming Language and Systems, </journal> <volume> 8(2) </volume> <pages> 244-263, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) [22, 33], since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics <ref> [6, 5, 30, 31, 15] </ref>. However, verification using sets of state sequences which represent the executions of a program is tedious and unnatural since all the possible interleavings of a program must be checked.
Reference: 6. <author> E.M. Clarke, </author> <title> E.A. Emerson, and A.P. Sistla. Automatic verification of finite state concurrent systems using temporal logic specifications:a practical approach. </title> <booktitle> 10th annual ACM Symposium on Principles of Programming Language, </booktitle> <pages> pages 117-126, </pages> <year> 1983. </year>
Reference-contexts: Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) [22, 33], since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics <ref> [6, 5, 30, 31, 15] </ref>. However, verification using sets of state sequences which represent the executions of a program is tedious and unnatural since all the possible interleavings of a program must be checked.
Reference: 7. <author> M. Clint. </author> <title> Program proving: </title> <journal> coroutines,. Acta Informatica, </journal> <volume> 2 </volume> <pages> 50-63, </pages> <year> 1973. </year>
Reference-contexts: This is done with use of "dummy" or auxiliary variables that relate program variables of one process to program variables of another. The need for such variables has been independently recognized by many. The first reference that shows the usefulness of auxiliary variables is found in <ref> [7] </ref>. Overall Proof Approach . As discussed before a CSP program is made up of component sequential processes executing in parallel. In general, to prove properties about the program, first properties of each component process are derived in isolation.
Reference: 8. <author> G. Cybenko, D. W. Krumme, and K. N. Venkataraman. </author> <title> Fixed hypercube embedding. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 35-39, </pages> <year> 1987. </year>
Reference-contexts: It has been known for a long time that the general graph embedding problem (i.e., subgraph isomorphism problem) is NP-complete. It was shown that the embedding of general graphs into the binary hypercube is also NP-complete <ref> [8] </ref>. However, with rich interconnection structure the hypercube contains as a subgraph many the regular structures (i.e., rings, two-dimensional meshes, higher-dimensional meshes, and almost complete binary trees). Most of the mapping research in these years has dealt with effectively simulating these regular structures in the hypercubes, (for example, [40]).
Reference: 9. <author> E. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1976. </year>
Reference-contexts: A sequential proof for it only proves facts about it running in isolation. With only one process running, communication commands deadlock. Thus, any predicate Q may be assumed to be true upon termination of a communication command because termination never occurs. The Law of the Excluded Miracle <ref> [9] </ref> states that the statement false should never be derived. This is the requirement to ensure a sound logic. The communication axiom does violate the Law of the Excluded Miracle.
Reference: 10. <author> E. Edmiston and R. A. Wagner. </author> <title> Parallelization of the dynamic programming algorithm for comparison of sequences. </title> <booktitle> In Proceedings of Int'l Conf. on Parallel Processing, </booktitle> <pages> pages 78-80, </pages> <year> 1987. </year>
Reference-contexts: A parallel version of the dynamic programming algorithm is quite straightforward to derive <ref> [10] </ref>.
Reference: 11. <author> M. Fischer. </author> <title> A theoretician's view of fault tolerant distributed computing. Fault-Tolerant Distributed Computing, </title> <booktitle> Lecture Notes in Computer Science 448, </booktitle> <pages> pages 1-9, </pages> <year> 1990. </year>
Reference-contexts: Separating the problem from its solution is an important contribution of having a theoretical foundation in that it opens the door to alternative solutions <ref> [11] </ref>. There are numerous examples of the use of mathematical models in the computer science literature. One example from the study of network topology is being able to compute the information carrying capacity of a network.
Reference: 12. <author> G. C. Fox and W. Furmaski. </author> <title> Load balancing loosely synchronous problems with a neural network. </title> <type> Technical report, </type> <institution> California Institute of Technology, </institution> <address> Pasedena, CA, </address> <month> February </month> <year> 1988. </year> <month> 65 </month>
Reference-contexts: Classically <ref> [12] </ref>, the goal of load balancing, given a process/communication digraph G (P; C), where P is the set of processes and C is the set of directed arcs C (i; j), is to find a partition G = G 0 [ G 1 [ ::: [ G T 1 of G
Reference: 13. <author> W. M. Gentleman. </author> <title> Some complexity results for matrix computations on parallel computers. </title> <journal> Journal of the ACM, </journal> <volume> 25(1) </volume> <pages> 112-115, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: Optimal Matrix Multiplication (in the abstract sense) As another mesh problem, consider Gentleman's Algorithm <ref> [13] </ref> which is an explicit parallel solution using a 2D mesh of processors to multiply two matrices. Assume we have N 2 processors arranged in an N fi N mesh.
Reference: 14. <author> J. Gustafson. </author> <title> Reevaluating amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: Thus the maximum speedup is lim S lim 1 N or S 2 no matter how many processors are used! These results seem disappointing. However, <ref> [14] </ref> in 1988 observed that programs are made parallel, for the most part, as they are have run times which grow as the problem scales. <p> Thus, we can calculate a scaled speedup S s , as S s = s + p Experimental results using this speedup measure report scaled speedups of 1020 on a 1024 processor machine <ref> [14] </ref>. There is still much debate, however, on the usefulness of this model. 12 2.2 Theoretical Basis for Speedup Given the two speedup models for S and S s given above, it is easy to calculate the speedup for a particular application.
Reference: 15. <author> T.A. Henzinger, Z. Manna, and A. Pnueli. </author> <title> Temporal proof methodologies for real-time systems. </title> <booktitle> 18th Annual ACM Symposium on Principles of Programming Language, </booktitle> <pages> pages 353-366, </pages> <year> 1990. </year>
Reference-contexts: Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) [22, 33], since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics <ref> [6, 5, 30, 31, 15] </ref>. However, verification using sets of state sequences which represent the executions of a program is tedious and unnatural since all the possible interleavings of a program must be checked.
Reference: 16. <author> C. Hoare. </author> <title> An axiomatic basis for computer programming. </title> <journal> Communications of the ACM, </journal> <volume> 12(10) </volume> <pages> 576-583, </pages> <year> 1969. </year>
Reference-contexts: The interpretation of the theorem is as follows: if P is true before the execution of S and if the execution of S terminates, then Q is true after the execution of S. P is said to be the precondition and Q the postcondition <ref> [16] </ref>. A statement, S, is partially correct with respect to the precondition P and a postcondition Q, if, whenever, P is true of S prior to execution, and if S terminates then Q is true of S after the execution of S terminates. <p> The following are common to all the axiomatic systems and apply to reasoning about sequential programs. The basis of the axiomatic approach to sequential programming can be found in <ref> [16] </ref>. The skip axiom is simple, since execution of the skip statement has no effect on any program or auxiliary variables. &lt; P &gt; skip &lt; P &gt; The axiom states that anything about the program and logical variables that holds before executing skip also holds after it has terminated.
Reference: 17. <author> C. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <year> 1978. </year>
Reference-contexts: Given that the only control we have in parallel programming, at the system level, is process creation and send/receive communication, all examples can be constructed using this primitive set of operations. Later we will present a more formal model of this in Hoare's CSP <ref> [17] </ref>. 1.2 PARALLEL SORTING Consider the problem of sorting an array a into ascending order using the a (very simple) Sequential Sorting Algorithm (Exchange Sort). 6 Sort N numbers a (1), a (2), ..., a (N ) into ascending order for i from 1 to N for j from 1 to <p> What is necessary is a way of classifying algorithms by their parallel complexity. The class N C is one such class. To explore the class N C, we need to first examine the fundamental nature of parallel processes. 2.3 CSP Hoare's model of concurrent programming, Communicating Sequential Processes (CSP) <ref> [17] </ref>, is a model reflecting properties that should be in all concurrent programming languages. It was not intended to be used as a programming language per se, but it does reflect Hoare's concerns of proving the correctness of programs. <p> Hoare has suggested the following three properties that every concurrent language should have: the ability to express parallelism, communication primitives and non-determinism. This section provides an informal brief description of the syntax and meaning of CSP commands. Full details of CSP are contained in <ref> [17] </ref>. Communicating Sequential Processes (CSP) was proposed as a preliminary solution to the problem of defining a synchronous message-based language. A CSP program consists of a static collection of processes. <p> Thus, P will hold if the repetition terminates. The repetition ends when no boolean guard is true, so :b 1 ^ :b 2 ^ ::: ^ :b n will also hold at that time. [25] does not have distributed termination which is contrary to Hoare's original version of CSP <ref> [17] </ref>. Distributed termination provides the means for automatic termination of a loop in one process because another process has terminated. It is assumed that termination of all loops occurs when all boolean guards are false. Example 5.
Reference: 18. <author> J. Hong. </author> <title> Computation, Computability, Similarity and Duality. </title> <publisher> Pittman, </publisher> <address> London, </address> <year> 1986. </year>
Reference-contexts: If the Turing Machine is the abstract computational model for a sequential program, what is the corresponding model for a concurrent program and how does this model relate to the sequential Turing Machine model? From <ref> [18] </ref>, the fundamental measures of complexity are parallel time, space, and sequential time. If we have an abstract model which provides these three measures, then we can succinctly define speedup and characterize classes of algorithms which are amenable to parallelism.
Reference: 19. <author> K. Hwang and Briggs F. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: If we assume that we can decompose the job into N parts, then speedup is just how much faster the decomposed job runs on N processors. Speedup measures address both the optimal and expected performance. Fig. 5. Speedup Models possible speedups. Minsky's conjecture <ref> [19] </ref> forms a lower bound on what we can reasonably expect from a parallel program. The key observation is that as N grows, the performance becomes dominated by system bottlenecks and communication. Thus, perhaps the best speedup, S is O (log 2 N ). <p> Techniques such as simulated annealing [27] provide good results, but are computationally complex. Parallel computing can help speed their evaluation. 3 Interconnection Networks and Embeddings In the presentation so far, we have assumed that all processors are connected to each other (a completely connected network). The crossbar switch <ref> [19] </ref> attempts to connect each processor to each other processor. However, the number of switch elements grows as the square of the number of processors, making this technology infeasible for large multicomputer networks. The bus interconnection [19], by contrast, is inexpensive, but exhibits a performance bottleneck as interprocessor communication grows. <p> The crossbar switch <ref> [19] </ref> attempts to connect each processor to each other processor. However, the number of switch elements grows as the square of the number of processors, making this technology infeasible for large multicomputer networks. The bus interconnection [19], by contrast, is inexpensive, but exhibits a performance bottleneck as interprocessor communication grows. Multistage interconnection networks attempt to minimize the cost of interconnecting processors by providing a subset of possible interconnection patterns between the processors, at any one time. Examples of multistage interconnection networks are shown in Figure 6.
Reference: 20. <institution> IBM. IBM Scalable PowerParallel System 9076-SP1, </institution> <year> 1993. </year>
Reference-contexts: The multistage interconnect is the basis for many commercial and research parallel processors such as PASM [38] and the IBM RS/6000-based POWER-PARALLEL System <ref> [20] </ref>. However, if we examine the examples of Section 1 the communication patterns between processors are all nearest neighbor. Indeed, the most natural parallel algorithms result from domain decomposition into spatially local communication patterns such as mesh, ring, or tree.
Reference: 21. <author> S. Katz and D. Peled. </author> <title> Interleaving set temporal logic. </title> <booktitle> 3rd Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 178-190, </pages> <year> 1987. </year>
Reference-contexts: Thus, many attempts have been made to design a temporal logic with appropriate formalism for distributed programs. That is the partial order approach <ref> [21, 34, 35, 33] </ref>, where the order is defined by the local events (events are executions of atomic operations) within a process and the events of sending and later receiving a message. All other events that are not related are arbitrary.
Reference: 22. <author> S. Katz and D. Peled. </author> <title> An efficient verification method for parallel and distributed programs. </title> <booktitle> Lecture Notes in Computer Science 354, </booktitle> <pages> pages 489-507, </pages> <year> 1988. </year>
Reference-contexts: Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) <ref> [22, 33] </ref>, since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics [6, 5, 30, 31, 15]. <p> Proof rules of ISTL This section presents the proof rules of ISTL fl ; for details the reader may refer to <ref> [22, 33] </ref>.
Reference: 23. <author> W. Kohler and K. Steiglitz. </author> <title> Characterization and theoretical comparison of branch-and-bound algorithms for permutation problems. </title> <journal> Journal of the ACM, </journal> <volume> 21(1) </volume> <pages> 140-156, </pages> <year> 1974. </year>
Reference-contexts: Theorem 33 states that if the branch and bound algorithm finds the minimal cost node as defined by the minimum cost function, which satisfies certain properties, then the minimal cost node is also the optimal node. Theorem 33. <ref> [23] </ref> Let s denote any node representing a minimal cost node, according to the cost function f , where f is monotonically nondecreasing. Then s is an optimal node in the search space. Theorem 34. [28] The optimization function, f, is a monotonic nondecreasing function as i increases.
Reference: 24. <author> E. Lander and J. P. Mesirov. </author> <title> Protein sequence comparison on a data parallel computer. </title> <booktitle> In Proceeding of the International Conf. on Parallel Processing, </booktitle> <pages> pages 257-263, </pages> <year> 1988. </year>
Reference-contexts: This dynamic programming algorithm can best be understood by considering the matrix C r;s = max &gt; &gt; &lt; 0 C r1;s + g where the gap constant g &lt; 0, and D is a correlation function between single elements <ref> [24] </ref>. A parallel version of the dynamic programming algorithm is quite straightforward to derive [10].
Reference: 25. <author> G.M. Levin and D. Gries. </author> <title> A proof technique for communicating sequential processes. </title> <journal> Acta Informatica, </journal> <volume> 15 </volume> <pages> 281-302, </pages> <year> 1981. </year>
Reference-contexts: The first is the sequential proofs of each individual process that makes assumptions about the effects of the communication commands. The second part is to ensure that the assumptions are "legitimate". This will be discussed later. This approach is taken in [2] and <ref> [25] </ref>. The second approach allows us to prove properties of the individual processes using the axioms and rules of inference applicable to the statements in the individual processes. <p> No system is more powerful than the other. However, there are very different approaches to thinking about the verification of the program and the applicability in a practical environment. The proof system presented in <ref> [25] </ref> is presented here for its relative ease of use. Axioms and Inference Rules Used For Sequential Reasoning . In addition to the axioms and inference rules of predicate logic, there is one axiom or inference rule for each type of statement, as well as some statement-independent inference rules. <p> Thus, P will hold if the repetition terminates. The repetition ends when no boolean guard is true, so :b 1 ^ :b 2 ^ ::: ^ :b n will also hold at that time. <ref> [25] </ref> does not have distributed termination which is contrary to Hoare's original version of CSP [17]. Distributed termination provides the means for automatic termination of a loop in one process because another process has terminated. It is assumed that termination of all loops occurs when all boolean guards are false. <p> Otherwise, this unrestricted use of auxiliary variables would destroy the soundness of the proof system. Hence, auxiliary variables are not necessary to the computation, but they are necessary for verification. The proof system in <ref> [25] </ref> allows for auxiliary variables to be global i.e. variables that can be shared between distinct processes. Global auxiliary variables (GAVs) are used to record part of the history of the communication sequence. Shared reference to auxiliary variables allow for assertions relating the different communication sequences.
Reference: 26. <author> H. Lutfiyya and B. McMillin. </author> <title> Comparison of three axiomatic proof systems. </title> <institution> UMR Department of Computer Science Technical Report CSC91-13, </institution> <year> 1991. </year>
Reference-contexts: These properties are then used to prove properties of the entire program. This is the approach of [39]. It has been shown <ref> [26] </ref> that it is irrelevant as to which axiomatic proof systems of program verification is chosen. This was done by showing that the axiomatic systems are equivalent in the sense that they allow us to prove the same properties. No system is more powerful than the other.
Reference: 27. <author> H. Lutfiyya, B. McMillin, P. Poshyanonda, and C. Dagli. </author> <title> Composite stock cutting through simulated annealing. </title> <journal> Journal of Mathematical and Computer Modeling, </journal> <volume> 16(1) </volume> <pages> 57-74, </pages> <year> 1992. </year>
Reference-contexts: However, parallel computers are useful in evaluating expensive hueristics for approximation to the solution of N P-Complete problems. Techniques such as simulated annealing <ref> [27] </ref> provide good results, but are computationally complex. Parallel computing can help speed their evaluation. 3 Interconnection Networks and Embeddings In the presentation so far, we have assumed that all processors are connected to each other (a completely connected network).
Reference: 28. <author> H. Lutfiyya, A. Sun, and B. McMillin. </author> <title> A fault tolerant branch and bound algorithm derived from program verification. </title> <booktitle> In IEEE Computers Software and Applications Conference(COMPSAC), </booktitle> <pages> pages 182-187, </pages> <year> 1992. </year>
Reference-contexts: Theorem 33. [23] Let s denote any node representing a minimal cost node, according to the cost function f , where f is monotonically nondecreasing. Then s is an optimal node in the search space. Theorem 34. <ref> [28] </ref> The optimization function, f, is a monotonic nondecreasing function as i increases. By Definition 32 and Theorem 34, once a solution is found, the manhattan distance decreases to zero resulting in k for some f (s i ) evaluated at level k. <p> In terms of fault tolerance, the idea of having a centralized control is discouraged because should the fault manifest itself within that particular processor appointed to be the controller, recovery would be impossible. We use as a model algorithm that of <ref> [28] </ref> The algorithm requires only workers to search the solution space. The initial task, s 0 , is assigned a designated worker 51 to work on. As time passes, more tasks are created. Each worker retains one task for himself and redistributes the rest to other idle workers.
Reference: 29. <author> M. Malek. </author> <title> Responsive systems: A challenge for the nineties. </title> <booktitle> In Proc. EUROMI-CRO'90, 16th Symp. in Microprocessing and Microprogramming, </booktitle> <pages> pages 622-628, </pages> <address> Amersterdam, The Netherlands, 1990. </address> <publisher> North Holland. </publisher>
Reference-contexts: From Lemma 36 and Theorem 33, we know that this minimal cost is the optimal cost. 2 5.3 Temporal Reasoning A system is responsive <ref> [29] </ref> if it responds to internal programs or external inputs in a timely, dependable and predictable manner. It is a necessity for a responsive system to manage initiation and termination of activities to meet the specified timing constraints. <p> The incorporation of real-time and fault tolerance into distributed parallel environments is a challenging task, while the specifications of the distributed system must be met within the deadlines in spite of the presence of failures. However, this integrated system or responsive computer system <ref> [29] </ref> can greatly benefit from the application of formal methods. Without formal techniques, life-critical distributed computer control programs cannot be relied on to produce correct results, in time, and, in the presence of failures.
Reference: 30. <author> Z. Manna and A. Pnueli. </author> <title> Verification of concurrent programs: The temporal proof principles. </title> <booktitle> Lecture Notes in Computer Science 131, </booktitle> <pages> pages 215-273, </pages> <year> 1982. </year>
Reference-contexts: Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) [22, 33], since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics <ref> [6, 5, 30, 31, 15] </ref>. However, verification using sets of state sequences which represent the executions of a program is tedious and unnatural since all the possible interleavings of a program must be checked.
Reference: 31. <author> Z. Manna and A. Pnueli. </author> <title> Verification of concurrent programs: A temporal proof system. </title> <booktitle> Lecture Notes in Computer Science 354, </booktitle> <year> 1989. </year>
Reference-contexts: Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) [22, 33], since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics <ref> [6, 5, 30, 31, 15] </ref>. However, verification using sets of state sequences which represent the executions of a program is tedious and unnatural since all the possible interleavings of a program must be checked.
Reference: 32. <author> S. Owicki and D. Gries. </author> <title> An axiomatic proof technique for parallel programs I. </title> <journal> Acta Informatica, </journal> <volume> 6 </volume> <pages> 319-340, </pages> <year> 1976. </year>
Reference-contexts: This necessitates the need for a Proof of Non-interference. This consists of showing that for each assertion T in process i , it must be shown that T is invariant over any parallel execution. This is the non-interference property of <ref> [32] </ref>. Asynchronous Message Passing Systems The proof systems that have been discussed up to this point are designed for synchronous programming primitives. Our work uses an extension of work discussed in [37].
Reference: 33. <author> D. Peled and A. Pnueli. </author> <title> Proving partial order liveness properties. </title> <booktitle> 17th Colloquium on Automata, Language and Programming, </booktitle> <pages> pages 553-571, </pages> <year> 1990. </year>
Reference-contexts: The liveness assertion (p ! EFq) ensures what values the program variables must possess eventually at a state with assertion q, starting from a state with assertion p. These assertions are derived from the temporal proof system of Interleaving Set Temporal Logic <ref> [33] </ref>, and are used to reason about progress property from one communication point to another. <p> Interleaving Set Temporal Logic Among many mathematical models, we choose Interleaving Set Temporal Logic (ISTL fl ) <ref> [22, 33] </ref>, since it is capable of representing intermediate behavior of distributed programs. 58 Traditionally, concurrent and distributed programs are verified using variants of temporal logics with interleaving semantics [6, 5, 30, 31, 15]. <p> Thus, many attempts have been made to design a temporal logic with appropriate formalism for distributed programs. That is the partial order approach <ref> [21, 34, 35, 33] </ref>, where the order is defined by the local events (events are executions of atomic operations) within a process and the events of sending and later receiving a message. All other events that are not related are arbitrary. <p> Proof rules of ISTL This section presents the proof rules of ISTL fl ; for details the reader may refer to <ref> [22, 33] </ref>.
Reference: 34. <author> S.S. Pinter and P. Wolper. </author> <title> A temporal logic for reasoning about partially ordered computations. </title> <booktitle> 3th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 28-37, </pages> <year> 1984. </year> <month> 66 </month>
Reference-contexts: Thus, many attempts have been made to design a temporal logic with appropriate formalism for distributed programs. That is the partial order approach <ref> [21, 34, 35, 33] </ref>, where the order is defined by the local events (events are executions of atomic operations) within a process and the events of sending and later receiving a message. All other events that are not related are arbitrary.
Reference: 35. <author> W. Reisig. </author> <title> Toward a temporal logic for casuality and choice in distributed systems,. </title> <editor> In J.W. de Bakker, W.P. Roever, and G. Rozenburg, editors, </editor> <title> Models of Concur-rency:Linear, Branching and Partial Orders, </title> <booktitle> LNCS354, </booktitle> <pages> pages 603-627. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Thus, many attempts have been made to design a temporal logic with appropriate formalism for distributed programs. That is the partial order approach <ref> [21, 34, 35, 33] </ref>, where the order is defined by the local events (events are executions of atomic operations) within a process and the events of sending and later receiving a message. All other events that are not related are arbitrary.
Reference: 36. <author> D. Riggins, B. McMillin, M. Underwood, L. Reeves, and E. Lu. </author> <title> Modeling of supersonic combustor flows using parallel computing. </title> <booktitle> Computer Systems in Engineering, </booktitle> <volume> 3 </volume> <pages> 217-219, </pages> <year> 1992. </year>
Reference-contexts: Consider the mode fluids problem <ref> [36] </ref> of cavity-driven flow whose physical domain chosen is shown in Figure 10. The pair of non-linear coupled differential equations 1,2 that describe this flow are easily solved sequentially using a standard second-order central differencing scheme.
Reference: 37. <author> R. Schlichting and F. Schneider. </author> <title> Using message passing for distributed programming: Proof rules and disciplines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 402-431, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: This is the non-interference property of [32]. Asynchronous Message Passing Systems The proof systems that have been discussed up to this point are designed for synchronous programming primitives. Our work uses an extension of work discussed in <ref> [37] </ref>. The work of [37] describes how to extend the notion of a "satisfaction proof" and "non-interference proof" for asynchronous message-passing primitives. <p> This is the non-interference property of [32]. Asynchronous Message Passing Systems The proof systems that have been discussed up to this point are designed for synchronous programming primitives. Our work uses an extension of work discussed in <ref> [37] </ref>. The work of [37] describes how to extend the notion of a "satisfaction proof" and "non-interference proof" for asynchronous message-passing primitives.
Reference: 38. <editor> H.J. Siegel and et. al. Pasm: </editor> <title> A partionable smid/mimd system for image processing and pattern recognition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30:934-947, </volume> <month> December </month> <year> 1981. </year>
Reference-contexts: Thus, each processor can communicate with each other processor using n hops in the switch, however, as mentioned above, only a subset of simultaneous connections are possible. The multistage interconnect is the basis for many commercial and research parallel processors such as PASM <ref> [38] </ref> and the IBM RS/6000-based POWER-PARALLEL System [20]. However, if we examine the examples of Section 1 the communication patterns between processors are all nearest neighbor. Indeed, the most natural parallel algorithms result from domain decomposition into spatially local communication patterns such as mesh, ring, or tree.
Reference: 39. <author> N. Soundararahan. </author> <title> Axiomatic semantics of communicating sequential processes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(6) </volume> <pages> 647-662, </pages> <year> 1984. </year>
Reference-contexts: These properties are then used to prove properties of the entire program. This is the approach of <ref> [39] </ref>. It has been shown [26] that it is irrelevant as to which axiomatic proof systems of program verification is chosen. This was done by showing that the axiomatic systems are equivalent in the sense that they allow us to prove the same properties.
Reference: 40. <editor> Q. F. Stout. Hypercubes and pyramids. In V. Cantoni and S. Levialdi, editors, </editor> <booktitle> Pyramidal Systems for Computer Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: However, with rich interconnection structure the hypercube contains as a subgraph many the regular structures (i.e., rings, two-dimensional meshes, higher-dimensional meshes, and almost complete binary trees). Most of the mapping research in these years has dealt with effectively simulating these regular structures in the hypercubes, (for example, <ref> [40] </ref>). Let f be an embedding function which maps a guest graph G into a host graph H. jV G j denotes the cardinality of the set V G . Terminology related to the mapping problem are formally defined as follows. Definition 10.
References-found: 40

