URL: http://www.cse.ogi.edu/~mak/PS/icassp97.ps
Refering-URL: http://www.cse.ogi.edu/CSLU/personnel/bios/mak.html
Root-URL: http://www.cse.ogi.edu
Email: mak@cse.ogi.edu  
Title: COMBINING ANNS TO IMPROVE PHONE RECOGNITION  for Spoken Language Understanding  
Author: Brian Mak 
Address: 20000 N.W. Walker Road, Portland, OR 97006, USA.  
Affiliation: Center  Oregon Graduate Institute of Science and Technology  
Abstract: In applying neural networks to speech recognition, one often finds that slightly different training configurations lead to significantly different networks. Thus different training sessions using different setups will likely end up in "mixed" network configurations representing different solutions in different regions of the data space. This sensitivity to the initial weights assigned, the training parameters and the training data can be used to enhance performance, using a committee of neural networks. In this paper, we study various ways to combine context-dependent (CD) and context-independent (CI) neural network phone estimators to improve phone recognition. As a result, we obtain 6.3% and 2.2% increase in accuracy in phone recognition using monophones and biphones respectively. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barnard, R. A. Cole, M Fanty, and P. Vermeulen. </author> <title> "Real-World Speech Recognition With Neural Networks". </title> <editor> In R. J. Alspector and T. X. Brown, editors, </editor> <booktitle> Applications of Neural Networks to Telecommunications (IWANNT95), </booktitle> <volume> volume 2, </volume> <pages> pages 186-193. </pages> <publisher> Lawrence Erlbaum Assoc., </publisher> <address> Hillsdale, New Jersey, </address> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION In the past decade, a number of connectionist approaches have enabled a new computing paradigm for speech recognition with some success <ref> [1, 6, 12, 15] </ref>. In these ANN-based speech recognizers, the neural network is usually employed as an estimator for the posterior probabilities of phones or other subword units. Powerful as neural networks are, training neural networks that generalize well with unseen data remains an ongoing research topic.
Reference: [2] <author> L. Breiman. </author> <title> "Stacked Regressions". </title> <type> Technical report, </type> <institution> University of California, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: One way to improve neural-network estimation is to use an ensemble of neural networks <ref> [2, 7, 8] </ref>, hereafter called a neural network committee. In theory, the committee generalization error is guaranteed to be less than the (weighted) average of member-network errors; the smaller the correlation between the member networks, the smaller the committee error. <p> COMMITTEE PHONE RECOGNITION If the committee is to work, from Eq.(1), we see that the member errors have to be small and the member networks should exhibit a large variance. Usually this is achieved by using non-overlapping or partially-overlapping training data for the member networks; for example, crossnet <ref> [2] </ref>, bootnet [11] and bagging [3]. In this study, due to the limited amount of training data, partially-overlapping training data are derived for the two member networks as follows: out of the 208 OGI TS files, 30 are reserved for testing all member networks.
Reference: [3] <author> L. Breiman. </author> <title> "Bagging Predictors". </title> <type> Technical report, </type> <institution> University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Usually this is achieved by using non-overlapping or partially-overlapping training data for the member networks; for example, crossnet [2], bootnet [11] and bagging <ref> [3] </ref>. In this study, due to the limited amount of training data, partially-overlapping training data are derived for the two member networks as follows: out of the 208 OGI TS files, 30 are reserved for testing all member networks.
Reference: [4] <author> D. Burshtein. </author> <title> "Robust Parametric Modeling of Durations in Hidden Markov Models". </title> <booktitle> Proceedings of IEEE ICASSP, </booktitle> <pages> pages 548-551, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Table 1 summarizes the architecture and some training parameters for the two types of phone networks. 3.2. Phone Recognition Phone recognition is performed on the OGI TS test set using Viterbi search. CI decoding uses both a bigram language model and gamma duration models as described in <ref> [4] </ref>. CD decoding uses only a bigram language model plus the generalized biphones constraint. The various thresholds and weightings of the language model and duration models are determined empirically using the validation data.
Reference: [5] <author> R. A. Cole, M. Noel, T. Lander, and T. Durham. </author> <title> "New Telephone Speech Corpora at CSLU". </title> <booktitle> Proceedings of Eurospeech, </booktitle> <pages> pages 821-824, </pages> <month> Sep </month> <year> 1995. </year>
Reference: [6] <author> A. Waibel et al. </author> <title> "Phoneme Recognition Using Time-Delay Neural Networks". </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37(3) </volume> <pages> 328-339, </pages> <year> 1989. </year>
Reference-contexts: 1. INTRODUCTION In the past decade, a number of connectionist approaches have enabled a new computing paradigm for speech recognition with some success <ref> [1, 6, 12, 15] </ref>. In these ANN-based speech recognizers, the neural network is usually employed as an estimator for the posterior probabilities of phones or other subword units. Powerful as neural networks are, training neural networks that generalize well with unseen data remains an ongoing research topic.
Reference: [7] <author> L. K. Hansen and P. Salamon. </author> <title> "Neural Network Ensembles". </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Powerful as neural networks are, training neural networks that generalize well with unseen data remains an ongoing research topic. Neural network training is sensitive to the initial assignment of weights , the training parameters and most importantly the training data. As pointed out by Hansen and Salamon <ref> [7] </ref>, a significant problem lies in the many local minima of the objective function used in tuning the weights; thus different training sessions using different setups will likely end up in "mixed" configurations representing different solutions in different regions of the data space. <p> One way to improve neural-network estimation is to use an ensemble of neural networks <ref> [2, 7, 8] </ref>, hereafter called a neural network committee. In theory, the committee generalization error is guaranteed to be less than the (weighted) average of member-network errors; the smaller the correlation between the member networks, the smaller the committee error.
Reference: [8] <author> A. Krogh and J. Vedelsby. </author> <title> "Neural Network Ensembles, Cross Validation and Active Learning". </title> <editor> In D.S. Touretzky G. Tesauro and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 231-238. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: One way to improve neural-network estimation is to use an ensemble of neural networks <ref> [2, 7, 8] </ref>, hereafter called a neural network committee. In theory, the committee generalization error is guaranteed to be less than the (weighted) average of member-network errors; the smaller the correlation between the member networks, the smaller the committee error. <p> i (x) : mapping function learned by the i-th member network ! i : weighting of the i-th member network in forming the committee f (x) : mapping function of the committee t (x) : the true mapping function E [] : expectation Then it can be proved (see e.g. <ref> [8] </ref> for a detailed proof) that E (t (x) f (x)) 2 fl K X ! i E (t (x) f i (x)) 2 fl K X ! i E (f i (x) f (x)) 2 fl where, f (x) = P K P K i=1 ! i = 1: That
Reference: [9] <author> B. Mak and E. Barnard. </author> <title> "Phone Clustering Using The Bhattacharyya Distance". </title> <booktitle> Proceedings of ICSLP, </booktitle> <pages> pages 2005-2008, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The CI network has 40 OGIBET monophone outputs, while the CD network has 429 generalized biphone outputs; both are trained on the OGI TS [10] (telephone speech) Corpus. The generalized biphones are derived from the same database with a data-driven approach using the Bhattacharyya distance as described in <ref> [9] </ref>. 3.1. Training of the Phone Networks Both types of phone networks have 56 input nodes and 50 hidden nodes. The 56 inputs represent 7-th order PLP coefficients and normalized energies from seven successive frames centered around the frame under investigation.
Reference: [10] <author> Y.K. Muthusamy, R.A. Cole, and B. T. Oshika. </author> <title> "The OGI Multi-Language Telephone Speech Corpus". </title> <booktitle> Proceedings of ICSLP, </booktitle> <address> II:895-898, </address> <month> Oct </month> <year> 1992. </year>
Reference-contexts: BASELINE SYSTEM We have been working with both CI and CD phone networks for some time. The CI network has 40 OGIBET monophone outputs, while the CD network has 429 generalized biphone outputs; both are trained on the OGI TS <ref> [10] </ref> (telephone speech) Corpus. The generalized biphones are derived from the same database with a data-driven approach using the Bhattacharyya distance as described in [9]. 3.1. Training of the Phone Networks Both types of phone networks have 56 input nodes and 50 hidden nodes.
Reference: [11] <author> B. Parmanto, P. Munro, and H. Doyle. </author> <title> "Improving Committee Diagnosis with Resampling Techniques". </title> <editor> In M. Mozer D. S. Touretzky and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Usually this is achieved by using non-overlapping or partially-overlapping training data for the member networks; for example, crossnet [2], bootnet <ref> [11] </ref> and bagging [3]. In this study, due to the limited amount of training data, partially-overlapping training data are derived for the two member networks as follows: out of the 208 OGI TS files, 30 are reserved for testing all member networks.
Reference: [12] <author> T. Robinson and F. Fallside. </author> <title> "A Recurrent Error Propagation Network Speech Recognition System". </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5(3) </volume> <pages> 259-274, </pages> <year> 1991. </year>
Reference-contexts: 1. INTRODUCTION In the past decade, a number of connectionist approaches have enabled a new computing paradigm for speech recognition with some success <ref> [1, 6, 12, 15] </ref>. In these ANN-based speech recognizers, the neural network is usually employed as an estimator for the posterior probabilities of phones or other subword units. Powerful as neural networks are, training neural networks that generalize well with unseen data remains an ongoing research topic.
Reference: [13] <author> V. Tresp and M. Taniguchi. </author> <title> "Combining Estimators Using Non-Constant Weighting Functions". </title> <editor> In D.S. Touretzky G. Tesauro and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 419-426. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference: [14] <author> D. H. Wolpert. </author> <title> "Stacked Generalization". </title> <booktitle> In Neural Networks, </booktitle> <volume> volume 5, </volume> <pages> pages 241-259. </pages> <year> 1992. </year>
Reference: [15] <author> G. Zavaliagkos, Y. Zhao, R. Schwartz, and J. Makhoul. </author> <title> "Integration of Segmental Neural Nets with Hidden Markov Models". </title> <booktitle> Proceedings of ARPA/MTO CSR Workshop, </booktitle> <pages> pages 71-76, </pages> <year> 1991. </year>
Reference-contexts: 1. INTRODUCTION In the past decade, a number of connectionist approaches have enabled a new computing paradigm for speech recognition with some success <ref> [1, 6, 12, 15] </ref>. In these ANN-based speech recognizers, the neural network is usually employed as an estimator for the posterior probabilities of phones or other subword units. Powerful as neural networks are, training neural networks that generalize well with unseen data remains an ongoing research topic.
References-found: 15

