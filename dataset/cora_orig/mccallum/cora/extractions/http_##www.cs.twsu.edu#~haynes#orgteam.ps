URL: http://www.cs.twsu.edu/~haynes/orgteam.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Co-adaptation in a Team  
Author: Thomas D. Haynes and Sandip Sen 
Note: Research partially supported by NSF Research Initiative Award IRI-9410180. This is a preprint of an article that is going to appear in International Journal of Computational Intelligence and Organizations (IJCIO) sometime late 1996 or early 1997.  
Address: College Ave.  Tulsa, OK 74104-3189 USA  
Affiliation: 600 South  Department of Mathematical Computer Sciences, The University of Tulsa  
Abstract-found: 0
Intro-found: 1
Reference: <editor> Angeline, P. and Kinnear, Jr., K. E., editors (1996). </editor> <booktitle> Advances in Genetic Programming 2. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, USA. </address>
Reference: <author> Angeline, P. J. and Pollack, J. B. </author> <year> (1993). </year> <title> Competitive environments evolve better solutions for complex tasks. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 264-278. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Barbuceanu, M. and Fox, M. S. </author> <year> (1995). </year> <title> COOL: A language for describing coordination in multiagent systems. </title> <editor> In Lesser, V., editor, </editor> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 17-24, </pages> <address> San Francisco, CA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Benda, M., Jagannathan, V., and Dodhiawala, R. </author> <year> (1986). </year> <title> On optimal cooperation of knowledge sources an empirical investigation. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing Advanced Technology Center, Boeing Computing Services, </institution> <address> Seattle, Washington. </address> <note> Co-adaptation in a Team 24 Bond, </note> <editor> A. H. and Gasser, L., editors (1988). </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We believe that cooperative co-evolution provides opportunities to produce solutions to problems that cannot be solved with implicit communication. 3 Pursuit Domain We have used the predator-prey pursuit game <ref> (Benda et al., 1986) </ref> to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. This domain involves multiple predator agents trying to capture a mobile prey agent in a grid world by surrounding it. <p> We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies. The original version of the predator-prey pursuit problem was introduced by Benda, et al. <ref> (Benda et al., 1986) </ref> and consisted of four blue (predator) agents trying to capture a red (prey) agent by surrounding it from four directions on a grid-world. Agent movements were limited to either a horizontal or a vertical step per time unit. The movement of the prey agent was random.
Reference: <author> Bull, L. and Fogarty, T. C. </author> <year> (1996). </year> <title> Evolution in cooperative multiagent environments. </title> <editor> In Sen, S., editor, </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <pages> pages 22-27, </pages> <address> Stanford University, CA. </address>
Reference-contexts: In our work, we can view each team member to be receiving the same evaluation as the entire team. Our assumption of allowing sharing of genetic information between team members is also used by other GA researchers working on the problem of cooperative co-evolution <ref> (Bull and Fogarty, 1996) </ref>. Co-adaptation in a Team 19 with an ordering of (3241). (b) has Parent j with an ordering of (4123). (c) has Child s, with two branches created via crossover. (d) has Child t, with two branches created via crossover.
Reference: <author> Davis, L., </author> <title> editor (1991). Handbook of genetic algorithms. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY. </address>
Reference-contexts: Although we use an artificial problem domain, the methodology we investigate is clearly promising enough to address broader challenges in the future. Genetic algorithms were once much maligned for working with benchmark problem suites, but nowadays almost all applications are in engineering fields <ref> (Davis, 1991) </ref>. Our work will show the potential for using GPs in developing effectively coordinated multiagent systems. We plan to further investigate real-life agents like coordinated meeting schedulers and cooperative controllers with the proposed methodology. <p> optimal solutions (unlike Simulated Annealing algorithms), they still pos Co-adaptation in a Team 10 sess some nice provable properties (optimal allocation of trials to substrings, evaluating exponential number of schemas with linear number of string evaluations, etc.), and have been found to be useful in a number of practical applications <ref> (Davis, 1991) </ref>. Koza's work on Genetic Programming (Koza, 1992) was motivated by the representational constraint in traditional GAs.
Reference: <author> DeJong, K. A. </author> <year> (1990). </year> <title> Genetic-algorithm-based learning. </title> <editor> In Kodratoff, Y. and Michalski, R. S., editors, </editor> <booktitle> Machine Learning, Volume III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Alamos, CA. </address>
Reference-contexts: Each team member always participates in the same team. Thus all of the points it is awarded, for both its individual contribution and the teams contribution, are correctly apportioned to the entire team. This approach is similar to "the Pitt approach" used for evolving Genetic-Based Machine Learning systems <ref> (DeJong, 1990) </ref>.
Reference: <author> Finin, T., Fritzon, R., McKay, D., and McEntire, R. </author> <year> (1994). </year> <title> KQML A language and protocol for knowledge and information exchange. </title> <booktitle> In Proceedings of the 13th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 126-136, </pages> <address> Seatle, WA. </address>
Reference: <author> Fox, M. S. </author> <year> (1981). </year> <title> An organizational view of distributed systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 70-80. </pages> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 140-150, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference: <author> Gasser, L., Rouquette, N., Hill, R. W., and Lieb, J. </author> <year> (1989). </year> <title> Representing and using organizational knowledge in DAI systems. </title> <editor> In Gasser, L. and Huhns, M. N., editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 55-78. </pages> <publisher> Pitman. </publisher>
Reference-contexts: The goal of this problem was to show the effectiveness of nine organizational structures, with varying degrees of agent cooperation and control, on the efficiency with which the predator agents could capture the prey. The approach undertaken by Gasser et al. <ref> (Gasser et al., 1989) </ref> allowed for the predators to occupy and maintain a Lieb configuration (each predator occupying a different quadrant, where a quadrant is defined by diagonals Co-adaptation in a Team 8 intersecting at the location of the prey) while homing in on the prey.
Reference: <author> Gotwald, Jr., W. H. </author> <year> (1995). </year> <title> Army Ants: The Biology of Social Predation. </title> <institution> Cornell University. </institution>
Reference: <author> Grefenstette, J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery systems. </title> <journal> Machine Learning, </journal> 3(2/3):225-246. 
Reference-contexts: it fair to evenly divide the score? Assuming k members to a team, if the actions of one individual accounted for a large share of the team's score, why should it only get 1 k th of the score? This problem is the same as the credit assignment problem in <ref> (Grefenstette, 1988) </ref>. Another way to create teams is to deterministically split the population into k sized teams. Thus the first k individuals would always form the first team. The problem with this is that it imposes an artificial ordering on the population.
Reference: <author> Grefenstette, J. and Daley, R. </author> <year> (1996). </year> <title> Methods for competitive and cooperative co-evolution. </title> <editor> In Sen, S., editor, </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <pages> pages 45-50, </pages> <address> Stanford University, CA. </address> <note> Co-adaptation in a Team 25 Haynes, </note> <author> T., Lau, K., and Sen, S. </author> <year> (1996). </year> <title> Learning cases to compliment rules for conflict resolution in multiagent systems. </title> <editor> In Sen, S., editor, </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <pages> pages 51-56, </pages> <address> Stanford University, CA. </address>
Reference: <author> Haynes, T. and Sen, S. </author> <year> (1996). </year> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In Wei, G. and Sen, S., editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 113-126. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Since the agents shared the same behavioral strategy, they were homogeneous, i.e., each population member represented the program of k agents. A simple algorithm to model the actions of others is to believe that they behave as you would in the same situation <ref> (Haynes et al., 1996) </ref>. With homogeneous agents, the agents can employ this algorithm since their models of other agents matches the actions of others. A key issue in DAI research is how can heterogeneous agents cooperate to form a successful team. <p> One of the observed behaviors in both the evolved homogeneous and hand-crafted behavioral strategies is that if two predators were in the same quadrant, then they would select the same action <ref> (Haynes et al., 1996) </ref>. This behavior would lead to deadlock situations, for example if predators 1 and 2 are lined up on the horizontal axis with respect to the prey P, then the predator stuck behind the other one cannot get to a capture position.
Reference: <author> Haynes, T., Sen, S., Schoenefeld, D., and Wainwright, R. </author> <year> (1995a). </year> <title> Evolving multiagent coordination strategies with genetic programming. </title> <type> Technical Report UTULSA-MCS-95-04, </type> <institution> The University of Tulsa. </institution>
Reference: <author> Haynes, T., Wainwright, R., Sen, S., and Schoenefeld, D. </author> <year> (1995b). </year> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <editor> In Eshel-man, L., editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 271-278, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Current research techniques in developing coordination strategies are mostly off-line mechanisms that use extensive domain knowledge to design from scratch the most appropriate cooperation strategy. In most cases a coordination strategy is chosen if it is reasonably good. In <ref> (Haynes et al., 1995b) </ref>, we presented a new approach for developing coordination strategies for multiagent problem solving situations, which is different from most of the existing techniques for constructing coordination strategies in two ways: * Strategies for coordination are incrementally constructed by repeatedly solving problems in the domain, i.e., on-line. *
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adpatation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: Such mutual adaptation and learning is of paramount importance to designers of agent groups and societies (Wei and Sen, 1996; Sen, 1996). Genetic programming (GP) (Koza, 1992) is an offshoot of genetic algorithms (GA) <ref> (Holland, 1975) </ref>, and effectively performs an implicit parallel search through the problem space. A population composed of random programs, or chromosomes, are constructed out of a domain specific language. Each chromosome can be evaluated by a domain specific fitness function. <p> problem is deceptively simple: if the agents have no memory and are no allowed communication, there exist simple prey strategies (such as sit still or move in a straight line) which consistently evade capture by the predator strategies. 4 Evolving Coordination Strategies 4.1 Genetic Programming Holland's work on adaptive systems <ref> (Holland, 1975) </ref> produced a class of biologically inspired algorithms known as genetic algorithms that can manipulate and develop solutions to optimization, learning, and other types of problems.
Reference: <editor> Kinnear, Jr., K. E., editor (1994). </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Knight, L. and Sen, S. </author> <year> (1995). </year> <title> PLEASE: A prototype learning system using genetic algorithms. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 429-435. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The Pitt approach bypasses the credit assignment problem, in that rules are only evaluated in the context of a ruleset. A similar mechanism as proposed in this paper has been used to successfully co-evolve a set of prototypes for supervised concept classification Co-adaptation in a Team 16 problems <ref> (Knight and Sen, 1995) </ref>.
Reference: <author> Korf, R. E. </author> <year> (1992). </year> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194. </pages>
Reference-contexts: In their research, the predator and prey agents took turns in making their moves. We believe this is not very realistic. A more realistic scenario is for all agents to choose their actions concurrently. This will introduce significant uncertainty and complexity into the problem. Korf <ref> (Korf, 1992) </ref> claims in his research that a discretization of the continuous world that allows only horizontal and vertical movements is a poor approximation. He calls this the orthogonal game. Korf developed several greedy solutions to problems where eight predators are allowed to move orthogonally as well as diagonally.
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> Co-adaptation in a Team 26 Koza, </note> <author> J. R. </author> <year> (1994). </year> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In this research we consider the latter problem and investigate a domain in which simple greedy behavior must be adapted to cooperative group behavior. Such mutual adaptation and learning is of paramount importance to designers of agent groups and societies (Wei and Sen, 1996; Sen, 1996). Genetic programming (GP) <ref> (Koza, 1992) </ref> is an offshoot of genetic algorithms (GA) (Holland, 1975), and effectively performs an implicit parallel search through the problem space. A population composed of random programs, or chromosomes, are constructed out of a domain specific language. Each chromosome can be evaluated by a domain specific fitness function. <p> Koza's work on Genetic Programming <ref> (Koza, 1992) </ref> was motivated by the representational constraint in traditional GAs.
Reference: <author> Krebs, J. R. and Davies, N. B. </author> <year> (1993). </year> <title> An Introduction to Behavioural Ecology. </title> <publisher> Blackwell Scientific Publications. </publisher>
Reference: <author> Lesser, V. R. </author> <year> (1995). </year> <title> Multiagent systems: An emerging subdiscipline of AI. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(3) </volume> <pages> 340-342. </pages>
Reference: <author> Levy, R. and Rosenschein, J. S. </author> <year> (1992). </year> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213. </pages>
Reference: <author> Malone, T. W. </author> <year> (1987). </year> <title> Modeling coordination in organizations and markets. </title> <journal> Management Science, </journal> <volume> 33(10) </volume> <pages> 1317-1332. </pages> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 151-158, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference: <author> Malone, T. W. </author> <year> (1990). </year> <title> Organizing information processing systems: Parallels between human organizations and computer systems. </title> <editor> In Robertson, S. P., Zachary, W., and Black, J. B., editors, </editor> <booktitle> Cognition, Computing, and Cooperation, </booktitle> <pages> pages 56-83. </pages> <publisher> Ablex Publishing Corporation. </publisher>
Reference: <author> Manela, M. and Campbell, J. A. </author> <year> (1993). </year> <title> Designing good pursuit problems as testbeds for Distributed AI: a novel application of Genetic Algorithms. </title> <booktitle> In Fifth European Workshop on Modelling Autonomous Agents in a Multi-Agent World, </booktitle> <address> Neuch^atel, Switzerland. </address>
Reference-contexts: Any ties are broken randomly. He claims this addition to the prey movements makes the problem Co-adaptation in a Team 9 considerably more difficult. Manela and Campbell investigated the utility of N fi M (predators fi prey) pursuit games as a testbed for DAI research. <ref> (Manela and Camp-bell, 1993) </ref> They utilized genetic algorithms to evolve parameters for decision modules. A difference between their domain and the others is that the grid is bounded, and not toroidal, i.e., the neighbors of a cell on the edge are those cells on the other edge.
Reference: <author> Montana, D. J. </author> <year> (1995). </year> <title> Strongly typed genetic programming. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 199-230. </pages>
Reference-contexts: Our approach for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm <ref> (Montana, 1995) </ref>, which is an extension of GP. To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions. <p> The set of all terminals is called the terminal set, and the set of all functions is called the function set. In traditional GP, all of the terminal and function set members must be of the same type. Montana <ref> (Montana, 1995) </ref> introduced STGP, in which the variables, constants, arguments, and returned values can be of any type.
Reference: <author> Mullen, T. and Wellman, M. P. </author> <year> (1995). </year> <title> A simple computational market for network information services. </title> <editor> In Lesser, V., editor, </editor> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> pages 283-289, </pages> <address> San Francisco, CA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Potter, M. A., Jong, K. A. D., and Grefenstette, J. J. </author> <year> (1995). </year> <title> A coevolutionary approach to learning sequential decision rules. </title> <editor> In Eshelman, L., editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 366-372. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher> <editor> Co-adaptation in a Team 27 Reynolds, C. W. </editor> <year> (1994). </year> <title> Competition, coevolution and the game of tag. </title> <booktitle> In Artificial Life IV. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The evaluation of a team is shared by all the members of the team. In the work of Potter et al., a team is formed by combining a member of the current subpop-ulation with the best members received from the other subpopulations <ref> (Potter et al., 1995) </ref>. The individual member of the subpopulation then receives an Co-adaptation in a Team 18 evaluation corresponding to the performance of the group thus formed. In our work, we can view each team member to be receiving the same evaluation as the entire team.
Reference: <author> Robbins, S. P. </author> <year> (1993). </year> <title> Organizational Behavior: Concepts, Controversies, and Applications. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Rosin, C. D. and Belew, R. K. </author> <year> (1995). </year> <title> Methods for competitive co-evolution: Finding opponents worth beating. </title> <editor> In Eshelman, L., editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 373-380. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Russell, S. and Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: 1 Introduction Russell and Norvig <ref> (Russell and Norvig, 1995) </ref> define an agent as anything which can be viewed as perceiving its environment through sensors and acting upon that environment through effectors. Insects, animals, robots, and humans all clearly fall into this definition.
Reference: <author> Sen, S. </author> <year> (1996). </year> <title> Adaptation, coevolution and learning in multiagent systems. Technical Report SS-96-01, </title> <publisher> AAAI Press, Stanford, </publisher> <address> CA. </address>
Reference: <author> Shaw, M. J. </author> <year> (1996). </year> <title> Cooperative problem solving and learning in multi-agent information systems. </title> <journal> International Journal of Computational Intelligence and Organizations. </journal>
Reference: <author> Singh, M. P. </author> <year> (1990). </year> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Working Papers of the 10th International Workshop on Distributed Artificial Intelligence. </booktitle>
Reference-contexts: This study, as well as the study by Singh <ref> (Singh, 1990) </ref> on using group intentions for agent coordination, lacks any experimental results that allow comparison with other work on this problem. Stephens and Merx (Stephens and Merx, 1989; Stephens and Merx, 1990) performed a series of experiments to demonstrate the relative effectiveness of three different control strategies.
Reference: <author> Stephens, L. M. and Merx, M. B. </author> <year> (1989). </year> <title> Agent organization as an effector of DAI system performance. </title> <booktitle> In Working Papers of the 9th International Workshop on Distributed Artificial Intelligence. </booktitle>
Reference: <author> Stephens, L. M. and Merx, M. B. </author> <year> (1990). </year> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop. </booktitle>
Reference: <author> Syswerda, G. </author> <year> (1989). </year> <title> Uniform crossover in genetic algorithms. </title> <editor> In Schaffer, J. D., editor, </editor> <booktitle> Proceedings of the 3rd International Conference on Genetic Algorithms, </booktitle> <pages> pages 2-9. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: TeamBranch This method is simply to pick one crossover point in the chromosome (see Figure 1). This is the traditional GP crossover mechanism. TeamUniform This crossover mechanism is to adapt the uniform crossover function from GA research <ref> (Syswerda, 1989) </ref> (see Figure 2). Basically Co-adaptation in a Team 17 we develop a uniform crossover mask for the programs inside a chromosome. A "1" indicates that the programs are copied into the respective child, while a "0" indicates that the programs will undergo crossover.
Reference: <author> Tanese, R. </author> <year> (1989). </year> <title> Distributed genetic algorithms. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 434-440, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman. </publisher> <editor> Co-adaptation in a Team 28 Wei, G. and Sen, S., editors (1996). </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems. Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Work in competitive co-evolution has also included island models, which involves evolving subpopulations with occasional migration <ref> (Tanese, 1989) </ref>. Our team strategies, TeamBranch and TeamUniform, may be thought of as cooperative co-evolution processes where each subpopulation consists of programs that represent one of the agents.
References-found: 40

