URL: ftp://ftp.eecs.umich.edu/people/hero/Preprints/paper1_r_dbl.ps.Z
Refering-URL: http://www.eecs.umich.edu/~hero/det_est.html
Root-URL: http://www.eecs.umich.edu
Title: Recursive Algorithms for Computing the Cramer-Rao Bound 1  
Author: Alfred O. Hero, Mohammad Usman, Anne C. Sauve, and Jeffrey A. Fessler 
Keyword: Key Words: Performance bounds, multi-dimensional parameter estimation, monotone matrix splitting iterations, Gauss-Seidel, preconditioned conjugate gradient.  
Date: SEPT. 1996 1  
Note: SUBMITTED TO IEEE TRANS. ON SIGNAL PROCESSING AUG. 1994 2ND REVISION SUBMITTED  
Abstract: Computation of the Cramer-Rao bound (CRB) on estimator variance requires the inverse or the pseudo-inverse Fisher information matrix (FIM). Direct matrix inversion can be computationally intractable when the number of unknown parameters is large. In this note we compare several iterative methods for approximating the CRB using matrix splitting and preconditioned conjugate gradient algorithms. For a large class of inverse problems we show that non-monotone Gauss-Seidel and preconditioned conjugate gradient algorithms require significantly fewer flops for convergence than monotone "bound preserving" algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. O. Hero and J. A. Fessler, </author> <title> "A recursive algorithm for computing CR-type bounds on estimator covariance," </title> <journal> IEEE Trans. on Inform. Theory, </journal> <volume> vol. 40, </volume> <pages> pp. 1205-1210, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The following iterative equation solvers are considered: monotone and non-monotone matrix splitting algorithms, such as the method of <ref> [1] </ref>, Ja-cobi, Gauss-Seidel, and preconditioned conjugate gradient algorithms. The extension of these algorithms to singular FIM is achieved by using matrix perturbation methods. We illustrate these algorithms for an important class of inverse problems arising in tomographic reconstruction, de-convolution, and image restoration. <p> ON SIGNAL PROCESSING AUG. 1994 2ND REVISION SUBMITTED SEPT. 1996 III. Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of <ref> [1] </ref> in the context of standard splitting iterations [7, Sec. 10.1], also known as stationary iterations [3], for approximating the solution to the linear equation (2). A. <p> A.2 Monotone Splitting (MS) Iterations Assume that F is symmetric positive definite and N = F F Y is symmetric non-negative definite. Then it follows from <ref> [1, Eq. (9)] </ref> that j (k+1) j (k) = _m T F 1 2 NF 1 2 _m 0. Assume also that the splitting algorithm (3) is initialized with fi (0) = 0. <p> Such a monotonic splitting (MS) algorithm yields a sequence of increasingly tight bounds on var ( ^ t). We can ensure that ae (F 1 N) = ae (I F 1 F Y ) &lt; 1 by selecting a matrix F for which F F Y is non-negative definite <ref> [1] </ref>. This was accomplished by selecting a diagonal matrix, denoted F EM , which was the FIM associated with a complete data space. We next present a general class of FIM-dominating split ting matrices F which ensure monotone convergence when fi (0) = 0. <p> Define the (2p 1)-diagonal banded matrix D p D p = Q + diag (jF Y Qj 1) : (4) where Q = ((f ij )) jijjp is a (2p 1)-diagonal banded matrix, F = ((f ij )), jAj = ((ja ij j)), 1 = <ref> [1; : : :; 1] </ref> T , and diag (x) is a diagonal matrix with the elements of the vector x along the diagonal. In particular, D 1 is a diagonal matrix with i-th diagonal element P n tridiagonal matrix [8]. <p> In this note we consider a positron emission tomography PET application similar to that considered in <ref> [1] </ref>. A down-sampled 32 fi 32 Hoffman brain phantom was used as the true image intensity . Here the unknown parameter vector consisted of a lexicographical ordering of the n = 640 pixel intensities within an ellipsoidal brain boundary. <p> A.2. These algorithms have convergence rates which improve with the number 2p1 of non-zero off-diagonal bands in D p . The monotone algorithm labeled EM uses the diagonal splitting matrix F = diag i i fli 1= i given in <ref> [1] </ref>, where A fli is the i-th column of A and diag i (a i ) denotes a diagonal matrix with the scalars a i arranged along the diagonal.
Reference: [2] <author> A. O. Hero and J. A. Fessler, </author> <title> "A fast recursive algorithm for computing CR-type bounds for image reconstruction problems," </title> <booktitle> in Proc. of IEEE Nuclear Science Symposium, </booktitle> <pages> pp. 1188-1190, </pages> <address> Orlando, FA, </address> <month> Oct. </month> <year> 1992. </year>
Reference: [3] <author> R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Don-garra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst, </author> <title> Templates for the solution of linear systems: building blocks for iterative methods, </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: ON SIGNAL PROCESSING AUG. 1994 2ND REVISION SUBMITTED SEPT. 1996 III. Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of [1] in the context of standard splitting iterations [7, Sec. 10.1], also known as stationary iterations <ref> [3] </ref>, for approximating the solution to the linear equation (2). A. Splitting Algorithms Let F and N be n fi n matrices which split F Y in the sense that F N = F Y . The matrix F is called a splitting matrix and is assumed to be non-singular. <p> 2 = [fi (k) ] T F Y [fi (k) ] is upper bounded by 2kfi (0) k F Y i p +1 , where is the spectral condition number of the matrix F 1 F Y , defined as the ratio of its largest to the smallest magnitude eigenvalues <ref> [3, Sec. 2.3.1] </ref>. V. CRB Approximation for Singular FIM The splitting iterations and CG algorithm described in the previous sections are only applicable to non-singular FIM F Y . <p> To implement this selection scheme the minimum positive singular value oe min of F Y must be available. In practice, oe min can be estimated using successive power iterations [7] or using a slightly modified implementation of the preconditioned conjugate gradient algorithm <ref> [3] </ref>. iterations. The limiting value of both of these algorithms is 41.3 which, as expected, lies below the true CR bound numerically calculated to be 43.5, by approximately 5%. Note that the GS algorithm has a highly oscillatory trajec 6 SUBMITTED TO IEEE TRANS.
Reference: [4] <author> A. O. Hero, J. A. Fessler, and M. Usman, </author> <title> "Exploring bias-variance tradeoffs using the uniform CR bound," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> vol. 44, </volume> <pages> pp. 2026-2042, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The CR bound on the variance of the estimator ^ t (Y ) is <ref> [4] </ref>, [5] Y () _m; (1) where _m = r m is the column gradient vector [ @m @ n ] T , and F + Y denotes the Moore-Penrose pseudo inverse [6]. When F Y is non-singular F + Y = F 1 Y the ordinary matrix inverse. <p> When F Y is non-singular F + Y = F 1 Y the ordinary matrix inverse. Note that the pseudo-inverse form of the CR bound is generally not generally achievable unless the vector _m lies in the range space of F Y <ref> [4] </ref>. Throughout this paper we will be interested in calculating the right hand side of (1). The method easily extends to calculation of the uniform lower bound presented in [4] for biased estimators. <p> of the CR bound is generally not generally achievable unless the vector _m lies in the range space of F Y <ref> [4] </ref>. Throughout this paper we will be interested in calculating the right hand side of (1). The method easily extends to calculation of the uniform lower bound presented in [4] for biased estimators. For non-singular FIM, the right hand side of the CR inequality (1) can be computed in O (n 3 ) flops by solving for x in the equation F Y x = _m: (2) 2 SUBMITTED TO IEEE TRANS.
Reference: [5] <author> C. R. Rao, </author> <title> Linear Statistical Inference and Its Applications, </title> <address> Wi-ley, New York, </address> <year> 1973. </year>
Reference-contexts: The CR bound on the variance of the estimator ^ t (Y ) is [4], <ref> [5] </ref> Y () _m; (1) where _m = r m is the column gradient vector [ @m @ n ] T , and F + Y denotes the Moore-Penrose pseudo inverse [6]. When F Y is non-singular F + Y = F 1 Y the ordinary matrix inverse.
Reference: [6] <author> F. A. Graybill, </author> <title> Matrices with Applications in Statistics, </title> <publisher> Wadsworth Publishing Co., </publisher> <address> Belmont CA, </address> <year> 1983. </year>
Reference-contexts: The CR bound on the variance of the estimator ^ t (Y ) is [4], [5] Y () _m; (1) where _m = r m is the column gradient vector [ @m @ n ] T , and F + Y denotes the Moore-Penrose pseudo inverse <ref> [6] </ref>. When F Y is non-singular F + Y = F 1 Y the ordinary matrix inverse. Note that the pseudo-inverse form of the CR bound is generally not generally achievable unless the vector _m lies in the range space of F Y [4].
Reference: [7] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations (2nd Edition), </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: ON SIGNAL PROCESSING AUG. 1994 2ND REVISION SUBMITTED SEPT. 1996 III. Recursive CR Bound Algorithms for Non-Singular FIM Here we describe the monotonically convergent algorithm of [1] in the context of standard splitting iterations <ref> [7, Sec. 10.1] </ref>, also known as stationary iterations [3], for approximating the solution to the linear equation (2). A. Splitting Algorithms Let F and N be n fi n matrices which split F Y in the sense that F N = F Y . <p> If ae (F 1 N) &lt; 1 then fi (k) converges to the vector F 1 Y _m and the approximating sequence j (k) converges to the CR bound _m T F 1 Y _m <ref> [7] </ref>. Many algorithms are splitting iterations, such as the Ja-cobi (J) and Gauss-Siedel (GS) iterations [7, Sec. 10.1]. <p> Many algorithms are splitting iterations, such as the Ja-cobi (J) and Gauss-Siedel (GS) iterations <ref> [7, Sec. 10.1] </ref>. Let the FIM have the additive decomposition F Y = D + U + L where D is diagonal and U and L are upper and lower triangular matrices with zero diagonal entries. <p> Such Fisher matrices F Y arise in many applications including the inverse problem considered in Sec. VI. IV. Preconditioned Conjugate Gradient Algorithm When the FIM F Y is positive definite, the preconditioned conjugate gradient (CG) algorithm can be used to approximate the solution x <ref> [7, Sec. 10.3] </ref> giving an approximation to the CR bound _m T x. The CG algorithm converges to the exact solution x in n iterations when run with HERO, USMAN, SAUVE AND FESSLER, "RECURSIVE ALGORITHMS FOR COMPUTING THE CR BOUND"3 infinite precision arithmetic. <p> However, when run to termination it is not computationally competitive with Gaussian elimination. We will show that with proper preconditioning matrix F the following prematurely stopped preconditioned CG algorithm <ref> [7, Algorithm 10.3.1] </ref> is quite competitive with direct methods and has significantly faster convergence than MS iterations. A. <p> is known that _m lies in the range space of F Y the CR bound _m T F + Y _m for singular F Y can be found in 4n 3 =3 flops using the QR factorization to solve for the min-norm solution x to F Y x = _m <ref> [7, Alg. 5.7.2] </ref>. However, typically the range space of F Y is unknown and much more computationally intensive algorithms are required, e.g. the singular value decomposition (SVD) (20n 3 flops). <p> To implement this selection scheme the minimum positive singular value oe min of F Y must be available. In practice, oe min can be estimated using successive power iterations <ref> [7] </ref> or using a slightly modified implementation of the preconditioned conjugate gradient algorithm [3]. iterations. The limiting value of both of these algorithms is 41.3 which, as expected, lies below the true CR bound numerically calculated to be 43.5, by approximately 5%.
Reference: [8] <author> A. O. Hero, M. Usman, A. Sauve, and J. A. Fessler, </author> <title> "Recursive algorithms for computing the Cramer-Rao bound," </title> <type> Technical report, </type> <institution> Comm. and Sig. Proc. Lab. (CSPL), Dept. EECS, University of Michigan, </institution> <address> Ann Arbor, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: In particular, D 1 is a diagonal matrix with i-th diagonal element P n tridiagonal matrix <ref> [8] </ref>. The following lemma follows directly from the diagonal dominance of the matrix D p F Y = diag (jF Y Qj 1) (F Y Q p ) [9, Corollary 7.2.1] and the easily verifiable fact that when F Y has non-negative entries: (D p F Y )1 = 0.
Reference: [9] <author> R. A. Horn and C. R. Johnson, </author> <title> Matrix Analysis, </title> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: In particular, D 1 is a diagonal matrix with i-th diagonal element P n tridiagonal matrix [8]. The following lemma follows directly from the diagonal dominance of the matrix D p F Y = diag (jF Y Qj 1) (F Y Q p ) <ref> [9, Corollary 7.2.1] </ref> and the easily verifiable fact that when F Y has non-negative entries: (D p F Y )1 = 0. Lemma 1: If F Y is an nfin symmetric matrix then D p F Y is non-negative definite.
Reference: [10] <author> S. D. Booth and J. A. Fessler, </author> <title> "Combined diagonal/fourier preconditioning methods for image reconstruction in emission tomography," </title> <booktitle> in IEEE Int. Conf. on Image Processing, </booktitle> <volume> volume 3, </volume> <pages> pp. 441-444, </pages> <address> Crystal City, VA, </address> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: The conjugate gradient algorithm labeled CGDF uses a special preconditioning matrix F consisting of a diagonal matrix, chosen to make F Y approximately circulant, followed by a Fourier-type preconditioner. The preconditioner used in CGDF is tailored to the spatially invariant PET application and is described in <ref> [10] </ref> in the context of fast least squares PET reconstruction algorithms. The GS algorithm shows very rapid convergence which is only slightly outdone by CGDF.
Reference: [11] <author> D. M. Young, </author> <title> Iterative solution of large linear systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Note that while the entries in the fourth column monotonically decrease as ae increases, the third column is not monotone decreasing. This illustrates the fact that the asymptotic convergence factor can be a poor predictor of the non-asymptotic behavior of matrix iterations <ref> [11] </ref>. Next we turn to the case of singular FIM arising in the so-called "missing angle problem" where image parameters must be estimated from a greatly reduced number and range of projections.
References-found: 11

