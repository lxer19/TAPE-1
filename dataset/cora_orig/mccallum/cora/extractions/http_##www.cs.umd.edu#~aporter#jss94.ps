URL: http://www.cs.umd.edu/~aporter/jss94.ps
Refering-URL: http://www.cs.umd.edu/~aporter/html/selected_pubs.html
Root-URL: 
Email: Internet: aporter@cs.umd.edu  
Title: Using Measurement-Driven Modeling to Provide Empirical Feedback to Software Developers  
Author: Adam A. Porter 
Date: October 1992  
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: Several authors have explored the application of classification methods to software development. These studies have concentrated on identifying modules that are difficult to develop or that have high fault density. While this information is important it provides little help in determining appropriate corrective action. This article extends previous work by applying one classification method, classification tree analysis (CTA), to more a fine-grained problem routinely encountered by developers. In this article we use CTA to identify software modules that have specific types of faults (i.e Logic, interface, etc.) We evaluate this approach using data collected from six actual software projects. Overall, CTA was able to correctly differentiate faulty modules from fault-free modules in 72% of the cases. Furthermore, 82% of the faulty modules were correctly identified. We also show that CTA outperformed two simpler classification strategies. 
Abstract-found: 1
Intro-found: 1
Reference: [BBT91] <author> Lionel C. Briand, Victor R. Basili, and William M. Thomas. </author> <title> Recognizing patterns for software development prediction and evaluation. </title> <booktitle> In Conference on Software Engineering Economics, </booktitle> <address> McLean, Virginia, </address> <month> May </month> <year> 1991. </year> <institution> Mitre Co. </institution>
Reference-contexts: In the following section we validate the the use of this approach, using data from actual software projects. 2 3 Validation Study Several authors have explored the application of different classification methods to software development. (See [MS90],[SP88], <ref> [BBT91] </ref>) These efforts have concentrated on coarse grained problems such as identifying modules that are difficult to develop or have high fault density. While this information is important, it provides little help in determining appropriate corrective action.
Reference: [BFOS84] <author> L. Breiman, J. Freidman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Monterey, CA, </address> <year> 1984. </year>
Reference-contexts: Although several classification approaches have been reported in the literature, we have focused classification trees because the models are straightforward to build and interpret. 2.1 Classification Tree Analysis Classification tree analysis is derived from the classification algorithms of ID3 [Qui86] and CART <ref> [BFOS84] </ref>. This approach provides an automated method for generating customizable, interpretable, models of software processes and objects. Unlike standard regression models, this approach places no constraints on its input data, providing a highly flexible framework for integrating symbolic and numeric attributes. The following paragraphs briefly outline the tree construction process.
Reference: [BP84] <author> V. R. Basili and B. T. Perricone. </author> <title> Software errors and complexity: An empirical investigation. </title> <journal> Communications of the ACM, </journal> <volume> 27(1) </volume> <pages> 42-52, </pages> <month> January </month> <year> 1984. </year>
Reference: [BSP83] <author> Victor R. Basili, Richard W. Selby, and Tsai Y. Phillips. </author> <title> Metric analysis and data validation across Fortran projects. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-9(6):652-663, </volume> <month> November </month> <year> 1983. </year>
Reference-contexts: For a discussion of the data validation methods, see <ref> [BSP83] </ref> and [BW84]. 3.3 Study Design There are 24 possible pairings of projects with fault types. For each of these pairings we determined the number of software modules in the corresponding project with the indicated type of fault.
Reference: [BW84] <author> V. R. Basili and D. M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(6):728-738, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: The data spans the beginning of design specification to the end of acceptance testing. Appendix A lists the 19 attributes. 3.2 Data Collection Methods and Analysis Tools The data collection and analysis for this study were based on a variety of data collection forms [BZM + 77], programmer interviews <ref> [BW84] </ref>, and static analysis tools [Dou87]. For a discussion of the data validation methods, see [BSP83] and [BW84]. 3.3 Study Design There are 24 possible pairings of projects with fault types. <p> lists the 19 attributes. 3.2 Data Collection Methods and Analysis Tools The data collection and analysis for this study were based on a variety of data collection forms [BZM + 77], programmer interviews <ref> [BW84] </ref>, and static analysis tools [Dou87]. For a discussion of the data validation methods, see [BSP83] and [BW84]. 3.3 Study Design There are 24 possible pairings of projects with fault types. For each of these pairings we determined the number of software modules in the corresponding project with the indicated type of fault.
Reference: [BZM + 77] <author> V. R. Basili, M. V. Zelkowitz, F. E. McGarry, Jr. R. W. Reiter, W. F. Truszkowski, and D. L. Weiss. </author> <title> The software engineering laboratory. </title> <type> Technical Report SEL-77-001, </type> <institution> Software Engineering Laboratory, NASA/Goddard Space Flight Center, Greenbelt, MD, </institution> <month> May </month> <year> 1977. </year> <month> 9 </month>
Reference-contexts: structure), 3 * Internal interface (module to module), * Logic / Control structure (flow of control incorrect, missing logic), and More complete definitions of these fault types can be found in [NAS90]. 3.1 Software Environment Data for this study was gathered from six moderate-sized projects developed in a NASA environment. <ref> [BZM + 77] </ref>[CMP + 82][McG82]. The software is used to control and simulate the operation of unmanned spacecraft. The systems range in size from 7000 to 34000 lines of Fortran source code. Between 5 and 140 person months were expended on their development over a period of 5-24 months. <p> The data spans the beginning of design specification to the end of acceptance testing. Appendix A lists the 19 attributes. 3.2 Data Collection Methods and Analysis Tools The data collection and analysis for this study were based on a variety of data collection forms <ref> [BZM + 77] </ref>, programmer interviews [BW84], and static analysis tools [Dou87]. For a discussion of the data validation methods, see [BSP83] and [BW84]. 3.3 Study Design There are 24 possible pairings of projects with fault types.
Reference: [CMP + 82] <author> D. N. Card, F. E. McGarry, J. Page, S. Eslinger, and V. R. Basili. </author> <title> The software engineering lab-oratory. </title> <type> Technical Report SEL-81-104, </type> <institution> Software Engineering Laboratory, NASA/Goddard Space Flight Center, Greenbelt, MD, </institution> <month> February </month> <year> 1982. </year>
Reference: [Dou87] <author> Dennis Lee Doubleday. Asap: </author> <title> An ada static source code analyzer program. </title> <type> Technical Report 1895, </type> <institution> Department of Computer Science, University of Maryland, College Park, Maryland, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: Appendix A lists the 19 attributes. 3.2 Data Collection Methods and Analysis Tools The data collection and analysis for this study were based on a variety of data collection forms [BZM + 77], programmer interviews [BW84], and static analysis tools <ref> [Dou87] </ref>. For a discussion of the data validation methods, see [BSP83] and [BW84]. 3.3 Study Design There are 24 possible pairings of projects with fault types. For each of these pairings we determined the number of software modules in the corresponding project with the indicated type of fault.
Reference: [McG82] <author> F. McGarry. </author> <title> Annotated bibliography of software engineering laboratory (sel)literature. </title> <type> Technical Report SEL-82-006, </type> <institution> Software Engineering Laboratory, NASA/Goddard Space Flight Center, Greenbelt, MD, </institution> <month> November </month> <year> 1982. </year>
Reference: [MIO87] <author> J. Musa, A. Iannino, and K. Okumoto. </author> <title> Software Reliability: Measurement, Prediction, and Application. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: [MS90] <author> R. Kent Madsen and Richard W. Selby. </author> <title> Metric-driven classification models for analyzing large-scale software. </title> <type> Technical report, </type> <institution> University of California, </institution> <year> 1990. </year> <note> (submitted for publication). </note>
Reference: [Mye79] <author> G. J. Myers. </author> <title> The Art of Software Testing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: A common view of the software lifecycle is that each phase consists of a construction activity followed by a review of the resulting products (i.e testing, analysis, inspections). Review processes offer significant opportunities for incremental process improvement because they account for a large expenditure of resources <ref> [Mye79] </ref>. Also, improved review processes will not disrupt later activities, while providing added assurance that upstream processes are performed adequately. To improve review processes, resources must be used as efficiently as possible. Several recent articles argue that review processes can be improved by systematically focusing effort on high-risk modules [She91][MIO87][BP84][Mye79].
Reference: [NAS90] <author> NASA. </author> <title> Software engineering laboratory (sel): Database organization and user's guide, revision 1. </title> <type> Technical Report SEL-89-101, </type> <institution> Software Engineering Laboratory, NASA/Goddard Space Flight Center, Greenbelt, MD, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: to isolate modules with the following four fault types: * Computational (fault in mathematical expression), * Data (value or structure), 3 * Internal interface (module to module), * Logic / Control structure (flow of control incorrect, missing logic), and More complete definitions of these fault types can be found in <ref> [NAS90] </ref>. 3.1 Software Environment Data for this study was gathered from six moderate-sized projects developed in a NASA environment. [BZM + 77][CMP + 82][McG82]. The software is used to control and simulate the operation of unmanned spacecraft.
Reference: [PS90] <author> Adam A. Porter and Richard W. Selby. </author> <title> Empirically guided software development using metric-based classification trees. </title> <journal> IEEE Software, </journal> <volume> 7(2) </volume> <pages> 46-54, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Unlike standard regression models, this approach places no constraints on its input data, providing a highly flexible framework for integrating symbolic and numeric attributes. The following paragraphs briefly outline the tree construction process. For a complete description of the classification tree algorithms, see <ref> [PS90] </ref>. The main goal of the classification tree process is to uncover relationships between a set of explanatory variables and a single dependent variable. The input to the tree construction process is a group of modules called the training set.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Journal of Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Although several classification approaches have been reported in the literature, we have focused classification trees because the models are straightforward to build and interpret. 2.1 Classification Tree Analysis Classification tree analysis is derived from the classification algorithms of ID3 <ref> [Qui86] </ref> and CART [BFOS84]. This approach provides an automated method for generating customizable, interpretable, models of software processes and objects. Unlike standard regression models, this approach places no constraints on its input data, providing a highly flexible framework for integrating symbolic and numeric attributes.
Reference: [She91] <author> Susan Sherer. </author> <title> A cost-effective approach to testing. </title> <journal> IEEE Software, </journal> <volume> 8(2) </volume> <pages> 34-40, </pages> <month> March </month> <year> 1991. </year>

References-found: 16

