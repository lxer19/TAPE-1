URL: ftp://ftp.cs.arizona.edu/reports/1993/TR93-12.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Title: Call Forwarding: A Simple Interprocedural Optimization Technique for Dynamically Typed Languages  
Author: Koen De Bosschere; Saumya K. Debray; David Gudeman; Sampath Kannan 
Note: TR 93-12  
Address: B-9000 Gent, Belgium  Tucson, AZ 85721, USA  
Affiliation: Electronics Laboratory Rijksuniversiteit Gent  Department of Computer Science The University of Arizona  
Abstract: This paper discusses call forwarding, a simple interprocedural optimization technique for dynamically typed languages. The basic idea behind the optimization is straightforward: find an ordering for the "entry actions" of a procedure, and generate multiple entry points for the procedure, such that the savings realized from different call sites bypassing different sets of entry actions, weighted by their estimated execution frequencies, is as large as possible. We show that the problem of computing optimal solutions to arbitrary call forwarding problems is NP-complete, and describe efficient heuristics for the problem. Experimental results indicate that (i) the heuristics are effective, in that the solutions produced are generally optimal or close to optimal; and (ii) the resulting optimization is effective, in that it leads to significant performance improvements for a number of benchmarks tested. fl The work of K. De Bosschere was supported by the National Fund for Scientific Research of Belgium and by the Belgian National incentive program for fundamental research in Artificial Intelligence, initiated by the Belgian State|Prime Minister's office|Science Policy Programming. The work of S. K. Debray and D. Gudeman was supported in part by the National Science Foundation under grant number CCR-9123520. The work of S. Kannan was supported in part by the National Science Foundation under grant number CCR-9108969. E-mail addresses: kdb@lem.rug.ac.be (K. De Bosschere), debray@cs.arizona.edu (S. K. Debray), gudeman@cs.arizona.edu (D. Gude-man), kannan@cs.arizona.edu (S. Kannan). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi and J. D. Ullman, </author> <booktitle> Compilers Principles, Techniques and Tools, </booktitle> <publisher> Addison 10 Wesley, </publisher> <year> 1986. </year>
Reference-contexts: cells, while Janus is a single assignment language where the program allocates new cons cells at each iteration|its performance can be attributed at least in part to the benefits of call forwarding. 6 Related Work The optimizations described here can be seen as generalizing some optimizations for traditional imperative languages <ref> [1] </ref>. In the special case of a (conditional or unconditional) jump whose target is a (conditional or unconditional) jump instruction, call forwarding generalizes the flow-of-control optimization that collapses chains of jump instructions. <p> able to deal with conditional jumps to conditional jumps (this turns out to be an important source of performance improvement in practice), while traditional compilers for imperative languages such as C and Fortran typically deal only with jump chains where there is at most one conditional jump (see, for example, <ref> [1] </ref>, p. 556). When we consider call forwarding for the last call in a recursive clause, what we get is essentially a generalization of code motion out of loops (e.g., see Section 4). <p> From the conventional definition of a "loop" in a flow graph (see, for example, <ref> [1] </ref>), there is one loop in the flow graph of this function that includes both the tail recursive call sites for f ().
Reference: [2] <author> T. Ball and J. Larus, </author> <title> "Optimally Profiling and Tracing Programs", </title> <booktitle> Proc. 19th. ACM Symp. on Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> Jan. </month> <year> 1992, </year> <pages> pp. 59-70. </pages>
Reference-contexts: Moreover, each call site has associated with it an estimate of its execution frequency: such estimates can be obtained from profile information, or from the structure of the call graph of the program <ref> [2, 13, 14, 21] </ref>.
Reference: [3] <author> M. Carlsson and J. Widen, </author> <title> SICStus Prolog User's Manual, </title> <institution> Swedish Institute of Computer Science, </institution> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog <ref> [3] </ref>, Strand [6], GHC [19] and Janus [11, 15], imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5].
Reference: [4] <author> C. Chambers and D. Ungar, </author> <title> "Iterative Type Analysis and Extended Message Splitting: Optimizing Dynamically Typed Object-Oriented Programs", </title> <booktitle> Proc. SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990, </year> <pages> pp. </pages> <month> 150-164. </month> <journal> SIGPLAN Notices vol. </journal> <volume> 25 no. </volume> <pages> 6. </pages>
Reference-contexts: in terms of the number of unnecessary entry actions that can be skipped, may not be as good for another call site, since in general, different call sites 1 In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [4, 5] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition. 1 have different information available about the actual parameters. <p> Figure 2 (a) gives the intermediate code that might be generated in a straightforward way. (In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [4, 5] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition.) The first three instructions of ave are entry actions that can be executed in any order. <p> However, we do not know of any system that attempts to order the entry actions carefully in order to maximize the savings from bypassing entry actions. Chambers and Ungar consider compile-time optimization techniques to reduce runtime type checking in dynamically typed object-oriented languages <ref> [4, 5] </ref>. Their approach uses type analysis to generate multiple copies of program fragments, in particular loop bodies, where each copy is specialized to a particular type and therefore can omit some type tests.
Reference: [5] <author> C. Chambers, D. Ungar and E. Lee, </author> <title> "An Efficient Implementation of SELF, A Dynamically Typed Object-Oriented Language Based on Prototypes", </title> <booktitle> Proc. OOPSLA '89, </booktitle> <address> New Orleans, LA, </address> <year> 1989, </year> <pages> pp. 49-70. </pages>
Reference-contexts: in terms of the number of unnecessary entry actions that can be skipped, may not be as good for another call site, since in general, different call sites 1 In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [4, 5] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition. 1 have different information available about the actual parameters. <p> a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus [11, 15], imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF <ref> [5] </ref>. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls. <p> Figure 2 (a) gives the intermediate code that might be generated in a straightforward way. (In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [4, 5] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition.) The first three instructions of ave are entry actions that can be executed in any order. <p> However, we do not know of any system that attempts to order the entry actions carefully in order to maximize the savings from bypassing entry actions. Chambers and Ungar consider compile-time optimization techniques to reduce runtime type checking in dynamically typed object-oriented languages <ref> [4, 5] </ref>. Their approach uses type analysis to generate multiple copies of program fragments, in particular loop bodies, where each copy is specialized to a particular type and therefore can omit some type tests.
Reference: [6] <author> I. Foster and S. Taylor, "Strand: </author> <title> A Practical Parallel Programming Tool", </title> <booktitle> Proc. 1989 North American Conference on Logic Programming, </booktitle> <address> Cleveland, Ohio, </address> <month> Oct. </month> <year> 1989, </year> <pages> pp. 497-512. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand <ref> [6] </ref>, GHC [19] and Janus [11, 15], imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5].
Reference: [7] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: It remains NP-complete even if every entry action has equal cost. Proof By reduction from the Optimal Linear Arrangement problem, which is known to be NP-complete <ref> [7, 8] </ref>. See the Appendix for details.
Reference: [8] <author> M. R. Garey, D. S. Johnson, and L. Stockmeyer, </author> <title> "Some Simplified NP-complete Graph Problems", </title> <journal> Theoretical Computer Science vol. </journal> <volume> 1, </volume> <pages> pp. 237-267, </pages> <year> 1976. </year>
Reference-contexts: It remains NP-complete even if every entry action has equal cost. Proof By reduction from the Optimal Linear Arrangement problem, which is known to be NP-complete <ref> [7, 8] </ref>. See the Appendix for details.
Reference: [9] <author> A. Goldberg and D. Robson, </author> <title> Smalltalk-80: The Language and its Implementation, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus [11, 15], imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk <ref> [9] </ref> and SELF [5]. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls.
Reference: [10] <author> R. E. Griswold and M. T. Griswold, </author> <title> The Implementation of the Icon Programming Language, </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1986. </year>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus [11, 15], imperative languages such as Icon <ref> [10] </ref> and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5]. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls.
Reference: [11] <author> D. Gudeman, K. De Bosschere, and S. K. Debray, </author> <title> "jc : An Efficient and Portable Implementation of Janus", </title> <booktitle> Proc. Joint International Conference and Symposium on Logic Programming, </booktitle> <address> Washington DC, Nov. 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus <ref> [11, 15] </ref>, imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5]. <p> The numbers presented reflect the performance of jc <ref> [11] </ref>, an implementation of a logic programming language called Janus [15] on a Sparcstation-1. 4 This system is currently available by anonymous FTP from cs.arizona.edu.
Reference: [12] <author> A. Houri and E. Shapiro, </author> <title> "A Sequential Abstract Machine for Flat Concurrent Prolog", in Concurrent Prolog: </title> <booktitle> Collected Papers, </booktitle> <volume> vol. 2, </volume> <editor> ed. E. </editor> <booktitle> Shapiro, </booktitle> <pages> pp. 513-574. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference: [13] <author> S. McFarling, </author> <title> "Program Optimization for Instruction Caches", </title> <booktitle> Proc. Third Int. Symp. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 183-191. </pages>
Reference-contexts: Moreover, each call site has associated with it an estimate of its execution frequency: such estimates can be obtained from profile information, or from the structure of the call graph of the program <ref> [2, 13, 14, 21] </ref>.
Reference: [14] <author> K. Pettis and R. C. Hansen, </author> <title> "Profile Guided Code Positioning", </title> <booktitle> Proc. SIGPLAN-90 Conf. on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990, </year> <pages> pp. 16-27. </pages>
Reference-contexts: Moreover, each call site has associated with it an estimate of its execution frequency: such estimates can be obtained from profile information, or from the structure of the call graph of the program <ref> [2, 13, 14, 21] </ref>.
Reference: [15] <author> V. Saraswat, K. Kahn, and J. Levy, </author> <title> "Janus: A step towards distributed constraint programming", </title> <booktitle> in Proc. 1990 North American Conference on Logic Programming, </booktitle> <address> Austin, TX, </address> <month> Oct. </month> <year> 1990, </year> <pages> pp. 431-446. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus <ref> [11, 15] </ref>, imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5]. <p> The numbers presented reflect the performance of jc [11], an implementation of a logic programming language called Janus <ref> [15] </ref> on a Sparcstation-1. 4 This system is currently available by anonymous FTP from cs.arizona.edu.
Reference: [16] <author> J. T. Schwartz, R. B. K. Dewar, E. Dubinsky, and E. Schonberg, </author> <title> Programming with Sets: An Introduction to SETL, </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus [11, 15], imperative languages such as Icon [10] and SETL <ref> [16] </ref>, and object-oriented languages such as Smalltalk [9] and SELF [5]. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls.
Reference: [17] <author> E. Shapiro, </author> <title> "The Family of Concurrent Logic Programming Languages", </title> <journal> Computing Surveys, </journal> <volume> vol. 21 no. 3, </volume> <month> Sept. </month> <year> 1989, </year> <pages> pp. 412-510. 11 </pages>
Reference: [18] <author> G. L. Steele Jr., </author> <title> Common Lisp: The Language, </title> <publisher> Digital Press, </publisher> <year> 1984. </year>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see <ref> [18] </ref>), logic programming languages such as Prolog [3], Strand [6], GHC [19] and Janus [11, 15], imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5].
Reference: [19] <author> K. Ueda, </author> <title> "Guarded Horn Clauses", in Concurrent Prolog: </title> <booktitle> Collected Papers, </booktitle> <volume> vol. 1, </volume> <editor> ed. E. </editor> <booktitle> Shapiro, </booktitle> <pages> pp. 140-156, </pages> <address> 1987. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This covers a wide variety of languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [18]), logic programming languages such as Prolog [3], Strand [6], GHC <ref> [19] </ref> and Janus [11, 15], imperative languages such as Icon [10] and SETL [16], and object-oriented languages such as Smalltalk [9] and SELF [5].
Reference: [20] <author> P. Van Roy, </author> <title> Can Logic Programming Execute as Fast as Imperative Programming?, </title> <type> PhD Dissertation, </type> <institution> University of California, Berkeley, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The idea of compiling functions with multiple entry points is not new: many Lisp systems do this, Yale Haskell generates dual entry points for its functions, and Aquarius Prolog generates multiple entry points for primitive operations <ref> [20] </ref>. However, we do not know of any system that attempts to order the entry actions carefully in order to maximize the savings from bypassing entry actions. Chambers and Ungar consider compile-time optimization techniques to reduce runtime type checking in dynamically typed object-oriented languages [4, 5].

References-found: 20

