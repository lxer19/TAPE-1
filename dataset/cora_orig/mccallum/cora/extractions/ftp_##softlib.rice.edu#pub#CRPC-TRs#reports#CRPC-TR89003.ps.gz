URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR89003.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F.L. </author> <title> Bauer (1974). "Computational Graphs and Rounding Errors", </title> <type> SINUM, </type> <institution> Vol.11, No.1, pp.87-96 </institution> . 
Reference-contexts: Then one can expect that the actually computed final value ~x m satisfies j~x m f (x)j i=1 jx i j ffix i : As shown by induction in <ref> [1] </ref> this inequality must hold if all functions f i are linear and the adjoint values x i are exact. Even though these two assumptions are rather unrealistic the right hand side above was found in [17] to provide a usually somewhat pessimistic upper bound on the total error.
Reference: [2] <author> W. Baur and V. </author> <title> Strassen (1983). "The Complexity of Partial Derivatives", </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 22, </volume> <month> pp.317-330. </month>
Reference-contexts: At least since the fifties these techniques have been developed by computational scientists in various fields, and several software implementations are now available. Although a theorem confirming Wolfe's assertion for rationals was published in 1983 by Baur and Strassen <ref> [2] </ref>, the optimization community took little notice of these developments. This can be partly explained by a lack of clarity in the customary terminology. Automatic differentiation is often confused with symbolic differentiation or even with the approximation of derivatives by divided differences.
Reference: [3] <editor> L.M. Beda et al (1959). </editor> <title> "Programs for Automatic Differentiation for the Machine BESM", Inst. Precise Mechanics and Computation Techniques, </title> <institution> Academy of Science, Moscow. </institution>
Reference-contexts: The literature relating to automatic differentiation is extensive and very diverse. The main stream of research and implementation has been concerned with the automatic evaluation of gradients ( or more generally truncated Taylor series ) in the forward mode. This effort goes back at least to Beda et al <ref> [3] </ref> in the Soviet Union and Wengert [30] in the United States. Numerous other references are contained in the paper by Kedem [21], the books by Rall [25] and Kagiwada et al [19], and the recent report by Fischer [11]. <p> It can be generated in a 'mechanical' fashion and is only about twice as long as the original program because each assignment to an intermediate quantity is simply augmented by the calculation of its gradient. This forward approach has been developed and advocated by several authors (See e.g. <ref> [3] </ref>, [30], [21], and [25]). Various software implementations will be discussed in Section 4. A simple count reveals that the calculation of our example gradient by the program above involves 1 2 n 2 nontrivial multiplications, so that q ' n=2.
Reference: [4] <author> D.G. </author> <month> Cacuci </month> <year> (1981). </year> <title> "Sensitivity Theory for Nonlinear Systems. I. Nonlinear Functional Analysis Approach", </title> <journal> Journal of Mathematical Physics, Vol.22, No.12, pp.2794-2802. </journal>
Reference-contexts: In general the researchers in this main stream were unaware of the reverse mode or continued to consider it as a somewhat obscure approach of a rather theoretical nature. Mathematically the reverse mode is closely related to adjoint differential equations. Nuclear engineers have long used adjoint sensitivity analysis <ref> [4] </ref>, [5] to evaluate the partial derivatives of certain system responses (e.g. the reactor temperature) with respect to thousands of design parameters. This approach yields all sensitivities simultaneously at a cost comparable to only a few reactor simulations.
Reference: [5] <author> D.G. </author> <month> Cacuci </month> <year> (1981). </year> <title> "Sensitivity Theory for Nonlinear Systems. II. Extension to Additional Classes of Responses", </title> <journal> Journal of Mathematical Physics, Vol.22, No.12, pp.2803-2812. </journal>
Reference-contexts: In general the researchers in this main stream were unaware of the reverse mode or continued to consider it as a somewhat obscure approach of a rather theoretical nature. Mathematically the reverse mode is closely related to adjoint differential equations. Nuclear engineers have long used adjoint sensitivity analysis [4], <ref> [5] </ref> to evaluate the partial derivatives of certain system responses (e.g. the reactor temperature) with respect to thousands of design parameters. This approach yields all sensitivities simultaneously at a cost comparable to only a few reactor simulations.
Reference: [6] <author> B.W.Char, K.O.Geddes, G.H.Gonnet, M.B.Monegan, </author> <title> and S.M.Watt (1988). "MAPLE Reference Manual, Fifth Edition", Symbolic Computation Group, </title> <institution> Department of Computer Science, University of Waterloo, Waterloo, </institution> <address> Ontario, Canada N2L 3G1. </address>
Reference-contexts: The entries in the first column represent the work ratio for divided differences, namely n + 1 with n being the number of variables. The three numbers in the second column were obtained as follows. The Helmholtz energy function f (x) was entered into the algebraic manipulation package MAPLE <ref> [6] </ref> and then differentiated symbolically using the grad command. On a Sun 3/140 with 16 megabytes real memory, the symbolic generation of the gradient always took several minutes, and when n was set to 30 the differentiation failed after 15 minutes due to a lack of memory space.
Reference: [7] <author> G. Di Pillo and L. </author> <month> Grippo </month> <year> (1986). </year> <title> "An Exact Penalty Method with Global Convergence Properties for Nonlinear Programming Problems", </title> <journal> SIAM J. Control Optim. Vol.23, pp.72-84. </journal>
Reference-contexts: Selected second derivatives of the Lagrangian occur in the gradient of smooth exact penalty functions <ref> [7] </ref> for constrained optimization. According to the second equation above, the desired vector of p + 1 st derivatives is the gradient of the dot product between v p and an analogous vector of p th derivatives. <p> If one wants the penalty functions to be exact, i.e. attain local minima right at the solutions of the constrained problem, then there are basically two choices. Either the penalty function is nonsmooth or it depends explicitly on the gradients of the objective and constraint functions <ref> [7] </ref>. In the latter case the resulting gradient and Hessian depend on second and third derivatives of the original problem functions respectively. Since this additional level of differentiation was thought to be unacceptable, nonsmooth penalty functions have generally been preferred.
Reference: [8] <author> L.C.W.Dixon and M.Mohseninia (1987). </author> <title> "The Use of the Extended Operations Set of ADA with Automatic Differentiation and the Truncated Newton Method", </title> <type> Technical Report No.176, </type> <institution> The Hatfield Polytechnic, Hatfield, U.K. </institution>
Reference-contexts: For example Hessian-vector products of the form r 2 f (x) v 1 can be used in the conjugate gradient method (See e.g. <ref> [8] </ref> and [20]). Second and third derivatives of the form r 2 f (x)v 1 v 2 and r 3 f (x)v 1 v 2 v 3 characterize the quadratic and cubic turning points [12] of bifurcation theory. <p> This does not require any extension or modification of the header file. Higher derivatives and some optimization of the computational graph can also be implemented by overloading. The forward evaluation of general and structured Hessians in the advanced language ADA is discussed by Dixon and Mohseninia in <ref> [8] </ref>.
Reference: [9] <author> L.C.W.Dixon (1987). </author> <title> "Automatic Differentiation and Parallel Processing in Optimisa-tion", </title> <type> Technical Report No.180, </type> <institution> The Hatfield Polytechnic, Hatfield, U.K. </institution>
Reference-contexts: Whenever two elementary functions do not directly or through intermediaries depend on each others result, they can be evaluated in either order or even concurrently on a parallel machine. This aspect has been examined in <ref> [9] </ref>, but will not be pursued any further here.
Reference: [10] <editor> Iu.G.Evtushenko (1982) . "Metody resheniia ekstremal'nykh zadach ikh primenenie v sistemakh optimizatsii", </editor> <publisher> Nauka Publishers, Moskow </publisher>
Reference-contexts: Consequently the work ratio for appropriate discretizations should be close to 2 and certainly below 5. In fact we may interpret reverse accumulation simply as a discrete analog of the classical adjoint equations from the calculus of variations and control theory <ref> [10] </ref>. Obviously the vector y need not be finite dimensional, and one can adopt the theoretical arguments and numerical techniques to more general evolution equations in Hilbert spaces.
Reference: [11] <author> H. </author> <title> Fischer (1987). "Automatic Differentiation: How to compute the Hessian matrix", </title> <type> Technical Report #104A, </type> <institution> Technische Universitat Munchen, Institut fur Angewandte Mathematik und Statistik. </institution> <month> 26 </month>
Reference-contexts: This effort goes back at least to Beda et al [3] in the Soviet Union and Wengert [30] in the United States. Numerous other references are contained in the paper by Kedem [21], the books by Rall [25] and Kagiwada et al [19], and the recent report by Fischer <ref> [11] </ref>. In general the researchers in this main stream were unaware of the reverse mode or continued to consider it as a somewhat obscure approach of a rather theoretical nature. Mathematically the reverse mode is closely related to adjoint differential equations. <p> itself. 16 Applying the complexity bound for the reverse mode separately to each component of the gradient one finds that workfr 2 f g n X work @x i Q i=1 work @x i : After division by workff g we obtain in agreement with the results in [17] and <ref> [11] </ref> workfr 2 f g workff g Q i=1 workf@f =@x i g workfrf g workfrf g workff g n Q 2 : In terms of powers of n this bound is unfortunately optimal, as one can see on the simple example f (x) = :5 [x T x + (a
Reference: [12] <author> A. Griewank and G.W. </author> <title> Reddien (1988). "Computation of Cusp Singularities for Opera--tor Equations and their Discretizations", </title> <type> Technical Memorandum ANL/MCS-TM-115, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne IL 60439. </institution> <note> To appear in the special issue on Continuation Techniques and Bifurcation Problems of the Journal of Computational and Applied Mathematics. </note>
Reference-contexts: Second and third derivatives of the form r 2 f (x)v 1 v 2 and r 3 f (x)v 1 v 2 v 3 characterize the quadratic and cubic turning points <ref> [12] </ref> of bifurcation theory. Moreover, the gradients of these scalars involve terms of the form r 3 f (x)~v 1 ~v 2 and r 4 f (x)~v 1 ~v 2 ~v 3 , which need be evaluated during the calculation of the turning points by Newton's method.
Reference: [13] <author> J.E. Horwedel, B.A. Worley, </author> <title> E.M. </title> <journal> Oblow, and F.G. </journal> <note> Pin (1988). "GRESS Version 0.0 Users Manual" ORNL/TM 10835, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, </institution> <address> Ten-nessee 37830, U.S.A. </address>
Reference-contexts: In a way this doubles up the structural information that is already contained in the program. 4.2 FORTRAN Precompiler There are at least three such implementations, namely JAKEF [14], GRESS <ref> [13] </ref>, and PADRE2 [17]. All three precompilers require the user to supply a source code for the 18 evaluation of f (x) in some dialect of FORTRAN. The dependent and independent variables must be nominated through explicit declarations or a naming convention.
Reference: [14] <author> K.E. </author> <note> Hillstrom (1985). "Users Guide for JAKEF", Technical Memorandum ANL/MCS-TM-16, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <address> Argonne IL 60439. </address>
Reference-contexts: The fifth column was obtained in almost the same way, except that the original program was written in FORTAN and then extended to the gradient routine by the precompiler JAKEF <ref> [14] </ref> (an update of Speelpennings original version JAKE [28]). The resulting pair of FORTRAN programs was run on the Sun 3 so that the execution times were naturally much smaller than those of the PASCAL-SC programs on the IBM XT. Nevertheless the comparison between runtime ratios provides some meaningful information. <p> In other words one has to store the type and data dependence of each elementary function in a suitable symbol table. In a way this doubles up the structural information that is already contained in the program. 4.2 FORTRAN Precompiler There are at least three such implementations, namely JAKEF <ref> [14] </ref>, GRESS [13], and PADRE2 [17]. All three precompilers require the user to supply a source code for the 18 evaluation of f (x) in some dialect of FORTRAN. The dependent and independent variables must be nominated through explicit declarations or a naming convention.
Reference: [15] <author> M. </author> <title> Iri (1984). "Simultaneous Computations of Functions, Partial Derivatives and Estimates of Rounding Errors Complexity and Practicality", </title> <journal> Japan Journal of Applied Mathematics, Vol.1, No.2 pp.223-252. </journal>
Reference-contexts: In this article we will concentrate on the goal of obtaining numerical derivative values at given arguments. The need for efficient and accurate derivative evaluations arises in particular during the iterative solution of nonlinear problems and the subsequent sensitivity analysis. Following several other authors, notably Iri <ref> [15] </ref>, we will argue that for these numerical purposes the reverse mode of automatic differentiation is far superior to symbolic differentiation or divided difference approximations. The latter technique is always less accurate and about as costly as the forward form of automatic differentiation. The paper is organized as follows. <p> This aspect has been examined in [9], but will not be pursued any further here. Also, in contrast to the analysis in <ref> [15] </ref>, we will not use the graph structure for our complexity bounds. 10 x 1 x 2 x 3 x 4 x 6 = f 6 hx 5 ; x 2 i x 8 = f 8 hx 7 ; x 4 i A A A A AK J J J
Reference: [16] <author> M. Iri, T. Tsuchiya, and M. </author> <title> Hoshi (1985). "Automatic Computation of Partial Derivatives and Rounding Error Estimates with Application to Large-Scale Systems of Nonlinear Equations" (in Japanese), </title> <journal> Journal of the Information Processing Society of Japan, </journal> <volume> Vol. </volume> <pages> 27, </pages> <address> No.4, pp.389-396. </address>
Reference: [17] <author> M. Iri, and K. </author> <title> Kubota (1987). "Methods of Fast Automatic Differentiation and Applications", </title> <note> Research memorandum RMI 87-0, </note> <institution> Department of Mathematical Engineering and Instrumentation Physics, Faculty of Engineering, University of Tokyo. </institution>
Reference-contexts: During this attempt he realized that the optimal gradient code can be obtained directly without any optimization by (what we call here) the reverse mode of automatic differentiation. Several other papers proposing the reverse or top down mode are referenced in the survey <ref> [17] </ref>. This excellent article discusses also the closely related issue of estimating evaluation errors. <p> Even though these two assumptions are rather unrealistic the right hand side above was found in <ref> [17] </ref> to provide a usually somewhat pessimistic upper bound on the total error. In that paper the local error bounds ffix i were obtained from the machine precision of the computer in question. <p> the function itself. 16 Applying the complexity bound for the reverse mode separately to each component of the gradient one finds that workfr 2 f g n X work @x i Q i=1 work @x i : After division by workff g we obtain in agreement with the results in <ref> [17] </ref> and [11] workfr 2 f g workff g Q i=1 workf@f =@x i g workfrf g workfrf g workff g n Q 2 : In terms of powers of n this bound is unfortunately optimal, as one can see on the simple example f (x) = :5 [x T x <p> In a way this doubles up the structural information that is already contained in the program. 4.2 FORTRAN Precompiler There are at least three such implementations, namely JAKEF [14], GRESS [13], and PADRE2 <ref> [17] </ref>. All three precompilers require the user to supply a source code for the 18 evaluation of f (x) in some dialect of FORTRAN. The dependent and independent variables must be nominated through explicit declarations or a naming convention.
Reference: [18] <author> R.H.F. Jackson, </author> <title> and G.P. McCormick (1988). "Second order Sensitivity Analysis in Factorable Programming: </title> <journal> Theory and Applications", Mathematical Programming, Vol.41, No.1, pp.1-28. </journal>
Reference-contexts: Almost all scalar functions of practical interest can be represented in this factorable form, which has been used extensively by McCormick et al. <ref> [18] </ref>. Rather than restricting ourselves to unary and binary elementary functions we allow for any number of arguments n i jJ i j &lt; i; where j j denotes cardinality.
Reference: [19] <author> H. Kagiwada, R. Kalaba, N.Rosakhoo, and Karl Spingarn (1986). </author> <title> "Numerical Derivatives and Nonlinear Analysis", </title> <booktitle> Vol. 31 of Mathematical Concepts and Methods in Science and Engineering Edt. </booktitle> <address> A.Miele, </address> <publisher> Plenum Press, </publisher> <address> New York and London </address>
Reference-contexts: This effort goes back at least to Beda et al [3] in the Soviet Union and Wengert [30] in the United States. Numerous other references are contained in the paper by Kedem [21], the books by Rall [25] and Kagiwada et al <ref> [19] </ref>, and the recent report by Fischer [11]. In general the researchers in this main stream were unaware of the reverse mode or continued to consider it as a somewhat obscure approach of a rather theoretical nature. Mathematically the reverse mode is closely related to adjoint differential equations. <p> On the other hand separate discretizations of the original and adjoint equation allow the usage of standard software, with automatic differentiation only being used to obtain the Jacobian of the right hand side <ref> [19] </ref>. With the benefit of hindsight one could also construct an 'optimal' spline representation of y (t) in order to economize on storage, especially if the integrator is adaptive and involves many tentative evaluations. Apparently nobody has studied the relative merits and computational performance of these various options.
Reference: [20] <author> K.V. Kim, Iu.E. Nesterov, V.A. Skokov, and B.V. </author> <month> Cherkasskii </month> <year> (1984). </year> <title> "An efficient Algorithm for Computing Derivatives and extremal Problems" English translation of "Effektivnyi algoritm vychisleniia proizvodnykh i ekstremal'nye zaduchi", </title> <journal> Ekonomika i matematicheskie metody, Vol.20, No.2, pp.309-318. </journal>
Reference-contexts: By definition we have x m = 1 and for i = 1 : : : n @f (x)=@x i = x i : As a consequence of the chain rule it can be shown (see e.g. <ref> [20] </ref>) that these adjoint quantities satisfy the relation x j = i2I j @x j where I j fi m : j 2 J i g: Thus we see that x j can be computed once all x i with i &gt; j are known. <p> For example Hessian-vector products of the form r 2 f (x) v 1 can be used in the conjugate gradient method (See e.g. [8] and <ref> [20] </ref>). Second and third derivatives of the form r 2 f (x)v 1 v 2 and r 3 f (x)v 1 v 2 v 3 characterize the quadratic and cubic turning points [12] of bifurcation theory.
Reference: [21] <author> G. </author> <title> Kedem (1980). "Automatic Differentiation of Computer Programs", </title> <journal> ACM TOMS, Vol.6, No.2, pp.150-165. </journal>
Reference-contexts: This effort goes back at least to Beda et al [3] in the Soviet Union and Wengert [30] in the United States. Numerous other references are contained in the paper by Kedem <ref> [21] </ref>, the books by Rall [25] and Kagiwada et al [19], and the recent report by Fischer [11]. In general the researchers in this main stream were unaware of the reverse mode or continued to consider it as a somewhat obscure approach of a rather theoretical nature. <p> This forward approach has been developed and advocated by several authors (See e.g. [3], [30], <ref> [21] </ref>, and [25]). Various software implementations will be discussed in Section 4. A simple count reveals that the calculation of our example gradient by the program above involves 1 2 n 2 nontrivial multiplications, so that q ' n=2. <p> In transforming the original program to the extended routine with automatic differentiation, all control statements are left unaltered. In effect this means that the form of the loop in the original program may become dependent on the current argument. As pointed out by Kedem <ref> [21] </ref> errors may arise when reals are tested for equality. For example the conditional assignment if x 6= 0 then y = (1 cos x)=x else y = 0 would lead to the derivative @y=@x at x = 0 being automatically evaluated as 0 rather than the correct value 1=2. <p> The key idea here is that the programmer can define new types of variables, whose occurence as arguments of an elementary function triggers the compiler to issue additional instructions. The source code itself remains essentially unchanged. Apparently the first implementation of this kind is due to Kedem <ref> [21] </ref>. Since FORTRAN itself does not support overloading, he used the general purpose precompiler AUGMENT, which allowed the user to write the original program in a Taylor made extension of FORTAN. The resulting source code was then precompiled into standard FORTRAN by AUGMENT.
Reference: [22] <editor> U. Kulisch et al (1987). </editor> <title> PASCAL-SC, A PASCAL Extension for Scientific Computation, Information Manual and Floppy Disk, </title> <publisher> B.G. Teubner, Stuttgart, and John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: For example when n = 20 the substitution took 7:13 and 160 seconds CPU time respectively. The results in the third and forth column were obtained on an IBM XT using the programming language PASCAL-SC <ref> [22] </ref>. Like other modern languages this extension of 8 Div. Diff. <p> A few years later Rall [26] achieved a much cleaner implementation of the forward mode in the language PASCAL-SC, an extension of PASCAL for PC Compatibles distributed by Teubner and Wiley <ref> [22] </ref>. The transformation process is extremely simple. Suppose we have a standard PASCAL code for the evaluation of a function in the variables X [1::N ] of type REAL.
Reference: [23] <author> G. </author> <month> Leitmann </month> <year> (1981). </year> <title> "The Calculus of Variations and Optimal Control" Vol.20 of Mathematical Concepts and Methods in Science and Engineering Edt. </title> <publisher> A.Miele, Plenum Press, </publisher> <address> New York and London 27 </address>
Reference-contexts: Interestingly enough this is exactly the information one needs to calculate the gradient of rf (x) by solving the so called adjoint differential equation <ref> [23] </ref>, z 0 (t) = F T y [y (t); t; x]z (t) with z (1) = w; 14 where F y denotes the Jacobian of the right hand side with respect to y.
Reference: [24] <author> D.Y. Peng and D.B. </author> <title> Robinson (1976). "A new two-constant Equation of State", </title> <institution> Ind. Eng. Chem. Fundamentals, Vol.15, pp.59-64. </institution>
Reference-contexts: This excellent article discusses also the closely related issue of estimating evaluation errors. Now let us examine various techniques for evaluating gradients on a couple of simple problems. 2 Comparisons on two Examples The use of a cubic equation of state <ref> [24] </ref> yields the Helmholtz energy of a mixed fluid in a unit volume at the absolute temperature T as f (x) = RT i=1 x i p log p 1 + (1 2)b T x where R is the universal gas constant and 0 x; b 2 R n ; A
Reference: [25] <author> L.B. </author> <title> Rall (1981). "Automatic Differentiation Techniques and Applications", </title> <booktitle> Springer Lecture Notes in Computer Science, </booktitle> <address> Vol.120 </address> . 
Reference-contexts: This remarkable result is achieved by one variant of automatic differentiation <ref> [25] </ref>, which simply implements the chain rule in a suitable fashion. The same approach can be used to compute second and higher derivatives. At least since the fifties these techniques have been developed by computational scientists in various fields, and several software implementations are now available. <p> This effort goes back at least to Beda et al [3] in the Soviet Union and Wengert [30] in the United States. Numerous other references are contained in the paper by Kedem [21], the books by Rall <ref> [25] </ref> and Kagiwada et al [19], and the recent report by Fischer [11]. In general the researchers in this main stream were unaware of the reverse mode or continued to consider it as a somewhat obscure approach of a rather theoretical nature. <p> This forward approach has been developed and advocated by several authors (See e.g. [3], [30], [21], and <ref> [25] </ref>). Various software implementations will be discussed in Section 4. A simple count reveals that the calculation of our example gradient by the program above involves 1 2 n 2 nontrivial multiplications, so that q ' n=2.
Reference: [26] <author> L.B. </author> <title> Rall (1984). "Differentiation in PASCAL-SC: Type GRADIENT", </title> <journal> ACM TOMS Vol.10,pp.161-184. </journal>
Reference-contexts: Kedem's extension of FORTRAN enabled the user to compute gradients or truncated Taylor series in the forward mode of automatic differentiation. A few years later Rall <ref> [26] </ref> achieved a much cleaner implementation of the forward mode in the language PASCAL-SC, an extension of PASCAL for PC Compatibles distributed by Teubner and Wiley [22]. The transformation process is extremely simple.
Reference: [27] <author> L.B. </author> <title> Rall (1987). "Optimal Implementation of Differentiation Arithmetic", in Computer Arithmetic, Scientific Computation and Programming Languages, </title> <editor> ed. U. Kulisch, </editor> <publisher> Teubner, Stuttgart. </publisher>
Reference: [28] <author> B.Speelpenning (1980). </author> <title> "Compiling fast Partial Derivatives of Functions given by Algorithms", </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, </institution> <address> IL 61801. </address>
Reference-contexts: In order to avoid any storage and manipulation of matrices the gradient is then utilized in a conjugate gradient like minimization routine. Apparently the first general purpose implementation of the reverse mode was the pre-compiler JAKE due to Speelpenning. In his unpublished thesis <ref> [28] </ref> Speelpenning showed that Wolfe's assertion is true, but did not state it formally. His original intention was to optimize the gradient code generated in the forward mode by sharing common expressions. <p> As we will see later the extended program can be generated automatically. Everything may be done by hand on our second example f (x) i=1 which was already used by Speelpenning <ref> [28] </ref>. Obviously the ith component of the gradient rf (x) is given by @f =@x i = j6=i If calculated in this form each gradient component involves n 1 multiplications and is thus almost as expensive to evaluate as the function f itself. <p> The fifth column was obtained in almost the same way, except that the original program was written in FORTAN and then extended to the gradient routine by the precompiler JAKEF [14] (an update of Speelpennings original version JAKE <ref> [28] </ref>). The resulting pair of FORTRAN programs was run on the Sun 3 so that the execution times were naturally much smaller than those of the PASCAL-SC programs on the IBM XT. Nevertheless the comparison between runtime ratios provides some meaningful information. <p> Preaccumulating its gradient would essentially halve the number of arcs, whose origins, destinations and values have to be stored until the global reverse sweep. Except in the simple cases mentioned above, the detection of suitable super-elements or f unnels <ref> [28] </ref> requires some combinatorial analysis of the computational graph. If the same function is evaluated over and over such a potentially very large preprocessing effort may well be justified.
Reference: [29] <author> W.C. Thacker and R.B. </author> <title> Long (1988). "Fitting Dynamics to Data", </title> <journal> Journal of Geophysical Research, Vol.93, No.C2, pp.1227-1240. </journal>
Reference-contexts: Similarly, in atmospheric and oceanographic research, adjoints of the governing partial differential equations have been used to obtain the gradients of residual norms with respect to initial conditions and other unknown quantities <ref> [29] </ref>. Here the residuals represent discrepancies between observed and predicted conditions in the atmosphere or ocean. Even though these 3 3D calculations may involve millions of variables, the gradient of the sum of squares can be obtained at essentially the same cost as an evaluation of the residual vector itself. <p> Hence, the argument goes, we might as well fully utilize this derivative information by employing a Gauss-Newton like procedure. However, as is the case for certain inverse problems <ref> [29] </ref>, the Jacobian matrix may be huge and dense, whereas reverse accumulation always yields the gradient cheaply. Then nonlinear conjugate gradients or a variable metric method with limited memory is clearly the only choice.
Reference: [30] <author> R.E. </author> <month> Wengert </month> <year> (1964). </year> <title> "A simple Automatic Derivative Evaluation Program". Com. </title> <journal> ACM, </journal> <volume> Vol. </volume> <month> 7,pp.463-464 </month> . 
Reference-contexts: The main stream of research and implementation has been concerned with the automatic evaluation of gradients ( or more generally truncated Taylor series ) in the forward mode. This effort goes back at least to Beda et al [3] in the Soviet Union and Wengert <ref> [30] </ref> in the United States. Numerous other references are contained in the paper by Kedem [21], the books by Rall [25] and Kagiwada et al [19], and the recent report by Fischer [11]. <p> It can be generated in a 'mechanical' fashion and is only about twice as long as the original program because each assignment to an intermediate quantity is simply augmented by the calculation of its gradient. This forward approach has been developed and advocated by several authors (See e.g. [3], <ref> [30] </ref>, [21], and [25]). Various software implementations will be discussed in Section 4. A simple count reveals that the calculation of our example gradient by the program above involves 1 2 n 2 nontrivial multiplications, so that q ' n=2.
Reference: [31] <institution> P.Wolfe (1982) ."Checking the Calculation of Gradients", ACM TOMS, Vol.6, </institution> <month> No.4, </month> <pages> pp. 337-343. </pages>
Reference-contexts: 1 Introduction In 1982 Phil Wolfe <ref> [31] </ref> made the following observation regarding the ratio between the cost of evaluating a gradient with n components and the cost of evaluating the underlying scalar function. If care is taken in handling quantities which are common to the function and derivatives, the ratio is usually around 1.5, not n+1. [31] <p> <ref> [31] </ref> made the following observation regarding the ratio between the cost of evaluating a gradient with n components and the cost of evaluating the underlying scalar function. If care is taken in handling quantities which are common to the function and derivatives, the ratio is usually around 1.5, not n+1. [31] The main purpose of this article is to demonstrate that Phil Wolfe's observation is in fact a theorem (with the average 1.5 replaced by an upper bound 5) and that care can be taken automatically.
Reference: [32] <author> B.A. Worley et al (1989). </author> <title> "Deterministic Sensitivity, and Uncertainty Analysis in Large Scale Computer Models", </title> <booktitle> Proceedings of 10th Annual DOE low level Waste Management Conference in Denver, </booktitle> <address> Aug.30 -Sept.1, </address> <year> 1988. </year> <month> 28 </month>
Reference-contexts: In contrast, thousands of these lengthy calculations would be needed to approximate all sensitivities by divided differences. For a recent survey on the software and applications in this field see the paper by Worley <ref> [32] </ref>. Similarly, in atmospheric and oceanographic research, adjoints of the governing partial differential equations have been used to obtain the gradients of residual norms with respect to initial conditions and other unknown quantities [29]. Here the residuals represent discrepancies between observed and predicted conditions in the atmosphere or ocean.
References-found: 32

