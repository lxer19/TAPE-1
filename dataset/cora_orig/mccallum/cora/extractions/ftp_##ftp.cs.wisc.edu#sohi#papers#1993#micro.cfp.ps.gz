URL: ftp://ftp.cs.wisc.edu/sohi/papers/1993/micro.cfp.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Title: Control Flow Prediction For Dynamic ILP Processors  
Author: Dionisios N. Pnevmatikatos Manoj Franklin Gurindar S. Sohi 
Address: Madison, WI 53706 Clemson, SC 29634 Madison, WI 53706  
Affiliation: Computer Sciences Department ECE Department Computer Sciences Department University of Wisconsin-Madison Clemson University University of Wisconsin-Madison  
Abstract: We introduce a technique to enhance the ability of dynamic ILP processors to exploit (speculatively executed) parallelism. Existing branch prediction mechanisms used to establish a dynamic window from which ILP can be extracted are limited in their abilities to: (i) create a large, accurate dynamic window, (ii) initiate a large number of instructions into this window in every cycle, and (iii) traverse multiple branches of the control flow graph per prediction. We introduce control flow prediction which uses information in the control flow graph of a program to overcome these limitations. We discuss how information present in the control flow graph can be represented using multiblocks, and conveyed to the hardware using Control Flow Tables and Control Flow Prediction Buffers. We evaluate the potential of control flow prediction on an abstract machine and on a dynamic ILP processing model. Our results indicate that control flow prediction is a powerful and effective assist to the hardware in making more informed run time decisions about program control flow. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman, </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: 1. Introduction Conditional branch instructions are a necessary evil in programs. They introduce control dependencies, the flow of control being described by the control flow graph (CFG) of the program <ref> [1] </ref>. Ordinarily, these control dependencies must be respected: no instruction that occurs after a branch can be executed before the branch. The plethora of conditional branches in many programs, however, forces the development of alternatives, because very little instruction-level parallelism (ILP) would be exposed if this simple-minded rule were obeyed.
Reference: [2] <author> T. Anderson, A. Berre, M. Mallison, H. Porter III, and B. Schneider, </author> <title> ``The HyperModel Benchmark,'' Extending Database Technology, </title> <year> 1990. </year>
Reference-contexts: Finally, we use two Object Oriented Database benchmarks written in C++: Sun-benchmark [5], and Tektronix <ref> [2] </ref>. Table 3 show the basic statistics for the programs, including the dynamic number of instructions we consider for execution, the percentage of these instructions that are branches (conditional and unconditional), the code size (in instructions) and the corresponding Control Flow Table size (in entries).
Reference: [3] <author> T. Austin and G. Sohi, </author> <title> ``Dynamic Dependency Analysis of Ordinary Programs,'' </title> <booktitle> Proc. 19th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Third, we will have created a larger, and more accurate, dynamic window. (The probability of correctly initiating the execution of BB5 is 100% as opposed to 37.2% with branch prediction.) A larger dynamic window allows more ILP to be exposed and exploited <ref> [3, 4] </ref>. Next, we describe multiblocks, a vehicle for conveying the necessary control flow information to the hardware prediction mechanism. 2.1. Multiblocks and their Use A multiblock is a subgraph of the CFG of a program; the first instruction of a multiblock is defined to be its entry point. <p> Note that these were programs that did not show much improvement in the abstract machine model also. These programs need software assist, such as function inlining and more sophisticated multiblock selection. For example, in Gcc, parallelism exists (albeit comparatively lower), and does so in reasonable-sized instruction windows <ref> [3] </ref>, but accurately establishing windows of this size is currently a problem. A major reason is due to loops that iterate only 2 or 3 times, resulting in poor prediction.
Reference: [4] <author> M. Butler, T. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow, </author> <title> ``Single Instruction Stream Parallelism Is Greater than Two,'' </title> <booktitle> Proc. 18th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Third, we will have created a larger, and more accurate, dynamic window. (The probability of correctly initiating the execution of BB5 is 100% as opposed to 37.2% with branch prediction.) A larger dynamic window allows more ILP to be exposed and exploited <ref> [3, 4] </ref>. Next, we describe multiblocks, a vehicle for conveying the necessary control flow information to the hardware prediction mechanism. 2.1. Multiblocks and their Use A multiblock is a subgraph of the CFG of a program; the first instruction of a multiblock is defined to be its entry point.
Reference: [5] <author> R. Cattell, </author> <title> ``Object Oriented Performance Measurement,'' </title> <booktitle> in Proc. of the 2nd International Workshop on OODBMS. </booktitle> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: Finally, we use two Object Oriented Database benchmarks written in C++: Sun-benchmark <ref> [5] </ref>, and Tektronix [2]. Table 3 show the basic statistics for the programs, including the dynamic number of instructions we consider for execution, the percentage of these instructions that are branches (conditional and unconditional), the code size (in instructions) and the corresponding Control Flow Table size (in entries).
Reference: [6] <author> P. Chang, S. Mahlke, W. Chen, N. Warter, and W. Hwu, </author> <title> ``IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors,'' </title> <booktitle> Proc. 18th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Whereas static techniques to alleviate the impact of control dependencies, such as Trace scheduling [7, 8], Predicated Execution [11], Superblock and Hyperblock scheduling <ref> [6, 15] </ref>, and Boosting [18], exploit information present in the CFG, dynamic techniques typically do not. Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window. <p> Fortunately, guarded instructions can be used in conjunction with control flow prediction, and this is something we are currently investigating. In fact, CFP can be used in conjunction with any software technique to enhance the parallelism for a dynamic ILP processor, such as superblock or hyperblock scheduling <ref> [6, 15] </ref>. A judicious use of these techniques can generate large nodes in a CFG that the software can optimize, and CFP can be used to navigate intelligently through the transformed CFG dynamically. 7. Conclusions In this paper we introduced the concept of control flow prediction.
Reference: [7] <author> R. Colwell, R. Nix, J. O'Donnell, D. Papworth, and P. Rodman, </author> <title> ``A VLIW Architecture for a Trace Scheduling Compiler,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 967-979, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Finally, if the processor can exploit the control dependence information, resolve multiple branches in one cycle, and follow multiple flows of control, the available parallelism increases to about 40. Whereas static techniques to alleviate the impact of control dependencies, such as Trace scheduling <ref> [7, 8] </ref>, Predicated Execution [11], Superblock and Hyperblock scheduling [6, 15], and Boosting [18], exploit information present in the CFG, dynamic techniques typically do not. Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window.
Reference: [8] <author> J. Fisher, </author> <title> ``Trace Scheduling: A Technique for Global Microcode Compaction,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Finally, if the processor can exploit the control dependence information, resolve multiple branches in one cycle, and follow multiple flows of control, the available parallelism increases to about 40. Whereas static techniques to alleviate the impact of control dependencies, such as Trace scheduling <ref> [7, 8] </ref>, Predicated Execution [11], Superblock and Hyperblock scheduling [6, 15], and Boosting [18], exploit information present in the CFG, dynamic techniques typically do not. Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window.
Reference: [9] <author> M. Franklin and G. Sohi, </author> <title> ``The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism,'' </title> <booktitle> Proc. 19th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: In this section, we discuss how the multiblock concept was used in the multis-calar processing model (erstwhile Expandable Split Window (ESW) model) <ref> [9] </ref>. 5.1. The Multiscalar Processor processor. It consists of several independent, identical execution stages, each of which is equivalent to a typical datapath found in modern processors. <p> In any given cycle, up to a fixed number of ready-to-execute instructions begin execution in each of the active stages. Hardware means are provided for forwarding values across stages, and to enforce register and memory dependencies <ref> [9] </ref>. 5.2. Experimental Results To study how much a conventional branch prediction mechanism limits the performance of the multiscalar processor, and to study the efficacy of multiblocks for the multiscalar processor, we used a detailed simulator.
Reference: [10] <author> M. D. Hill and A. J. Smith, </author> <title> ``Evaluating Associativity in CPU Caches,'' </title> <journal> IEEE Transactions On Computer, </journal> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: We use 15 programs as benchmarks in our evaluation. Compress, Eqntott, Espresso, Gcc, Sc, and Xlisp are C programs and Doduc and Spice are FORTRAN programs from the SPEC92 benchmark suite, We also use Yacc, Tex and three simulators written in C: Tycho, a cache simulator <ref> [10] </ref>, SuperMips, a simulator of a superscalar CPU using the MIPS instruction set, and ThisSim, the simulator used for the studies of this section. Finally, we use two Object Oriented Database benchmarks written in C++: Sun-benchmark [5], and Tektronix [2].
Reference: [11] <author> P. Y. T. Hsu and E. S. Davidson, </author> <title> ``Highly Concurrent Scalar Processing,'' </title> <booktitle> Proc. 13th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Finally, if the processor can exploit the control dependence information, resolve multiple branches in one cycle, and follow multiple flows of control, the available parallelism increases to about 40. Whereas static techniques to alleviate the impact of control dependencies, such as Trace scheduling [7, 8], Predicated Execution <ref> [11] </ref>, Superblock and Hyperblock scheduling [6, 15], and Boosting [18], exploit information present in the CFG, dynamic techniques typically do not. Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window.
Reference: [12] <author> D. Kaeli and P. Emma, </author> <title> ``Branch History Table Prediction of Moving Target Branches Due to Subroutine Returns,'' </title> <booktitle> Proc. 18th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: The CFPB is accessed once for every multiblock activation to determine the multiblock's size and targets. Between the two targets, the prediction mechanism selects one target. If the selected target is the special value Return, then the actual target is predicted using a return stack <ref> [12] </ref>. If the selected target is Unknown, indicating an indirect branch, the predictor can optimistically use the last outcome of this branch. <p> It accepts executable images of programs, and executes them; it is not trace driven. We conducted two sets of experiments one with branch prediction and the other with control flow prediction. Both sets use a pattern-based prediction scheme (cf. section 4) plus a call/return stack <ref> [12] </ref> to predict the target of procedure call returns. For the former set, when a branch instruction is fetched by an execution stage, a lookup is done in a BTB to determine the branch target.
Reference: [13] <author> M. Lam and R. Wilson, </author> <title> ``Limits of Control Flow on Parallelism,'' </title> <booktitle> Proc. 19th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Knowledge of the CFG is essential to any technique that tries to expose speculative ILP. A recent study by Lam and Wilson has underscored the importance of the knowledge of control-dependence relationships amongst instructions for ILP processors <ref> [13] </ref>. Their study shows that an (abstract) ILP processor which performs branch prediction and speculative execution but allows only a single flow of control can extract a parallelism of only about 7.
Reference: [14] <author> J. K. F. Lee and A. J. Smith, </author> <title> ``Branch Prediction Strategies and Branch Target Buffer Design,'' </title> <journal> IEEE Computer, </journal> <volume> vol. 17, </volume> <pages> pp. 6-22, </pages> <month> Jan. </month> <year> 1984. </year>
Reference-contexts: Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window. Dynamic branch prediction schemes make decisions about a branch when the branch is encountered, for example, when it is fetched and decoded. The decision is made using a Branch History Table (BHT) <ref> [14, 17] </ref>, which typically stores a condensed history of past taken/not taken outcomes; this history is updated when the branch outcome is known. <p> While some recent prediction mechanisms [16, 20] do not require the branch address to predict its outcome, the identity of the branch must still be known so that the predicted target address can be determined (either using a BTB structure <ref> [14] </ref>, or by decoding the branch instruction). After the prediction decision, instructions from the predicted path need to be fetched, and the next branch in the predicted path encountered (and its identity determined), before its target can be predicted. <p> Control Flow Prediction Buffers (CFPB) The Control Flow Prediction Buffer (CFPB) is a cache of CFT entries, with each entry appended with sufficient information to make dynamic prediction decisions. If a counter based branch prediction mechanism is used, the CFPB resembles a BTB <ref> [14] </ref> augmented with multiblock information. The CFPB is accessed once for every multiblock activation to determine the multiblock's size and targets. Between the two targets, the prediction mechanism selects one target.
Reference: [15] <author> S. Mahlke, D. Lin, W. Chen, R. Hank, and R. Bringmann, </author> <title> ``Effective Compiler Support for Predicated Execution Using the Hyperblock,'' </title> <booktitle> Proc. of the 25th Annual Workshop on Microprogramming and Microarchitecture, </booktitle> <year> 1992. </year>
Reference-contexts: Whereas static techniques to alleviate the impact of control dependencies, such as Trace scheduling [7, 8], Predicated Execution [11], Superblock and Hyperblock scheduling <ref> [6, 15] </ref>, and Boosting [18], exploit information present in the CFG, dynamic techniques typically do not. Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window. <p> Fortunately, guarded instructions can be used in conjunction with control flow prediction, and this is something we are currently investigating. In fact, CFP can be used in conjunction with any software technique to enhance the parallelism for a dynamic ILP processor, such as superblock or hyperblock scheduling <ref> [6, 15] </ref>. A judicious use of these techniques can generate large nodes in a CFG that the software can optimize, and CFP can be used to navigate intelligently through the transformed CFG dynamically. 7. Conclusions In this paper we introduced the concept of control flow prediction.
Reference: [16] <author> S.-T. Pan, K. So, and J. T. Rahmeh, </author> <title> ``Improving the Accuracy of Dynamic Branch Prediction Using Branch Correlation,'' </title> <booktitle> Proc. Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: First, to predict a branch, its identity must be known, i.e., the branch must be encountered by the hardware. With normal branch prediction, a prediction is made when the branch instruction is fetched by the fetch unit. While some recent prediction mechanisms <ref> [16, 20] </ref> do not require the branch address to predict its outcome, the identity of the branch must still be known so that the predicted target address can be determined (either using a BTB structure [14], or by decoding the branch instruction). <p> If the structure of the table (s) used by the prediction mechanism is different from the structure of the CFPB (as in the cases of [20] and <ref> [16] </ref>), then the prediction state field in the CFPB can be eliminated, and the CFPB is just a cache of the CFT entries. Because the CFPB is a cache, fragmentation is not an issue and we can use instruction addresses to access the CFPB.
Reference: [17] <author> J. E. Smith, </author> <title> ``A Study of Branch Prediction Strategies,'' </title> <booktitle> Proc. 8th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981. </year>
Reference-contexts: Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window. Dynamic branch prediction schemes make decisions about a branch when the branch is encountered, for example, when it is fetched and decoded. The decision is made using a Branch History Table (BHT) <ref> [14, 17] </ref>, which typically stores a condensed history of past taken/not taken outcomes; this history is updated when the branch outcome is known.
Reference: [18] <author> M. Smith, M. Lam, and M. Horowitz, </author> <title> ``Boosting Beyond Static Scheduling in a Superscalar Processor,'' </title> <booktitle> Proc. 17th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Whereas static techniques to alleviate the impact of control dependencies, such as Trace scheduling [7, 8], Predicated Execution [11], Superblock and Hyperblock scheduling [6, 15], and Boosting <ref> [18] </ref>, exploit information present in the CFG, dynamic techniques typically do not. Instead, they rely only on dynamic branch prediction techniques to construct the dynamic window. Dynamic branch prediction schemes make decisions about a branch when the branch is encountered, for example, when it is fetched and decoded.
Reference: [19] <author> T. Yeh, D. Marr, and Y. Patt, </author> <title> ``Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache,'' </title> <booktitle> Proceedings of ICS-7, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: This problem has been identified and partly addressed in [21], where the prediction mechanism can perform one prediction per cycle (two or more predictions per cycle in <ref> [19] </ref>) as long as the next branch lies inside the block of instructions fetched from the instruction cache. The second problem has to do with the initiation size, that is, the number of instructions that can enter into the dynamic window in a given cycle.
Reference: [20] <author> T. Yeh and Y. Patt, </author> <title> ``A Comparison of Dynamic Branch Predictors that use Two Levels of Branch History,'' </title> <booktitle> Proc. 20th Annual Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: First, to predict a branch, its identity must be known, i.e., the branch must be encountered by the hardware. With normal branch prediction, a prediction is made when the branch instruction is fetched by the fetch unit. While some recent prediction mechanisms <ref> [16, 20] </ref> do not require the branch address to predict its outcome, the identity of the branch must still be known so that the predicted target address can be determined (either using a BTB structure [14], or by decoding the branch instruction). <p> If the structure of the table (s) used by the prediction mechanism is different from the structure of the CFPB (as in the cases of <ref> [20] </ref> and [16]), then the prediction state field in the CFPB can be eliminated, and the CFPB is just a cache of the CFT entries. Because the CFPB is a cache, fragmentation is not an issue and we can use instruction addresses to access the CFPB. <p> assumption will not alter our results in terms of the units used to present the results, it would alter the results per units of time; the magnitude would depend upon the CFPB and BTB miss penalty.) Fourth, both the BTB and the CFPB use a GAs (8,64) two-level branch predictor <ref> [20] </ref>. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Dynamic Branch Ratio Static Instructions iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii Code size CFT size Program (Millions) Cond.
Reference: [21] <author> T. Yeh and Y. Patt, </author> <title> ``A Comprehensive Instruction Fetch Mechanism for a Processor Supporting Speculative Execution,'' </title> <booktitle> Proc. of the 25th Annual Workshop on Microprogramming and Microarchi-tecture (Micro 25), </booktitle> <pages> pp. 129-139, </pages> <year> 1992. </year>
Reference-contexts: If we like to sustain a rate of one prediction per cycle, the identity of the next branch to be predicted must be known when a prediction is made. This problem has been identified and partly addressed in <ref> [21] </ref>, where the prediction mechanism can perform one prediction per cycle (two or more predictions per cycle in [19]) as long as the next branch lies inside the block of instructions fetched from the instruction cache.
References-found: 21

