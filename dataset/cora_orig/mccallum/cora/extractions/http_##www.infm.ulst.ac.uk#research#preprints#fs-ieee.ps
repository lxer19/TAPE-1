URL: http://www.infm.ulst.ac.uk/research/preprints/fs-ieee.ps
Refering-URL: http://www.infm.ulst.ac.uk/research/preprints.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: fh.wang, da.bell, fd.murtaghg@ulst.ac.uk  
Title: Axiomatic approach to feature subset selection based on relevance  
Author: Hui Wang, David Bell, Fionn Murtagh 
Address: Northland Road, Londonderry, BT48 7JL Northern Ireland, UK  
Affiliation: School of Information and Software Engineering Faculty of Informatics University of Ulster Magee College  
Abstract: In this paper an axiomatic characterisation of feature subset selection is presented. Two axioms are presented: sufficiency axiom | preservation of learning information, and necessity axiom | minimising encoding length. The sufficiency axiom concerns the existing dataset and is derived based on the following understanding: any selected feature subset should be able to describe the training dataset without losing information, i.e., it is consistent with the training dataset. The necessity axiom concerns predictability and is derived from Occam's razor, which states that the simplest among different alternatives is preferred for prediction. The two axioms are then re-stated in terms of relevance in a concise form: maximising both the r(X; Y ) and r(Y ; X) relevance. Based on the relevance characterisation, a heuristic selection algorithm is presented and experimented with. The results support the axioms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. W. Aha and R. L. Bankert. </author> <title> Feature selection for case-based classification of cloud types. </title> <booktitle> In Working notes of the AAAI94 Workshop on Case-based Reasoning, </booktitle> <pages> pages 106-112. </pages> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: Broadly speaking, FSS is to select a subset of features from the feature space which is good enough regarding its ability to describe the training dataset and to predict for future cases. There is a wealth of algorithms for FSS (see for example, <ref> [2, 15, 1, 17, 14, 24] </ref>). With regard to how to evaluate the goodness of a subset of features, the FSS methods fall into two broad categories: filter approach and wrapper approach, which are illustrated in Figures 1 and 2.
Reference: [2] <author> H. Almuallim and T. G. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proc. Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Broadly speaking, FSS is to select a subset of features from the feature space which is good enough regarding its ability to describe the training dataset and to predict for future cases. There is a wealth of algorithms for FSS (see for example, <ref> [2, 15, 1, 17, 14, 24] </ref>). With regard to how to evaluate the goodness of a subset of features, the FSS methods fall into two broad categories: filter approach and wrapper approach, which are illustrated in Figures 1 and 2. <p> Typically, the feature subset which performs best for the induction algorithm will be selected. Both types of approach to FSS are closely related to the notion of relevance. For example, FOCUS <ref> [2] </ref>, RELIEF [15] and Schlimmer's model [22] use "relevance" to estimate the goodness of feature subset in one way or another. Section 5.2 presents a review in this respect. <p> In other words, Y is conditionally independent of given . 1 Target or target concept is usually defined as a subset of an instance space <ref> [2] </ref>, which can be interpreted as a bi-partition of the instance space. Here we use it in the more general sense: a target concept is an arbitrary partition of the instance space. <p> From this we can say that our two axiomatic characterisations are possible interpretations of the sufficiency and necessity requirement proposed in [15]. In practice, the best feature subsets are measured in pragmatic ways. For example, in FOCUS <ref> [2] </ref> a good feature subset is a minimal subset which is consistent with the training dataset. Here the consistency can be understood as the sufficiency requirement, since only when the feature subset is consistent with the given dataset can it qualify to describe the dataset without losing learning information. <p> Here all the minimal determinations are sufficient, but which of these is necessary is left open. 5.2 Re-modelling using the relevance framework Many FSS algorithms use "relevance" to estimate feature usefulness in one way or another. The FOCUS <ref> [2] </ref> algorithm starts with an empty feature set and carries out breadth-first search until it finds a minimal combination of features which is consistent with the training dataset. The 8 features in are relevant to the target concept C.
Reference: [3] <author> B. Amirikian and H. Nishimura. </author> <title> What size network is good for generalization of a specific task of interest? Neural Networks, </title> <booktitle> 7(2) </booktitle> <pages> 321-329, </pages> <year> 1994. </year> <month> 10 </month>
Reference-contexts: This principle is becoming influential in machine learning, where this principle can be formulated as: given two hypotheses that both are consistent with a training set of examples of a given task, the simpler one will guess better on future examples of this task <ref> [4, 27, 3] </ref>. It has been shown (see for example, [4]) that, under very general assumptions, Occam's razor produces hypotheses that with high probability will be predictive of future cases. One basic question is concerned with the meaning of "simplicity", namely Occam simplicity. <p> One basic question is concerned with the meaning of "simplicity", namely Occam simplicity. Typically Occam simplicity is associated with the difficulty of implementing a given task, namely complexity of implementation. For example, the number of hidden neurons in neural networks <ref> [3] </ref>; the number of leaf nodes of a decision tree [10, 11]; the minimum description length (MDL) [21, 20]; and the encoding length [23].
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: This principle is becoming influential in machine learning, where this principle can be formulated as: given two hypotheses that both are consistent with a training set of examples of a given task, the simpler one will guess better on future examples of this task <ref> [4, 27, 3] </ref>. It has been shown (see for example, [4]) that, under very general assumptions, Occam's razor produces hypotheses that with high probability will be predictive of future cases. One basic question is concerned with the meaning of "simplicity", namely Occam simplicity. <p> It has been shown (see for example, <ref> [4] </ref>) that, under very general assumptions, Occam's razor produces hypotheses that with high probability will be predictive of future cases. One basic question is concerned with the meaning of "simplicity", namely Occam simplicity. Typically Occam simplicity is associated with the difficulty of implementing a given task, namely complexity of implementation.
Reference: [5] <author> R. Caruana and D. Freitag. </author> <booktitle> How useful is relevance? In Proceedings of the 1994 AAAI Fall Symposium on Relevance, </booktitle> <pages> pages 21-25. </pages> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: As shown in [25] their strong relevance and weak relevance can be characterised by our relevance formalism, so the wrapper scheme can also be modelled by our relevance, r (X; C) &gt; 0. However, Caruana and Freitag <ref> [5] </ref> observe that not all features that are relevant are necessarily useful for induction.
Reference: [6] <author> E. F. Codd. </author> <title> A relational model of data for large shared data banks. </title> <journal> Commun. ACM, </journal> <volume> 3(6) </volume> <pages> 377-387, </pages> <year> 1970. </year>
Reference-contexts: It is regarded as a variable here. 2 In this paper we use X i to refer to both a variable and the domain of the variable, when this can be identified from the context. 3 <ref> [6, 12] </ref>. We use the notation in [12]. A relation scheme R is a set of variables (features).
Reference: [7] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of information theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: Therefore any selected feature subset, if it is expected to work well on the given dataset, should preserve the existing relationship between X and Y hidden in the dataset. A natural measure of this relationship is the mutual information <ref> [7] </ref>. We call this relationship learning information. Specifically, given a learning task r [X [ Y ], the learning information is the mutual information I (X; Y ). Furthermore, suppose and are two subsets of X. <p> The following two lemmas follow directly from the chain rule for mutual information and the non-negativity of mutual information. Lemma 2.1 Given r [X [ Y ]. For any X, I (; Y ) I (X; Y ). From this lemma and the additivity of mutual information <ref> [7] </ref> we know that given a SFS , removing all the remaining features will not lose learning information contained in the original dataset.
Reference: [8] <author> S. Davies and S. Russell. </author> <title> NP-Completeness of Searches for Smallest Possible Feature Sets. </title> <booktitle> In Proceedings of the 1994 AAAI Fall Symposium on Relevance, </booktitle> <pages> pages 37-39. </pages> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: A straightforward algorithm is to systematically examine all feature subsets and find one which satisfies the above two axioms. Unfortunately, as shown in <ref> [8] </ref>, this class of algorithms turns out to be NP-complete. Branch and bound based on the characteristics of relevance was attempted [25], but it was shown to be also exponential in general. So we attempted heuristic approaches. Here we are to present our preferred heuristic FSS algorithm.
Reference: [9] <author> P. A. Devijver and J. Kittler. </author> <title> Pattern recognition: A statistical approach. </title> <address> New York: </address> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: 1 Introduction The problem of feature subset selection (FSS hereafter) has long been an active research topic within statistics and pattern recognition (e.g., <ref> [9] </ref>), but most work in this area has dealt with linear regression.
Reference: [10] <author> U. Fayyad and K. Irani. </author> <title> What should be minimized in a decision tree? In AAAI-90: </title> <booktitle> Proceedings of 8th National Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: Typically Occam simplicity is associated with the difficulty of implementing a given task, namely complexity of implementation. For example, the number of hidden neurons in neural networks [3]; the number of leaf nodes of a decision tree <ref> [10, 11] </ref>; the minimum description length (MDL) [21, 20]; and the encoding length [23].
Reference: [11] <author> U. Fayyad and K. Irani. </author> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In AAAI-92: Proceedings of 10th National Conference on Artificial Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: Typically Occam simplicity is associated with the difficulty of implementing a given task, namely complexity of implementation. For example, the number of hidden neurons in neural networks [3]; the number of leaf nodes of a decision tree <ref> [10, 11] </ref>; the minimum description length (MDL) [21, 20]; and the encoding length [23].
Reference: [12] <author> Joe R. Hill. </author> <title> Relational Databases: A Tutorial for Statisticians. </title> <editor> In E. M. Keramidas and S. M. Kaufman, editors, </editor> <booktitle> Computing Science and Statistics: Proc. of the 23rd Symposium on the Interface, </booktitle> <pages> pages 86-93, </pages> <year> 1991. </year>
Reference-contexts: It is regarded as a variable here. 2 In this paper we use X i to refer to both a variable and the domain of the variable, when this can be identified from the context. 3 <ref> [6, 12] </ref>. We use the notation in [12]. A relation scheme R is a set of variables (features). <p> It is regarded as a variable here. 2 In this paper we use X i to refer to both a variable and the domain of the variable, when this can be identified from the context. 3 [6, 12]. We use the notation in <ref> [12] </ref>. A relation scheme R is a set of variables (features). A relation (table) over R is an indicator function for a set of tuples, written r [R] : r [R](t) = 1 if the tuple t is in the relation; r [R](t) = 0 otherwise.
Reference: [13] <author> Jerry R. Hobbs. </author> <title> Granularity. </title> <booktitle> In Proc. IJCAI85, </booktitle> <pages> pages 432-435, </pages> <year> 1985. </year>
Reference-contexts: Our feature selection algorithm didn't change this situation. In C4.5, continuous features are treated as discrete features in such a way that their values are divided into two groups, each of which is a discrete cluster used in the classification. From the granularity point of view <ref> [13] </ref>, the granularity of the continuous features are made simply too coarse. In our feature selection algorithm, continuous features are treated as discrete features in such a way that each continuous value is taken to be a discrete value and is used individually in the classification.
Reference: [14] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th international conference on machine learning, </booktitle> <pages> pages 121-129. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Broadly speaking, FSS is to select a subset of features from the feature space which is good enough regarding its ability to describe the training dataset and to predict for future cases. There is a wealth of algorithms for FSS (see for example, <ref> [2, 15, 1, 17, 14, 24] </ref>). With regard to how to evaluate the goodness of a subset of features, the FSS methods fall into two broad categories: filter approach and wrapper approach, which are illustrated in Figures 1 and 2. <p> The general argument for wrapper approaches is that the induction method that will use the feature subset should provide a better estimate of accuracy than a separate measure that may have an entirely different inductive bias. John, Kohavi, and Pfleger <ref> [14] </ref> were the first to present the wrapper idea as a general framework for feature selection. The generic wrapper technique must still use some measure to select among alternative features.
Reference: [15] <author> K. Kira and L. A. Rendell. </author> <title> The feature selection problem: traditional methods and a new algorithm. </title> <booktitle> In AAAI-92, </booktitle> <pages> pages 129-134, </pages> <year> 1992. </year>
Reference-contexts: Broadly speaking, FSS is to select a subset of features from the feature space which is good enough regarding its ability to describe the training dataset and to predict for future cases. There is a wealth of algorithms for FSS (see for example, <ref> [2, 15, 1, 17, 14, 24] </ref>). With regard to how to evaluate the goodness of a subset of features, the FSS methods fall into two broad categories: filter approach and wrapper approach, which are illustrated in Figures 1 and 2. <p> Section 5.1 presents a review on the empirical use of the notion of goodness in this category. There is a special type in this approach | feature weighting <ref> [15] </ref>, which is slightly different from the mainstream filter approach in the way the search for good feature set is conducted. <p> Basically the mainstream approach evaluates each subset of features and finds the "optimal", while the weighting approach weighs each individual feature and selects a "quasi-optimal" set of features, typically those whose weights exceed a given threshold <ref> [15, 17] </ref>. In the wrapper approach, feature selection is done with the help of induction algorithms. The feature selection algorithm conducts a search for a good feature set using the induction algorithm itself as part of the evaluation function. <p> Typically, the feature subset which performs best for the induction algorithm will be selected. Both types of approach to FSS are closely related to the notion of relevance. For example, FOCUS [2], RELIEF <ref> [15] </ref> and Schlimmer's model [22] use "relevance" to estimate the goodness of feature subset in one way or another. Section 5.2 presents a review in this respect. <p> Australian: 2,14,8,3,13,7,10,9,5,6,4,12,11,1; Diabetes: 7,6,2,5,8,4,1,3; Heart: 5,8,10,13,3,12,1,4,9,11,2,7,6. 5 Comparison with related work In this section we are to take a closer look at some related work from the relevance point of view and compare them with ours. 5.1 How is the best feature subset characterised in the literature? In <ref> [15] </ref>, the best feature subset is characterised as sufficient and necessary to describe the target. <p> However the nature of the sufficiency and necessity requirement was not made clear in <ref> [15] </ref>. In the context of learning from examples, it seems reasonable that sufficiency concerns the ability of a feature subset to describe the given dataset (called qualified later on), while the necessity concerns the optimality among all the qualified feature subsets regarding predictive ability. <p> From this we can say that our two axiomatic characterisations are possible interpretations of the sufficiency and necessity requirement proposed in <ref> [15] </ref>. In practice, the best feature subsets are measured in pragmatic ways. For example, in FOCUS [2] a good feature subset is a minimal subset which is consistent with the training dataset. <p> The minimality of feature subset can be understood as the necessity requirement, as it was used as a bias of learning regarding which subset can predict better for future cases. In RELIEF <ref> [15] </ref>, a good subset is one whose elements each has a relevance level greater than a given threshold. Here the relevancy and the threshold together determine whether a given feature subset is sufficient (or qualified) to describe the given dataset.
Reference: [16] <author> R. Kohavi and D. Sommerfield. </author> <title> Feature Subset Selection Using the Wrapper Method: Over-fitting and Dynamic Search Space Topology. </title> <editor> In U. M. Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proceedings of KDD'95, </booktitle> <pages> pages 192-197, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction The problem of feature subset selection (FSS hereafter) has long been an active research topic within statistics and pattern recognition (e.g., [9]), but most work in this area has dealt with linear regression. In the past few years, researchers in machine learning have realised (see for example, <ref> [18, 16] </ref>) that practical algorithms in supervised machine learning degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output. Therefore FSS has since received considerable attention from machine learning researchers interested in improving the performance of their algorithms. <p> For example, FOCUS [2], RELIEF [15] and Schlimmer's model [22] use "relevance" to estimate the goodness of feature subset in one way or another. Section 5.2 presents a review in this respect. Although the wrapper approach does not use the relevance measure directly, it is shown <ref> [16] </ref> that the "optimal" feature subset obtained this way must be from the relevant feature set (strongly relevant and weakly relevant features). However, the mathematical foundation for FSS is still lacking [26]. In [25], a unified framework for relevance was proposed. <p> The major disadvantage of wrapper methods over filter methods is the former's computational cost, which results from calling the induction algorithm for each feature set considered. This cost has led some researchers to invent ingenious techniques for speeding the evaluation process. The wrapper scheme in <ref> [16] </ref> does not use the relevance measure directly; rather, it uses the accuracy obtained by applying an induction algorithm as the measure for the goodness of feature sets.
Reference: [17] <author> I. Kononenko. </author> <title> Estimating attributes: analysis and extensions of RELIEF. </title> <booktitle> In Proceedings of the 1994 European Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: Broadly speaking, FSS is to select a subset of features from the feature space which is good enough regarding its ability to describe the training dataset and to predict for future cases. There is a wealth of algorithms for FSS (see for example, <ref> [2, 15, 1, 17, 14, 24] </ref>). With regard to how to evaluate the goodness of a subset of features, the FSS methods fall into two broad categories: filter approach and wrapper approach, which are illustrated in Figures 1 and 2. <p> Basically the mainstream approach evaluates each subset of features and finds the "optimal", while the weighting approach weighs each individual feature and selects a "quasi-optimal" set of features, typically those whose weights exceed a given threshold <ref> [15, 17] </ref>. In the wrapper approach, feature selection is done with the help of induction algorithms. The feature selection algorithm conducts a search for a good feature set using the induction algorithm itself as part of the evaluation function.
Reference: [18] <author> Pat Langley. </author> <title> Selection of relevant features in machine learning. In Relevance: </title> <booktitle> proc. 1994 AAAI Fall Symposium, </booktitle> <pages> pages 127-131. </pages> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The problem of feature subset selection (FSS hereafter) has long been an active research topic within statistics and pattern recognition (e.g., [9]), but most work in this area has dealt with linear regression. In the past few years, researchers in machine learning have realised (see for example, <ref> [18, 16] </ref>) that practical algorithms in supervised machine learning degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output. Therefore FSS has since received considerable attention from machine learning researchers interested in improving the performance of their algorithms. <p> Therefore FSS has since received considerable attention from machine learning researchers interested in improving the performance of their algorithms. Common machine learning algorithms, including top-down induction of decision trees, such as CART, ID3, and C4.5, and nearest-neighbour algorithms (such as instance-based learning), are known to suffer from irrelevant features <ref> [18, 19] </ref>. A good choice of features may not only help improve performance accuracy, but also aid in finding smaller models for the data, resulting in better understanding and interpretation of the data.
Reference: [19] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine learning, neural and statistical classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Therefore FSS has since received considerable attention from machine learning researchers interested in improving the performance of their algorithms. Common machine learning algorithms, including top-down induction of decision trees, such as CART, ID3, and C4.5, and nearest-neighbour algorithms (such as instance-based learning), are known to suffer from irrelevant features <ref> [18, 19] </ref>. A good choice of features may not only help improve performance accuracy, but also aid in finding smaller models for the data, resulting in better understanding and interpretation of the data.
Reference: [20] <author> J. Quinlan and R. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: Typically Occam simplicity is associated with the difficulty of implementing a given task, namely complexity of implementation. For example, the number of hidden neurons in neural networks [3]; the number of leaf nodes of a decision tree [10, 11]; the minimum description length (MDL) <ref> [21, 20] </ref>; and the encoding length [23]. However, Wolpert [27] noticed that the complexity of implementation is not directly related to the issue of prediction or generalisation, therefore there is no direct reason to believe that minimisation of such a complexity measure will result in improvement of general-isation.
Reference: [21] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> Ann. Statist., </journal> <volume> 14 </volume> <pages> 1080-1100, </pages> <year> 1986. </year> <month> 11 </month>
Reference-contexts: Typically Occam simplicity is associated with the difficulty of implementing a given task, namely complexity of implementation. For example, the number of hidden neurons in neural networks [3]; the number of leaf nodes of a decision tree [10, 11]; the minimum description length (MDL) <ref> [21, 20] </ref>; and the encoding length [23]. However, Wolpert [27] noticed that the complexity of implementation is not directly related to the issue of prediction or generalisation, therefore there is no direct reason to believe that minimisation of such a complexity measure will result in improvement of general-isation.
Reference: [22] <author> J. C. Schlimmer. </author> <title> Efficiently inducing determinations: a complete and systematic search algo-rithm that uses optimal pruning. </title> <booktitle> In ML93, </booktitle> <pages> pages 284-290, </pages> <year> 1993. </year>
Reference-contexts: Typically, the feature subset which performs best for the induction algorithm will be selected. Both types of approach to FSS are closely related to the notion of relevance. For example, FOCUS [2], RELIEF [15] and Schlimmer's model <ref> [22] </ref> use "relevance" to estimate the goodness of feature subset in one way or another. Section 5.2 presents a review in this respect. <p> Here the relevancy and the threshold together determine whether a given feature subset is sufficient (or qualified) to describe the given dataset. But there is no direct justification as to why the feature subset determined in this way would perform better in predicting for future cases, i.e., necessary. In <ref> [22] </ref> a good subset is one of the minimal determinations, but nothing is mentioned as to which one is the best. <p> Compared to FOCUS, this method is computationally efficient. Furthermore, it allows features to be ranked by relevance. Schlimmer <ref> [22] </ref> described a related approach that carries out a systematic search through the space of feature sets for all (not just the one with minimal cardinality) minimal determinations which are consistent with training dataset.
Reference: [23] <author> H. Schweitzer. </author> <title> Occam algorithms for computing visual motion. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(11) </volume> <pages> 1033-1042, </pages> <year> 1995. </year>
Reference-contexts: For example, the number of hidden neurons in neural networks [3]; the number of leaf nodes of a decision tree [10, 11]; the minimum description length (MDL) [21, 20]; and the encoding length <ref> [23] </ref>. However, Wolpert [27] noticed that the complexity of implementation is not directly related to the issue of prediction or generalisation, therefore there is no direct reason to believe that minimisation of such a complexity measure will result in improvement of general-isation.
Reference: [24] <author> D. B. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill-climbing algorithms. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pages 293-301. </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Broadly speaking, FSS is to select a subset of features from the feature space which is good enough regarding its ability to describe the training dataset and to predict for future cases. There is a wealth of algorithms for FSS (see for example, <ref> [2, 15, 1, 17, 14, 24] </ref>). With regard to how to evaluate the goodness of a subset of features, the FSS methods fall into two broad categories: filter approach and wrapper approach, which are illustrated in Figures 1 and 2.
Reference: [25] <author> Hui Wang. </author> <title> Towards a unified framework of relevance . PhD thesis, </title> <institution> Faculty of Informatics, University of Ulster, </institution> <address> N. Ireland, UK, </address> <month> October </month> <year> 1996. </year> <note> http://www.infm.ulst.ac.uk/~hwang/thesis.ps. </note>
Reference-contexts: Although the wrapper approach does not use the relevance measure directly, it is shown [16] that the "optimal" feature subset obtained this way must be from the relevant feature set (strongly relevant and weakly relevant features). However, the mathematical foundation for FSS is still lacking [26]. In <ref> [25] </ref>, a unified framework for relevance was proposed. In this framework relevance is quantified and related to mutual information, and furthermore, it was shown that this quantification satisfies the axiomatic charac-terisations of relevance laid down by leading researchers in this area. <p> A straightforward algorithm is to systematically examine all feature subsets and find one which satisfies the above two axioms. Unfortunately, as shown in [8], this class of algorithms turns out to be NP-complete. Branch and bound based on the characteristics of relevance was attempted <ref> [25] </ref>, but it was shown to be also exponential in general. So we attempted heuristic approaches. Here we are to present our preferred heuristic FSS algorithm. Our objective is to find a sufficient subset of features, which is close to optimal in the above axiomatic sense. <p> The FOCUS [2] algorithm starts with an empty feature set and carries out breadth-first search until it finds a minimal combination of features which is consistent with the training dataset. The 8 features in are relevant to the target concept C. In terms of the relevance framework <ref> [25] </ref>, this requirement amounts to r (; C) = 1 and jj being minimum. RELIEF is a feature relevance estimation algorithm, but the meaning of relevance is different from ours and has not been theoretically justified. <p> However, Kohavi and Sommerfield show that the "optimal" feature set X obtained this way must be from the relevant feature set (strongly relevant and weakly relevant features). As shown in <ref> [25] </ref> their strong relevance and weak relevance can be characterised by our relevance formalism, so the wrapper scheme can also be modelled by our relevance, r (X; C) &gt; 0. However, Caruana and Freitag [5] observe that not all features that are relevant are necessarily useful for induction.
Reference: [26] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer systems that learn classification and predication methods from statistics, neural networks, machine learning, and expert systems. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1991. </year> <note> ISBN 1-55860-065-5. </note>
Reference-contexts: Although the wrapper approach does not use the relevance measure directly, it is shown [16] that the "optimal" feature subset obtained this way must be from the relevant feature set (strongly relevant and weakly relevant features). However, the mathematical foundation for FSS is still lacking <ref> [26] </ref>. In [25], a unified framework for relevance was proposed. In this framework relevance is quantified and related to mutual information, and furthermore, it was shown that this quantification satisfies the axiomatic charac-terisations of relevance laid down by leading researchers in this area.
Reference: [27] <author> D. H. Wolpert. </author> <title> The relationship between Occam's Razor and convergent guessing. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 319-368, </pages> <year> 1990. </year>
Reference-contexts: This principle is becoming influential in machine learning, where this principle can be formulated as: given two hypotheses that both are consistent with a training set of examples of a given task, the simpler one will guess better on future examples of this task <ref> [4, 27, 3] </ref>. It has been shown (see for example, [4]) that, under very general assumptions, Occam's razor produces hypotheses that with high probability will be predictive of future cases. One basic question is concerned with the meaning of "simplicity", namely Occam simplicity. <p> For example, the number of hidden neurons in neural networks [3]; the number of leaf nodes of a decision tree [10, 11]; the minimum description length (MDL) [21, 20]; and the encoding length [23]. However, Wolpert <ref> [27] </ref> noticed that the complexity of implementation is not directly related to the issue of prediction or generalisation, therefore there is no direct reason to believe that minimisation of such a complexity measure will result in improvement of general-isation. Wolpert [27] then derived the uniform simplicity measure, which is concerned exclusively <p> However, Wolpert <ref> [27] </ref> noticed that the complexity of implementation is not directly related to the issue of prediction or generalisation, therefore there is no direct reason to believe that minimisation of such a complexity measure will result in improvement of general-isation. Wolpert [27] then derived the uniform simplicity measure, which is concerned exclusively with how learning generalises. Wolpert showed [27] that when expressed in terms of the uniform simplicity measure Occam's razor is indeed a way to set up a good generaliser. <p> Wolpert <ref> [27] </ref> then derived the uniform simplicity measure, which is concerned exclusively with how learning generalises. Wolpert showed [27] that when expressed in terms of the uniform simplicity measure Occam's razor is indeed a way to set up a good generaliser. <p> The main disadvantage of uniform simplicity measure is that the calculation of it needs "all learning sets and all questions", as well as guessing distribution and simplicity distribution <ref> [27] </ref>. This is impossible in practice. It seems that uniform simplicity measures have only theoretical significance. Fortunately many of the conventional simplicity measures are shown to be rough approximations to the uniform simplicity measure [27]. In practice we can only rely on approximations, like those mentioned above. <p> it needs "all learning sets and all questions", as well as guessing distribution and simplicity distribution <ref> [27] </ref>. This is impossible in practice. It seems that uniform simplicity measures have only theoretical significance. Fortunately many of the conventional simplicity measures are shown to be rough approximations to the uniform simplicity measure [27]. In practice we can only rely on approximations, like those mentioned above. Back to our problem: Most of the practical simplicity measures (approximations to uniform simplicity measure) are model-dependent. However we are looking at FSS independently of any learning model, so a model-independent simplicity measure is required.
References-found: 27

