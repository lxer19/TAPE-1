URL: http://www.cs.unm.edu/~bap/papers/colt-96.ps.gz
Refering-URL: http://www.cs.unm.edu/~bap/publications.html
Root-URL: http://www.cs.unm.edu
Email: zador@salk.edu  barak.pearlmutter@alumni.cs.cmu.edu  
Title: VC Dimension of an Integrate-and-Fire Neuron Model  
Author: Anthony M. Zador Barak A. Pearlmutter 
Address: 10010 N. Torrey Pines Rd. La Jolla, CA 92037  MS 0515 9500 Gilman Drive La Jolla, CA 92093-0515  
Affiliation: The Salk Institute  Dept. of Cog. Sci., UCSD  
Abstract: We find the VC dimension of a leaky integrate-and-fire neuron model. The VC dimension quantifies the ability of a function class to partition an input pattern space, and can be considered a measure of computational capacity. In this case, the function class is the class of integrate-and-fire models generated by varying the integration time constant o and the threshold , the input space they partition is the space of continuous-time signals, and the binary partition is specified by whether or not the model reaches threshold and spikes at some specified time. We show that the VC dimension diverges only logarithmically with the input signal bandwidth N , where the signal bandwidth is determined by the noise inherent in the process of spike generation. For reasonable estimates of the signal bandwidth, the VC dimension turns out to be quite small (10). We also extend this approach to arbitrary passive dendritic trees. The main contributions of this work are (1) it offers a novel treatment of the computational capacity of this class of dynamic system; and (2) it provides a framework for analyzing the computational capabilities of the dynamical systems defined by networks of spiking neurons. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abu-Mostafa, Y. S. </author> <year> (1989). </year> <title> The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning. </title> <journal> Neural Computation, </journal> <volume> 1(3), </volume> <pages> 312-317. </pages>
Reference: <author> Anthony, M. </author> <year> (1994). </year> <title> Probabilistic Analysis of Learning in Artificial Neural Networks: The PAC Model and its Variants. Mathematics preprint series LSE-MPS-67, </title> <institution> Dept. of Mathematics, London School of Economics, Houghton St., London WC2A 2AE, UK. </institution> <note> Also available as NeuroCOLT technical report NC-TR-94-3, ftp://cscx.cs.rhbnc.ac.uk/pub/neurocolt/tech reports/. </note>
Reference: <author> Bartlett, P. L., Long, P. M., and Williamson, R. C. </author> <year> (1994). </year> <title> FAT-Shattering and the Learnability of Real-Valued Functions.. </title> <booktitle> In COLT (1994), </booktitle> <pages> pp. 299-310. </pages>
Reference: <author> Baum, E. and Haussler, D. </author> <year> (1989). </year> <title> What Size Net Gives Valid Generalization?. </title> <journal> Neural Computation, </journal> <volume> 1(1), </volume> <pages> 151-160. </pages>
Reference-contexts: This is true, for example, in feedforward linear threshold networks, where the VC dimension is equal to the number of free weights, up to a logarithmic factor <ref> (Baum and Haus-sler 1989) </ref>. In our case, we expected the VC dimension to be about two, since there were two free parameters ( and o .) Furthermore, a small VC dimension for the integrate-and-fire model conforms to our intuitive notion of the simplicity of this model.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis Dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36, </volume> <pages> 929-965. </pages>
Reference-contexts: It gives an upper bound on the number of exemplars required to guar-antee that a set of parameters fit to data will provide a good fit for new data <ref> (Blumer et al. 1989) </ref>. It has been applied in the neural network literature to give a measure of the number of patterns needed to train a network of a given size.
Reference: <author> Brown, T. H., Zador, A. M., Mainen, Z. F., and Claiborne, B. J. </author> <year> (1992). </year> <booktitle> Hebbian Computations in Hippocampal Dendrites and Spines. In McKenna et al.(1992), </booktitle> <pages> pp. 81-116. </pages>
Reference: <author> Bryant and Segundo (1976). </author> <title> Spike Initiation by Transmem-brane Current: A White-Noise Analysis. </title> <journal> Journal of Physiology, </journal> <volume> 260, </volume> <pages> 279-314. </pages> <booktitle> COLT (1994). Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> Haussler, D., Kearns, M., Seung, H. S., and Tishby, N. </author> <year> (1994). </year> <title> Rigorous Learning Curve Bounds from Statistical Mechanics.. </title> <booktitle> In COLT (1994), </booktitle> <pages> pp. 67-75. </pages>
Reference-contexts: In both cases, the apparent VC dimension in the presense of noise conformed much more closely with our intuitive notion that it should be rather small. It will be interesting to see whether related notions of computational capacity, such as those derived from work on average generalization <ref> (Haussler, Kearns, Seung, and Tishby 1994) </ref>, can be extended to dynamical systems in a similar way. Acknowledgments This paper previously appeared in journal form (Zador and Pearlmutter 1996).
Reference: <author> Hille, B. </author> <year> (1992). </year> <title> Ionic Channels of Excitable Membranes (Second edition). </title> <publisher> Sinauer Associates, </publisher> <address> Sunderland, Massachusetts. </address>
Reference: <author> Hodgkin, A. and Huxley, A. </author> <year> (1952). </year> <title> Currents carried by sodium and potassium ions through the membrane of the giant axon of Loligo. </title> <journal> Journal of Physiology, </journal> <volume> 116, </volume> <pages> 449-472. </pages>
Reference: <author> Koch, C., Bernander, O., and Douglas, R. J. </author> <year> (1995). </year> <title> Do neurons have a voltage or a current threshold for action potential initiation?. </title> <journal> Journal of Computational Neu-roscience, </journal> <volume> 2, </volume> <pages> 63-82. </pages>
Reference-contexts: To quote Bryant and Segundo (1976), A simple model of the spike-triggering system, consisting of a linear filter (first-order Wiener kernel) folowed by a threshold device with dead-time, was quite accurate in predicting experimentally observed spike timings. Similar results seem to hold in cortical neurons <ref> (Koch et al. 1995) </ref>. The success of simple leaky integrate-and-fire models is not surprising in light of all that is known about neuronal biophysics. Neurons can be thought of as nonlinear distributed electrical circuits. <p> McKenna et al. (1992).) The leaky integrate-and-fire model with reset is nevertheless a standard starting point for considering dynamical aspects of neuron behavior. A recent careful examination of its validity <ref> (Koch et al. 1995) </ref> cepts which shatters some set of M inputs. Here we have shown something more general, since our construction proceeds for any set of 2 M concepts from our concept class.
Reference: <author> Koch, C., Poggio, T., and Torre, V. </author> <year> (1982). </year> <title> Retinal Ganglion Cells: a Functional Interpretation of Dendritic Morphology. </title> <journal> Proc. of the Royal Soc. of London B, </journal> <volume> 298, </volume> <pages> 227-264. </pages>
Reference: <author> Maass, W. </author> <year> (1995). </year> <title> Vapnik-Chervonenkis Dimension of Neural Networks. </title> <editor> In Arbib, M. A. (Ed.), </editor> <booktitle> Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pp. 1000-1002. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Maass, W. </author> <year> (1996). </year> <title> Lower bounds for the computational power of networks of spiking neurons. </title> <booktitle> Neural Computation, </booktitle> <pages> 8(1). </pages>
Reference: <editor> McKenna, T., Davis, J., and Zornetzer, S. F. (Eds.). </editor> <year> (1992). </year> <title> Single Neuron Computation. </title> <publisher> Academic Press. </publisher>
Reference: <author> Mel, B. W. </author> <year> (1992). </year> <title> NMDA-Based Pattern Discrimination in a Modeled Cortical Neuron. </title> <journal> Neural Computation, </journal> <volume> 4(4), </volume> <pages> 502-516. </pages>
Reference: <editor> Sakmann, B. and Neher, E. (Eds.). </editor> <year> (1983). </year> <title> Single Channel Recording. </title> <publisher> Plenum Press. </publisher>
Reference: <author> Shepherd, G. and Brayton, R. </author> <year> (1987). </year> <title> Logic operations are properties of computer-simulated interactions between excitable dendritic spines. </title> <journal> Journal of Neuroscience, </journal> <volume> 21, </volume> <pages> 151-166. </pages>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11), </volume> <pages> 1134-1142. </pages>
Reference-contexts: Given any set of 2 M concepts, we can find a set of M inputs that these concepts shatter. This has consequences in the application to PAC learning <ref> (Valiant 1984) </ref>, where it corresponds to generalizing one of the two worst-case assumptions of the PAC criterion.
Reference: <author> Vapnik, V. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities. </title> <journal> Theory of Probability and Its Applications, </journal> <volume> 16, </volume> <pages> 264-280. </pages>
Reference-contexts: We show that the VC dimension diverges logarithmically with the input signal bandwidth N . 2 Review of VC dimension The VC dimension <ref> (Vapnik and Chervonenkis 1971) </ref> is a measure of the richness of a class of boolean functions. It gives an upper bound on the number of exemplars required to guar-antee that a set of parameters fit to data will provide a good fit for new data (Blumer et al. 1989). <p> In the context of learning theory the VC dimension is useful because of a relation between the number of labeled exemplars in a training set and the probability of generating the correct output on a new exemplar <ref> (Vapnik and Chervonen-kis 1971) </ref>. If the number of exemplars is greater than the VC dimension, then the probability of producing an incorrect response decreases exponentially with the number of exemplars.
Reference: <author> Zador, A. M., Claiborne, B. J., and Brown, T. H. </author> <year> (1992). </year> <title> Nonlinear Pattern Separation in Single Hippocampal Neurons with Active Dendritic Membrane. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pp. 51-58. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Zador, A. M. and Pearlmutter, B. A. </author> <year> (1996). </year> <title> VC dimension of an Integrate and Fire Neuron Model. </title> <journal> Neural Computation, </journal> <volume> 8(3), </volume> <pages> 611-624. </pages>
Reference-contexts: It will be interesting to see whether related notions of computational capacity, such as those derived from work on average generalization (Haussler, Kearns, Seung, and Tishby 1994), can be extended to dynamical systems in a similar way. Acknowledgments This paper previously appeared in journal form <ref> (Zador and Pearlmutter 1996) </ref>.
References-found: 22

