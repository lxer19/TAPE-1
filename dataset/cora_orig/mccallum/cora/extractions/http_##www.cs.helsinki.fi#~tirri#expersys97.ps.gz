URL: http://www.cs.helsinki.fi/~tirri/expersys97.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Title: A Bayesian Approach for Retrieving Relevant Cases  
Author: Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri 
Keyword: Case-based reasoning, Probability theory, Bayesian similarity metrics  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Note: Pp. 67-72 in Artificial Intelligence Applications (Proceedings of the EXPERSYS-97 Confer- ence), edited by P.Smith. IITT International, Gournay sur Marne, 1997.  
Email: Email: cosco@cs.Helsinki.FI  
Phone: 26,  
Web: URL: http://www.cs.Helsinki.FI/research/cosco/  
Abstract: The problem of finding the set of most relevant cases from a given database, with respect to the decision making situation at hand, is frequently encountered in many real-world domains. In the case-based reasoning framework this task is commonly known as the case matching problem. Case matching is an important problem in several commercially significant application areas, such as industrial configuration and manufacturing problems. Earlier approaches to the case matching problem typically rely on some distance measure, e.g., the Euclidean distance, although there is no a priori guarantee that such measures really reflect the useful similarities and dissimilarities between the cases. In this paper we introduce a novel approach to the case matching problem based on Bayesian probability theory, and propose a Bayesian case matching measure for scoring the cases with respect to a given decision making situation. The Bayesian case matching score discussed is currently being applied in a real-world manufacturing problem. The empirical results show that when encountered with cases where many of the feature values have been removed, even a relatively small number of remaining values is sufficient for retrieving the original case from the case base by using the proposed measure. The experiments also show that the approach is computationally very efficient. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Aha. </author> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathematical, Empirical, an Psychological Observations. </title> <type> PhD thesis, </type> <institution> University of California, Irvine, </institution> <year> 1990. </year>
Reference-contexts: In traditional machine learning, this distribution is typically approximated by using a single model, e.g., a decision-tree or a neural network, which is constructed by using the given sample of problem domain data. In contrast to this model-based approach, in the case-based reasoning (CBR) approach <ref> [1, 3, 14, 18, 21] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: [2] <author> D. Aha. </author> <note> Editorial for a special issue on lazy learning. Artificial Intelligence Review (to appear), </note> <year> 1997. </year>
Reference-contexts: This type of machine learning is often also referred to as lazy learning, since the algorithms defer all the essential computation until the prediction phase <ref> [2] </ref>. For making predictions, the CBR algorithms typically use a distance function (e.g., Euc-lidean distance) for scoring the cases, i.e., determining the most relevant data items for the prediction task in question (case matching). <p> In [23, 22, 20, 19] we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24]. The approach suggested can be seen as a partially lazy approach <ref> [2] </ref>, i.e., a hybrid between the traditional machine learning and the lazy learning approach, which is based solely on the given data. The studies were based on the probabilistic viewpoint, where groups of data vectors are transformed into distributions, which can be seen as sample points in a distribution space.
Reference: [3] <author> C. Atkeson. </author> <title> Memory based approaches to approximating continuous functions. </title> <editor> In M. Casdagli and S. Eubank, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting. Proceedings Volume XII in the Santa Fe Institute Studies in the Sciences of Complexity. </booktitle> <publisher> Addison Wesley, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: In traditional machine learning, this distribution is typically approximated by using a single model, e.g., a decision-tree or a neural network, which is constructed by using the given sample of problem domain data. In contrast to this model-based approach, in the case-based reasoning (CBR) approach <ref> [1, 3, 14, 18, 21] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: [4] <author> C. Atkeson, A. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <note> AI Review to appear, </note> <year> 1995. </year>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function <ref> [4, 11] </ref>. In [23, 22, 20, 19] we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24].
Reference: [5] <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference: [6] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Further assuming that the parameter vectors ff and ki are independent, and applying the results in <ref> [6, 12] </ref>, the Bayesian case matching score (2) for case ~ d j can be computed by S ( ~ d j j~u) = k=1 K X h k + k P K Y f kiu i + kiu i P n i ; (3) where the product goes over all
Reference: [7] <author> M.H. </author> <title> DeGroot. Optimal statistical decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: Since the family of Dirichlet densities is conjugate (see e.g. <ref> [7] </ref>) to the family of multinomials, we assume that (ff 1 ; : : : ; ff K ) ~ Di ( 1 ; : : : ; K ), and ( ki1 ; : : : ; kin i ) ~ Di ( ki1 ; : : : ; kin
Reference: [8] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11]. In [23, 22, 20, 19] we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family <ref> [8, 24] </ref>. The approach suggested can be seen as a partially lazy approach [2], i.e., a hybrid between the traditional machine learning and the lazy learning approach, which is based solely on the given data.
Reference: [9] <author> D. Fischer. </author> <title> Noise-tolerant conceptual clustering. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 825-830, </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: The predictive distributions required for making predictions could then be computed by using the CBR approach in this distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks has been suggested in <ref> [9, 10, 13] </ref>. In [15] we presented a new, improved probabilistic formalization of CBR of a purely lazy nature. The framework extended our earlier results by presenting a Bayesian approach for making (discrete) predictions directly from data, without the transformation step between the original sample space and the distribution space.
Reference: [10] <author> D. Fischer and D. Talbert. </author> <title> Inference using probabilistic concept trees. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 191-202, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: The predictive distributions required for making predictions could then be computed by using the CBR approach in this distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks has been suggested in <ref> [9, 10, 13] </ref>. In [15] we presented a new, improved probabilistic formalization of CBR of a purely lazy nature. The framework extended our earlier results by presenting a Bayesian approach for making (discrete) predictions directly from data, without the transformation step between the original sample space and the distribution space.
Reference: [11] <author> J.H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <type> Unpublished manuscript. </type> <institution> Available by anonymous ftp from Stanford Research Institute (Menlo Park, CA) at playfair.stanford.edu., </institution> <year> 1994. </year>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function <ref> [4, 11] </ref>. In [23, 22, 20, 19] we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24].
Reference: [12] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Further assuming that the parameter vectors ff and ki are independent, and applying the results in <ref> [6, 12] </ref>, the Bayesian case matching score (2) for case ~ d j can be computed by S ( ~ d j j~u) = k=1 K X h k + k P K Y f kiu i + kiu i P n i ; (3) where the product goes over all
Reference: [13] <author> S. Kasif, S. Salzberg, D. Waltz, J. Rachlin, and D. Aha. </author> <title> Towards a framework for memory-based reasoning. </title> <type> Manuscript, </type> <note> in review., </note> <year> 1995. </year>
Reference-contexts: The predictive distributions required for making predictions could then be computed by using the CBR approach in this distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks has been suggested in <ref> [9, 10, 13] </ref>. In [15] we presented a new, improved probabilistic formalization of CBR of a purely lazy nature. The framework extended our earlier results by presenting a Bayesian approach for making (discrete) predictions directly from data, without the transformation step between the original sample space and the distribution space.
Reference: [14] <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: In traditional machine learning, this distribution is typically approximated by using a single model, e.g., a decision-tree or a neural network, which is constructed by using the given sample of problem domain data. In contrast to this model-based approach, in the case-based reasoning (CBR) approach <ref> [1, 3, 14, 18, 21] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: [15] <author> P. Kontkanen, P. Myllymaki, T. Silander, and H. Tirri. </author> <title> Bayes optimal lazy learning. Technical Report NC-TR-97-031, </title> <booktitle> ESPRIT Working Group 8556: Neural and Computational Learning (NeuroCOLT), </booktitle> <year> 1997. </year>
Reference-contexts: The predictive distributions required for making predictions could then be computed by using the CBR approach in this distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks has been suggested in [9, 10, 13]. In <ref> [15] </ref> we presented a new, improved probabilistic formalization of CBR of a purely lazy nature. The framework extended our earlier results by presenting a Bayesian approach for making (discrete) predictions directly from data, without the transformation step between the original sample space and the distribution space. <p> Quite interestingly, in <ref> [15] </ref> we showed that with the Bayesian CBR approach, the case adaptation problem can be solved directly in one pass, without the standard two-phase CBR methodology where the cases are first ordered according to some scoring function, and the best cases are then used in the adaptation phase for making predictions. <p> From the Bayesian point of view, we can regard the CBR approach as a requirement for marginalizing, i.e., integrating, out all the individual models. This means that the problem domain probability distribution is approximated by P ( ~ d) P ( ~ d j D; ) = fi2M In <ref> [15] </ref> we showed how formula (1) can be used for solving case adaptation tasks, i.e., for predicting the values of the unknown variables, denoted here by V, given the values of the known values U = X n V, and the sample data D.
Reference: [16] <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: It should also be emphasized that although the particular set of assumptions used in the experiments may at first sight appear overly simplifying, the empirical results <ref> [16] </ref> show that with such assumptions the approach may perform surprisingly well in terms of prediction accuracy, when compared to the results obtained by alternative methods. The experiments also showed that the BCBR approach is computationally very efficient. Acknowledgments.
Reference: [17] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: Some simple function, such as majority voting in classification problems, can then used for determining the prediction from the most relevant data items (case adaptation). It has been shown in various studies (see e.g., <ref> [17] </ref> for references) 68 EXPERSYS-97 that this type of an approach can in some cases produce quite accurate predictions, when compared to alternative machine learning methods. However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11].
Reference: [18] <author> A. Moore. </author> <title> Acquisition of dynamic control knowledge for a robotic manipulator. </title> <booktitle> In Seventh International Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In traditional machine learning, this distribution is typically approximated by using a single model, e.g., a decision-tree or a neural network, which is constructed by using the given sample of problem domain data. In contrast to this model-based approach, in the case-based reasoning (CBR) approach <ref> [1, 3, 14, 18, 21] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: [19] <author> P. Myllymaki and H. Tirri. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Francisco, March 1993. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11]. In <ref> [23, 22, 20, 19] </ref> we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24].
Reference: [20] <author> P. Myllymaki and H. Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11]. In <ref> [23, 22, 20, 19] </ref> we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24].
Reference: [21] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: In traditional machine learning, this distribution is typically approximated by using a single model, e.g., a decision-tree or a neural network, which is constructed by using the given sample of problem domain data. In contrast to this model-based approach, in the case-based reasoning (CBR) approach <ref> [1, 3, 14, 18, 21] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: [22] <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> A Bayesian framework for case-based reasoning. </title> <editor> In I. Smith and B. Faltings, editors, </editor> <booktitle> Advances in Case-Based Reasoning, volume 1168 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 413-427, </pages> <booktitle> Proceedings of the 3rd European Workshop, </booktitle> <address> Lausanne, Switzerland, November 1996. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg. </address>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11]. In <ref> [23, 22, 20, 19] </ref> we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24].
Reference: [23] <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11]. In <ref> [23, 22, 20, 19] </ref> we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family [8, 24].
Reference: [24] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: However, the performance of the algorithms seems to be highly sensitive to the selection of the distance function [4, 11]. In [23, 22, 20, 19] we proposed a Bayesian framework for CBR based on probability theory and the finite mixture model family <ref> [8, 24] </ref>. The approach suggested can be seen as a partially lazy approach [2], i.e., a hybrid between the traditional machine learning and the lazy learning approach, which is based solely on the given data.
References-found: 24

