URL: http://www.cs.ucsb.edu/oocsb/self/papers/hoelzle-thesis.ps.Z
Refering-URL: http://www.cs.ucsb.edu/oocsb/self/papers/urs-thesis.html
Root-URL: http://www.cs.ucsb.edu
Title: ADAPTIVE OPTIMIZATION FOR SELF: RECONCILING HIGH PERFORMANCE WITH EXPLORATORY PROGRAMMING  
Author: Urs Hlzle 
Degree: A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY  
Date: August 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Mark B. Abbot and Larry L. Peterson. </author> <title> A Language-Based Approach to Protocol Implementation. </title> <journal> Computer Communications Review 22 (4): </journal> <pages> 27-38, </pages> <year> 1992. </year>
Reference-contexts: Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences [103]. In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks <ref> [1] </ref>, and dynamic linking [67]. <p> Non-predicted messages are much slower The first example is taken from the Smalltalk world, where a Smalltalk programmer noted on the newsgroup comp.- lang.smalltalk that the following two code sequences had vastly different performance even though they perform the same amount of work (in source-level terms): nil1: x isNil ifTrue: <ref> [ 1 ] </ref> ifFalse: [ 2 ] nil2: x ifNil: [ 1 ] ifNotNil: [ 2 ] The programmer was puzzled by the fact that the first expression ran almost ten times faster on his Smalltalk system than the second expression, even though it involves two sends (isNil and ifTrue:ifFalse:) instead <p> from the Smalltalk world, where a Smalltalk programmer noted on the newsgroup comp.- lang.smalltalk that the following two code sequences had vastly different performance even though they perform the same amount of work (in source-level terms): nil1: x isNil ifTrue: <ref> [ 1 ] </ref> ifFalse: [ 2 ] nil2: x ifNil: [ 1 ] ifNotNil: [ 2 ] The programmer was puzzled by the fact that the first expression ran almost ten times faster on his Smalltalk system than the second expression, even though it involves two sends (isNil and ifTrue:ifFalse:) instead of just one.
Reference: [2] <author> Ali-Reza Adl-Tabatabai and Thomas Gross. </author> <title> Detection and recovery of endangered variables caused by instruction scheduling. </title> <booktitle> In PLDI '93 Conference Proceedings, </booktitle> <pages> pp. 13-25. </pages> <note> Published in SIGPLAN Notices 28(6), </note> <month> June </month> <year> 1993. </year>
Reference-contexts: slower The first example is taken from the Smalltalk world, where a Smalltalk programmer noted on the newsgroup comp.- lang.smalltalk that the following two code sequences had vastly different performance even though they perform the same amount of work (in source-level terms): nil1: x isNil ifTrue: [ 1 ] ifFalse: <ref> [ 2 ] </ref> nil2: x ifNil: [ 1 ] ifNotNil: [ 2 ] The programmer was puzzled by the fact that the first expression ran almost ten times faster on his Smalltalk system than the second expression, even though it involves two sends (isNil and ifTrue:ifFalse:) instead of just one. <p> where a Smalltalk programmer noted on the newsgroup comp.- lang.smalltalk that the following two code sequences had vastly different performance even though they perform the same amount of work (in source-level terms): nil1: x isNil ifTrue: [ 1 ] ifFalse: <ref> [ 2 ] </ref> nil2: x ifNil: [ 1 ] ifNotNil: [ 2 ] The programmer was puzzled by the fact that the first expression ran almost ten times faster on his Smalltalk system than the second expression, even though it involves two sends (isNil and ifTrue:ifFalse:) instead of just one. <p> Unfortunately, if the source-level state of the program must be recoverable at every point in the program, i.e., at virtually every instruction boundary (to support single-stepping), existing techniques <ref> [2, 3, 65, 143, 37] </ref> severely restrict the optimizations that could be performed, effectively disabling many common optimizations. We solve this dilemma by relaxing the debugging requirements placed on optimized code. <p> Similarly, some other optimizations (e.g., code motion or induction variable elimination) may sometimes not be performed if the debuggers recovery techniques are not powerful enough to hide the optimizations effects at one or more interrupt points. The extent of this problems depends on the particular recovery techniques used (e.g., <ref> [2, 3, 65, 143, 37] </ref>) and on the average distance between interrupt points. All these optimizations can be performed, however, if there is no interrupt point within the affected variables scope (see Section 10.2.4). <p> Code sequences are restricted so that no stores are completed unless the entire statement runs to completion. Thus, if an error occurs during execution of the statement, the debugger can display the program state as if the program were stopped at the beginning of the statement. Adl-Tabatabai et al. <ref> [2, 3] </ref> investigate the problem of detecting and recovering the values of source-level variables in the presence of instruction scheduling. Other recovery mechanisms are described by Coutant et al. [37] and by Schlaeppi and Warren [114].
Reference: [3] <author> Ali-Reza Adl-Tabatabai and Thomas Gross. </author> <title> Evicted variables and the interaction of global register allocation and symbolic debugging. </title> <booktitle> In POPL 93 Conference Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: Instead of performing splitting in the front end, we could obtain similarly good code with back end optimization. Heintz [64] proposes such an optimization (called rerouting predecessors) for a Smalltalk compiler, and similar back-end optimizations for conventional languages are well-known <ref> [3, 10] </ref>. <p> Unfortunately, if the source-level state of the program must be recoverable at every point in the program, i.e., at virtually every instruction boundary (to support single-stepping), existing techniques <ref> [2, 3, 65, 143, 37] </ref> severely restrict the optimizations that could be performed, effectively disabling many common optimizations. We solve this dilemma by relaxing the debugging requirements placed on optimized code. <p> Similarly, some other optimizations (e.g., code motion or induction variable elimination) may sometimes not be performed if the debuggers recovery techniques are not powerful enough to hide the optimizations effects at one or more interrupt points. The extent of this problems depends on the particular recovery techniques used (e.g., <ref> [2, 3, 65, 143, 37] </ref>) and on the average distance between interrupt points. All these optimizations can be performed, however, if there is no interrupt point within the affected variables scope (see Section 10.2.4). <p> Code sequences are restricted so that no stores are completed unless the entire statement runs to completion. Thus, if an error occurs during execution of the statement, the debugger can display the program state as if the program were stopped at the beginning of the statement. Adl-Tabatabai et al. <ref> [2, 3] </ref> investigate the problem of detecting and recovering the values of source-level variables in the presence of instruction scheduling. Other recovery mechanisms are described by Coutant et al. [37] and by Schlaeppi and Warren [114].
Reference: [4] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. CompilersPrinciples, </author> <title> Techniques, Tools. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference: [5] <author> Ole Agesen, Jens Palsberg, and Michael I. Schwartzbach. </author> <title> Type Inference of SELF: Analysis of Objects with Dynamic and Multiple Inheritance. </title> <booktitle> In ECOOP '93 Conference Proceedings, p. </booktitle> <pages> 247-267. </pages> <address> Kaiserslautern, Germany, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Some forms of type inference can infer concrete receiver types, thus enabling the compiler to inline sends. For example, the inferencer described by Agesen et al. <ref> [5] </ref> can infer the set of concrete possible receiver types for every expression in a SELF program. Compared to type feedback information, type inference may compute better (smaller) sets for some sends because it can guarantee the absence of the unknown type. <p> (the com piler shares about 80% of its code with the interpreter, CecilInt) CecilInt 9,000 interpreter for the Cecil language [27] running a short Cecil test program Mango 7,000 automatically generated lexer/parser for ANSI C, parsing a 700 line C file Typeinf 8,600 type inferencer for SELF as described in <ref> [5] </ref> UI1 15,200 prototype user interface using animation techniques [28] b UI3 4,000 experimental 3D user interface Table 7-1. <p> However, such optimizations could be useful in an application extractor generating a stand-alone executable for an application, or in a system using batch-style compilation. Such an optimizer could also use additional information from profilers [30] or from type inferencers <ref> [5] </ref> to generate even better code. Finally, SELF-93 inherits the overcustomization problem from previous SELF systems.
Reference: [6] <author> Frances E. Allen and John Cocke. </author> <title> A Catalogue of Optimizing Transformations. In Design and Optimization of Compilers, </title> <editor> ed. R. Rustin. </editor> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference: [7] <author> Pascal Andr and Jean-Claude Royer. </author> <title> Optimizing method search with lookup caches and incremental coloring. </title> <booktitle> OOPSLA 92 Conference Proceedings, p. </booktitle> <pages> 110-127, </pages> <address> Vancouver, BC, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: However, it is possible to use dispatch tables in dynamically-typed languages as well, albeit with some added complications <ref> [7, 50, 135] </ref>.
Reference: [8] <institution> Apple Computer, Inc. </institution> <note> Object Pascal Users Manual. Cupertino, </note> <year> 1988. </year>
Reference-contexts: For example, the Apple Object Pascal linker <ref> [8] </ref> turned dynamically-dispatched calls into statically-bound calls if a type had exactly one implementation (e.g., the system contained only a CartesianPoint class and no PolarPoint class).
Reference: [9] <author> A. H. Borning and D. H. H. Ingalls. </author> <title> A Type Declaration and Inference System for Smalltalk. </title> <booktitle> In Conference Record of the Ninth Annual Symposium on Foundations of Computer Science, p. </booktitle> <pages> 133-139, </pages> <year> 1982. </year>
Reference: [10] <author> Frank Mueller and David B. Whalley. </author> <title> Avoiding Unconditional Jumps by Code Replication. </title> <booktitle> In Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 322-330. </pages> <note> Published as SIGPLAN Notices 27(7), </note> <month> July </month> <year> 1992. </year>
Reference-contexts: Instead of performing splitting in the front end, we could obtain similarly good code with back end optimization. Heintz [64] proposes such an optimization (called rerouting predecessors) for a Smalltalk compiler, and similar back-end optimizations for conventional languages are well-known <ref> [3, 10] </ref>.
Reference: [11] <author> Robert G. Atkinson. </author> <title> Hurricane: An Optimizing Compiler for Smalltalk. </title> <booktitle> In OOPSLA 86 Conference Proceedings, p. </booktitle> <pages> 151-158, </pages> <address> Portland, OR, </address> <month> September, </month> <year> 1986. </year> <note> Published as SIGPLAN Notices 21(11), </note> <month> November </month> <year> 1986. </year>
Reference-contexts: Atkinson partially implemented a Smalltalk compiler that used type declarations as hints to generate better code <ref> [11] </ref>. For example, if the programmer declared a local variable to be of type class X, the compiler could look up message sends to that variable and inline the called method.
Reference: [12] <author> J. R. Bell. </author> <title> Threaded Code. </title> <journal> Communications of the ACM 16 </journal> <pages> 370-372, </pages> <year> 1973. </year>
Reference-contexts: For example, Mitchell [97] proposed to convert parts of dynamically-typed interpreted programs into compiled form, assuming that the types of variables remained constant. Compiled code was generated as a side-effect of interpreting an expression for the first time. Similarly, threaded code <ref> [12] </ref> was used early on to remove some of the interpretation overhead. Interpreters or dynamic compilers traditionally have been used for two reasons: first, some languages are hard to efficiently compile statically, usually because the source code alone does not contain enough low-level implementation type information to generate efficient code.
Reference: [13] <author> A. H. Borning. </author> <title> Classes Versus Prototypes in Object-Oriented Languages. </title> <booktitle> In Proceedings of the 1986 ACM/IEEE Fall Joint Computer Conference, p. </booktitle> <pages> 36-40, </pages> <address> Dallas, TX, </address> <month> November </month> <year> 1986. </year>
Reference: [14] <author> Brian K. Bray and M. J. Flynn. </author> <title> Write caches as an alternative to write buffers. </title> <type> Technical Report CSL-TR 91-470, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The SPARCstation-2s write buffer organization is somewhat unfortunate and incurs high write costs compared to other common designs. To avoid skewing our data, we chose not to model the SS-2 write buffer and instead assume a perfect write buffer. Previous work (e.g., <ref> [14, 81] </ref>) has shown that writes can be absorbed with virtually no cost, and Diwan et al. [49] report a write buffer CPI contribution of less than 0.02 for a 6-element deep buffer and a cache organization similar to the DECstation 500/200. 70 back ends. <p> We use time overhead rather than miss ratios because comparing miss ratios can be misleading. First, write misses are free since we assume they can be absorbed by a write buffer or write cache <ref> [14, 81] </ref>. Furthermore, data miss ratios are not directly related to execution time if (as is customary) they are given relative to the number of data references since the density of loads may vary from program to program.
Reference: [15] <author> M. A. Bulyonkov. </author> <title> Polyvariant mixed computation for analyzer programs. </title> <journal> Acta Informatica 21, </journal> <volume> p. </volume> <pages> 473-484, </pages> <year> 1984. </year>
Reference-contexts: Przybylski et al. generate a specialized cache simulator for each different set of simulation parameters [104]. Keppel et al. discuss value-specific runtime specialization for a variety of applications [83]. In more theoretically-oriented areas of computer science, customization is known as partial evaluation <ref> [15] </ref>. 2.5.3 Previous SELF compilers The SELF compiler described in this thesis has two predecessors. The first SELF compiler [22, 23] introduced customization and splitting and achieved reasonably good performance.
Reference: [16] <author> Brad Calder, Dirk Grunwald, and Benjamin Zorn. </author> <title> Quantifying Behavioral Differences Between C and C++ Programs. </title> <type> Technical Report CU-CS-698-94, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: C++s execution behavior (and language philosophy) is much further away from SELF, but we believe it will nevertheless benefit from type feedback. First, measurements of large C++ programs <ref> [16] </ref> have shown that calls are almost five times more frequent in C++ programs than in C programs, and that the average size of a C++ virtual function is only 30 instructions, six times smaller than the average C function. <p> future as they become more familiar with object-oriented programming styles; for example, recent versions of the Interviews framework [92] use virtual functions more frequently than previous versions. instrumented program source program optimized program compiler compiler type feedback data 43 To give a concrete example, the DOC document editor measured in <ref> [16] </ref> performs a virtual call every 75 instructions; given that a C++ virtual call uses about 5 instructions and usually incurs two load stalls and a stall for the indirect function call, we estimate that this program spends roughly 10% of its time dispatching virtual functions. <p> Even for a hybrid language like C++ (which has C at its core and thus shouldnt be too different), significant differences were found. Table 8-1 shows data from a study by Calder et al. <ref> [16] </ref> that measured several large C++ applications and compared them to the SPECint92 suite and other C programs. The study used the GNU C and C++ compilers on the MIPS architecture. <p> The middle box in each category (filled gray) represents complete SELF programs (i.e., all C++ SPECint92 ratio C++ / SPEC basic block size 8.0 4.9 1.6 call/return frequency 4.6% 0.7% 6.7 instructions per conditional branch 15.9 6.4 2.5 Table 8-1. Differences between C++ and C (from <ref> [16] </ref>) 90 user-mode instructions). We have measured both since SELF programs often call routines in the runtime system, for example, to allocate objects or call functions defined in C libraries. <p> If SELF used indirect function calls to implement message dispatch, one could explain the lower frequency of conditional branches with the object-oriented programming style which typically replaces if or switch statements with dynamic dispatch; Calder et al. observed this effect when comparing C++ programs to the SPEC C programs <ref> [16] </ref>. However, since the SELF implementation does not use indirect function calls, we cannot explain the difference in this way. It is possible that SELFs optimizer eliminates enough dispatch type tests to lower the overall frequency of comparisons. Table 8-2 summarizes the main differences in instruction usage. <p> Overall, there are few differences between SELF programs and the SPEC C programs, a surprising result considering that the two languages are very different. What is even more surprising is that SELF is closer to C than is C++. Figure 8-3 summarizes the C++ data from <ref> [16] </ref> and our own measurements. In each category, SELF is closer to SPEC than is C++.
Reference: [17] <author> Brad Calder and Dirk Grunwald. </author> <title> Reducing Indirect Function Call Overhead in C++ Programs. </title> <booktitle> In 21st Annual ACM Symposium on Principles of Programming Languages, p. </booktitle> <pages> 397-408, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: For type feedback to work well, the dynamic number of receiver types per call site should be close to one, i.e., one or two receiver types should dominate. A large fraction of call sites in C++ have this property <ref> [17, 57] </ref>, and it also holds in other object-oriented programming languages (e.g., Smalltalk, SELF, Sather, and Eiffel); this is the reason why inline caching [44, 70] works well in these languages as an implementation of dynamic dispatch. <p> Based on measurements of C++ programs, Calder and Grunwald <ref> [17] </ref> argue that type feedback would be beneficial for C++. Unfortunately, they apparently were not aware of previous work; their proposed if conversion appears to be identical to inline caching [44], and a proposed extension of if conversion is identical to type feedback (which was first presented in [70]).
Reference: [18] <author> G. Chaitin et al. </author> <title> Register Allocation via Coloring. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pp. 98-105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Although it is well-known that algorithms based on graph coloring <ref> [18] </ref> often produce very good register allocations, we chose not to implement such an allocator because we considered its compile-time cost to be too high. Instead, the register allocator relies on usage counts.
Reference: [19] <author> D. D. Chamberlin et al. </author> <title> Support for Repetitive Transactions and Ad Hoc Queries in System R. </title> <journal> ACM Transactions on Database Systems 6(1), </journal> <volume> p. </volume> <pages> 70-94, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks [1], and dynamic linking [67]. Dynamic compilation has also been used in other areas such as database query optimization <ref> [19, 42] </ref>, microcode generation [107], and fast instruction set emulation [34, 93]. 2.5.2 Customization The idea of customizing portions of compiled code to some specific environment is closely related to dynamic compilation since the environment information is often not available until runtime.
Reference: [20] <author> Craig Chambers. </author> <title> Cost of garbage collection in the Self system. </title> <booktitle> OOPSLA'91 GC Workshop, </booktitle> <month> October </month> <year> 1991. </year>
Reference: [21] <author> Craig Chambers, </author> <title> The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Finally, the virtual machine also contains numerous primitives that can be invoked by SELF programs to perform arithmetic, I/O, graphics, and so on. New primitives can be dynamically linked into the system at runtime. References [88], [115], and <ref> [21] </ref> contain further details about the system. 2.2.2 Efficiency Since SELFs pure semantics threatened to make programs extremely inefficient, much of the early implementation effort went into compiler techniques for optimizing SELF programs. Some of these techniques were very successful: Dynamic compilation. <p> The combination of all these features makes it easier to find program errors. Safe primitives make efficient implementation harder <ref> [21] </ref>. For example, every integer addition has to check for over-ow, thus slowing it down. More importantly, the result type of integer operations is unknown: all primitives take a failure block argument, and if the operation fails, a message is sent to this argument. <p> Conversely, whenever the user changes a source method, all compiled code depending on the old definition is invalidated. To accomplish this, the system keeps dependency links between source and compiled methods <ref> [71, 21] </ref>. Since there is no explicit compilation or linking step, the traditional edit-compile-link-run cycle is collapsed into an edit-run cycle. <p> Compile pauses were always noticeable, however, since all code was compiled with full optimization. The compiler was implemented in about 9,500 lines of C++. The second compiler <ref> [21, 24] </ref> (called SELF-91) was designed to remove the restrictions of the first compiler and to further increase runtime performance. It introduced iterative type analysis and had a much more sophisticated back end. <p> In addition, performance should be reasonably stable: minor changes in the source program (i.e., changes that do not affect the complexity of the algorithm and thus should intuitively be performance-neutral) should not significantly affect performance. Simplicity. The previous SELF compiler <ref> [21] </ref> consisted of over 26,000 lines of C++ code and employed many complicated optimizations. For example, the control ow graph kept changing as a result of type analysis, which in turn inuenced the results of the analysis. Consequently, the compiler was relatively hard to understand and change. <p> Based on experience from previous SELF compilers, we included customization, splitting, and block creation optimizations from the start since they have been shown to be clear winners <ref> [21] </ref>. performs high-level optimizations such as type feedback-based inlining and splitting, resulting in a graph of intermediate code nodes. The second phase then performs some common optimizations such as copy propagation, dead code elimination, and register allocation. <p> Strictly speaking, this rule isnt essential since inlining would stop once the maximum estimated method size is reached, but we decided to include it anyway since it results in better compile-time and execution performance. 53 little effort. In Chambers terminology, our compiler uses extended reluctant splitting <ref> [21] </ref>, i.e., it may split a send even if the send does not immediately follow the merge in control ow (extended splitting, see Figure 6-6), but it does not split a send unless it is profitable (reluctant splitting). <p> Second, loop splitting would be relatively expensive to implement since it requires type analysis to realize its benefits <ref> [21] </ref>. Instead of performing splitting in the front end, we could obtain similarly good code with back end optimization. Heintz [64] proposes such an optimization (called rerouting predecessors) for a Smalltalk compiler, and similar back-end optimizations for conventional languages are well-known [3, 10]. <p> Since our compiler does not perform extensive type analysis, this advantage is less pronounced; however, for the previous compiler it was essential <ref> [21] </ref>. Also, the above code eliminates all overhead related to the argument block since it is never created. <p> To evaluate the benefits of type feedback and adaptive reoptimization, we compared three different SELF implementations (Table 7-2). SELF-91 is the previous implementation using iterative type analysis <ref> [21] </ref>; this system does not use recompilation, nor does it use type feedback. SELF-93 is our current system as described in the previous chapters, using dynamic recompilation and type feedback. Finally, SELF-93-nofeedback is SELF-93 with both recompilation and type feedback turned off. <p> SELF-93 nofeedback Same as SELF-93, but without type feedback and recompilation; all methods are always optimized from the beginning. SELF-91 Chambers SELF compiler <ref> [21] </ref> using iterative type analysis; all methods are always optimized from the beginning. This compiler has been shown to achieve excellent performance for smaller programs [21]. <p> SELF-93 nofeedback Same as SELF-93, but without type feedback and recompilation; all methods are always optimized from the beginning. SELF-91 Chambers SELF compiler <ref> [21] </ref> using iterative type analysis; all methods are always optimized from the beginning. This compiler has been shown to achieve excellent performance for smaller programs [21]. Smalltalk ParcPlace Smalltalk-80, release 4.0 (based on techniques described in [44]) C++ GNU C++ compiler (version 2.5.5), and Sun CC 2.0 (based on AT&T cfront 2.0), both using the highest optimization level (-O2 option) Table 7-2. <p> For these benchmarks, SELF-93 runs about two to three times faster than ParcPlace Smalltalk which is generally regarded as the fastest commercially available Smalltalk system (see Figure 7-8 ), even though SELFs language model is purer and thus harder to implement efficiently <ref> [21] </ref>. Furthermore, SELF-93 is only 1.7 to 2.4 times slower than optimized C++ despite the radically different language models and the fact that SELF-93s back end is clearly inferior to that of the C++ compilers. <p> The dynamic nature of type feedback results in more stable performance than static analysis. 7.4.1 Type analysis exhibits unstable performance To measure the effect of reduced static type information on performance, we took nine small integer programs from the Stanford benchmark suite that was used to evaluate the SELF-91 compiler <ref> [21] </ref>. The inner loops of these microbenchmarks are typically one to five lines long and perform mostly integer arithmetic and array accesses. As originally written, many of these benchmarks provide static type information to the compiler for many important values. <p> A single test to verify the type of the array adds another four instructions to the loop, thus halving performance. In addition, the impact of code generation weaknesses (unfilled delay slots, redundant register moves) tends to be amplified in very small loops. benchmarks <ref> [21] </ref>, most of which are only a few lines long. Most programs come in two versions, plain and oo (object-oriented). The oo versions have been rewritten to make the main data structure the receiver. <p> To determine the impact of debugger-visible names, the SELF-91 compiler was changed to release registers allocated to dead variables even if they were visible at an interrupt point. The performance improvement with the changed compiler was insignificant (less than 2%) for a wide range of programs <ref> [21] </ref>. <p> This polling slows down typical programs by about 4%; some numerical programs with very tight loops are slowed down by up to 13% <ref> [21] </ref>. With a more complicated runtime system using conditional traps, the overhead could be reduced to one cycle per check, and loop unrolling could further reduce the problem for tight loops. <p> For example, the relocation information needed for garbage collection is about 17% the size of This study was performed before SELF-93 was fully functional. Thus, we used SELF-91 <ref> [21] </ref> for the measurements. Since this compiler has a better back end, the results are probably more indicative of the true performance impact of source-level debugging than if we had used SELF-93 which performs fewer optimizations. 135 the actual machine code. <p> PIC. Polymorphic Inline Cachean extension of inline caching that handles polymorphic call sites more efficiently. See Chapter 3. Polymorphic send. A send that encounters more than one receiver type during execution. SELF-91. The previous SELF implementation described by Chambers <ref> [21] </ref>. SELF-93. The SELF implementation described in this thesis. Splitting. The idea of delaying a control ow merge (and the loss of type information that comes with the merge) by copying code (see Section 2.2.2 on page 5). 144 Type. Types come in two avors.
Reference: [22] <author> Craig Chambers and David Ungar. </author> <title> Customization: Optimizing Compiler Technology for SELF, a Dynamically-Typed Object-Oriented Programming Language. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(7), </note> <month> July </month> <year> 1989. </year>
Reference-contexts: Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph <ref> [23, 22, 24] </ref>. For example, suppose that an object is known to be an integer in one branch of an if statement and a oating-point number in the other branch (Figure 2-3). <p> Keppel et al. discuss value-specific runtime specialization for a variety of applications [83]. In more theoretically-oriented areas of computer science, customization is known as partial evaluation [15]. 2.5.3 Previous SELF compilers The SELF compiler described in this thesis has two predecessors. The first SELF compiler <ref> [22, 23] </ref> introduced customization and splitting and achieved reasonably good performance. For the Stanford integer benchmarks and the Richards benchmark, programs ran on the order of 4-6 times slower than optimized C , or about two times faster than the fastest Smalltalk-80 system at that time. <p> In the context of compilation, type always means implementation type, i.e., a type describing the exact object layout and its set of methods. Although SELF has no explicit types at the language level, the implementation maintains type descriptors (called maps <ref> [22] </ref>), and each object contains a pointer to its type descriptor. method objects inline caches First compiler phase: SELF-specific optimizations runtime system intermediate code tree of inlined scopes (for intermediate code to source mapping) Second compiler phase: classical optimizations Third compiler phase: code generation compiled method (including scope descriptors and pc-to-source <p> Such an approach depends on having the intermediate code of the inlining candidates available at compile time; this is not the case in SELF because the inlining candidates were compiled at an earlier time. 6.1.3 Splitting Splitting avoids type tests by copying parts of the control ow graph <ref> [23, 22, 24] </ref>.
Reference: [23] <author> Craig Chambers, David Ungar, and Elgin Lee. </author> <title> An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> In OOPSLA 89 Conference Proceedings, p. </booktitle> <pages> 49-70, </pages> <address> New Orleans, LA, </address> <month> October </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October </month> <year> 1989. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year> <month> 158 </month>
Reference-contexts: All objects except integers and oats consist of at least two words: a header word and a pointer to the objects map <ref> [23] </ref>. A map describes an objects format and can be viewed as the low-level type of the object. Objects with the same format share the same map, so that the object layout information is stored only once. <p> That is, there is no separate compilation phase: execution and compilation are interleaved. (SELFs use of dynamic compilation was inspired by the Deutsch-Schiffman Smalltalk system [44].) Customization. Customization allows the compiler to determine the types of many message receivers in a method <ref> [23] </ref>. It extends dynamic compilation by exploiting the fact that many messages within a method are sent to self. The compiler creates a separate compiled version of a given source method for each receiver type (Figure 2-1). <p> Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph <ref> [23, 22, 24] </ref>. For example, suppose that an object is known to be an integer in one branch of an if statement and a oating-point number in the other branch (Figure 2-3). <p> Keppel et al. discuss value-specific runtime specialization for a variety of applications [83]. In more theoretically-oriented areas of computer science, customization is known as partial evaluation [15]. 2.5.3 Previous SELF compilers The SELF compiler described in this thesis has two predecessors. The first SELF compiler <ref> [22, 23] </ref> introduced customization and splitting and achieved reasonably good performance. For the Stanford integer benchmarks and the Richards benchmark, programs ran on the order of 4-6 times slower than optimized C , or about two times faster than the fastest Smalltalk-80 system at that time. <p> The performance impact of inline cache misses becomes more severe in highly efficient systems, where it can no longer be ignored. For example, measurements for the SELF-90 system showed that the Richards benchmark spent about 25% of its time handling inline cache misses <ref> [23] </ref>. We will use the term polymorphic for call sites where polymorphism is actually used. <p> A map is the systems internal representation of an implementation type; all objects having exactly the same structure (i.e., the same list of slot names and the same constant slot contents) share the same map <ref> [23] </ref>. These scope descriptors are needed to support source-level debugging; see Chapter 10. <p> Such an approach depends on having the intermediate code of the inlining candidates available at compile time; this is not the case in SELF because the inlining candidates were compiled at an earlier time. 6.1.3 Splitting Splitting avoids type tests by copying parts of the control ow graph <ref> [23, 22, 24] </ref>. <p> To support this reconstruction, the SELF compiler generates scope descriptors <ref> [23] </ref> for each scope contained in a compiled method, i.e., for the initial source method and all methods inlined within it. <p> A straightforward extension of the descriptor structure could be used to handle variables whose values can be computed from other values (such as eliminated induction variables). The scope descriptor mechanism was originally developed by Chambers, Ungar, and Lee <ref> [23] </ref> and was recently reimplemented by Lars Bak to reduce memory requirements. struct ScopeDesc - oop method; // pointer to the method object ScopeDesc* caller; // scope into which this scope was inlined (if any) int posWithinCaller; // source position within caller ScopeDesc* enclosingScope; // lexically enclosing scope (if any) NameDesc <p> To invalidate the compiled code affected by such a change, the SELF system maintains dependency links <ref> [23] </ref> between compiled code and the objects representing source code methods. For example, if a compiled method contains inlined copies of a method that was changed, the compiled method is discarded. <p> Inline cache. A dispatch mechanism often used in dynamically-typed languages (see Chapter 3). Map. An internal data structure describing the implementation type of an object, i.e., its exact format and the contents of its constant slots <ref> [23] </ref>. Among other things, maps are used for message dispatch and for customization. Megamorphic send. A highly polymorphic send (very many receiver types occur). Method. An object representing a source method (function). The method object contains a byte-coded version of the source code and is created by the parser.
Reference: [24] <author> Craig Chambers and David Ungar. </author> <title> Iterative Type Analysis and Extended Message Splitting: Optimizing Dynamically-Typed Object-Oriented Programs. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 150-164, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year> <note> Published as SIGPLAN Notices 25(6), </note> <month> June </month> <year> 1990. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph <ref> [23, 22, 24] </ref>. For example, suppose that an object is known to be an integer in one branch of an if statement and a oating-point number in the other branch (Figure 2-3). <p> Compile pauses were always noticeable, however, since all code was compiled with full optimization. The compiler was implemented in about 9,500 lines of C++. The second compiler <ref> [21, 24] </ref> (called SELF-91) was designed to remove the restrictions of the first compiler and to further increase runtime performance. It introduced iterative type analysis and had a much more sophisticated back end. <p> Such an approach depends on having the intermediate code of the inlining candidates available at compile time; this is not the case in SELF because the inlining candidates were compiled at an earlier time. 6.1.3 Splitting Splitting avoids type tests by copying parts of the control ow graph <ref> [23, 22, 24] </ref>.
Reference: [25] <author> Craig Chambers, David Ungar, Bay-Wei Chang, and Urs Hlzle. </author> <title> Parents are Shared Parts: Inheritance and Encapsulation in SELF. Published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: SELF has multiple inheritance. The inheritance design underwent several changes over the years. SELF-87 only had single inheritance, but SELF-90 introduced prioritized multiple inheritance combined with a new privacy mechanism for better encapsulation <ref> [25, 134] </ref> and a sender path tiebreaker rule for disambiguating between equal-priority parents [115]. More recently, the pendulum has swung back towards simplicity: SELF-92 [115] eliminated the sender path tiebreaker because it tended to hide ambiguities, and SELF-93 eliminated prioritized inheritance and privacy from the language.
Reference: [26] <author> Craig Chambers and David Ungar. </author> <title> Making Pure Object-Oriented Languages Practical. </title> <booktitle> OOPSLA 91 Conference Proceedings, </booktitle> <address> Phoenix, AZ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Together, these optimizations elevated SELFs performance to a very reasonable level. For example, Chambers and Ungar reported that SELF significantly outperformed the ParcPlace Smalltalk-80 implementation for a suite of small C-like integer benchmarks <ref> [26] </ref>. 2.2.3 Source-level semantics A combination of language and implementation features ensures that the behavior of all programs, even erroneous ones, can be understood solely in source-level terms. <p> For example, starting the prototype user interface took about 60 seconds , out of which about 50 seconds were compile time. Although the SELF compiler was about as fast as the standard C compiler <ref> [26] </ref>, it was still too slow. 4.1 Simple code generation To improve the responsiveness of the system, we decided to implement a non-inlining compiler (NIC). Its task is to compile methods as quickly as possible, without attempting any optimizations. Consequently, its code generation strategy is extremely simple. <p> Only a few years later, Chambers and Ungar showed that SELF could achieve excellent performance for a restricted set of programs <ref> [26] </ref>. Unfortunately, large object-oriented programs still didnt perform as well. Furthermore, runtime performance was achieved at the expense of considerable compiler complexity and compilation speeds that were too slow for interactive use.
Reference: [27] <author> Craig Chambers. </author> <title> The Cecil Language - Specification and Rationale. </title> <institution> University of Washington, </institution> <type> Technical Report CS TR 93-03-05, </type> <year> 1993. </year>
Reference-contexts: for external primitives callable by SELF Richards 400 simple operating system simulator originally written in BCPL by Martin Richards lar ge benchmarks CecilComp 11,500 Cecil-to-C compiler compiling the Fibonacci function (the com piler shares about 80% of its code with the interpreter, CecilInt) CecilInt 9,000 interpreter for the Cecil language <ref> [27] </ref> running a short Cecil test program Mango 7,000 automatically generated lexer/parser for ANSI C, parsing a 700 line C file Typeinf 8,600 type inferencer for SELF as described in [5] UI1 15,200 prototype user interface using animation techniques [28] b UI3 4,000 experimental 3D user interface Table 7-1.
Reference: [28] <author> Bay-Wei Chang and David Ungar. </author> <title> Animation: From cartoons to the user interface. </title> <booktitle> User Interface Software and Technology Conference Proceedings, </booktitle> <address> Atlanta, GA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Figure 3-4 shows the arity distribution of non-empty inline caches, i.e., the degree of polymorphism exhibited by call sites. The distribution was taken after using the prototype SELF user interface <ref> [28] </ref> for about a minute. The overwhelming majority of call sites have only one receiver type; this is why normal inline caching works well. Quite a few call sites have two different receiver types; a frequent case are boolean messages, since true and false are two different types in SELF. <p> with the interpreter, CecilInt) CecilInt 9,000 interpreter for the Cecil language [27] running a short Cecil test program Mango 7,000 automatically generated lexer/parser for ANSI C, parsing a 700 line C file Typeinf 8,600 type inferencer for SELF as described in [5] UI1 15,200 prototype user interface using animation techniques <ref> [28] </ref> b UI3 4,000 experimental 3D user interface Table 7-1. <p> All pauses measurements use pause clustering unless specifically mentioned otherwise. That is, the term pause always means clustered pause. 9.2.1 Pauses during an interactive session We measured the compilation pauses occurring during a 50-minute session of the SELF user interface <ref> [28] </ref>. The session involved completing a SELF tutorial, which includes browsing, editing, and making small programming changes. During the tutorial, we discovered a bug in the cut-and-paste code, so that the session also includes some real-life debugging.
Reference: [29] <author> Pohua P. Chang, Scott A. Mahlke, William Y. Chen, and Wen-Mei W. Hwu. </author> <title> Profile-guided automatic inline expansion for C programs. </title> <journal> SoftwarePractice and Experience 22(5), </journal> <volume> p. </volume> <pages> 349-369, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In other words, it remains to be seen if global (whole-program) analyses can significantly improve performance over a type-feedback based system. 5.8.4 Inlining Inlining has been studied extensively for procedural languages <ref> [29, 36, 39, 62, 74] </ref>. For these languages, inlining is simpler since they do not (or only rarely) use dynamic dispatch. Furthermore, inlining candidates are easier to analyze because the space and time cost of most language constructs is known. <p> Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. Other systems use profile information to assist classic code optimizations [30], procedure inlining <ref> [29, 94] </ref>, trace scheduling [53], and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime. <p> However, aggressive inlining generally significantly increases the performance of SELF programs. This effect is in marked contrast to studies of other languages (such as C or Fortran) where inlining 73 was found to have only small benefits, if any <ref> [29, 36, 39, 62, 74] </ref>. We attribute this discrepancy to a number of signif icant differences between SELF and conventional languages: 1.
Reference: [30] <author> Pohua P. Chang, Scott A. Mahlke, and W. W. Hwu. </author> <title> Using profile information to assist classic code optimizations. </title> <type> Technical Report UILU-ENG-91-2219, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. Other systems use profile information to assist classic code optimizations <ref> [30] </ref>, procedure inlining [29, 94], trace scheduling [53], and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime. <p> However, such optimizations could be useful in an application extractor generating a stand-alone executable for an application, or in a system using batch-style compilation. Such an optimizer could also use additional information from profilers <ref> [30] </ref> or from type inferencers [5] to generate even better code. Finally, SELF-93 inherits the overcustomization problem from previous SELF systems.
Reference: [31] <author> David Chase. </author> <title> Garbage Collection and Other Optimizations. </title> <type> Ph.D. dissertation, </type> <institution> Computer Science Department, Rice University, </institution> <year> 1987. </year>
Reference-contexts: Interrupt points also lessen the impact of garbage collection on compiler optimization. Garbage collections can only occur at interrupt points, and so the compiler can generate code between interrupt points that temporarily violates the invariants needed by the garbage collector <ref> [31] </ref>. 10.3 Updating active methods During debugging, a programmer might not only change the value of a variable but also the definition of a method. <p> In many cases, the optimizations inhibited by garbage collection are very similar to those inhibited by debugging requirements, such as dead store elimination and some forms of common subexpression elimination <ref> [31] </ref>. Thus, it would be difficult to separate the impact of garbage collection on optimization from the impact of full source-level debugging. While we could not measure the full performance impact of our debugging scheme, inspection of the generated code indicated no obvious debugging-related inefficiencies.
Reference: [32] <author> Andrew A. Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert System: Compiler and Runtime Support for Efficient, Fine-Grained Concurrent Object-Oriented Programs. </title> <institution> University of Illinois at Urbana-Champaign, Technical Report UIUC DCS-R-93-1815, </institution> <year> 1993. </year>
Reference-contexts: For example, Kessler et al. use it to implement fast breakpoints for debuggers [84]. Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences [103]. In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism <ref> [32, 105] </ref> and to eliminate the overhead of protocol stacks [1], and dynamic linking [67].
Reference: [33] <author> Robert F. Cmelik, Shing I. Kong, David R. Ditzel, and Edmund J. Kelly. </author> <title> An Analysis of MIPS and SPARC Instruction Set Utilization on the SPEC Benchmarks. </title> <booktitle> ASPLOS IV, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Since there are only four benchmarks in the SPECint89 suite, the individual data points for C are shown directly, and the box plots (dotted) are given for reference only. The SPECint89 data are taken from Cmelik et al <ref> [33] </ref>. 1. Overall, there are few differences between the SPEC benchmarks and SELF. Often, the differences between the individual SPEC benchmarks are bigger than the difference between SELF and C (in the graph, the SPEC boxes are usually larger than the corresponding SELF boxes). 2. <p> SPECInt89 instruction usage (from <ref> [33] </ref>) CecilComp CecilInt DeltaBlue Mango PrimMaker Richards T ypeinf UI1 UI3 geomean load 16.8% 17.4% 18.2% 16.9% 19.4% 14.7% 15.4% 15.9% 11.7% 16.1% store 7.2% 7.1% 6.2% 5.1% 10.0% 4.9% 5.6% 6.2% 6.9% 6.4% cond. branch 13.9% 12.0% 16.7% 13.4% 11.3% 17.5% 13.9% 16.2% 13.2% 14.1% uncond. branch 2.9% 2.3% 3.6%
Reference: [34] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <institution> Sun Micro-systems Laboratories, Technical Report SMLI TR-93-12, </institution> <year> 1993. </year>
Reference-contexts: Dynamic compilation has also been used in other areas such as database query optimization [19, 42], microcode generation [107], and fast instruction set emulation <ref> [34, 93] </ref>. 2.5.2 Customization The idea of customizing portions of compiled code to some specific environment is closely related to dynamic compilation since the environment information is often not available until runtime. For example, Mitchells system [97] specialized arithmetic operations to the runtime types of the operands. <p> To accurately measure execution times, we wrote a SPARC simulator based on the spa [76] and shade <ref> [34] </ref> tracing tools and the dinero cache simulator [66]. The simulator models the Cypress CY7C601 implementation of the SPARC architecture running at 40 MHz, i.e., the chip used in the SPARCstation-2 (SS-2) workstation.
Reference: [35] <author> Thomas J. Conroy and Eduardo Pelegri-Llopart. </author> <title> An Assessment of Method-Lookup Caches for Smalltalk-80 Implementations. </title> <booktitle> In [86]. </booktitle>
Reference: [36] <author> K.D. Cooper, M.W. Hall, and Ken Kennedy. </author> <title> Procedure Cloning. </title> <booktitle> In IEEE Intl. Conference on Computer Languages, p. </booktitle> <pages> 96-105, </pages> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Customization has also been applied to more traditional, batch-oriented compilers. For example, Cooper et al. describe a FORTRAN compiler that can create customized versions of procedures to enable certain loop optimizations <ref> [36] </ref>. Nicolau describes a FORTRAN compiler that dynamically selects the appropriate statically-generated version of a loop [99]. Saltz et al. delay loop scheduling until runtime [112]. Przybylski et al. generate a specialized cache simulator for each different set of simulation parameters [104]. <p> In other words, it remains to be seen if global (whole-program) analyses can significantly improve performance over a type-feedback based system. 5.8.4 Inlining Inlining has been studied extensively for procedural languages <ref> [29, 36, 39, 62, 74] </ref>. For these languages, inlining is simpler since they do not (or only rarely) use dynamic dispatch. Furthermore, inlining candidates are easier to analyze because the space and time cost of most language constructs is known. <p> However, aggressive inlining generally significantly increases the performance of SELF programs. This effect is in marked contrast to studies of other languages (such as C or Fortran) where inlining 73 was found to have only small benefits, if any <ref> [29, 36, 39, 62, 74] </ref>. We attribute this discrepancy to a number of signif icant differences between SELF and conventional languages: 1.
Reference: [37] <author> Deborah S. Coutant, Sue Meloy, and Michelle Ruscetta. </author> <title> DOC: A Practical Approach to Source-Level Debugging of Globally Optimized Code. </title> <booktitle> In Proceedings of the SIGPLAN 88 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 125-134. </pages>
Reference-contexts: Most existing systems do not support the debugging of optimized code. Programs can either be optimized for full speed, or they can be compiled without optimizations for full source-level debugging. Recently, techniques have been developed that strive to make it possible to debug optimized code <ref> [65, 143, 37] </ref>. However, none of these systems is able to provide full source-level debugging. For example, it generally is not possible to obtain the values of all source-level variables, to single-step through the program, or to change the value of a variable. <p> Unfortunately, if the source-level state of the program must be recoverable at every point in the program, i.e., at virtually every instruction boundary (to support single-stepping), existing techniques <ref> [2, 3, 65, 143, 37] </ref> severely restrict the optimizations that could be performed, effectively disabling many common optimizations. We solve this dilemma by relaxing the debugging requirements placed on optimized code. <p> In contrast, neither single-step nor finish could generally be provided by previous systems for debugging optimized code <ref> [144, 37] </ref>. 132 10.4.1 Single-step Because every source point has an interrupt point associated with it in a deoptimized method, the implementation of single-stepping becomes trivial. The system deoptimizes the current activation and restarts the process with the interrupt ag already set. <p> Similarly, some other optimizations (e.g., code motion or induction variable elimination) may sometimes not be performed if the debuggers recovery techniques are not powerful enough to hide the optimizations effects at one or more interrupt points. The extent of this problems depends on the particular recovery techniques used (e.g., <ref> [2, 3, 65, 143, 37] </ref>) and on the average distance between interrupt points. All these optimizations can be performed, however, if there is no interrupt point within the affected variables scope (see Section 10.2.4). <p> Adl-Tabatabai et al. [2, 3] investigate the problem of detecting and recovering the values of source-level variables in the presence of instruction scheduling. Other recovery mechanisms are described by Coutant et al. <ref> [37] </ref> and by Schlaeppi and Warren [114]. Zellweger [143, 144] describes an interactive source-level debugger for Cedar which handles two optimizations, procedure inlining and cross-jumping, to provide expected behavior in most cases.
Reference: [38] <author> Zarka Cvetanovic and Dileep Bhandarkar. </author> <title> Characterization of Alpha AXP performance using TCP and SPEC workloads. </title> <booktitle> ISCA21 Conference Proceedings, p. </booktitle> <pages> 60-70, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Other studies have shown that the I-cache overhead of scientific and commercial workloads can be similar to the overhead measured for SELF. For example, Cvetanovic and Bhandarkar <ref> [38] </ref> report an I-cache miss overhead of 30-40% of total execution time for some SPECint92 benchmarks and the TCP benchmarks running on a DEC Alpha system. 8.5 Data cache behavior an infinite cache is only 6.6%.
Reference: [39] <author> Jack W. Davidson and Anne M. Holler. </author> <title> A study of a C function inliner. </title> <journal> SoftwarePractice and Experience 18(8): </journal> <pages> 775-90. </pages>
Reference-contexts: In other words, it remains to be seen if global (whole-program) analyses can significantly improve performance over a type-feedback based system. 5.8.4 Inlining Inlining has been studied extensively for procedural languages <ref> [29, 36, 39, 62, 74] </ref>. For these languages, inlining is simpler since they do not (or only rarely) use dynamic dispatch. Furthermore, inlining candidates are easier to analyze because the space and time cost of most language constructs is known. <p> However, aggressive inlining generally significantly increases the performance of SELF programs. This effect is in marked contrast to studies of other languages (such as C or Fortran) where inlining 73 was found to have only small benefits, if any <ref> [29, 36, 39, 62, 74] </ref>. We attribute this discrepancy to a number of signif icant differences between SELF and conventional languages: 1.
Reference: [40] <author> Jeffrey Dean and Craig Chambers. </author> <title> Training compilers for better inlining decisions. </title> <institution> University of Washington Technical Report 93-05-05, </institution> <year> 1993. </year>
Reference-contexts: In contrast, the source reveals almost no 45 implementation information in a pure object-oriented language since every single operation involves dynamic dispatch, including arithmetic, boolean operations, and control structures. 5.8.5 Profile-based compilation Dean and Chambers <ref> [40] </ref> describe a system based on the SELF-91 compiler that uses the first compilation as an experiment and records inlining decisions and their benefits in a database. <p> Second, undoing an inlining decision is tricky to implement and might significantly complicate the design of the compiler. For these reasons, we did not pursue this strategy further. Dean and Chambers propose another strategy <ref> [40] </ref>, using the first compilation as the optimistic experiment and to record the decisions and their benefits in an inlining database. <p> A later study using more programs showed a 20% reduction of compile time with no performance degradation, or a compile time reduction of 25% with an execution performance penalty of a factor of 1.6 <ref> [40] </ref>. <p> Reducing instruction cache misses. Since SELF-93 spends 40% of its time in I-cache misses with a 32K cache, reducing these misses could bring significant performance benefits. One technique that could help reducing code size is a system making better-informed inlining decisions (similar to those proposed by Dean and Chambers <ref> [40] </ref>), taking into account not only the size of the inlining candidate but also the potential speedup. Such a system could possibly reduce code size without significantly increasing the number of instructions executed, and thus reduce overall execution time because of the reduced cache overhead. <p> With adaptive recompilation, a system can provide both good runtime performance and good interactive performance, even for a pure object-oriented language like SELF. These problems are similar to the problems encountered by Dean and Chambers in their work towards better inlining decisions based on results of previous compilations <ref> [40] </ref>. 123 Even on a previous-generation workstation like the SPARCstation-2, fewer than 200 pauses exceeded 200 ms during a 50-minute interaction, and 21 pauses exceeded one second.
Reference: [41] <author> Jeffrey Dean and Craig Chambers. </author> <title> Toward better inlining decisions using inlining trials. </title> <booktitle> 1994 ACM Conference on Lisp and Functional Programming, p. </booktitle> <pages> 273-282, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference: [42] <author> Marcia A. Derr and Shinichi Morishita. </author> <title> Design and Implementation of the Glue-Nail Database System. </title> <booktitle> In SIGMOD '93 Conference Proceedings, </booktitle> <pages> pp. 147-156, </pages> <year> 1993. </year> <note> Published as SIGMOD Record, 22(2), </note> <month> June </month> <year> 1993. </year>
Reference-contexts: In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks [1], and dynamic linking [67]. Dynamic compilation has also been used in other areas such as database query optimization <ref> [19, 42] </ref>, microcode generation [107], and fast instruction set emulation [34, 93]. 2.5.2 Customization The idea of customizing portions of compiled code to some specific environment is closely related to dynamic compilation since the environment information is often not available until runtime.
Reference: [43] <author> L. Peter Deutsch. </author> <title> The Dorado Smalltalk-80 Implementation: Hardware Architectures Impact on Software Architecture. </title> <booktitle> In [86]. </booktitle>
Reference-contexts: In contrast, Smalltalk-80 has a much more machine-oriented byte code format; in fact, the byte codes were directly interpreted by microcode in early Smalltalk implementations <ref> [43] </ref>. For example, Smalltalk has special byte codes for instance variable accesses (specifying the slot number), local accesses, and the most frequent primitives (arithmetic and control transfer). <p> Williams and Wolczko argue that software-controlled caching improved the performance and locality of object-oriented systems [142]. Xerox Dorado Smalltalk, for a long time the fastest Smalltalk implementation available, contained microcode support for large portions of the Smalltalk virtual machine <ref> [43] </ref>. However, none of these systems used an optimizing compiler comparable to SELF-93, and so the results of previous studies may not be valid for the current SELF implementation.
Reference: [44] <author> L. Peter Deutsch and Alan Schiffman. </author> <title> Efficient Implementation of the Smalltalk-80 System. </title> <booktitle> Proceedings of the 11th Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <year> 1984. </year>
Reference-contexts: Some of these techniques were very successful: Dynamic compilation. The compiler dynamically translates source methods into compiled methods on demand. That is, there is no separate compilation phase: execution and compilation are interleaved. (SELFs use of dynamic compilation was inspired by the Deutsch-Schiffman Smalltalk system <ref> [44] </ref>.) Customization. Customization allows the compiler to determine the types of many message receivers in a method [23]. It extends dynamic compilation by exploiting the fact that many messages within a method are sent to self. <p> Type Prediction. Certain messages are almost exclusively sent to particular receiver types. For such messages, the compiler uses an optimization originally introduced by early Smalltalk systems <ref> [44, 132] </ref>: it predicts the type of the source method for finding the minimum of two numbers compiled method specialized for integer receivers compiled method specialized for oating-point receivers customization 6 receiver based on the message name and inserts a runtime type test before the message send to test for the <p> Along the branch where the type test succeeds, the compiler has precise information about the type of the receiver and can statically bind and inline a copy of the message. For example, existing SELF and Smalltalk systems predict that + will be sent to an integer <ref> [128, 58, 44] </ref>, since measurements indicate that this occurs 90% of the time [130]. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. <p> Some of these systems used mechanisms similar to customization (e.g., Guibas and Wyatt [61]) and inline caching (Saal and Weiss [111]). Deutsch and Schiffman pioneered the use of dynamic compilation for object-oriented systems. Their Smalltalk-80 implementation <ref> [44] </ref> dynamically translates the byte codes defined by the Smalltalk virtual machine [58] into native machine code and caches the compiled code for later use; Deutsch and Schiffman estimate that just using simple dynamic compilation instead of interpretation speeds up their system by a factor of 1.6, and that a more <p> Compared to the first compiler, the SELF-91 compiler was considerably more complex, consuming about 26,000 lines of C++ code. 2.5.4 Smalltalk-80 compilers Smalltalk-80 is probably the object-oriented language closest to SELF, and several projects have investigated techniques to speed up Smalltalk programs. 2.5.4.1 The Deutsch-Schiffman system The Deutsch-Schiffman system <ref> [44] </ref> represents the state of the art of commercial Smalltalk implementations. It contains a simple but very fast dynamic compiler that performs only peephole optimizations but no inlining. However, unlike SELF the Smalltalk-80 implementation hardwires certain important methods such as integer addition and messages implementing if statements and certain loops. <p> For example, several studies have shown that in Smalltalk code, the receiver type at a given call site remains constant 95% of the time <ref> [44, 130, 132] </ref>. This locality of type usage can be exploited by caching the looked-up method address at the call site. <p> This locality of type usage can be exploited by caching the looked-up method address at the call site. Because the lookup result is cached in line at every call site (i.e., in the case of a hit no separate lookup cache is accessed), the technique is called inline caching <ref> [44, 132] </ref>. contains a call to the systems lookup routine. The first time this call is executed, the lookup routine finds the target method. But before branching to the target, the lookup routine changes the call instruction to point to the target method just found (Figure 3-2). <p> Although the SELF implementation with its optimizing compiler is quite different from Smalltalk systems, the miss ratios agree well with those observed in previous studies of Smalltalk systems, which observed miss ratios on the order of 5% <ref> [44, 130, 132] </ref>. The miss ratios do not directly correlate to the speedups observed when introducing PICs because the benchmarks have very different call frequencies (differing by more than a factor of five). <p> Alternatively, the interpreter could just add inline caching to the straightforward interpreter by adding one pointer per send byte code to cache the method last invoked by that send; each method would also cache the last receiver type. Similar organizations have been used for Smalltalk interpreters <ref> [44] </ref>. Since most byte codes are sends, this approach would require roughly one word per byte code. 4.6 The NIC and interactive performance One of the goals of adding a non-optimizing compiler to the SELF system was to reduce compile pauses, i.e. to improve the responsiveness of the system. <p> A large fraction of call sites in C++ have this property [17, 57], and it also holds in other object-oriented programming languages (e.g., Smalltalk, SELF, Sather, and Eiffel); this is the reason why inline caching <ref> [44, 70] </ref> works well in these languages as an implementation of dynamic dispatch. Therefore, we expect type feedback to work well for these languages; the higher the frequency of dynamically-dispatched calls, the more beneficial type feedback could be. <p> For example, Lisp systems usually inline the integer case of generic arithmetic and handle all other type combinations with a call to a routine in the runtime system. The Deutsch-Schiffman Smalltalk compiler was the first object-oriented system to predict integer receivers for common message names such as + <ref> [44] </ref>. However, none of these systems predicted types dynamically as does our system; there was no feedback. 5.8.2 Customization Other systems have used mechanisms similar to customization, which is a limited form of runtime type feedback. <p> Based on measurements of C++ programs, Calder and Grunwald [17] argue that type feedback would be beneficial for C++. Unfortunately, they apparently were not aware of previous work; their proposed if conversion appears to be identical to inline caching <ref> [44] </ref>, and a proposed extension of if conversion is identical to type feedback (which was first presented in [70]). Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. <p> SELF-91 Chambers SELF compiler [21] using iterative type analysis; all methods are always optimized from the beginning. This compiler has been shown to achieve excellent performance for smaller programs [21]. Smalltalk ParcPlace Smalltalk-80, release 4.0 (based on techniques described in <ref> [44] </ref>) C++ GNU C++ compiler (version 2.5.5), and Sun CC 2.0 (based on AT&T cfront 2.0), both using the highest optimization level (-O2 option) Table 7-2. <p> Many systems (e.g., most Smalltalk-80 and SELF implementations) statically predict the type of certain messages. For example, the receiver of ifTrue: is predicted to be either the true or false object, and the receiver of + is predicted to be an integer <ref> [128, 58, 44, 130] </ref>. Static type prediction has obvious performance advantages, but it contributes to unstable performance. First, the specialized messages execute much faster than semantically equivalent code using non-specialized selectors. <p> In order to create a runtime location for the variable, it might be necessary to transform an activation in the middle of the stack, which the system currently cannot do. However, this is not a fundamental problem; for example, the transformed stack frames could be heap-allocated as described in <ref> [44] </ref>. An even simpler solution would be to always allocate stack locations for eliminated variables. These locations would be unused during normal program execution but would spring into life when the programmer manually changed the value of an eliminated variable. <p> While this comparison should be taken with a grain of salt, it indicates that despite the increased functionality, the space overhead for the data structures supporting source-level debugging is probably not higher than in other systems. 10.7 Related work The Smalltalk-80 system described by Deutsch and Schiffman <ref> [44] </ref> pioneered the use of dynamic compilation and interrupt points. To hide the effects of compilation to native code, compiled methods included a mapping from compiled code to source position.
Reference: [45] <author> L. Peter Deutsch. </author> <title> Private Communication. </title>
Reference-contexts: As a result, the Deutsch-Schiffman compiler can generate efficient code for those constructs which could otherwise only be achieved with optimizations similar to those performed by the SELF compilers. The Deutsch-Schiffman compiler uses on the order of 50 instructions to generate one compiled machine instruction <ref> [45] </ref> so that compile pauses are virtually unnoticeable. In addition to dynamic compilation, the Deutsch-Schiffman system also pioneered several other optimization techniques. Inline caching speeds up message lookup by caching the last lookup result at the call site (see Chapter 3). <p> The performance impact of these decisions will be examined in Section 4.3 and 4.4. 4.2 Compilation speed The NIC uses roughly 400 instructions to generate one SPARC instruction. This is significantly slower than the simple compiler described by Deutsch and Schiffman which uses around 50 instructions per generated instruction <ref> [45] </ref>. There are several reasons for this difference. First, SELFs byte codes are much higher-level than the byte codes of the Smalltalk-80 virtual machine.
Reference: [46] <author> R. Dixon, T. McKee, P. Schweitzer, and M. Vaughan. </author> <title> A Fast Method Dispatcher for Compiled Languages with Multiple Inheritance. </title> <booktitle> In OOPSLA 89 Conference Proceedings, p. </booktitle> <pages> 211-214, </pages> <address> New Orleans, LA, </address> <month> October, </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October, </month> <year> 1989. </year> <month> 159 </month>
Reference: [47] <author> David Ditzel. </author> <title> Private Communication, </title> <year> 1993. </year>
Reference-contexts: The SPARC V9 architecture [119] corrects this aw so that window trap handlers can execute in user mode. This modification reduces the window handling overhead by more than a factor of 10 (!) to 19 instructions per trap <ref> [47] </ref>. With this change, the register window handling overhead should become very small.
Reference: [48] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory Subsystem Performance of Programs with Intensive Heap Allocation. </title> <booktitle> In 21st Annual ACM Symposium on Principles of Programming Languages, p. </booktitle> <pages> 1-14, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: This organization is common in current workstations (e.g., the DECstation 5000/200) and has been shown to be effective for programs with intensive heap allocation <ref> [48] </ref>. The SPARCstation-2s write buffer organization is somewhat unfortunate and incurs high write costs compared to other common designs. To avoid skewing our data, we chose not to model the SS-2 write buffer and instead assume a perfect write buffer. <p> Even with an 8K data cache, the overhead would still be very modest, with a median of 12%. This is quite surprising since most of the programs allocate several MBytes of data. However, our data is consistent with that of Diwan et al. who have measured allocation-intensive ML programs <ref> [48] </ref> and found very low data cache overheads for the same cache organization (write-allocate, subblock placement). Our data also confirms that of Reinhold [108] who investigated the cache performance of large Lisp programs.
Reference: [49] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Private communication, </title> <month> August </month> <year> 1993. </year>
Reference-contexts: To avoid skewing our data, we chose not to model the SS-2 write buffer and instead assume a perfect write buffer. Previous work (e.g., [14, 81]) has shown that writes can be absorbed with virtually no cost, and Diwan et al. <ref> [49] </ref> report a write buffer CPI contribution of less than 0.02 for a 6-element deep buffer and a cache organization similar to the DECstation 500/200. 70 back ends. For example, SELF-91 compiles a division by 2 into a shift in Richards inner loop, whereas SELF-93-nofeedback does not perform this optimization.
Reference: [50] <author> Karel Driesen. </author> <title> Selector Table Indexing and Sparse Arrays. </title> <booktitle> In OOPSLA 93 Conference Proceedings, </booktitle> <pages> pp. 259-270, </pages> <address> Washington, D.C., </address> <year> 1993. </year> <note> Published as SIGPLAN Notices 28(10), </note> <month> October </month> <year> 1993. </year>
Reference-contexts: However, it is possible to use dispatch tables in dynamically-typed languages as well, albeit with some added complications <ref> [7, 50, 135] </ref>.
Reference: [51] <author> Eric J. Van Dyke. </author> <title> A dynamic incremental compiler for an interpretative language. </title> <journal> HP Journal, </journal> <volume> p. </volume> <pages> 17-24, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: Similarly, APL compilers created specialized code for certain expressions <ref> [80, 51, 61] </ref>. Of these systems, the HP APL compiler [51] comes closest to the customization technique used by SELF. The HP APL/3000 system compiles code on a statement-by-statement basis. <p> Similarly, APL compilers created specialized code for certain expressions [80, 51, 61]. Of these systems, the HP APL compiler <ref> [51] </ref> comes closest to the customization technique used by SELF. The HP APL/3000 system compiles code on a statement-by-statement basis. In addition to performing APL-specific optimizations, the compiled code is specialized according to the specific operand types (number of dimensions, size of each dimension, element type, and storage layout). <p> The first one, dynamic compilation (also called lazy compilation), moves compilation to runtime where additional information is available and can be used to better optimize the late-bound operations. This is the approach taken by SELF and several previous systems (e.g., van Dykes APL compiler <ref> [51] </ref> ). Compared to these systems, SELF takes laziness one step further by not trying to do the best possible job right away (i.e., when compiled code is first needed). <p> For example, Mitchells system [97] specialized arithmetic operations to the runtime types of the operands. Similarly, APL compilers created specialized code for certain expressions <ref> [80, 51, 61] </ref>. Of these systems, the HP APL compiler [51] was the most exible. The system compiled code on a statement-by-statement basis. In addition to performing APL-specific optimizations, compiled code was specialized according to the specific operand types (number of dimensions, size of each dimension, element type, etc.). <p> For example, Mitchells system [97] specialized arithmetic operations to the runtime types of the operands. Similarly, APL compilers created specialized code for certain expressions [80, 51, 61]. Of these systems, the HP APL compiler <ref> [51] </ref> was the most exible. The system compiled code on a statement-by-statement basis. In addition to performing APL-specific optimizations, compiled code was specialized according to the specific operand types (number of dimensions, size of each dimension, element type, etc.).
Reference: [52] <author> Peter H. Feiler. </author> <title> A Language-Oriented Interactive Programming Environment Based on Compilation Technology. </title> <type> Ph.D. dissertation, </type> <institution> Carnegie-Mellon University, </institution> <year> 1983. </year>
Reference-contexts: Since the SELF system can switch to unoptimized code, it is able to avoid these problems. LOIPE <ref> [52] </ref> uses transparent incremental recompilation for debugging purposes. For example, when the user sets a breakpoint in some procedure, this procedure is converted to unoptimized form to make debugging easier.
Reference: [53] <author> John R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. Other systems use profile information to assist classic code optimizations [30], procedure inlining [29, 94], trace scheduling <ref> [53] </ref>, and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime. The main goal of his work was to minimize the total cost of running a program which presumably was executed only once.
Reference: [54] <author> Margaret A. Ellis and Bjarne Stroustrup, </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: That is, even if a lookup cache is 100% effective (all accesses hit), sends are still relatively slow because the lookup cache probe adds roughly ten instructions to every send. Various complications ensue when using multiple inheritance; for a discussion, see e.g. <ref> [54] </ref>.
Reference: [55] <author> Michael Franz. </author> <title> Technological steps toward a software component industry. </title> <booktitle> International Conference on Programming Languages and System Architecture, Springer Verlag Lecture Notes in Computer Science 782, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Franz <ref> [55] </ref> describes a variation of dynamic compilation that generates machine code at load time from a compact intermediate-code representation. Like the byte codes contained in Smalltalk snapshots, the intermediate code is architecture-independent; in addition, it is also intended to be language-independent (although the current implementation supports only Oberon).
Reference: [56] <author> Free Software Foundation. </author> <title> GNU C++ compiler. </title> <address> Boston, MA, </address> <year> 1993. </year>
Reference-contexts: The implementation we use, ParcPlace Smalltalk-80, is a widely used commercial implementation and is regarded as the fastest Smalltalk implementation available today. To provide another point of reference, we also compare SELF-93 with the most widely used statically-typed object-oriented language, C++. We use the GNU C++ compiler <ref> [56] </ref> because it is probably the most widely used C++ implementation and generates very good code, and Sun CC, which is typical of C++ implementations based on a. Excluding blank lines b.
Reference: [57] <author> Charles D. Garrett, Jeffrey Dean, David Grove, and Craig Chambers. </author> <title> Measurement and Application of Dynamic Receiver Class Distributions. </title> <type> Technical Report CSE-TR-94-03-05, </type> <institution> University of Washington, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: For type feedback to work well, the dynamic number of receiver types per call site should be close to one, i.e., one or two receiver types should dominate. A large fraction of call sites in C++ have this property <ref> [17, 57] </ref>, and it also holds in other object-oriented programming languages (e.g., Smalltalk, SELF, Sather, and Eiffel); this is the reason why inline caching [44, 70] works well in these languages as an implementation of dynamic dispatch. <p> The type profile is much more likely to remain unchanged from input to input than the time Table A-6 in Appendix A has detailed data. faster 75 profile. (A recent study by Garrett et al. <ref> [57] </ref> confirmed this hypothesis for Cecil and C++.) Second, the system can dynamically adapt to changes in the time or type profile by reoptimizing new hot spots and by recompiling to incorporate new types.
Reference: [58] <author> Adele Goldberg and David Robson, </author> <title> Smalltalk-80: The Language and Its Implementation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Smith at Xerox PARC [115]. Conceived as an alternative to the Smalltalk-80 programming language <ref> [58] </ref>, SELF attempts to maximize programmer productivity in an exploratory programming environment by keeping the language simple and pure without reducing expressiveness and malleability. <p> Along the branch where the type test succeeds, the compiler has precise information about the type of the receiver and can statically bind and inline a copy of the message. For example, existing SELF and Smalltalk systems predict that + will be sent to an integer <ref> [128, 58, 44] </ref>, since measurements indicate that this occurs 90% of the time [130]. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. <p> Some of these systems used mechanisms similar to customization (e.g., Guibas and Wyatt [61]) and inline caching (Saal and Weiss [111]). Deutsch and Schiffman pioneered the use of dynamic compilation for object-oriented systems. Their Smalltalk-80 implementation [44] dynamically translates the byte codes defined by the Smalltalk virtual machine <ref> [58] </ref> into native machine code and caches the compiled code for later use; Deutsch and Schiffman estimate that just using simple dynamic compilation instead of interpretation speeds up their system by a factor of 1.6, and that a more sophisticated compiler gains almost a factor of two. <p> Many systems (e.g., most Smalltalk-80 and SELF implementations) statically predict the type of certain messages. For example, the receiver of ifTrue: is predicted to be either the true or false object, and the receiver of + is predicted to be an integer <ref> [128, 58, 44, 130] </ref>. Static type prediction has obvious performance advantages, but it contributes to unstable performance. First, the specialized messages execute much faster than semantically equivalent code using non-specialized selectors.
Reference: [59] <author> Susan L. Graham, Peter Kessler, and Marshall McKusick. </author> <title> An execution profiler for modular programs. </title> <journal> Software Practice and Experience 13 </journal> <pages> 617-685. </pages>
Reference-contexts: Thus, we believe that type feedback is probably easier to add to a conventional batch-style compilation system. In such a system, optimization would proceed in three phases (Figure 5-3). First, the executable is instrumented to record receiver types, for example with a gprof-like profiler <ref> [59] </ref>. (The standard gprof profiler already collects almost all information needed by type feedback, except that its data is caller-specific rather than call-site specific, i.e., it does not separate two calls of foo if both come from the same function.) Then, the application is run with one or more test inputs
Reference: [60] <author> Justin Graver and Ralph Johnson. </author> <title> A Type System for Smalltalk. </title> <booktitle> In Conference Record of the 17th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> San Francisco, CA, </address> <month> January, </month> <year> 1990. </year>
Reference: [61] <author> Leo J. Guibas and Douglas K. Wyatt. </author> <title> Compilation and Delayed Evaluation in APL. </title> <booktitle> In Fifth Annual ACM Symposium on Principles of Programming Languages, p. </booktitle> <pages> 1-8, </pages> <year> 1978. </year>
Reference-contexts: Not surprisingly, APL systems were among the first to exploit dynamic compilers. For example, Johnston [80] describes an APL system using dynamic compilation as an efficient alternative to interpretation. Some of these systems used mechanisms similar to customization (e.g., Guibas and Wyatt <ref> [61] </ref>) and inline caching (Saal and Weiss [111]). Deutsch and Schiffman pioneered the use of dynamic compilation for object-oriented systems. <p> Similarly, APL compilers created specialized code for certain expressions <ref> [80, 51, 61] </ref>. Of these systems, the HP APL compiler [51] comes closest to the customization technique used by SELF. The HP APL/3000 system compiles code on a statement-by-statement basis. <p> For example, Mitchells system [97] specialized arithmetic operations to the runtime types of the operands. Similarly, APL compilers created specialized code for certain expressions <ref> [80, 51, 61] </ref>. Of these systems, the HP APL compiler [51] was the most exible. The system compiled code on a statement-by-statement basis. In addition to performing APL-specific optimizations, compiled code was specialized according to the specific operand types (number of dimensions, size of each dimension, element type, etc.).
Reference: [62] <author> Mary Wolcott Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> Technical Report COMP TR91-157 (Ph.D. thesis), </type> <institution> Computer Science Department, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: In other words, it remains to be seen if global (whole-program) analyses can significantly improve performance over a type-feedback based system. 5.8.4 Inlining Inlining has been studied extensively for procedural languages <ref> [29, 36, 39, 62, 74] </ref>. For these languages, inlining is simpler since they do not (or only rarely) use dynamic dispatch. Furthermore, inlining candidates are easier to analyze because the space and time cost of most language constructs is known. <p> However, aggressive inlining generally significantly increases the performance of SELF programs. This effect is in marked contrast to studies of other languages (such as C or Fortran) where inlining 73 was found to have only small benefits, if any <ref> [29, 36, 39, 62, 74] </ref>. We attribute this discrepancy to a number of signif icant differences between SELF and conventional languages: 1. <p> Interestingly, modern computer architectures create similar situations for conventional languages. For example, inlining a single call may dramatically speed up a Fortran program if a loop can be parallelized with the improved data dependence information <ref> [62] </ref>. The speedup does not result from reducing the execution time of the inlined code but from optimizations enabled in the caller. to the overall speedup. <p> Now, with adaptive recompilation, it should be possible to customize lazily, i.e., only in performance-critical parts of the program, and only if customization would benefit the code (i.e., if many messages are sent to self). Similarly, the system could choose to perform method cloning <ref> [62] </ref> rather than method inlining.
Reference: [63] <author> Gilbert J. Hansen, </author> <title> Adaptive Systems for the Dynamic Run-Time Optimization of Programs. </title> <type> Ph.D. Thesis, </type> <institution> Carn-egie-Mellon University, </institution> <year> 1974. </year>
Reference-contexts: Other systems use profile information to assist classic code optimizations [30], procedure inlining [29, 94], trace scheduling [53], and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran <ref> [63] </ref>. His compiler optimized the inner loops of Fortran programs at runtime. The main goal of his work was to minimize the total cost of running a program which presumably was executed only once.
Reference: [64] <author> Richard L. Heintz. </author> <title> Low-Level Optimizations for an Object-Oriented Programming Language. M. Sc. </title> <type> Thesis, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: Second, loop splitting would be relatively expensive to implement since it requires type analysis to realize its benefits [21]. Instead of performing splitting in the front end, we could obtain similarly good code with back end optimization. Heintz <ref> [64] </ref> proposes such an optimization (called rerouting predecessors) for a Smalltalk compiler, and similar back-end optimizations for conventional languages are well-known [3, 10].
Reference: [65] <author> John L. Hennessy. </author> <title> Symbolic Debugging of Optimized Code. </title> <journal> ACM Transactions of Programming Languages and Systems 4(3), </journal> <month> July </month> <year> 1982. </year>
Reference-contexts: Most existing systems do not support the debugging of optimized code. Programs can either be optimized for full speed, or they can be compiled without optimizations for full source-level debugging. Recently, techniques have been developed that strive to make it possible to debug optimized code <ref> [65, 143, 37] </ref>. However, none of these systems is able to provide full source-level debugging. For example, it generally is not possible to obtain the values of all source-level variables, to single-step through the program, or to change the value of a variable. <p> Unfortunately, if the source-level state of the program must be recoverable at every point in the program, i.e., at virtually every instruction boundary (to support single-stepping), existing techniques <ref> [2, 3, 65, 143, 37] </ref> severely restrict the optimizations that could be performed, effectively disabling many common optimizations. We solve this dilemma by relaxing the debugging requirements placed on optimized code. <p> Interrupt points have been used in other systems before; see Section 10.7 for a discussion of the Deutsch-Schiffman Smalltalk-80 system. Also, Hennessy <ref> [65] </ref> used a similar mechanism to define stopping points between each statement, as did Zellweger [144]. real stack frame f (includes virtual activations vf 1 , vf 2 , and vf 3 ) stack grows downwards g returns vf 2 real stack frame g deoptimization vf 1 vf 2 vf 3 <p> Similarly, some other optimizations (e.g., code motion or induction variable elimination) may sometimes not be performed if the debuggers recovery techniques are not powerful enough to hide the optimizations effects at one or more interrupt points. The extent of this problems depends on the particular recovery techniques used (e.g., <ref> [2, 3, 65, 143, 37] </ref>) and on the average distance between interrupt points. All these optimizations can be performed, however, if there is no interrupt point within the affected variables scope (see Section 10.2.4). <p> Thus, using interrupt points and deoptimization amplifies the benefits of techniques aimed at recovering source-level values. Most of the techniques discussed below could be combined with our work to support even more optimizations. Hennessy <ref> [65] </ref> addresses the problem of recovering the values of variables in the presence of selected local and global code reordering optimizations. His algorithms can detect when a variable has an incorrect value (in terms of the source program) and can sometimes reconstruct the source-level value.
Reference: [66] <author> Mark D. Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> Technical Report UCB/CSD 87/381, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: To accurately measure execution times, we wrote a SPARC simulator based on the spa [76] and shade [34] tracing tools and the dinero cache simulator <ref> [66] </ref>. The simulator models the Cypress CY7C601 implementation of the SPARC architecture running at 40 MHz, i.e., the chip used in the SPARCstation-2 (SS-2) workstation. It also accurately models the memory system of a SS-2, with the exception of a different cache organization.
Reference: [67] <author> W. Wilson Ho and Ronald A. Olsson. </author> <title> An Approach to Genuine Dynamic Linking. </title> <journal> SoftwarePractice and Experience 21(4), </journal> <volume> p. </volume> <pages> 375-390, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences [103]. In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks [1], and dynamic linking <ref> [67] </ref>.
Reference: [68] <author> C. A. R. Hoare. </author> <title> Hints on programming language design. </title> <booktitle> In First Annual ACM Symposium on Principles of Programming Languages, p. </booktitle> <pages> 31-40, </pages> <year> 1973. </year>
Reference-contexts: For example, Hoare <ref> [68] </ref> argues as follows: The only language which has been optimized with general success is Fortran, which was specifically designed for that purpose.
Reference: [69] <author> Urs Hlzle, Bay-Wei Chang, Craig Chambers, Ole Agesen, and David Ungar. </author> <title> The SELF Manual, </title> <note> Version 1.1. Unpublished manual, </note> <month> February </month> <year> 1991. </year>
Reference: [70] <author> Urs Hlzle, Craig Chambers, and David Ungar. </author> <title> Optimizing Dynamically-Typed Object-Oriented Languages with Polymorphic Inline Caches. </title> <booktitle> In ECOOP91 Conference Proceedings, Geneva, 1991. Published as Springer Verlag Lecture Notes in Computer Science 512, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: The programs (with the exception of PolyTest) can be considered fairly typical object-oriented programs and cover a variety of programming styles. More detailed data about the benchmarks is given in <ref> [70] </ref>. Parser. A recursive-descent parser for an earlier version of the SELF syntax (550 lines). PrimitiveMaker. A program generating C++ and SELF stub routines from a description of primitives (850 lines). UI. The SELF user interface prototype (3000 lines) running a short interactive session. <p> A large fraction of call sites in C++ have this property [17, 57], and it also holds in other object-oriented programming languages (e.g., Smalltalk, SELF, Sather, and Eiffel); this is the reason why inline caching <ref> [44, 70] </ref> works well in these languages as an implementation of dynamic dispatch. Therefore, we expect type feedback to work well for these languages; the higher the frequency of dynamically-dispatched calls, the more beneficial type feedback could be. <p> Unfortunately, they apparently were not aware of previous work; their proposed if conversion appears to be identical to inline caching [44], and a proposed extension of if conversion is identical to type feedback (which was first presented in <ref> [70] </ref>). Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. <p> However, we have ample evidence that this explanation is not accurate; for example, about half of the non-inlined sends in Richards are access methods that just return (or set) the value of an instance variable <ref> [70] </ref>. If the compiler had known the receiver type of these sends, it would have inlined them. faster 71 7.3 Type feedback works To evaluate the performance impact of type feedback, we compared SELF-93 with SELF-93-nofeedback. <p> Apparently, type feedback exposes many new inlining opportunities that SELF-93-nofeedback cannot exploit because it lacks receiver type information. For example, in SELF-93-nofeedback about half of the non-inlined sends in Richards are access methods that just return (or set) the value of an instance variable <ref> [70] </ref>. All of these sends are inlined in SELF-93. See Table A-4 in Appendix A for detailed data. faster 72 It is also interesting to look at the number of calls relative to completely unoptimized SELF.
Reference: [71] <author> Urs Hlzle, Craig Chambers, and David Ungar. </author> <title> Debugging Optimized Code with Dynamic Deoptimization. </title> <booktitle> In Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 32-43. </pages> <note> Published as SIGPLAN Notices 27(7), </note> <month> July </month> <year> 1992. </year> <month> 160 </month>
Reference-contexts: Conversely, whenever the user changes a source method, all compiled code depending on the old definition is invalidated. To accomplish this, the system keeps dependency links between source and compiled methods <ref> [71, 21] </ref>. Since there is no explicit compilation or linking step, the traditional edit-compile-link-run cycle is collapsed into an edit-run cycle.
Reference: [72] <author> Urs Hlzle. </author> <title> A Fast Write Barrier for Generational Garbage Collectors. </title> <booktitle> Proceedings of the OOPSLA93 Workshop on Garbage Collection, </booktitle> <address> Washington, D.C., </address> <month> September </month> <year> 1993. </year>
Reference-contexts: For example, store checks slow down Richards by about 8% on a SPARCstation-2. A detailed analysis is beyond the scope of this thesis, and we refer the interested reader to <ref> [72] </ref> for more information. 6.3.3 Block zapping Consider the following method: foo = ( | localVar | globalVar: [ localVar: hello ] ) The first 2 fields of every object are used by the system, so the first user-defined field has an offset of 8 bytes. 64 In this example, the
Reference: [73] <author> Antony Hosking, J. Eliot B. Moss, and Darko Stefanovic. </author> <title> A comparative performance evaluation of write barrier implementations. </title> <booktitle> In OOPSLA'92 Proceedings, </booktitle> <pages> pp. 92-109. </pages>
Reference: [74] <author> W.W. Hwu and P.P. Chang. </author> <title> Inline function expansion for compiling C programs. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 246-57, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(7), </note> <month> July </month> <year> 1989. </year>
Reference-contexts: In other words, it remains to be seen if global (whole-program) analyses can significantly improve performance over a type-feedback based system. 5.8.4 Inlining Inlining has been studied extensively for procedural languages <ref> [29, 36, 39, 62, 74] </ref>. For these languages, inlining is simpler since they do not (or only rarely) use dynamic dispatch. Furthermore, inlining candidates are easier to analyze because the space and time cost of most language constructs is known. <p> However, aggressive inlining generally significantly increases the performance of SELF programs. This effect is in marked contrast to studies of other languages (such as C or Fortran) where inlining 73 was found to have only small benefits, if any <ref> [29, 36, 39, 62, 74] </ref>. We attribute this discrepancy to a number of signif icant differences between SELF and conventional languages: 1.
Reference: [75] <author> Daniel H. Ingalls. </author> <title> A Simple Technique for Handling Multiple Polymorphism. </title> <booktitle> In OOPSLA 86 Conference Proceedings, </booktitle> <address> Portland, OR, </address> <year> 1986. </year> <note> Published as SIGPLAN Notices 21(11), </note> <month> November, </month> <year> 1986. </year>
Reference: [76] <author> Gordon Irlam. </author> <title> SPASPARC analyzer toolset. </title> <note> Available via ftp from cs.adelaide.edu.au, </note> <year> 1991. </year>
Reference-contexts: For example, system A is 2 times faster than system B means that the geometric mean of the benchmarks execution time ratios (time taken by B divided by time taken by A) is 2. To accurately measure execution times, we wrote a SPARC simulator based on the spa <ref> [76] </ref> and shade [34] tracing tools and the dinero cache simulator [66]. The simulator models the Cypress CY7C601 implementation of the SPARC architecture running at 40 MHz, i.e., the chip used in the SPARCstation-2 (SS-2) workstation.
Reference: [77] <author> Geeske Joel, Gregory Aplet, and Peter M. Vitousek. </author> <title> Leaf morphology along environmental gradients in Hawaiian Metrosideros polymorpha. </title> <booktitle> Biotropica 26(1), p. </booktitle> <pages> 17-22, </pages> <year> 1994. </year>
Reference: [78] <author> Ralph Johnson(ed.). </author> <title> Workshop on Compiling and Optimizing Object-Oriented Programming Languages. </title> <booktitle> In Addendum to the OOPSLA 87 Conference Proceedings, p. </booktitle> <pages> 59-65, </pages> <address> Orlando, FL, </address> <month> October, </month> <year> 1987. </year> <note> Published as SIGPLAN Notices 23(5), </note> <month> May </month> <year> 1988. </year>
Reference: [79] <author> Ralph E. Johnson, Justin O. Graver, and Lawrence W. Zurawski. </author> <title> TS: An Optimizing Compiler for Smalltalk. </title> <booktitle> In OOPSLA 88 Conference Proceedings, p. </booktitle> <pages> 18-26, </pages> <address> San Diego, CA, </address> <month> October </month> <year> 1988. </year> <note> Published as SIGPLAN Notices 23(11), </note> <month> November </month> <year> 1988. </year>
Reference-contexts: The TS compiler <ref> [79] </ref> took a similar approach. Types were specified as sets of classes, and the compiler could statically bind calls where the receiver type was a single class. Methods were only inlined if the programmer had marked them as inlinable. <p> Thus (and because it was written in Smalltalk itself), the compiler was very slow, needing 15 to 30 seconds to compile the benchmarks which were all very short <ref> [79] </ref>. Few published data on the efficiency of the generated code are available since the TS compiler was never fully completed and could compile only very small benchmark programs. <p> Data comparing TS to the Tektronix Smalltalk interpreter on a 68020-based workstation indicate that TS ran about twice as fast as the Deutsch-Schiffman system for small benchmarks that were annotated with type declarations for TS <ref> [79] </ref>. However, this comparison is not entirely valid because TS did not implement the full semantics of some Smalltalk constructs. For example, integer arithmetic is not checked for overow. This could have a significant impact on the performance of the small TS benchmarks. <p> However, the system does not use lazy conversion. Furthermore, their definition of inspection points allows asynchronous events such as user interrupts to be delayed arbitrarily. Some of their ideas were implemented for the Typed Smalltalk compiler <ref> [79] </ref>, but the system apparently could only run very small programs and was not used in practice, unlike our system which is in daily use.
Reference: [80] <author> Ronald L. Johnston. </author> <title> The Dynamic Incremental Compiler of APL"3000. In Proceedings of the APL 79 Conference. </title> <note> Published as APL Quote Quad 9(4), </note> <author> p. </author> <month> 82-87. </month>
Reference-contexts: APL is a language that is both hard to compile statically (because most operators are polymorphic) and that emphasizes interactive use. Not surprisingly, APL systems were among the first to exploit dynamic compilers. For example, Johnston <ref> [80] </ref> describes an APL system using dynamic compilation as an efficient alternative to interpretation. Some of these systems used mechanisms similar to customization (e.g., Guibas and Wyatt [61]) and inline caching (Saal and Weiss [111]). Deutsch and Schiffman pioneered the use of dynamic compilation for object-oriented systems. <p> Similarly, APL compilers created specialized code for certain expressions <ref> [80, 51, 61] </ref>. Of these systems, the HP APL compiler [51] comes closest to the customization technique used by SELF. The HP APL/3000 system compiles code on a statement-by-statement basis. <p> For example, Mitchells system [97] specialized arithmetic operations to the runtime types of the operands. Similarly, APL compilers created specialized code for certain expressions <ref> [80, 51, 61] </ref>. Of these systems, the HP APL compiler [51] was the most exible. The system compiled code on a statement-by-statement basis. In addition to performing APL-specific optimizations, compiled code was specialized according to the specific operand types (number of dimensions, size of each dimension, element type, etc.).
Reference: [81] <author> Norm Jouppi. </author> <title> Cache Write Policies and Performance. </title> <booktitle> In ISCA'20 Conference Proceedings, </booktitle> <pages> pp. 191-201, </pages> <address> San Diego, CA, </address> <year> 1993. </year> <note> Published as Computer Architecture News 21(2), </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The SPARCstation-2s write buffer organization is somewhat unfortunate and incurs high write costs compared to other common designs. To avoid skewing our data, we chose not to model the SS-2 write buffer and instead assume a perfect write buffer. Previous work (e.g., <ref> [14, 81] </ref>) has shown that writes can be absorbed with virtually no cost, and Diwan et al. [49] report a write buffer CPI contribution of less than 0.02 for a 6-element deep buffer and a cache organization similar to the DECstation 500/200. 70 back ends. <p> We use time overhead rather than miss ratios because comparing miss ratios can be misleading. First, write misses are free since we assume they can be absorbed by a write buffer or write cache <ref> [14, 81] </ref>. Furthermore, data miss ratios are not directly related to execution time if (as is customary) they are given relative to the number of data references since the density of loads may vary from program to program.
Reference: [82] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> A Case for Runtime Code Generation. </title> <type> Technical Report UW TR 91-11-04, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, </address> <year> 1991. </year>
Reference-contexts: Compilation from intermediate code to machine code is fast enough to make the loader competitive with standard loaders. Dynamic compilation is also useful for applications other than language implementation <ref> [82] </ref> and has been used in a wide variety of ways. For example, Kessler et al. use it to implement fast breakpoints for debuggers [84]. Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences [103].
Reference: [83] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> Evaluating Runtime-Compiled Value-Specific Optimizations. </title> <type> Technical Report UW TR 93-11-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, </address> <year> 1993. </year>
Reference-contexts: Saltz et al. delay loop scheduling until runtime [112]. Przybylski et al. generate a specialized cache simulator for each different set of simulation parameters [104]. Keppel et al. discuss value-specific runtime specialization for a variety of applications <ref> [83] </ref>. In more theoretically-oriented areas of computer science, customization is known as partial evaluation [15]. 2.5.3 Previous SELF compilers The SELF compiler described in this thesis has two predecessors. The first SELF compiler [22, 23] introduced customization and splitting and achieved reasonably good performance.
Reference: [84] <author> Peter Kessler. </author> <title> Fast Breakpoints: </title> <booktitle> Design and Implementation. In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 78-84, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year> <note> Published as SIGPLAN Notices 25(6), </note> <month> June </month> <year> 1990. </year>
Reference-contexts: Dynamic compilation is also useful for applications other than language implementation [82] and has been used in a wide variety of ways. For example, Kessler et al. use it to implement fast breakpoints for debuggers <ref> [84] </ref>. Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences [103]. In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks [1], and dynamic linking [67].
Reference: [85] <author> Gregor Kiczales and Luis Rodriguez. </author> <title> Efficient Method Dispatch in PCL. </title> <type> Technical Report SSL-89-95, </type> <note> Xerox PARC, </note> <year> 1989. </year>
Reference-contexts: For example, the expression x &lt; 0 ifTrue: [ doSomething ] Richards and DeltaBlue are the only SELF benchmarks that have been translated to Smalltalk. Therefore, we included a subset of the commonly used Smalltalk Macro benchmarks <ref> [85] </ref> in our comparison. Benchmark slowdown factor Richards 10. DeltaBlue 8.1 AllCallsOn 15. AllImplementors 3.9 ClassOrganizer 7.6 Compiler 5.5 Decompiler 5.2 PrintHierarchy 5.8 Table 4-4.
Reference: [86] <author> Glenn Krasner, ed., </author> <title> Smalltalk-80: Bits of History and Words of Advice. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Many previous studies have found the execution characteristics of object-oriented languages to be very different from C and have argued the need for object-oriented architectures. For example, Smalltalk studies <ref> [86, 132] </ref> have shown calls to be much more frequent than in other languages. Even for a hybrid language like C++ (which has C at its core and thus shouldnt be too different), significant differences were found. <p> However, we can get a rough estimate by assuming that without register windows each call would store 1.5 values on the stack, namely the receiver and 0.5 arguments (the average number of arguments is taken from Smalltalk <ref> [86] </ref>). Non-leaf calls must save the return PC and frame pointer (2 words); we also assume they save two locals. Lacking any data, we assume that 50% of the calls are non-leaf calls. Together, the average call stores and reloads 3.5 words.
Reference: [87] <editor> Douglas Lea. Customization in C++. </editor> <booktitle> In Proceedings of the 1990 Usenix C++ Conference, p. </booktitle> <pages> 301-314, </pages> <address> San Fran-cisco, CA, </address> <month> April, </month> <year> 1990. </year>
Reference: [88] <author> Elgin Lee. </author> <title> Object Storage and Inheritance for SELF, a Prototype-Based Object-Oriented Programming Language. </title> <type> Engineers thesis, </type> <institution> Stanford University, </institution> <year> 1988. </year>
Reference-contexts: The code cache keeps approximate LRU information to determine which compiled methods to ush. Finally, the virtual machine also contains numerous primitives that can be invoked by SELF programs to perform arithmetic, I/O, graphics, and so on. New primitives can be dynamically linked into the system at runtime. References <ref> [88] </ref>, [115], and [21] contain further details about the system. 2.2.2 Efficiency Since SELFs pure semantics threatened to make programs extremely inefficient, much of the early implementation effort went into compiler techniques for optimizing SELF programs. Some of these techniques were very successful: Dynamic compilation.
Reference: [89] <author> Henry Lieberman and Carl Hewitt. </author> <title> A Real-Time Garbage Collector Based on the Lifetime of Objects. </title> <booktitle> Communications of the ACM 26 (6): </booktitle> <pages> 419-429. </pages>
Reference-contexts: when speculation fails, since it can recompile a method (reverting to non-speculative tests) should speculation fail frequently. 6.3.2 Store checks Generational garbage collectors need to keep track of references from older to younger generations so that younger generations can be garbage-collected without inspecting every object in the older generation (s) <ref> [89, 131] </ref>. The set of locations potentially containing pointers to newer objects is often called the remembered set [132]. At every store, the system must ensure that the updated location is added to the remembered set if the store creates a reference from an older to a newer object.
Reference: [90] <author> Henry Lieberman. </author> <title> Using Prototypical Objects to Implement Shared Behavior in Object-Oriented Systems. </title> <booktitle> In OOPSLA 86 Conference Proceedings, p. </booktitle> <pages> 214-223, </pages> <address> Portland, OR, </address> <month> September </month> <year> 1986. </year> <note> Published as SIGPLAN Notices 21(11), </note> <month> November </month> <year> 1986. </year>
Reference-contexts: SELFs main other highlights are listed below. SELF is dynamically-typed: programs contain no type declarations. SELF is based on prototypes <ref> [115, 90, 91] </ref> rather than classes. Every object is self-describing and can be changed independently. In addition to the exibility of this approach, prototype-based systems can also avoid the complexity introduced by metaclasses. SELF has multiple inheritance. The inheritance design underwent several changes over the years.
Reference: [91] <author> Henry Lieberman, Lynn Andrea Stein, and David Ungar. </author> <booktitle> The Treaty of Orlando. In Addendum to the OOPSLA 87 Conference Proceedings, p. </booktitle> <pages> 43-44, </pages> <address> Orlando, FL, </address> <month> October </month> <year> 1987. </year> <note> Published as SIGPLAN Notices 23(5), </note> <month> May </month> <year> 1988. </year>
Reference-contexts: SELFs main other highlights are listed below. SELF is dynamically-typed: programs contain no type declarations. SELF is based on prototypes <ref> [115, 90, 91] </ref> rather than classes. Every object is self-describing and can be changed independently. In addition to the exibility of this approach, prototype-based systems can also avoid the complexity introduced by metaclasses. SELF has multiple inheritance. The inheritance design underwent several changes over the years.
Reference: [92] <author> Mark Linton, John Vlissides, and Paul Calder. </author> <title> Composing User Interfaces with Interviews. </title> <booktitle> IEEE Computer 22(2) </booktitle> <pages> 8-22, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Third, we expect that C++ programmers will make even more use of virtual functions in the future as they become more familiar with object-oriented programming styles; for example, recent versions of the Interviews framework <ref> [92] </ref> use virtual functions more frequently than previous versions. instrumented program source program optimized program compiler compiler type feedback data 43 To give a concrete example, the DOC document editor measured in [16] performs a virtual call every 75 instructions; given that a C++ virtual call uses about 5 instructions and
Reference: [93] <author> Cathy May. </author> <title> MIMIC: A fast system /370 simulator. </title> <booktitle> In SIGPLAN'87 Symposium on Interpreters and Interpretive Techniques, </booktitle> <month> June </month> <year> 1987. </year>
Reference-contexts: Dynamic compilation has also been used in other areas such as database query optimization [19, 42], microcode generation [107], and fast instruction set emulation <ref> [34, 93] </ref>. 2.5.2 Customization The idea of customizing portions of compiled code to some specific environment is closely related to dynamic compilation since the environment information is often not available until runtime. For example, Mitchells system [97] specialized arithmetic operations to the runtime types of the operands.
Reference: [94] <author> Scott McFarling. </author> <title> Procedure Merging with Instruction Caches. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 71-79. </pages> <note> Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1991. </year> <month> 161 </month>
Reference-contexts: Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. Other systems use profile information to assist classic code optimizations [30], procedure inlining <ref> [29, 94] </ref>, trace scheduling [53], and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime.
Reference: [95] <author> Scott McFarling. </author> <title> Program analysis and optimization for machines with instruction cache. </title> <type> Technical Report CSL-TR-91-493, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts <ref> [95, 96] </ref>. Other systems use profile information to assist classic code optimizations [30], procedure inlining [29, 94], trace scheduling [53], and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime.
Reference: [96] <institution> MIPS Computer Systems, MIPS Language Programmers Guide. MIPS Computer Systems, </institution> <address> Sunnyvale, CA, </address> <year> 1986. </year>
Reference-contexts: Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts <ref> [95, 96] </ref>. Other systems use profile information to assist classic code optimizations [30], procedure inlining [29, 94], trace scheduling [53], and register allocation [136, 98]. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime.
Reference: [97] <author> J. G. Mitchell, </author> <title> Design and Construction of Flexible and Efficient Interactive Programming Systems. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1970. </year>
Reference-contexts: The idea of dynamically creating compiled code rather than using traditional batch-style compilation grew of the quest for faster interpreters; by compiling to native code, some interpreter overhead (especially the decoding of the pseudo-code instructions) can be avoided. For example, Mitchell <ref> [97] </ref> proposed to convert parts of dynamically-typed interpreted programs into compiled form, assuming that the types of variables remained constant. Compiled code was generated as a side-effect of interpreting an expression for the first time. Similarly, threaded code [12] was used early on to remove some of the interpretation overhead. <p> For example, Mitchells system <ref> [97] </ref> specialized arithmetic operations to the runtime types of the operands. Whenever the type of a variable changed, all 10 compiled code which depended on its type was discarded. <p> However, none of these systems predicted types dynamically as does our system; there was no feedback. 5.8.2 Customization Other systems have used mechanisms similar to customization, which is a limited form of runtime type feedback. For example, Mitchells system <ref> [97] </ref> specialized arithmetic operations to the runtime types of the operands. Similarly, APL compilers created specialized code for certain expressions [80, 51, 61]. Of these systems, the HP APL compiler [51] was the most exible. The system compiled code on a statement-by-statement basis.
Reference: [98] <author> W. G. Morris. </author> <title> A Prototype Coagulating Code Generator. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 45-58. </pages> <note> Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1991. </year>
Reference-contexts: Like the Deutsch-Schiffman compiler, the SOAR compiler did not perform extensive global optimizations or method inlining. On the hardware side, SOAR was a variation of the Berkeley RISC II processor <ref> [98] </ref>; the most important hardware features were register windows and tagged integer instructions. <p> Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. Other systems use profile information to assist classic code optimizations [30], procedure inlining [29, 94], trace scheduling [53], and register allocation <ref> [136, 98] </ref>. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime. The main goal of his work was to minimize the total cost of running a program which presumably was executed only once.
Reference: [99] <author> Alexandru Nicolau. </author> <title> Run-time disambiguation: Coping with statically unpredictable dependencies. </title> <journal> IEEE Transactions on Computers 38(5) </journal> <pages> 663-678, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Customization has also been applied to more traditional, batch-oriented compilers. For example, Cooper et al. describe a FORTRAN compiler that can create customized versions of procedures to enable certain loop optimizations [36]. Nicolau describes a FORTRAN compiler that dynamically selects the appropriate statically-generated version of a loop <ref> [99] </ref>. Saltz et al. delay loop scheduling until runtime [112]. Przybylski et al. generate a specialized cache simulator for each different set of simulation parameters [104]. Keppel et al. discuss value-specific runtime specialization for a variety of applications [83].
Reference: [100] <author> John Ousterhout. </author> <title> Why aren't operating systems getting faster as fast as hardware? DEC Western Research Laboratory, </title> <note> Technical Note TN-11, </note> <year> 1989. </year>
Reference-contexts: The advantage of the byte marking scheme is its speed. In SELF-91, a store check involves just 3 SPARC instructions in addition to the actual store: In fact, traps seem to become slower as hardware becomes faster <ref> [100] </ref>. However, recent designs attempt to reduce trap overhead; for example, the SPARC V9 architecture [119] allows low-overhead user-mode traps.
Reference: [101] <author> David A. Patterson and David Ditzel. </author> <title> The Case for the Reduced Instruction Set Computer. </title> <booktitle> Computer Architecture News 8(6) </booktitle> <pages> 25-33, </pages> <month> October </month> <year> 1980. </year>
Reference: [102] <author> David A. Patterson. </author> <title> Reduced Instruction Set Computers. </title> <booktitle> Communications of the ACM 28 (1): </booktitle> <pages> 8-21, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Similarly, a window underow trap is used to reload such a ushed register set from memory if it is needed again. Register windows were originally proposed as part of the Berkeley RISC architecture, with the intention of making procedure calls as fast as possible <ref> [102] </ref>. If the variation in the procedure call depth does not significantly exceed the number of available register windows (usually, 7 or 8 on current SPARC implementations), most calls are executed without any memory accesses.
Reference: [103] <author> Rob Pike, Bart N. Locanthi, and John F. Reiser. </author> <title> Hardware/Software Trade-Offs for Bitmap Graphics on the Blit. </title> <journal> SoftwarePractice and Experience 15(2), </journal> <volume> p. </volume> <pages> 131-151, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: For example, Kessler et al. use it to implement fast breakpoints for debuggers [84]. Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences <ref> [103] </ref>. In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks [1], and dynamic linking [67].
Reference: [104] <author> Steven Przybylski, Mark Horowitz, and John Hennessy. </author> <title> Performance Trade-offs in Cache Design. </title> <booktitle> Proceedings of the 15th International Symposium on Computer Architecture, p. </booktitle> <pages> 290-298, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Nicolau describes a FORTRAN compiler that dynamically selects the appropriate statically-generated version of a loop [99]. Saltz et al. delay loop scheduling until runtime [112]. Przybylski et al. generate a specialized cache simulator for each different set of simulation parameters <ref> [104] </ref>. Keppel et al. discuss value-specific runtime specialization for a variety of applications [83]. In more theoretically-oriented areas of computer science, customization is known as partial evaluation [15]. 2.5.3 Previous SELF compilers The SELF compiler described in this thesis has two predecessors.
Reference: [105] <author> Calton Pu and Henry Massalin. </author> <title> An Overview of the Synthesis Operating System. </title> <type> Technical Report CUCS-470-89, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, </address> <year> 1989. </year>
Reference-contexts: For example, Kessler et al. use it to implement fast breakpoints for debuggers [84]. Pike et al. speed up bit-blt graphics primitives by dynamically generating optimal code sequences [103]. In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism <ref> [32, 105] </ref> and to eliminate the overhead of protocol stacks [1], and dynamic linking [67].
Reference: [106] <author> William Pugh and Grant Weddell. </author> <title> Two-Directional Record Layout for Multiple Inheritance. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 85-91, </pages> <address> White Plains, NY, </address> <month> June, </month> <year> 1990. </year> <note> Published as SIGPLAN Notices 25(6), </note> <month> June </month> <year> 1990. </year>
Reference: [107] <author> B. R. Rau. </author> <title> Levels of Representation of Programs and the Architecture of Universal Host Machines. </title> <booktitle> Proceedings of Micro 11, Asilomar, </booktitle> <address> CA, </address> <month> November </month> <year> 1978. </year>
Reference-contexts: In operating systems, dynamic compilation has been used to efficiently support fine-grain parallelism [32, 105] and to eliminate the overhead of protocol stacks [1], and dynamic linking [67]. Dynamic compilation has also been used in other areas such as database query optimization [19, 42], microcode generation <ref> [107] </ref>, and fast instruction set emulation [34, 93]. 2.5.2 Customization The idea of customizing portions of compiled code to some specific environment is closely related to dynamic compilation since the environment information is often not available until runtime.
Reference: [108] <author> Mark B. Reinhold. </author> <title> Cache Performance of Garbage-Collected Programming Languages. </title> <publisher> Technical Report MIT/ LCS/TR-581, MIT, </publisher> <month> September </month> <year> 1993. </year>
Reference-contexts: However, our data is consistent with that of Diwan et al. who have measured allocation-intensive ML programs [48] and found very low data cache overheads for the same cache organization (write-allocate, subblock placement). Our data also confirms that of Reinhold <ref> [108] </ref> who investigated the cache performance of large Lisp programs. Diwan et al also measured that the data cache overhead of their ML programs increases substantially with a write-noallocate policy, i.e., with a cache that does not allocate a cache line on a write miss.
Reference: [109] <author> Stephen E. Richardson. </author> <title> Evaluating interprocedural code optimization techniques. </title> <type> Ph.D. thesis, </type> <institution> Computer Systems Laboratory Technical Report CSL TR 91-460, Stanford University, </institution> <month> Feb </month> <year> 1991. </year>
Reference-contexts: Compilers for more conventional languages (e.g., GNU C [120]) have used the size of intermediate code to guide inlining decisions, making the decision before or after optimizing the intermediate code <ref> [109] </ref>.
Reference: [110] <author> John R. Rose. </author> <title> Fast Dispatch Mechanisms for Stock Hardware. </title> <booktitle> In OOPSLA 88 Conference Proceedings, p. </booktitle> <pages> 27-35, </pages> <address> San Diego, CA, </address> <month> October, </month> <year> 1988. </year> <note> Published as SIGPLAN Notices 23(11), </note> <month> November, </month> <year> 1988. </year>
Reference: [111] <author> H. J. Saal and Z. Weiss. </author> <title> A Software High Performance APL Interpreter. </title> <type> APL Quote Quad 9 </type> (4):74-81, 1979. 
Reference-contexts: Not surprisingly, APL systems were among the first to exploit dynamic compilers. For example, Johnston [80] describes an APL system using dynamic compilation as an efficient alternative to interpretation. Some of these systems used mechanisms similar to customization (e.g., Guibas and Wyatt [61]) and inline caching (Saal and Weiss <ref> [111] </ref>). Deutsch and Schiffman pioneered the use of dynamic compilation for object-oriented systems. <p> For example, SOAR (a Smalltalk implementation for a RISC processor) would have been 33% slower without inline caching [132]. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does the SELF system. A similar technique was used by Saal and Weiss APL interpreter <ref> [111] </ref> which used specialized routines for some operations (such as generic arithmetic) and speculatively used the previous specialization for the next call.
Reference: [112] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and Runtime Compilation. </title> <booktitle> Proceedings of the International Workshop on Compilers for Parallel Computers, </booktitle> <address> Paris, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: For example, Cooper et al. describe a FORTRAN compiler that can create customized versions of procedures to enable certain loop optimizations [36]. Nicolau describes a FORTRAN compiler that dynamically selects the appropriate statically-generated version of a loop [99]. Saltz et al. delay loop scheduling until runtime <ref> [112] </ref>. Przybylski et al. generate a specialized cache simulator for each different set of simulation parameters [104]. Keppel et al. discuss value-specific runtime specialization for a variety of applications [83].
Reference: [113] <author> Michael Sannella, John Maloney, Bjorn Freeman-Benson, and Alan Borning. </author> <title> Multi-way versus One-way Constraints in User Interfaces: Experience with the DeltaBlue Algorithm. </title> <booktitle> SoftwarePractice and Experience 23 (5): </booktitle> <pages> 529-566. </pages>
Reference-contexts: SELF-91 could not execute the CecilComp benchmark unless it was run using the simulator described in Section 7.1. (We believe that the bug is related to register window ushing, which would explain why the simulated benchmark runs correctly.) Benchmark Size (lines) a Description small benchmarks DeltaBlue 500 two-way constraint solver <ref> [113] </ref> developed at the University of Washington PrimMaker 1100 program generating glue stubs for external primitives callable by SELF Richards 400 simple operating system simulator originally written in BCPL by Martin Richards lar ge benchmarks CecilComp 11,500 Cecil-to-C compiler compiling the Fibonacci function (the com piler shares about 80% of its
Reference: [114] <author> H. Schlaeppi and H. Warren. </author> <title> Design of the FDS Interactive Debugging System. </title> <institution> IBM Research Report RC7214, IBM Yorktown Heights, </institution> <month> July </month> <year> 1978. </year>
Reference-contexts: Adl-Tabatabai et al. [2, 3] investigate the problem of detecting and recovering the values of source-level variables in the presence of instruction scheduling. Other recovery mechanisms are described by Coutant et al. [37] and by Schlaeppi and Warren <ref> [114] </ref>. Zellweger [143, 144] describes an interactive source-level debugger for Cedar which handles two optimizations, procedure inlining and cross-jumping, to provide expected behavior in most cases.
Reference: [115] <author> The SELF Group. </author> <title> The SELF Manual, </title> <note> Version 2.0. Unpublished manual, </note> <month> August </month> <year> 1992. </year>
Reference-contexts: Then, we will describe the compilation process of our new system, and finally we will review related work. 2.1 The SELF language SELF is a dynamically-typed prototype-based object-oriented language originally designed in 1986 by David Ungar and Randall B. Smith at Xerox PARC <ref> [115] </ref>. Conceived as an alternative to the Smalltalk-80 programming language [58], SELF attempts to maximize programmer productivity in an exploratory programming environment by keeping the language simple and pure without reducing expressiveness and malleability. <p> SELFs main other highlights are listed below. SELF is dynamically-typed: programs contain no type declarations. SELF is based on prototypes <ref> [115, 90, 91] </ref> rather than classes. Every object is self-describing and can be changed independently. In addition to the exibility of this approach, prototype-based systems can also avoid the complexity introduced by metaclasses. SELF has multiple inheritance. The inheritance design underwent several changes over the years. <p> SELF has multiple inheritance. The inheritance design underwent several changes over the years. SELF-87 only had single inheritance, but SELF-90 introduced prioritized multiple inheritance combined with a new privacy mechanism for better encapsulation [25, 134] and a sender path tiebreaker rule for disambiguating between equal-priority parents <ref> [115] </ref>. More recently, the pendulum has swung back towards simplicity: SELF-92 [115] eliminated the sender path tiebreaker because it tended to hide ambiguities, and SELF-93 eliminated prioritized inheritance and privacy from the language. All control structures are user-defined. For example, SELF has no if statement. <p> SELF-87 only had single inheritance, but SELF-90 introduced prioritized multiple inheritance combined with a new privacy mechanism for better encapsulation [25, 134] and a sender path tiebreaker rule for disambiguating between equal-priority parents <ref> [115] </ref>. More recently, the pendulum has swung back towards simplicity: SELF-92 [115] eliminated the sender path tiebreaker because it tended to hide ambiguities, and SELF-93 eliminated prioritized inheritance and privacy from the language. All control structures are user-defined. For example, SELF has no if statement. Instead, control structures are implemented with message sends and blocks (closures), just like in Smalltalk-80. <p> Finally, the virtual machine also contains numerous primitives that can be invoked by SELF programs to perform arithmetic, I/O, graphics, and so on. New primitives can be dynamically linked into the system at runtime. References [88], <ref> [115] </ref>, and [21] contain further details about the system. 2.2.2 Efficiency Since SELFs pure semantics threatened to make programs extremely inefficient, much of the early implementation effort went into compiler techniques for optimizing SELF programs. Some of these techniques were very successful: Dynamic compilation.
Reference: [116] <author> Robert A. Shaw. </author> <title> Empirical Analysis of a LISP System. </title> <institution> Stanford University, Computer Systems Laboratory, </institution> <type> Technical Report CSL-TR-88-351, </type> <year> 1988. </year>
Reference-contexts: However, recent designs attempt to reduce trap overhead; for example, the SPARC V9 architecture [119] allows low-overhead user-mode traps. Similar but less efficient schemes have been proposed by Sobalvarro [119] and Shaw <ref> [116] </ref>. 63 st [%obj + offset], %ptr store ptr into objects field add %obj, offset, %temp calculate address of updated word sll %temp, k, %temp divide by card size 2 k (shift left) st %g0, [%byte_map + %temp] clear byte in byte map This code sequence assumes that the register byte_map
Reference: [117] <author> Patrick G. Sobalvarro. </author> <title> A lifetime-based collector for LISP systems on general-purpose computers. B.S. </title> <type> Thesis, </type> <institution> EECS Dept., Massachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference: [118] <author> SPARC International. </author> <title> The SPARC Architecture Manual (Version 8). </title> <publisher> Prentice Hall, </publisher> <address> NJ, </address> <year> 1992. </year>
Reference-contexts: Unoptimized code calls many primitives, even for simple operations like integer arithmetic and comparisons. Together, all the primitives (including block allocation and GC) use about 20-30% of the total execution time. 4.3.4 Register windows The SPARC architecture <ref> [118] </ref> defines a set of overlapping register windows which allows a procedure to save the callers state by switching to a new set of registers. <p> our data, any claims regarding the necessity of architectural support for an object-oriented language should be regarded with caution unless the system used for the study employs state-of-the-art optimization techniques, or unless such optimizations have been shown to be ineffective for that particular language. 8.2 Register windows The SPARC architecture <ref> [118] </ref> defines a set of overlapping register windows which allows a procedure to save the callers state by switching to a new set of registers.
Reference: [119] <author> SPARC International. </author> <title> The SPARC Architecture Manual (Version 9). </title> <publisher> Prentice Hall, </publisher> <address> NJ, </address> <year> 1993. </year>
Reference-contexts: In SELF-91, a store check involves just 3 SPARC instructions in addition to the actual store: In fact, traps seem to become slower as hardware becomes faster [100]. However, recent designs attempt to reduce trap overhead; for example, the SPARC V9 architecture <ref> [119] </ref> allows low-overhead user-mode traps. Similar but less efficient schemes have been proposed by Sobalvarro [119] and Shaw [116]. 63 st [%obj + offset], %ptr store ptr into objects field add %obj, offset, %temp calculate address of updated word sll %temp, k, %temp divide by card size 2 k (shift left) <p> However, recent designs attempt to reduce trap overhead; for example, the SPARC V9 architecture <ref> [119] </ref> allows low-overhead user-mode traps. Similar but less efficient schemes have been proposed by Sobalvarro [119] and Shaw [116]. 63 st [%obj + offset], %ptr store ptr into objects field add %obj, offset, %temp calculate address of updated word sll %temp, k, %temp divide by card size 2 k (shift left) st %g0, [%byte_map + %temp] clear byte in byte map This code sequence assumes that <p> For example, the trap handler has to make sure that the loads and stores are executed with the memory access permissions of the user process, not of the kernel. The SPARC V9 architecture <ref> [119] </ref> corrects this aw so that window trap handlers can execute in user mode. This modification reduces the window handling overhead by more than a factor of 10 (!) to 19 instructions per trap [47]. With this change, the register window handling overhead should become very small.
Reference: [120] <author> Richard Stallman. </author> <title> The GNU C compiler. Free Software Foundation, </title> <booktitle> 1991. </booktitle> <pages> 162 </pages>
Reference-contexts: Compilers for more conventional languages (e.g., GNU C <ref> [120] </ref>) have used the size of intermediate code to guide inlining decisions, making the decision before or after optimizing the intermediate code [109].
Reference: [121] <author> Peter Steenkiste and John Hennessy. </author> <title> Tags and type checking in LISP: Hardware and Software Approaches. </title> <booktitle> In ASPLOS II Conference Proceedings, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: Thus, on another RISC architecture, it is likely that a SOAR-like system would be slowed down more than SELF. SELF-93-nofeedback SELF-93 84 11% to 24% of execution time being spent in tag handling for Lisp on the MIPS-X <ref> [121] </ref>, and Taylor reported that 12% to 35% of all instructions involved tag checks on the SPUR machine (which was a tagged architecture) [126].
Reference: [122] <author> Markus Strssler. </author> <title> Implementierung von Oberon-SELF. </title> <type> Diploma Thesis, </type> <institution> Institute for Computer Systems, ETH Zrich, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: The interpreter could dynamically translate source methods into interpreted methods (imethods) and then interpret the imethods. The main advantage of this approach is that the imethod format could be tuned for efficient interpretation. At least one SELF interpreter has been implemented elsewhere <ref> [122] </ref>, and its author reports execution times that are about 500 times slower than Oberon-2 code on some small integer benchmarks. However, the interpreter was not especially optimized. 32 For example, it could have inline caches, which could significantly speed up sends.
Reference: [123] <author> Sun Microsystems. </author> <title> The Viking Microprocessor (T.I. TMS S390Z50) User Documentation. Part No. </title> <address> 800-4510-02, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Furthermore, existing superscalar SPARC implementations cannot execute the tagged instructions in parallel with other instructions <ref> [123] </ref> because they can trap, and so one tagged instruction consumes as much time as several other instructions (up to three for SuperSPARC [123]). <p> Furthermore, existing superscalar SPARC implementations cannot execute the tagged instructions in parallel with other instructions <ref> [123] </ref> because they can trap, and so one tagged instruction consumes as much time as several other instructions (up to three for SuperSPARC [123]). We assumed that the compiler has no type information on the arguments of integer additions or subtractions, and thus has to explicitly test both tags for all integer operations.
Reference: [124] <author> Norihisa Suzuki. </author> <title> Inferring Types in Smalltalk. </title> <booktitle> In Proceedings of the 8th Symposium on the Principles of Programming Languages, </booktitle> <year> 1981. </year>
Reference: [125] <author> Norihisa Suzuki and Minoru Terada. </author> <title> Creating Efficient Systems for Object-Oriented Languages. </title> <booktitle> In Proceedings of the 11th Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, </address> <month> January, </month> <year> 1984. </year>
Reference: [126] <author> G. Taylor, P. Hilfinger, J. Larus, D. Patterson, and B. Zorn. </author> <title> Evaluation of the SPUR Lisp Architecture. </title> <booktitle> Proceedings of the 13th Symposium on Computer Architecture, p. </booktitle> <pages> 444-452, </pages> <address> Tokyo, </address> <year> 1986. </year>
Reference-contexts: SELF-93-nofeedback SELF-93 84 11% to 24% of execution time being spent in tag handling for Lisp on the MIPS-X [121], and Taylor reported that 12% to 35% of all instructions involved tag checks on the SPUR machine (which was a tagged architecture) <ref> [126] </ref>. In conclusion, type feedback reduces the number of type tests, the amount of work (path length) per type test, and the total execution time spend in type tests.
Reference: [127] <author> Andrew P. Tolmach and Andrew W. Appel. </author> <title> Debugging Standard ML Without Reverse Engineering. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Nice, France, </address> <month> June </month> <year> 1990, </year> <pages> p. 1-12. </pages>
Reference-contexts: As far as we know, most of the support for optimized code in LOIPE was not actually implemented. Tolmach and Appel <ref> [127] </ref> describe a debugger for ML where the compiler always performs optimizations, but where the program is automatically annotated with debugging statements before compilation. To debug an optimized program, the programmer has to manually recompile and re-execute the program.
Reference: [128] <author> David Ungar and David Patterson. </author> <title> Berkeley Smalltalk: Who Knows Where the Time Goes? In [86]. </title>
Reference-contexts: Along the branch where the type test succeeds, the compiler has precise information about the type of the receiver and can statically bind and inline a copy of the message. For example, existing SELF and Smalltalk systems predict that + will be sent to an integer <ref> [128, 58, 44] </ref>, since measurements indicate that this occurs 90% of the time [130]. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. <p> Lookup caches are very effective in reducing the lookup overhead. Berkeley Smalltalk, for example, would have been 37% slower without a lookup cache <ref> [128] </ref>. 3.2 Dispatch tables Statically-typed languages often implement message lookup with dispatch tables. A common variant is to use the message name (encoded as an integer) to index into a type-specific dispatch table which contains the address of the target function. <p> Many systems (e.g., most Smalltalk-80 and SELF implementations) statically predict the type of certain messages. For example, the receiver of ifTrue: is predicted to be either the true or false object, and the receiver of + is predicted to be an integer <ref> [128, 58, 44, 130] </ref>. Static type prediction has obvious performance advantages, but it contributes to unstable performance. First, the specialized messages execute much faster than semantically equivalent code using non-specialized selectors.
Reference: [129] <author> D. Ungar, R. Blau, P. Foley, D. Samples, and D. Patterson. </author> <title> Architecture of SOAR: Smalltalk on a RISC. </title> <booktitle> In Eleventh Annual International Symposium on Computer Architecture, </booktitle> <address> Ann Arbor, MI, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The optimizing SELF compiler extends static type prediction by dynamically predicting receiver types based on type feedback information (Chapter 5). 2.5.4.2 SOAR The goal of the SOAR (Smalltalk On A RISC) project was to speed up Smalltalk through a combination of hardware and software <ref> [132, 129] </ref>. On the software side, SOAR used a non-dynamic native-code compiler (i.e., all methods were compiled to native code immediately after being parsed in), inline caching, type prediction, and a generation scavenging garbage collector [131].
Reference: [130] <author> David Ungar and David Patterson. </author> <title> What Price Smalltalk? In IEEE Computer 20(1), </title> <month> January </month> <year> 1987. </year>
Reference-contexts: For example, existing SELF and Smalltalk systems predict that + will be sent to an integer [128, 58, 44], since measurements indicate that this occurs 90% of the time <ref> [130] </ref>. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. <p> For example, several studies have shown that in Smalltalk code, the receiver type at a given call site remains constant 95% of the time <ref> [44, 130, 132] </ref>. This locality of type usage can be exploited by caching the looked-up method address at the call site. <p> Although the SELF implementation with its optimizing compiler is quite different from Smalltalk systems, the miss ratios agree well with those observed in previous studies of Smalltalk systems, which observed miss ratios on the order of 5% <ref> [44, 130, 132] </ref>. The miss ratios do not directly correlate to the speedups observed when introducing PICs because the benchmarks have very different call frequencies (differing by more than a factor of five). <p> Many systems (e.g., most Smalltalk-80 and SELF implementations) statically predict the type of certain messages. For example, the receiver of ifTrue: is predicted to be either the true or false object, and the receiver of + is predicted to be an integer <ref> [128, 58, 44, 130] </ref>. Static type prediction has obvious performance advantages, but it contributes to unstable performance. First, the specialized messages execute much faster than semantically equivalent code using non-specialized selectors.
Reference: [131] <author> David Ungar. </author> <title> Generation Scavenging: A Non-Disruptive High-Performance Storage Reclamation Algorithm. </title> <booktitle> SIGPLAN Symposium on Practical Software Development Environments, p. </booktitle> <pages> 157-167, </pages> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1984. </year>
Reference-contexts: Objects are stored in the heap; a generation scavenging garbage collector <ref> [131] </ref> reclaims unused objects. Object references are tagged with a two-bit tag in the lower two bits of every 32-bit word (tag 00 is for integers, 01 for pointers, 10 for oats and 11 for object headers). <p> On the software side, SOAR used a non-dynamic native-code compiler (i.e., all methods were compiled to native code immediately after being parsed in), inline caching, type prediction, and a generation scavenging garbage collector <ref> [131] </ref>. Like the Deutsch-Schiffman compiler, the SOAR compiler did not perform extensive global optimizations or method inlining. On the hardware side, SOAR was a variation of the Berkeley RISC II processor [98]; the most important hardware features were register windows and tagged integer instructions. <p> when speculation fails, since it can recompile a method (reverting to non-speculative tests) should speculation fail frequently. 6.3.2 Store checks Generational garbage collectors need to keep track of references from older to younger generations so that younger generations can be garbage-collected without inspecting every object in the older generation (s) <ref> [89, 131] </ref>. The set of locations potentially containing pointers to newer objects is often called the remembered set [132]. At every store, the system must ensure that the updated location is added to the remembered set if the store creates a reference from an older to a newer object. <p> Cache parameters of current workstations 101 <ref> [131] </ref>. After every scavenge, this creation space is empty.
Reference: [132] <author> David Ungar. </author> <title> The Design and Evaluation of a High-Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Type Prediction. Certain messages are almost exclusively sent to particular receiver types. For such messages, the compiler uses an optimization originally introduced by early Smalltalk systems <ref> [44, 132] </ref>: it predicts the type of the source method for finding the minimum of two numbers compiled method specialized for integer receivers compiled method specialized for oating-point receivers customization 6 receiver based on the message name and inserts a runtime type test before the message send to test for the <p> The optimizing SELF compiler extends static type prediction by dynamically predicting receiver types based on type feedback information (Chapter 5). 2.5.4.2 SOAR The goal of the SOAR (Smalltalk On A RISC) project was to speed up Smalltalk through a combination of hardware and software <ref> [132, 129] </ref>. On the software side, SOAR used a non-dynamic native-code compiler (i.e., all methods were compiled to native code immediately after being parsed in), inline caching, type prediction, and a generation scavenging garbage collector [131]. <p> For example, several studies have shown that in Smalltalk code, the receiver type at a given call site remains constant 95% of the time <ref> [44, 130, 132] </ref>. This locality of type usage can be exploited by caching the looked-up method address at the call site. <p> This locality of type usage can be exploited by caching the looked-up method address at the call site. Because the lookup result is cached in line at every call site (i.e., in the case of a hit no separate lookup cache is accessed), the technique is called inline caching <ref> [44, 132] </ref>. contains a call to the systems lookup routine. The first time this call is executed, the lookup routine finds the target method. But before branching to the target, the lookup routine changes the call instruction to point to the target method just found (Figure 3-2). <p> Inline caching is very effective in reducing lookup overhead because the hit rates are high and the hit cost is low. For example, SOAR (a Smalltalk implementation for a RISC processor) would have been 33% slower without inline caching <ref> [132] </ref>. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does the SELF system. <p> Although the SELF implementation with its optimizing compiler is quite different from Smalltalk systems, the miss ratios agree well with those observed in previous studies of Smalltalk systems, which observed miss ratios on the order of 5% <ref> [44, 130, 132] </ref>. The miss ratios do not directly correlate to the speedups observed when introducing PICs because the benchmarks have very different call frequencies (differing by more than a factor of five). <p> The set of locations potentially containing pointers to newer objects is often called the remembered set <ref> [132] </ref>. At every store, the system must ensure that the updated location is added to the remembered set if the store creates a reference from an older to a newer object. The maintenance of this invariant is usually referred to as write barrier or store check. <p> In comparison, SOAR, a Smalltalk implementation on a RISC processor with special hardware support, spent 23% of its time in method dispatch and would have spent another 24% for integer tag checking had it not included special hardware <ref> [132] </ref>. In Lisp systems, tag checking is the closest equivalent to type tests since Lisp does not have dynamic dispatch. <p> For example, Ungar reported that the SOAR system would have been 46% slower without register windows and 26% slower without instructions for tagged arithmetic <ref> [132] </ref>. Williams and Wolczko argue that software-controlled caching improved the performance and locality of object-oriented systems [142]. Xerox Dorado Smalltalk, for a long time the fastest Smalltalk implementation available, contained microcode support for large portions of the Smalltalk virtual machine [43]. <p> Many previous studies have found the execution characteristics of object-oriented languages to be very different from C and have argued the need for object-oriented architectures. For example, Smalltalk studies <ref> [86, 132] </ref> have shown calls to be much more frequent than in other languages. Even for a hybrid language like C++ (which has C at its core and thus shouldnt be too different), significant differences were found. <p> With 7 register windows, the median overhead for unoptimized SELF code is 140%. 94 For comparison, the SOAR architecture had 8 windows with 8 registers each; Ungar reports that only about 2.5% of all calls caused a register window overow <ref> [132] </ref>. For unoptimized SELF this ratio is 9.4%. The reason for the significant difference is most probably that SELF doesnt hardwire common control structures but Smalltalk-80 does. Thus, if statements or loops contribute to the call depth in SELF but not in Smalltalk. <p> In contrast, Ungar reports that SOAR would have been 46% slower without register windows <ref> [132] </ref>. This discrepancy stems from the non-optimizing compiler used in SOAR which did not perform any inlining, resulting in a higher call frequency. <p> SOAR Smalltalk would have been 26% slower without them <ref> [132] </ref>. But how useful are they for a system with an optimizing compiler such as SELF-93? To answer this question, we first need to look at the code generated for integer addition (or subtraction) in SELF-93. <p> Thus, it appears that removing the instructions for tagged addition and subtraction from the SPARC architecture would not significantly reduce SELF-93s performance. This result stands in marked contrast to Ungars measurements showing that SOAR would have been 26% slower without instructions for tagged arithmetic <ref> [132] </ref>. Why the big difference? By analyzing Ungars data, we could find several reasons for the higher estimate: Ungars data includes speedups from several tagged instructions (such as load and load class) that are not present on the SPARC or not needed in SELF-93.
Reference: [133] <author> David Ungar and Randall B. Smith. </author> <title> SELF: The Power of Simplicity. </title> <booktitle> In OOPSLA 87 Conference Proceedings, p. </booktitle> <pages> 227-241, </pages> <address> Orlando, FL, </address> <month> October </month> <year> 1987. </year> <note> Published as SIGPLAN Notices 22(12), </note> <month> December </month> <year> 1987. </year> <title> Also published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: This dissertation describes how to reduce the overhead of dynamic dispatch while preserving the responsiveness required for an interactive exploratory programming environment. Our research vehicle is the object-oriented language SELF <ref> [133] </ref>. SELFs pure semantics exacerbate the implementation problems faced by object-oriented languages: in SELF, every single operation (even assignment) involves late binding. Thus, the language is an ideal test case for optimizations that reduce the overhead of late binding. Equally importantly, SELF was designed for exploratory programming.
Reference: [134] <author> David Ungar, Craig Chambers, Bay-Wei Chang, and Urs Hlzle. </author> <title> Organizing Programs without Classes. Published in Lisp and Symbolic Computation 4(3), </title> <publisher> Kluwer Academic Publishers, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: SELF has multiple inheritance. The inheritance design underwent several changes over the years. SELF-87 only had single inheritance, but SELF-90 introduced prioritized multiple inheritance combined with a new privacy mechanism for better encapsulation <ref> [25, 134] </ref> and a sender path tiebreaker rule for disambiguating between equal-priority parents [115]. More recently, the pendulum has swung back towards simplicity: SELF-92 [115] eliminated the sender path tiebreaker because it tended to hide ambiguities, and SELF-93 eliminated prioritized inheritance and privacy from the language.
Reference: [135] <author> Jan Vitek and Nigel Horspool. </author> <title> Taming message passing: Efficient method lookup for dynamically-typed languages. </title> <booktitle> In ECOOP 94 Proceedings, p. </booktitle> <pages> 432-449, </pages> <booktitle> Springer Verlag Lecture Notes on Computer Science 821, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: However, it is possible to use dispatch tables in dynamically-typed languages as well, albeit with some added complications <ref> [7, 50, 135] </ref>.
Reference: [136] <author> David W. Wall. </author> <title> Global Register Allocation at Link Time. </title> <booktitle> In SIGPLAN 86 Symposium on Compiler Construction, p. 264-275.Published as SIGPLAN Notices 21 (7), </booktitle> <month> July </month> <year> 1986. </year>
Reference-contexts: Some modern compilers for conventional languages use runtime feedback in the from of execution profiling information to perform branch scheduling and to reduce cache conicts [95, 96]. Other systems use profile information to assist classic code optimizations [30], procedure inlining [29, 94], trace scheduling [53], and register allocation <ref> [136, 98] </ref>. Hansen describes an adaptive compiler for Fortran [63]. His compiler optimized the inner loops of Fortran programs at runtime. The main goal of his work was to minimize the total cost of running a program which presumably was executed only once.
Reference: [137] <author> David W. Wall. </author> <title> Predicting Program Behavior Using Real or Estimated Profiles. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, p. </booktitle> <pages> 59-70. </pages> <note> Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1991. </year>
Reference-contexts: Of course, even with these precautions, predicting future receiver types based on past receiver types is nothing more than an educated guess. Similar guesses are made by optimizing compilers that base decisions on execution profiles taken from previous runs <ref> [137] </ref>. However, it is likely that a programs type profile is more stable than its time profile [57]usually, no new types are created at runtime. <p> In summary, the SELF-93 compiler brings SELFs performance to a level that is competitive with other object-oriented languages, without compromising SELFs pure semantics. 7.3.2 Different inputs A possible objection to the above results is that the performance of SELF-93 was measured using the same inputs on which it was trained <ref> [137] </ref>. Intuitively, this should have a smaller impact for SELF than it does for a conventional system. First, the main benefit of feedback is type information; that is, the programs type profile is more important than its time profile.
Reference: [138] <author> Paul R. Wilson and Thomas G. Mohler. </author> <title> Design of the Opportunistic Garbage Collector. </title> <booktitle> In OOPSLA 89 Conference Proceedings, p. </booktitle> <pages> 23-35, </pages> <address> New Orleans, LA, </address> <month> October, </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October, </month> <year> 1989. </year>
Reference-contexts: Thus, if the system decides that a certain method should be optimized, the actual optimizing compilation could be deferred if desired. For example, the system could enter the optimization requests into a queue and process them during the users think pauses (similar to opportunistic garbage collection <ref> [138] </ref>). Alternatively, optimizing compilations could be performed in parallel with program execution on a multiprocessor machine. With the increasing speed of hardware, interpreters or nonoptimizing dynamic compilers (as used in current Smalltalk systems) may no longer represent the optimal compromise between performance and responsiveness.
Reference: [139] <author> Paul R Wilson and Thomas G Moher. </author> <title> A card-marking scheme for controlling intergenerational references in generation-based GC on stock hardware. </title> <journal> SIGPLAN Notices 24 </journal> (5):87-92. 
Reference-contexts: However, in the general case a store check must be executed for every store operation. Since stores are frequent, an efficient write barrier implementation is essential. SELF-91s write barrier implementation is based on Wilsons card marking scheme <ref> [139] </ref>. In this scheme, the heap is divided into cards of size 2 k words (typically, k = 5..7), and every card has an associated byte in a separate vector. A store check simply clears the byte corresponding to the location being updated.
Reference: [140] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching Considerations for Generational GC: a Case for Large and Set-Associative Caches. </title> <institution> University of Illinois at Chicago Technical Report UI-EECS-90-5, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Without subblock placement, the old contents of memory would be read unnecessarily, only to be overwritten by the initializing stores. Wilson et al. <ref> [140] </ref> argue that this allocation behavior will result in especially high cache overheads for direct-mapped caches since the allocation pointer will sweep through the entire cache and evict every single cache line. Therefore, caches should be associative, and the size of creation space should be smaller than the cache size.
Reference: [141] <author> Mario Wolczko. </author> <title> Private communication, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: See Table A-1 in Appendix A for detailed data. Ifor Williams obtained similar results when analyzing Smalltalk-80 traces <ref> [141] </ref>.
Reference: [142] <author> Ifor Williams and Mario Wolczko. </author> <title> An Object-Based Memory Architecture. </title> <booktitle> In Proc. 4th Intl. Workshop on persistent object systems, </booktitle> <address> Martha's Vineyard, MA, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: For example, Ungar reported that the SOAR system would have been 46% slower without register windows and 26% slower without instructions for tagged arithmetic [132]. Williams and Wolczko argue that software-controlled caching improved the performance and locality of object-oriented systems <ref> [142] </ref>. Xerox Dorado Smalltalk, for a long time the fastest Smalltalk implementation available, contained microcode support for large portions of the Smalltalk virtual machine [43].
Reference: [143] <author> Polle T. Zellweger. </author> <title> An Interactive High-Level Debugger for Control-Flow Optimized Programs. </title> <note> Xerox PARC Technical Report CSL-83-1, January 1983. 163 </note>
Reference-contexts: Most existing systems do not support the debugging of optimized code. Programs can either be optimized for full speed, or they can be compiled without optimizations for full source-level debugging. Recently, techniques have been developed that strive to make it possible to debug optimized code <ref> [65, 143, 37] </ref>. However, none of these systems is able to provide full source-level debugging. For example, it generally is not possible to obtain the values of all source-level variables, to single-step through the program, or to change the value of a variable. <p> Unfortunately, if the source-level state of the program must be recoverable at every point in the program, i.e., at virtually every instruction boundary (to support single-stepping), existing techniques <ref> [2, 3, 65, 143, 37] </ref> severely restrict the optimizations that could be performed, effectively disabling many common optimizations. We solve this dilemma by relaxing the debugging requirements placed on optimized code. <p> Similarly, some other optimizations (e.g., code motion or induction variable elimination) may sometimes not be performed if the debuggers recovery techniques are not powerful enough to hide the optimizations effects at one or more interrupt points. The extent of this problems depends on the particular recovery techniques used (e.g., <ref> [2, 3, 65, 143, 37] </ref>) and on the average distance between interrupt points. All these optimizations can be performed, however, if there is no interrupt point within the affected variables scope (see Section 10.2.4). <p> Adl-Tabatabai et al. [2, 3] investigate the problem of detecting and recovering the values of source-level variables in the presence of instruction scheduling. Other recovery mechanisms are described by Coutant et al. [37] and by Schlaeppi and Warren [114]. Zellweger <ref> [143, 144] </ref> describes an interactive source-level debugger for Cedar which handles two optimizations, procedure inlining and cross-jumping, to provide expected behavior in most cases.
Reference: [144] <author> Polle T. Zellweger. </author> <title> Interactive Source-Level Debugging of Optimized Programs. </title> <type> Ph.D. dissertation, </type> <institution> Computer Science Department, University of California, Berkeley, </institution> <year> 1984. </year> <note> Also published as Xerox PARC Technical Report CSL-84-5, </note> <month> May </month> <year> 1984. </year>
Reference-contexts: Merely providing correct execution semantics despite optimization is not enough: for optimal programmer productivity, the system should provide the illusion of directly executing the programmers source code. In other words, the system should provide interpreter semantics at compiled-code speed, combining global optimization with expected behavior <ref> [144] </ref>. Most existing systems do not support the debugging of optimized code. Programs can either be optimized for full speed, or they can be compiled without optimizations for full source-level debugging. Recently, techniques have been developed that strive to make it possible to debug optimized code [65, 143, 37]. <p> Interrupt points have been used in other systems before; see Section 10.7 for a discussion of the Deutsch-Schiffman Smalltalk-80 system. Also, Hennessy [65] used a similar mechanism to define stopping points between each statement, as did Zellweger <ref> [144] </ref>. real stack frame f (includes virtual activations vf 1 , vf 2 , and vf 3 ) stack grows downwards g returns vf 2 real stack frame g deoptimization vf 1 vf 2 vf 3 every virtual activation that was contained in f now has its own real stack frame <p> In contrast, neither single-step nor finish could generally be provided by previous systems for debugging optimized code <ref> [144, 37] </ref>. 132 10.4.1 Single-step Because every source point has an interrupt point associated with it in a deoptimized method, the implementation of single-stepping becomes trivial. The system deoptimizes the current activation and restarts the process with the interrupt ag already set. <p> Adl-Tabatabai et al. [2, 3] investigate the problem of detecting and recovering the values of source-level variables in the presence of instruction scheduling. Other recovery mechanisms are described by Coutant et al. [37] and by Schlaeppi and Warren [114]. Zellweger <ref> [143, 144] </ref> describes an interactive source-level debugger for Cedar which handles two optimizations, procedure inlining and cross-jumping, to provide expected behavior in most cases.
Reference: [145] <author> Benjamin Zorn. </author> <title> Barrier Methods for Garbage Collection. </title> <type> Technical Report CU-CS-494-90, </type> <institution> University of Colo-rado at Boulder, </institution> <month> November </month> <year> 1990. </year>
Reference: [146] <author> Lawrence W. Zurawski and Ralph E. Johnson. </author> <title> Debugging Optimized Code With Expected Behavior. </title> <type> Unpublished manuscript, </type> <year> 1992. </year>
Reference-contexts: As in our system, interrupts were delayed until the next call or backward branch. Since the compiler performed no global optimizations, the system could provide expected behavior without deoptimization. Zurawski and Johnson <ref> [146] </ref> describe a model for a debugger (developed concurrently with this work) which closely resembles ours, using inspection points and dynamic deoptimization to provide expected behavior for optimized code. However, the system does not use lazy conversion.
References-found: 146

