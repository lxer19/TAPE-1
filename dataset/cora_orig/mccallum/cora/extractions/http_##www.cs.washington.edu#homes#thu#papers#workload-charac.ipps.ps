URL: http://www.cs.washington.edu/homes/thu/papers/workload-charac.ipps.ps
Refering-URL: http://www.cs.washington.edu/homes/thu/papers/pps.abstract.html
Root-URL: 
Title: Parallel Application Characterization for Multiprocessor Scheduling Policy Design spectrum between aggressively dynamic and static allocation
Author: Thu D. Nguyen, Raj Vaswani, and John Zahorjan 
Note: In the  
Address: Box 352350  Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  
Abstract: Much of the recent work on multiprocessor scheduling disciplines has used abstract workload models to explore the fundamental, high-level properties of the various alternatives. As continuing work on these policies increases their level of sophistication, however, it is clear that the choice of appropriate policies must be guided at least in part by the typical behavior of actual parallel applications. Our goal in this paper is to examine a variety of such applications, providing measurements of properties relevant to scheduling policy design. We give measurements for both hand-coded parallel programs (from the SPLASH benchmark suites) and compiler-parallelized programs (from the PERFECT Club suite) running on a KSR-2 shared-memory multiprocessor. The measurements we present are intended primarily to address two aspects of multiprocessor scheduling policy design: We address these questions through three sets of measurements: First, we examine application speedup, and the sources of speedup loss. Our results confirm that there is considerable variation in job speedup, and that the bulk of the speedup loss is due to communication and idleness. Next, we examine runtime measurement of speedup information. We begin by looking at how such information might be acquired accurately and at acceptable cost. We then investigate the extent to which recent measurements of speedup accurately predict the future, and so the extent to which such measurements might reasonably be expected to guide allocation decisions. Finally, we examine the durations of individual processor idle periods, and relate these to the cost of reallocating a processor at those times. These results shed light on the potential for aggressively dynamic policies to improve performance. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Agarwal and A. Gupta. </author> <title> Memory-Reference Characteristics of Multiprocessor Applications under MACH. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: However, because they were attempting to address architectural issues, they concentrate on different measures (e.g., memory and I/O requirements) than those presented here. Similarly, many other researchers (e.g., <ref> [9, 1, 27] </ref>) report results from studies of application memory behavior. 3 The Experimental Environment 3.1 Hardware and Software Platform All measurements were done on a Kendall Square Research KSR-2 COMA shared-memory multiprocessor. Our machine consists of 60 40-MHz dual-issue proprietary processors, partitioned into two clusters of 30.
Reference: 2. <author> I. Ashok and J. Zahorjan. </author> <title> Scheduling a Mixed Interactive and Batch Workload on a Parallel, Shared Memory Supercomputer. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 616-625, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: While current proposals for space-sharing systems that address support of interactive work typically partition available processors into two pools, one to run interactive jobs and one to run parallel jobs <ref> [34, 2] </ref>, taking advantage of processors idled by parallel jobs could reduce the size of the partition dedicated to interactive work.
Reference: 3. <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Scharzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The PERFECT Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: We use programs from the SPLASH and SPLASH-2 benchmark suites [30, 37] to represent hand-coded parallel applications, and programs from the PERFECT Club benchmark suite <ref> [3] </ref> and an industrial fluid dynamics program (obtained from Analytical Methods, Inc.) to represent compiler-parallelized sequential applications. The remainder of the paper is organized as follows. Section 2 discusses related work. Section 3 documents our experimental platform. <p> Section 7 gives our conclusions. 2 Related Work The PERFECT Club benchmark suite is one of several standard benchmark suites used to measure the capability of parallelizing compilers <ref> [3] </ref>. As such, many studies have characterized the behavior of a number of these PERFECT Club programs, e.g., [7, 11, 26].
Reference: 4. <author> J. Chen, Y. Endo, K. Chan, D. Mazieres, A. Dias, M. Seltzer, and M. Smith. </author> <title> The Measured Performance of Personal Computer Operating Systems. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating system Principles, </booktitle> <pages> pages 299-313, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Note that while we have relied on the specific hardware counters on the KSR-2 processor, many modern processors include similar functionality. For example, both the DEC Alpha and the Intel Pentium processors contain counters for various sorts of cache misses, which could be translated into estimates of communication cost <ref> [31, 4] </ref>. 5.2 Using Speedup Measurements to Predict Future Behavior For runtime speedup measurements to be of practical use to schedulers, application speedups must be predictable.
Reference: 5. <author> S.-H. Chiang, R. K. Mansharamani, and M. K. Vernon. </author> <title> Use of Application Characteristics and Limited Preemption for Run-To-Completion Parallel Processor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 33-44, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Because such information has not been widely This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center. available, many scheduling studies have been performed using analytic or synthetic workload models <ref> [19, 17, 29, 5] </ref>. While such artificial workloads are a valuable tool, the increasing sophistication of the policies being studied requires a corresponding increase in the sophistication of the workload models.
Reference: 6. <author> E. C. Cooper and R. P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: This information is made available to the system and user jobs through a set of read-only registers. The KSR-2 runs a variant of the OSF/1 UNIX operating system. We use CThreads <ref> [6] </ref>, an efficient user-level threads package, as the vehicle of parallelism. We instrumented CThreads using the event monitors to collect the data presented in the remainder of this paper. SPLASH and SPLASH-2 programs run directly on CThreads.
Reference: 7. <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Supercomputer Performance Evaluation and the Perfect Benchmarks. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, ACM SIGARCH Computer Architecture News, </booktitle> <pages> pages 254-266, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Section 7 gives our conclusions. 2 Related Work The PERFECT Club benchmark suite is one of several standard benchmark suites used to measure the capability of parallelizing compilers [3]. As such, many studies have characterized the behavior of a number of these PERFECT Club programs, e.g., <ref> [7, 11, 26] </ref>. However, these studies have typically focused on properties of the code that affect a compiler's ability to parallelize them, and not, as we do, on properties of their execution that affect a scheduler's ability to best schedule parallelized versions of the programs.
Reference: 8. <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cypher et al. <ref> [8] </ref> report measurements for a number of applications running on message-passing multiprocessor systems in a manner similar to ours. However, because they were attempting to address architectural issues, they concentrate on different measures (e.g., memory and I/O requirements) than those presented here.
Reference: 9. <author> F. Darema-Rogers, G. Pfister, and K. </author> <title> So. Memory Access Patterns of Parallel Scientific Programs. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 46-58, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: However, because they were attempting to address architectural issues, they concentrate on different measures (e.g., memory and I/O requirements) than those presented here. Similarly, many other researchers (e.g., <ref> [9, 1, 27] </ref>) report results from studies of application memory behavior. 3 The Experimental Environment 3.1 Hardware and Software Platform All measurements were done on a Kendall Square Research KSR-2 COMA shared-memory multiprocessor. Our machine consists of 60 40-MHz dual-issue proprietary processors, partitioned into two clusters of 30.
Reference: 10. <author> J. J. Dongarra and T. Dunigan. </author> <title> Message-Passing Performance of Various Computers. </title> <type> Technical Report CS-95-299, </type> <institution> University of Tennessee, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: We refer the reader to [24] for an expanded version of this paper containing more comprehensive data. 1 Note, however, that Dongarra and Dunigan have measured a peak bandwidth of only 8 MB/s on a KSR-1, which has 20-MHz processors connected by the same network as in the KSR-2 <ref> [10] </ref>. 2 All applications were measured while running default data sets that came with the benchmark suites, except that the number of iterations for QCD were reduced from 100 to 2 to shorten execution times in our experiments. Application Exec. time (secs) Description Barnes y 1159.14 Barnes-Hut N-body simulation.
Reference: 11. <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Parallelization of Four Perfect-Benchmark Programs. </title> <type> Technical Report 1193, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Section 7 gives our conclusions. 2 Related Work The PERFECT Club benchmark suite is one of several standard benchmark suites used to measure the capability of parallelizing compilers [3]. As such, many studies have characterized the behavior of a number of these PERFECT Club programs, e.g., <ref> [7, 11, 26] </ref>. However, these studies have typically focused on properties of the code that affect a compiler's ability to parallelize them, and not, as we do, on properties of their execution that affect a scheduler's ability to best schedule parallelized versions of the programs.
Reference: 12. <author> D. G. Feitelson and B. Nitzberg. </author> <title> Job Characteristics of a Production Parallel Scientific Workload on the NASA Ames iPSC/860. </title> <booktitle> In Proceedings of the IPPS'95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 337-360, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Furthermore, we contrast the behaviors of these applications with those of compiler-parallelized applications, and consider the implications of their differences to the design of parallel processor scheduling policies. Feitelson and Nitzberg <ref> [12] </ref> report a variety of statistics on the parallel workloads of an iPSC/860 located at NASA Ames.
Reference: 13. <author> K. Guha. </author> <title> Using Parallel Program Characteristics in Dynamic Processor Allocation Policies. </title> <type> Technical Report CS-95-03, </type> <institution> Department of Computer Science, York University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: First, we examine application speedup, and the sources of speedup loss. Our results confirm that there is considerable variation among jobs, and provide information that will support work on the use of speedup information in making scheduling decisions <ref> [13, 23] </ref>. Second, because it is at least burdensome, and perhaps impossible, to accurately collect and supply such information at job submission time, we look at the problem of estimating job speedup at runtime.
Reference: 14. <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 120-133, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The cache penalty component of reallocation cost reflects the fact that dynamic movement of processors can adversely affect program cache behavior, and therefore performance. The importance of cache performance to modern processor speed has motivated the recent work on cache-affinity scheduling <ref> [33, 32, 14, 35] </ref>. To evaluate the cache related cost of dynamic reallocation, we look at the worst-case times on the KSR-2. Recall that each processor in the KSR-2 has two caches, a 256 KByte processor cache and a 32 MB attraction memory.
Reference: 15. <author> A. Karlin, K. Li, M. S. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Ousterhout [25], Lo and Gligor [18], and Karlin et al. <ref> [15] </ref> take this approach in the context of implementing locks for mutual exclusion, where such filtering is called two-phase blocking or spin-then-block. McCann et. al. [20] have proposed a delayed reallocation scheme as part of a dynamic scheduling policy. Fig. 8.
Reference: 16. <institution> Kendall Square Research Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <note> KSR Fortran Programming, </note> <year> 1993. </year>
Reference-contexts: We use CThreads [6], an efficient user-level threads package, as the vehicle of parallelism. We instrumented CThreads using the event monitors to collect the data presented in the remainder of this paper. SPLASH and SPLASH-2 programs run directly on CThreads. We use both the KSR KAP <ref> [16] </ref> and Stanford SUIF compilers [36] to parallelize sequential programs. We use both systems because they represent different tradeoffs in technology and product maturity. KSR KAP is a commercial product that has been adapted specifically to the KSR architecture and optimized through productization.
Reference: 17. <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The Performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-236, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Because such information has not been widely This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center. available, many scheduling studies have been performed using analytic or synthetic workload models <ref> [19, 17, 29, 5] </ref>. While such artificial workloads are a valuable tool, the increasing sophistication of the policies being studied requires a corresponding increase in the sophistication of the workload models.
Reference: 18. <author> S.-P. Lo and V. Gligor. </author> <title> A Comparative Analysis of Multiprocessor Scheduling Algorithms. </title> <booktitle> In Proceedings of the 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 356-63, </pages> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: Ousterhout [25], Lo and Gligor <ref> [18] </ref>, and Karlin et al. [15] take this approach in the context of implementing locks for mutual exclusion, where such filtering is called two-phase blocking or spin-then-block. McCann et. al. [20] have proposed a delayed reallocation scheme as part of a dynamic scheduling policy. Fig. 8.
Reference: 19. <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Scheduling in Multiprogrammed Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Because such information has not been widely This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center. available, many scheduling studies have been performed using analytic or synthetic workload models <ref> [19, 17, 29, 5] </ref>. While such artificial workloads are a valuable tool, the increasing sophistication of the policies being studied requires a corresponding increase in the sophistication of the workload models.
Reference: 20. <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Policy for Mul-tiprogrammed Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, the KSR-2 processor reallocation mechanism was designed for ease of implementation, and uses a simple but very inefficient approach. In contrast, measurements of a Sequent Symmetry, an older shared-memory multiprocessor with much slower processors, indicate path length costs for context switching of about 750s <ref> [20] </ref>. Based on this somewhat conflicting information, it appears that context switch path length costs below 1ms are easily possible on modern multiprocessors. However, it is unlikely that designers of production systems will invest the effort to optimize context switching until it becomes clear that there is a tangible payoff. <p> Ousterhout [25], Lo and Gligor [18], and Karlin et al. [15] take this approach in the context of implementing locks for mutual exclusion, where such filtering is called two-phase blocking or spin-then-block. McCann et. al. <ref> [20] </ref> have proposed a delayed reallocation scheme as part of a dynamic scheduling policy. Fig. 8. Residual idleness as a percentage of processor time assuming a 10ms total reallocation cost. A natural question to ask is how much idleness exists whose duration exceeds the context switch delay time.
Reference: 21. <author> A. J. Musciano and T. L. Sterling. </author> <title> Efficient Dynamic Scheduling of Medium-Grained Tasks for General Purpose Parallel Processing. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 166-175, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: A second, longer term objective, is to work towards an accurate and efficient scheme for measuring speedup at runtime. It is well-known that loss of speedup in shared-memory systems arise from the following factors <ref> [21, 28] </ref>: 1. Idleness: at times, parallel programs must idle allocated processors because of insufficient parallelism or load imbalance. 2.
Reference: 22. <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Maximizing Speedup Through Self-Tuning of Processor Allocation. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <pages> pages 463-468, </pages> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: On the other hand, the average difference alone is not sufficient information: it understates the error because occasional very incorrect predictions might induce a scheduler to make unfortunate allocation choices that can degrade performance much more than proportional to the error in the predictions (see, for example <ref> [22] </ref>). At the other extreme, looking at the maximum single-prediction error probably overestimates error, since errors of that magnitude may be exceedingly rare. <p> Stated differently, we expect efficiency measurements to be most accurate in predicting future behavior when the measurement interval corresponds to the execution of some section of code that will be repeated. In related work <ref> [22, 23] </ref>, we have made use of this observation, exploiting a particularly simple (but also quite common) program structure: an outer sequential loop that drives the execution.
Reference: 23. <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Using Runtime Measured Workload Characteristics in Parallel Processor Scheduling. </title> <booktitle> In Proceedings of the IPPS'96 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: First, we examine application speedup, and the sources of speedup loss. Our results confirm that there is considerable variation among jobs, and provide information that will support work on the use of speedup information in making scheduling decisions <ref> [13, 23] </ref>. Second, because it is at least burdensome, and perhaps impossible, to accurately collect and supply such information at job submission time, we look at the problem of estimating job speedup at runtime. <p> Speedup is typically much worse for compiler-parallelized applications than for hand-coded applications. Most speedup curves are relatively smooth and roughly convex-shaped. This implies that speedup values for a relatively few allocations might allow reasonably accurate extrapolation to other allocations. (See <ref> [23] </ref> for an application of this idea to scheduling.) For most hand-coded applications, there is an allocation beyond which they slow down gradually. With the exception of ARC2D when parallelized by KAP, all compiler-parallelized jobs slow down significantly after achieving their peak speedups. <p> Stated differently, we expect efficiency measurements to be most accurate in predicting future behavior when the measurement interval corresponds to the execution of some section of code that will be repeated. In related work <ref> [22, 23] </ref>, we have made use of this observation, exploiting a particularly simple (but also quite common) program structure: an outer sequential loop that drives the execution.
Reference: 24. <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Parallel Application Characterization for Multiprocessor Scheduling Policy Design. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <note> In preparation. </note>
Reference-contexts: For reasons of space, in what follows, we show results for only a representative sample of our seventeen applications. We refer the reader to <ref> [24] </ref> for an expanded version of this paper containing more comprehensive data. 1 Note, however, that Dongarra and Dunigan have measured a peak bandwidth of only 8 MB/s on a KSR-1, which has 20-MHz processors connected by the same network as in the KSR-2 [10]. 2 All applications were measured while
Reference: 25. <author> J. K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Proceedings of 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: This set of results for compiler-parallelized applications were particularly surprising, suggesting that sequential portions typically run for only short periods of time. 6.3 Idle Period Time Distribution One approach to dealing with short idle periods is to filter them by waiting a short time before context switching. Ousterhout <ref> [25] </ref>, Lo and Gligor [18], and Karlin et al. [15] take this approach in the context of implementing locks for mutual exclusion, where such filtering is called two-phase blocking or spin-then-block. McCann et. al. [20] have proposed a delayed reallocation scheme as part of a dynamic scheduling policy. Fig. 8.
Reference: 26. <author> P. Petersen and D. Padua. </author> <title> Machine-Independent Evaluation of Parallelizing Compilers. </title> <type> Technical Report 1173, </type> <institution> Center for Supercomputing Research and Development, </institution> <year> 1992. </year>
Reference-contexts: Section 7 gives our conclusions. 2 Related Work The PERFECT Club benchmark suite is one of several standard benchmark suites used to measure the capability of parallelizing compilers [3]. As such, many studies have characterized the behavior of a number of these PERFECT Club programs, e.g., <ref> [7, 11, 26] </ref>. However, these studies have typically focused on properties of the code that affect a compiler's ability to parallelize them, and not, as we do, on properties of their execution that affect a scheduler's ability to best schedule parallelized versions of the programs.
Reference: 27. <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, because they were attempting to address architectural issues, they concentrate on different measures (e.g., memory and I/O requirements) than those presented here. Similarly, many other researchers (e.g., <ref> [9, 1, 27] </ref>) report results from studies of application memory behavior. 3 The Experimental Environment 3.1 Hardware and Software Platform All measurements were done on a Kendall Square Research KSR-2 COMA shared-memory multiprocessor. Our machine consists of 60 40-MHz dual-issue proprietary processors, partitioned into two clusters of 30.
Reference: 28. <author> K. C. Sevcik. </author> <title> Characterizations of Parallelism in Applications and their Use in Scheduling. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: A second, longer term objective, is to work towards an accurate and efficient scheme for measuring speedup at runtime. It is well-known that loss of speedup in shared-memory systems arise from the following factors <ref> [21, 28] </ref>: 1. Idleness: at times, parallel programs must idle allocated processors because of insufficient parallelism or load imbalance. 2.
Reference: 29. <author> K. C. Sevcik. </author> <title> Application Scheduling and Processor Allocation in Multiprogrammed Parallel Processing Systems. Performance Evaluation, </title> 19(2/3):107-140, Mar. 1994. 
Reference-contexts: Because such information has not been widely This work was supported in part by the National Science Foundation (Grants CCR-9123308 and CCR-9200832) and the Washington Technology Center. available, many scheduling studies have been performed using analytic or synthetic workload models <ref> [19, 17, 29, 5] </ref>. While such artificial workloads are a valuable tool, the increasing sophistication of the policies being studied requires a corresponding increase in the sophistication of the workload models.
Reference: 30. <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: We use programs from the SPLASH and SPLASH-2 benchmark suites <ref> [30, 37] </ref> to represent hand-coded parallel applications, and programs from the PERFECT Club benchmark suite [3] and an industrial fluid dynamics program (obtained from Analytical Methods, Inc.) to represent compiler-parallelized sequential applications. The remainder of the paper is organized as follows. Section 2 discusses related work. <p> For the SPLASH and SPLASH-2 benchmark suites, Singh et al. <ref> [30] </ref> and Woo et al. [37] provide significant information, including speedup, cache behavior, and synchronization wait time. Our measurements supplement their reports by quantifying sources of speedup loss, as well as more fine-grained application behaviors such as frequency and duration of idle periods.
Reference: 31. <author> R. L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: Note that while we have relied on the specific hardware counters on the KSR-2 processor, many modern processors include similar functionality. For example, both the DEC Alpha and the Intel Pentium processors contain counters for various sorts of cache misses, which could be translated into estimates of communication cost <ref> [31, 4] </ref>. 5.2 Using Speedup Measurements to Predict Future Behavior For runtime speedup measurements to be of practical use to schedulers, application speedups must be predictable.
Reference: 32. <author> M. Squillante and E. Lazowska. </author> <title> Using Processor-Cache Affinity Information in Shared-Memory Multiprocessor Scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The cache penalty component of reallocation cost reflects the fact that dynamic movement of processors can adversely affect program cache behavior, and therefore performance. The importance of cache performance to modern processor speed has motivated the recent work on cache-affinity scheduling <ref> [33, 32, 14, 35] </ref>. To evaluate the cache related cost of dynamic reallocation, we look at the worst-case times on the KSR-2. Recall that each processor in the KSR-2 has two caches, a 256 KByte processor cache and a 32 MB attraction memory.
Reference: 33. <author> D. Thiebaut and H. S. Stone. </author> <title> Footprints in the Cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: The cache penalty component of reallocation cost reflects the fact that dynamic movement of processors can adversely affect program cache behavior, and therefore performance. The importance of cache performance to modern processor speed has motivated the recent work on cache-affinity scheduling <ref> [33, 32, 14, 35] </ref>. To evaluate the cache related cost of dynamic reallocation, we look at the worst-case times on the KSR-2. Recall that each processor in the KSR-2 has two caches, a 256 KByte processor cache and a 32 MB attraction memory.
Reference: 34. <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: While current proposals for space-sharing systems that address support of interactive work typically partition available processors into two pools, one to run interactive jobs and one to run parallel jobs <ref> [34, 2] </ref>, taking advantage of processors idled by parallel jobs could reduce the size of the partition dedicated to interactive work.
Reference: 35. <author> R. Vaswani and J. Zahorjan. </author> <title> The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: The cache penalty component of reallocation cost reflects the fact that dynamic movement of processors can adversely affect program cache behavior, and therefore performance. The importance of cache performance to modern processor speed has motivated the recent work on cache-affinity scheduling <ref> [33, 32, 14, 35] </ref>. To evaluate the cache related cost of dynamic reallocation, we look at the worst-case times on the KSR-2. Recall that each processor in the KSR-2 has two caches, a 256 KByte processor cache and a 32 MB attraction memory.
Reference: 36. <author> R. P. Wilson, R. S. French, C. S. Wilson, S. P. Amarasinghe, J. M. Anderson, S. W. K. Tjiang, S.-W. Liao, C.-W. Tseng, M. W. Hall, M. S. Lam, and J. L. Hennessy. </author> <title> SUIF: An Infrastructure for Research on Parallelizing and Optimizing Comilers. </title> <type> Technical report, </type> <institution> Computer Systems Laboratory, Stanford Univeristy. </institution>
Reference-contexts: We instrumented CThreads using the event monitors to collect the data presented in the remainder of this paper. SPLASH and SPLASH-2 programs run directly on CThreads. We use both the KSR KAP [16] and Stanford SUIF compilers <ref> [36] </ref> to parallelize sequential programs. We use both systems because they represent different tradeoffs in technology and product maturity. KSR KAP is a commercial product that has been adapted specifically to the KSR architecture and optimized through productization. SUIF, on the other hand, is a research vehicle.
Reference: 37. <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, , and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: We use programs from the SPLASH and SPLASH-2 benchmark suites <ref> [30, 37] </ref> to represent hand-coded parallel applications, and programs from the PERFECT Club benchmark suite [3] and an industrial fluid dynamics program (obtained from Analytical Methods, Inc.) to represent compiler-parallelized sequential applications. The remainder of the paper is organized as follows. Section 2 discusses related work. <p> For the SPLASH and SPLASH-2 benchmark suites, Singh et al. [30] and Woo et al. <ref> [37] </ref> provide significant information, including speedup, cache behavior, and synchronization wait time. Our measurements supplement their reports by quantifying sources of speedup loss, as well as more fine-grained application behaviors such as frequency and duration of idle periods.
References-found: 37

