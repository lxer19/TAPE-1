URL: http://www.cs.huji.ac.il/papers/parallel-processing/maxi.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/parallel-processing/header.html
Root-URL: 
Email: E-mail contact: rudolph@cs.huji.ac.il  
Title: Run-Time Support for Parallel Language Constructs in a Tightly Coupled Multiprocessor language/system interface deals with
Author: Dror G. Feitelson Yosi Ben-Asher Moshe Ben Ezra Iaakov Exman Lior Picherski Larry Rudolph Dror Zernik 
Note: The  Current address: IBM T. J. Watson Research Center, P. O.  Current address:  Current address:  
Address: 91904 Jerusalem, Israel  Box 218, Yorktown Heights, NY 10598  31999 Haifa, Israel.  32000 Haifa, Israel.  
Affiliation: Institute of Computer Science The Hebrew University of Jerusalem  Department of Mathematics and Computer Science, University of Haifa,  Department of Electrical Engineering, The Technion,  
Abstract: This paper describes the run-time implementation of a parallel programming language. Unlike traditional designs, our system exploits both shared memory aspects and message passing features. It enjoys the benefits of both polling and interrupts, giving more weight to the former, i.e. processors do not interrupt each other unless absolutely necessary. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> "Scheduler activations: effective kernal support for the user-level management of parallelism". </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 10(1), </volume> <pages> pp. 53-79, </pages> <month> Feb </month> <year> 1992. </year>
Reference-contexts: The runtime system supporting the language constructs can be implemented either in the operating system kernel, or else it can be implemented as a user-level software package 4 above the kernel <ref> [1, 27] </ref>. We have chosen to build our system on top of a real-time operating system kernel. The runtime system should have more control over tasks and other system resources than is traditionally permitted in multi-user, interactive, time-sharing operating systems such as Unix.
Reference: [2] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy, </author> <title> "The performance implications of thread management alternatives for shared-memory multiprocessors". </title> <journal> IEEE Trans. Comput. </journal> <volume> 38(12), </volume> <pages> pp. 1631-1644, </pages> <month> Dec </month> <year> 1989. </year>
Reference-contexts: Thus state information such as the activity's stack is always local and does not have to be copied. Local queues also reduce contention for global locks, which could degrade performance <ref> [2] </ref>. At the application level, the fact that activities do not migrate also opens the possibility for associating data structures with activities, so that these data structures are always in local memory (this is important for any NUMA architecture).
Reference: [3] <author> Y. Ben-Asher and D. G. Feitelson, </author> <title> Performance and Overhead Measurements on the Makbilan. </title> <type> Technical Report 91-5, </type> <institution> Dept. Computer Science, The Hebrew University of Jerusalem, </institution> <month> Oct </month> <year> 1991. </year> <month> 31 </month>
Reference-contexts: As access to on-board memory is faster than access to memory on remote boards, Makbilan is a non-uniform memory access (NUMA) machine <ref> [3] </ref>. The processors have on-board caches, but they do not cache remote references. Hence there is no issue of cache coherence. * The box also includes one board that acts as a Unix host, and boards for a bus controller, a peripherals interface, and a terminal controller. <p> This kernel is highly optimized to provide fast task creation and context switching. In fact, context switching in RMK is faster than the context switching instruction provided in the i386 instruction set, because the kernel manipulates the hardware data structures directly [17, p. 7-35]: we clocked it at 72s <ref> [3] </ref>. Parallel activities 1 i386 and Multibus-II are Intel trademarks. 2 Recently, a smaller configuration with the newer and fast i486 processors also became operational. It uses the same Maxi runtime system. 6 are implemented by RMK tasks (which are the RMK equivalents of Unix processes). <p> It uses the same Maxi runtime system. 6 are implemented by RMK tasks (which are the RMK equivalents of Unix processes). The overhead for task creation and termination is about 1ms <ref> [3] </ref>. The scheduling time quantum was set to 50ms in Maxi. A real-time kernel, like RMK, has many advantages over the usual academic choice of a Unix kernel because of several reasons.
Reference: [4] <author> Y. Ben-Asher, D. G. Feitelson, and L. Rudolph, </author> <title> "ParC | an extension of C for shared memory parallel processing". </title> <journal> Software | Pract. & Exp., </journal> <note> to appear. </note>
Reference-contexts: In particular, the paper describes the implementation of such system calls in Maxi, the Makbilan runtime system. The Makbilan research multiprocessor is a shared-memory, bus-based machine, which is used to execute programs written in a locally defined parallel programming language, ParC <ref> [20, 4] </ref>. Maxi contains novel techniques that are widely applicable, although it is specifically tailored for the Makbilan and for the shared-memory constructs of ParC, to serve as an environment for research and comparison of alternative algorithms. This paper discusses support for manipulations of group of threads. <p> All such interactions are done by Maxi, and are described in the next section. 2.3 The ParC Language ParC is a superset of the C programming language, designed and implemented by our group and intended to support parallel programming in a shared memory environment <ref> [20, 4] </ref>. <p> It is local to the activity in which it is declared. Global variables are shared by all the activities. All this is implemented by the ParC compiler, which sets up pointers between the stacks of the different activities <ref> [4] </ref>. Therefore no runtime support is needed for handling the variety of types of memory accesses. The Makbilan is a NUMA machine since each processor has local memory, shared memory is distributed, and bus accesses are priority based. <p> The head and tail are locked independently, allowing spawns and activity creation to proceed concurrently. As activities are always generated from the oldest descriptor, the activity tree is executed in breadth-first order 4 . This ensures a degree of fairness in the execution <ref> [4] </ref>. The effectiveness of the load balancing scheme is demonstrated by our measurement results from the following experiment. The experiment was performed on a 10-processor configuration. A total work of 100,000,000 assignments to the same global variable was divided equally among different numbers of activities. <p> For example, such behavior is useful when a set of parallel activities are spawned to speed up a search through a large data structure. The activity that finds the required item then pbreaks, terminating the search of all the other activities <ref> [4] </ref>. Terminating a subtree of activities can be done in synchronous or asynchronous styles. The synchronous style means that the whole system is interrupted and immediately takes steps to terminate the relevant activities.
Reference: [5] <author> P. Brinch Hansen, </author> <title> "The programming language Concurrent Pascal". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> 1(2), </volume> <pages> pp. 199-207, </pages> <month> Jun </month> <year> 1975. </year>
Reference-contexts: In the extreme case, this degenerates to the naive algorithm where a separate RMK task was created for each activity at the outset. Note that envelopes are more flexible than previous proposals for pre-creation of tasks (e.g. in <ref> [5] </ref>), because the number generated is adapted to the characteristics of the program. When there are many short independent tasks, the overhead is on the order of a procedure 21 call. When independent tasks are very long, the envelopes introduce a small overhead relative to the total execution time.
Reference: [6] <author> D. Citron, I. Exman, and D. Feitelson, </author> <title> MKMONITOR A Parallel Monitor, or What You See is What You Program. </title> <type> Technical Report 91-19, </type> <institution> Dept. Computer Science, The Hebrew University of Jerusalem, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: Maxi is a research environment. Several versions and some support tools have been 30 developed, while the system itself is also used for research on parallel algorithms and for parallel programming courses. The various versions support gang scheduling, (rather than scheduling each activity individually) [15], monitored mode <ref> [6] </ref>, and deadlock detection mode [14]. Acknowledgments The ParC language was first developed by Yosi Ben-Asher. Some extensions were later added by Izhar Matkevich and Dana Ron. The compiler and simulator were written by Yosi Ben-Asher, Marcelo Bilezker, and Itzik Nudler.
Reference: [7] <author> R. F. Cmelik, N. H. Gehani, and W. D. Roome, </author> <title> "Experience with multiple processor versions of Concurrent C". </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> 15(3), </volume> <pages> pp. 335-344, </pages> <month> Mar </month> <year> 1989. </year>
Reference: [8] <author> E. C. Cooper and R. P. Draves, </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Dept. Computer Science, Carnegie-Mellon University, </institution> <month> Jun </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Parallel programming is the activity of delineating the instructions to be carried out by a number of processors and their co-relations. Low-level parallel programming facilities, such as C-threads <ref> [8] </ref>, require the programmer to fork parallel activities one at a time. If many parallel activities are required, the programmer must consider, e.g. spawning them in a loop or using a logarithmic tree structure [29]. Alternatively, a high-level parallel programming language may be used.
Reference: [9] <author> J. B. Dennis, </author> <title> "Data flow supercomputers". </title> <booktitle> Computer, </booktitle> <pages> pp. 48-56, </pages> <month> Nov </month> <year> 1980. </year>
Reference-contexts: Much research has been done on the mapping and scheduling of such fine-grain activities so as to achieve high performance. Dataflow architectures use extensive hardware support to achieve this goal <ref> [9, 25] </ref>. The common approach on conventional multiprocessors is to increase the granularity at compile time based on an analysis of the program structure (see, e.g., [22]).
Reference: [10] <author> J. Edler, A. Gottlieb, and J. Lipkis, </author> <title> "Considerations for massively parallel UNIX systems on the NYU Ultracomputer and IBM RP3". </title> <booktitle> In EUUG (European UNIX system User Group) Autumn '86 Conf. Proc., </booktitle> <pages> pp. 383-403, </pages> <month> Sep </month> <year> 1986. </year> <title> Another version appeared in Proc. </title> <booktitle> Winter USENIX Technical Conf., </booktitle> <pages> pp. 193-210, </pages> <month> Jan </month> <year> 1986. </year>
Reference-contexts: Each processor accesses and updates these structures as necessary. Of course, appropriate locks are required. For example, process scheduling is done via a central global task queue where processors insert and delete tasks during context switch operations <ref> [10] </ref>. Dynamic memory allocation schemes also work in a similar fashion. On the other hand, operating systems for parallel machines with distributed memory and efficient message passing mechanisms usually employ a distributed processing approach. That is, each processor maintains private system data structures. <p> Of course each has its own advantages. One extreme possibility is to use self-scheduling from a single global workpile 3 . Each processor executes an activity for a single time quantum and then replaces the activity back onto the workpile and chooses the next activity from the workpile <ref> [10, 19] </ref>. This approach has been applied in the past to parallel machines with uniform access times to memory.
Reference: [11] <author> P.A. Emrath, M.S. Anderson, R.R. Barton, and R.E. McGrath, </author> <title> "The Xylem operating system", </title> <booktitle> Intl. Conf. on Parallel Processing, </booktitle> <volume> vol I, </volume> <pages> pp 67-70, </pages> <month> Aug </month> <year> 1991. </year>
Reference-contexts: Had we decided to use a global queue we would have been unable to use RMK, or impose severe restrictions on its usage. A closely related approach appears in <ref> [11] </ref>, but in Maxi we provide a scheduling scheme which is more sensitive to the load on each processor. The rest of this section first describes how activities are allocated to processors at run-time.
Reference: [12] <author> I. Exman, D. G. Feitelson, and Y. I. Freidman, </author> <title> How To Kill an Activity Tree. </title> <type> Technical Report 91-20, </type> <institution> Dept. Computer Science, The Hebrew University of Jerusalem, </institution> <month> Dec </month> <year> 1991. </year>
Reference-contexts: However, these problems are easily dealt with by a source-level transformation, which is implemented by the ParC compiler <ref> [12] </ref>. 26 6.2 Algorithms for Activity Identification We have designed two algorithms for identifying the related activities that should be terminated. The considerations in comparing them involve the overhead that is required when activities are spawned, and the resulting efficiency of terminating a subtree. <p> It is expected that in most cases system tables will be the limiting resource, and not the coding scheme. 6.3 Implementation Results Run-time libraries implementing the last two algorithms have been written <ref> [12] </ref>. The performance of the two approaches is tabulated in Table 3. As expected, the coding scheme is more efficient. However, considering that the coding limits the structure of the activity tree, we decided to provide users with both options.
Reference: [13] <author> J. E. Faust and H. M. Levy, </author> <title> "The performance of an object-oriented threads package". </title> <booktitle> In Object-Oriented Prog. Syst., Lang., & Appl. Conf. Proc., </booktitle> <pages> pp. 278-288, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: The main advantage of the envelopes mechanism, though, is that it does not require any user intervention. It should also be noted, however, that envelopes are less flexible than the thread management mechanisms used in various thread packages (e.g. <ref> [13, 28] </ref>). These mechanisms use their knowledge of the application to perform fine-grain thread scheduling without operating system intervention. In effect, they replicate operating system services within the application.
Reference: [14] <author> D. G. Feitelson, </author> <title> "Deadlock detection without wait-for graphs". </title> <booktitle> Parallel Computing 17(12), </booktitle> <pages> pp. 1377-1383, </pages> <month> Dec </month> <year> 1991. </year>
Reference-contexts: Several versions and some support tools have been 30 developed, while the system itself is also used for research on parallel algorithms and for parallel programming courses. The various versions support gang scheduling, (rather than scheduling each activity individually) [15], monitored mode [6], and deadlock detection mode <ref> [14] </ref>. Acknowledgments The ParC language was first developed by Yosi Ben-Asher. Some extensions were later added by Izhar Matkevich and Dana Ron. The compiler and simulator were written by Yosi Ben-Asher, Marcelo Bilezker, and Itzik Nudler.
Reference: [15] <author> D. G. Feitelson and L. Rudolph, </author> <title> "Gang scheduling performance benefits for fine-grain synchronization". </title> <journal> J. Parallel & Distributed Comput. </journal> <volume> 16(4), </volume> <pages> pp. 306-318, </pages> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Maxi is a research environment. Several versions and some support tools have been 30 developed, while the system itself is also used for research on parallel algorithms and for parallel programming courses. The various versions support gang scheduling, (rather than scheduling each activity individually) <ref> [15] </ref>, monitored mode [6], and deadlock detection mode [14]. Acknowledgments The ParC language was first developed by Yosi Ben-Asher. Some extensions were later added by Izhar Matkevich and Dana Ron. The compiler and simulator were written by Yosi Ben-Asher, Marcelo Bilezker, and Itzik Nudler.
Reference: [16] <author> INMOS Ltd., </author> <title> Occam Programming Manual. </title> <publisher> Prentice-Hall, </publisher> <year> 1984. </year> <month> 32 </month>
Reference: [17] <author> Intel Corporation, </author> <title> iRMK I.2 Real-Time Kernel Reference Manual. 1988. Order number 462660-001. </title>
Reference-contexts: The local kernel is Intel's RMK <ref> [17] </ref>, which is a real-time kernel designed to use hardware support provided by the i386 and the Multibus-II. This kernel is highly optimized to provide fast task creation and context switching. <p> This kernel is highly optimized to provide fast task creation and context switching. In fact, context switching in RMK is faster than the context switching instruction provided in the i386 instruction set, because the kernel manipulates the hardware data structures directly <ref> [17, p. 7-35] </ref>: we clocked it at 72s [3]. Parallel activities 1 i386 and Multibus-II are Intel trademarks. 2 Recently, a smaller configuration with the newer and fast i486 processors also became operational.
Reference: [18] <author> E. Mohr, D. A. Kranz, and R. H. Halstead, Jr., </author> <title> "Lazy task creation: a technique for increasing the granularity of parallel programs". </title> <journal> IEEE Trans. Parallel & Distributed Syst. </journal> <volume> 2(3), </volume> <pages> pp. 264-280, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: In effect, the granularity is increased on-line when possible. This is actually an automatic on-line version of the lparfor construct. A similar approach has also been suggested for functional programming <ref> [18] </ref>.
Reference: [19] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> "Guided self scheduling: a practical scheduling scheme for parallel supercomputers". </title> <journal> IEEE Trans. Comput. </journal> <volume> C-36(12), </volume> <pages> pp. 1425-1439, </pages> <month> Dec </month> <year> 1987. </year>
Reference-contexts: Of course each has its own advantages. One extreme possibility is to use self-scheduling from a single global workpile 3 . Each processor executes an activity for a single time quantum and then replaces the activity back onto the workpile and chooses the next activity from the workpile <ref> [10, 19] </ref>. This approach has been applied in the past to parallel machines with uniform access times to memory. <p> The main question that remains is how to divide the activities between the representatives. Equal division may not be optimal if the activities are not identical. A promising approach is to always allocate 1=P of the remaining activities, as was suggested for guided self-scheduling of parallel loop iterations <ref> [19] </ref>. 5.3 Experimental Results Experimental results using envelopes and representatives are presented in Table 2. More detailed results can be found in [24].
Reference: [20] <author> L. Rudolph and Y. Ben-Asher, </author> <title> The PARC System. </title> <type> Technical Report CS-88-8, </type> <institution> Leibniz Center for Research in Computer Science, Hebrew University of Jerusalem, </institution> <month> Aug </month> <year> 1988. </year>
Reference-contexts: In particular, the paper describes the implementation of such system calls in Maxi, the Makbilan runtime system. The Makbilan research multiprocessor is a shared-memory, bus-based machine, which is used to execute programs written in a locally defined parallel programming language, ParC <ref> [20, 4] </ref>. Maxi contains novel techniques that are widely applicable, although it is specifically tailored for the Makbilan and for the shared-memory constructs of ParC, to serve as an environment for research and comparison of alternative algorithms. This paper discusses support for manipulations of group of threads. <p> All such interactions are done by Maxi, and are described in the next section. 2.3 The ParC Language ParC is a superset of the C programming language, designed and implemented by our group and intended to support parallel programming in a shared memory environment <ref> [20, 4] </ref>.
Reference: [21] <author> L. Rudolph, M. Slivkin-Allalouf, and E. Upfal, </author> <title> "A simple load balancing scheme for task allocation in parallel machines". </title> <booktitle> In 3rd Symp. Parallel Algorithms & Architectures, </booktitle> <pages> pp. 237-245, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: As the get work activity competes with other activities for the processor, the rate at which it is scheduled depends on the load on the processor. Thus it will be scheduled more often on lightly loaded processors, causing them to take more additional work <ref> [21] </ref>. Individual activities are not placed in the global list as this would incur a large overhead when a large number of light activities are spawned at once. Instead, each item on the list is a data structure called a spawn descriptor (Figure 3).
Reference: [22] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Dataflow architectures use extensive hardware support to achieve this goal [9, 25]. The common approach on conventional multiprocessors is to increase the granularity at compile time based on an analysis of the program structure (see, e.g., <ref> [22] </ref>). We advocate the support of ultra-light-weight tasks in the runtime system, and contend that efficient support is possible at run time without previous knowledge about the program.
Reference: [23] <author> M. L. Scott, T. J. LeBlanc, and B. D. Marsh, </author> <title> "Design rationale for Psyche, a general-purpose multiprocessor operating system". </title> <booktitle> In Intl. Conf. Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 255-262, </pages> <month> Aug </month> <year> 1988. </year>
Reference-contexts: Moreover, in a NUMA architecture, it is difficult to take advantage of the memory structure. At the other extreme is local scheduling with each processor maintaining it own local workpile, and the activities are mapped to processors via program command <ref> [23] </ref>. The advantages are numerous. At the system level, local queues are considered more efficient because activities do not move from one board to another. Thus state information such as the activity's stack is always local and does not have to be copied.
Reference: [24] <author> K. Shteiman, D. Feitelson, L. Rudolph, and I. Exman, </author> <title> "Envelopes in adaptive local queues for MIMD load balancing". </title> <booktitle> In CONPAR'92/VAPP-V, </booktitle> <month> Sep </month> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: A promising approach is to always allocate 1=P of the remaining activities, as was suggested for guided self-scheduling of parallel loop iterations [19]. 5.3 Experimental Results Experimental results using envelopes and representatives are presented in Table 2. More detailed results can be found in <ref> [24] </ref>. Four different codes are used as examples (the code given in the table is shorthand that captures the essence of the real code, but is not real ParC syntax). In the first, 10000 activities are spawned, and each performs some local work that takes 1ms.
Reference: [25] <author> V. P. Srini, </author> <title> "An architectural comparison of dataflow systems". </title> <booktitle> Computer 19(3), </booktitle> <pages> pp. 68-88, </pages> <month> Mar </month> <year> 1986. </year>
Reference-contexts: Much research has been done on the mapping and scheduling of such fine-grain activities so as to achieve high performance. Dataflow architectures use extensive hardware support to achieve this goal <ref> [9, 25] </ref>. The common approach on conventional multiprocessors is to increase the granularity at compile time based on an analysis of the program structure (see, e.g., [22]).
Reference: [26] <author> B. H. Tay and A. L. Ananda, </author> <title> "A survey of remote procedure calls". </title> <journal> Operating Systems Rev. </journal> <volume> 24(3), </volume> <pages> pp. 68-79, </pages> <month> Jul </month> <year> 1990. </year>
Reference-contexts: That is, each processor maintains private system data structures. Processors communicate via message passing and inter-processor interrupts. In many cases, these facilities are used to provide high-level remote procedure calls (RPC) <ref> [26] </ref>. This allows the kernel itself to be parallelized [27], so that one processor to directly influence other processors' actions by issuing remote procedure calls. For example, threads (activities) can be created, deleted, suspended, or resumed on a remote processor.
Reference: [27] <author> A. Tevanian, Jr., R. F. Rashid, D. B. Golub, D. L. Black, E. Cooper, and M. W. Young, </author> <title> "Mach threads and the Unix kernel: the battle for control". </title> <booktitle> In Proc. Summer USENIX Technical Conf., </booktitle> <pages> pp. 185-197, </pages> <month> Jun </month> <year> 1987. </year> <month> 33 </month>
Reference-contexts: The runtime system supporting the language constructs can be implemented either in the operating system kernel, or else it can be implemented as a user-level software package 4 above the kernel <ref> [1, 27] </ref>. We have chosen to build our system on top of a real-time operating system kernel. The runtime system should have more control over tasks and other system resources than is traditionally permitted in multi-user, interactive, time-sharing operating systems such as Unix. <p> That is, each processor maintains private system data structures. Processors communicate via message passing and inter-processor interrupts. In many cases, these facilities are used to provide high-level remote procedure calls (RPC) [26]. This allows the kernel itself to be parallelized <ref> [27] </ref>, so that one processor to directly influence other processors' actions by issuing remote procedure calls. For example, threads (activities) can be created, deleted, suspended, or resumed on a remote processor. Note that the affected kernel may be required to field many such remote requests at once.
Reference: [28] <author> A. Tucker and A. Gupta, </author> <title> "Process control and scheduling issues for multiprogrammed shared-memory multiprocessors". </title> <booktitle> In 12th Symp. Operating Systems Principles, </booktitle> <pages> pp. 159-166, </pages> <month> Dec </month> <year> 1989. </year>
Reference-contexts: The main advantage of the envelopes mechanism, though, is that it does not require any user intervention. It should also be noted, however, that envelopes are less flexible than the thread management mechanisms used in various thread packages (e.g. <ref> [13, 28] </ref>). These mechanisms use their knowledge of the application to perform fine-grain thread scheduling without operating system intervention. In effect, they replicate operating system services within the application.
Reference: [29] <author> D. Vrsalovic, Z. Segall, D. Siewiorek, F. Gregoretti, E. Caplan, C. Fineman, S. Kravitz, T. Lehr, and M. Russinovich, </author> <title> "MPC multiprocessor C language for consistent abstract shared data type paradigms". </title> <booktitle> In 22nd Ann. Hawaii Intl. Conf. System Sciences, </booktitle> <volume> vol. I, </volume> <pages> pp. 171-180, </pages> <month> Jan </month> <year> 1989. </year>
Reference-contexts: Low-level parallel programming facilities, such as C-threads [8], require the programmer to fork parallel activities one at a time. If many parallel activities are required, the programmer must consider, e.g. spawning them in a loop or using a logarithmic tree structure <ref> [29] </ref>. Alternatively, a high-level parallel programming language may be used. Such languages typically include constructs for spawning all the required activities at once, thus delegating the question of an efficient implementation to the runtime system.
References-found: 29

