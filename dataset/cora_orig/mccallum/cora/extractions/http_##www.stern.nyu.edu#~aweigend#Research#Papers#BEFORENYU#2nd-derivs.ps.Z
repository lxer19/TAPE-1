URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/2nd-derivs.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Brief Papers Computing Second Derivatives in Feed-Forward Networks: A Review  
Author: Wray L. Buntine and Andreas S. Weigend 
Date: 3, MAY 1994 1  
Note: IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 5, NO.  
Abstract: The calculation of second derivatives is required by recent training and analysis techniques of connectionist networks, such as the elimination of superfluous weights, and the estimation of confidence intervals both for weights and network outputs. We here review and develop exact and approximate algorithms for calculating second derivatives. For networks with jwj weights, simply writing the full matrix of second derivatives requires O(jwj 2 ) operations. For networks of radial basis units or sigmoid units, exact calculation of the necessary intermediate terms requires of the order of 2h + 2 backward/forward-propagation passes where h is the number of hidden units in the network. We also review and compare three approximations (ignoring some components of the second derivative, numerical differentiation, and scoring). Our algorithms apply to arbitrary activation functions, networks, and error functions (for instance, with connections that skip layers, or radial basis functions, or cross-entropy error and Softmax units, etc.). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. R. Barron and R. L. Barron, </author> <title> Statistical learning networks: A unifying view, in INTERFACE'88-20th Symposium on the Interface: </title> <journal> Computing Science and Statistics, American Statistical Association, </journal> <year> 1988, </year> <pages> pp. 192-203. </pages>
Reference-contexts: Mackay [15] uses numerical differentiation to calculate the Hessian of the error. An approximation of the Minimum Description Length principle that does not require calculation of the second derivatives is given by Weigend et al. [24]; see also Barron and Barron <ref> [1] </ref>. 5) After learning: Network analysis. One of the striking differences between connectionist modeling and traditional statistics is the larger number of parameters in neural networks. However, a key feature is that the potential number of parameters is often much larger than the effective number of parameters.
Reference: [2] <author> C. Bishop, </author> <title> Exact calculation of the Hessian matrix for the multilayer perceptron, </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 494-501, </pages> <year> 1992. </year>
Reference-contexts: Instead we use the notion of derivatives local to a unit and its immediate inputs/outputs, and derivatives global to the network, since these correspond to the key facets of the computation. Bishop Fig. 1. A simple network. Weights are shown as lines between units, biases as terminated lines. <ref> [2] </ref> avoided this complication by restricting results to sigmoidal units with error a simple sum and with no connections skipping layers. <p> Equation (12) remains unchanged but the other two simplify. Werbos et al. present a formulation using ordered derivatives, and Bishop <ref> [2] </ref> presents the simplified case where there are no connections skipping layers, and the error E is a sum over network outputs (i.e., E m;l = 0 for m 6= l). Equation (12) corresponds to Bishop's equation (2.17) and (10) corresponds to Bishop's equation (3.2) [2]. <p> using ordered derivatives, and Bishop <ref> [2] </ref> presents the simplified case where there are no connections skipping layers, and the error E is a sum over network outputs (i.e., E m;l = 0 for m 6= l). Equation (12) corresponds to Bishop's equation (2.17) and (10) corresponds to Bishop's equation (3.2) [2]. Corollary 2 (s-type): Assume that for every unit activation input is s-type and there is no sequence of connections from units m to n.
Reference: [3] <author> S. Becker and Y. Le Cun, </author> <title> Improving the convergence of backpropagation learning with second order methods, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> David S. Touretzky, Geoffrey E. Hinton, and Terrence J. Sejnowski, Eds. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988, </year> <pages> pp. 29-37. </pages>
Reference-contexts: Since the second derivatives have to be computed for each weight update, the speed of the computation is crucial here. For large networks, calculation of the full Hessian was considered prohibitive. In Section IV-A we review several approximations: Becker and Le Cun <ref> [3] </ref> suggest a simple diagonal approximation to the Hessian. El-Jaroudi and Makhoul [7] make a block matrix approximation. <p> Of course, additional first derivatives @u m =@u n and @u i =@u n are required and can be got in approximately h backpropagation passes. A similar method is suggested by Becker and Le Cun <ref> [3] </ref> This approximation is therefore of the same computational order as the exact calculations described previously, but requires little additional algorithm overhead other than the backpropagation algorithm.
Reference: [4] <author> J. S. Bridle, </author> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition, in Neuro-computing: Algorithms, Architectures and Applications, </title> <editor> F. Fougelman-Soulie and J. Herault, Eds. </editor> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1989, </year> <pages> pp. 223-236. </pages>
Reference-contexts: Seemingly an exception to these two classes are so-called Softmax units (Bridle, <ref> [4] </ref>). They are a convenient choice for the 1-of-N classification tasks: The constraints on probabilities to be nonnegative and add up to one are modeled by using exponential units that are normalized, and this choice leads to a simple update rule in backpropagation.
Reference: [5] <author> E. B. Baum and F. Wilczek, </author> <title> Supervised learning of probability distributions by neural networks, </title> <booktitle> in Neural Information Processing Systems (NIPS), </booktitle> <editor> D. Z. Anderson, Ed. </editor> <year> 1987, </year> <pages> pp. 52-61. </pages>
Reference-contexts: B. Scoring A second form of approximation exists if the network error function being minimized corresponds to the negative logarithm of the likelihood of the training sample, as is often the case when using mean-square error or cross-entropy error functions <ref> [5] </ref>, [6], [7].
Reference: [6] <author> W. L. Buntine and A. </author> <title> S Weigend, Bayesian back-propagation, </title> <journal> Complex Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 603-643, </pages> <year> 1991. </year>
Reference-contexts: Examples are the estimation of the generalization error of the network and of the precision of the network outputs (i.e., confidence intervals or error bars), as well as the comparison of different networks trained on the same data, see Buntine and Weigend <ref> [6] </ref> and MacKay [15]. The network pruning strategy given by Buntine and Weigend [6] has the advantage over Hassibi and Stork in that it does not require calculation of the inverse of the Hessian. Mackay [15] uses numerical differentiation to calculate the Hessian of the error. <p> the generalization error of the network and of the precision of the network outputs (i.e., confidence intervals or error bars), as well as the comparison of different networks trained on the same data, see Buntine and Weigend <ref> [6] </ref> and MacKay [15]. The network pruning strategy given by Buntine and Weigend [6] has the advantage over Hassibi and Stork in that it does not require calculation of the inverse of the Hessian. Mackay [15] uses numerical differentiation to calculate the Hessian of the error. <p> Details are given by Bun-tine and Weigend <ref> [6] </ref> and by Rumelhart et al. [22]. For many uses mentioned in the introduction we are interested in second derivatives for the sum (or average) error over the entire training set (the so-called batch mode), or at least a reasonable-sized subset. <p> B. Scoring A second form of approximation exists if the network error function being minimized corresponds to the negative logarithm of the likelihood of the training sample, as is often the case when using mean-square error or cross-entropy error functions [5], <ref> [6] </ref>, [7].
Reference: [7] <author> A. El-Jaroudi and J. Makhoul, </author> <title> A new error criterion for posterior probability estimation with neural nets, </title> <booktitle> in Int. Joint Conf. on Neural Networks, </booktitle> <address> San Diego, CA, </address> <year> 1990, </year> <pages> pp. </pages> <month> III-185-192. </month>
Reference-contexts: For large networks, calculation of the full Hessian was considered prohibitive. In Section IV-A we review several approximations: Becker and Le Cun [3] suggest a simple diagonal approximation to the Hessian. El-Jaroudi and Makhoul <ref> [7] </ref> make a block matrix approximation. Fahlman [8] uses in the Quickprop algorithm a simple diagonal approximation and also uses numerical differentiation from the error derivatives of the previous cycle to approximate the diagonal terms of the Hessian (which we discuss in Section IV-C). 2) Within learning: Indirect second-order methods. <p> This is the dominant term in the calculation. Notice that for the special case where we only use the block diagonals, for instance, as assumed by El-Jaroudi and Makhoul <ref> [7] </ref>, @ 2 E=@w m;i =@w n;j for n = m, of a network, Equation (10) should be used. Thus, (10) replaces (12) in Step 3b) and the calculation is only done for the unit m = n. Likewise for Step 3c). IV. APPROXIMATIONS A. <p> B. Scoring A second form of approximation exists if the network error function being minimized corresponds to the negative logarithm of the likelihood of the training sample, as is often the case when using mean-square error or cross-entropy error functions [5], [6], <ref> [7] </ref>.
Reference: [8] <author> S. E. Fahlman, </author> <title> Faster-learning variations on backpropagation: An empirical study, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> D. S. Touretzky, G. E. Hinton, and T. J. Sejnowski, Eds. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988, </year> <pages> pp. 38-51. </pages>
Reference-contexts: For large networks, calculation of the full Hessian was considered prohibitive. In Section IV-A we review several approximations: Becker and Le Cun [3] suggest a simple diagonal approximation to the Hessian. El-Jaroudi and Makhoul [7] make a block matrix approximation. Fahlman <ref> [8] </ref> uses in the Quickprop algorithm a simple diagonal approximation and also uses numerical differentiation from the error derivatives of the previous cycle to approximate the diagonal terms of the Hessian (which we discuss in Section IV-C). 2) Within learning: Indirect second-order methods.
Reference: [9] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, second edition. </title> <publisher> Baltimore: Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Mtller [16], [17] and Pearlmutter [20] have both independently suggest numerical differentiation and exact calculation to compute this product. The calculation can also be used iteratively in the power method 2 <ref> [9] </ref> to efficiently approximate the principle eigenvectors of the Hessian. The principle eigenvectors are used by Le Cun, Simard, and Pearlmutter [14] to speed up gradient descent. 3) Within learning: Analysis. <p> Hassibi and Stork [10] apply the Sherman-Morrison-Woodbury formula for iterative matrix inversion <ref> [9] </ref>, [21] to find the inverse of an approximate Hessian. In Section IV-B of this paper, we suggest a stronger justification for their approximation. <p> With this approximation, a sequential calculation for the inverse of the second derivative is also available, as given by Hassibi and Stork [10]. The Sherman-Morrison-Woodbury formula for iterative matrix inversion <ref> [9] </ref>, [21] is: (A + b d T ) 1 = A 1 (1 + d T A 1 b) where A is a square matrix and b and d are column vectors of the same size.
Reference: [10] <author> B. Hassibi and D. G. Stork, </author> <title> Second order derivatives for network pruning: Optimal brain surgeon, </title> <booktitle> in Advances in Neural Information Processing Systems 5 (NIPS*92), </booktitle> <editor> S. J. Hanson, J. Cowan, and L. Giles, Eds. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 164-171. </pages>
Reference-contexts: For example, Le Cun, Denker and Solla [12] (Optimal Brain Damage) use the Hessian of the error (calculated with the Becker and Le Cun approximation referred to above) to simplify the network by pruning weights in order to achieve good generalization performance. Hassibi and Stork <ref> [10] </ref> apply the Sherman-Morrison-Woodbury formula for iterative matrix inversion [9], [21] to find the inverse of an approximate Hessian. In Section IV-B of this paper, we suggest a stronger justification for their approximation. <p> With this approximation, a sequential calculation for the inverse of the second derivative is also available, as given by Hassibi and Stork <ref> [10] </ref>. The Sherman-Morrison-Woodbury formula for iterative matrix inversion [9], [21] is: (A + b d T ) 1 = A 1 (1 + d T A 1 b) where A is a square matrix and b and d are column vectors of the same size.
Reference: [11] <author> R. A. Jacobs, </author> <title> Increased rates of convergence through learning rate adaption, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: It is well known that the speed of training in least mean square algorithms is related to the ratio of the largest to the smallest eigenvalues. A good description is given by Jacob <ref> [11] </ref>, and further analysis is presented by Le Cun, Kanter and Solla [13]. This ratio is called the condition number and is also associated with the accuracy to which the minimum can be calculated.
Reference: [12] <author> Y. Le Cun, J. S. Denker, and S. A. Solla, </author> <title> Optimal brain damage, </title> <booktitle> in Advances in Neural Information Processing Systems 2 (NIPS*89), </booktitle> <editor> D. S. Touretzky, Ed. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990, </year> <pages> pp. 598-605. </pages>
Reference-contexts: For example, Le Cun, Denker and Solla <ref> [12] </ref> (Optimal Brain Damage) use the Hessian of the error (calculated with the Becker and Le Cun approximation referred to above) to simplify the network by pruning weights in order to achieve good generalization performance. <p> different parameterization so their form is different): @ 2 E m X @u 2 u m l + @u l l w 2 @ 2 E m;i @u 2 u m u m + @u m 00 2 These can be computed with one backpropagation pass, so calculation is efficient <ref> [12] </ref>.
Reference: [13] <author> Y. Le Cun, I. Kanter, and S. A. Solla, </author> <title> Second order properties of error surfaces: Learning time and generalization, </title> <booktitle> in Advances in Neural Information Proccessing Systems 3 (NIPS*90), </booktitle> <editor> R. P. Lippmann, J. Moody, and D. S. Touretzky, Eds. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 918-924. </pages>
Reference-contexts: It is well known that the speed of training in least mean square algorithms is related to the ratio of the largest to the smallest eigenvalues. A good description is given by Jacob [11], and further analysis is presented by Le Cun, Kanter and Solla <ref> [13] </ref>. This ratio is called the condition number and is also associated with the accuracy to which the minimum can be calculated. The condition number can be approximated by approximating the largest and smallest eigenvalues with the power method. 4) After learning: Network pruning.
Reference: [14] <author> Y. Le Cun, P. Y. Simard, and B. A. Pearlmutter, </author> <title> Automatic learning rate maximization by on-line estimation of the Hessian's Eigenvectors, </title> <booktitle> in Advances in Neural Information Processing Systems 5 (NIPS*92), </booktitle> <editor> S. J. Hanson, J. Cowan, and L. Giles, Eds. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 156-163. </pages>
Reference-contexts: The calculation can also be used iteratively in the power method 2 [9] to efficiently approximate the principle eigenvectors of the Hessian. The principle eigenvectors are used by Le Cun, Simard, and Pearlmutter <ref> [14] </ref> to speed up gradient descent. 3) Within learning: Analysis. It is well known that the speed of training in least mean square algorithms is related to the ratio of the largest to the smallest eigenvalues.
Reference: [15] <author> D. J. C. MacKay, </author> <title> A practical Bayesian framework for backpropagation networks, </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 448-472, </pages> <year> 1992. </year>
Reference-contexts: Examples are the estimation of the generalization error of the network and of the precision of the network outputs (i.e., confidence intervals or error bars), as well as the comparison of different networks trained on the same data, see Buntine and Weigend [6] and MacKay <ref> [15] </ref>. The network pruning strategy given by Buntine and Weigend [6] has the advantage over Hassibi and Stork in that it does not require calculation of the inverse of the Hessian. Mackay [15] uses numerical differentiation to calculate the Hessian of the error. <p> as the comparison of different networks trained on the same data, see Buntine and Weigend [6] and MacKay <ref> [15] </ref>. The network pruning strategy given by Buntine and Weigend [6] has the advantage over Hassibi and Stork in that it does not require calculation of the inverse of the Hessian. Mackay [15] uses numerical differentiation to calculate the Hessian of the error. An approximation of the Minimum Description Length principle that does not require calculation of the second derivatives is given by Weigend et al. [24]; see also Barron and Barron [1]. 5) After learning: Network analysis. <p> MacKay, however, found the method inaccurate for his purposes <ref> [15] </ref> and instead dropped the first term from Equation (12) and unfolds the recursion leaving a calculation that requires only the first derivatives, @ 2 E l;k @u m @u n m u (j) He reports this is inaccurate when approximating the determinant of the matrix of second derivatives, or looking <p> C. Numerical Differentiation A third more exact approximation of second derivatives can be done by numerical differentiation of the first derivatives, which in turn can be calculated using standard backpropagation with (5). MacKay has done this by numerical differentiation of the derivatives w.r.t. the weights <ref> [15] </ref>. If there are jwj different weights in the network, then this requires jwj + 1 backpropagation passes to compute the necessary first derivatives (although many intermediate results can be cached so full passes are not needed).
Reference: [16] <author> M. F. Mtller, </author> <title> Exact Calculation of the Product of the Hessian Matrix of Feed-Forward Network Error Functions and a Vector in O(N) Time, </title> <type> Technical Report PB-432, </type> <institution> Computer Science Department, Aarhus University, Denmark, </institution> <year> 1993. </year>
Reference-contexts: Here second derivatives are used during line search, so the full Hessian is not required, just the product of the Hessian and a given vector. Mtller <ref> [16] </ref>, [17] and Pearlmutter [20] have both independently suggest numerical differentiation and exact calculation to compute this product. The calculation can also be used iteratively in the power method 2 [9] to efficiently approximate the principle eigenvectors of the Hessian.
Reference: [17] <author> M. F. Mtller, </author> <title> A scaled conjugate gradient algorithm for fast supervised learning, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 6, no. 4, </volume> <pages> pp. 525-533, </pages> <year> 1993. </year>
Reference-contexts: Here second derivatives are used during line search, so the full Hessian is not required, just the product of the Hessian and a given vector. Mtller [16], <ref> [17] </ref> and Pearlmutter [20] have both independently suggest numerical differentiation and exact calculation to compute this product. The calculation can also be used iteratively in the power method 2 [9] to efficiently approximate the principle eigenvectors of the Hessian. <p> Mtller and Pearlmutter have applied this numerical ap proach to calculate the second derivatives along a particular arc <ref> [17] </ref>, [20] in two backpropagation passes. Let v be a column vector in weight space expressing a direction of interest, and let 4 be some small value d E dwdw v 4 dE fi fi w=w+v4 dE ! V.
Reference: [18] <author> P. McCullagh and J. A. Nelder, </author> <title> Generalized Linear Models, second edition. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1989. </year>
Reference-contexts: This follows from the probabilistic identity Z @ 1 @ 2 = @ 2 log p (zj) p (zj) dz; for the probability density function p (zj) parameterized by a vector of parameters . This approximation is used in Fisher's scoring method for maximum likelihood training <ref> [18] </ref>. It is best used during search when fast estimates of second derivatives are required (see, for instance, [21], Section 14.4).
Reference: [19] <author> J. E. Moody, </author> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems, </title> <booktitle> in Advances in Neural Information Processing Systems 4 (NIPS*91), </booktitle> <editor> J. E. Moody, S. J. Hanson, and R. P. Lippmann, Eds. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992, </year> <pages> pp. 847-854. </pages>
Reference-contexts: One of the striking differences between connectionist modeling and traditional statistics is the larger number of parameters in neural networks. However, a key feature is that the potential number of parameters is often much larger than the effective number of parameters. Moody <ref> [19] </ref> uses the Hessian of the error in a regularization framework to estimate the effective number of parameters of the network.
Reference: [20] <author> B. A. Pearlmutter, </author> <title> Fast exact multiplication by the Hessian, Neural Computation, </title> <note> submitted. </note>
Reference-contexts: Here second derivatives are used during line search, so the full Hessian is not required, just the product of the Hessian and a given vector. Mtller [16], [17] and Pearlmutter <ref> [20] </ref> have both independently suggest numerical differentiation and exact calculation to compute this product. The calculation can also be used iteratively in the power method 2 [9] to efficiently approximate the principle eigenvectors of the Hessian. <p> Mtller and Pearlmutter have applied this numerical ap proach to calculate the second derivatives along a particular arc [17], <ref> [20] </ref> in two backpropagation passes. Let v be a column vector in weight space expressing a direction of interest, and let 4 be some small value d E dwdw v 4 dE fi fi w=w+v4 dE ! V.
Reference: [21] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing. </booktitle> <address> Cambridge, UK: </address> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: 1) Within learning: Approximate second-order methods. Simple backpropagation is a first order gradient descent method. Learning speed can be improved if information from second derivatives is also used, for instance in a Newton-Raphson type framework (see <ref> [21] </ref>, Section Manuscript received September 5, 1991; revised August 1, 1992. W. L. Buntine is with RIACS at NASA Ames Research Center, Moffett Field, CA 94035-1000, USA. A. S. <p> There is another class of second order optimization algorithms that do not require direct calculation of the Hessian (or any approximation) because they operate in an iterative manner. The conjugate gradient and related algorithms (see <ref> [21] </ref>, Section 10.6) are generally considered the most powerful all-purpose minimization algorithms (although it is not clear whether this scales to very large networks). Here second derivatives are used during line search, so the full Hessian is not required, just the product of the Hessian and a given vector. <p> Hassibi and Stork [10] apply the Sherman-Morrison-Woodbury formula for iterative matrix inversion [9], <ref> [21] </ref> to find the inverse of an approximate Hessian. In Section IV-B of this paper, we suggest a stronger justification for their approximation. <p> This approximation, ignoring the second derivatives, corresponds to the Levenberg-Marquardt approximation in nonlinear least squares (see <ref> [21] </ref>, Section 14.4). Clearly, better approximations terms are available that ignore a few less terms. B. <p> This approximation is used in Fisher's scoring method for maximum likelihood training [18]. It is best used during search when fast estimates of second derivatives are required (see, for instance, <ref> [21] </ref>, Section 14.4). The approximation would be misleading when good approximations for error bars are required because it assumes that the thing we are attempting to evaluate is approximately true (that the weights are correct). <p> With this approximation, a sequential calculation for the inverse of the second derivative is also available, as given by Hassibi and Stork [10]. The Sherman-Morrison-Woodbury formula for iterative matrix inversion [9], <ref> [21] </ref> is: (A + b d T ) 1 = A 1 (1 + d T A 1 b) where A is a square matrix and b and d are column vectors of the same size.
Reference: [22] <author> D. E. Rumelhart, R. Durbin, R. Golden, and Y. Chauvin, </author> <title> Backpropagation: The theory, in Backpropagation: Theory, Architectures and Applications. </title> <editor> Y. Chauvin and D. E. Rumelhart, Eds. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1994. </year>
Reference-contexts: Details are given by Bun-tine and Weigend [6] and by Rumelhart et al. <ref> [22] </ref>. For many uses mentioned in the introduction we are interested in second derivatives for the sum (or average) error over the entire training set (the so-called batch mode), or at least a reasonable-sized subset.
Reference: [23] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> Learning internal representations by error propagation, in Parallel Distributed Processing, </title> <editor> D. E. Rumelhart, J. L. McClelland, and the PDP Research Group, Eds. </editor> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986, </year> <pages> pp. 318-362. </pages>
Reference: [24] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart, </author> <title> Predicting the future: a connectionist approach, </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 1, </volume> <pages> pp. 193-209, </pages> <year> 1990. </year>
Reference-contexts: Mackay [15] uses numerical differentiation to calculate the Hessian of the error. An approximation of the Minimum Description Length principle that does not require calculation of the second derivatives is given by Weigend et al. <ref> [24] </ref>; see also Barron and Barron [1]. 5) After learning: Network analysis. One of the striking differences between connectionist modeling and traditional statistics is the larger number of parameters in neural networks. <p> For accounting purposes, it can also be treated as a generalized bias term, n = w n;0 . In this simple formula we assume spherical symmetry ( n is a scalar). This case is discussed further in Weigend et al. <ref> [24] </ref>. 5 The sigmoid is a composition of two operators: an affine mapping giving v n , followed by a nonlinear transformation f n . First, the inputs into a hidden unit h are linearly combined, and an offset or bias w n;0 is added.
Reference: [25] <author> P. J. Werbos, T. McAvoy, and T. Su, </author> <title> Neural networks, system identification, and control in the chemical process industry, in Handbook of Intelligent Control, </title> <editor> D. A. White and D. A. Sofge, Eds. </editor> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992, </year> <pages> pp. 283-356. </pages>
Reference-contexts: Now, why a paper on second derivativesisn't using second derivatives nothing but the chain rule for differentiation? Yesbut there are many ways, with varying degrees of efficiency and accuracy. To handle the complication of second derivatives over a complex network, Werbos, McAvoy, and Su <ref> [25] </ref> introduced the notion of the ordered derivative. Instead we use the notion of derivatives local to a unit and its immediate inputs/outputs, and derivatives global to the network, since these correspond to the key facets of the computation. Bishop Fig. 1. A simple network. <p> So the distinction between the two forms must be maintained. Examples and the explicit formulae for r-type and s-type activations below will make this clearer. Werbos, McAvoy and Su <ref> [25] </ref> use the notion of an ordered derivative to maintain the same distinction 6 . <p> Likewise, due to u m l the second term sums over activation units l that m connects to. 7 The corresponding equation using the Werbos et al. notation of ordered derivatives is (see <ref> [25] </ref>, equation (99)). BUNTINE AND WEIGEND: COMPUTING SECOND DERIVATIVES IN FEED-FORWARD NETWORKS 5 A. Basic Theory If we assume nothing about the form of the activation functions, we can apply the chain rule to obtain a general form of the second derivative.
Reference: [26] <author> A. S. Weigend and D. E. Rumelhart, </author> <title> Generalization through minimal networks with application to forecasting, </title> <booktitle> in INTERFACE'91-23rd Symposium on the Interface: Computing Science and Statistics, </booktitle> <editor> E. M. Keramidas, Ed. </editor> <booktitle> Interface Foundation of North America, </booktitle> <year> 1991, </year> <pages> pp. 362-370. </pages>
Reference-contexts: Moody [19] uses the Hessian of the error in a regularization framework to estimate the effective number of parameters of the network. Whereas Moody only considers the effective number of parameters at the end of the training process, when the error has reached a minimum, Weigend and Rumelhart <ref> [26] </ref> analyze the effective network size during training (via the dimension of the space spanned by the activations of the hidden units.) The gradual increase in network size with training time provides a justification for another heuristic for obtaining good generalization: to train a large (oversized) network, but stop training early
Reference: [27] <author> A. S. Weigend, </author> <title> On overfitting and effective number of hidden units, </title> <booktitle> in Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <editor> M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman, and A. S. Weigend, Eds. </editor> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: analyze the effective network size during training (via the dimension of the space spanned by the activations of the hidden units.) The gradual increase in network size with training time provides a justification for another heuristic for obtaining good generalization: to train a large (oversized) network, but stop training early <ref> [27] </ref>. Now, why a paper on second derivativesisn't using second derivatives nothing but the chain rule for differentiation? Yesbut there are many ways, with varying degrees of efficiency and accuracy.
References-found: 27

