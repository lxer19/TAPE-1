URL: http://www.csc.calpoly.edu/~tpederse/aaai98-cmpl.ps.gz
Refering-URL: http://www.csc.calpoly.edu/~tpederse/pubs.html
Root-URL: http://www.csc.calpoly.edu
Email: pedersen@seas.smu.edu  bruce@cs.unca.edu  
Title: Knowledge Lean Word-Sense Disambiguation  
Author: Ted Pedersen Rebecca Bruce 
Address: Dallas, TX 75275-0112  Asheville, NC 28804  
Affiliation: Department of Computer Science and Engineering Southern Methodist University  Department of Computer Science University of North Carolina at Asheville  
Note: Appears in the Proceedings of the Fifteenth National Conference on Artificial Intelligence, July 1998, Madison, WI  
Abstract: We present a corpus-based approach to word-sense disambiguation that only requires information that can be automatically extracted from untagged text. We use unsupervised techniques to estimate the parameters of a model describing the conditional distribution of the sense group given the known contextual features. Both the EM algorithm and Gibbs Sampling are evaluated to determine which is most appropriate for our data. We compare their disambiguation accuracy in an experiment with thirteen different words and three feature sets. Gibbs Sampling results in small but consistent improvement in disambiguation accuracy over the EM algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bruce, R., and Wiebe, J. </author> <year> 1994. </year> <title> Word-sense disambiguation using decomposable models. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 139-146. </pages>
Reference-contexts: The features described above are defined over a small contextual window (local-context) and are selected to produce low dimensional event spaces. Local-context features have been used successfully in a variety of supervised approaches to disambiguation (e.g., <ref> (Bruce & Wiebe 1994) </ref>, (Ng & Lee 1996)). Feature Sets A, B and C The 3 feature sets used in these experiments are designated A, B and C pand are formulated as shown below. <p> Related Work There is an abundance of literature on word-sense disambiguation. Our knowledge-lean approach differs from most in that it does not require any knowledge resources beyond raw text. Corpus-based approaches often use supervised learning algorithms with sense-tagged text (e.g., (Leacock, Towell, & Voorhees 1993), <ref> (Bruce & Wiebe 1994) </ref>, (Mooney 1996)) or multi-lingual parallel corpora (e.g., (Gale, Church, & Yarowsky 1992)). An approach that significantly reduces the amount of sense-tagged data required is described in (Yarowsky 1995).
Reference: <author> Bruce, R.; Wiebe, J.; and Pedersen, T. </author> <year> 1996. </year> <title> The measure of a model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 101-112. </pages>
Reference-contexts: This indicates that different features may be needed to accommodate larger numbers of senses. The line data (Leacock, Towell, & Voorhees 1993) is taken from the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus and tagged with WordNet senses. The remaining twelve words <ref> (Bruce, Wiebe, & Pedersen 1996) </ref> were agree: (total count: 1109) to concede after disagreement: 74% to share the same opinion: 26% close: (total count: 1354) to (cause to) end: 77% to (cause to) stop operation: 23% help: (total count: 1267) to enhance inanimate object: 78% to assist human object: 22% include:
Reference: <author> Dempster, A.; Laird, N.; and Rubin, D. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM 4 In Schutze's evaluation, tagged text is not required to label the sense groupings and establish the accuracy of the disambiguation experiment. Thus the experiment is fully automatic and free from dependence on any external knowledge source. algorithm. </title> <journal> Journal of the Royal Statistical Society B 39 </journal> <pages> 1-38. </pages>
Reference-contexts: We employ the Expectation Maximization (EM) algorithm <ref> (Dempster, Laird, & Rubin 1977) </ref> and Gibbs Sampling (Geman & Geman 1984) to estimate model parameters from untagged data. <p> The M-step makes maximum likelihood estimates of the parameters given the imputed values of the sufficient statistics. These steps alternate until the parameter estimates in iteration k 1 and k differ by less than *. The EM algorithm for the exponential family of probabilistic models is introduced in <ref> (Dempster, Laird, & Rubin 1977) </ref>. The Naive Bayes model is a decomposable model which is a member of the exponential family with special properties that simplify the formulation of the E-step (Lauritzen 1995).
Reference: <author> Gale, W.; Church, K.; and Yarowsky, D. </author> <year> 1992. </year> <title> A method for disambiguating word senses in a large corpus. </title> <booktitle> Computers and the Humanities 26 </booktitle> <pages> 415-439. </pages>
Reference-contexts: Our knowledge-lean approach differs from most in that it does not require any knowledge resources beyond raw text. Corpus-based approaches often use supervised learning algorithms with sense-tagged text (e.g., (Leacock, Towell, & Voorhees 1993), (Bruce & Wiebe 1994), (Mooney 1996)) or multi-lingual parallel corpora (e.g., <ref> (Gale, Church, & Yarowsky 1992) </ref>). An approach that significantly reduces the amount of sense-tagged data required is described in (Yarowsky 1995). Yarowsky suggests a variety of options for automatically seeding a supervised disambiguation algorithm; one is to identify collocations that uniquely distinguish between senses.
Reference: <author> Gale, W.; Church, K.; and Yarowsky, D. </author> <year> 1995. </year> <title> Discrimination decisions for 100,000 dimensional spaces. </title> <journal> Journal of Operations Research 55 </journal> <pages> 323-344. </pages>
Reference-contexts: The performance of Gibbs Sampling in the current study also falls short of that of McQuitty's for adjectives and verbs which supports the previous conclusion. The EM algorithm is used with a Naive Bayes classifier in <ref> (Gale, Church, & Yarowsky 1995) </ref> to distinguish city names from people's names. A narrow window of context, one or two words to either side, was found to perform better than wider windows.
Reference: <author> Geman, S., and Geman, D. </author> <year> 1984. </year> <title> Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 6 </journal> <pages> 721-741. </pages>
Reference-contexts: We employ the Expectation Maximization (EM) algorithm (Dempster, Laird, & Rubin 1977) and Gibbs Sampling <ref> (Geman & Geman 1984) </ref> to estimate model parameters from untagged data. Both are well known and widely used iterative algorithms for estimating model parameters in the presence of missing data; in our case, the missing data are the senses of the ambiguous words.
Reference: <author> Geweke, J. </author> <year> 1992. </year> <title> Evaluating the accuracy of sampling-based approaches to calculating posterior moments. </title> <editor> In Bernardo, J.; Berger, J.; Dawid, A.; and Smith, A., eds., </editor> <booktitle> Bayesian Statistics 4. </booktitle> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: It is suggested that some portion of the early iterations be discarded. This process is commonly known as a "burn-in". We use a 500 iteration burn-in and monitor the following 1000 iterations for convergence using the measure proposed in <ref> (Geweke 1992) </ref>. If the chains have not converged, then additional iterations are performed until they do.
Reference: <author> Lauritzen, S. </author> <year> 1995. </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis 19 </journal> <pages> 191-201. </pages>
Reference-contexts: The EM algorithm for the exponential family of probabilistic models is introduced in (Dempster, Laird, & Rubin 1977). The Naive Bayes model is a decomposable model which is a member of the exponential family with special properties that simplify the formulation of the E-step <ref> (Lauritzen 1995) </ref>. The EM algorithm for Naive Bayes proceeds as follows: 1. randomly initialize p (F i jS), set k = 1 2. E-step: count (F i ; S) = p (SjF i ) fi count (F i ) 3.
Reference: <author> Leacock, C.; Towell, G.; and Voorhees, E. </author> <year> 1993. </year> <title> Corpus-based statistical sense resolution. </title> <booktitle> In Proceedings of the ARPA Workshop on Human Language Technology, </booktitle> <pages> 260-265. </pages>
Reference-contexts: However, preliminary experiments with 6 senses show that accuracy degrades considerably, to approximately 25 to 30 percent, depending on the feature set. This indicates that different features may be needed to accommodate larger numbers of senses. The line data <ref> (Leacock, Towell, & Voorhees 1993) </ref> is taken from the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus and tagged with WordNet senses. <p> Related Work There is an abundance of literature on word-sense disambiguation. Our knowledge-lean approach differs from most in that it does not require any knowledge resources beyond raw text. Corpus-based approaches often use supervised learning algorithms with sense-tagged text (e.g., <ref> (Leacock, Towell, & Voorhees 1993) </ref>, (Bruce & Wiebe 1994), (Mooney 1996)) or multi-lingual parallel corpora (e.g., (Gale, Church, & Yarowsky 1992)). An approach that significantly reduces the amount of sense-tagged data required is described in (Yarowsky 1995).
Reference: <author> Meng, X., and van Dyk, D. </author> <year> 1997. </year> <title> The EM algorithm an old folk-song sung to a new fast tune (with discussion). </title> <journal> Journal of Royal Statistics Society, Series B 59(3):511|567. </journal>
Reference-contexts: We use Gibbs Sampling to impute the missing values for S and then sample values for the parameters. Gibbs Sampling is often cast as a stochastic version of the EM algorithm (e.g., <ref> (Meng & van Dyk 1997) </ref>). However, in general Gibbs Sampling is applicable to a wider class of problems than the EM algorithm. A Gibbs Sampler generates chains of values for the missing senses S and the parameters p (F i jS) via iterative sampling. <p> However, in our experiments the EM algorithm often converged quite quickly, usually within 20 iterations, to a global maximum. These results suggest that a combination of the EM algorithm and Gibbs Sampling might be appropriate. <ref> (Meng & van Dyk 1997) </ref> propose that the Gibbs Sampler start at the point the Feature Set A Feature Set B Feature Set C Maj.
Reference: <author> Mooney, R. </author> <year> 1996. </year> <title> Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 82-91. </pages>
Reference-contexts: The advantage of this approach is two-fold: (1) there is a large body of evidence recommending the use of the Naive Bayes model in word-sense disambiguation (e.g., (Lea-cock, Towell, & Voorhees 1993), <ref> (Mooney 1996) </ref>, (Ng 1997)) and (2) unsupervised techniques for parameter estimation, once developed, could be easily applied to other parametric forms in the class of decomposable models. <p> Related Work There is an abundance of literature on word-sense disambiguation. Our knowledge-lean approach differs from most in that it does not require any knowledge resources beyond raw text. Corpus-based approaches often use supervised learning algorithms with sense-tagged text (e.g., (Leacock, Towell, & Voorhees 1993), (Bruce & Wiebe 1994), <ref> (Mooney 1996) </ref>) or multi-lingual parallel corpora (e.g., (Gale, Church, & Yarowsky 1992)). An approach that significantly reduces the amount of sense-tagged data required is described in (Yarowsky 1995).
Reference: <author> Ng, H., and Lee, H. </author> <year> 1996. </year> <title> Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics, </booktitle> <pages> 40-47. </pages>
Reference-contexts: The features described above are defined over a small contextual window (local-context) and are selected to produce low dimensional event spaces. Local-context features have been used successfully in a variety of supervised approaches to disambiguation (e.g., (Bruce & Wiebe 1994), <ref> (Ng & Lee 1996) </ref>). Feature Sets A, B and C The 3 feature sets used in these experiments are designated A, B and C pand are formulated as shown below.
Reference: <author> Ng, H. </author> <year> 1997. </year> <title> Exemplar-based word sense disambiguation: Some recent improvements. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 208-213. </pages>
Reference-contexts: The advantage of this approach is two-fold: (1) there is a large body of evidence recommending the use of the Naive Bayes model in word-sense disambiguation (e.g., (Lea-cock, Towell, & Voorhees 1993), (Mooney 1996), <ref> (Ng 1997) </ref>) and (2) unsupervised techniques for parameter estimation, once developed, could be easily applied to other parametric forms in the class of decomposable models. <p> We use Gibbs Sampling to impute the missing values for S and then sample values for the parameters. Gibbs Sampling is often cast as a stochastic version of the EM algorithm (e.g., <ref> (Meng & van Dyk 1997) </ref>). However, in general Gibbs Sampling is applicable to a wider class of problems than the EM algorithm. A Gibbs Sampler generates chains of values for the missing senses S and the parameters p (F i jS) via iterative sampling. <p> However, in our experiments the EM algorithm often converged quite quickly, usually within 20 iterations, to a global maximum. These results suggest that a combination of the EM algorithm and Gibbs Sampling might be appropriate. <ref> (Meng & van Dyk 1997) </ref> propose that the Gibbs Sampler start at the point the Feature Set A Feature Set B Feature Set C Maj.
Reference: <author> Pedersen, T., and Bruce, R. </author> <year> 1997. </year> <title> Distinguishing word senses in untagged text. </title> <booktitle> In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> 197-207. </pages>
Reference-contexts: A comparison of the EM algorithm and two agglomerative clustering algorithms as applied to unsupervised word-sense disambiguation is discussed in (Ped-ersen & Bruce 1997). Using the same data used in this study, <ref> (Pedersen & Bruce 1997) </ref> found that McQuitty's agglomerative algorithm is significantly more accurate for adjectives and verbs while the EM algorithm is significantly more accurate for nouns. These results indicate that McQuitty's analysis, which is based on counts of dissimilar features, is most appropriate for highly skewed data sets.
Reference: <author> Schutze, H. </author> <title> (in press) 1998. Automatic word sense discrimination. </title> <note> Computational Linguistics. </note>
Reference-contexts: A narrow window of context, one or two words to either side, was found to perform better than wider windows. They report an accuracy percentage in the mid-nineties when applied to Dixon, a name found to be quite ambiguous. A recent knowledge-lean approach to sense discrimination is discussed in <ref> (Schutze in press 1998) </ref>. Ambiguous words are clustered into sense groups based on second-order co-occurrences: two instances of an ambiguous word are assigned to the same sense if the words that they co-occur with likewise co-occur with similar words in the training data.
Reference: <author> Yarowsky, D. </author> <year> 1995. </year> <title> Unsupervised word sense disambiguation rivaling supervised methods. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 189-196. </pages>
Reference-contexts: Corpus-based approaches often use supervised learning algorithms with sense-tagged text (e.g., (Leacock, Towell, & Voorhees 1993), (Bruce & Wiebe 1994), (Mooney 1996)) or multi-lingual parallel corpora (e.g., (Gale, Church, & Yarowsky 1992)). An approach that significantly reduces the amount of sense-tagged data required is described in <ref> (Yarowsky 1995) </ref>. Yarowsky suggests a variety of options for automatically seeding a supervised disambiguation algorithm; one is to identify collocations that uniquely distinguish between senses. Yarowsky achieves an accuracy of more than 90% when disambiguating between two senses for twelve different words. <p> The performance of Gibbs Sampling in the current study also falls short of that of McQuitty's for adjectives and verbs which supports the previous conclusion. The EM algorithm is used with a Naive Bayes classifier in <ref> (Gale, Church, & Yarowsky 1995) </ref> to distinguish city names from people's names. A narrow window of context, one or two words to either side, was found to perform better than wider windows.
References-found: 16

