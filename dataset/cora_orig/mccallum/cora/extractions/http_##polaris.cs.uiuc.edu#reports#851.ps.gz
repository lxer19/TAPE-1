URL: http://polaris.cs.uiuc.edu/reports/851.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: ALGORITHMIC DESIGN ON THE CEDAR MULTIPROCESSOR  
Author: Michael Berry Hsin-Chu Chen Efstratios Gallopoulos Ulrike Meier Allan Tuchman Harry Wijshoff Gung-Chung Yang 
Note: FX/8, and Cray X-MP/48 are presented.  
Date: April 1, 1989  
Abstract: The CEDAR system under development in the Center for Supercomputing Research and Development (CSRD) at the University of Illinois at Urbana-Champaign is a clustered shared-memory multiprocessor system which can be used to support parallel processing for a wide range of numerical and non-numerical applications. In this paper, we survey selected algorithms and applications for CEDAR which are under investigation by members of the CSRD Applications Group. Topics include the design of sparse basic linear algebra kernels, the Conjugate Gradient method for linear systems of equations, direct block tridiagonal linear system solvers, a domain decomposition technique for structural mechanics, boundary integral domain decomposition for partial differential equations, parallel algorithms for circuit simulation, and parallelization of a computer graphics technique (ray tracing). Performance results on the 2-cluster CEDAR system, Alliant 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berry and A. Sameh. </author> <title> Multiprocessor schemes for solving block tridi-agonal linear systems. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(3) </volume> <pages> 37-57, </pages> <year> 1988. </year>
Reference-contexts: Spike Algorithm The algorithm we have implemented for the solution of (3) on the 2-cluster CEDAR system, which capitalizes upon the efficiency of the block LU ( LDL T ) factorization and associated block column sweep algorithms for diagonally dominant (symmetric positive definite) block tridiagonal linear systems, is discussed in <ref> [1] </ref>. The derivation is based upon dense block LU ( LDL T ) algorithms which primarily rely on matrix-matrix (BLAS3) primitives for their efficiency. <p> If we assume that matrix A in (3) is diagonally dominant (or symmetric positive definite), then it directly follows that each block tridiagonal matrix ~ A i is also diagonally dominant (or symmetric positive definite). Hence, we can apply a block LU (LDL T ) algorithm (see <ref> [1] </ref>) on each CEDAR cluster in order to solve the above systems with multiple right-hand-sides in parallel. ~ C 1 ~ A 1 A = 13 Given W 1 , Y 1 , g 1 , and g 2 , then form and solve the corresponding 2m fi 2m reduced system. <p> The particular system Ax = f used in experiments below is given by a ij random number 2 [0; 10] ; a ii a ii + j f i 10:0 + random number 2 <ref> [0; 1] </ref> : In Figure 3, we indicate the dedicated elapsed execution (wall-clock) times for solving 50 systems of orders ranging from N = 128 to N = 4096 for the blocksize m = 16.
Reference: [2] <author> B. Buzbee, G. Golub, and C. Nielson. </author> <title> On direct methods for solving Poisson's equation. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 7(4) </volume> <pages> 627-656, </pages> <month> December </month> <year> 1970. </year>
Reference: [3] <author> H.C. Chen. </author> <title> The SAS Domain Decomposition Method for Structural Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1988. </year> <note> Also available as CSRD Technical Report No. 754, </note> <institution> Center for Supercomputing Research and Development. </institution> <month> 30 </month>
Reference-contexts: Algebraically, it is a special matrix decomposition method for handling linear systems, eigenvalue problems, or generalized eigenvalue problems when the coefficient matrices involved, A 2 C nfin say, satisfy the relation A = PAP where P is some reflection matrix (or symmetric signed permutation matrix excluding the identity matrix) [5], <ref> [3] </ref>, [4]. The matrix A is said to be reflexive with respect to P or said to possess the SAS property. The method takes advantage of special properties possessed by the reflexive matrices to decompose a single problem into two or more smaller independent subproblems via orthogonal transformations. <p> The speedups of employing 2, 4, and 8 processors of the Alliant FX/8 in solving the linear subsystems using the Cholesky decomposition method are approximately 1:98, 3:75, and 6:75 respectively. See <ref> [3] </ref> and [4] for more details.
Reference: [4] <author> H.C. Chen and A. Sameh. </author> <title> A matrix decomposition method for or--thotropic elasticity problems. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 10(1) </volume> <pages> 39-64, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: it is a special matrix decomposition method for handling linear systems, eigenvalue problems, or generalized eigenvalue problems when the coefficient matrices involved, A 2 C nfin say, satisfy the relation A = PAP where P is some reflection matrix (or symmetric signed permutation matrix excluding the identity matrix) [5], [3], <ref> [4] </ref>. The matrix A is said to be reflexive with respect to P or said to possess the SAS property. The method takes advantage of special properties possessed by the reflexive matrices to decompose a single problem into two or more smaller independent subproblems via orthogonal transformations. <p> The speedups of employing 2, 4, and 8 processors of the Alliant FX/8 in solving the linear subsystems using the Cholesky decomposition method are approximately 1:98, 3:75, and 6:75 respectively. See [3] and <ref> [4] </ref> for more details.
Reference: [5] <author> H.C. Chen and A. Sameh. </author> <title> Numerical linear algebra algorithms on the CEDAR system. In A.K. </title> <editor> Noor, editor, </editor> <booktitle> Parallel Computations and Their Impact on Mechanics, </booktitle> <pages> pages 101-125, </pages> <booktitle> The American Society of Mechanical Engineers, </booktitle> <year> 1987. </year>
Reference-contexts: Algebraically, it is a special matrix decomposition method for handling linear systems, eigenvalue problems, or generalized eigenvalue problems when the coefficient matrices involved, A 2 C nfin say, satisfy the relation A = PAP where P is some reflection matrix (or symmetric signed permutation matrix excluding the identity matrix) <ref> [5] </ref>, [3], [4]. The matrix A is said to be reflexive with respect to P or said to possess the SAS property. The method takes advantage of special properties possessed by the reflexive matrices to decompose a single problem into two or more smaller independent subproblems via orthogonal transformations.
Reference: [6] <author> J. Dongarra, J. DuCroz, S. Hammarling, and R. Hanson. </author> <title> A Proposal for an Extended Set of Fortran Basic Linear Algebra Subprograms. </title> <type> Technical Report 41, </type> <institution> Argonne National Laboratory, Mathematics and Computer Science Division, </institution> <month> December </month> <year> 1984. </year>
Reference-contexts: Whereas the development of these BLAS primitives for dense computations evolved rapidly and has received a lot of attention in the literature (see <ref> [6] </ref> and [10]), the same cannot be said for sparse computations. This is mainly due to the fact that the computational complexity of sparse computations on parallel/super computers is not very well understood, and that there are no paradigms developed for implementing sparse computations on these architectures.
Reference: [7] <author> I. Duff. </author> <title> A set of Fortran subroutines for sparse unsymmetric linear systems. </title> <type> Technical Report R8730, </type> <institution> Atomic Energy Research Establishment, </institution> <address> Harwell, England, </address> <year> 1977. </year>
Reference: [8] <author> P. Emrath. </author> <title> Xylem: an operating system for the cedar multiprocessor. </title> <journal> IEEE Software, </journal> <volume> 2(4) </volume> <pages> 30-37, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Array data can be prefetched into local buffers in the global interface modules before they are needed. 1 Such as numerical methods for domain decomposition. 3 2.3 Software While CEDAR hardware supports vector instructions and intracluster parallelism, intercluster parallelism is supported by the software of the Xylem operating system kernel <ref> [8] </ref> and the CEDAR Fortran run-time library. Xylem is based on Alliant's operating system, Concentrix, which is in turn based on Berkeley's 4:2 implementation of UNIX. The constructs in CEDAR Fortran give the programmer access to the primary architectural features.
Reference: [9] <author> P. Emrath, D. Padua, and P. Yew. </author> <title> CEDAR architecture and its software. </title> <booktitle> In Hawaii International Conference on System Sciences, Hawaii, </booktitle> <year> 1989. </year> <month> Jan. </month> <pages> 3-6, </pages> <year> 1989. </year>
Reference-contexts: The shared global memory has multiple memory modules so that array data can be stored across all memory modules in order to reduce memory conflicts and to allow data to be accessed in parallel. Long global memory access delays can be offset in two ways (see <ref> [9] </ref> and [19]): 1. For programs with good data locality 1 , the cluster memory with a smaller and faster shared data cache can be used to minimize the number of global memory accesses from a CE. 2.
Reference: [10] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(1) </volume> <pages> 12-48, </pages> <year> 1988. </year>
Reference-contexts: Whereas the development of these BLAS primitives for dense computations evolved rapidly and has received a lot of attention in the literature (see [6] and <ref> [10] </ref>), the same cannot be said for sparse computations. This is mainly due to the fact that the computational complexity of sparse computations on parallel/super computers is not very well understood, and that there are no paradigms developed for implementing sparse computations on these architectures. <p> The derivation is based upon dense block LU ( LDL T ) algorithms which primarily rely on matrix-matrix (BLAS3) primitives for their efficiency. As discussed in <ref> [10] </ref>, such block methods are preferable on hierarchical memory architectures, such as CEDAR, in that high performance can be sustained via improved data locality. <p> The synchronization for forming and solving the reduced system is efficiently 15 handled by Fortran BUSY/WAIT kernels. The particular system Ax = f used in experiments below is given by a ij random number 2 <ref> [0; 10] </ref> ; a ii a ii + j f i 10:0 + random number 2 [0; 1] : In Figure 3, we indicate the dedicated elapsed execution (wall-clock) times for solving 50 systems of orders ranging from N = 128 to N = 4096 for the blocksize m = 16.
Reference: [11] <author> E. Gallopoulos and D. Lee. </author> <title> Boundary integral domain decomposition on hierarchical memory multiprocessors. </title> <booktitle> pages 488-499, ACM Intl. Conf. Supercomputing, </booktitle> <month> July </month> <year> 1988. </year>
Reference: [12] <author> E. Gallopoulos and D. Lee. </author> <title> Fast Laplace solver by boundary integral based domain decomposition. </title> <booktitle> Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December </month> <year> 1987. </year>
Reference: [13] <author> E. Gallopoulos and Y. Saad. </author> <title> Parallel block cyclic reduction algorithm for the fast solution of elliptic equations. </title> <journal> Parallel Comput., </journal> <note> 1987. To appear. </note>
Reference-contexts: A discussion of the mapping and performance of the algorithm for the Alliant FX/8 can be found in <ref> [13] </ref>. A CEDAR version ([14]) is currently under implementation. We also note that a natural 2-cluster decomposition is offered in the solution of the problem with periodic boundary conditions in the one direction. In that case, the resulting system is block tridiagonal and cyclic.
Reference: [14] <author> E. Gallopoulos and A. Sameh. </author> <title> Solving elliptic equations on the Cedar multiprocessor. </title> <editor> In M. H. Wright, editor, </editor> <title> Aspects of Computation on 31 Asynchronous Parallel Processors, </title> <publisher> North-Holland Elsevier, </publisher> <year> 1988. </year> <note> To appear. </note>
Reference: [15] <author> F. G. Gustavson. </author> <title> Two fast algorithms for sparse matrices: multiplication and permuted transposition. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 4(3) </volume> <pages> 250-269, </pages> <year> 1978. </year>
Reference-contexts: This is mainly caused by the fact that sparse matrices are stored in a condensed format in order to minimize the storage requirements. Some of the commonly used storage formats are Coordinate Scheme, Sparse 4 Row (Column)-wise Format <ref> [15] </ref>, and Jagged Diagonal Presentation [21]. On an architecture with a hierarchical memory system, like CEDAR, the data manipulation requirements become even more stringent.
Reference: [16] <author> M. Guzzi. </author> <title> CEDAR Fortran Programmer's Handbook. </title> <type> Technical Report 601, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Variables or arrays may also be declared GLOBAL or CLUSTER. These declarations determine the location attribute of the page where the variables and arrays are allocated (see <ref> [16] </ref>). The request to spawn a new cluster task can be made in CEDAR Fortran by calling the CTSKSTART routine. Synchronization of the spawned task with previously existing tasks can be achieved via the CTSKWAIT kernel. Parameters for the new cluster task may be passed by value or by reference. <p> The Y i 's and W i 's, which contain the m fi m subblocks used to form the reduced system, are not shared across clusters and thus can overwrite the same arrays storing the ~ A i 's. For further descriptions of CEDAR Fortran memory concepts see <ref> [16] </ref> and [19]. 5.3 Two-Cluster CEDAR Results Two versions of the SPIKE algorithm have been implemented on the 2-cluster CEDAR system in which each cluster has 2 computational elements (CE's).
Reference: [17] <author> D. Kuck, E. Davidson, D. Lawrie, and A. Sameh. </author> <title> Parallel supercomputing today and the CEDAR approach. </title> <journal> Science, </journal> <volume> 231(4740) </volume> <pages> 967-974, </pages> <year> 1986. </year>
Reference-contexts: kernels, the Conjugate Gradient method for linear systems of equations, the solution of block tridiagonal linear systems, domain decomposition techniques for structural mechanics and for the solution of partial differential equations, algorithms for circuit simulation, and a computer graphics technique (ray tracing). 2 CEDAR Architecture and Software The CEDAR supercomputer <ref> [17] </ref>, under development at the Center for Supercomputing Research and Development of the University of Illinois at Urbana-Champaign, consists of multiple processor clusters connected through an interconnection network to a globally shared memory. The prototype machine is planned to have 4 clusters, each a slightly modified Alliant FX/8 multiprocessor.
Reference: [18] <author> Nelson Max. </author> <title> Vectorized procedural models for natural terrain: waves and islands in the sunset. </title> <journal> Computer Graphics, </journal> <volume> 15(3) </volume> <pages> 317-324, </pages> <month> July </month> <year> 1981. </year> <booktitle> ACM Siggraph '81 Conference Proceedings. </booktitle>
Reference-contexts: Although the algorithm iterates quite regularly over independent pixels, each primary ray can spawn a tree of secondary rays quite different from its neighbors in depth and shape. For these reasons the brute force algorithm does not yield itself to vectorization. Some vectorization has been done however ([23], <ref> [18] </ref>). For these same reasons, however, the algorithm is embarrassingly easy to parallelize. In the extreme case an array of 512 by 512 processors, each 27 with access to the model description, could compute the entire image in the time it takes to compute only the most complicated pixel.
Reference: [19] <author> R. McGrath and P. Emrath. </author> <title> Using Memory in the CEDAR System. </title> <type> Technical Report 655, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: The shared global memory has multiple memory modules so that array data can be stored across all memory modules in order to reduce memory conflicts and to allow data to be accessed in parallel. Long global memory access delays can be offset in two ways (see [9] and <ref> [19] </ref>): 1. For programs with good data locality 1 , the cluster memory with a smaller and faster shared data cache can be used to minimize the number of global memory accesses from a CE. 2. <p> For further descriptions of CEDAR Fortran memory concepts see [16] and <ref> [19] </ref>. 5.3 Two-Cluster CEDAR Results Two versions of the SPIKE algorithm have been implemented on the 2-cluster CEDAR system in which each cluster has 2 computational elements (CE's).
Reference: [20] <author> U. Meier and A. Sameh. </author> <title> The behavior of conjugate gradient algorithms on a multivector processor with a hierarchical memory. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 24 </volume> <pages> 13-32, </pages> <year> 1988. </year>
Reference-contexts: We can solve (1) iteratively using the classical Conjugate Gradient (CG) algorithm (see <ref> [20] </ref>) which has the form: Given x 0 is an arbitrary initial approximation to x, do until stopping criteria are fulfilled 7 q Ap; ff fl x x + ffp; fl new r T r; fl ; fl fl new The basic iteration of the classical conjugate gradient algorithm (CG) can
Reference: [21] <author> R. Melhem. </author> <title> Parallel solution of linear systems with striped sparse matrices. </title> <journal> Parallel Computing, </journal> <volume> 6(3) </volume> <pages> 165-184, </pages> <year> 1988. </year>
Reference-contexts: This is mainly caused by the fact that sparse matrices are stored in a condensed format in order to minimize the storage requirements. Some of the commonly used storage formats are Coordinate Scheme, Sparse 4 Row (Column)-wise Format [15], and Jagged Diagonal Presentation <ref> [21] </ref>. On an architecture with a hierarchical memory system, like CEDAR, the data manipulation requirements become even more stringent.
Reference: [22] <author> L. Nagel. </author> <title> SPICE2: A Computer Program to Simulate Semiconductor Circuits. </title> <type> Technical Report ERL-M520, </type> <institution> University of California at Berkeley, Berkeley, </institution> <address> CA, </address> <month> May </month> <year> 1975. </year>
Reference-contexts: Based on this conviction, circuit simulation was included as a major research topic in the very early stage of the project, and a public domain circuit simulator called SPICE from UC Berkeley (see <ref> [22] </ref> and [26]) was chosen as our main focus. Mathematically, the problem arising from circuit simulation is a system of stiff Differential-Algebraic Equations (DAE).
Reference: [23] <author> David J. Plunkett and Michael J. Bailey. </author> <title> The vectorization of a ray-tracing algorithm for improved execution speed. </title> <journal> Computer Graphics and Applications, </journal> <volume> 5(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1985. </year>
Reference: [24] <author> David F. Rogers. </author> <title> Procedural Elements for Computer Graphics. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: scaled-up naturally, we expect that our parallel SPICE code running on a two-cluster CEDAR will match that of 1 CPU of a Cray X-MP machine. 9 Ray Tracing on CEDAR 9.1 Background Ray tracing is a computer graphics technique for producing some of the most photorealistic images possible today ([29], <ref> [24] </ref>). The model to be rendered is usually a list of simple geometric primitives. The algorithm iterates through each pixel (sample point) in the output image to compute the color and intensity of that pixel.
Reference: [25] <author> A. Sameh. </author> <title> On two numerical algorithms for multiprocessors. </title> <booktitle> NATO Adv. Res. Workshop on High-Speed Computation, 1983. Series F: Computer and Systems Sciences 7. </booktitle> <pages> 32 </pages>
Reference-contexts: The following SPIKE algorithm (see <ref> [25] </ref>) on the 2-cluster CEDAR system partitions the original block tridiagonal system in (3) into 2 subsystems in which each subsystem has m+1 right-hand-sides (including original right-hand-side f ), where m is the block size of matrix A.
Reference: [26] <author> A. Sangiovanni-Vincentelli. </author> <title> Circuit simulation. Computer Aids for VLSI Circuits, </title> <type> 19-113, </type> <year> 1981. </year>
Reference-contexts: Based on this conviction, circuit simulation was included as a major research topic in the very early stage of the project, and a public domain circuit simulator called SPICE from UC Berkeley (see [22] and <ref> [26] </ref>) was chosen as our main focus. Mathematically, the problem arising from circuit simulation is a system of stiff Differential-Algebraic Equations (DAE).
Reference: [27] <author> R. A. Sweet. </author> <title> A cyclic reduction algorithm for solving block tridiagonal systems of arbitrary dimension. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 14(4) </volume> <pages> 707-720, </pages> <month> September </month> <year> 1977. </year>
Reference: [28] <author> R. A. Sweet. </author> <title> A parallel and vector cyclic reduction algorithm. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 9(4) </volume> <pages> 761-765, </pages> <month> July </month> <year> 1988. </year>
Reference: [29] <author> Turner Whitted. </author> <title> An improved illumination model for shaded display. </title> <journal> Communications of the ACM, </journal> <volume> 23(6) </volume> <pages> 343-349, </pages> <year> 1980. </year>
Reference: [30] <author> H.A.G. Wijshoff. </author> <title> Implementing Sparse BLAS Primitives on Concurrent/Vector Processors: a Case Study. </title> <type> Technical Report 843, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <month> January </month> <year> 1989. </year> <month> 33 </month>
Reference-contexts: The effect of indirect addressing and parallelization were left out of this discussion. For a more detailed account of these issues and experimental data on the implementation of the primitives SpMxM (V) the reader is referred to <ref> [30] </ref>. 4 Conjugate Gradient Method Consider the linear system of n 2 equations Ax = f ; (1) where A is a symmetric positive definite block tridiagonal matrix with tridi-agonal n fi n-diagonal blocks and diagonal off-diagonal blocks.
References-found: 30

