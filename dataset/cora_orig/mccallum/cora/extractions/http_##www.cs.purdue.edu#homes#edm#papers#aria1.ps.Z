URL: http://www.cs.purdue.edu/homes/edm/papers/aria1.ps.Z
Refering-URL: http://www.cs.purdue.edu/research/PaCS/ariadne.html
Root-URL: http://www.cs.purdue.edu
Email: fedm,regog@cs.purdue.edu  
Title: Ariadne: Architecture of a Portable Threads system supporting Mobile Processes  
Author: Edward Mascarenhas Vernon Rego 
Keyword: thread, process, migration, scheduling, parallel, distributed, kernel  
Note: Research supported in part by ONR-9310233, ARO-93G0045 and NATO-CRG900108.  
Date: CSD-TR 95-017  
Address: West Lafayette, IN 47907  
Affiliation: Department of Computer Sciences Purdue University  
Abstract: Threads exhibit a simply expressed and powerful form of concurrency, easily exploitable in applications that run on both uni- and multi-processors, shared- and distributed-memory systems. This paper presents the design and implementation of Ariadne: a layered, C-based software architecture for multi-threaded computing on a variety of platforms. Ariadne is a portable user-space threads system that runs on shared- and distributed-memory multiprocessors. It can be used for parallel and distributed applications. Thread-migration is supported at the application level in homogeneous environments (e.g., networks of SPARCs and Sequent Symmetrys, Intel hypercubes). Threads may migrate between processes to access remote data, preserving locality of reference for computations with a dynamic data space. Ariadne can be tuned to specific applications through a customization layer. Support is provided for scheduling via a built-in or application-specific scheduler, and interfacing with any communications library. Ariadne currently runs on the SPARC (SunOS 4.x and SunOS 5.x), Sequent Symmetry, Intel i860, Silicon Graphics workstation (IRIX), and IBM RS/6000 environments. We present simple performance benchmarks comparing Ariadne to threads libraries in the SunOS 4.x and SunOS 5.x systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. J. Rochkind, </author> <title> Advanced Unix Programming, </title> <publisher> Prentice Hall, </publisher> <year> 1985. </year>
Reference-contexts: For example, a Unix process typically executes within a single thread of control, with direct access to user data in its own address space and access to system data via system calls <ref> [1] </ref>. Interprocess or shared-memory communication may be used to provide data-sharing between two or more processes, allowing for either truly concurrent operation on a multiprocessor, or pseudo-concurrent operation on a uniprocessor. The resulting computation may be viewed as multithreaded, with two or more threads of control. <p> Context-switching involves saving a running thread's register state, including all floating point registers, and the signal mask (if required), and restoring the next runnable thread's corresponding state. For portability, Ariadne accomplishes all this using the standard setjmp () and 5 longjmp () subroutines 1 available in Unix environments <ref> [1] </ref>. Porting Ariadne entails rewriting only thread initialization code about 5 lines of assembly code on a SPARC! * Ease of Use. The basic user interface is simple. Use of Ariadne requires a basic understanding of concurrent programming and C/C++.
Reference: [2] <author> A. S. Birell, </author> <title> `An introduction to programming with threads', </title> <type> Technical Report 35, </type> <institution> DEC Systems Research Center, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: By keeping thread contexts small, context-switching costs between threads can be made a fraction of context-switching costs between processes. Many threads may coexist in a process's address space, allowing for cheap and efficient multithreading. Because threads may execute concurrently, however, programming with threads requires care <ref> [2] </ref>. For example, because a thread is free to access any virtual address, it can access and (inadvertently) destroy the stack of any other thread [3]. Ariadne is a threads library that provides light-weight threads in multiprocessor Unix environments.
Reference: [3] <author> A. S. Tanenbaum, </author> <title> Distributed Operating Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Because threads may execute concurrently, however, programming with threads requires care [2]. For example, because a thread is free to access any virtual address, it can access and (inadvertently) destroy the stack of any other thread <ref> [3] </ref>. Ariadne is a threads library that provides light-weight threads in multiprocessor Unix environments. A single process may host thousands of threads, with low cost thread-creation, context-switching, and synchronization. <p> User-space threads are relatively easy to implement, and can be made portable and flexible; they can be modified without making changes to the OS kernel. Context-switching between user-space threads is at least an order or magnitude faster than kernel traps <ref> [3] </ref>. Other advantages include customizability and scalability. The latter is particularly important because space for tables and stacks can be hard to come by in the kernel. <p> In kernel-space implementations, threads are created and destroyed through kernel calls, with no need for a runtime system <ref> [3] </ref>. Thread-state information is kept within the kernel, and is associated with a thread-entry in a process-table. Calls that may block a thread are implemented as system calls - implying a higher cost than with a corresponding call to a runtime system in user-space threads.
Reference: [4] <author> V. S. Sunderam, </author> <title> `PVM: a Framework for Parallel Distributed Computing', </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <month> 315-339 </month> <year> (1990). </year>
Reference-contexts: Support for parallel programming on shared-memory multiprocessors and in distributed-memory environments is provided via inter-process thread migration. Distributed-Ariadne is realizable through any one of a variety of communication libraries or parallel programming environments, because thread-migration is layered upon communication primitives. Current implementations support PVM <ref> [4] </ref> and Conch [5], though Ariadne's communications interface makes these easily replaceable by other communication libraries. Our original motivation for designing and implementing the Ariadne system was to support process-oriented parallel and distributed simulations [6]. <p> The distributed environment may be configured according to a user's need, perhaps utilizing a given communications library or distributed computing software. A clean and simple interface allows Ariadne to be used flexibly with arbitrary communications software systems. Prototype implementations of distributed-Ariadne include support for PVM 3.3.4 <ref> [4] </ref> and Conch [5]. On a distributed-memory system, Ariadne initializes one or more distributed processes henceforth called D-processes on each processor. Each D-process is given a unique integer id, starting with 0.
Reference: [5] <author> B. Topol, `Conch: </author> <title> Second generation heterogeneous computing', </title> <type> Technical report, </type> <institution> Department of Math and Computer Science, Emory University, </institution> <year> 1992. </year>
Reference-contexts: Support for parallel programming on shared-memory multiprocessors and in distributed-memory environments is provided via inter-process thread migration. Distributed-Ariadne is realizable through any one of a variety of communication libraries or parallel programming environments, because thread-migration is layered upon communication primitives. Current implementations support PVM [4] and Conch <ref> [5] </ref>, though Ariadne's communications interface makes these easily replaceable by other communication libraries. Our original motivation for designing and implementing the Ariadne system was to support process-oriented parallel and distributed simulations [6]. Simulation based on processes are popular because they are known to map easily into implementations [7]. <p> The distributed environment may be configured according to a user's need, perhaps utilizing a given communications library or distributed computing software. A clean and simple interface allows Ariadne to be used flexibly with arbitrary communications software systems. Prototype implementations of distributed-Ariadne include support for PVM 3.3.4 [4] and Conch <ref> [5] </ref>. On a distributed-memory system, Ariadne initializes one or more distributed processes henceforth called D-processes on each processor. Each D-process is given a unique integer id, starting with 0. <p> Both calls are embedded in the a_thread_pack () primitive. In Figure 10 is shown a simplified implementation of the a_migrate () function, using a_thread_pack (), a_thread_unpack (), and related support functions. In this example we utilize the Conch communications library <ref> [5] </ref> for message passing: functions with a c_ prefix are Conch primitives. The thread is first placed into an image from where it is packed into a buffer that is sent across the network (see Figure 10, lines 8 -14). <p> With reliable communication, this scheme has proven simple to implement, and effective. The controller process is chosen arbitrarily: with Conch we use a front-end (see <ref> [5] </ref>) process, and with PVM we use the first process that is created. The termination algorithm is implemented using Ariadne's customization layer. If Ariadne processes frequently create and destroy threads, state transitions may be frequent. Messages enabling such transitions, and messages sent to the controller may result in high traffic.
Reference: [6] <author> J. Sang, E. Mascarenhas, and V. Rego, </author> <title> `Process mobility in distributed-memory simulation systems', </title> <booktitle> Proceeding of the 1993 Winter Simulation Conference, </booktitle> <year> 1993, </year> <pages> pp. 722-730. </pages>
Reference-contexts: Current implementations support PVM [4] and Conch [5], though Ariadne's communications interface makes these easily replaceable by other communication libraries. Our original motivation for designing and implementing the Ariadne system was to support process-oriented parallel and distributed simulations <ref> [6] </ref>. Simulation based on processes are popular because they are known to map easily into implementations [7]. The idea is to identify active objects in a real system and implement them as processes in a program which simulates the system. <p> On the other hand, threads are generally small, and moving thread state from one processor to another - thread-migration is as cheap as message-passing <ref> [6] </ref>. Operating in concert with our motivation for implementing processes in parallel simulations are our requirements of portability and flexibility. Portability is important to us because of the cheap parallelism available on workstation clusters and hardware multiprocessors. <p> At the destination, the application's update function may use the identifier on the stack to locate a copy, replacement, or equivalent object pointer. The application must make such an object available at a destination processor. Thread migration has been shown <ref> [6] </ref> to offer definite advantages in parallel simulation and other applications. Advantages include program simplicity, locality of reference, and one-time transmission. The Distributed Shared Memory (DSM) approach for accessing objects requires a remote procedure call (RPC) like mechanism, with round-trip transmission delay [6]. <p> Thread migration has been shown <ref> [6] </ref> to offer definite advantages in parallel simulation and other applications. Advantages include program simplicity, locality of reference, and one-time transmission. The Distributed Shared Memory (DSM) approach for accessing objects requires a remote procedure call (RPC) like mechanism, with round-trip transmission delay [6]. DSM algorithms require some combination of data replication and/or data migration. Which method works best for remote object access depends on the application and the execution environment. A combination of methods may also work well.
Reference: [7] <author> J. Banks and J. Carson, </author> <title> `Process interaction simulation languages', Simulation, </title> <type> 44(5), </type> <month> 225-235 </month> <year> (1985). </year>
Reference-contexts: Our original motivation for designing and implementing the Ariadne system was to support process-oriented parallel and distributed simulations [6]. Simulation based on processes are popular because they are known to map easily into implementations <ref> [7] </ref>. The idea is to identify active objects in a real system and implement them as processes in a program which simulates the system.
Reference: [8] <author> E. Mascarenhas and V. Rego, </author> <title> `Migrant Threads on Process Farms: Parallel Programming with Ariadne', </title> <note> Technical Report in preparation, </note> <institution> Computer Sciences Department, Purdue University, </institution> <year> 1995. </year>
Reference-contexts: A customizable scheduler is provided simply because different applications may require different scheduling policies. For example, in simulation applications, threads are scheduled based on minimum time-stamp. Though this was not a guiding design consideration, we have found Ariadne to be useful in general multithreaded distributed computing <ref> [8] </ref>. It has been used for parallel applications requiring a large number of threads: particle physics, numerical analysis, and simulation. The choice of granularity, or equivalently, the mapping between a thread and a set of system objects is user-dependent. <p> In this paper we describe the architecture of Ariadne: focus is on design, implementation, and performance. Customization features with the parallel programming model, and detailed examples, are provided in <ref> [8] </ref>. 2 Overview of Ariadne The Ariadne system consists of three layers, as shown in Figure 1. The lower-most layer contains the kernel, the middle layer provides customization and thread support, and the top layer provides custom modules. The kernel layer provides facilities for thread creation, initialization, destruction, and context-switching. <p> The exploitation of threads as active system objects tends to simplify parallel application development, as shown in the quicksort and matrix multiplication examples. Examples illustrating the use of Ariadne in shared- and distributed-memory environments can be found in <ref> [8] </ref>. 28 (1) /* shmatmult.c parallel matrix multiplication program */ #include "aria.h" #include "shm.h" /* shared segment allocator */ #include "mem.h" /* default segment memory manager */ #include "schedshm.h" /* custom scheduler */ #define SHM_KEY 100 /* user shared memory segment */ struct usr_shmem -double *r, *a, *b;-; struct usr_shmem* usr_shmem_ptr;
Reference: [9] <author> D. Bertsekas, </author> <title> Parallel and Distributed Computation: Numerical Methods, </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: The choice of granularity, or equivalently, the mapping between a thread and a set of system objects is user-dependent. For example, in particle-physics applications a thread may represent one or more particles; in Successive Over-Relaxation (SOR) methods <ref> [9] </ref> for solving systems of linear equations, a thread may represent computation at one or more neighboring grid points on a template; in simulations of personal communication systems a thread may represent a mobile phone. Dynamic system objects are naturally representable by mobile threads: computations which can migrate between processes.
Reference: [10] <author> Sun Microsystems, Inc., Sun 825-1250-01, </author> <title> SunOS Programming Utilities and Libries: Lightweight Processes, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: These applications suggest that Ariadne may be used for a variety of parallel and distributed computations. Related Work Threads can be supported in user-space or in kernel-space. Each approach has its advantages and disadvantages. Some examples of user-space threads systems include the Sun-LWP lightweight process library on SunOS 4.x <ref> [10] </ref>, Quick Threads [11], an implementation of the POSIX standard for threads [12], Rex [13], and Cthreads [14]. Because kernel support is not required for implementing user-space threads, they can be implemented on an OS that does not provide threads. <p> The total elapsed time to migrate (send) a thread (message) from process A to process B and back from B to A is shown. setup of red zone stacks protection against stack overflow <ref> [10] </ref>. The Sun-MT library also sets up protected stacks of default size 1MB. Ariadne's context-switching is a little more expensive than context-switching in Sun-LWP and Sun-MT, because Ariadne invokes several functions during a context-switch.
Reference: [11] <author> D. Keppel, </author> <title> `Tools and techniques for building fast portable threads packages', </title> <type> Technical Report UWCSE 93-05-06, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Related Work Threads can be supported in user-space or in kernel-space. Each approach has its advantages and disadvantages. Some examples of user-space threads systems include the Sun-LWP lightweight process library on SunOS 4.x [10], Quick Threads <ref> [11] </ref>, an implementation of the POSIX standard for threads [12], Rex [13], and Cthreads [14]. Because kernel support is not required for implementing user-space threads, they can be implemented on an OS that does not provide threads.
Reference: [12] <author> F. Mueller, </author> <title> `A library implementation of POSIX threads under UNIX', </title> <booktitle> Winter USENIX, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 29-41. </pages>
Reference-contexts: Related Work Threads can be supported in user-space or in kernel-space. Each approach has its advantages and disadvantages. Some examples of user-space threads systems include the Sun-LWP lightweight process library on SunOS 4.x [10], Quick Threads [11], an implementation of the POSIX standard for threads <ref> [12] </ref>, Rex [13], and Cthreads [14]. Because kernel support is not required for implementing user-space threads, they can be implemented on an OS that does not provide threads.
Reference: [13] <author> S. Crane, </author> <title> `The REX lightweight process library', </title> <type> Technical report, </type> <institution> Imperial College of Science and Technology, </institution> <address> London, England, </address> <year> 1993. </year> <month> 30 </month>
Reference-contexts: Related Work Threads can be supported in user-space or in kernel-space. Each approach has its advantages and disadvantages. Some examples of user-space threads systems include the Sun-LWP lightweight process library on SunOS 4.x [10], Quick Threads [11], an implementation of the POSIX standard for threads [12], Rex <ref> [13] </ref>, and Cthreads [14]. Because kernel support is not required for implementing user-space threads, they can be implemented on an OS that does not provide threads.
Reference: [14] <author> K. Schwan, et al., </author> <title> `A C Thread Library for Multiprocessors', </title> <type> Technical Report GIT-ICS-91/02, </type> <institution> Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: Each approach has its advantages and disadvantages. Some examples of user-space threads systems include the Sun-LWP lightweight process library on SunOS 4.x [10], Quick Threads [11], an implementation of the POSIX standard for threads [12], Rex [13], and Cthreads <ref> [14] </ref>. Because kernel support is not required for implementing user-space threads, they can be implemented on an OS that does not provide threads. User-space threads are relatively easy to implement, and can be made portable and flexible; they can be modified without making changes to the OS kernel.
Reference: [15] <author> D. Stein and D. Shah, </author> <title> `Implementing lightweight threads', </title> <booktitle> Proceedings of the 1992 USENIX Summer Conference. SunSoft, </booktitle> <publisher> Inc., </publisher> <year> 1992, </year> <pages> pp. 1-9. </pages>
Reference-contexts: Because kernel-threads can impair performance when present in large numbers, it is usual practice to create only as many kernel-threads as the attainable level of concurrency, given an application and execution environment. Multiprocessor operating systems such as SunOS 5.3 <ref> [15] </ref>, Mach [16], and Topaz [17] provide support for kernel threads. Since kernel-thread management primitives perform poorly in comparison to user-space threads, it is not uncommon to have user-space threads created atop kernel-threads.
Reference: [16] <author> A. Tevanian, R. Rashid, D. Golub, D. Black, E. Cooper, and M Young, </author> <title> `Mach threads and the unix kernel: The battle for control', </title> <booktitle> Proceedings of the 1987 USENIX Summer Conference, </booktitle> <year> 1987, </year> <pages> pp. 185-197. </pages>
Reference-contexts: Because kernel-threads can impair performance when present in large numbers, it is usual practice to create only as many kernel-threads as the attainable level of concurrency, given an application and execution environment. Multiprocessor operating systems such as SunOS 5.3 [15], Mach <ref> [16] </ref>, and Topaz [17] provide support for kernel threads. Since kernel-thread management primitives perform poorly in comparison to user-space threads, it is not uncommon to have user-space threads created atop kernel-threads. Systems exemplifying this approach include the multithreads library on SunOS 5.x (Sun-MT [18]), Mach (CThreads), and Topaz (WorkCrews [19]).
Reference: [17] <author> C. Thacker, L. Stewart, and E. Satterthwaite Jr., `Firefly: </author> <title> A multiprocessor workstation', </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8), </volume> <month> 909-920 </month> <year> (1988). </year>
Reference-contexts: Because kernel-threads can impair performance when present in large numbers, it is usual practice to create only as many kernel-threads as the attainable level of concurrency, given an application and execution environment. Multiprocessor operating systems such as SunOS 5.3 [15], Mach [16], and Topaz <ref> [17] </ref> provide support for kernel threads. Since kernel-thread management primitives perform poorly in comparison to user-space threads, it is not uncommon to have user-space threads created atop kernel-threads. Systems exemplifying this approach include the multithreads library on SunOS 5.x (Sun-MT [18]), Mach (CThreads), and Topaz (WorkCrews [19]).
Reference: [18] <author> M. L. Powell, S. R. Kleiman, S. Barton, D. Shah, D. Stein, and M. Weeks, </author> <title> `SunOS Multi-thread Architecture', </title> <booktitle> Proceedings of the 1991 USENIX Winter Conference. </booktitle> <publisher> Sun Microsystems Inc., </publisher> <year> 1991, </year> <pages> pp. 65-79. </pages>
Reference-contexts: Since kernel-thread management primitives perform poorly in comparison to user-space threads, it is not uncommon to have user-space threads created atop kernel-threads. Systems exemplifying this approach include the multithreads library on SunOS 5.x (Sun-MT <ref> [18] </ref>), Mach (CThreads), and Topaz (WorkCrews [19]). User-threads are multiplexed over available kernel-threads in a shared-memory multiprocessor environment. An idea advocated recently is the combination of kernel-space and user-space threads, to obtain the advantages of both. Scheduler activations [20], and Psyche [21] are examples of such systems. <p> Timings for setjmp and longjmp operations are also reported, to give an indication of the additional work done by Ariadne during context-switching. Synchronization time is measured by forcing two threads to repeatedly synchronize with one another, as done in Reference <ref> [18] </ref>. Timings involving Ariadne were measured on the SPARCclassic (SunOS 4.1), SPARCstation 20 (SunOS 5.3), Sequent Symmetry, IBM RS/6000, and Intel i860 processors. Timings involving Sun-LWP were measured on the SPARCclassic, and timings involving Sun-MT were measured on a SPARCstation 20. All measurements are reported in Table 2.
Reference: [19] <author> M. Vandevoorde and E. Roberts, `Workcrews: </author> <title> An abstraction for controlling parallelism', </title> <booktitle> Int. J. Parallel Program., </booktitle> <volume> 17(4), </volume> <month> 347-366 </month> <year> (1988). </year>
Reference-contexts: Since kernel-thread management primitives perform poorly in comparison to user-space threads, it is not uncommon to have user-space threads created atop kernel-threads. Systems exemplifying this approach include the multithreads library on SunOS 5.x (Sun-MT [18]), Mach (CThreads), and Topaz (WorkCrews <ref> [19] </ref>). User-threads are multiplexed over available kernel-threads in a shared-memory multiprocessor environment. An idea advocated recently is the combination of kernel-space and user-space threads, to obtain the advantages of both. Scheduler activations [20], and Psyche [21] are examples of such systems.
Reference: [20] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> `Scheduler activations: Effective kernel support for the user-level management of parallelism', </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1), </volume> <month> 53-79 </month> <year> (1992). </year>
Reference-contexts: User-threads are multiplexed over available kernel-threads in a shared-memory multiprocessor environment. An idea advocated recently is the combination of kernel-space and user-space threads, to obtain the advantages of both. Scheduler activations <ref> [20] </ref>, and Psyche [21] are examples of such systems. In scheduler-activations, kernel-threads do up-calls into user space providing execution contexts that may be scheduled on the threads system.
Reference: [21] <author> B. D. Marsh, M. L. Scott, T. J. LeBlanc, and E. P. Markatos, </author> <title> `First class user-level threads', </title> <booktitle> Symposium on Operating System Principles, </booktitle> <year> 1991, </year> <pages> pp. 110-121. </pages>
Reference-contexts: User-threads are multiplexed over available kernel-threads in a shared-memory multiprocessor environment. An idea advocated recently is the combination of kernel-space and user-space threads, to obtain the advantages of both. Scheduler activations [20], and Psyche <ref> [21] </ref> are examples of such systems. In scheduler-activations, kernel-threads do up-calls into user space providing execution contexts that may be scheduled on the threads system. When a scheduler activation is about to block, an up-call informs the threads system of this event, enabling the system to schedule another thread.
Reference: [22] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy, </author> <title> `Presto: A system for object-oriented parallel programming', </title> <journal> Software-Practice and Experience, </journal> <volume> 18(8), </volume> <month> 713-732 </month> <year> (1988). </year>
Reference-contexts: To implement these mechanisms, it is necessary to modify parts of the OS kernel. Ariadne is a user-space threads library for parallel and distributed systems. On shared-memory multiprocessors, Ariadne multiplexes user threads on top of multiple processes, as is done in PRESTO <ref> [22] </ref>. An Ariadne thread accesses a remote object in a distributed-memory system by migrating to the location of the object. Other systems providing parallel programming with threads include Amber [23] and Clouds [24]. As in Ariadne, these systems provide threads and objects as basic building blocks.
Reference: [23] <author> J. S. Chase, F. G. Amador, E. D. Lazowska, H. M. Levy, and R. J. Littlefield, </author> <title> `The Amber System: Parallel programming on a network of multiprocessors', </title> <booktitle> Symposium on Operating System Principles, </booktitle> <year> 1989, </year> <pages> pp. 147-158. </pages>
Reference-contexts: On shared-memory multiprocessors, Ariadne multiplexes user threads on top of multiple processes, as is done in PRESTO [22]. An Ariadne thread accesses a remote object in a distributed-memory system by migrating to the location of the object. Other systems providing parallel programming with threads include Amber <ref> [23] </ref> and Clouds [24]. As in Ariadne, these systems provide threads and objects as basic building blocks. By providing a neat interface between the threads library and a parallel programming environment, with requisite support for multithreading, Ariadne encourages tailored solutions to problems. <p> Further, this slot must be reserved for the thread on all machines in the distributed system, just in case the thread makes an appearance at a machine. Such an approach is adopted by the Amber system <ref> [23] </ref>. A significant advantage of this approach is that migration of a thread from one machine to another does not require address translation of pointers on the stack. But this advantage is had at the expense of address space.
Reference: [24] <author> P. Dasgupta, R. Ananthanarayanan, S. Menon, A. Mohindra, and R. Chen, </author> <title> `Distributed programming with Objects and Threads in the Clouds System', </title> <type> Technical Report GIT-GC 91/26, </type> <institution> Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: On shared-memory multiprocessors, Ariadne multiplexes user threads on top of multiple processes, as is done in PRESTO [22]. An Ariadne thread accesses a remote object in a distributed-memory system by migrating to the location of the object. Other systems providing parallel programming with threads include Amber [23] and Clouds <ref> [24] </ref>. As in Ariadne, these systems provide threads and objects as basic building blocks. By providing a neat interface between the threads library and a parallel programming environment, with requisite support for multithreading, Ariadne encourages tailored solutions to problems.
Reference: [25] <author> K. Chung, J. Sang, and V. Rego, `Sol-es: </author> <title> An object-oriented platform for event-scheduled simulations', </title> <booktitle> Proceedings of The Summer Simulation Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The support layer enables end-applications and software systems to use Ariadne 4 via supervised kernel access. The customization layer provides a set of independent components that aid in customizing Ariadne for specialized use; e.g., the use of Ariadne for simulation or real-time applications. The Sol <ref> [25] </ref> simulation library uses Ariadne for scheduling and handling events the next thread to run is a handler for the lowest time-stamped event in the system. The kernel is interfaced with a built-in scheduler based on priority queues.
Reference: [26] <author> A. Aho, J. Hopcroft, and J. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Though this introduces race conditions and complicates the implementation, such complexity is transparent to the user. More details on how race conditions are handled are provided in Section 3. A Simple Example We illustrate the use of Ariadne with a simple example on sorting, using the well-known quicksort algorithm <ref> [26] </ref>. As shown below, the sequential QUICKSORT (S) function operates recursively, to sort a list S of n integers.
Reference: [27] <author> C. Shub, </author> <title> `Native code process-originated migration in a heterogeneous environment', </title> <booktitle> Proceedings of the ACM 18th Annual Computer Science Conference, </booktitle> <year> 1990, </year> <pages> pp. 266-270. 31 </pages>
Reference-contexts: Despite this, process migration has received considerable attention in the research community. Proposals for its application include load sharing, resource sharing, communication overhead reduction, and failure robustness <ref> [27, 28] </ref>. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [29], DEMOS/MP [30]. At the current time, Ariadne supports thread migration on distributed environments of homogeneous machines.
Reference: [28] <author> J. M. Smith, </author> <title> `A Survey of Process Migration Mechanisms', </title> <booktitle> ACM Operating System Review, </booktitle> <month> 28-40 </month> <year> (1988). </year>
Reference-contexts: Despite this, process migration has received considerable attention in the research community. Proposals for its application include load sharing, resource sharing, communication overhead reduction, and failure robustness <ref> [27, 28] </ref>. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [29], DEMOS/MP [30]. At the current time, Ariadne supports thread migration on distributed environments of homogeneous machines.
Reference: [29] <author> M. M. Theimer, K. A. Lantz, and D. R. Cheriton, </author> <title> `Preemptable remote execution facilities for the V-system', </title> <booktitle> Poceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <year> 1985, </year> <pages> pp. 2-12. </pages>
Reference-contexts: Despite this, process migration has received considerable attention in the research community. Proposals for its application include load sharing, resource sharing, communication overhead reduction, and failure robustness [27, 28]. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V <ref> [29] </ref>, DEMOS/MP [30]. At the current time, Ariadne supports thread migration on distributed environments of homogeneous machines. One proposal for thread migration simplifies addressing problems between machines by using a static preallocation of thread address spaces on all machines.
Reference: [30] <author> M. L. Powell and B. P. Miller, </author> <title> `Process migration in DEMOS/MP', </title> <booktitle> Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1983, </year> <pages> pp. 110-119. </pages>
Reference-contexts: Despite this, process migration has received considerable attention in the research community. Proposals for its application include load sharing, resource sharing, communication overhead reduction, and failure robustness [27, 28]. Dynamic migration is usually addressed in the context of distributed operating systems, for example, V [29], DEMOS/MP <ref> [30] </ref>. At the current time, Ariadne supports thread migration on distributed environments of homogeneous machines. One proposal for thread migration simplifies addressing problems between machines by using a static preallocation of thread address spaces on all machines.
Reference: [31] <author> E. Dijkstra and C. Scholten, </author> <title> `Termination detection for Diffusing Computations', </title> <journal> Information Processing Letters, </journal> <volume> 11(1), </volume> <month> 1-4 </month> <year> (1980). </year>
Reference-contexts: The usual strategy is to base termination on a local predicate B i which becomes true when every process P i in the distributed system has terminated. Typical termination algorithms, such as described in <ref> [31] </ref>, assume that processes are configured in special ways rings, trees or predefined cycles, using topological information to aid in termination. A number of different algorithms have been proposed based on diffusing computations, ring structures, value-carrying tokens and time-stamping. A survey of such algorithms can be found in [32].
Reference: [32] <author> M. Raynal, </author> <title> Distributed Algorithms and Protocols, </title> <publisher> John Wiley and Sons Ltd., </publisher> <year> 1988. </year>
Reference-contexts: A number of different algorithms have been proposed based on diffusing computations, ring structures, value-carrying tokens and time-stamping. A survey of such algorithms can be found in <ref> [32] </ref>. In Ariadne, each process determines its own status to be either active or inactive.
Reference: [33] <author> E. Mascarenhas, V. Rego, and V. Sunderam, </author> <title> `Ariadne User Manual', </title> <type> Technical Report 94-081, </type> <institution> Computer Sciences Department, Purdue University, </institution> <year> 1994. </year>
Reference-contexts: After initialization, the user may create Ariadne threads and perform various thread-related tasks. The basic system primitives are shown in Table 1. Details on the use of these and other parallel programming primitives, and illustrative examples, can be found in the Ariadne User Manual <ref> [33] </ref>. The main system thread may use a_exit () to signal termination. If no other user-created threads exist, the threads system terminates and the application may continue to run without Ariadne threads on a return from a_exit (). Otherwise, the main thread blocks until all other user-created threads terminate. <p> This example involves matrix multiplication, and is meant to exhibit several features of Ariadne: shared memory management, scheduler customization, and support for a large number of threads. A similar example illustrating the use of Ariadne in a distributed environment can be found in <ref> [33] </ref>. The program is given two matrices as input, with pointers a and b to the matrices declared on line 8 (see Figure 12). The program is to multiply the matrices and output a product.
Reference: [34] <author> SunSoft, Sun 801-5294-10, </author> <note> SunOS 5.3 Guide to Multithread Programming, </note> <month> November </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: The Thread Create operation entails the creation of threads with and without preallocated stacks, respectively. In Sun-LWP, preallocation is done with the aid of lwp_setstkcache () and lwp_newstk () primitives. With Sun-MT, stacks are cached by default so that repetitive thread creation is cheap <ref> [34] </ref>. The Sun-MT threads library measurements are based on the default stack size allocated by the library. Context Switch time is measured by timing context-switches between two specific threads.
References-found: 34

