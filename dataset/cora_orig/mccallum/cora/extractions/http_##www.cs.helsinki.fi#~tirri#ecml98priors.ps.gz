URL: http://www.cs.helsinki.fi/~tirri/ecml98priors.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Phone: 26,  2  
Title: Bayesian and Information-Theoretic Priors for Bayesian Network Parameters  
Author: Petri Kontkanen Petri Myllymaki Tomi Silander Henry Tirri and Peter Grunwald 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  P.O. Box 94079, NL-1090 GB Amsterdam, The Netherlands  
Affiliation: 1 Complex Systems Computation Group (CoSCo)  Department of Computer Science  CWI, Department of Algorithms and Architectures  
Date: April 24-28,1998.  
Note: To appear in the 10th European Conference on Machine Learning (ECML-98), Chemnitz, Germany,  
Abstract: We consider Bayesian and information-theoretic approaches for determining non-informative prior distributions in a parametric model family. The information-theoretic approaches are based on the recently modified definition of stochastic complexity by Rissanen, and on the Minimum Message Length (MML) approach by Wallace. The Bayesian alternatives include the uniform prior, and the equivalent sample size priors. In order to be able to empirically compare the different approaches in practice, the methods are instantiated for a model family of practical importance, the family of Bayesian networks.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R.A. Baxter and J.O. Oliver. </author> <title> MDL and MML: Similarities and differences. </title> <type> Technical Report 207, </type> <institution> Department of Computer Science, Monash University, </institution> <year> 1994. </year>
Reference-contexts: An MML Prior Minimum Message Length (MML) Inference [14, 15] is based on a similar philosophy to the MDL principle, but there are also some subtle differences which cause the actual formulas used in MDL and MML estimation to differ considerably (see <ref> [1] </ref> for a detailed discussion on this subject).
Reference: 2. <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: that there exists a code that is itself not dependent on any prior distribution of parameters, and which yields even shorter codelengths than the code with lengths log P ev (D), except for the special case where a particular prior (fi) / jI (fi)j 1=2 , the so-called Jeffrey's prior <ref> [2, 3] </ref>, is used for P ev (D). Here jI (fi)j denotes the determinant of the Fisher information matrix I (fi) as defined in [2]. <p> Here jI (fi)j denotes the determinant of the Fisher information matrix I (fi) as defined in <ref> [2] </ref>. <p> In the following all the conditional distributions of the variables, given their parents, are assumed to be multinomial, i.e., X ijq i ~ Multi (1; i q i 1 ; : : : ; i Since the family of Dirichlet distributions is conjugate (see e.g. <ref> [2] </ref>) to the family of multinomials, it would be convenient if we could assume that the prior distributions of the parameters are from this family. <p> From the definition of Dirich-let distributions <ref> [2] </ref>, it is relatively easy to see that both the uniform prior and the ESS priors are Dirichlet distributions (see, e.g., [6]). For the subclass of Bayesian Networks used in our experiments reported in [8], Jeffrey's prior is of the Dirichlet form too.
Reference: 3. <author> B.S. Clarke and A.R. Barron. </author> <title> Jeffrey's prior is asymptotically least favorable under entropy risk. </title> <journal> Journal of Statistical Planning and Inference, </journal> <volume> 41 </volume> <pages> 37-60, </pages> <year> 1994. </year>
Reference-contexts: that there exists a code that is itself not dependent on any prior distribution of parameters, and which yields even shorter codelengths than the code with lengths log P ev (D), except for the special case where a particular prior (fi) / jI (fi)j 1=2 , the so-called Jeffrey's prior <ref> [2, 3] </ref>, is used for P ev (D). Here jI (fi)j denotes the determinant of the Fisher information matrix I (fi) as defined in [2].
Reference: 4. <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: In Section 3 the priors and predictive distributions are instantiated for the special case where the models are defined by a Bayesian network structure with a particular arbitrary, but fixed, topology. For some of the priors, this instantiation has been presented in <ref> [4, 6] </ref>. The contribution of this section is to derive explicit formulas for the MDL and MML priors for the case of Bayesian networks, which involves computing the (expected) Fisher information matrix for Bayesian networks. <p> If the subjective prior is Dirichlet, it is easy to see that the resulting prior is of the Dirichlet form too. Consequently, all the priors used here are Dirichlet, which allows us to derive explicit expressions for P map and P ev , as shown in <ref> [4, 6] </ref>. The computation of the ESS priors for Bayesian Networks can be found in [5]. In the following we show how to compute the Jeffrey's prior (fi) for Bayesian networks, which is required for determining the MDL and MML priors discussed above.
Reference: 5. <author> D. Heckerman. </author> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, Advanced Technology Division, </institution> <address> One Mi-crosoft Way, Redmond, WA 98052, </address> <year> 1996. </year>
Reference-contexts: While all of these satisfy some optimality criterion, in practice they give rise to different predictions. The main purpose of this paper is to compute several different "optimal" prior distributions P for a model class of practical importance, the class of Bayesian networks (see, e.g., <ref> [5] </ref>). In particular, we will compare priors which are in accordance with the Bayesian interpretation of probability, to priors motivated by information-theoretic considerations: a prior based on Rissanen's Minimum Description Length (MDL) principle [10], and a prior based on Wallace & Boulton's Minimum Message Length (MML) principle [15]. <p> We can now interpret D 0 as a priori data that governs how strongly we let our predictions be influenced by the actual sample D (see <ref> [5] </ref>). An MDL Prior Intuitively speaking, the Minimum Description Length (MDL) Principle [10-12] states that the more we are able to compress a set of data, the more we have learned about it, and the better we will be able to predict future data. <p> Consequently, all the priors used here are Dirichlet, which allows us to derive explicit expressions for P map and P ev , as shown in [4, 6]. The computation of the ESS priors for Bayesian Networks can be found in <ref> [5] </ref>. In the following we show how to compute the Jeffrey's prior (fi) for Bayesian networks, which is required for determining the MDL and MML priors discussed above. Let I () denote the indicator function, i.e., I (a; b) = 1 if a = b and 0 otherwise.
Reference: 6. <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In Section 3 the priors and predictive distributions are instantiated for the special case where the models are defined by a Bayesian network structure with a particular arbitrary, but fixed, topology. For some of the priors, this instantiation has been presented in <ref> [4, 6] </ref>. The contribution of this section is to derive explicit formulas for the MDL and MML priors for the case of Bayesian networks, which involves computing the (expected) Fisher information matrix for Bayesian networks. <p> From the definition of Dirich-let distributions [2], it is relatively easy to see that both the uniform prior and the ESS priors are Dirichlet distributions (see, e.g., <ref> [6] </ref>). For the subclass of Bayesian Networks used in our experiments reported in [8], Jeffrey's prior is of the Dirichlet form too. <p> If the subjective prior is Dirichlet, it is easy to see that the resulting prior is of the Dirichlet form too. Consequently, all the priors used here are Dirichlet, which allows us to derive explicit expressions for P map and P ev , as shown in <ref> [4, 6] </ref>. The computation of the ESS priors for Bayesian Networks can be found in [5]. In the following we show how to compute the Jeffrey's prior (fi) for Bayesian networks, which is required for determining the MDL and MML priors discussed above.
Reference: 7. <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> On predictive distributions and Bayesian networks. Technical Report NC-TR-97-032, </title> <booktitle> ESPRIT Working Group on Neural and Computational Learning (NeuroCOLT), </booktitle> <year> 1997. </year>
Reference-contexts: I (fi), which gives us (fi) / jI (fi)j = i=1 q i =1 q i ) 2 l=1 q i l ) 1 / i=1 q i =1 q i ) 2 l=1 q i l ) 1 Details of the derivation of this result can be found in <ref> [7] </ref>. 4 Conclusion In this paper we have discussed various Bayesian and information-theoretic approaches for determining non-informative prior distributions in a parametric model family: Minimum Description Length (MDL) prior, Minimum Message Length (MML) prior, uniform prior, and equivalent sample size priors.
Reference: 8. <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> A comparison of non-informative priors for Bayesian networks. Technical Report NC-TR-98-002, </title> <booktitle> ESPRIT Working Group on Neural and Computational Learning (NeuroCOLT), </booktitle> <year> 1998. </year>
Reference-contexts: For comparing the predictive distributions presented in this paper, we have run an extensive series of tests on real world data, but due to space constrains the results of the tests are presented elsewhere <ref> [8] </ref>. 2 Predictive Distributions and Their Priors We model the problem domain by a set X of m discrete random variables, X = fX 1 ; : : : ; X m g, where a random variable X i can take on any of the values in the set X i <p> From the definition of Dirich-let distributions [2], it is relatively easy to see that both the uniform prior and the ESS priors are Dirichlet distributions (see, e.g., [6]). For the subclass of Bayesian Networks used in our experiments reported in <ref> [8] </ref>, Jeffrey's prior is of the Dirichlet form too. Moreover, we have seen in the previous section that the priors we need for the MML predictive distributions are arrived at by dividing the user's subjective prior by Jeffrey's prior. <p> To be able to study the relevance of the various approaches in practice, we instantiated the methods for the family of Bayesian networks. Our empirical results reported in <ref> [8] </ref> show that while in the case of large training samples all methods give very good results, some of them perform close to optimal already when only a very small amount of training data is available.
Reference: 9. <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Interestingly, while the formula for the MDL predictive distribution involves multiplying P (Djfi) by Jeffrey's prior, the formula for the MML predictive distribution involves dividing P (Djfi) by Jeffrey's prior. 3 Priors for Bayesian Networks A Bayesian (belief) network <ref> [9, 13] </ref> is a representation of a probability distribution over a set of discrete variables, consisting of an acyclic directed graph, where the nodes correspond to domain variables X 1 ; : : : ; X m .
Reference: 10. <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3) </volume> <pages> 223-239 and 252-265, </pages> <year> 1987. </year>
Reference-contexts: In particular, we will compare priors which are in accordance with the Bayesian interpretation of probability, to priors motivated by information-theoretic considerations: a prior based on Rissanen's Minimum Description Length (MDL) principle <ref> [10] </ref>, and a prior based on Wallace & Boulton's Minimum Message Length (MML) principle [15]. Though MDL and MML are similar in spirit, we will see that they do not lead to the same prior distribution.
Reference: 11. <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: Here by the "shortest code" one means the code that gives as short as possible a code length to all possible data sets D. It follows from the Kraft Inequality (see for example <ref> [11] </ref>) that the stochastic complexity S can be written as S = log P sc where P sc is some probability distribution. There are several reasons why S = log P ev (D) = R P (Djfi)(fi)dfi is a good candidate for defining the stochastic complexity explicitly [11]. <p> (see for example <ref> [11] </ref>) that the stochastic complexity S can be written as S = log P sc where P sc is some probability distribution. There are several reasons why S = log P ev (D) = R P (Djfi)(fi)dfi is a good candidate for defining the stochastic complexity explicitly [11].
Reference: 12. <author> J. Rissanen. </author> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(1) </volume> <pages> 40-47, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: There are several reasons why S = log P ev (D) = R P (Djfi)(fi)dfi is a good candidate for defining the stochastic complexity explicitly [11]. Recently, however, Rissanen <ref> [12] </ref> has shown that there exists a code that is itself not dependent on any prior distribution of parameters, and which yields even shorter codelengths than the code with lengths log P ev (D), except for the special case where a particular prior (fi) / jI (fi)j 1=2 , the so-called <p> Here jI (fi)j denotes the determinant of the Fisher information matrix I (fi) as defined in [2]. In this case it can be shown <ref> [12] </ref> that under suitable technical conditions, P ev and P sc asymptotically coincide: log P sc (D) = log P ev (D) + o (1); (5) which means that from the MDL point of view, the optimal predictive distribution is obtained by using P ev with Jeffrey's prior.
Reference: 13. <author> R.D. Shachter. </author> <title> Probabilistic inference and influence diagrams. </title> <journal> Operations Research, </journal> <volume> 36(4) </volume> <pages> 589-604, </pages> <month> July-August </month> <year> 1988. </year>
Reference-contexts: Interestingly, while the formula for the MDL predictive distribution involves multiplying P (Djfi) by Jeffrey's prior, the formula for the MML predictive distribution involves dividing P (Djfi) by Jeffrey's prior. 3 Priors for Bayesian Networks A Bayesian (belief) network <ref> [9, 13] </ref> is a representation of a probability distribution over a set of discrete variables, consisting of an acyclic directed graph, where the nodes correspond to domain variables X 1 ; : : : ; X m .
Reference: 14. <author> C.S. Wallace and D.M Boulton. </author> <title> An information measure for classifiation. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: An MML Prior Minimum Message Length (MML) Inference <ref> [14, 15] </ref> is based on a similar philosophy to the MDL principle, but there are also some subtle differences which cause the actual formulas used in MDL and MML estimation to differ considerably (see [1] for a detailed discussion on this subject).
Reference: 15. <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: In particular, we will compare priors which are in accordance with the Bayesian interpretation of probability, to priors motivated by information-theoretic considerations: a prior based on Rissanen's Minimum Description Length (MDL) principle [10], and a prior based on Wallace & Boulton's Minimum Message Length (MML) principle <ref> [15] </ref>. Though MDL and MML are similar in spirit, we will see that they do not lead to the same prior distribution. In Section 2 we introduce the general setting of the problem by discussing and motivating the priors we will use. <p> An MML Prior Minimum Message Length (MML) Inference <ref> [14, 15] </ref> is based on a similar philosophy to the MDL principle, but there are also some subtle differences which cause the actual formulas used in MDL and MML estimation to differ considerably (see [1] for a detailed discussion on this subject). <p> Omitting all mathematical details (which can be found in <ref> [15] </ref>), the MML-optimal model fi mml (D) is defined by fi mml (D) = arg max P (Djfi)P (fi) jI (fi)j 1=2 ; (6) where jI (fi)j is the determinant of the Fisher information matrix.
References-found: 15

