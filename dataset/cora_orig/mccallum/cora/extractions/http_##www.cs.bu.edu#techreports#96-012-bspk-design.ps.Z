URL: http://www.cs.bu.edu/techreports/96-012-bspk-design.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: amr@das.harvard.edu  heddaya@cs.bu.edu  
Title: Communicable Memory and Lazy Barriers for Bulk Synchronous Parallelism in BSPk  
Author: Amr Fahmy 
Note: Also available as TR-09-96,  
Date: September 20, 1996  
Address: Abdelsalam Heddaya  Boston University  
Affiliation: Aiken Computation Lab Harvard University  Computer Science Dept.  Harvard University, Aiken Computation Lab.  
Pubnum: BU-CS-96-012  
Abstract: Communication and synchronization stand as the dual bottlenecks in the performance of parallel systems, and especially those that attempt to alleviate the programming burden by incurring overhead in these two domains. We formulate the notions of communicable memory and lazy barriers to help achieve efficient communication and synchronization. These concepts are developed in the context of BSPk, a toolkit library for programming networks of workstations|and other distributed memory architectures in general|based on the Bulk Synchronous Parallel (BSP) model. BSPk emphasizes efficiency in communication by minimizing local memory-to-memory copying, and in barrier synchronization by not forcing a process to wait unless it needs remote data. Both the message passing (MP) and distributed shared memory (DSM) programming styles are supported in BSPk. MP helps processes efficiently exchange short-lived unnamed data values, when the identity of either the sender or receiver is known to the other party. By contrast, DSM supports communication between processes that may be mutually anonymous, so long as they can agree on variable names in which to store shared temporary or long-lived data. fl Research supported in part by NSF grant MCB-9527181. y Some of this work was done while this author was on sabbatical leave at Harvard University's Aiken Computation Lab and Dept. of Biological Chemistry and Molecular Pharmacology. fl This document's URL is hhttp://www.cs.bu.edu/techreports/96-012-bspk-design.ps.Zi. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Selim G. Akl. </author> <title> Parallel sorting algorithms. </title> <publisher> Academic Press, </publisher> <address> Orlando, Florida, </address> <year> 1985. </year>
Reference-contexts: BSP grew out of an earlier effort to validate the practicality of the PRAM algorithmic model [22], for which most of the existing parallel algorithms have been designed <ref> [1, 16] </ref>, but whose realization on practical hardware suffers from serious performance handicaps. 3 While the spirited defense of the PRAM mounted by some theoreticians [23] deserves a fair experimental shake, the merits of BSP as a model do not hinge exclusively on its success in mediating efficiently between the PRAM
Reference: [2] <author> R.D. Alpert, C. Dubnicki, E.W. Felten, and K. Li. </author> <title> Design and implementation of NX message passing using Shrimp virtual memory mapped communication. </title> <booktitle> In Proc. of the 1996 International Conference on Parallel Processing, </booktitle> <address> Bloomingdale, Illinois, </address> <month> Aug. 12-16 </month> <year> 1996. </year>
Reference-contexts: This minimizes the CPU and memory overheads, and permits more flexible congestion control protocols. User-level communication systems that successfully employ similar ideas to achieve very low overhead include U-Net [24] and NX/Shrimp <ref> [2] </ref>. Both of these systems aim to support parallel applications that are able to present their data units for communication in a suitable form. BSPk does precisely that, and therefore is poised to take direct advantage of the efficiencies afforded by such systems as U-Net and NX/Shrimp. <p> Therefore, all of our communication primitives are defined to respect only the ordering of communication events, relative to the boundaries of the supersteps. To achieve this, we employ 7 Examples of such communication subsystems include U-Net [24] and NX/Shrimp <ref> [2] </ref>. 6 typedef struct f char [24] GlobVarName; /* Optional */ int GlobVarInx; /* Optional */ int NumElems; int ElemSize; int OwnerPID; int SenderPID; /* Used only in receive buffers. */ void * BasePtr; int Offset; g comemPtrType; 7 a message counting scheme that enables data messages to be used to <p> An effort has been mounted to standardize on a library called BSP Worldwide [12]. We discuss each of these in turn in this section. Systems that support high performance user-level communication include U-Net [24] and NX <ref> [2] </ref>. The main problems we found with other BSP systems concern superfluous restrictions and inefficiencies in the use of memory for communication, and extra waiting for synchronization even when the communication pattern is known to the application program in advance.
Reference: [3] <author> R.H. Bisseling and W.F. McColl. </author> <title> Scientific computing on bulk synchronous parallel architec tures (short version). </title> <editor> In B. Pehrson and I. Simon, editors, </editor> <booktitle> Proc. 13th IFIP World Computer 12 Congress (Volume 1). </booktitle> <publisher> Elsevier, </publisher> <year> 1994. </year> <note> Full paper available as technical report 836, </note> <institution> Dept. of Math, Univ. of Utrecht, </institution> <address> Holland. </address>
Reference-contexts: Another important consideration has to do with process control utilities, and I/O. BSPk provides neither, which means that we must implement BSPk within an environment that does. A number of algorithms and applications have been developed directly in the BSP model <ref> [3, 11, 20] </ref>. However, we view BSPk more as a general purpose foundation, on top of which it is possible to design and implement support for global distributed data structures [10], uniform address-space shared memory [17], multi-threading, and PRAM programming [23].
Reference: [4] <author> T.E. Cheatham, A. Fahmy, D.C. Stefanescu, and L.G. Valiant. </author> <title> Bulk synchronous parallel com puting: A paradigm for transportable software. </title> <booktitle> In Proc. 28th Hawaii International Conference on System Sciences, </booktitle> <address> Maui, Hawaii, </address> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: For an example of a BSP algorithm that demonstrates the power of the model, see the provably optimally portable 4 matrix multiplication one described in <ref> [4] </ref>. BSP suffers from the apparent problem of having to incur the full cost of barrier synchronization, even when weaker ordering would be acceptable by the application. The impact of this shortcoming is worsened when L is large, hence the possible need for specialized synchronization hardware. <p> In many parallel algorithms this number is known, an example, among many others, is the BSP algorithm for matrix multiplication <ref> [4] </ref>. Using this observation, BSPk supports three primitives to achieve the effect of barriers. At the end of each superstep bspk sync () is called.
Reference: [5] <author> D.D. Clark and D.L. Tennenhouse. </author> <title> Architectural considerations for a new generation of pro tocols. </title> <booktitle> In Proc. ACM SICGOMM'90, </booktitle> <pages> pages 200-208, </pages> <month> Sep. </month> <year> 1990. </year> <note> Published as special issue of Computer Communication Review, vol. 20, number 4. </note>
Reference-contexts: Our approach in BSPk is consistent with the recent trend in the operating system and data communication research communities towards application level communication, synchronization, and resource management. We base our communicable memory on the design principle of application level framing (ALF) proposed in <ref> [5] </ref>. ALF stipulates that the application break up its communication units into packet frames that can be sent over the network without being copied, segmented or sequenced. This minimizes the CPU and memory overheads, and permits more flexible congestion control protocols. <p> Table 1 lists all the important components of the BSPk interface. The BSPk memory allocation and handling primitives provide the communicable memory (comem) abstraction which appears to the user program as contiguous regions of dynamic heap memory, yet is internally represented so as to permit application level framing (ALF) <ref> [5] </ref>, and hence zero-copy communication when sent across the network. <p> both MP and DSM may profitably be mixed even in the same program. 4 Communicable Memory The BSPk memory allocation and handling primitives provide the communicable memory abstraction which appears to the user program as contiguous heap memory, yet is internally represented so as to permit application level framing (ALF) <ref> [5] </ref>, and hence zero-copy communication when sent across the network. This can be achieved because the user computes in the same communicable memory that serves as the communication buffer.
Reference: [6] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Thus, BSP ignores the topology of the network in the sense that all nodes are considered equidistant, and disregards any other special purpose hardware that might exist in the machine, except to the extent that it influences the values of L and g. The competing LogP model <ref> [6] </ref> adds one more parameter|the minimum intersend interval|to the mix, which, in the authors' view, does not change the essence of the model. For an example of a BSP algorithm that demonstrates the power of the model, see the provably optimally portable 4 matrix multiplication one described in [4].
Reference: [7] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C: Version 1.0. </title> <type> Technical report, </type> <institution> Univ. of California, Berkeley, EECS, Computer Science Division, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Hence, we view the layering from top to bottom as follows: parallel application, BSPk, user-level communication subsystem, exokernel. There exists a number of programming systems that implement the BSP algorithmic model in ways that differ from ours in BSPk. These include: the Split-C programming language <ref> [7] </ref>, the Oxford BSP library [19], and the Green BSP library [13]. Split-C and Oxford BSP support distributed shared memory, while Green BSP provides for message passing. <p> The barrier synchronization for this example has zero cost. 7 Other BSP Systems The Bulk Synchronous Parallel computing model (BSP) is currently supported by Split-C <ref> [7] </ref>, by the Oxford BSP library [19], and by the Green BSP library [13]. An effort has been mounted to standardize on a library called BSP Worldwide [12]. We discuss each of these in turn in this section.
Reference: [8] <author> D.R. Engler, M.F. Kaashoek, and J. O'Toole Jr. Exokernel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proc. 15th ACM Symp. on Operating System Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: BSPk does precisely that, and therefore is poised to take direct advantage of the efficiencies afforded by such systems as U-Net and NX/Shrimp. At the bottommost layer, an ex-okernel <ref> [8] </ref> would export hardware resources directly to user-level communication subsystems, thus enabling them to realize their promised performance without jeopardizing protection. Hence, we view the layering from top to bottom as follows: parallel application, BSPk, user-level communication subsystem, exokernel.
Reference: [9] <author> A. Fahmy and A. Heddaya. BSPk: </author> <title> Low overhead communication constructs and logical bar riers for Bulk Synchronous Parallel programming. </title> <journal> Bulletin of the IEEE Technical Committee on Operating Systems and Application Environments (TCOS), </journal> <volume> 8(2) </volume> <pages> 27-32, </pages> <month> Summer </month> <year> 1996. </year> <note> (Ex-tended abstract). </note>
Reference-contexts: 1 Introduction The economical programming of parallel machines is hampered by lack of consensus on a universal intermediate machine model that provides: (1) efficiency of implementation: (2) cost model simplicity and accuracy, (3) architecture independence, and (4) programming convenience. In an extended abstract of this paper <ref> [9] </ref>, we describe the design of BSPk 1 , a library and toolkit that we propose as a candidate that meets the above requirements. Here, we elaborate on our design rationale and include detailed implementation notes and BSPk programming examples.
Reference: [10] <author> A.F. Fahmy and R.A. Wagner. </author> <title> On the distribution and transportation of data structures in parallel and distributed systems. </title> <type> Technical Report TR-27-95, </type> <institution> Harvard University, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: By making comem pointers usable as is from any node in the system, copying can be carried out without any need for pointer modifications of transmitted pointer structures. Thus, comem lays the ground work for supporting efficient communication of complicated data structures <ref> [10] </ref>. Implementing Comem An example implementation of a comem is shown in Figures 2 and 3. A comem region consists of a set of constant size packet frames, and an index array whose length is determined at the time of invocation of bspk malloc. <p> A number of algorithms and applications have been developed directly in the BSP model [3, 11, 20]. However, we view BSPk more as a general purpose foundation, on top of which it is possible to design and implement support for global distributed data structures <ref> [10] </ref>, uniform address-space shared memory [17], multi-threading, and PRAM programming [23]. Each of these abstractions has demonstrable benefits that can be reaped only if BSPk proves in practice to be as efficient a testbed for their support as we hope.
Reference: [11] <author> A.V. Gerbessiotis and L.G. Valiant. </author> <title> Direct bulk-synchronous parallel algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22, </volume> <year> 1994. </year>
Reference-contexts: Another important consideration has to do with process control utilities, and I/O. BSPk provides neither, which means that we must implement BSPk within an environment that does. A number of algorithms and applications have been developed directly in the BSP model <ref> [3, 11, 20] </ref>. However, we view BSPk more as a general purpose foundation, on top of which it is possible to design and implement support for global distributed data structures [10], uniform address-space shared memory [17], multi-threading, and PRAM programming [23].
Reference: [12] <author> J.W. Goudreau, J.M.D. Hill, K. Lang, B. McColl, S.B. Rao, D.C. Stefanescu, T. Suel, and T. Tsantilas. </author> <title> A proposal for the BSP Worldwide standard library (preliminary version). </title> <type> Technical report, </type> <institution> Oxford Parallel Group, Oxford Univ., </institution> <month> April </month> <year> 1996. </year> <note> Available as URL http://www.bsp-worldwide.org/standard/stand2.htm. </note>
Reference-contexts: These include: the Split-C programming language [7], the Oxford BSP library [19], and the Green BSP library [13]. Split-C and Oxford BSP support distributed shared memory, while Green BSP provides for message passing. An effort has recently been mounted to standardize on a library called BSP Worldwide <ref> [12] </ref>, which supports both MP and DSM, but without integrating the underlying memory management support as BSPk does. BSPk differs from all of these systems in supporting zero-copy communication and message counting logical barriers. A more detailed discussion of each of these libraries can be found in Section 7. <p> An effort has been mounted to standardize on a library called BSP Worldwide <ref> [12] </ref>. We discuss each of these in turn in this section. Systems that support high performance user-level communication include U-Net [24] and NX [2]. <p> BSP Worldwide. This is a nascent effort to propose a standard BSP library interface, based on the experience through Oxford BSP, and Green BSP libraries, and some of the applications developed using them <ref> [12] </ref>. The above comparison between BSPk and these two libraries applies to BSP-WW, except for BSP-WW's support for both MP and DSM. 8 Discussion In order to implement BSPk, and realize the performance gains that its design promises, we need a user-level communication system such as U-Net [24].
Reference: [13] <author> M.W. Goudreau, K. Lang, S.B. Rao, and T. Tsantilas. </author> <title> The Green BSP library. </title> <type> Technical Report CS-TR-95-11, </type> <institution> Dept. of Computer Science, Univ. of Central Florida, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: There exists a number of programming systems that implement the BSP algorithmic model in ways that differ from ours in BSPk. These include: the Split-C programming language [7], the Oxford BSP library [19], and the Green BSP library <ref> [13] </ref>. Split-C and Oxford BSP support distributed shared memory, while Green BSP provides for message passing. An effort has recently been mounted to standardize on a library called BSP Worldwide [12], which supports both MP and DSM, but without integrating the underlying memory management support as BSPk does. <p> The barrier synchronization for this example has zero cost. 7 Other BSP Systems The Bulk Synchronous Parallel computing model (BSP) is currently supported by Split-C [7], by the Oxford BSP library [19], and by the Green BSP library <ref> [13] </ref>. An effort has been mounted to standardize on a library called BSP Worldwide [12]. We discuss each of these in turn in this section. Systems that support high performance user-level communication include U-Net [24] and NX [2].
Reference: [14] <author> A. Heddaya and A.F. Fahmy. </author> <title> OS support for portable bulk synchronous parallel programs. </title> <type> Technical Report BU-CS-94-013, </type> <institution> Boston Univ., Computer Science Dept., </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: A more detailed discussion of each of these libraries can be found in Section 7. Also, an attempt towards defining operating system requirements for the support of the BSP model can be found in <ref> [14] </ref>.
Reference: [15] <author> Jon Hill. </author> <title> The Oxford BSP toolset and profiling system. Source code available through http://www.comlab.ox.ac.uk/oucl/oxpara/, Aug. </title> <year> 1996. </year>
Reference-contexts: This is done in log p steps where p is the number of processes. Subsequently, BSPk broadcasts the sum in another log p steps. The Oxford BSP library <ref> [15] </ref> achieves the effect of barriers by performing a similar computation on every superstep, whether it is needed or not, by calculating the number of messages that each process will receive during the current superstep using a hypercube-embedded tree. 6 Example BSPk Programs The following examples can be viewed as program
Reference: [16] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: BSP grew out of an earlier effort to validate the practicality of the PRAM algorithmic model [22], for which most of the existing parallel algorithms have been designed <ref> [1, 16] </ref>, but whose realization on practical hardware suffers from serious performance handicaps. 3 While the spirited defense of the PRAM mounted by some theoreticians [23] deserves a fair experimental shake, the merits of BSP as a model do not hinge exclusively on its success in mediating efficiently between the PRAM
Reference: [17] <author> Richard P. LaRowe and Carla Schlatter Ellis. </author> <title> Experimental comparison of memory manage ment policies for NUMA multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: A number of algorithms and applications have been developed directly in the BSP model [3, 11, 20]. However, we view BSPk more as a general purpose foundation, on top of which it is possible to design and implement support for global distributed data structures [10], uniform address-space shared memory <ref> [17] </ref>, multi-threading, and PRAM programming [23]. Each of these abstractions has demonstrable benefits that can be reaped only if BSPk proves in practice to be as efficient a testbed for their support as we hope.
Reference: [18] <author> F. Thomas Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes, volume I. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>
Reference-contexts: The quantitative aspects of BSP deserve brief mention, so as to permit the reader to judge the level of its simplicity in comparison to the simpler PRAM on the one hand, and to the more complex communication-topology-aware models <ref> [18] </ref> on the other. Two parameters capture the communication and synchronization costs involved in running BSP programs [21].
Reference: [19] <author> R. Miller. </author> <title> A library for bulk synchronous parallel programming. </title> <booktitle> In Proc. British Comp. Soc. Parallel Processing Specialist Group Workshop on General Purpose Parallel Computing, </booktitle> <month> Dec. 22 </month> <year> 1993. </year>
Reference-contexts: There exists a number of programming systems that implement the BSP algorithmic model in ways that differ from ours in BSPk. These include: the Split-C programming language [7], the Oxford BSP library <ref> [19] </ref>, and the Green BSP library [13]. Split-C and Oxford BSP support distributed shared memory, while Green BSP provides for message passing. <p> The barrier synchronization for this example has zero cost. 7 Other BSP Systems The Bulk Synchronous Parallel computing model (BSP) is currently supported by Split-C [7], by the Oxford BSP library <ref> [19] </ref>, and by the Green BSP library [13]. An effort has been mounted to standardize on a library called BSP Worldwide [12]. We discuss each of these in turn in this section. Systems that support high performance user-level communication include U-Net [24] and NX [2].
Reference: [20] <author> M. Nibhanupudi, C. Norton, and B. Szymanski. </author> <title> Plasma simulation on networks of workstations using the bulk synchronous parallel model. </title> <booktitle> In Proc. International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <address> Athens, GA, </address> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Another important consideration has to do with process control utilities, and I/O. BSPk provides neither, which means that we must implement BSPk within an environment that does. A number of algorithms and applications have been developed directly in the BSP model <ref> [3, 11, 20] </ref>. However, we view BSPk more as a general purpose foundation, on top of which it is possible to design and implement support for global distributed data structures [10], uniform address-space shared memory [17], multi-threading, and PRAM programming [23].
Reference: [21] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Comm. ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Here, we elaborate on our design rationale and include detailed implementation notes and BSPk programming examples. The bulk synchronous parallel (BSP) model <ref> [21] </ref> represents the conceptual foundation of BSPk, enabling the latter to serve as a host for direct programming, as a run-time system, and as a target for optimizing compilation of increasingly high level languages. <p> Other systems that implement the BSP programming model are reviewed and contrasted with BSPk in Section 7, which is followed by a general discussion section. 2 The BSP Model The bulk synchronous parallel (BSP) algorithmic model <ref> [21] </ref> forms the basis of the BSPk programming model. A BSP computation is structured as a sequence of supersteps each followed by a barrier synchronization. <p> Two parameters capture the communication and synchronization costs involved in running BSP programs <ref> [21] </ref>.
Reference: [22] <author> L.G. Valiant. </author> <title> General purpose parallel architectures. </title> <editor> In Jan van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume I. </booktitle> <publisher> Elsevier & MIT Press, </publisher> <address> Amsterdam, New York and Cambridge (Mass.), </address> <year> 1990. </year>
Reference-contexts: BSP grew out of an earlier effort to validate the practicality of the PRAM algorithmic model <ref> [22] </ref>, for which most of the existing parallel algorithms have been designed [1, 16], but whose realization on practical hardware suffers from serious performance handicaps. 3 While the spirited defense of the PRAM mounted by some theoreticians [23] deserves a fair experimental shake, the merits of BSP as a model do
Reference: [23] <author> U. Vishkin. </author> <title> A case for the PRAM as a standard programmer's model. </title> <editor> In F. Meyer auf der Heide, B. Monien, and A.L. Rosenberg, editors, </editor> <booktitle> Parallel Architectures and their Efficient Use, </booktitle> <pages> pages 11-19. </pages> <publisher> Springer-Verlag, </publisher> <month> Nov. 11-13 </month> <year> 1992. </year>
Reference-contexts: of an earlier effort to validate the practicality of the PRAM algorithmic model [22], for which most of the existing parallel algorithms have been designed [1, 16], but whose realization on practical hardware suffers from serious performance handicaps. 3 While the spirited defense of the PRAM mounted by some theoreticians <ref> [23] </ref> deserves a fair experimental shake, the merits of BSP as a model do not hinge exclusively on its success in mediating efficiently between the PRAM and the hardware. <p> However, we view BSPk more as a general purpose foundation, on top of which it is possible to design and implement support for global distributed data structures [10], uniform address-space shared memory [17], multi-threading, and PRAM programming <ref> [23] </ref>. Each of these abstractions has demonstrable benefits that can be reaped only if BSPk proves in practice to be as efficient a testbed for their support as we hope.
Reference: [24] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> a user-level network interface for parallel and distributed computing. </title> <booktitle> In Proc. 15th ACM Symp. on Operating System Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> Dec. </month> <year> 1995. </year> <month> 14 </month>
Reference-contexts: This minimizes the CPU and memory overheads, and permits more flexible congestion control protocols. User-level communication systems that successfully employ similar ideas to achieve very low overhead include U-Net <ref> [24] </ref> and NX/Shrimp [2]. Both of these systems aim to support parallel applications that are able to present their data units for communication in a suitable form. BSPk does precisely that, and therefore is poised to take direct advantage of the efficiencies afforded by such systems as U-Net and NX/Shrimp. <p> Therefore, all of our communication primitives are defined to respect only the ordering of communication events, relative to the boundaries of the supersteps. To achieve this, we employ 7 Examples of such communication subsystems include U-Net <ref> [24] </ref> and NX/Shrimp [2]. 6 typedef struct f char [24] GlobVarName; /* Optional */ int GlobVarInx; /* Optional */ int NumElems; int ElemSize; int OwnerPID; int SenderPID; /* Used only in receive buffers. */ void * BasePtr; int Offset; g comemPtrType; 7 a message counting scheme that enables data messages to <p> Therefore, all of our communication primitives are defined to respect only the ordering of communication events, relative to the boundaries of the supersteps. To achieve this, we employ 7 Examples of such communication subsystems include U-Net <ref> [24] </ref> and NX/Shrimp [2]. 6 typedef struct f char [24] GlobVarName; /* Optional */ int GlobVarInx; /* Optional */ int NumElems; int ElemSize; int OwnerPID; int SenderPID; /* Used only in receive buffers. */ void * BasePtr; int Offset; g comemPtrType; 7 a message counting scheme that enables data messages to be used to trigger the beginning of new supersteps, <p> An effort has been mounted to standardize on a library called BSP Worldwide [12]. We discuss each of these in turn in this section. Systems that support high performance user-level communication include U-Net <ref> [24] </ref> and NX [2]. The main problems we found with other BSP systems concern superfluous restrictions and inefficiencies in the use of memory for communication, and extra waiting for synchronization even when the communication pattern is known to the application program in advance. <p> The above comparison between BSPk and these two libraries applies to BSP-WW, except for BSP-WW's support for both MP and DSM. 8 Discussion In order to implement BSPk, and realize the performance gains that its design promises, we need a user-level communication system such as U-Net <ref> [24] </ref>. This system will permit the efficient implementation of both comem and message counting barriers. Another important consideration has to do with process control utilities, and I/O. BSPk provides neither, which means that we must implement BSPk within an environment that does.
References-found: 24

