URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/98-05.tr.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: frapaport|ehrlichg@cs.buffalo.edu  
Title: A COMPUTATIONAL THEORY OF VOCABULARY ACQUISITION  
Author: William J. Rapaport and Karen Ehrlich 
Note: Science).  
Date: April 29, 1998  
Web: http://www.cs.buffalo.edu/sneps/  
Address: NY 14260-2000  
Affiliation: Department of Computer Science and Center for Cognitive Science State University of New York at Buffalo, Buffalo,  
Abstract: As part of an interdisciplinary project to develop a computational cognitive model of a reader of narrative text, we are developing a computational theory of how natural-language-understanding systems can automatically acquire new vocabulary by determining from context the meaning of words that are unknown, misunderstood, or used in a new sense. `Context' includes surrounding text, grammatical information, and background knowledge, but no external sources. Our thesis is that the meaning of such a word can be determined from context, can be revised upon further encounters with the word, converges to a dictionary-like definition if enough context has been provided and there have been enough exposures to the word, and eventually settles down to a steady state that is always subject to revision upon further encounters with the word. The system is being implemented in the SNePS knowledge-representation and reasoning system. This essay is forthcoming as a chapter in Iwanska, ucja, & Shapiro, Stuart C. (1999), Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language (Menlo Park, CA/Cambridge, MA: AAAI Press/MIT Press). The present version is Technical Report 98-?? (Buffalo: SUNY Buffalo Department of Computer Science) and Technical Report 98-? (Buffalo: SUNY Buffalo Center for Cognitive 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berwick, Robert C. </author> <year> (1983), </year> <title> Learning Word Meanings from Examples, </title> <booktitle> Proceedings of the 8th International Joint Conference on Artificial Intelligence (IJCAI-83; Karlsruhe, </booktitle> <address> West Germany) (Los Altos, CA: </address> <publisher> William Kaufmann): </publisher> <pages> 459-461. </pages>
Reference: <author> Campbell, Alistair E. </author> <year> (1996), </year> <title> Resolution of the Dialect Problem in Communication through Ontological Mediation, </title> <booktitle> Proceedings of the AAAI-96 Workshop on Detecting, Preventing, and Repairing Human-Machine Miscommunication (Portland, </booktitle> <address> OR); hhttp://www.cs.uwm.edu/~mcroy/mnm.htmli. </address>
Reference: <author> Campbell, Alistair E., & Shapiro, Stuart C. </author> <year> (1995), </year> <title> Ontological Mediation: An Overview, Proceedings of the IJCAI Workshop on Basic Ontological Issues for Knowledge Sharing (Montreal, Quebec, Canada). The Compact Edition of the Oxford English Dictionary: Complete Text Reproduced Micrographically (New York: </title> <publisher> Oxford University Press, </publisher> <year> 1971). </year>
Reference: <author> Cravo, Maria R., & Martins, Joao P. </author> <year> (1993), </year> <title> SNePSwD: A Newcomer to the SNePS Family, </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 5: </journal> <pages> 135-148. </pages>
Reference-contexts: We have modified SNeBR so that it now automatically creates a revision of the culprit belief (see Ehrlich 1995 for the algorithms). The current version of SNePS allows ordinary deductive reasoning. SNePSwD also allows default reasoning <ref> (Cravo & Martins 1993) </ref>. Default rules allow the system to infer probable conclusions in the absence of specific information to the contrary.
Reference: <editor> Duchan, Judith F.; Bruder, Gail A.; & Hewitt, Lynne E. (eds.) </editor> <booktitle> (1995), Deixis in Narrative: A Cognitive Science Perspective (Hillsdale, </booktitle> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates). </publisher> <editor> Ehrlich, Karen (1995), </editor> <title> Automatic Vocabulary Expansion through Narrative Context, </title> <type> Technical Report 95-09 (Buffalo: </type> <institution> SUNY Buffalo Department of Computer Science). </institution>
Reference-contexts: 1 Introduction As part of an interdisciplinary project to develop a computational cognitive model of a reader of narrative text <ref> (Duchan et al. 1995) </ref>, we are developing a computational theory of how a natural-language-understanding system (either human or artificial) can automatically acquire new vocabulary by determining from context the meaning of words that are unknown, misunderstood, or used in a new sense (Ehrlich 1995), 1 where `context' includes surrounding text, grammatical
Reference: <author> Ehrlich, Karen, & Rapaport, William J. </author> <year> (1997), </year> <title> A Computational Theory of Vocabulary Expansion, </title> <booktitle> Proceedings of the 19th Annual Conference of the Cognitive Science Society (Stanford University) (Mahwah, </booktitle> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 205-210. </pages>

Reference: <author> Granger, Richard H. </author> <year> (1977), </year> <title> Foul-Up: A Program that Figures Out Meanings of Words from Context, </title> <booktitle> Proceedings of the 5th International Joint Conference on Artificial Intelligence (IJCAI-77, </booktitle> <publisher> MIT) (Los Altos, </publisher> <address> CA: </address> <publisher> William Kaufmann): </publisher> <pages> 67-68. </pages> <note> Haas, </note> <author> Norman, & Hendrix, </author> <title> Gary (1983), Learning by Being Told: Acquiring Knowledge for Information Management, </title> <editor> in Riszard S. Michalski, Jaime G. Carbonell, & Tom M. Mitchell (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Palo Alto, </booktitle> <address> CA: </address> <publisher> Tioga Press): </publisher> <pages> 405-428. </pages>
Reference: <author> Harman, </author> <title> Gilbert (1974), Meaning and Semantics, </title> <editor> in Milton K. Munitz & Peter K. Unger (eds.), </editor> <title> Semantics and Philosophy (New York: </title> <publisher> New York University Press): </publisher> <month> 1-16. </month> <title> Harman, Gilbert (1982), Conceptual Role Semantics, </title> <journal> Notre Dame Journal of Formal Logic 23: </journal> <pages> 242-256. </pages>
Reference: <author> Hastings, Peter M. </author> <year> (1994), </year> <title> Automatic Acquisition of Word Meanings from Natural Language Contexts, </title> <type> Ph.D. </type> <institution> disseration (Ann Arbor: University of Michigan Department of Computer Science and Engineering). </institution>
Reference-contexts: means (cf., e.g., Haas & Hendrix 1983, Zernik & Dyer 1987), (3) it could (either by itself or with the help of another system) figure out the meaning by finding a synonym for the unknown word or by locating the meaning of the word in some taxonony or other schema <ref> (cf., e.g., Hastings 1994) </ref>, or (4) it could figure out the meaning entirely from the context in which the word was encountered, with no outside help and no pre-established schema within which to fit it. The last of these is what our system does.
Reference: <author> Hastings, Peter M., & Lytinen, Steven L. </author> <year> (1994a), </year> <title> The Ups and Downs of Lexical Acquisition, </title> <booktitle> Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94, </booktitle> <address> Seattle) (Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press): </publisher> <pages> 754-759. </pages>
Reference: <author> Hastings, Peter M., & Lytinen, Steven L. </author> <year> (1994b), </year> <title> Objects, Actions, Nouns, and Verbs, </title> <booktitle> Proceedings of the 16th Annual Conference of the Cognitive Science Society (Hillsdale, </booktitle> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 397-402. </pages> <month> Higginbotham, </month> <title> James (1989), Elucidations of Meaning, </title> <booktitle> Linguistics and Philosophy 12: </booktitle> <pages> 465-517. </pages>
Reference: <author> Hunt, Alan, & Koplas, Geoffrey D. </author> <year> (1997), </year> <title> Definitional Vocabulary Acquisition Using Natural Language Processing and a Dynamic Lexicon, </title> <type> SNeRG Technical Note 29 (Buffalo: </type> <institution> SUNY Buffalo Department of Computer Science, SNePS Research Group). </institution>
Reference-contexts: subclass, parts, ownership, or skills, and then defines the word for the listener in terms of the listener's already-known words. (See Campbell & Shapiro 1995, Campbell 1996, and hhttp://www.cs.buffalo.edu/~aec/OM/index.htmli.) 9 Future Work Much remains to be done: We are now developing and using a grammar for natural-language input and output <ref> (Hunt & Koplas 1997) </ref>, as well as for potential use in determining meanings via morphological (and perhaps etymological) information. We plan to expand our algorithms for verb acquisition, and to investigate adjective acquisition.
Reference: <author> Johnson-Laird, Philip N. </author> <year> (1987), </year> <title> The Mental Representation of the Meanings of Words, </title> <editor> in Alvin I. Goldman (ed.), </editor> <booktitle> Readings in Philosophy and Cognitive Science (Cambridge, </booktitle> <address> MA: </address> <publisher> MIT Press, </publisher> <year> 1993): </year> <pages> 561-583. </pages>
Reference: <author> Kiersey, David M. </author> <year> (1982), </year> <title> Word Learning with Hierarchy Guided Inference, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence (AAAI-82, </booktitle> <address> Pittsburgh) (Los Altos, CA: </address> <publisher> Morgan Kaufmann): </publisher> <pages> 172-178. </pages>
Reference-contexts: New information compatible with all facets of the model could be added; conflicting facets of the model could be replaced with new information. Those with lower verbal skills tended to use the model holistically, with new compatible information broadening or restricting the domain <ref> (Kiersey 1982) </ref> and incompatible information causing the rejection of the model (and perhaps the adoption of a different model).
Reference: <editor> Malory, Sir Thomas (1470), Le Morte Darthur, ed. by R. M. </editor> <address> Lumiansky (New York: </address> <publisher> Collier Books, </publisher> <year> 1982). </year>
Reference: <author> Martins, Joao, & Cravo, Maria R. </author> <year> (1991), </year> <title> How to Change Your Mind, </title> <booktitle> Nous 25: </booktitle> <pages> 537-551. </pages>
Reference: <author> Martins, Joao, & Shapiro, Stuart C. </author> <year> (1988), </year> <title> A Model for Belief Revision, </title> <booktitle> Artificial Intelligence 35: </booktitle> <pages> 25-79. </pages> <address> Meinong, </address> <month> Alexius </month> <year> (1910), </year> <title> Uber Annahmen, </title> <booktitle> 3rd edition, based on the 2nd edition of 1910 (Leipzig: </booktitle> <publisher> Verlag von Johann Ambrosius Barth, </publisher> <year> 1928). </year>
Reference-contexts: When combinations of asserted propositions lead to a contradiction, SNeBR, the SNePS belief-revision package, allows Cassie to remove from the inconsistent context one or more of those propositions <ref> (Martins & Shapiro 1988) </ref>. Once a premise is no longer asserted, the conclusions that depended on it are no longer asserted in that context. <p> Assertions having no kn cat attached are beliefs that the system has derived. (SNeBR assigns each proposition an origin tag as either a hypothesis (a proposition received as input) or a derivation. If the proposition is derived, information about the hypotheses used in the derivation is also stored <ref> (Martins & Shapiro 1988) </ref>.
Reference: <author> Nutter, J. </author> <title> Terry (1983), Default Reasoning Using Monotonic Logic: A Modest Proposal, </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence (AAAI-83; Washington, </booktitle> <address> DC) (Los Altos, CA: </address> <publisher> Morgan Kaufmann): </publisher> <pages> 297-300. </pages> <note> 21 Quillian, </note> <author> M. </author> <title> Ross (1968), Semantic Memory, </title> <editor> in Marvin Minsky (ed.), </editor> <booktitle> Semantic Information Processing (Cambridge, </booktitle> <address> MA: </address> <publisher> MIT Press): </publisher> <pages> 227-270. </pages>
Reference: <author> Quillian, M. </author> <title> Ross (1969), The Teachable Language Comprehender: A Simulation Program and Theory of Language, </title> <journal> Communications of the ACM 12: </journal> <pages> 459-476. </pages>
Reference: <author> Rapaport, William J. </author> <year> (1981), </year> <title> How to Make the World Fit Our Language: An Essay in Meinongian Semantics, </title> <booktitle> Grazer Philosophische Studien 14: </booktitle> <pages> 1-21. </pages>
Reference-contexts: included scent in their definitions; however, scent was not mentioned in the Malory text, so there was no reason they should have included it, nor is it necessary for understanding the text. 4 3 Fundamental Theses We hold that linguistic contexts (and their mental counterparts) can provide meanings for expressions <ref> (Rapaport 1981) </ref>. This includes non-referential expressions: If `unicorn' is an unknown term, and we read that a unicorn looks like a horse with one horn, then we can hypothesize a meaning for `unicorn'e.g., a one-horned, horselike animaleven though the term `unicorn' does not refer (cf. Meinong 1910: 25f, Rapaport 1981).
Reference: <author> Rapaport, William J. </author> <year> (1988), </year> <title> Syntactic Semantics: Foundations of Computational Natural-Language Understanding, </title> <editor> in James H. Fetzer (ed.), </editor> <booktitle> Aspects of Artificial Intelligence (Dordrecht, </booktitle> <publisher> Holland: Kluwer Academic Publishers): </publisher> <editor> 81-131; reprinted in Eric Dietrich (ed.), </editor> <title> Thinking Computers and Virtual Persons: </title> <booktitle> Essays on the Intentionality of Machines (San Diego: </booktitle> <publisher> Academic Press, </publisher> <year> 1994): </year> <pages> 225-273. </pages>
Reference: <author> Rapaport, William J. </author> <year> (1991), </year> <title> Predication, Fiction, </title> <journal> and Artificial Intelligence, </journal> <volume> Topoi 10: </volume> <pages> 79-111. </pages>
Reference-contexts: When our system encounters a contradiction derived from combining story information with background knowledge, it must decide which of the premises leading to the contradiction should be revised <ref> (cf. Rapaport 1991, Rapaport & Shapiro 1995) </ref>. To facilitate such decisions, each of the assertions that we build into the system's knowledge base and each of the assertions in the story is tagged with a knowledge category (kn cat). <p> The distinction between assertions tagged intrinsic and those tagged life is not exactly analogous to distinctions between necessary and contingent properties, or between analytic and synthetic definitions, 10 but a loose analogy to such distinctions may capture some sense of the distinction being drawn here <ref> (cf. Rapaport 1991, Rapaport & Shapiro 1995) </ref>. At present, all the kn cats (except for questionable) are assigned by a human at the time the proposition is input. <p> The assignment of the kn cat story could be handled automatically: The system would simply include it as a part of each proposition built from the parse of a sentence in a story <ref> (as in Rapaport 1991, Rapaport & Shapiro 1995) </ref>.
Reference: <author> Rapaport, William J. </author> <year> (1995), </year> <title> Understanding Understanding: Syntactic Semantics and Computational Cognition, </title> <editor> in James E. Tomberlin (ed.), </editor> <booktitle> AI, Connectionism, and Philosophical Psychology, Philosophical Perspectives, Vol. </booktitle> <address> 9 (Atascadero, CA: Ridgeview): </address> <note> 49-88; to be reprinted in Clark, </note> <editor> Andy, & Toribio, </editor> <booktitle> Josefa (1998), Language and Meaning in Cognitive Science: Cognitive Issues and Semantic Theory, Artificial Intelligence and Cognitive Science: Conceptual Issues, Vol. </booktitle> <address> 4 (Hamden, CT: </address> <publisher> Garland). </publisher>
Reference: <author> Rapaport, William J.; Segal, Erwin M.; Shapiro, Stuart C.; Zubin, David A.; Bruder, Gail A.; Duchan, Judith F.; Almeida, Michael J.; Daniels, Joyce H.; Galbraith, Mary M.; Wiebe, Janyce M.; & Yuhan, </author> <title> Albert Hanyong (1989a), Deictic Centers and the Cognitive Structure of Narrative Comprehension, </title> <type> Technical Report 89-01 (Buffalo: </type> <institution> SUNY Buffalo Department of Computer Science). </institution>
Reference: <author> Rapaport, William J.; Segal, Erwin M.; Shapiro, Stuart C.; Zubin, David A.; Bruder, Gail A.; Duchan, Judith F.; & Mark, David M. </author> <year> (1989b), </year> <title> Cognitive and Computer Systems for Understanding Narrative Text, </title> <type> Technical Report 89-07 (Buffalo: </type> <institution> SUNY Buffalo Department of Computer Science). </institution>
Reference: <author> Rapaport, William J., & Shapiro, Stuart C. </author> <year> (1995), </year> <title> Cognition and Fiction, </title> <editor> in Judith F. Duchan, Gail A. Bruder, </editor> & <publisher> Lynne E. </publisher>
Reference: <editor> Hewitt (eds.), </editor> <booktitle> Deixis in Narrative: A Cognitive Science Perspective (Hillsdale, </booktitle> <address> NJ: </address> <institution> Lawrence Erlbaum Associates): </institution> <note> 107-128; an abridged and slightly edited version appears as Rapaport, </note> <author> William J., & Shapiro, Stuart C. </author> <title> (forthcoming), Cognition and Fiction: An Introduction, </title> <editor> in Ashwin Ram & Kenneth Moorman (eds.), </editor> <title> Understanding Language Understanding: Computational Models of Reading (Cambridge, </title> <address> MA: </address> <publisher> MIT Press). </publisher>
Reference: <author> Rapaport, William J.; Shapiro, Stuart C.; & Wiebe, Janyce M. </author> <year> (1997), </year> <title> Quasi-Indexicals and Knowledge Reports, </title> <booktitle> Cognitive Science 21: </booktitle> <pages> 63-107. </pages>
Reference: <author> Rosch, </author> <title> Eleanor (1978), Principles of Categorization, </title> <editor> in Eleanor Rosch & Barbara B. Lloyd (eds.), </editor> <title> Cognition and Categorization (Hillsdale, </title> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 27-48. </pages>
Reference: <author> Schank, Roger C., & Abelson, Robert P. </author> <year> (1977), </year> <title> Scripts, Plans, Goals and Understanding (Hillsdale, </title> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates). </publisher>
Reference-contexts: Granger's Foul-Up (1977) was designed specifically to work in conjunction with a Script Applier Mechanism <ref> (Schank & Abelson 1977) </ref> to allow stories of events to be understood in terms of stereotypical event sequences despite the presence of unknown words.
Reference: <author> Searle, John R. </author> <year> (1980), </year> <title> Minds, Brains, and Programs, </title> <booktitle> Behavioral and Brain Sciences 3: </booktitle> <pages> 417-457. </pages>
Reference-contexts: The reader's interpretation is a mapping from the sentences (considered as a syntactic domain) to the reader's mental concepts (considered as the semantic domain). (In this way, semantics arises out of syntax, and Searle's Chinese-Room Argument <ref> (Searle 1980) </ref> can be refuted; see Rapaport 1988, 1995.) We can then take the meaning of a word (as understood by the readeri.e., a cognitive agent) to be its position in a network of words, propositions, and other concepts (Quillian 1968, 1969).
Reference: <author> Segal, Erwin M. </author> <year> (1995), </year> <title> A Cognitive-Phenomenological Theory of Fictional Narrative, </title> <editor> in Judith F. Duchan, Gail A. Bruder, & Lynne E. Hewitt (eds.), </editor> <booktitle> Deixis in Narrative: A Cognitive Science Perspective (Hillsdale, </booktitle> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 61-78. </pages> <month> Sellars, </month> <title> Wilfrid (1955), Some Reflections on Language Games, </title> <booktitle> in Science, Perception and Reality (London: </booktitle> <publisher> Routledge & Kegan Paul, </publisher> <year> 1963): </year> <pages> 321-358. </pages>
Reference: <author> Shapiro, Stuart C. </author> <year> (1979), </year> <title> The SNePS Semantic Network Processing System, </title> <editor> in Nicholas Findler (ed.), </editor> <title> Associative Networks: Representation and Use of Knowledge by Computers (New York: </title> <publisher> Academic Press): </publisher> <pages> 179-203. </pages>
Reference: <author> Shapiro, Stuart C. </author> <year> (1982), </year> <title> Generalized Augmented Transition Network Grammars for Generation from Semantic Networks, </title> <journal> American Journal of Computational Linguistics 8: </journal> <pages> 12-25. </pages>
Reference: <author> Shapiro, Stuart C. </author> <year> (1989), </year> <title> The CASSIE Projects: An Approach to Natural Language Competence, </title> <booktitle> Proceedings of the 4th Portugese Conference on Artificial Intelligence (Lisbon) (Berlin: </booktitle> <publisher> Springer-Verlag): </publisher> <pages> 362-380. </pages>
Reference: <author> Shapiro, Stuart C., & Rapaport, William J. </author> <year> (1987), </year> <title> SNePS Considered as a Fully Intensional Propositional Semantic Network, </title> <editor> in Nick Cercone & Gordon McCalla (eds.), </editor> <booktitle> The Knowledge Frontier: Essays in the Representation of Knowledge (New York: Springer-Verlag): 262-315; shorter version appeared in Proceedings of the 5th National Conference on Artificial Intelligence (AAAI-86, </booktitle> <address> Philadelphia) (Los Altos, CA: </address> <note> Morgan Kaufmann): 278-283; revised version of the shorter version appears as A Fully Intensional Propositional Semantic Network, </note> <editor> in Leslie Burkholder (ed.), </editor> <booktitle> Philosophy and the Computer (Boulder, </booktitle> <publisher> CO: Westview Press, </publisher> <year> 1992): </year> <pages> 75-91. </pages>
Reference: <author> Shapiro, Stuart C., & Rapaport, William J. </author> <year> (1991), </year> <title> Models and Minds: Knowledge Representation for Natural-Language Competence, </title> <editor> in Robert Cummins & John Pollock (eds.), </editor> <booktitle> Philosophy and AI: Essays at the Interface (Cambridge, </booktitle> <address> MA: </address> <publisher> MIT Press): </publisher> <pages> 215-259. </pages>
Reference-contexts: When our system encounters a contradiction derived from combining story information with background knowledge, it must decide which of the premises leading to the contradiction should be revised <ref> (cf. Rapaport 1991, Rapaport & Shapiro 1995) </ref>. To facilitate such decisions, each of the assertions that we build into the system's knowledge base and each of the assertions in the story is tagged with a knowledge category (kn cat). <p> The distinction between assertions tagged intrinsic and those tagged life is not exactly analogous to distinctions between necessary and contingent properties, or between analytic and synthetic definitions, 10 but a loose analogy to such distinctions may capture some sense of the distinction being drawn here <ref> (cf. Rapaport 1991, Rapaport & Shapiro 1995) </ref>. At present, all the kn cats (except for questionable) are assigned by a human at the time the proposition is input. <p> The assignment of the kn cat story could be handled automatically: The system would simply include it as a part of each proposition built from the parse of a sentence in a story <ref> (as in Rapaport 1991, Rapaport & Shapiro 1995) </ref>.
Reference: <author> Shapiro, Stuart C., & Rapaport, William J. </author> <year> (1992), </year> <title> The SNePS Family, Computers and Mathematics with Applications 23: </title> <editor> 243-275; reprinted in Fritz Lehmann (ed.), </editor> <booktitle> Semantic Networks in Artificial Intelligence (Oxford: </booktitle> <publisher> Pergamon Press, </publisher> <year> 1992): </year> <pages> 243-275. </pages> <note> 22 Shapiro, </note> <author> Stuart C., & Rapaport, William J. </author> <year> (1995), </year> <title> An Introduction to a Computational Reader of Narrative, </title> <editor> in Judith F. Duchan, Gail A. Bruder, & Lynne E. Hewitt (eds.), </editor> <booktitle> Deixis in Narrative: A Cognitive Science Perspective (Hillsdale, </booktitle> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 79-105. </pages> <month> Siskind, </month> <title> Jeffrey Mark (1996), A Computational Study of Cross-Situational Techniques for Learning Word-to-Meaning Mappings, </title> <journal> Cognition 61: </journal> <pages> 39-91. </pages>
Reference: <author> Srihari, Rohini K. </author> <year> (1991), </year> <title> PICTION: A System that Uses Captions to Label Human Faces in Newspaper Photographs, </title> <booktitle> Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91, </booktitle> <address> Anaheim) (Menlo Park, CA: </address> <publisher> AAAI Press/MIT Press): </publisher> <pages> 80-85. </pages>
Reference: <author> Srihari, Rohini K., & Rapaport, William J. </author> <year> (1989), </year> <title> Extracting Visual Information From Text: Using Captions to Label Human Faces in Newspaper Photographs, </title> <booktitle> Proceedings of the 11th Annual Conference of the Cognitive Science Society (Ann Arbor, </booktitle> <address> MI) (Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 364-371. </pages>
Reference-contexts: In most cases of reading, any mental representation (including imagery) would be produced by the text, so visual perception of a real-world situation does not arise, except for illustrated texts. Although Cassie does not use illustrations, she could in principle <ref> (Srihari & Rapaport 1989, Srihari 1991) </ref>. Siskind's system begins with a mapping between a whole meaning and a whole utterance, and infers mappings between their parts. Cassie already has both of those mappings and seeks to infer definition-style relations between the unknown word and the rest of the knowledge base.
Reference: <author> Sternberg, Robert J. </author> <year> (1987), </year> <title> Most Vocabulary is Learned from Context, </title> <editor> in Margaret G. McKeown & Mary E. Curtis (eds.), </editor> <title> The Nature of Vocabulary Acquisition (Hillsdale, </title> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates): </publisher> <pages> 89-105. </pages>
Reference: <author> Winston, Patrick Henry (1975), </author> <title> Learning Structural Descriptions from Examples, </title> <editor> in Patrick Henry Winston (ed.), </editor> <title> The Psychology of Computer Vision (New York: </title> <publisher> McGraw-Hill): </publisher> <editor> 157-209; reprinted in Ronald J. Brachman & Hector J. Levesque (eds.), </editor> <booktitle> Readings in Knowledge Representation (Los Altos, </booktitle> <address> CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1985): </year> <pages> 141-168. </pages> <editor> Zadrozny, Wlodek, & Jensen, Karen (1991), </editor> <booktitle> Semantics of Paragraphs, Computational Linguistics 17: </booktitle> <pages> 171-209. </pages>
Reference: <author> Zernik, Uri, & Dyer, Michael G. </author> <year> (1987), </year> <title> The Self-Extending Phrasal Lexicon, </title> <booktitle> Computational Linguistics 13: </booktitle> <pages> 308-327. 23 </pages>
Reference-contexts: A complete lexicon cannot be manually encoded, nor could it contain new words or new meanings <ref> (Zernik & Dyer 1987) </ref>. Text-understanding, message-processing, and information-extraction systems need to be robust in the presence of unknown expressions, especially systems using unconstrained input text and operating independently of human intervention, such as intelligent agents.
References-found: 43

