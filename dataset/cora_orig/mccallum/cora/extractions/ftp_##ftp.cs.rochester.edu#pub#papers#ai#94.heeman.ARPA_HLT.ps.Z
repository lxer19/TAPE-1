URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/94.heeman.ARPA_HLT.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/heeman/papers.html
Root-URL: 
Email: fheeman,jamesg@cs.rochester.edu  
Title: Tagging Speech Repairs  
Author: Peter A. Heeman and James Allen 
Address: Rochester, New York, 14627  
Affiliation: Department of Computer Science University of Rochester  
Abstract: This paper describes a method of detecting speech repairs that uses a part-of-speech tagger. The tagger is given knowledge about category transitions for speechrepairs, and so is able to mark a transition either as a likely repair or as fluent speech. Other contextual clues, such as editing terms, word fragments, and word matchings, are also factored in by modifying the transition probabilities. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. F. and Schubert, L. K. </author> <year> (1991). </year> <title> The TRAINS project. </title> <type> Technical Report 382, </type> <institution> Department of Computer Science, University of Rochester. </institution>
Reference-contexts: However, they do not address the problem of determining the correction or distinguishing modification repairs from abridged repairs. 3. The Corpus As part of the TRAINS project <ref> (Allen and Schubert, 1991) </ref>, which is a long term research project to build a conversationally proficient planning assistant, we are collecting a corpus of problem solving dialogs.
Reference: <author> Bear, J., Dowding, J., and Shriberg, E. </author> <year> (1992). </year> <title> Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. </title> <booktitle> In Proceedings of the 30 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 56-63. </pages>
Reference-contexts: But this fl Presented at the ARPA Workshop on Human Language Technology, Princeton NJ, March 1994. does not mean that syntactic clues cannot be used. One powerful predictor of modification repairs is the presence of a syntactic anomaly <ref> (c.f. Bear, Dowding and Shriberg, 1992) </ref> at the interruption point. <p> Hin-dle achieved a correction recall rate of 97% on his corpus; however, this was obtained by assuming that speech repairs were marked by an explicit edit signal and with part-of-speech tags externally supplied. The SRI group <ref> (Bear, Dowding and Shriberg, 1992) </ref> removed the assumption of an explicit edit signal, and employed simple pattern matching techniques for detecting and correcting modification repairs (they removed all utterances with abridged repairs from their corpus).
Reference: <author> Brieman, L., Friedman, J. H., and Olshen, R. A. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, Monterrey, </publisher> <address> CA. </address>
Reference-contexts: For instance, a fragment is twice as likely to be part of an abridged repair than it is to be part of a modification repair. One way to exploit these clues is to try to learn how to combine them, using a technique such as CART <ref> (Brieman, Friedman and Olshen, 1984) </ref>. However, a more intuitive approach is to adjust the transition probabilities for a modification repair to better reflect the more specific information that is known.
Reference: <author> Church, K. </author> <year> (1988). </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Preceedings of the 2nd Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages>
Reference: <author> Dowding, J., Gawron, J. M., Appelt, D., Bear, J., Cherny, L., Moore, R., and Moran, D. </author> <year> (1993). </year> <title> Gemini: A natural language system for spoken-language understanding. </title> <booktitle> In Proceedings of the 31 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 54-61. </pages>
Reference-contexts: They also tried combining syntactic and semantic knowledge in a parser-first approachfirst try to parse the input and if that fails, invoke repair strategies based on their pattern matching technique. In a test set of 756 utterances containing 26 repairs <ref> (Dowding et al., 1993) </ref>, they obtained a detection recall rate of 42% and a precision of 84.6%; for correction, they obtained a recall rate of 30% and a precision rate of 62%. Nakatani and Hirschberg (1993) investigated using acoustic information to detect the interruption point of speech repairs.
Reference: <author> Gross, D., Allen, J., and Traum, D. </author> <year> (1992). </year> <title> The TRAINS 91 dialogues. </title> <type> Trains Technical Note 92-1, </type> <institution> Department of Computer Science, University of Rochester. </institution>
Reference-contexts: The dialogs involve two participants, one who is playing the role of a user and has a certain task to accomplish, and another, who is playing the role of the system by acting as a planning assistant <ref> (Gross, Allen and Traum, 1992) </ref>. The entire corpus consists of 112 dialogs totaling almost eight hours in length and containing about 62,000 words and 6300 speaker turns. These dialogs have been segmented into utterance files (c.f.
Reference: <author> Heeman, P. A. and Allen, J. </author> <year> (1994a). </year> <title> Annotating speech repairs. </title> <type> unpublished manuscript. </type>
Reference-contexts: Also, editing terms (filled pauses and clue words) are labeled with et, and the interruption point with int, which will be before any editing terms associated with the repair, and after the fragment, if present. (Further details of our annotation scheme can be found in <ref> (Heeman and Allen, 1994a) </ref>.) Below is a sample annotation, with removed text go to oran-, editing term um, and resumed text go to. go| to| oran-| um| go| to| Corning m1| m2| x| int| et| m1| m2| Table 1 gives a breakdown of the modification speech repairs (that do not interfere
Reference: <author> Heeman, P. A. and Allen, J. </author> <year> (1994b). </year> <title> Detecting and correcting speech repairs. </title> <booktitle> To appear in the 31th Meeting of the Association for Computational Linguistics. </booktitle>
Reference-contexts: We have argued for the need to distinguish modification repairs from abridged repairs, because this distinction would be useful in determining the correction. We have implemented a pattern builder <ref> (Heeman and Allen, 1994b) </ref>, which builds potential repair patterns based on word matches and word replacements. However, the pattern builder has only limited knowledge which it can use to decide which patterns are likely repairs.
Reference: <author> Heeman, P. A. and Allen, J. </author> <year> (1994c). </year> <title> Dialogue transcription tools. </title> <type> unpublished manuscript. </type>
Reference-contexts: The entire corpus consists of 112 dialogs totaling almost eight hours in length and containing about 62,000 words and 6300 speaker turns. These dialogs have been segmented into utterance files <ref> (c.f. Heeman and Allen, 1994c) </ref>; words have been transcribed and the speech repairs have been annotated. For a training set, we use 40 of the dialogs, consisting of 24,000 words; and for testing, 7 of the dialogs, consisting of 5800 words.
Reference: <author> Hindle, D. </author> <year> (1983). </year> <title> Deterministic parsing of syntactic non-fluencies. </title> <booktitle> In Proceedings of the 21 st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 123-128. </pages>
Reference: <author> Levelt, W. J. M. </author> <year> (1983). </year> <title> Monitoring and self-repair in speech. </title> <journal> Cognition, </journal> <volume> 14 </volume> <pages> 41-104. </pages>
Reference: <author> Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. </author> <year> (1993). </year> <title> Building a large annotated corpus of english: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference-contexts: In order to provide a large training corpus for the statistical model, we use a tagged version of the Brown corpus, from the Penn Tree-bank <ref> (Marcus, Santorini and Marcinkiewicz, 1993) </ref>. We removed all punctuation in order to more closely approximate unsegmented spoken speech. This corpus provides us with category transition probabilities for fluent speech. These probabilities have also been used to bootstrap our algorithm in order to determine the category with with Edit Total Frag.
Reference: <author> Nakatani, C. and Hirschberg, J. </author> <year> (1993). </year> <title> A speech-first model for repair detection and correction. </title> <booktitle> In Proceedings of the 31 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 46-53. </pages>
Reference-contexts: Thus, we combine the information such that the individual pieces do not have to give a `yes' or a `no', but rather, all can contribute to the decision. 6.1. Fragments Assuming that fragments can be detected automatically <ref> (c.f. Nakatani and Hirschberg, 1993) </ref>, the question arises as to what the tagger should do with them. If the tagger treats them as lexical items, the words on either side of the fragment will be separated. This will cause two problems.
Reference: <author> Walker, M. A. </author> <year> (1993). </year> <title> Informational redundancy and resource bounds in dialogue. </title> <institution> Doctoral dissertion, Institute for Research in Cognitive Science report IRCS-93-45, University of Pennsylva-nia. </institution>
Reference-contexts: The tags of the 450 or so hand-annotated modification repairs were then used for setting the transition probabilities around modification repairs. Another problem that we encountered was interference between adjacent utterances in the same turn. Subsequent utterances often build on, or even repeat what was previously said <ref> (Walker, 1993) </ref>. Consider the following utterance. that's all you need you only need one tanker (d93-8.3 utt79) The tagger incorrectly hypothesized that this was a modification repair with an interruption point after the first occurrence of the word need.
Reference: <author> Wang, M. Q. and Hirschberg, J. </author> <year> (1992). </year> <title> Automatic classification of intonational phrase boundaries. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 6 </volume> <pages> 175-196. </pages>
Reference-contexts: Recently, Dowding et al. (1993) reported syntactic and semantic coverage of 86% for the Darpa Airline reservation corpus. Unrestricted dialogs will present even more difficulties; not only will the speech be more ungrammatical, but there is also the problem of segmenting the dialog into utterance units <ref> (c.f. Wang and Hirschberg, 1992) </ref>. If speech repairs can be detected and corrected before parsing and semantic interpretation, this should simplify those modules as well as make them more robust. 2. Previous Work Several different strategies have been discussed in the literature for detecting and correcting speech repairs.
Reference: <author> Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., and Palmucci, J. </author> <year> (1993). </year> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 359-382. </pages>
Reference-contexts: Since the context is limited, we are making the Markov assumption, that the next transition depends only on the input, which is the word that we are currently trying to tag and the previous categories. Good part-of-speech results can be obtained using only the preceding category <ref> (Weischedel et al., 1993) </ref>, which is what we will be using. In this case, the number of states of the Markov model will be N , where N is the number of tags.
References-found: 16

