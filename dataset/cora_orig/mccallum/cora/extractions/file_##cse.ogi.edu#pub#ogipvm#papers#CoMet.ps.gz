URL: file://cse.ogi.edu/pub/ogipvm/papers/CoMet.ps.gz
Refering-URL: http://www.cse.ogi.edu/~walpole/publications.html
Root-URL: http://www.cse.ogi.edu
Email: otto,walpole@cse.ogi.edu  
Title: CoMet: A Synthetic Benchmark for Message-Passing Architectures  
Author: Nalini Ganapati, Steve W. Otto, and Jonathan Walpole 
Date: February 20, 1994  
Address: 20000 NW Walker Rd, PO Box 91000 Portland, Oregon, USA 97291-1000  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Abstract: Rapid advances in hardware technology have led to wide diversity in parallel computer architectures. This diversity makes it difficult to evaluate or compare the performance of different parallel computers. Existing benchmarks tend either to be too architecture-specific, or too high-level. Both problems can result in benchmarks that not only provide insufficient information on the performance characteristics of the computer being tested, but are also difficult to port. New benchmarking approaches are needed for new architectural classes, particularly distributed-memory, message-passing computers. This paper focuses on benchmarking distributed-memory message-passing computers. A synthetic benchmark called CoMet (COmmunication METrics), is presented. CoMet is based on common communication patterns found in parallel scientific algorithms. This paper presents the CoMet design, and describes an implementation of CoMet on the Intel iPSC/860. CoMet is freely available by anonymous FTP from Oregon Graduate Institute.
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [1] <author> Cliff Addison, Vladimir Getov, Anthony Hey, Roger Hockney, and Ivan Wolton. </author> <title> The Genesis distributed-memory benchmarks. </title> <booktitle> Proc. of Parallel Processors - Benchmarking and Assessment, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: The following sections present a synthetic benchmark, called CoMet (COmmunication MET-rics), designed specifically for evaluating distributed-memory, message-passing computers. Unlike other benchmarks, such as Genesis <ref> [1, 20] </ref>, the NAS Parallel Benchmarks [2], MPLinpack [10], and CPEP [24], which are based on application programs, CoMet is a synthetic benchmark containing code to measure the communication characteristics of the system.
Reference: [2] <author> D. H. Bailey, E. Barszcz, L. Dagum, and H. D. Simon. </author> <title> NAS parallel benchmark results. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <year> 1992. </year>
Reference-contexts: The following sections present a synthetic benchmark, called CoMet (COmmunication MET-rics), designed specifically for evaluating distributed-memory, message-passing computers. Unlike other benchmarks, such as Genesis [1, 20], the NAS Parallel Benchmarks <ref> [2] </ref>, MPLinpack [10], and CPEP [24], which are based on application programs, CoMet is a synthetic benchmark containing code to measure the communication characteristics of the system. As well as measuring basic communication costs, CoMet also includes communication patterns found in typical scientific and engineering applications.
Reference: [3] <author> A. L. Beguelin, J. J. Dongarra, A. Geist, R. J. Manchek, and V. S. Sunderam. </author> <title> Heterogeneous network computing. </title> <booktitle> In Sixth SIAM Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: In addition to outlining CoMet's key components in general terms, we described a specific implementation of CoMet on the Intel iPSC/860 and discussed the results obtained. An obvious area for future work is to port CoMet to other architectures, particularly those far removed from the iPSC/860. Ports to PVM <ref> [3, 12] </ref> and MPI [16] would be interesting and useful. The benchmark itself could also be extended to include kernels that measure characteristics of interrupt-driven communication such as the hsend/hrecv of the iPSC/860 or "active" messages such as those proposed by von Eicken, et al. [30].
Reference: [4] <author> Rudolf Berrendorf and Jukka Helin. </author> <title> Evaluating the basic performance of the Intel iPSC/860 parallel computer. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4(3) </volume> <pages> 223-40, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Bomans and Roose [6], Dunigan [13, 14], Berrendorf and Helin <ref> [4] </ref> have all used the above empirical formulae in interpreting the communication characteristics of the iPSC/860. However, the echo graph also shows discontinuities at around 100, 2048 and 4096 bytes.
Reference: [5] <author> M. Berry, G.Cybenko, and J. Larson. </author> <title> Scientific benchmark characterisations. </title> <journal> Parallel Computing, </journal> <volume> 17, </volume> <year> 1991. </year>
Reference-contexts: Kernel benchmarks consist of larger code segments extracted from real applications. They represent frequently used algorithms that are thought to contribute most to application execution times. Example kernel benchmarks include Livermore Loops [15], NAS kernels <ref> [5] </ref> and LINPACK routines [9, 32]. These benchmarks are a compromise between synthetic benchmarks and full-blown application benchmarks: they are relatively easy to port and support fairly detailed information about application performance. Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing. <p> Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing. Well-known benchmarks in this category include the SPEC [8], Perfect <ref> [7, 5] </ref> and Euroben benchmark suites [29]. These benchmarks accurately reflect the performance characteristics of a machine with respect to specific classes of real-world applications. However, the porting process can be difficult and the amount of work put into porting the benchmarks can have a major impact on results.
Reference: [6] <author> L. Bomans, D.Roose, and R. Hempel. </author> <title> The Argonne/GMD macros in FORTRAN for portable parallel programmning and their implementation on the Intel iPSC/2. </title> <journal> Parallel Computing, </journal> <volume> 15 </volume> <pages> 119-32, </pages> <year> 1990. </year>
Reference-contexts: The adjacency matrix can be specified by hand or in other ways. In our implementation on the iPSC/860 it is specified using Gray codes [17]. Although there have been several attempts at message-passing standards, such as PARMACS <ref> [6] </ref>, PICL [19], and MPI [16], none are yet established as standards. CoMet implements communications using high-level macros and functions to describe the different patterns. The current macros are written in NX/2, but can be ported to other systems. <p> The time taken for a message of N bytes to be echoed from a node n hops away thus becomes, t (N ) = t startup + N t perbyte + (n 1)h; where h is the incurred overhead per hop. Bomans and Roose <ref> [6] </ref>, Dunigan [13, 14], Berrendorf and Helin [4] have all used the above empirical formulae in interpreting the communication characteristics of the iPSC/860. However, the echo graph also shows discontinuities at around 100, 2048 and 4096 bytes.
Reference: [7] <author> George Cybenko. </author> <title> Supercomputer performance trends and the perfect benchmarks. </title> <type> Technical Report 1093, </type> <institution> CSRD, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing. Well-known benchmarks in this category include the SPEC [8], Perfect <ref> [7, 5] </ref> and Euroben benchmark suites [29]. These benchmarks accurately reflect the performance characteristics of a machine with respect to specific classes of real-world applications. However, the porting process can be difficult and the amount of work put into porting the benchmarks can have a major impact on results. <p> The primary motivation for this compromise is (a) distributed-memory multiprocessors are predominantly used for scientific applications that make extensive use of matrix manipulations, and (b) it is extremely difficult to port complete application-based benchmarks across the diverse range of distributed-memory multiprocessor architectures <ref> [7] </ref>. 6 Conclusions This paper has described a synthetic benchmark for message-passing architectures. The benchmark, called CoMet, consists of kernels to measure a machine's basic communication characteristics under 24 light and heavy load. CoMet also contains some higher-level kernels that measure a machine's performance on common matrix manipulations.
Reference: [8] <author> Kaivalya M. Dixit. </author> <title> The SPEC benchmarks. </title> <journal> Parallel Computing, </journal> <volume> 17, </volume> <year> 1991. </year>
Reference-contexts: These benchmarks are a compromise between synthetic benchmarks and full-blown application benchmarks: they are relatively easy to port and support fairly detailed information about application performance. Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing. Well-known benchmarks in this category include the SPEC <ref> [8] </ref>, Perfect [7, 5] and Euroben benchmark suites [29]. These benchmarks accurately reflect the performance characteristics of a machine with respect to specific classes of real-world applications.
Reference: [9] <author> J. Dongarra. </author> <title> Performance of various computers using standard linear equations software in a fortran environment. </title> <type> Technical Report CS-89-85, </type> <institution> Computer Science Department, University of Tennessee, Knoxville, TN, </institution> <year> 1989. </year>
Reference-contexts: Kernel benchmarks consist of larger code segments extracted from real applications. They represent frequently used algorithms that are thought to contribute most to application execution times. Example kernel benchmarks include Livermore Loops [15], NAS kernels [5] and LINPACK routines <ref> [9, 32] </ref>. These benchmarks are a compromise between synthetic benchmarks and full-blown application benchmarks: they are relatively easy to port and support fairly detailed information about application performance. Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing.
Reference: [10] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadel-phia, PA, </address> <year> 1979. </year>
Reference-contexts: The following sections present a synthetic benchmark, called CoMet (COmmunication MET-rics), designed specifically for evaluating distributed-memory, message-passing computers. Unlike other benchmarks, such as Genesis [1, 20], the NAS Parallel Benchmarks [2], MPLinpack <ref> [10] </ref>, and CPEP [24], which are based on application programs, CoMet is a synthetic benchmark containing code to measure the communication characteristics of the system. As well as measuring basic communication costs, CoMet also includes communication patterns found in typical scientific and engineering applications.
Reference: [11] <author> J. Dongarra and D. Sorensen. </author> <title> Linear algebra on high performance computers. </title> <booktitle> In Proceedings of Parallel Computing, </booktitle> <year> 1986. </year>
Reference: [12] <author> Jack Dongarra, Al Geist, Robert Manchek, and Vaidy Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <booktitle> Computers in Physics, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: In addition to outlining CoMet's key components in general terms, we described a specific implementation of CoMet on the Intel iPSC/860 and discussed the results obtained. An obvious area for future work is to port CoMet to other architectures, particularly those far removed from the iPSC/860. Ports to PVM <ref> [3, 12] </ref> and MPI [16] would be interesting and useful. The benchmark itself could also be extended to include kernels that measure characteristics of interrupt-driven communication such as the hsend/hrecv of the iPSC/860 or "active" messages such as those proposed by von Eicken, et al. [30].
Reference: [13] <author> T. H. Dunigan. </author> <title> Performance of the Intel iPSC/860 and Ncube 6400 hypercubes. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1285-1302, </pages> <year> 1991. </year>
Reference-contexts: The time taken for a message of N bytes to be echoed from a node n hops away thus becomes, t (N ) = t startup + N t perbyte + (n 1)h; where h is the incurred overhead per hop. Bomans and Roose [6], Dunigan <ref> [13, 14] </ref>, Berrendorf and Helin [4] have all used the above empirical formulae in interpreting the communication characteristics of the iPSC/860. However, the echo graph also shows discontinuities at around 100, 2048 and 4096 bytes.
Reference: [14] <author> Thomas Dunigan. </author> <title> Communication performance of the intel touchstone delta mesh. </title> <type> Technical Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: The time taken for a message of N bytes to be echoed from a node n hops away thus becomes, t (N ) = t startup + N t perbyte + (n 1)h; where h is the incurred overhead per hop. Bomans and Roose [6], Dunigan <ref> [13, 14] </ref>, Berrendorf and Helin [4] have all used the above empirical formulae in interpreting the communication characteristics of the iPSC/860. However, the echo graph also shows discontinuities at around 100, 2048 and 4096 bytes.
Reference: [15] <author> J. T. Feo. </author> <title> An analysis of the computational and parallel complexity of the Livermore Loops. </title> <booktitle> Parallel Computing 7, </booktitle> <year> 1988. </year>
Reference-contexts: Kernel benchmarks consist of larger code segments extracted from real applications. They represent frequently used algorithms that are thought to contribute most to application execution times. Example kernel benchmarks include Livermore Loops <ref> [15] </ref>, NAS kernels [5] and LINPACK routines [9, 32]. These benchmarks are a compromise between synthetic benchmarks and full-blown application benchmarks: they are relatively easy to port and support fairly detailed information about application performance. Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing.
Reference: [16] <author> The Message Passing Interface Forum. </author> <title> Document for a standard message-passing interface. </title> <type> Technical Report CS-93-214, </type> <institution> University of Tennessee, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: The adjacency matrix can be specified by hand or in other ways. In our implementation on the iPSC/860 it is specified using Gray codes [17]. Although there have been several attempts at message-passing standards, such as PARMACS [6], PICL [19], and MPI <ref> [16] </ref>, none are yet established as standards. CoMet implements communications using high-level macros and functions to describe the different patterns. The current macros are written in NX/2, but can be ported to other systems. The locally synchronous 2 communication 2 In the locally synchronous model [16], a send blocks until the <p> [6], PICL [19], and MPI <ref> [16] </ref>, none are yet established as standards. CoMet implements communications using high-level macros and functions to describe the different patterns. The current macros are written in NX/2, but can be ported to other systems. The locally synchronous 2 communication 2 In the locally synchronous model [16], a send blocks until the application buffer is copied into system space and is available for reuse. Similarly, a receive blocks until the contents of the message are completely copied into the process's application buffer. 3 macros used in CoMet are blocked send, blocked recv, and blocked broadcast. <p> An obvious area for future work is to port CoMet to other architectures, particularly those far removed from the iPSC/860. Ports to PVM [3, 12] and MPI <ref> [16] </ref> would be interesting and useful. The benchmark itself could also be extended to include kernels that measure characteristics of interrupt-driven communication such as the hsend/hrecv of the iPSC/860 or "active" messages such as those proposed by von Eicken, et al. [30].
Reference: [17] <author> G. Fox, M. Johnson, G.Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, Inc., </publisher> <year> 1988. </year>
Reference-contexts: The adjacency matrix can be specified by hand or in other ways. In our implementation on the iPSC/860 it is specified using Gray codes <ref> [17] </ref>. Although there have been several attempts at message-passing standards, such as PARMACS [6], PICL [19], and MPI [16], none are yet established as standards. CoMet implements communications using high-level macros and functions to describe the different patterns. <p> The message lengths in the communication part of the kernel ranged from 0 to about 85000, to observe the behavior of small to large messages in the benchmark kernels. To map the hypercube topology onto a two-dimensional mesh, a Gray code was used <ref> [17] </ref>. The elements in the mesh represent nodes in the system such that adjacent elements in the matrix are also physical neighbors. The dimensions of the mesh are determined by variables proc dim1 and proc dim2. <p> The function get hop node () which returns the identity of nodes n hops away, is defined using bit arithmetic that characterizes the hypercube configuration <ref> [17] </ref>. The update guard, shift matrix, transpose, row broadcast and col broadcast kernels were written using a combination of csend, crecv, isend and irecv.
Reference: [18] <author> Nalini Ganapati. CoMet: </author> <title> A synthetic benchmark for message-passing architectures. </title> <type> Master's thesis, </type> <institution> Oregon Graduate Institute of Science & Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: CoMet also provides a set of support functions and macros for tasks such as initialization, cleanup, timing, and topology description (the number of nodes, the mapping of nodes to the adjacency matrix, the identification of nearest neighbors, and the measurement of inter-node distances, etc). For more details see <ref> [18] </ref>. Finally, our approach to benchmark integrity is to specify explicitly the functions and macros that are allowed to be modified by CoMet users. Users are not supposed to alter any other parts of the benchmark source code. CoMet does, however, permit the use of any level of compiler optimization.
Reference: [19] <author> G. A. Geist. </author> <title> A user's guide to PICL: a portable instrumented communication library. </title> <type> Technical Report ORNL/TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The adjacency matrix can be specified by hand or in other ways. In our implementation on the iPSC/860 it is specified using Gray codes [17]. Although there have been several attempts at message-passing standards, such as PARMACS [6], PICL <ref> [19] </ref>, and MPI [16], none are yet established as standards. CoMet implements communications using high-level macros and functions to describe the different patterns. The current macros are written in NX/2, but can be ported to other systems.
Reference: [20] <author> Anthony J. G. Hey. </author> <title> The Genesis distributed memory benchmarks. </title> <journal> Parallel Computing, </journal> <volume> 17, </volume> <month> December </month> <year> 1991. </year>
Reference-contexts: The following sections present a synthetic benchmark, called CoMet (COmmunication MET-rics), designed specifically for evaluating distributed-memory, message-passing computers. Unlike other benchmarks, such as Genesis <ref> [1, 20] </ref>, the NAS Parallel Benchmarks [2], MPLinpack [10], and CPEP [24], which are based on application programs, CoMet is a synthetic benchmark containing code to measure the communication characteristics of the system.
Reference: [21] <author> Intel Corporation. </author> <title> iPSC/860 Programmer's Manual, </title> <year> 1991. </year>
Reference-contexts: Communication paths between any two nodes are established dynamically using free channels and an e-cube algorithm [25]. These paths are freed once the communication request has been completed. Each node on the iPSC/860 hypercube runs the NX/2 node operating system. NX/2 <ref> [26, 21] </ref> performs process management and message passing and allows only one application to execute per node at a time. 3.2 Communication Protocols in NX/2 The communication protocol used by NX/2 is based on buffers.
Reference: [22] <author> S. Kambhatla, J. Inouye, and J.Walpole. </author> <title> Experiences with BeLinda: A Synthetic Linda Benchmark for Parallel Computing Platforms. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 155-162, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: These benchmarks can be categorized as either synthetic, kernel, or application-based benchmarks. Synthetic benchmarks consist of code segments that reflect frequently used program constructs and basic machine functions. Existing benchmarks in this category include Whetstones [32], Dhrys-tones [32, 31], and BeLinda <ref> [22] </ref>. The simplicity of benchmarks at this level usually makes them easy to port from one architecture to another. However, relating the results to performance predictions for real applications requires a detailed understanding of the application's use of the benchmark primitives.
Reference: [23] <author> R. Littlefield. </author> <title> Modeling node bandwidth limits and their effect on vector combining algorithms. </title> <type> Technical Report PNL-SA-20425, </type> <institution> Batelle Pacific Northwest Laboratory, </institution> <year> 1992. </year>
Reference-contexts: This illustrates that the vector, global sum primitive available follows a logarithmic, tree reduction scheme. Other algorithms are competitive with this, depending on various parameters such as the vector length. See Littlefield for a study of alternative algorithms <ref> [23] </ref>. Finally, the last results (Figures 24, 25, 26 and 27), pertain to the overlap of computation and communication. The fact that the iPSC/860 has separate communication and computation hardware means that communication and computation can be partially overlapped.
Reference: [24] <author> P. Messina, C. Baillie, E. Felten, P. Hipes, R. Williams, A. Alagar, A. Kamrath, R. Leary, W. Pfeiffer, J. Rogers, and D. Walker. </author> <title> Benchmarking advanced architecture computers. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 195-255, </pages> <month> September </month> <year> 1990. </year> <month> 26 </month>
Reference-contexts: The following sections present a synthetic benchmark, called CoMet (COmmunication MET-rics), designed specifically for evaluating distributed-memory, message-passing computers. Unlike other benchmarks, such as Genesis [1, 20], the NAS Parallel Benchmarks [2], MPLinpack [10], and CPEP <ref> [24] </ref>, which are based on application programs, CoMet is a synthetic benchmark containing code to measure the communication characteristics of the system. As well as measuring basic communication costs, CoMet also includes communication patterns found in typical scientific and engineering applications.
Reference: [25] <author> Steve Nugent. </author> <booktitle> The iPSC/2 direct-connect technology. In 3rd conference on Hypercube Concur--rent Computers and Applications, </booktitle> <address> New York, NY, 1988. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Since one channel in the DCM is reserved for special I/O, the maximum size of the iPSC/860 is 128 nodes. Communication paths between any two nodes are established dynamically using free channels and an e-cube algorithm <ref> [25] </ref>. These paths are freed once the communication request has been completed. Each node on the iPSC/860 hypercube runs the NX/2 node operating system.
Reference: [26] <author> Paul Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of the 3rd conference on Hypercube Concurrent Computers and Applications, </booktitle> <year> 1988. </year>
Reference-contexts: Communication paths between any two nodes are established dynamically using free channels and an e-cube algorithm [25]. These paths are freed once the communication request has been completed. Each node on the iPSC/860 hypercube runs the NX/2 node operating system. NX/2 <ref> [26, 21] </ref> performs process management and message passing and allows only one application to execute per node at a time. 3.2 Communication Protocols in NX/2 The communication protocol used by NX/2 is based on buffers.
Reference: [27] <author> Willi Schonauer and Harmut Hafner. </author> <title> Performance estimates of supercomputers. </title> <journal> Parallel Computing, </journal> <volume> 17, </volume> <year> 1991. </year>
Reference-contexts: However, the porting process can be difficult and the amount of work put into porting the benchmarks can have a major impact on results. In addition to these benchmarks, micro-measurements and global performance formulae have been used to estimate supercomputer performance <ref> [27] </ref>. These approaches provide a more detailed view of the underlying hardware characteristics than the higher-level benchmarks described above. According to the classification presented above, CoMet is a synthetic benchmark. That is, its components are considerably smaller than application based benchmarks.
Reference: [28] <institution> Thinking Machines Corporation. </institution> <type> Connection Machine CM-5 Technical Summary, </type> <month> October </month> <year> 1991. </year>
Reference-contexts: The overhead of the repeat loop and dummy function calls is measured separately and subtracted from the total time in order to derive the final measurement. CoMet is designed to work over a wide range of architectures, from loosely coupled networks of workstations to hypercube and tree-based communication networks <ref> [28] </ref>. However, it is unrealistic to expect the benchmark kernels to be completely architecture independent. Since CoMet contains two-dimensional matrix operations, it is important to map the matrix to the processing nodes in a manner appropriate to the architecture. CoMet defines a two-dimensional matrix of processors that captures nearest-neighbor mappings.
Reference: [29] <author> Aad J. van der Steen. </author> <title> The benchmark of the Euroben group. </title> <booktitle> Parallel Computing 17, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: Application-based benchmarks are implementations of real-world applications, usually in the area of scientific computing. Well-known benchmarks in this category include the SPEC [8], Perfect [7, 5] and Euroben benchmark suites <ref> [29] </ref>. These benchmarks accurately reflect the performance characteristics of a machine with respect to specific classes of real-world applications. However, the porting process can be difficult and the amount of work put into porting the benchmarks can have a major impact on results.
Reference: [30] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <journal> Communications of ACM, </journal> <pages> pages 256-66, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Ports to PVM [3, 12] and MPI [16] would be interesting and useful. The benchmark itself could also be extended to include kernels that measure characteristics of interrupt-driven communication such as the hsend/hrecv of the iPSC/860 or "active" messages such as those proposed by von Eicken, et al. <ref> [30] </ref>. At the application level, CoMet should be made more comprehensive by including more communication patterns from scientific computing. Finally, the performance of many applications also depends on the performance of secondary storage accesses.
Reference: [31] <author> R. Weicker. Dhrystone: </author> <title> A synthetic systems programming benchmark. </title> <journal> Communications of ACM, </journal> <year> 1984. </year>
Reference-contexts: These benchmarks can be categorized as either synthetic, kernel, or application-based benchmarks. Synthetic benchmarks consist of code segments that reflect frequently used program constructs and basic machine functions. Existing benchmarks in this category include Whetstones [32], Dhrys-tones <ref> [32, 31] </ref>, and BeLinda [22]. The simplicity of benchmarks at this level usually makes them easy to port from one architecture to another. However, relating the results to performance predictions for real applications requires a detailed understanding of the application's use of the benchmark primitives.

References-found: 32

