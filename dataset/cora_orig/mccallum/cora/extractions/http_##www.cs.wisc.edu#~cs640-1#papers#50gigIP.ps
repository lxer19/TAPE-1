URL: http://www.cs.wisc.edu/~cs640-1/papers/50gigIP.ps
Refering-URL: http://www.cs.wisc.edu/~cs640-1/740.syllabus.html
Root-URL: http://www.cs.wisc.edu
Title: A Fifty Gigabit Per Second IP Router  
Author: C. Partridge, P. Carvey, E. Burgess, I. Castineyra, T. Clarke, L. Graham, M. Hathaway, P. Herman, A. King, S. Kohlami, T. Ma, J. Mcallen, T. Mendez, W. Milliken, R. Osterlind, R. Pettyjohn, J. Rokosz, J. Seeger, M. Sollins, S. Storch, B. Tober, G. Troxel, D. Waitzman and S. Winterble 
Note: (a part of GTE Corporation)  
Affiliation: BBN Technologies  
Abstract: Aggressive research on gigabit per second networks has led to dramatic improvements in network transmission speeds. One result of these improvements has been to put pressure on router technology to keep pace. This paper describes a router, nearly completed, which is more than fast enough to keep up with the latest transmission technologies. The router has a backplane speed of 50 Gb/s and can forward tens of millions of packets per second. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> Alpha 21164 Microprocessor; Hardware Reference Manual, Digital Equipment Corporation (April 1995). </institution>
Reference-contexts: The focus in this section is on features that impact how the Alpha functions in the forwarding engine. A more detailed description of the 21164 and the Alpha architecture in general can be found in <ref> [1, 31] </ref>. The Alpha 21164 is a 64-bit, 32-register, super-scalar RISC processor. There are two integer logic units called E0 and E1 and two oating point units called FA and FM. The four logic units are distinct.
Reference: 2. <author> Asthana, A., C. Delph, H.V. Jagadish, and P. Krzyzanowski, </author> <title> Tow ards a Gigabit IP Router, </title> <journal> Journal of High Speed Networks, </journal> <volume> 1, 4, </volume> <pages> pp. </pages> <month> 281-288 </month> <year> (1992). </year>
Reference-contexts: When an line card receives a new packet, it sends the packet header to a forwarding engine. The forwarding engine then determines how the packet should be routed. The development of our forwarding engine design was inuenced by the Bell Labs router <ref> [2] </ref>, which, although it has a different architecture, had to solve similar problems. 3.1. A Brief Description of the Alpha 21164 Processor At the heart of each forwarding engine is a 415 MHz Digital Equipment Corporation Alpha 21164 processor. <p> Related Work and Conclusion Many of the features of the MGR have been inuenced by prior work. The Bell Labs router <ref> [2] </ref> similarly divided work between interfaces, which moved packets among themselves, and forwarding engines which, based on the packet headers, directed how the packets should be moved. Tantawy and Zitterbart [33] have examined how parallel IP header processing might be.
Reference: 3. <author> Baker, F., </author> <title> Requirements for IP Version 4 Routers; RFC-1812, Internet Request For Comments, </title> <month> 1812 (June </month> <year> 1995). </year>
Reference-contexts: For IPv4, this set of standards is summarized in the Internet Router Requirements <ref> [3] </ref>. Our router achieves all three goals (but for one minor variance from the IPv4 router requirements, discussed below). This paper presents our multigigabit router, called the MGR, which is nearly completed. <p> Within that simple description, however, lies a number of complexities. (As an illustration of the complexities, consider the fact that the Internet Engineering Task Force's Requirements for IP Version 4 Routers <ref> [3] </ref> is 175 pages long and cites over one hundred related references and standards.) In this section, we present an overview of the MGR design and point out its major and minor innovations. After this section, the rest of the paper discusses the details of each module. 2.1. <p> This code performs all the steps required by the Internet Router Requirements <ref> [3] </ref> except one: it doesn't check the IP header checksum, but simply updates it. The update algorithm is safe [4, 18, 29]. If the checksum is bad, it will remain bad after the update.
Reference: 4. <editor> Braden, B., D. Borman, and C. Partridge, </editor> <title> Computing the Internet Checksum; RFC 1071, Internet Request for Comments, </title> <month> 1071 (September </month> <year> 1988). </year>
Reference-contexts: This code performs all the steps required by the Internet Router Requirements [3] except one: it doesn't check the IP header checksum, but simply updates it. The update algorithm is safe <ref> [4, 18, 29] </ref>. If the checksum is bad, it will remain bad after the update.
Reference: 5. <author> Bradner, S., and A. Mankin, IPng: </author> <title> Internet Protocol Next Generation, </title> <publisher> Addison-Wesley (1995). </publisher>
Reference: 6. <author> Brodnik, A., S. Carlsson, M. Degermark, and S. Pink, </author> <title> Small Forwarding Tables for Fast Routing Lookups, </title> <booktitle> Proc. ACM SIGCOMM '97, </booktitle> <month> Cannes (September </month> <year> 1997). </year>
Reference-contexts: Since the forwarding engines only require a summary of the data in the route (in particular, next hop information), their copies of the routing table, called forwarding tables can be very small (as little as 100KB for about 50K routes <ref> [6] </ref>). Second, the design uses a switched backplane. Until very recently the standard router used a shared bus, rather than a switched backplane. How-ev er, to go fast, one really needs the parallelism of a switch. Our particular switch was custom designed to meet the needs of an IP router. <p> The routing table uses the binary hash scheme developed by Waldvogel, Varghese, Turner and Plattner [34]. (We also hope to experiment with the algorithm described in <ref> [6] </ref> developed at Lulea University). Since the forwarding table contains prefix routes and the route cache is a cache of routes for particular destinations, the processor has to convert the forwarding table entry into an appropriate, destination-specific, cache entry. 2. Headers with errors.
Reference: 7. <author> Chiappa, N., </author> <title> Data Packet Switch Using a Primary Processing Unit to Designate One of a Plurality of Data Stream Control Circuits to Selectively Handle the Header Processing of Incoming Packets in One Data Packet Stream (US Patent #5,249,292) (28 September 1993). </title>
Reference: 8. <author> Deering, S., and R. Hinden, </author> <title> Internet Protocol, Version 6 (IPv6); RFC-1883, Internet Requests for Comments, </title> <month> 1883 (January </month> <year> 1996). </year>
Reference-contexts: This is a large penalty to pay to check for a rare error that can be caught end-to-end. Indeed, for this reason, IPv6 does not include a header checksum <ref> [8] </ref>. Certain datagrams are not handled in the fast path code. These datagrams can be divided into five categories: 1. Headers whose destination misses in the route cache. This is the most common case.
Reference: 9. <author> Demers, A., S. Keshav, and S. Shenker, </author> <title> Analysis and Simulation of a Fair Queueing Algorithm, Internetwork: </title> <journal> Research and Experience, </journal> <volume> 1, 1, </volume> <pages> pp. 3-26, </pages> <publisher> John Wiley & Sons (September 1990). </publisher>
Reference: 10. <author> Fedor, M., </author> <title> Gated: A Multi-routing Protocol Daemon for UNIX, </title> <booktitle> Proc. 1988 Summer USENIX Conference, </booktitle> <pages> pp. 365-376, </pages> <address> San Francisco, CA (1988). </address>
Reference-contexts: Second, we needed a BSD UNIX platform because we wanted to speed the development process by porting existing free software such as gated <ref> [10] </ref> to the MGR whenever possible and almost all this software is implemented for BSD UNIX. 7. Managing Routing and Forwarding Tables Routing information in the MGR is managed jointly by the network processor and the forwarding engines.
Reference: 11. <author> Feldmeier, </author> <title> D.C., Improving Gateway Performance with a Routing-Table Cache, </title> <booktitle> Proc. IEEE INFOCOM '88, </booktitle> <pages> pp. 298-307, </pages> <address> New Orleans (March 1988). </address>
Reference-contexts: Since each route entry takes 64 bits, we have a maximum cache size of approximately 12,000 routes. Studies of locality in packet streams at routers suggest a cache this size should yield a hit rate well in excess of 95% <ref> [11, 15, 13] </ref>. Our own tests with a traffic trace from FIX West (a major inter exchange point in the Internet) suggest a 12,000 entry cache will have a hit rate in excess of 95%. The tertiary cache (Bcache) is an external memory of several megabytes managed by the processor.
Reference: 12. <author> Floyd, S., and V. Jacobson, </author> <title> Random Early Detection Gateways for Congestion Avoidance, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1, 4, </volume> <pages> pp. </pages> <month> 397-413 (August </month> <year> 1993). </year>
Reference-contexts: The third event occurs when the network processor informs the scheduler of changes in the allocation of bandwidth among users. The fourth event is a timer event, needed for certain scheduling algorithms. The processor can also initiate messages to the network processor. Some packet handling algorithms such as RED <ref> [12] </ref>, require the scheduler to notify the network processor when a packet is discarded. Any link-layer based scheduling (such as that required by ATM) is done separately by a link-layer scheduling, after that packet scheduler has passed the packet on for transmission.
Reference: 13. <author> Heimlich, S.A., </author> <title> Traffic Characterization of the NSFNET National Backbone, </title> <booktitle> Proc. Winter 1990 USENIX Conference, </booktitle> <pages> pp. 207-227, </pages> <address> Washington DC (January 1990). </address>
Reference-contexts: Since each route entry takes 64 bits, we have a maximum cache size of approximately 12,000 routes. Studies of locality in packet streams at routers suggest a cache this size should yield a hit rate well in excess of 95% <ref> [11, 15, 13] </ref>. Our own tests with a traffic trace from FIX West (a major inter exchange point in the Internet) suggest a 12,000 entry cache will have a hit rate in excess of 95%. The tertiary cache (Bcache) is an external memory of several megabytes managed by the processor.
Reference: 14. <author> Jain, R., </author> <title> A Comparison of Hashing Schemes for Address Lookup in Computer Networks, </title> <journal> IEEE Tr ans. on Communications, </journal> <volume> 40, 10, </volume> <pages> pp. </pages> <month> 1570-1573 (October </month> <year> 1992). </year>
Reference: 15. <author> Jain, R., and S. Routhier, </author> <title> Packet Trains: Measurements and a New Model for Computer Network Traffic, </title> <journal> IEEE Journal on Selected Areas of Communication, </journal> <volume> 4, 6, </volume> <pages> pp. </pages> <month> 986-995 (September </month> <year> 1986). </year>
Reference-contexts: Since each route entry takes 64 bits, we have a maximum cache size of approximately 12,000 routes. Studies of locality in packet streams at routers suggest a cache this size should yield a hit rate well in excess of 95% <ref> [11, 15, 13] </ref>. Our own tests with a traffic trace from FIX West (a major inter exchange point in the Internet) suggest a 12,000 entry cache will have a hit rate in excess of 95%. The tertiary cache (Bcache) is an external memory of several megabytes managed by the processor. <p> Even with an increasing number of conversations, it appears that packet trains <ref> [15] </ref> will continue to ensure that there is a strong chance that two datagrams arriving close together will be headed for the same destination. A modest hit rate seems assured, and thus we believe using a cache makes sense.
Reference: 16. <author> Lamaire, R.O, and D.N. Serpanos, </author> <title> Two Dimensional Round Robin Schedulers for Packet Switches with Multiple Input Queues, </title> <journal> IEEE/ACM Tr ans. on Networking, </journal> <pages> pp. 471-482. </pages> <month> (October 94). </month>
Reference-contexts: This technique is called Wa ve Front Allocation <ref> [16, 32] </ref> and is illustrated in the middle of Figure 4. However, for a 1515 matrix, Wav e Front Allocation requires 29 steps, which is still too many. But one can refine the process by grouping positions in the matrix, and doing wavefront allocations across the groups.
Reference: 17. <author> Lewis, H.R., and L. Denenberg, </author> <title> Data Structures & Their Algorithms, </title> <editor> p. </editor> <publisher> Harper Collins (1991). </publisher>
Reference: 18. <author> Mallory, T., and A. Kullberg, </author> <title> Incremental Updating of the Internet Checksum; RFC-1141, Internet Requests for Comments, </title> <month> 1141 (January </month> <year> 1990). </year>
Reference-contexts: This code performs all the steps required by the Internet Router Requirements [3] except one: it doesn't check the IP header checksum, but simply updates it. The update algorithm is safe <ref> [4, 18, 29] </ref>. If the checksum is bad, it will remain bad after the update.
Reference: 19. <author> McKeown, N., M. Izzard, A. Mekkittikul, B. Eller-sick, and M. Horowitz, </author> <title> The Tiny Tera: A Packet Switch Core, </title> <note> IEEE Micro (January 1997). </note>
Reference: 20. <author> Karol, M.J., M.G. Hluchyj, and S.P. Morgan, </author> <title> Input Versus Output Queueing on a Space-Division Packet Switch, </title> <journal> IEEE Trans. Communications, </journal> <volume> 35, 12, </volume> <pages> pp. </pages> <month> 1347-1356 (December </month> <year> 1987). </year>
Reference-contexts: It is an input-queued switch in which each input keeps a separate FIFO and bids separately for each output. Keeping track of traffic for each output separately means the switch does not suffer Head-of-Line blocking <ref> [20] </ref> and it has been shown by simulation [30] and more recently proved [21] that such a switch can achieve 100% throughput. The key design choice in this style of switch is its allocation algorithm - how one arbitrates among the various bids.
Reference: 21. <author> McKeown, N., V. Anantharam, and J. Walrand, </author> <title> Achieving 100% Throughput in an Input-Queued Switch, </title> <booktitle> Proc. IEEE Infocom '96, </booktitle> <address> San Francisco (March 1996). </address>
Reference-contexts: It is an input-queued switch in which each input keeps a separate FIFO and bids separately for each output. Keeping track of traffic for each output separately means the switch does not suffer Head-of-Line blocking [20] and it has been shown by simulation [30] and more recently proved <ref> [21] </ref> that such a switch can achieve 100% throughput. The key design choice in this style of switch is its allocation algorithm - how one arbitrates among the various bids.
Reference: 22. <author> Mogul, J.C., and S.E. Deering., </author> <title> Path MTU Discovery; RFC-1191, Internet Requests for Comments, </title> <month> 1191 (November </month> <year> 1990). </year>
Reference-contexts: Datagrams that must be fragmented. Rather than requiring line cards to support fragmentation logic, we do fragmentation on the network processor. Now that IP MTU discovery <ref> [22] </ref> is prevalent, fragmentation should be rare. 5. Multicast datagrams. Multicast datagrams require special routing, since the routing of the datagram is dependent on the source address and the inbound link as well as the multicast destination.
Reference: 23. <author> Prabhakar, B., N. McKeown, and R. Ahuja, </author> <title> Mul-ticast Scheduling for Input-Queued Switches, </title> <journal> IEEE Journal on Selected Areas of Communications (May 1997). </journal>
Reference-contexts: On the switch, those line cards not involved in the multi-cast can concurrently make transfers among themselves, while the multicast transactions are going on. The fact that our switch copies multiple times makes it less effective than some other switch designs (e.g. <ref> [23] </ref>), but still much better than a bus. -7 reverse of that made in many ATM switches and is why we built our own switch optimized for IP traffic.) 4.1. Switch Details The switch has two pin interfaces to each function card.
Reference: 24. <author> Newman, P., </author> <title> IP Switching and Gigabit Routers, </title> <journal> IEEE Communications Magazine (February 1997). </journal>
Reference-contexts: Tantawy and Zitterbart [33] have examined how parallel IP header processing might be. So too, several people have looked at ways to adapt switches to support IP traffic <ref> [27, 24] </ref>. Beyond the innovations outlined in section 2.2, we believe the MGR makes two important contributions. The first is the MGR's emphasis on examining every datagram header.
Reference: 25. <author> Partridge, C., </author> <title> How Slow Is One Gigabit Per Second?, </title> <journal> ACM Computer Communication Review, </journal> <volume> 21, 1, </volume> <pages> pp. </pages> <month> 44-53 (January </month> <year> 1990). </year>
Reference-contexts: Overview of the Router Architecture A router is a deceptively simple piece of equipment. At minimum, it is a collection of network interfaces, some sort of bus or connection fabric connecting those interfaces, and some software 1 See <ref> [25] </ref>. Some experts argue for more or less packet processing power. Those arguing for more power note that a TCP/IP datagram containing an ACK but no data is 320 bits long. Link-layer headers typically increase this to approximately 400 bits.
Reference: 26. <author> Partridge, C., </author> <title> Gigabit Networking, </title> <publisher> Addison Wes-ley Publishers (1994). -12 </publisher>
Reference-contexts: 1. Introduction Transmission link bandwidths keep improving, at a seemingly inexorable rate, as the result of research in transmission technology <ref> [26] </ref>.
Reference: 27. <author> Parulkar, G., D.C. Schmidt, and J. Turner, IP/ATM: </author> <title> A Strategy for Integrating IP with ATM, </title> <journal> Proc. of ACM SIGCOMM '95 (Special Issue of ACM Computer Communication Review), </journal> <volume> 25, 4, </volume> <pages> pp. </pages> <month> 49-59 (October </month> <year> 1995). </year>
Reference-contexts: Tantawy and Zitterbart [33] have examined how parallel IP header processing might be. So too, several people have looked at ways to adapt switches to support IP traffic <ref> [27, 24] </ref>. Beyond the innovations outlined in section 2.2, we believe the MGR makes two important contributions. The first is the MGR's emphasis on examining every datagram header.
Reference: 28. <author> Plummer, D., </author> <title> Ethernet Address Resolution Protocol: Or converting network protocol addresses to 48.bit Ethernet address for transmission on Ethernet hardware, Internet Request for Comments, </title> <month> 826 (November </month> <year> 1992). </year>
Reference-contexts: A side comment about the link-layer address is in order. Many networks have dynamic schemes for mapping IP addresses to link-layer addresses. A good example is the Address Resolution Protocol, used for Ethernet <ref> [28] </ref>. If a router gets a datagram to an IP address whose Ethernet address it doesn't know, it is supposed to send an ARP message and hold the datagram until it gets an ARP reply with the necessary Ethernet address.
Reference: 29. <author> Rijsinghani, A., </author> <title> Computation of the Internet Checksum via Incremental Update; RFC-1624, Internet Request for Comments, </title> <month> 1624 (May </month> <year> 1994). </year>
Reference-contexts: This code performs all the steps required by the Internet Router Requirements [3] except one: it doesn't check the IP header checksum, but simply updates it. The update algorithm is safe <ref> [4, 18, 29] </ref>. If the checksum is bad, it will remain bad after the update.
Reference: 30. <author> Robinson, J., </author> <title> The Monet Switch, Internet Research Steering Group Workshop on Architectures for Very-High-Speed Networks; RFC-1152, DDN Network Information Center, </title> <address> Cambridge, MA (24-26 January 1990). </address>
Reference-contexts: It is an input-queued switch in which each input keeps a separate FIFO and bids separately for each output. Keeping track of traffic for each output separately means the switch does not suffer Head-of-Line blocking [20] and it has been shown by simulation <ref> [30] </ref> and more recently proved [21] that such a switch can achieve 100% throughput. The key design choice in this style of switch is its allocation algorithm - how one arbitrates among the various bids.
Reference: 31. <author> Sites, Richard L., </author> <title> Alpha Architecture Reference Manual, </title> <publisher> Digital Press (1992). </publisher>
Reference-contexts: The focus in this section is on features that impact how the Alpha functions in the forwarding engine. A more detailed description of the 21164 and the Alpha architecture in general can be found in <ref> [1, 31] </ref>. The Alpha 21164 is a 64-bit, 32-register, super-scalar RISC processor. There are two integer logic units called E0 and E1 and two oating point units called FA and FM. The four logic units are distinct.
Reference: 32. <author> Tamir, Y., and H.C. Chi, </author> <title> Symmetric Crossbar Arbiters for VLSI Communications Switches, </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 4, No. </volume> <month> 1 </month> <year> (1993). </year>
Reference-contexts: This technique is called Wa ve Front Allocation <ref> [16, 32] </ref> and is illustrated in the middle of Figure 4. However, for a 1515 matrix, Wav e Front Allocation requires 29 steps, which is still too many. But one can refine the process by grouping positions in the matrix, and doing wavefront allocations across the groups.
Reference: 33. <author> Tantawy, A., and M. Zitterbart, </author> <title> Multiprocessing in High-Performance IP Routers, Protocols for High-Speed Networks, </title> <booktitle> III (Proc. IFIP 6.1/6.4 Workshop), </booktitle> <publisher> Elsevier, </publisher> <address> Stockholm (13-15 May 1992). </address>
Reference-contexts: The Bell Labs router [2] similarly divided work between interfaces, which moved packets among themselves, and forwarding engines which, based on the packet headers, directed how the packets should be moved. Tantawy and Zitterbart <ref> [33] </ref> have examined how parallel IP header processing might be. So too, several people have looked at ways to adapt switches to support IP traffic [27, 24]. Beyond the innovations outlined in section 2.2, we believe the MGR makes two important contributions.

References-found: 33

