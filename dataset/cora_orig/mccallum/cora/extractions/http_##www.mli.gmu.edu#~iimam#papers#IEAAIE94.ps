URL: http://www.mli.gmu.edu/~iimam/papers/IEAAIE94.ps
Refering-URL: http://www.mli.gmu.edu/~iimam/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: iimam@aic.gmu.edu  
Title: AN EXPERIMENTAL STUDY OF DISCOVERY IN LARGE TEMPORAL DATABASES  
Author: Ibrahim F. Imam 
Address: Fairfax, VA. 22030  
Affiliation: Center for Artificial Intelligence George Mason University  
Abstract: This paper is published in the Proceeding of the Seventh International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems IEA/AIE-94, pp. 171-180, June 1-3, Anger, F.D., Rodriguez, R.V., and Ali, M., (Eds.), Gordon and Breach Science Publishers, 1994. 
Abstract-found: 1
Intro-found: 1
Reference: [BGS*91] <author> Bergadano, F., Giordana, A., Saitta, L., Brancadori, F., and De Marchi, D., </author> <title> Integrated Learning in a Real Domain, Knowledge Discovery In Databases, </title> <editor> Piatetsky-Shapiro, G., Frawley, W., (Eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: This knowledge is examined against data of the last 5 months. 2. THE DISCOVERY TOOLS Machine learning systems play a great role in discovering interesting knowledge in databases due to their capabilities in performing different types of inferences (e.g. <ref> [BGS*91] </ref>; [GMT*91]; [MKK*92a,b]; [Sch92]). The experiment presented uses the symbolic learning system AQ15 and the feature selection program IS to perform the discovery. The rest of this section presents a brief description on those systems. 2.1.
Reference: [CS93] <author> Chan, P. and Stolfo, </author> <title> S . , Toward Parallel and distributed Learning by MetaLearning, </title> <booktitle> Proceeding of the AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Washington D.C., </address> <month> July 11-12, </month> <year> 1993. </year>
Reference-contexts: To overcome this problem, some discovery systems refine the database before the discovery process begins. The refinement operations reduce complexity, size, or noise in the data. Examples of such techniques include domain quantization, data reduction, partial classification, noise pruning, data optimization, etc. (e.g. <ref> [CS93] </ref>; [FI92]; [KMK91]; [Qui90]; attractive approach to simplifying the discovery process involves splitting the learning space into a set of subspaces. This can be done by taking one attribute out and generating as many subspaces, using the remaining attributes values, as the legal number of (quantized) values of that attribute.
Reference: [FI92] <author> Fayyad, U.M., and Irani, </author> <title> K.B., On the Handling of Continous-Valued Attributes in Decision Tree Generation, </title> <journal> in Journal of Machine Learning, </journal> <volume> Vol. 8, No. 1, </volume> <pages> pp. 87-102, </pages> <year> 1992. </year>
Reference-contexts: To overcome this problem, some discovery systems refine the database before the discovery process begins. The refinement operations reduce complexity, size, or noise in the data. Examples of such techniques include domain quantization, data reduction, partial classification, noise pruning, data optimization, etc. (e.g. [CS93]; <ref> [FI92] </ref>; [KMK91]; [Qui90]; attractive approach to simplifying the discovery process involves splitting the learning space into a set of subspaces. This can be done by taking one attribute out and generating as many subspaces, using the remaining attributes values, as the legal number of (quantized) values of that attribute.
Reference: [GMT*91] <author> Gonzalez, A.J., Myler, H.R., Towhidnejad, M., McKenzie, </author> <title> F.D., Kladke, R.R., and Laureano, R . , Automated Extraction of Knowledge from Computer-Aided Design Databases, Knowledge Discovery In Databases, </title> <editor> Piatetsky-Shapiro, G., Frawley, W., (Eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: This knowledge is examined against data of the last 5 months. 2. THE DISCOVERY TOOLS Machine learning systems play a great role in discovering interesting knowledge in databases due to their capabilities in performing different types of inferences (e.g. [BGS*91]; <ref> [GMT*91] </ref>; [MKK*92a,b]; [Sch92]). The experiment presented uses the symbolic learning system AQ15 and the feature selection program IS to perform the discovery. The rest of this section presents a brief description on those systems. 2.1.
Reference: [IMK93] <editor> Imam, I.F., Michalski, R.S., and Kerschberg, L. </editor> <title> Discovering Attribute Dependence in Databases by Integrating Symbolic Learning and Statistical Analysis Techniques, </title> <booktitle> Proceeding of the AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Washington D.C., </address> <month> July 11-12, </month> <year> 1993. </year>
Reference-contexts: These patterns describe five different classes of the monthly change in the trade value. The method determines a set of attributes relevant to the monthly change. The relevant attributes are obtained by the importance score program (IS) <ref> [IMK93] </ref>. Any attribute relevant in more than one month is considered relevant to the whole data set. Other interesting relationships are discovered between some of the relevant attributes (individually) and the decision classes. These relationships are represented by a simple diagrams.
Reference: [IV94] <author> Imam, I.F., Vafaie, H., </author> <title> An Empirical Comparison Between Global and Greedy-Like Search for Feature Selection, </title> <booktitle> in the proceeding of the 7th Florida AI Research Symposium, </booktitle> <address> Florida, </address> <year> 1994. </year>
Reference-contexts: In IC and DC modes, rulesets can be evaluated in any order. 2.2. Measuring the Attributes Importance The importance of an attribute depends on how this attributes values can classify different decision classes. To determine such a measurements, the method uses the Importance Score (IS) method <ref> [IV94] </ref>. The method was developed as a feature selection tool that searches for the minimum set of attributes, the most important ones, which produce a better predictive accuracy.
Reference: [KMK91] <author> Kaufman, K.A., Michalski, R.S., and Kerschberg, L., </author> <title> (b) "An Architecture for Integrating Machine Learning and Discovery Programs Into a Data Analysis System, </title> <booktitle> Proceeding of the AAAI-91 workshop on Knowledge Discovery in Databases, </booktitle> <address> Anaheim, CA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: To overcome this problem, some discovery systems refine the database before the discovery process begins. The refinement operations reduce complexity, size, or noise in the data. Examples of such techniques include domain quantization, data reduction, partial classification, noise pruning, data optimization, etc. (e.g. [CS93]; [FI92]; <ref> [KMK91] </ref>; [Qui90]; attractive approach to simplifying the discovery process involves splitting the learning space into a set of subspaces. This can be done by taking one attribute out and generating as many subspaces, using the remaining attributes values, as the legal number of (quantized) values of that attribute. <p> The experiment is done on data of 17 month period, the 12 months of 1989 and the first five of 1990. An analysis and some experiments were done on small and random portion of this data <ref> [KMK91] </ref>. The data is sorted by destination, by product, and by date. The products are organized into a hierarchy of about 150 groups. A product may be defined as crop, device, animal, etc. The data contains a total number of 12580 records. Each record has 39 attributes values.
Reference: [Mic73] <author> Michalski, </author> <title> R.S., </title> <booktitle> AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition in Proceeding of the First International Joint Conference on Pattern Recognition, </booktitle> <address> Washington, DC, </address> <pages> pp. 3-17, </pages> <month> October 30- November, </month> <year> 1973. </year>
Reference-contexts: The learned descriptions are represented in the form of a set of decision rules, expressed in an attributional logic calculus, called variable-valued logic 1 or VL1 <ref> [Mic73] </ref>. A distinctive feature of this representation is that it employs, in addition to standard logic operators, the internal disjunction operator (a disjunction of values of the same attribute in a condition) and the range operator (to express conditions involving a range of discrete or continuous values).
Reference: [Mic83] <author> Michalski, </author> <title> R.S., A Theory and Methodology of Inductive Learning, </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 20, </volume> <pages> pp. 111-116, </pages> <year> 1983. </year>
Reference-contexts: The rest of this section presents a brief description on those systems. 2.1. The AQ15 rule learning program AQ15 learns decision rules for a given set of decision classes from examples of decisions, using the STAR methodology <ref> [Mic83] </ref>. The simplest algorithm based on this methodology, called AQ, starts with a seed example of a given decision class, and generates a set of the most general conjunctive descriptions of the seed that do not cover examples of other decision classes (alternative decision rules for the seed example). <p> These operators help to simplify rules involving multivalued discrete attributes; the second operator is also used for creating logical expressions involving continuous attributes. AQ15 can generate decision rules that represent either characteristic or discriminant concept descriptions, depending on the settings of its parameters <ref> [Mic83] </ref>. A characteristic description states properties that are true for all objects in the concept. The simplest characteristic concept description is in the form of a single conjunctive rule (in general, it can be a set of such rules).
Reference: [MMH*86] <author> Michalski, R.S., Mozetic, I . , Hong, J., and Lavrac, N., </author> <title> The MultiPurpose Incremental Learning System AQ15 and its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> pp. 1041-1045, </pages> <address> Philadelphia, PA: </address> , <year> 1986. </year>
Reference-contexts: The Trade data is concerned with the import and export of goods between the US and the world. The experiment uses the time factor to partition the space of the discovery. It uses the learning system AQ15 <ref> [MMH*86] </ref> and the INLEN system [MKK*92a,b] to discover common pattern in the data. The Importance Score program (IS) (IMK93) is used to obtain the relevant attributes and to drive other types of information.
Reference: [MKK*92a] <author> Michalski, R.S., Kerschberg, L., Kaufman, K.A., and Ribeiro, J.S., </author> <title> (a) Searcing for Knowledge in Large Databases, </title> <booktitle> Proceedings of the First International Conference on Expert Systems and Development, </booktitle> <address> Cairo, Egypt, </address> <year> 1992. </year>
Reference: [MKK*92b] <author> Michalski, R.S., Kerschberg, L., Kaufman, K.A., and Ribeiro, J.S., </author> <title> (b) Mining for Knowledge in Databases: The INLEN Architecture, Initial Implementation and First Results, </title> <booktitle> Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, </booktitle> <volume> Vol. 1, No. 1, </volume> <month> August </month> <year> 1992. </year>
Reference: [Qui90] <author> Quinlan, J. R. </author> <title> Probabilistic decision trees, </title> <editor> in Y. Kodratoff and R.S. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> (pp. 63-111), </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: To overcome this problem, some discovery systems refine the database before the discovery process begins. The refinement operations reduce complexity, size, or noise in the data. Examples of such techniques include domain quantization, data reduction, partial classification, noise pruning, data optimization, etc. (e.g. [CS93]; [FI92]; [KMK91]; <ref> [Qui90] </ref>; attractive approach to simplifying the discovery process involves splitting the learning space into a set of subspaces. This can be done by taking one attribute out and generating as many subspaces, using the remaining attributes values, as the legal number of (quantized) values of that attribute.
Reference: [ S S 9 2 ] <author> Scheines, R., & Spirtes, </author> <title> P . , Finding Latent Variable Models in Large Databases, </title> <journal> International Journal of Intelligent Systems, Yager, R.R. (Ed.), pp. </journal> <volume> 609-621, Vol. 7, No. 7, </volume> <month> Sep., </month> <year> 1992. </year>
Reference: [ S c h 9 3 ] <author> Schlimmer, J., </author> <title> Using Learned Dependencies to Automatically Construct Sufficient and Sensible Editing Views, </title> <booktitle> Proceeding of the AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Washington D.C., </address> <month> July 11-12, </month> <year> 1993. </year>
Reference: [ S W 8 4 ] <author> Shoshani, A., & Wong, H.K.T., </author> <title> Characteristics of Scientific Databases, </title> <booktitle> Proceedings of the 10th International Conference on Very Large DataBases, </booktitle> <address> Singapore, </address> <month> August, </month> <year> 1984. </year>
Reference: [ U F S 9 1 ] <author> Uthurusamy, R., Fayyad, U., & Spangler, S., </author> <title> Learning Useful Rules from Inconclusive Data, Knowledge Discovery In Databases, </title> <editor> Shapiro, G., Frawley, W., (Eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference: [Zia91] <author> Ziarko, W., </author> <title> The Discovery, Analysis, and Representation of Data Dependencies in Databases, Knowledge Discovery In Databases, </title> <editor> Shapiro, G., Frawley, W., (Eds.), </editor> <publisher> AAAI Press, </publisher> <year> 1991. </year>
References-found: 18

