URL: http://www.umiacs.umd.edu/users/lsd/papers/ijcpr.ps.Z
Refering-URL: http://www.umiacs.umd.edu/users/lsd/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: High Performance Computing for Land Cover Dynamics  
Author: Rahul Parulekar Larry Davis Rama Chellappa Joel Saltz Alan Sussman John Townshend 
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies (UMIACS) University of Maryland  
Abstract: We present the overall goals of our research program on the application of high performance computing to remote sensing applications, specifically applications in land cover dynamics. This involves developing scalable and portable programs for a variety of image and map data processing applications, eventually integrated with new models for parallel I/O of large scale images and maps. Here, we focus on our research in classification of remotely sensed images, describing new parallel algorithms for classification using Markov Random Fields and for classifying segmentations of images using a multistage decision procedure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> An extended version available as University of Maryland Technical Report CS-TR-3052 and UMIACS-TR-93-29. </note>
Reference-contexts: to undergo substantial radiometric and geometric pre-processing to prepare them for data extraction using advanced algorithms. 3 Multiblock PARTI We have developed a runtime support library for parallelizing application codes that involve multiple structured grids (meshes), such as pyramids and quadtrees, on distributed memory machines, called the Multiblock Parti library <ref> [1, 6, 7] </ref>. These meshes may be nested (for multigrid codes) and/or irregularly coupled (called multiblock or irregularly coupled regular mesh problems). Several forms of run-time support are required for multiple structured grid applications.
Reference: [2] <author> Zeki Bozkus, Alok Choudhary, Geoffrey Fox, Tomasz Haupt, Sanjay Ranka, and Min-You Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <note> To appear in the Journal of Parallel and Distributed Computing, </note> <year> 1994. </year>
Reference-contexts: We now discuss the design of the runtime library. Since, in typical multiblock and multigrid applications, the number of blocks and their respective sizes is not known until runtime, the distribution of blocks onto processors is done at runtime. The distributed array descriptors (DADs) <ref> [2] </ref> for the arrays representing these blocks are, therefore, generated at runtime. Distributed array descriptors contain information about the portions of the arrays residing on each processor, and are used at runtime for performing communication and distributing loop iterations.
Reference: [3] <author> Peter Burt and Edward Adelson. </author> <title> The Laplacian pyramid as a compact image code. </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-31(4):532-540, </volume> <year> 1983. </year>
Reference-contexts: The algorithm is essentially the one described in the paper by Burt and Adelson <ref> [3] </ref>, with a few differences. The following is a brief summary of the algorithm. The first step is the building of a pyramid of low-pass filtered versions of the image. <p> In this implementation the operator at every such point was chosen as the locally symmetric 5x5 kernel function that resembles the Gaussian function, as was suggested in <ref> [3] </ref> with the peak value of the Gaussian kernel chosen as 0.4.
Reference: [4] <author> Al Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: We view these primitives as forming a portion of a portable, compiler independent, runtime support library. This library is currently implemented on the Intel iPSC/860 and Paragon, the Thinking Machines' CM-5, the IBM SP-1 and the PVM message passing environment for networks of workstations <ref> [4] </ref>. The design of the library is architecture independent and therefore it can be easily ported on any distributed memory parallel machine or any environment which supports message passing (e.g. Express). The library primitives can currently be invoked from Fortran or C programs.
Reference: [5] <author> Michael Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: This implies that the interaction between grid points is restricted to nearby neighbors. Such communication is handled by allocation of extra space at the beginning and end of each array dimension on each processor. These extra elements are called overlap, or ghost, cells <ref> [5] </ref>. Depending upon the data access pattern in a loop, the required data is copied from other processors and is stored in the overlap cells. The final form of support provided by the multi-block Parti library is to distribute loop iterations and transform global distributed array references into local references.
Reference: [6] <author> A. Sussman, J. Saltz, R. Das, S. Gupta, D. Mavriplis, R. Ponnusamy, and K. Crowley. </author> <title> PARTI primitives for unstructured and block structured problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):73-86, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: to undergo substantial radiometric and geometric pre-processing to prepare them for data extraction using advanced algorithms. 3 Multiblock PARTI We have developed a runtime support library for parallelizing application codes that involve multiple structured grids (meshes), such as pyramids and quadtrees, on distributed memory machines, called the Multiblock Parti library <ref> [1, 6, 7] </ref>. These meshes may be nested (for multigrid codes) and/or irregularly coupled (called multiblock or irregularly coupled regular mesh problems). Several forms of run-time support are required for multiple structured grid applications.
Reference: [7] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: to undergo substantial radiometric and geometric pre-processing to prepare them for data extraction using advanced algorithms. 3 Multiblock PARTI We have developed a runtime support library for parallelizing application codes that involve multiple structured grids (meshes), such as pyramids and quadtrees, on distributed memory machines, called the Multiblock Parti library <ref> [1, 6, 7] </ref>. These meshes may be nested (for multigrid codes) and/or irregularly coupled (called multiblock or irregularly coupled regular mesh problems). Several forms of run-time support are required for multiple structured grid applications.
References-found: 7

