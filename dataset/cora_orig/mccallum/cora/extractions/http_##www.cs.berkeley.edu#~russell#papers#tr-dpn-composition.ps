URL: http://www.cs.berkeley.edu/~russell/papers/tr-dpn-composition.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Title: Compositional Modeling With DPNs  
Author: Geoffrey Zweig Stuart Russell 
Address: Berkeley, California 94720  
Affiliation: Computer Science Division (EECS) University of California  
Date: September 1997  
Pubnum: Report No. UCB/CSD-97-970  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Constantin F. Aliferis and Gregory F. Cooper. </author> <title> A structurally and temporally extended Bayesian belief network model: Definitions, properties and modeling techniques. </title> <booktitle> In Proc. </booktitle> <address> UAI-96, </address> <year> 1996. </year>
Reference-contexts: Many other issues in temporal modeling with DPNs, especially the modeling of processes over multiple time scales, are discussed in <ref> [1] </ref>. <p> This is reasonable when there is an underlying physical system such as the vocal tract which behaves differently in different phonetic contexts, but which nevertheless has the same parts. The assumption can be relaxed easily <ref> [1] </ref>. 5 attached represent just one acceptable assignment of values to them. The tongue moves from the alveolar ridge to the back of the mouth; the lips move from an unrounded to a rounded configuration. The properties of each node are shown to the right.
Reference: [2] <author> J. Binder, D. Koller, S. Russell, K. </author> <title> Kanazawa. Adaptive Probabilistic Networks with Hidden Variables. </title> <note> To appear in Machine Learning. </note>
Reference-contexts: Hence these parameters can be estimated more accurately with a fixed amount of data [13, 7]. * There are efficient, general-purpose algorithms for doing inference [3, 12] and learning <ref> [2, 12] </ref> in DPNs. No special-purpose algorithms need be derived for handling representational extensions to HMMs that are required in areas such as speech recognition (see, e.g., [6]). * Sharing variables between submodels leads to a natural way of describing transitional behavior. <p> We refer to a specific set of observations as o; similarly, a specific assignment of values to the hidden state variables is denoted by s. Recent research has developed a set of algorithms for computing with probabilistic networks; <ref> [9, 2, 8] </ref> provide a good introduction. Specifically, there are dynamic programming procedures for 1. Computing the probability of an observation sequence: P (o) = P 2. Computing the most likely assignment of values to the hidden variables: max s P (o; s). 3. <p> The unfactored representation has exponentially more state-evolution parameters, and requires exponentially more examples to achieve equal performance. With just one hidden state variable, the two representations are identical. statistical advantages over a simpler unfactored representation, as suggested in <ref> [7, 2] </ref>. Figure 6 is illustrative. We generated data from a model with the topology shown at the bottom of Figure 1. We then learned the model parameters for both this factored model and an unfactored model with an equal amount of hidden state.
Reference: [3] <author> J. Binder, K. Murphy, S. Russell. </author> <title> Space-efficient inference in dynamic probabilistic networks. </title>
Reference-contexts: DPNs are factored representations of a probability distribution, and often have exponentially fewer parameters than unfactored representations such as standard HMMs. Hence these parameters can be estimated more accurately with a fixed amount of data [13, 7]. * There are efficient, general-purpose algorithms for doing inference <ref> [3, 12] </ref> and learning [2, 12] in DPNs. No special-purpose algorithms need be derived for handling representational extensions to HMMs that are required in areas such as speech recognition (see, e.g., [6]). * Sharing variables between submodels leads to a natural way of describing transitional behavior.
Reference: [4] <author> C. Burnett, J. Holzrichter, L. Ng, R. Leonard, and W. Lea. </author> <title> Micropower Radar Measurements of Human Vocal Articulator Motions. </title> <institution> Lawrence Livermore National Laboratory report UCRL-JC-124821. </institution> <year> 1996. </year> <month> 8 </month>
Reference-contexts: Without an explicit articulatory model, we achieve a word error rate of approximately 5%, and we are in the process of training a detailed articulatory model with articulatory data obtained from real-time, low-power radar imaging of the vocal tract <ref> [4] </ref>. 5 Discussion Compositional DPN modeling seems to be a flexible and expressive way to represent and learn complex stochastic temporal models. In addition to speech, we are applying this technology to automobile control and human driver modeling.
Reference: [5] <author> T. Dean and K. </author> <title> Kanazawa. Probabilistic Temporal Reasoning. </title> <booktitle> Proceedings of the Seventh National Con--ference on Artificial Intelligence. </booktitle> <pages> 524-528. </pages> <year> 1988. </year>
Reference-contexts: This is a fairly standard choice in areas such as speech and handwriting recognition, although some systems also use stochastic context-free grammars (SCFGs) for representing the higher-level structure of whole sentences. The second issue is submodel representation. Here we describe the use of dynamic probabilistic networks (DPNs) <ref> [5] </ref> (also called factorial HMMs [7]), which decompose the state of the process being modeled into a set of state variables. DPNs include both HMMs and Kalman filters as special cases [12, 11]. The use of DPNs for submodels confers several advantages over HMMs, which we describe below. <p> A probabilistic network has a convenient graphical representation in which the variables appear as nodes, and a variable's parents are specified by the arcs leading into it. In the dynamic case, a probabilistic network models a system as it evolves over time <ref> [5] </ref>. At each point in time, the values X 1 ; : : :; X n are of interest. For example, to model car-driving, we are interested in lane-position and speed at each point in time.
Reference: [6] <author> J. Erler and G. Freeman. </author> <title> An HMM-based Speech Recognizer using Overlapping Articulatory Features. </title> <journal> Journal of the Acoustical Society of America. </journal> <volume> 100(1) 2500-2513. </volume> <year> 1996. </year>
Reference-contexts: No special-purpose algorithms need be derived for handling representational extensions to HMMs that are required in areas such as speech recognition (see, e.g., <ref> [6] </ref>). * Sharing variables between submodels leads to a natural way of describing transitional behavior. This is important for modeling coarticulation in speech recognition where the pronunciation of a phonetic unit depends on the positions of the tongue, jaw, and other articulators at the end of the preceding phonetic unit.
Reference: [7] <author> Z. Ghahramani and M. Jordan. </author> <title> Factorial Hidden Markov Models. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: The second issue is submodel representation. Here we describe the use of dynamic probabilistic networks (DPNs) [5] (also called factorial HMMs <ref> [7] </ref>), which decompose the state of the process being modeled into a set of state variables. DPNs include both HMMs and Kalman filters as special cases [12, 11]. The use of DPNs for submodels confers several advantages over HMMs, which we describe below. <p> Our approach has the following advantages: * Statistical efficiency. DPNs are factored representations of a probability distribution, and often have exponentially fewer parameters than unfactored representations such as standard HMMs. Hence these parameters can be estimated more accurately with a fixed amount of data <ref> [13, 7] </ref>. * There are efficient, general-purpose algorithms for doing inference [3, 12] and learning [2, 12] in DPNs. <p> The unfactored representation has exponentially more state-evolution parameters, and requires exponentially more examples to achieve equal performance. With just one hidden state variable, the two representations are identical. statistical advantages over a simpler unfactored representation, as suggested in <ref> [7, 2] </ref>. Figure 6 is illustrative. We generated data from a model with the topology shown at the bottom of Figure 1. We then learned the model parameters for both this factored model and an unfactored model with an equal amount of hidden state.
Reference: [8] <author> D. Heckerman. </author> <title> A Tutorial on Learning Bayesian Networks. </title> <institution> Microsoft Research, MSR-TR-95-06. </institution> <year> 1995. </year>
Reference-contexts: We refer to a specific set of observations as o; similarly, a specific assignment of values to the hidden state variables is denoted by s. Recent research has developed a set of algorithms for computing with probabilistic networks; <ref> [9, 2, 8] </ref> provide a good introduction. Specifically, there are dynamic programming procedures for 1. Computing the probability of an observation sequence: P (o) = P 2. Computing the most likely assignment of values to the hidden variables: max s P (o; s). 3.
Reference: [9] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann Publishers. </publisher> <year> 1988. </year>
Reference-contexts: The work of [12] also emphasizes the modeling power of DPNs in the context of speech recognition, but does not deal with submodel concatenation. 2 Probabilistic Networks and Dynamic Probabilistic Networks In recent years, probabilistic or Bayesian networks <ref> [9] </ref> have emerged as the primary method for representing and manipulating probabilistic information in the AI community. <p> We refer to a specific set of observations as o; similarly, a specific assignment of values to the hidden state variables is denoted by s. Recent research has developed a set of algorithms for computing with probabilistic networks; <ref> [9, 2, 8] </ref> provide a good introduction. Specifically, there are dynamic programming procedures for 1. Computing the probability of an observation sequence: P (o) = P 2. Computing the most likely assignment of values to the hidden variables: max s P (o; s). 3.
Reference: [10] <author> L. Rabiner and B. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <booktitle> Prentice-Hall Signal Processing Series. </booktitle> <year> 1993. </year>
Reference: [11] <author> S. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: The second issue is submodel representation. Here we describe the use of dynamic probabilistic networks (DPNs) [5] (also called factorial HMMs [7]), which decompose the state of the process being modeled into a set of state variables. DPNs include both HMMs and Kalman filters as special cases <ref> [12, 11] </ref>. The use of DPNs for submodels confers several advantages over HMMs, which we describe below. However, composing DPN submodels is not quite as simple as composing HMMs.
Reference: [12] <author> P. Smyth, D. Heckerman, and M. Jordan. </author> <title> Probabilistic independence networks for hidden Markov probability models. </title> <booktitle> Neural Computation 9(2), </booktitle> <pages> 227-269. </pages> <year> 1997. </year>
Reference-contexts: The second issue is submodel representation. Here we describe the use of dynamic probabilistic networks (DPNs) [5] (also called factorial HMMs [7]), which decompose the state of the process being modeled into a set of state variables. DPNs include both HMMs and Kalman filters as special cases <ref> [12, 11] </ref>. The use of DPNs for submodels confers several advantages over HMMs, which we describe below. However, composing DPN submodels is not quite as simple as composing HMMs. <p> DPNs are factored representations of a probability distribution, and often have exponentially fewer parameters than unfactored representations such as standard HMMs. Hence these parameters can be estimated more accurately with a fixed amount of data [13, 7]. * There are efficient, general-purpose algorithms for doing inference <ref> [3, 12] </ref> and learning [2, 12] in DPNs. No special-purpose algorithms need be derived for handling representational extensions to HMMs that are required in areas such as speech recognition (see, e.g., [6]). * Sharing variables between submodels leads to a natural way of describing transitional behavior. <p> Hence these parameters can be estimated more accurately with a fixed amount of data [13, 7]. * There are efficient, general-purpose algorithms for doing inference [3, 12] and learning <ref> [2, 12] </ref> in DPNs. No special-purpose algorithms need be derived for handling representational extensions to HMMs that are required in areas such as speech recognition (see, e.g., [6]). * Sharing variables between submodels leads to a natural way of describing transitional behavior. <p> Many other issues in temporal modeling with DPNs, especially the modeling of processes over multiple time scales, are discussed in [1]. The work of <ref> [12] </ref> also emphasizes the modeling power of DPNs in the context of speech recognition, but does not deal with submodel concatenation. 2 Probabilistic Networks and Dynamic Probabilistic Networks In recent years, probabilistic or Bayesian networks [9] have emerged as the primary method for representing and manipulating probabilistic information in the AI
Reference: [13] <author> G. Zweig. </author> <title> A Forward-Backward Algorithm for Inference in Bayesian Networks and An Empirical Comparison with HMMs. </title> <type> Master's Thesis, </type> <institution> University of California at Berkeley. </institution> <year> 1996. </year> <month> 9 </month>
Reference-contexts: Our approach has the following advantages: * Statistical efficiency. DPNs are factored representations of a probability distribution, and often have exponentially fewer parameters than unfactored representations such as standard HMMs. Hence these parameters can be estimated more accurately with a fixed amount of data <ref> [13, 7] </ref>. * There are efficient, general-purpose algorithms for doing inference [3, 12] and learning [2, 12] in DPNs.
References-found: 13

