URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-39.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: TOWARDS DOMAIN-INDEPENDENT MACHINE INTELLIGENCE  
Author: Robert Levinson 
Keyword: Conceptual graphs, semantic distance, similarity metric, machine learning, reinforcement learning, pattern construction, pattern recognition, adaptation, domain independence, nearest neighbor, experience-based learning.  
Address: Santa Cruz, CA 95064 U.S.A  
Affiliation: Department of Computer and Information Sciences University of California Santa Cruz  
Pubnum: UCSC-CRL-93-39  
Email: E-mail: levinson@cis.ucsc.edu  
Phone: Phone: 408-459-2097 FAX: 408-459-4829  
Date: November 4, 1993  
Abstract: Adaptive predictive search (APS), is a learning system framework, which given little initial domain knowledge, increases its decision-making abilities in complex problems domains. In this paper we give an entirely domain-independent version of APS that we are implementing in the PEIRCE conceptual graphs workbench. By using conceptual graphs as the "base language" a learning system is capable of refining its own pattern language for evaluating states in the given domain that it finds itself in. In addition to generalizing APS to be domain-independent and CG-based we describe fundamental principles for the development of AI systems based on the structured pattern approach of APS. It is hoped that this effort will lead the way to a more principled, and well-founded approach to the problems of mechanizing machine intelligence. The APS framework has been applied to a number of complex problem domains (including chess, othello, pente and image alignment) where the combinatorics of the state space is large and the learning process only receives reinforcement at the end of each search. The unique features of the APS framework are its pattern-weight representation of control knowledge and its integration of several learning techniques including temporal difference learning, simulated annealing, and genetic algorithms. Morph, an APS chess sytem, is now being translated into PEIRCE. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [1] <author> J.L. Aronson. </author> <title> Truth and semantic networks. </title> <booktitle> In Fourth Annual Workshop on Conceptual Structures, </booktitle> <pages> pages 161-171, </pages> <year> 1989. </year>
Reference-contexts: In fact, all else being equal, the most useful patterns are those with extreme weights, that occur frequently and have a low standard deviation (weight variance over time). Sowa and others <ref> [1, 17] </ref> also evaluate maximal joins between a query graph and a database graph by using other syntactic criteria (such as path distance between types and common subtypes) that ignore the context in which the join will be used.
Reference: [2] <author> H.W. Davis and S. Chenoweth. </author> <title> The mathematical modeling of heuristics. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <pages> pages 191-228, </pages> <year> 1992. </year>
Reference-contexts: These node pairs would also be labeled with the correlation with the reinforcement value. A matrix with all correlation values (unary and binary) we will call a "correlation structure". Existing well-understood statistical methods are fully applicable here <ref> [2] </ref>, yet rarely have they been used as a basis for symbolic representation. As statistics is the science of processing numerical experiential data (to reveal mathematical structure), its uses within AI have been dramatically under exploited. 4.
Reference: [3] <author> J. Gould and R. Levinson. </author> <title> Experience-based adaptive search. </title> <booktitle> In Machine Learning:A Multi-Strategy Approach, </booktitle> <volume> volume 4. </volume> <publisher> Morgan Kauffman, </publisher> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: In the past patterns have been inserted into this database through the following four methods: search context rules, generalization and specialization, reverse engineering, and genetic operators <ref> [3] </ref>. The actual database insertion and retrieval takes place using algorithms that exploit the structure in the partial order to do a minimal number of comparison tests [5, 7].
Reference: [4] <author> R. E. Korf. </author> <title> Learning to solve problems by searching for macro-operators. </title> <publisher> Pitman, </publisher> <year> 1985. </year>
Reference-contexts: For example states in the 8-puzzle <ref> [4] </ref> could be represented as collections of nine position-tile patterns (see * A mechanism for periodically rewarding or punishing the system based on its performance. In principle this is all that would be provided to the system and all that is expected.
Reference: [5] <author> R. Levinson. </author> <title> Pattern associativity and the retrieval of semantic networks. Computers and Mathematics with Applications, </title> <booktitle> 23(6-9):573-600, 1992. Part 2 of Special Issue on Semantic Networks in Artificial Intelligence, </booktitle> <editor> Fritz Lehmann, editor. </editor> <booktitle> Also reprinted on pages 573-600 of the book, Semantic Networks in Artificial Intelligence, </booktitle> <editor> Fritz Lehmann, editor, </editor> <publisher> Pergammon Press, </publisher> <year> 1992. </year>
Reference-contexts: The actual database insertion and retrieval takes place using algorithms that exploit the structure in the partial order to do a minimal number of comparison tests <ref> [5, 7] </ref>. Search context rules are the only pattern addition scheme that does not rely on patterns already in the database; thus, they are the only way patterns are added to an empty database. <p> We are assuming that a state evaluated in later games (search experiences) has a more accurate value than states evaluated in earlier games. 4. Now, to evaluate a state S one uses the "semantic distance" mechanism of the Method IV retrieval scheme <ref> [5, 7] </ref> which will return for each graph a "closeness" determined as follows: (a) Convert S to a conceptual graph as always. (b) Insert S in the database. <p> The structure that represents this adjacency information we term an "adjacency hypergraph". 5. The collection of the compatibility matrix, correlation matrix and the adjacency hypergraph we term "basis conceptual structure" for the given problem. 6. Using refinement <ref> [5] </ref> equivalence classes of nodes in the basis structure may be identified using some theshhold for matching the real-valued correlations. Nodes with similar properties should be treated as similar in processing.
Reference: [6] <author> R. Levinson, B. Beach, R. Snyder, T. Dayan, and K. Sohn. </author> <title> Adaptive-predictive game-playing programs. </title> <journal> Journal of Experimental and Theoretical AI, </journal> <note> 1992. To appear. Also appears as Tech Report UCSC-CRL-90-12, </note> <institution> University of California, Santa Cruz. </institution>
Reference-contexts: Since APS adheres to the experience-based learning framework, it can be applied to new domains without requiring the programmer to be an expert in the domain. In fact, APS has been applied to a variety of domains including chess (we call this system "Morph"), othello, pente, and image alignment <ref> [6] </ref>. <p> Although Morph has been given very little assistance it would be false to say that it was supplied "no" domain knowledge. In earlier versions of APS, the system was supplied a pattern representation language by the user that was deemed to be appropriate for the given problem domain <ref> [6] </ref>. For example, some patterns constructed for the game Othello are displayed in Figure 0.1. In the case of Morph, a graph representation of attacks and defends relationships was provided, as well as a notion of material. See Figure 0.2 for an example. <p> Finally, in a system with many components (pws) to be adjusted, learning rates should be allowed to differ across these components. Simulated annealing provides this capability. APS has produced encouraging results in a variety of domains studied as classroom projects <ref> [6] </ref>, including Othello, Tetris, 8-puzzle, Tic-Tac-Toe, Pente, image alignment, Missionary and Cannibals and more. Currently, others are studying the application of the Morph-APS shell 1 to GO, Shogi and Chinese Chess.
Reference: [7] <author> R. Levinson and G. Ellis. </author> <title> Multilevel hierarchical retrieval. </title> <note> Knowledge-Based Systems, 1992. To appear. </note>
Reference-contexts: The actual database insertion and retrieval takes place using algorithms that exploit the structure in the partial order to do a minimal number of comparison tests <ref> [5, 7] </ref>. Search context rules are the only pattern addition scheme that does not rely on patterns already in the database; thus, they are the only way patterns are added to an empty database. <p> We are assuming that a state evaluated in later games (search experiences) has a more accurate value than states evaluated in earlier games. 4. Now, to evaluate a state S one uses the "semantic distance" mechanism of the Method IV retrieval scheme <ref> [5, 7] </ref> which will return for each graph a "closeness" determined as follows: (a) Convert S to a conceptual graph as always. (b) Insert S in the database.
Reference: [8] <author> Robert Levinson. </author> <title> A pattern-weight formulation of search knowledge. </title> <type> Technical Report UCSC-CRL-91-15, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year> <note> Revision to appear in Computational Intelligence. </note>
Reference-contexts: The major reason for using pws over another form of knowledge representation is their uniformity. Pws can simulate other forms of search control and due to their low level of granularity and uniformity more power and flexibility is possible <ref> [8] </ref>. For example, pws have all the expressive power of macro tables. Additionally, they allow switching over from one macro sequence to another and allow for the combination of two or more macro tables [8]. <p> and due to their low level of granularity and uniformity more power and flexibility is possible <ref> [8] </ref>. For example, pws have all the expressive power of macro tables. Additionally, they allow switching over from one macro sequence to another and allow for the combination of two or more macro tables [8].
Reference: [9] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga Press, </publisher> <year> 1983. </year>
Reference-contexts: A search context rule is a deterministic procedure that builds up a pattern given the previously mentioned inputs. In concept induction schemes <ref> [9, 11, 13, 16] </ref> the goal is to find a concept description to correctly classify a set of positive and negative examples. In general, the smaller description that does the job, the better. Sometimes the concept description needs to be made more specific to make a further distinction.
Reference: [10] <author> S. Minton. </author> <title> Constraint based generalization- learning game playing plans from single examples. </title> <booktitle> In Proceedings of AAAI-84, </booktitle> <pages> pages 251-254. </pages> <publisher> AAAI, </publisher> <year> 1984. </year>
Reference-contexts: The utility of a pattern can be measured as a function of many factors including age, number of updates, uses, size, extremeness and variance. These attributes will be elaborated upon in the next section. We are exploring a variety of utility functions <ref> [10] </ref>. Using such functions, patterns below a certain level of utility can be deleted. Deletion is also necessary for efficiency considerations: the larger the database, the slower the system learns.
Reference: [11] <author> T. M. Mitchell, J. G. Carbonell, and R. S. Michalski, </author> <title> editors. Machine Learning: A Guide to Current Research. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year>
Reference-contexts: A search context rule is a deterministic procedure that builds up a pattern given the previously mentioned inputs. In concept induction schemes <ref> [9, 11, 13, 16] </ref> the goal is to find a concept description to correctly classify a set of positive and negative examples. In general, the smaller description that does the job, the better. Sometimes the concept description needs to be made more specific to make a further distinction.
Reference: [12] <author> T. M. Mitchell, R. Keller, and S. Kedar-Cabelli. </author> <title> Explanation based generalization: A unifying view. </title> <booktitle> In Machine Learning 1, </booktitle> <pages> pages 47-80. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1986. </year>
Reference-contexts: Successive weights in the macro sequence will gradually approach a favorable reinforcement; thus, the system is then motivated to move in this direction. Reverse engineering extends a macro sequence by adding a pattern. This extension is similar to Explanation-Based-Generalization (EBG) <ref> [12] </ref> or goal regression. The idea is to take the most important pattern in one state, s 1 , and back it up to get its preconditions in the preceding state, s 2 . These preconditions then form a new pattern p 2 .
Reference: [13] <author> T Niblett and A. Shapiro. </author> <title> Automatic induction of classification rules for chess endgames. </title> <type> Technical Report MIP-R-129, </type> <institution> Machine Intelligence Research Unit, University of Edinburgh, </institution> <year> 1981. </year>
Reference-contexts: A search context rule is a deterministic procedure that builds up a pattern given the previously mentioned inputs. In concept induction schemes <ref> [9, 11, 13, 16] </ref> the goal is to find a concept description to correctly classify a set of positive and negative examples. In general, the smaller description that does the job, the better. Sometimes the concept description needs to be made more specific to make a further distinction.
Reference: [14] <author> S. Omohundro. </author> <title> Efficient algorithms with neural network behavior. </title> <type> Technical Report UIUCDCS-R-87-1331, </type> <institution> University of Illinois, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: Either use the weight (pre-stored evaluation) of the closest match or a weighted average of a set of close matches of varying degree. This scheme is similar to the "nearest neighbor" approaches applied to bit vectors in pattern recognition or information retreival tasks <ref> [14] </ref>. Here these approaches are being generalized to graphs and higher level features are being developed dynamically. Further, closeness is a function of the reinforcement values rather than assigned apriori.
Reference: [15] <author> Barney Pell. </author> <title> A computer game-learning tournament. </title> <note> (In Preparation), </note> <year> 1991. </year>
Reference-contexts: For example, the patterns above can be represented in CGs as in Figure 0.3 . We can now see that APS involves the automated construction of a semantic distance function over unpreclassified To test these ideas APS is being applied in the "uninformed MetaGame universe" <ref> [15] </ref>. This is a universe of chess-like games in which the system is not supplied the rules or objective of the game it is playing!. The structure of the rest of the paper is as follows. Section 2 discusses the current APS system framework in detail.
Reference: [16] <author> J. R. Quinlan. </author> <title> Induction on decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: A search context rule is a deterministic procedure that builds up a pattern given the previously mentioned inputs. In concept induction schemes <ref> [9, 11, 13, 16] </ref> the goal is to find a concept description to correctly classify a set of positive and negative examples. In general, the smaller description that does the job, the better. Sometimes the concept description needs to be made more specific to make a further distinction.
Reference: [17] <author> A. Ralescu and A. Fadlalla. </author> <title> The issue of semantic distance in knowledge representation with conceptual graphs. </title> <booktitle> In Proceedings of Fifth Annual Workshop on Conceptual Structures, </booktitle> <pages> pages 141-142, </pages> <year> 1990. </year>
Reference-contexts: In fact, all else being equal, the most useful patterns are those with extreme weights, that occur frequently and have a low standard deviation (weight variance over time). Sowa and others <ref> [1, 17] </ref> also evaluate maximal joins between a query graph and a database graph by using other syntactic criteria (such as path distance between types and common subtypes) that ignore the context in which the join will be used.
Reference: [18] <author> C. E. Shannon. </author> <title> Programming a computer for playing chess. </title> <journal> Philosophical Magazine, </journal> <volume> 41(7) </volume> <pages> 256-275, </pages> <year> 1950. </year> <note> See also Levy, </note> <editor> D.N.L. (ed) (1988), </editor> <title> Computer Games I, </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <pages> pp. 81-88. </pages>
Reference-contexts: abovementioned requirements of the complex problem domains considered in this paper: The game tree forms the state space, each game representing a search path through the space; reinforcement is only provided at the end of the game; and, finally, it has a large cardinality of states (around 10 40 ) <ref> [18] </ref>. Furthermore, few efforts use previous search experience in this area despite the high costs of search. In order to focus the research on the learning mechanisms, Morph has been constrained to using only one-ply of search.
Reference: [19] <author> J. F. Sowa. </author> <title> Conceptual Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: may not be the case since the useful patterns may be very remote from the state descriptions, but in practice these methods should serve well , especially when coupled with the statistics-based relation learning methods described below. 0.3.2 Relationship to previous work on semantic distance and in conceptual graphs Sowa <ref> [19] </ref> defines semantic distance based on a frequency interpretation. Types that are shared are considered to have importance inversely proportional to the frequency of occurence of them or their subtypes in "schemata" or stored database graphs.
Reference: [20] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This method differs from supervised learning, where the value of each state S i is moved toward the reinforcement value. It has been shown that TD learning converges faster than the supervised method for Markov model learning <ref> [20] </ref>. The success of TD learning stems from the fact that value adjustments are localized. Thus, if the system makes decisions all the way through the search and makes a mistake toward the end, the earlier decisions are not harshly penalized.
Reference: [21] <author> S. Watanabe. </author> <title> Pattern Recognition:Human and Mechanical. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Once these weights are determined how should they be combined? 0.3.1 Giving autonomy to APS: semantic distance discovery In the book Pattern Recognition <ref> [21] </ref>, Watanabe gives the remarkable Theorem of the Ugly Duckling: Insofar as we use a finite set of predicates that are capable of distinguishing any two objects considered, the number of predicates shared by any two such objects is constant, independent of the choice of the two objects.
References-found: 22

