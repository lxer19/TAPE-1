URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-253.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-253/
Root-URL: http://www.cs.dartmouth.edu
Title: Low-level Interfaces for High-level Parallel I/O  
Author: Nils Nieuwejaar David Kotz 
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Note: Appeared at the Workshop for I/O in Parallel and Distributed Systems at IPPS '95, pp. 47-62. Available at  
Pubnum: PCS-TR95-253  
Email: fnils,dfkg@cs.dartmouth.edu  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR95-253.ps.Z.  
Abstract: As the I/O needs of parallel scientific applications increase, file systems for multiprocessors are being designed to provide applications with parallel access to multiple disks. Many parallel file systems present applications with a conventional Unix-like interface that allows the application to access multiple disks transparently. By tracing all the activity of a parallel file system in a production, scientific computing environment, we show that many applications exhibit highly regular, but non-consecutive I/O access patterns. Since the conventional interface does not provide an efficient method of describing these patterns, we present three extensions to the interface that support strided, nested-strided, and nested-batched I/O requests. We show how these extensions can be used to express common access patterns. 
Abstract-found: 1
Intro-found: 1
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: This research was supported in part by the NASA Ames Research Center under Agreement Number NCC 2-849. 2 The Conventional Interface Many existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed. <p> While our focus in this paper is the improvement of the interface to a linear file model, the enhancement or outright replacement of that model is worthy of further investigation. One common enhancement to the conventional interface is a shared file pointer <ref> [Pie89, RP95, BGST93, FBD94] </ref>, which provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174).
Reference: [BHK + 91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ouster-hout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <year> 1991. </year>
Reference-contexts: A common characteristic of many file-system workloads, particularly scientific file-system workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file-system workload, we found that while almost 93% of all files were accessed sequentially, 2 consecutive access was primarily limited to those files that were only opened by one compute node.
Reference: [CBF93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <year> 1993. </year>
Reference-contexts: It should be noted that this interface could be built on top of the extensions described above. 5.2 Vesta The Vesta file system <ref> [CBF93, CF94, FCHP95] </ref> breaks away from the traditional one-dimensional file structure. Files in Vesta are two-dimensional and are partitioned according to explicit user commands.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: It should be noted that this interface could be built on top of the extensions described above. 5.2 Vesta The Vesta file system <ref> [CBF93, CF94, FCHP95] </ref> breaks away from the traditional one-dimensional file structure. Files in Vesta are two-dimensional and are partitioned according to explicit user commands.
Reference: [CFH + 95] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: Watson Research Center, which derives much of its philosophy and interface from the MPI message-passing standard <ref> [CFH + 95] </ref>. In MPI-IO, file I/O is modeled as message passing. That is, reading from a file is analogous to receiving a message and writing to a file is analogous to sending a message.
Reference: [Cra94] <institution> Cray Research. </institution> <note> listio manual page, 1994. Publication SR-2012. </note>
Reference-contexts: The only vendor we are aware of that provides a strided interface is Cray Research, but their interface is currently not offered on their massively parallel T3D machines <ref> [Cra94] </ref>. The following interface allows applications to issue simple-strided requests: bytes = read strided (fid, buf, offset, record size, stride size, quant) Beginning at offset, the file system will read quant records of record size bytes, and store them contiguously in memory at buf.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year> <month> 15 </month>
Reference-contexts: where the user must request one small piece of data at a time, forcing the file system to service requests in a particular order. 5 Other Unconventional Interfaces 5.1 nCUBE A file-system interface proposed for the nCUBE is based on a two-step mapping of a file into the compute-node memories <ref> [DdR92] </ref>. The first step is to provide a mapping from subfiles stored on multiple disks to an abstract dataset (a traditional one-dimensional I/O stream). The second step is mapping the 12 abstract dataset into the compute-node memories.
Reference: [FBD94] <author> Craig S. Freedman, Josef Burger, and David J. Dewitt. </author> <title> SPIFFI | a scalable parallel file system for the Intel Paragon. </title> <note> Submitted to IEEE TPDS, </note> <year> 1994. </year>
Reference-contexts: While our focus in this paper is the improvement of the interface to a linear file model, the enhancement or outright replacement of that model is worthy of further investigation. One common enhancement to the conventional interface is a shared file pointer <ref> [Pie89, RP95, BGST93, FBD94] </ref>, which provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174).
Reference: [FCHP95] <author> Dror G. Feitelson, Peter F. Corbett, Yarson Hsu, and Jean-Pierre Prost. </author> <title> Parallel I/O systems and interfaces for parallel computers. In Multiprocessor Systems | Design and Integration. </title> <publisher> World Scientific, </publisher> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: It should be noted that this interface could be built on top of the extensions described above. 5.2 Vesta The Vesta file system <ref> [CBF93, CF94, FCHP95] </ref> breaks away from the traditional one-dimensional file structure. Files in Vesta are two-dimensional and are partitioned according to explicit user commands.
Reference: [IBM94] <author> IBM. </author> <title> AIX Version 3.2 General Programming Concepts, </title> <booktitle> twelfth edition, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: For those applications, we introduce a nested-batched interface. One common example of a batched I/O interface may be seen in the POSIX lio listio () function, which allows the user to submit a list of simple read ()/write () requests in a single operation <ref> [IBM94] </ref>.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In <ref> [KN94] </ref>, we discuss the results of a workload characterization study in which we recorded all the parallel file-system activity on an iPSC/860 at NASA Ames' Numerical Aerodynamics Simulation (NAS) facility. <p> It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially [MK91]. Results in <ref> [KN94, PEK + 95] </ref>, however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment. <p> The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). Intel's CFS provides this mode in addition to several more structured access modes (e.g., round-robin access to the file pointer) [Pie89]. However, the tracing study described in <ref> [KN94] </ref> found that CFS's shared file pointers are rarely used in practice and suggests that poor performance and a failure to match the needs of applications are the likely causes. 3 Access Patterns To this point, most parallel file systems have been optimized to support large (many kilobyte) file accesses. <p> The workload study described in <ref> [KN94] </ref> shows that while some parallel scientific applications do issue a relatively small number of large requests, there are many applications that issue thousands or millions of small (&lt; 200 bytes) requests, putting a great deal of stress on current file systems. As in [KN94] we define a sequential request to <p> The workload study described in <ref> [KN94] </ref> shows that while some parallel scientific applications do issue a relatively small number of large requests, there are many applications that issue thousands or millions of small (&lt; 200 bytes) requests, putting a great deal of stress on current file systems. As in [KN94] we define a sequential request to be one that is at a higher file offset than the previous request from the same compute node, and a consecutive request to be a sequential request that begins where the previous request ended. <p> We define an interval to be the distance between the end of one access and the beginning of the next. While <ref> [KN94] </ref> shows that almost 99% of all files are accessed with fewer than 3 different intervals, that study made no distinction between single-node and multi-node files. <p> Since many models of physical events require logically adjacent nodes to share boundary information, this could be an important restriction. This can be seen in the file-sharing results in <ref> [KN94] </ref>, which show that most read-only files had at least some bytes that were accessed by multiple processors. 13 It should be noted that the same results show that in many cases, the strict partitioning offered by nCUBE and Vesta may match the applications' needs for write-only files. 5.3 MPI-IO MPI-IO
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Not only would reducing the number of requests lower the aggregate latency costs (particularly for those applications that issue thousands or millions of very small requests), but recent work has shown that providing a file system with this level of information can lead to tremendous performance improvements <ref> [Kot94] </ref>. We introduce three new interfaces in increasing order of complexity and power. While these interfaces are intended to be used in a multiprocessor file system where files will be shared among multiple processes, we have not included any primitives to explicitly control synchronization or file sharing. <p> This ability to reorder data transfers can be used to achieve remarkable performance gains <ref> [Kot94] </ref>, and is a distinct advantage of this interface over any interface where the user must request one small piece of data at a time, forcing the file system to service requests in a particular order. 5 Other Unconventional Interfaces 5.1 nCUBE A file-system interface proposed for the nCUBE is based
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: This research was supported in part by the NASA Ames Research Center under Agreement Number NCC 2-849. 2 The Conventional Interface Many existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed.
Reference: [LMKQ89] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: One common enhancement to the conventional interface is a shared file pointer [Pie89, RP95, BGST93, FBD94], which provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in <ref> [LMKQ89] </ref>, page 174). Intel's CFS provides this mode in addition to several more structured access modes (e.g., round-robin access to the file pointer) [Pie89].
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Experience has shown that this simple model of a file is well suited to uniprocessor applications that tend to access files in a simple, sequential fashion [OCH + 85]. It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially <ref> [MK91] </ref>. Results in [KN94, PEK + 95], however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment. <p> A common characteristic of many file-system workloads, particularly scientific file-system workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file-system workload, we found that while almost 93% of all files were accessed sequentially, 2 consecutive access was primarily limited to those files that were only opened by one compute node. <p> Looking more closely, we found that while 51% of all multi-node files were accessed at most once by each node (i.e., there were 0 intervals) and 16% of all multi-node files had only 1 interval, over 26% of multi-node files had 5 or more different intervals. Since previous studies <ref> [MK91] </ref> have shown that scientific applications rarely access files randomly, the fact that a large number of multi-node files have many different intervals suggests that these files are being accessed in some complex, but possibly regular, pattern. 3.1 Strided accesses Although files may be opened by multiple nodes simultaneously, we are
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Experience has shown that this simple model of a file is well suited to uniprocessor applications that tend to access files in a simple, sequential fashion <ref> [OCH + 85] </ref>. It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially [MK91]. Results in [KN94, PEK + 95], however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment. <p> A common characteristic of many file-system workloads, particularly scientific file-system workloads, is that files are accessed consecutively <ref> [OCH + 85, BHK + 91, MK91] </ref>. In the parallel file-system workload, we found that while almost 93% of all files were accessed sequentially, 2 consecutive access was primarily limited to those files that were only opened by one compute node.
Reference: [PEK + 95] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: It has similarly proven to be appropriate for scientific, vector applications that also tend to access files sequentially [MK91]. Results in <ref> [KN94, PEK + 95] </ref>, however, show that sequential access to consecutive portions of a file is much less common in a multiprocessor environment.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: This research was supported in part by the NASA Ames Research Center under Agreement Number NCC 2-849. 2 The Conventional Interface Many existing multiprocessor file systems are based on the conventional Unix-like file-system interface in which files are seen as an addressable, linear stream of bytes <ref> [BGST93, Pie89, LIN + 93] </ref>. To provide higher throughput, the file system typically declusters files (i.e., scatters the blocks of each file across multiple disks), thus allowing parallel access to the file, reducing the effect of the bottleneck imposed by the relatively slow disk speed. <p> While our focus in this paper is the improvement of the interface to a linear file model, the enhancement or outright replacement of that model is worthy of further investigation. One common enhancement to the conventional interface is a shared file pointer <ref> [Pie89, RP95, BGST93, FBD94] </ref>, which provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). <p> The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174). Intel's CFS provides this mode in addition to several more structured access modes (e.g., round-robin access to the file pointer) <ref> [Pie89] </ref>.
Reference: [RP95] <author> Brad Rullman and David Payne. </author> <title> An efficient file I/O interface for parallel applications. DRAFT presented at the Workshop on Scalable I/O, </title> <booktitle> Frontiers '95, </booktitle> <month> February </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: While our focus in this paper is the improvement of the interface to a linear file model, the enhancement or outright replacement of that model is worthy of further investigation. One common enhancement to the conventional interface is a shared file pointer <ref> [Pie89, RP95, BGST93, FBD94] </ref>, which provides a mechanism for regulating access to a shared file by multiple processes in a single application. The simplest shared file pointer is one which supports an atomic-append mode (as in [LMKQ89], page 174).
References-found: 19

