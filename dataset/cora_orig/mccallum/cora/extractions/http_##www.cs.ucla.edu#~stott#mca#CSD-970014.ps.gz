URL: http://www.cs.ucla.edu/~stott/mca/CSD-970014.ps.gz
Refering-URL: http://www.cs.ucla.edu/~stott/mca/
Root-URL: http://www.cs.ucla.edu
Title: Monte Carlo Arithmetic: a framework for the statistical analysis of roundoff error practical benefits. It
Author: D. Stott Parker Paul R. Eggert Brad Pierce 
Keyword: floating-point arithmetic, floating-point rounding, roundoff error, random rounding, ANSI/IEEE floating-point standards, significance arithmetic, Monte Carlo methods  
Note: Randomization has both theoretical and  of  AMS(MOS) subject classifications: 65C05, 65C20, 65G05, 65G10, 65J05, 68M07, 62P99  
Address: Los Angeles, CA 90095-1596  360 N. Sepulveda Blvd., Suite 2055 El Segundo, CA 90245-4462  Los Angeles, CA 90095-1596  
Affiliation: Computer Science Department University of California  Twin Sun, Inc.  Computer Science Department University of California  
Email: stott@cs.ucla.edu  eggert@twinsun.com  pierce@cs.ucla.edu  
Date: March 30, 1997  
Web: error.  
Abstract: Monte Carlo Arithmetic (MCA) is an extension of standard floating-point arithmetic that exploits randomness in basic floating-point operations. MCA uses randomization to implement random rounding | which forces roundoff errors to be randomly distributed | and random unrounding | which randomly extends the inputs of arithmetic operations to arbitrary precision. Random rounding can be used to produce roundoff errors that are truly random and uncorrelated, and that have zero expected bias. Random unrounding detects catastrophic cancellation, which is the primary way that significant digits are lost in numerical computation. Randomization also can be used to vary precision dynamically, and to implement inexact values (values known to only a few significant digits). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> American National Standards Institute, </institution> <month> ANSI/IEEE Std </month> <year> 754-1985: </year> <title> IEEE standard for binary floating-point arithmetic, </title> <address> New York, 12 Aug. </address> <year> 1985. </year> <title> See also: "An American National Standard, IEEE standard for binary floating-point arithmetic", </title> <journal> SIGPLAN Notices, </journal> <volume> 22:2, </volume> <pages> 9-25, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: For example, rather than insist that floating-point addition obey x y = f l [ x + y ] = (x + y) (1 + ffi) where ffi is some deterministically-defined value (such as the relative error incurred by rounding to the nearest floating-point number, as in IEEE floating-point arithmetic <ref> [1, 15, 43] </ref>), we allow ffi to be a random variable. The result of every arithmetic operation is randomized in a predefined way. As a result, the addition x y can yield different values if evaluated multiple times. <p> Conventional floating-point arithmetic, which adopts the assumptions of forward error analysis, makes the simplest possible guess | it treats all operands as if they were exact. For example, it extends single-precision numbers to double-precision format by padding the significands with zeros. In the IEEE floating-point standard <ref> [1] </ref>, the `inexact result' (or `precision') flag bit in the status word indicates whether the result would be exact if the operands were exact 3 . 3.2 Randomization An alternative approach is to guess a random real value for an inexact floating-point number.
Reference: [2] <author> R. Alt, J. Vignes, </author> <title> "Validation of results of collocation methods for ODEs with the CADNA library", </title> <journal> Appl. Numer. Math. </journal> <volume> 21:2, </volume> <pages> 119-139, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Notice that MCA makes no such assumptions or claims; a detailed comparison of MCA with CESTAC and CADNA is made in [57]. Another significant problem presented by CADNA is that comparisons, and hence boolean val ues, can become inexact. In CADNA, comparisons are based on statistics <ref> [2, p.133] </ref>: x &lt;fl y b x b y &lt; ff b 2 y where b x is the computed average of x, b x is its computed standard deviation, and ff is half the Student t confidence interval width depending on the desired confidence level 1 ff.
Reference: [3] <author> R.L. Ashenhurst, N. </author> <title> Metropolis, "Unnormalized Floating Point Arithmetic", </title> <journal> J. ACM 6:3, </journal> <pages> 415-428, </pages> <month> July </month> <year> 1959. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 14 4.5 Automatic monitoring of significance Significance arithmetic was popularized by Ashenhurst and Metropolis in a series of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [3, 4, 50] </ref>. A basic problem is defining what is meant by "the number of significant digits" [59, x3.1]. Significance arithmetic is difficult to implement when this measure is a real number, but is less compelling when it is always an integer.
Reference: [4] <author> R.L. Ashenhurst, N. </author> <title> Metropolis, "Error Estimation in Computer Calculation", in Computers and Computing, </title> <journal> AMM Slaught Memorial Papers, American Mathematical Monthly 72:2, </journal> <pages> 47-48, </pages> <month> February </month> <year> 1965. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 14 4.5 Automatic monitoring of significance Significance arithmetic was popularized by Ashenhurst and Metropolis in a series of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [3, 4, 50] </ref>. A basic problem is defining what is meant by "the number of significant digits" [59, x3.1]. Significance arithmetic is difficult to implement when this measure is a real number, but is less compelling when it is always an integer.
Reference: [5] <author> J.-C. Bajard, D. Michelucci, J.-M. Moreau, J.-M. Muller, </author> <title> Introduction to the Special Issue on "Real Numbers and Computers", </title> <journal> Journal of Universal Computer Science 1:7, </journal> <pages> page 438, </pages> <month> Jul 28, </month> <year> 1995. </year> <title> Available on the internet at many sites. </title>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 5 2.2 Two startling examples Consider the following two marvelous examples adapted from <ref> [5] </ref>. First, define the sequence (x k ) x 0 = 1:5100050721319 k 20x 3 k 24) = (4x 3 k + 70x k 50): As demonstrated in Table 5, depending on the precision of one's machine, the sequence converges to either 1, 2, 3, or 4.
Reference: [6] <author> J.L. Barlow, </author> <title> E.H. Bareiss, "On Roundoff Error Distributions in Floating Point and Logarithmic Arithmetic", </title> <booktitle> Computing 34:4, </booktitle> <pages> 325-347, </pages> <year> 1985. </year>
Reference-contexts: Other statistical studies of computer arithmetic include <ref> [6, 7, 17, 47, 56] </ref>. In [56], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [7] <author> R.P. Brent, </author> <title> "Static and Dynamic Numerical Characteristics of Floating-Point Arithmetic", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-22:6, </volume> <pages> 598-607, </pages> <month> June </month> <year> 1973. </year>
Reference-contexts: Other statistical studies of computer arithmetic include <ref> [6, 7, 17, 47, 56] </ref>. In [56], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [8] <author> M.-C. Brunet, F. Chatelin, "CESTAC, </author> <title> a tool for a stochastic round-off error analysis in scientific computing", </title> <note> pp. 11-20 in Numerical Mathematics and Applications, </note> <editor> R. Vichnevetsky and J. Vignes (eds.), Elsevier/North-Holland, </editor> <year> 1986. </year>
Reference-contexts: In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method. As described in <ref> [8] </ref>, CESTAC works on numerical programs in which each floating-point expression `X *Y ' has been preprocessed to PER (X *Y ), so for example X +Y +Z becomes PER (PER (X +Y )+Z), 7 Kahan remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling <p> Vignes' survey [69] accompanies an entire journal issue with papers describing applications of CESTAC. Both PER and the perturbation mode of PEPER work identically <ref> [8, 69] </ref>: when the underlying machine arithmetic works with ordinary rounding, PER adds a least significant bit 1 with probability 1 4 , adds 0 with probability 1 2 , and subtracts 1 with probability 1 4 .
Reference: [9] <author> W. Buchholz, </author> <title> Planning a Computer System: Project Stretch, </title> <publisher> NY: McGraw-Hill, </publisher> <year> 1962. </year>
Reference-contexts: Significance arithmetic is difficult to implement when this measure is a real number, but is less compelling when it is always an integer. Their work was one of the inspirations for the Project Stretch design <ref> [9] </ref>. Its floating-point system included a distant relative of random normalization, called `noisy mode', that could be used to monitor significance loss: To aid in significance studies, a noisy mode is provided in which the low-order bits of results are modified. <p> Running the same problem twice, first in the normal mode and then in the noisy mode, gives an estimate of the significance of the results. || <ref> [9, p.25] </ref> After an extensive search, the most effective technique [for monitoring significance loss] turned out to be both elegant and remarkably simple. ... Any operand or result that is shifted left to be normalized requires a corresponding number of zeros to be shifted in at the right. ... <p> Furthermore, significance loss is relatively rare, so that running a problem twice when significance loss is suspected does not pose a serious problem. What is serious is the possibility of unsuspected significance loss. || <ref> [9, p.102] </ref> Interval computation [51, 52], which generalizes floating-point numbers from single values to statistical objects, has failed to gain widespread popularity, because it is very pessimistic [72, pp.566-567].
Reference: [10] <author> A.C. Callahan, </author> <title> "Random rounding: Some principles and applications", </title> <booktitle> Proc. 1976 IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Philadelphia, PA, USA, </address> <month> 12-14 April </month> <year> 1976, </year> <pages> 501-504, </pages> <year> 1976. </year>
Reference-contexts: Henrici was intrigued by this experimental approach, and showed that their results could be predicted analytically to within an error of 10% [35]. Callahan <ref> [10] </ref> demonstrates the use of random rounding in signal processing applications, stressing the usefulness of truly random roundoff errors. He discusses an often-used simple recursive integrator that can have "dramatic correlated quantization error effects unless random rounding is employed" [10, p.501]. 6 See also [44] and [36, x1.17,x2.6]. <p> Callahan [10] demonstrates the use of random rounding in signal processing applications, stressing the usefulness of truly random roundoff errors. He discusses an often-used simple recursive integrator that can have "dramatic correlated quantization error effects unless random rounding is employed" <ref> [10, p.501] </ref>. 6 See also [44] and [36, x1.17,x2.6]. Copyright c fl1996, 1997 D. Stott Parker, Paul R.
Reference: [11] <author> S.L. Campbell, C.D. Meyer, Jr., </author> <title> Generalized Inverses of Linear Transformations, </title> <publisher> NY: Pitman, 1979; reprinted by Dover Publications, </publisher> <year> 1991. </year>
Reference: [12] <author> F. Chatelin, M.-C. Brunet, </author> <title> "A probabilistic round-off error propagation model. Application to the eigenvalue problem", </title> <booktitle> in [16], </booktitle> <pages> 139-160, </pages> <year> 1990. </year>
Reference-contexts: analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth [46] and Sterbenz [59, x3.1.2] Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., <ref> [12, 13] </ref>, and very recently [14]). Her work is exceptionally clear and rigorous, and careful about its assumptions. <p> When the operands are inexact, on the other hand, both operations can give inexact results. Chatelin and Brunet warn <ref> [12] </ref> that perturbation of the algorithm itself (affecting its branching structure, and not just perturbing its input) probably makes the function computed by the program nonanalytic, and requires qualitatively different analysis. We agree.
Reference: [13] <author> F. Chatelin, V. Fraysse, </author> <title> "Elements of a Condition Theory for the Computational Analysis of Algorithms", in Iterative Methods in Linear Algebra, </title> <editor> R. Beauwens and P. de Groen (eds.), </editor> <publisher> Elsevier North-Holland, </publisher> <pages> 15-25, </pages> <year> 1992. </year>
Reference-contexts: analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth [46] and Sterbenz [59, x3.1.2] Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., <ref> [12, 13] </ref>, and very recently [14]). Her work is exceptionally clear and rigorous, and careful about its assumptions.
Reference: [14] <author> F. Chaitin-Chatelin, V. Fraysse, </author> <title> Lectures on Finite Precision Computations, </title> <address> Philadelphia: </address> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: digit frequencies, etc., are surveyed by Knuth [46] and Sterbenz [59, x3.1.2] Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., [12, 13], and very recently <ref> [14] </ref>). Her work is exceptionally clear and rigorous, and careful about its assumptions. <p> Since 1988 Chaitin-Chatelin has developed a MATLAB toolbox called PRECISE that allows users to perform statistical backward error analysis and sensitivity analysis experiments, with emphasis on linear system solution, eigencomputations, polynomial root finding, and general nonlinear (matrix or polynomial) equation solving under componentwise or normwise perturbations <ref> [14] </ref>. 4.4 Random rounding Let x be a real number that is to be rounded to a floating-point format F .
Reference: [15] <author> W.J. Cody, J.T. Coonen, D.M. Gay, K. Hanson, et al. </author> <title> "A proposed radix- and word-length-independent standard for floating-point arithmetic", </title> <journal> SIGNUM Newsletter 20:1, </journal> <pages> 37-51, </pages> <month> Jan. </month> <year> 1985. </year> <title> Copyright c fl1996, </title> <editor> 1997 D. Stott Parker, Paul R. </editor> <title> Eggert, </title> <type> Brad Pierce 21 </type>
Reference-contexts: For example, rather than insist that floating-point addition obey x y = f l [ x + y ] = (x + y) (1 + ffi) where ffi is some deterministically-defined value (such as the relative error incurred by rounding to the nearest floating-point number, as in IEEE floating-point arithmetic <ref> [1, 15, 43] </ref>), we allow ffi to be a random variable. The result of every arithmetic operation is randomized in a predefined way. As a result, the addition x y can yield different values if evaluated multiple times.
Reference: [16] <author> M.G. Cox, S. Hammarling, </author> <title> Reliable Numerical Computation, </title> <publisher> NY: Oxford University Press, </publisher> <year> 1990. </year>
Reference: [17] <author> J.W. Demmel, </author> <title> "The probability that a numerical analysis problem is difficult", </title> <booktitle> Mathematics of Computation 50:182, </booktitle> <pages> 449-480, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Other statistical studies of computer arithmetic include <ref> [6, 7, 17, 47, 56] </ref>. In [56], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [18] <author> W. Feller, </author> <title> An Introduction to Probability Theory and its Applications, </title> <publisher> NY: J. Wiley & Sons, </publisher> <year> 1968. </year>
Reference: [19] <author> G.S. Fishman, </author> <title> Monte Carlo: Concepts, Algorithms, and Applications, </title> <publisher> NY: Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: As Kahan [44, p.24] points out, computations can be "virulently unstable but in a way that almost always diverges to the same wrong destination". 1 Fast random number generators can be implemented efficiently with feedback shift registers <ref> [19, x7.15-16] </ref>. Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 3 2 Some Examples 2.1 A simple example Kahan (e.g., [24, 41, 42]) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic. <p> An experimental quantity e x is then viewed as having a true value x and an observational error (inaccuracy, inexactness, uncertainty) ~, which is modeled as a random variable. 4.2 Monte Carlo methods Monte Carlo methods <ref> [19, 32, 33] </ref> solve problems by treating them as experiments involving random variables. The random variables can represent truly random physical processes, or just abstractions of deterministic processes. <p> Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals. Today Monte Carlo analysis is enjoying a resurgence of interest <ref> [19, 54] </ref>. 4.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 [55] and 1951 [28], who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling [37, p.7] were
Reference: [20] <author> G.E. Forsythe, R.A. Leibler, </author> <title> "Matrix inversion by a Monte Carlo method.", Mathematical Tables and Other Aids to Computation 4, </title> <type> 127-129, </type> <year> 1950. </year>
Reference-contexts: By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results [49]. Interest in Monte Carlo methods in the 1950s spread to many areas of numerical analysis, and this is reflected in the paper by Forsythe and Leibler <ref> [20] </ref> (elaborating an idea of von Neumann and Ulam), and in Householder's book [38]. Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals.
Reference: [21] <author> G.E. </author> <title> Forsythe, "Round-off errors in numerical integration on automatic machinery. Preliminary report", </title> <journal> Bull. AMS 56, </journal> <volume> 61, </volume> <year> 1950. </year>
Reference-contexts: This definition is equivalent to that in Section 3.3. Random rounding was considered in the early 1950s by George Forsythe <ref> [21, 22] </ref>. He noted [23] that, although Huskey's recent work [40] on the ENIAC showed roundoff errors in some problems are not distributed like independent random variables 6 , better error estimates could be obtained if they were. <p> He suggested that they can be forced to be random: It seems clear that in the integration of smooth functions the ordinary rounding-off errors will frequently not be distributed like independent random variables. To circumvent [the problems of correlated errors * noted by Huskey], the present writer <ref> [21] </ref> has proposed a random rounding-off procedure which make * a true random variable. Suppose, for example, that a real number u is to be rounded off to an integer. Let [u] be the greatest integer not exceeding u, and let u [u] = v. <p> Forsythe [23] In concluding <ref> [21] </ref>, Forsythe announces: "Tests with I.B.M. equipment indicate that random roundoff probably eliminates a priori the peculiarities of round-off found by Huskey on the ENIAC." Henrici [34, ch.5] gives a good review of early statistical analyses of roundoff error in numerical integration, and argues that it can be analyzed statistically. <p> With MCA, this argument does not work. This is demonstrated with the equivalent plot, Figure 4, produced with MCA using uniform input randomization and deterministic IEEE default rounding. Randomization forces the roundoff errors to be random. Forsythe predicted randomization would have this effect <ref> [21] </ref>, but because this problem has considerable cancellation, we have used input randomization, and not just the random rounding proposed by Forsythe. Figure 5 also shows the distribution of these values. The computation of rp involves 16 arithmetic operations, and the resulting distribution is quite close to normal.
Reference: [22] <author> G.E. </author> <title> Forsythe, "Note on rounding-off errors" (review by J. Todd), </title> <journal> Math. Rev. </journal> <volume> 12, 208, </volume> <year> 1951. </year>
Reference-contexts: This definition is equivalent to that in Section 3.3. Random rounding was considered in the early 1950s by George Forsythe <ref> [21, 22] </ref>. He noted [23] that, although Huskey's recent work [40] on the ENIAC showed roundoff errors in some problems are not distributed like independent random variables 6 , better error estimates could be obtained if they were.
Reference: [23] <author> G.E. </author> <title> Forsythe, "Reprint of a note on rounding-off errors", </title> <journal> SIAM Review 1:1, </journal> <pages> 66-67, </pages> <year> 1959. </year> <title> Originally written June 1950 at the National Bureau of Standards, </title> <address> Los Angeles, CA, </address> <note> and abstracted in [22]. </note>
Reference-contexts: This definition is equivalent to that in Section 3.3. Random rounding was considered in the early 1950s by George Forsythe [21, 22]. He noted <ref> [23] </ref> that, although Huskey's recent work [40] on the ENIAC showed roundoff errors in some problems are not distributed like independent random variables 6 , better error estimates could be obtained if they were. <p> Forsythe <ref> [23] </ref> In concluding [21], Forsythe announces: "Tests with I.B.M. equipment indicate that random roundoff probably eliminates a priori the peculiarities of round-off found by Huskey on the ENIAC." Henrici [34, ch.5] gives a good review of early statistical analyses of roundoff error in numerical integration, and argues that it can be
Reference: [24] <author> G. </author> <title> Forsythe, "Solving a quadratic equation on a computer", </title> <booktitle> The Mathematical Sciences, </booktitle> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 3 2 Some Examples 2.1 A simple example Kahan (e.g., <ref> [24, 41, 42] </ref>) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic.
Reference: [25] <author> C.F. </author> <title> Gauss, Theoria Combinationis Observationum Erroribus Minimis Obnoxiae (Theory of the Combination of Observations Least Subject to Errors), translated by G.W. Stewart, </title> <address> Philadelphia, PA: </address> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: The probability that x has a value that lies within standard deviations of its mean is given by the error function Pr [ jx j ] = erf ( = 2 ) = p Z e 1 dz: For example, in <ref> [25, x9] </ref>, Gauss noted that the probabilities of an error lying within = 1, = 2:57, and = 3:89 standard deviations are, respectively, 0.6827, 0.99, and 0.9999. <p> In the 1820s Gauss revised the theory of errors and the method of least squares in Theoria Combinationis Observationum Erroribus Minimis Obnoxiae, of which an excellent recent edition by Stewart is now available <ref> [25] </ref>. This work is remarkable in many ways, and mainstream scientific and statistical discipline have so thoroughly adopted its ideas that much of it today seems like common sense. <p> Several noteworthy advances of this work were that Gauss removed his earlier assumption that the error distributions be normal (proposing instead to measure the central value and precision of a distribution by its mean and standard deviation , which he formulated in terms of moments <ref> [25, x5-8] </ref>), considered the problem of estimating the precision of the samples from their standard deviation, and criticized the use of the `biased' standard deviation estimate b naive = v u n i=1 where b is the average of the samples. <p> Gauss argued that it tends to overstate the precision, and that it should be replaced <ref> [25, x38] </ref> by the unbiased estimate b = u t 1 n X ( x i b ) 2 where (typically 1) is the number of parameters involved. This estimate is called unbiased since the expected value of ( b 2 2 ) is zero.
Reference: [26] <author> P.E. Gill, W. Murray, M.H. Wright, </author> <title> Numerical Linear Algebra and Optimization, </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [27] <author> D. Goldberg, </author> <title> "What every computer scientist should know about floating-point arithmetic", </title> <journal> ACM Computing Surveys 23:1, </journal> <pages> 5-48, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 7 3 Monte Carlo Arithmetic We assume the reader is familiar with basic issues in floating-point arithmetic. Goldberg's tutorial <ref> [27] </ref> and Higham's encyclopedia [36, chs.1-5] are great references. A more complete presentation of most of this material is in [57]. 3.1 Exact and inexact values Exact values are real numbers that can be exactly represented in a given floating-point format. <p> Thus statistical inference is embedded in the comparison operator, and stochastic arithmetic is similar to the classical theory of propagation of error (or uncertainty) [61]. MCA presents this problem as well. In MCA, we recommend defining comparisons in terms of the underlying arithmetic operations, as programmers often assume this <ref> [27] </ref>: x &lt;fl y (x y) &lt; 0: Actually, every fixed-precision arithmetic system will present this problem.
Reference: [28] <author> H.H. Goldstine, J. von Neumann, </author> <title> "Numerical inverting of matrices of high order. II", </title> <journal> Proc. Amer. Math. Soc. </journal> <volume> 2, </volume> <pages> 188-202, </pages> <year> 1951. </year>
Reference-contexts: Today Monte Carlo analysis is enjoying a resurgence of interest [19, 54]. 4.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 [55] and 1951 <ref> [28] </ref>, who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling [37, p.7] were not reflective of practice. <p> In the sequel paper <ref> [28] </ref>, following the development of Monte Carlo, Goldstine and von Neumann gave statistical bounds on the results of Gaussian elimination using the norms of a random matrix.
Reference: [29] <author> H.H. Goldstine, </author> <title> A History of Numerical Analysis from the 16th Through the 19th Century, NY: Springer-Verlag, </title> <booktitle> Studies in the History of Mathematics and Physical Sciences 2, </booktitle> <year> 1977. </year>
Reference-contexts: Gauss became the target of scornful allegations by Legendre, who had published his treatise on least squares in 1805 (cf. <ref> [29, p.210] </ref>, [60, pp.145-146]). Copyright c fl1996, 1997 D. Stott Parker, Paul R.
Reference: [30] <author> H.H. Goldstine, </author> <title> The Computer: From Pascal to von Neumann, </title> <publisher> Princeton University Press, </publisher> <year> 1993. </year>
Reference-contexts: We sensed that at least for positive definite matrices the Gaussian procedure could be shown to be quite stable. | Goldstine <ref> [30, p.290] </ref> In [55, p.1036], von Neumann and Goldstine argued that modeling roundoff errors with independent probabilistic estimates is natural since we know their average and worst-case values, and are ignorant of their exact distribution.
Reference: [31] <author> G.H. Golub, C.F. Van Loan, </author> <title> Matrix Computations: Second Edition, </title> <publisher> Baltimore: Johns Hop-kins University Press, </publisher> <year> 1989. </year>
Reference: [32] <author> J.H. Halton, </author> <title> "A Retrospective and Prospective Survey of the Monte Carlo Method", </title> <journal> SIAM Review 12:1, </journal> <pages> 1-63, </pages> <year> 1970. </year>
Reference-contexts: An experimental quantity e x is then viewed as having a true value x and an observational error (inaccuracy, inexactness, uncertainty) ~, which is modeled as a random variable. 4.2 Monte Carlo methods Monte Carlo methods <ref> [19, 32, 33] </ref> solve problems by treating them as experiments involving random variables. The random variables can represent truly random physical processes, or just abstractions of deterministic processes.
Reference: [33] <author> J.M. </author> <title> Hammersley, D.C. Handscomb, Monte Carlo Methods, </title> <publisher> NY: Chapman and Hall, </publisher> <year> 1965. </year>
Reference-contexts: An experimental quantity e x is then viewed as having a true value x and an observational error (inaccuracy, inexactness, uncertainty) ~, which is modeled as a random variable. 4.2 Monte Carlo methods Monte Carlo methods <ref> [19, 32, 33] </ref> solve problems by treating them as experiments involving random variables. The random variables can represent truly random physical processes, or just abstractions of deterministic processes. <p> A review of ENIAC in 1946 brought together Enrico Fermi, Stanislaw Ulam, and John von Neumann. In order to simulate random neutron diffusion in fissile material (extending the atomic bomb development of World War II), Ulam and von Neumann developed the Monte Carlo approach throughout 1947 <ref> [33, 53] </ref>. By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results [49].
Reference: [34] <author> P. Henrici, </author> <title> Error Propagation for Difference Methods, </title> <publisher> NY: J. Wiley & Sons, </publisher> <year> 1963. </year> <title> Copyright c fl1996, </title> <editor> 1997 D. Stott Parker, Paul R. </editor> <title> Eggert, </title> <type> Brad Pierce 22 </type>
Reference-contexts: Forsythe [23] In concluding [21], Forsythe announces: "Tests with I.B.M. equipment indicate that random roundoff probably eliminates a priori the peculiarities of round-off found by Huskey on the ENIAC." Henrici <ref> [34, ch.5] </ref> gives a good review of early statistical analyses of roundoff error in numerical integration, and argues that it can be analyzed statistically.
Reference: [35] <author> P. Henrici, </author> <title> "Tests of Probabilistic Models for the Propagation of Roundoff Errors", </title> <journal> Comm. ACM 9:6, </journal> <pages> 409-410, </pages> <month> June </month> <year> 1966. </year>
Reference-contexts: Henrici was intrigued by this experimental approach, and showed that their results could be predicted analytically to within an error of 10% <ref> [35] </ref>. Callahan [10] demonstrates the use of random rounding in signal processing applications, stressing the usefulness of truly random roundoff errors. He discusses an often-used simple recursive integrator that can have "dramatic correlated quantization error effects unless random rounding is employed" [10, p.501]. 6 See also [44] and [36, x1.17,x2.6].
Reference: [36] <author> N.J. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <address> Philadelphia, PA: </address> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 7 3 Monte Carlo Arithmetic We assume the reader is familiar with basic issues in floating-point arithmetic. Goldberg's tutorial [27] and Higham's encyclopedia <ref> [36, chs.1-5] </ref> are great references. A more complete presentation of most of this material is in [57]. 3.1 Exact and inexact values Exact values are real numbers that can be exactly represented in a given floating-point format. <p> An excellent summary of these papers is given by Wilkinson [72], and also by Higham <ref> [36, x9.6] </ref>, who reproduces the following commentary by Goldstine about this work: 5 We did not feel it reasonable that so skilled a computer as Gauss would have fallen into the trap that Hotelling thought he had noted ... <p> Callahan [10] demonstrates the use of random rounding in signal processing applications, stressing the usefulness of truly random roundoff errors. He discusses an often-used simple recursive integrator that can have "dramatic correlated quantization error effects unless random rounding is employed" [10, p.501]. 6 See also [44] and <ref> [36, x1.17,x2.6] </ref>. Copyright c fl1996, 1997 D. Stott Parker, Paul R. <p> With the right statistical caveats, we can thus formally prove that Monte Carlo arithmetic avoids certain floating-point anomalies. See [57] for more details. Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 17 5.2 Roundoff errors actually do become random Kahan [44] and others (e.g., Higham <ref> [36, x1.17,x2.6] </ref>) argue that statistical analyses of roundoff error are improperly founded because they assume roundoff errors are random.
Reference: [37] <author> H. Hotelling, </author> <title> "Some new methods in matrix inversion", </title> <journal> Ann. Math. Stat. </journal> <volume> 14, </volume> <pages> 1-34, </pages> <year> 1943. </year>
Reference-contexts: of interest [19, 54]. 4.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 [55] and 1951 [28], who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling <ref> [37, p.7] </ref> were not reflective of practice.
Reference: [38] <author> A.S. </author> <title> Householder, Principles of Numerical Analysis, </title> <publisher> NY: McGraw-Hill, </publisher> <address> 1953. </address> <publisher> Reprinted by Dover Publications, </publisher> <year> 1974. </year>
Reference-contexts: Interest in Monte Carlo methods in the 1950s spread to many areas of numerical analysis, and this is reflected in the paper by Forsythe and Leibler [20] (elaborating an idea of von Neumann and Ulam), and in Householder's book <ref> [38] </ref>. Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals. <p> Specific other points that can be argued about MCA include: * MCA is a simple way to bring some of the benefits of the Monte Carlo method <ref> [38] </ref> to floating-point computation. The Monte Carlo method offers simplicity; it replaces exact computation with random sampling, and replaces exact analysis with statistical analysis. * MCA is a probabilistic way to detect occurrences of catastrophic cancellation in numeric computations.
Reference: [39] <author> T.E. Hull, J.R. Swenson, </author> <title> "Tests of Probabilistic Models for Propagation of Roundoff Errors", </title> <journal> Comm. ACM 9:2, </journal> <pages> 108-113, </pages> <month> February </month> <year> 1966. </year>
Reference-contexts: Random rounding (`probabilistic rounding') was used by Hull and Swenson <ref> [39] </ref> in order to test his hypothesis that ordinary rounding can be modeled statistically as a random process. Their experiments confirmed that the cumulative roundoff errors obtained with random rounding were similar to those obtained with ordinary rounding, and that probabilistic models of roundoff propagation are generally valid.
Reference: [40] <author> H.D. Huskey, </author> <title> "On the precision of a certain procedure of numerical integration", </title> <editor> J. </editor> <booktitle> Research National Bureau of Standards 42, </booktitle> <pages> 57-62, </pages> <year> 1949. </year> <title> Includes the appendix: D.R. Hartree, "Note on Systematic Rounding-off Errors in Numerical Integration", </title> <publisher> p.62. </publisher>
Reference-contexts: This definition is equivalent to that in Section 3.3. Random rounding was considered in the early 1950s by George Forsythe [21, 22]. He noted [23] that, although Huskey's recent work <ref> [40] </ref> on the ENIAC showed roundoff errors in some problems are not distributed like independent random variables 6 , better error estimates could be obtained if they were.
Reference: [41] <author> W. Kahan, </author> <title> "A survey of error analysis," invited paper, </title> <booktitle> Proc. IFIP Congress 1971, </booktitle> <pages> 200-206, </pages> <month> August </month> <year> 1971. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 3 2 Some Examples 2.1 A simple example Kahan (e.g., <ref> [24, 41, 42] </ref>) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic.
Reference: [42] <author> W. Kahan, </author> <title> "The programming environment's contribution to program robustness," </title> <journal> SIGNUM Newsletter, </journal> <month> Oct. </month> <year> 1981, </year> <month> p.10. </month>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 3 2 Some Examples 2.1 A simple example Kahan (e.g., <ref> [24, 41, 42] </ref>) has stressed that even computations as simple as solving ax 2 bx + c = 0 present interesting problems for floating-point arithmetic.
Reference: [43] <author> W.V. Kahan, </author> <title> "Lecture Notes on the Status of IEEE Standard 754 for Binary Floating-Point Arithmetic" (work in progress), </title> <institution> Dept. of Elect. Eng. & Computer Science, UC Berkeley, </institution> <note> dated May 31, 1996. Currently available as: http://http.cs.berkeley.edu/~wkahan/ieee754status/ieee754.ps </note>
Reference-contexts: For example, rather than insist that floating-point addition obey x y = f l [ x + y ] = (x + y) (1 + ffi) where ffi is some deterministically-defined value (such as the relative error incurred by rounding to the nearest floating-point number, as in IEEE floating-point arithmetic <ref> [1, 15, 43] </ref>), we allow ffi to be a random variable. The result of every arithmetic operation is randomized in a predefined way. As a result, the addition x y can yield different values if evaluated multiple times.
Reference: [44] <author> W.V. Kahan, </author> <title> "The Improbability of PROBABILISTIC ERROR ANALYSES for Numerical Computations", </title> <booktitle> lecture notes prepared for the UC Berkeley Statistics Colloquium, </booktitle> <month> 28 Febru-ary </month> <year> 1996, </year> <note> and subsequently revised (4 March 1996). (An earlier version of this lecture was presented at the third ICIAM Congress, 3-7 July, 1995.) Currently available as: http://http.cs.berkeley.edu/~wkahan/improber.ps </note>
Reference-contexts: However, each of these has drawbacks. Monte Carlo error analysis is not intended to replace these methods, but rather to complement them. Competent engineers rightly distrust all numerical computations and seek corroboration from alternative numerical methods, from scale models, from prototypes, from experience, ... . | W. Kahan <ref> [44, p.34] </ref> We simply hope that Monte Carlo analysis of roundoff error will prove to be a useful addition to the toolbox of competent engineers. With randomized floating-point arithmetic, if a program is applied to the same input several times, then each recalculation yields a slightly different answer. <p> It should be emphasized that Monte Carlo error analysis can give no iron-clad guarantees that serious roundoff error is absent in a computation. As Kahan <ref> [44, p.24] </ref> points out, computations can be "virulently unstable but in a way that almost always diverges to the same wrong destination". 1 Fast random number generators can be implemented efficiently with feedback shift registers [19, x7.15-16]. Copyright c fl1996, 1997 D. Stott Parker, Paul R. <p> Callahan [10] demonstrates the use of random rounding in signal processing applications, stressing the usefulness of truly random roundoff errors. He discusses an often-used simple recursive integrator that can have "dramatic correlated quantization error effects unless random rounding is employed" [10, p.501]. 6 See also <ref> [44] </ref> and [36, x1.17,x2.6]. Copyright c fl1996, 1997 D. Stott Parker, Paul R. <p> which each floating-point expression `X *Y ' has been preprocessed to PER (X *Y ), so for example X +Y +Z becomes PER (PER (X +Y )+Z), 7 Kahan remarks that the CESTAC patents can be circumvented with IEEE standard arithmetic by randomly toggling the IEEE directed rounding control bits <ref> [44, p.19] </ref>. Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 15 where PER is a function that implements random perturbation. With CESTAC, the program is executed 2 or 3 times, and its results (output variables) are stored. <p> A value that has no significant digits is treated as an `informational zero' [68], and when tested by a comparison operator these zeroes produce exceptions. Kahan <ref> [44] </ref> has raised strong objections to the CESTAC approach, taking special issue with its assumption that roundoff errors are normally distributed. Kahan demonstrates examples for which a CESTAC-based software package makes "extravagantly optimistic" claims of accuracy. <p> Kahan demonstrates examples for which a CESTAC-based software package makes "extravagantly optimistic" claims of accuracy. He takes a strong stand, saying at one point that "probabilistic estimates of error are probably useless or worse" <ref> [44, p.7] </ref>. Notice that MCA makes no such assumptions or claims; a detailed comparison of MCA with CESTAC and CADNA is made in [57]. Another significant problem presented by CADNA is that comparisons, and hence boolean val ues, can become inexact. <p> With the right statistical caveats, we can thus formally prove that Monte Carlo arithmetic avoids certain floating-point anomalies. See [57] for more details. Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 17 5.2 Roundoff errors actually do become random Kahan <ref> [44] </ref> and others (e.g., Higham [36, x1.17,x2.6]) argue that statistical analyses of roundoff error are improperly founded because they assume roundoff errors are random.
Reference: [45] <author> D.E. Knuth, </author> <title> The Art of Computer Programming. Vol. II: Seminumerical Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1969. </year>
Reference-contexts: Rounding discards information, hence increases inexactness. 2 This is an important distinction. In scientific computation most quantities are inexact; constants like Avogadro's number 6:0225 fi 10 23 are known only to a few digits <ref> [45, x4.2.2.B] </ref>. The inexactness can be due to ignorance, uncertainty, estimation, measurement or computational error, but the upshot is that we have only a few significant digits. Sterbenz [59] mentions a variety of proposals for implementing this distinction in computer arithmetic. <p> By transforming floating-point arithmetic into a Monte Carlo discipline we obtain many useful statistical properties. 5.1 Some floating-point anomalies are avoided It is well-known that floating-point arithmetic is not associative. Knuth gives the following example for eight-digit decimal arithmetic <ref> [45, p.196] </ref>: ( 11111113: 11111111: ) 7:5111111 = 2:0000000 7:5111111 = 9:5111111; This anomaly is a direct manifestation of catastrophic cancellation.
Reference: [46] <author> D.E. Knuth, </author> <booktitle> The Art of Computer Programming. Vol. II: Seminumerical Algorithms, 2nd edition, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1981. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 13 Early statistical analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth <ref> [46] </ref> and Sterbenz [59, x3.1.2] Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., [12, 13], and very recently [14]).
Reference: [47] <author> D. Kuck, D.S. Parker, A.H. Sameh, </author> <title> "Analysis of Floating-Point Rounding Methods", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-26:7, </volume> <pages> 643-650, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: Other statistical studies of computer arithmetic include <ref> [6, 7, 17, 47, 56] </ref>. In [56], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [48] <author> M. La Porte, J. Vignes, </author> <title> "Methode numerique de detection de la singularite d'une matrice", </title> <journal> Numer. Math 23:1, </journal> <pages> 73-81, </pages> <year> 1974. </year>
Reference-contexts: Interval arithmetic and significance arithmetic were considered by Turing as early as 1946 [72, p.566]. 4.6 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at the IFIP conference in 1974 [63]. The paper <ref> [48] </ref> apparently spawned the `permutation-perturbation' method, performing Gaussian elimination on a matrix both with random initial permutation of the matrix columns, and with random perturbation of some of the matrix entries (setting their least significant bits to 0 or 1, i.e., perturbing by 0 or 2 p ).
Reference: [49] <author> N. Metropolis, S. Ulam, </author> <title> "The Monte Carlo method", </title> <journal> J. Amer. Stat. Assoc. </journal> <volume> 44, 335, </volume> <year> 1949. </year>
Reference-contexts: By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results <ref> [49] </ref>. Interest in Monte Carlo methods in the 1950s spread to many areas of numerical analysis, and this is reflected in the paper by Forsythe and Leibler [20] (elaborating an idea of von Neumann and Ulam), and in Householder's book [38].
Reference: [50] <author> N. </author> <title> Metropolis, "Analyzed Binary Computing", </title> <journal> IEEE Trans. Comput. </journal> <volume> C-22:6, </volume> <pages> 573-576, </pages> <month> June </month> <year> 1973. </year>
Reference-contexts: Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 14 4.5 Automatic monitoring of significance Significance arithmetic was popularized by Ashenhurst and Metropolis in a series of papers arguing the need for numerical systems to track the accuracy (significant digits) of their results <ref> [3, 4, 50] </ref>. A basic problem is defining what is meant by "the number of significant digits" [59, x3.1]. Significance arithmetic is difficult to implement when this measure is a real number, but is less compelling when it is always an integer.
Reference: [51] <author> R.E. Moore, </author> <title> Interval Analysis, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1966. </year>
Reference-contexts: Furthermore, significance loss is relatively rare, so that running a problem twice when significance loss is suspected does not pose a serious problem. What is serious is the possibility of unsuspected significance loss. || [9, p.102] Interval computation <ref> [51, 52] </ref>, which generalizes floating-point numbers from single values to statistical objects, has failed to gain widespread popularity, because it is very pessimistic [72, pp.566-567].
Reference: [52] <author> R.E. Moore, </author> <title> Methods and Applications of Interval Analysis, </title> <address> Philadelphia, PA: </address> <note> SIAM, 1979. Copyright c fl1996, </note> <author> 1997 D. Stott Parker, Paul R. Eggert, </author> <type> Brad Pierce 23 </type>
Reference-contexts: Furthermore, significance loss is relatively rare, so that running a problem twice when significance loss is suspected does not pose a serious problem. What is serious is the possibility of unsuspected significance loss. || [9, p.102] Interval computation <ref> [51, 52] </ref>, which generalizes floating-point numbers from single values to statistical objects, has failed to gain widespread popularity, because it is very pessimistic [72, pp.566-567].
Reference: [53] <author> N. </author> <title> Metropolis, "The Beginning of the Monte Carlo method", </title> <address> Los Alamos Science 15, 125-130, </address> <year> 1987. </year>
Reference-contexts: A review of ENIAC in 1946 brought together Enrico Fermi, Stanislaw Ulam, and John von Neumann. In order to simulate random neutron diffusion in fissile material (extending the atomic bomb development of World War II), Ulam and von Neumann developed the Monte Carlo approach throughout 1947 <ref> [33, 53] </ref>. By 1949 the method worked impressively on the ENIAC and von Neumann, Ulam, Fermi, Metropolis and others had produced elegant theoretical results [49].
Reference: [54] <author> H. Niederreiter, </author> <title> Random Number Generation and Quasi-Monte Carlo Methods, </title> <address> Philadelphia: </address> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: Although practical frustrations, notably meager computing resources, put a damper on this nave enthusiasm, Monte Carlo methods have found heavy use in physical simulation, optimization, and evaluation of multi-dimensional and awkward integrals. Today Monte Carlo analysis is enjoying a resurgence of interest <ref> [19, 54] </ref>. 4.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 [55] and 1951 [28], who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling [37, p.7] were
Reference: [55] <author> J. von Neumann, H.H. Goldstine, </author> <title> "Numerical inverting of matrices of high order", </title> <journal> Bull. Amer. Math. Soc. </journal> <volume> 53, </volume> <pages> 1021-1099, </pages> <year> 1947. </year>
Reference-contexts: Today Monte Carlo analysis is enjoying a resurgence of interest [19, 54]. 4.3 Statistical roundoff analysis The use of statistical methods in numerical analysis arguably began with the pioneering works of von Neumann and Goldstine in 1947 <ref> [55] </ref> and 1951 [28], who decided to show that the 1943 gloomy exponential forward error bounds for Gaussian elimination of Hotelling [37, p.7] were not reflective of practice. <p> We sensed that at least for positive definite matrices the Gaussian procedure could be shown to be quite stable. | Goldstine [30, p.290] In <ref> [55, p.1036] </ref>, von Neumann and Goldstine argued that modeling roundoff errors with independent probabilistic estimates is natural since we know their average and worst-case values, and are ignorant of their exact distribution.
Reference: [56] <author> D.S. Parker, </author> <title> "The Statistical Theory of Relative Errors in Floating-Point Computation", M.S. </title> <type> Thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois, Urbana, IL, </institution> <year> 1976. </year>
Reference-contexts: Other statistical studies of computer arithmetic include <ref> [6, 7, 17, 47, 56] </ref>. In [56], Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions. <p> Other statistical studies of computer arithmetic include [6, 7, 17, 47, 56]. In <ref> [56] </ref>, Parker develops a statistical theory of relative errors in floating-point computation that generalizes floating-point numbers to real-valued distributions (represented to arbitrary precision by their mean and higher moment values), and floating-point operations to operations on distributions.
Reference: [57] <author> D.S. Parker, </author> <title> "Monte Carlo Arithmetic: systematic random improvements upon floating-point arithmetic", </title> <institution> UCLA Computer Science Department, </institution> <type> Technical Report CSD-970002, </type> <month> February </month> <year> 1997. </year>
Reference-contexts: Stott Parker, Paul R. Eggert, Brad Pierce 7 3 Monte Carlo Arithmetic We assume the reader is familiar with basic issues in floating-point arithmetic. Goldberg's tutorial [27] and Higham's encyclopedia [36, chs.1-5] are great references. A more complete presentation of most of this material is in <ref> [57] </ref>. 3.1 Exact and inexact values Exact values are real numbers that can be exactly represented in a given floating-point format. Inexact values, by contrast, are either real values that must be rounded (to an approximation) in order to fit this format, or real values that are not completely known. <p> He takes a strong stand, saying at one point that "probabilistic estimates of error are probably useless or worse" [44, p.7]. Notice that MCA makes no such assumptions or claims; a detailed comparison of MCA with CESTAC and CADNA is made in <ref> [57] </ref>. Another significant problem presented by CADNA is that comparisons, and hence boolean val ues, can become inexact. <p> With the right statistical caveats, we can thus formally prove that Monte Carlo arithmetic avoids certain floating-point anomalies. See <ref> [57] </ref> for more details. Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 17 5.2 Roundoff errors actually do become random Kahan [44] and others (e.g., Higham [36, x1.17,x2.6]) argue that statistical analyses of roundoff error are improperly founded because they assume roundoff errors are random. <p> Stott Parker, Paul R. Eggert, Brad Pierce 19 6 Conclusion We have argued that Monte Carlo Arithmetic (MCA) has interesting practical uses in numerical computations. We sketched how MCA can give useful results without imposing a great deal of overhead. Other examples and case studies are presented in <ref> [57] </ref>. Although it is certainly no panacea, MCA does give perspective and does give reasonable estimates on the accuracy of computed results.
Reference: [58] <author> B.A. Pierce, </author> <title> Applications of randomization to floating-point arithmetic and to linear systems solution, </title> <type> Ph.D. dissertation, </type> <institution> UCLA Computer Science Department, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: (see p.13). 3.4 Random Unrounding A natural randomized arithmetic comes from using input randomization: if ` *fi' is the floating-point approximation to the `*' operation with input randomization, then x *fi y = round ( randomize (x) * randomize (y) ): Input randomization can be viewed naturally as random unrounding <ref> [58] </ref>, which is the random conversion of a (rounded-off, inexact) floating-point value to a real value. In [58], Pierce develops a detailed implementation that maintains `real' values in registers with higher precision than in memory, and uses rounding and random unrounding | storing to and loading from memory | only when <p> is the floating-point approximation to the `*' operation with input randomization, then x *fi y = round ( randomize (x) * randomize (y) ): Input randomization can be viewed naturally as random unrounding <ref> [58] </ref>, which is the random conversion of a (rounded-off, inexact) floating-point value to a real value. In [58], Pierce develops a detailed implementation that maintains `real' values in registers with higher precision than in memory, and uses rounding and random unrounding | storing to and loading from memory | only when necessary.
Reference: [59] <author> P.H. Sterbenz, </author> <title> Floating-Point Computation, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: The inexactness can be due to ignorance, uncertainty, estimation, measurement or computational error, but the upshot is that we have only a few significant digits. Sterbenz <ref> [59] </ref> mentions a variety of proposals for implementing this distinction in computer arithmetic. For example, it was implemented in the NORC calculator in the early 1950s [59, p.196]. <p> Sterbenz [59] mentions a variety of proposals for implementing this distinction in computer arithmetic. For example, it was implemented in the NORC calculator in the early 1950s <ref> [59, p.196] </ref>. Because an inexact value could represent a whole range of possibilities, one can only guess which of these real numbers it `actually' represents. <p> Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 13 Early statistical analyses of rounding methods, leading digit frequencies, etc., are surveyed by Knuth [46] and Sterbenz <ref> [59, x3.1.2] </ref> Probably the best recent work in statistical analysis of error in numerical computations is by Chaitin-Chatelin and her coworkers, who analyze the effects of specific perturbations on the robustness of numerical algorithms (e.g., [12, 13], and very recently [14]). <p> A basic problem is defining what is meant by "the number of significant digits" <ref> [59, x3.1] </ref>. Significance arithmetic is difficult to implement when this measure is a real number, but is less compelling when it is always an integer. Their work was one of the inspirations for the Project Stretch design [9].
Reference: [60] <editor> S.M. Stigler, </editor> <booktitle> The History of Statistics: The Measurement of Uncertainty before 1900, </booktitle> <address> Cam-bridge, MA: Belknap/Harvard U. </address> <publisher> Press, </publisher> <year> 1986. </year>
Reference-contexts: are modeled with random variables, and the virtual precision t can be changed as needed. 4 Related Work 4.1 The statistical theory of error The statistical nature of error has been important to scientists and numerical analysts for centuries, with a particularly strong early emphasis in the field of astronomy <ref> [60] </ref>. <p> Even though the position of the body observed might be considered unknown, the distribution of errors was, for Simpson, known. By basing his analysis upon this known distribution, he was able to come to grips with a stochastic structure for the unknown position. || Stigler <ref> [60, pp.91-94] </ref>. Simpson built directly upon the work of De Moivre, who in 1738 had succeeded in showing that Bernoulli's binomial distribution tended asymptotically to the normal distribution [60, p.82]. <p> Simpson built directly upon the work of De Moivre, who in 1738 had succeeded in showing that Bernoulli's binomial distribution tended asymptotically to the normal distribution <ref> [60, p.82] </ref>. The normal distribution of a variable x, having mean and standard deviation , is Pr [ x t ] = (t) = p Z t e 1 ) dx: Many basic foundations of mathematical statistics were then developed by Laplace. <p> Remarkably, although he published works on the normal distribution, error distributions, and Laplace transforms (developed from De Moivre and Simpson's generating functions), Laplace at first missed the idea of adopting the normal distribution as an error distribution, and e x 2 as an error curve <ref> [60, p.143] </ref>. He spent much of the 1770s working on the idea of an error curve, and focused particularly on the probability density (x) = (m=2) e mjxj [60, p.111]. <p> He spent much of the 1770s working on the idea of an error curve, and focused particularly on the probability density (x) = (m=2) e mjxj <ref> [60, p.111] </ref>. <p> Gauss became the target of scornful allegations by Legendre, who had published his treatise on least squares in 1805 (cf. [29, p.210], <ref> [60, pp.145-146] </ref>). Copyright c fl1996, 1997 D. Stott Parker, Paul R. <p> Copyright c fl1996, 1997 D. Stott Parker, Paul R. Eggert, Brad Pierce 11 of linear equations with least squares from the observed values is also the solution maximizing the probability of the observed values, assuming that the observations are of the same degree of accuracy and are normally distributed <ref> [60, pp.140-141] </ref>. The impact of the method of least squares was immense; it was widely adopted in astronomy and geodesy [60, pp.39-40]. The normal distribution is still commonly referred to as the `Gaussian' distribution, although it was developed much earlier by both DeMoivre and Laplace. <p> The impact of the method of least squares was immense; it was widely adopted in astronomy and geodesy <ref> [60, pp.39-40] </ref>. The normal distribution is still commonly referred to as the `Gaussian' distribution, although it was developed much earlier by both DeMoivre and Laplace. Seizing on Gauss' work, Laplace produced the Central Limit Theorem (see Appendix C.3) in 1810 and his classic Theorie analytique des probabilites in 1812.
Reference: [61] <author> J.R. Taylor, </author> <title> An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements, </title> <address> Mill Valley, CA: </address> <publisher> University Science Books (Oxford University Press), </publisher> <year> 1982. </year>
Reference-contexts: b = n i=1 v u n 1 i=1 The expected error in b is measured by the standard deviation of b , called the standard error: standard error = = p n; which is estimated by b = p Use of standard error is common in scientific work (e.g., <ref> [61, 71] </ref>), and it is traditional to write " = average estimated standard error" (" = b b = p This notation is misleading since it does not give hard bounds on the error, but instead gives `one standard deviation' bounds. <p> This distinction is still one of the first topics in texts on experimental error (cf. <ref> [61] </ref>), and is fundamental to experimental design. Random errors are much easier to analyze, both formally and experimentally. The central idea behind Gauss' approach is the identification of random errors with random variables, which have underlying distributions and independence properties. <p> Thus statistical inference is embedded in the comparison operator, and stochastic arithmetic is similar to the classical theory of propagation of error (or uncertainty) <ref> [61] </ref>. MCA presents this problem as well. In MCA, we recommend defining comparisons in terms of the underlying arithmetic operations, as programmers often assume this [27]: x &lt;fl y (x y) &lt; 0: Actually, every fixed-precision arithmetic system will present this problem.
Reference: [62] <author> A.M. </author> <title> Turing, "Rounding-Off Errors in Matrix Processes", </title> <journal> Quart. J. Mech. </journal> <volume> 1, </volume> <pages> 287-308, </pages> <year> 1948. </year>
Reference-contexts: Turing, in his 1948 analysis of Gaussian elimination that originated the idea of LDU matrix decomposition, had made an analysis for random matrices also <ref> [62, p.299] </ref>. 5 This perspective of Von Neumann addresses only Gaussian elimination, and not, for example, iterative computations like those in Section 2.2. Copyright c fl1996, 1997 D. Stott Parker, Paul R.
Reference: [63] <author> J. Vignes, M. La Porte, </author> <title> "Error Analysis in Computing", </title> <booktitle> Proc. IFIP 1974, </booktitle> <publisher> North-Holland, </publisher> <pages> 610-614, </pages> <year> 1974. </year>
Reference-contexts: Interval arithmetic and significance arithmetic were considered by Turing as early as 1946 [72, p.566]. 4.6 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at the IFIP conference in 1974 <ref> [63] </ref>. <p> In <ref> [63] </ref> `permutation' has evolved to mean changing the order in which additions are performed, and `perturbation' is the addition of a random 0 or 1 value to the least significant bit of a floating-point fraction. These were both implemented in a single FORTRAN function P, later called PEPER [64].
Reference: [64] <author> J. Vignes, </author> <title> "New methods for evaluating the validity of the results of mathematical computations", </title> <booktitle> Mathematics and Computers in Simulation XX, </booktitle> <pages> 227-249, </pages> <year> 1978. </year>
Reference-contexts: In [63] `permutation' has evolved to mean changing the order in which additions are performed, and `perturbation' is the addition of a random 0 or 1 value to the least significant bit of a floating-point fraction. These were both implemented in a single FORTRAN function P, later called PEPER <ref> [64] </ref>. Vignes and Ung patented the idea in Europe in 1979 [65], and in the USA in 1983 [66]. 7 The retrospective survey [67] reviews the results of a decade of research on the permutation-perturbation method.
Reference: [65] <author> J. Vignes, V. </author> <type> Ung, </type> <institution> Procede et ensemble de calcul aleatoirement par defaut ou par exces, pour fournir des resultats de calcul avec le nombre de chiffres significatifs exacts, </institution> <note> European Patent No. 7902784 (1979). </note>
Reference-contexts: These were both implemented in a single FORTRAN function P, later called PEPER [64]. Vignes and Ung patented the idea in Europe in 1979 <ref> [65] </ref>, and in the USA in 1983 [66]. 7 The retrospective survey [67] reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method.
Reference: [66] <author> J. Vignes, V. </author> <title> Ung, Arrangement for determining number of exact significant figures in calculated results, </title> <type> U.S. </type> <note> Patent 4,367,536 (1983). </note>
Reference-contexts: These were both implemented in a single FORTRAN function P, later called PEPER [64]. Vignes and Ung patented the idea in Europe in 1979 [65], and in the USA in 1983 <ref> [66] </ref>. 7 The retrospective survey [67] reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method.
Reference: [67] <author> J. Vignes, R. Alt, </author> <title> "An Efficient Stochastic Method for Round-off Error Analysis", in Accurate Scientific Computations (LNCS #235), </title> <editor> W.L. Miranker and R.A. Toupin (eds.), </editor> <address> NY: </address> <publisher> Springer-Verlag, </publisher> <pages> 183-205, </pages> <year> 1985. </year>
Reference-contexts: These were both implemented in a single FORTRAN function P, later called PEPER [64]. Vignes and Ung patented the idea in Europe in 1979 [65], and in the USA in 1983 [66]. 7 The retrospective survey <ref> [67] </ref> reviews the results of a decade of research on the permutation-perturbation method. In dozens of subsequent papers, Vignes and coworkers refer to the `permutation-perturbation' method as the CESTAC (Controle et Estimation STochastique des Arrondis de Calcul) method.
Reference: [68] <author> J. </author> <type> Vignes, </type> <institution> "Zero mathematique et zero informatique", Comptes Rendus de l'Academie des Sciences, Serie I (Mathematique) 303:20, </institution> <month> 997-1000, 21 Dec. </month> <year> 1986. </year>
Reference-contexts: It requires a different implementation, in which each arithmetic expression is treated like a CESTAC program: it is evaluated 2 or 3 times, then the results averaged (and tested statistically if desired). A value that has no significant digits is treated as an `informational zero' <ref> [68] </ref>, and when tested by a comparison operator these zeroes produce exceptions. Kahan [44] has raised strong objections to the CESTAC approach, taking special issue with its assumption that roundoff errors are normally distributed. Kahan demonstrates examples for which a CESTAC-based software package makes "extravagantly optimistic" claims of accuracy.
Reference: [69] <author> J. Vignes, </author> <title> "Review on stochastic approach to round-off error analysis and its applications," </title> <booktitle> Mathematics and Computers in Simulation 30:6, </booktitle> <pages> 481-491, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Finally the mean value and number of significant digits of these results are computed and printed by a module that performs a Student t computation on their 2 or 3 values. Vignes' survey <ref> [69] </ref> accompanies an entire journal issue with papers describing applications of CESTAC. <p> Vignes' survey [69] accompanies an entire journal issue with papers describing applications of CESTAC. Both PER and the perturbation mode of PEPER work identically <ref> [8, 69] </ref>: when the underlying machine arithmetic works with ordinary rounding, PER adds a least significant bit 1 with probability 1 4 , adds 0 with probability 1 2 , and subtracts 1 with probability 1 4 .
Reference: [70] <author> J. Vignes, </author> <title> "A stochastic arithmetic for reliable scientific computation", </title> <booktitle> Mathematics and Computers in Simulation 35, </booktitle> <pages> 233-261, </pages> <year> 1993. </year>
Reference-contexts: This method can be implemented efficiently, and without extended precision. More recently, CESTAC has been reformulated as stochastic arithmetic and incorporated in the CADNA (Control of Accuracy and Debugging for Numerical Applications) library, which implements stochastic arithmetic for FORTRAN and Ada programs <ref> [70] </ref>. Stochastic arithmetic includes not only the basic arithmetic operators with perturbation, but also operators for comparing stochastic values (one can statistically test the hypotheses x &lt; y or x = y, for stochastic values x and y). This is a significant change from CESTAC.
Reference: [71] <editor> B.L. van der Waerden, </editor> <booktitle> Mathematical Statistics, 2nd edition, </booktitle> <address> NY: </address> <publisher> Springer-Verlag, </publisher> <year> 1969. </year>
Reference-contexts: b = n i=1 v u n 1 i=1 The expected error in b is measured by the standard deviation of b , called the standard error: standard error = = p n; which is estimated by b = p Use of standard error is common in scientific work (e.g., <ref> [61, 71] </ref>), and it is traditional to write " = average estimated standard error" (" = b b = p This notation is misleading since it does not give hard bounds on the error, but instead gives `one standard deviation' bounds. <p> The relationship between the normal distribution and error was established in 1795 when, at the age of eighteen, Gauss conceived the method of least squares. 4 Inspired by the work of Laplace, Gauss' initial theory of errors <ref> [71, Chs.6-7] </ref> took errors to be normally distributed, which was supported by his hypothesis of elementary errors, i.e., that the total error in these observations is a sum of individual, independent errors of small variance.
Reference: [72] <author> J.H. Wilkinson, </author> <title> "Modern Error Analysis", </title> <journal> SIAM Review 13:4, </journal> <pages> 548-568, </pages> <month> October </month> <year> 1971. </year>
Reference-contexts: An excellent summary of these papers is given by Wilkinson <ref> [72] </ref>, and also by Higham [36, x9.6], who reproduces the following commentary by Goldstine about this work: 5 We did not feel it reasonable that so skilled a computer as Gauss would have fallen into the trap that Hotelling thought he had noted ... <p> What is serious is the possibility of unsuspected significance loss. || [9, p.102] Interval computation [51, 52], which generalizes floating-point numbers from single values to statistical objects, has failed to gain widespread popularity, because it is very pessimistic <ref> [72, pp.566-567] </ref>. Interval arithmetic and significance arithmetic were considered by Turing as early as 1946 [72, p.566]. 4.6 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at the IFIP conference in 1974 [63]. <p> Interval arithmetic and significance arithmetic were considered by Turing as early as 1946 <ref> [72, p.566] </ref>. 4.6 Stochastic computer arithmetic Randomized numerical methods have been studied since the early 1970s by Vignes, who presented the idea at the IFIP conference in 1974 [63].
References-found: 72

