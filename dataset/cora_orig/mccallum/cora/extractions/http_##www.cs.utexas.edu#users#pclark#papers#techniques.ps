URL: http://www.cs.utexas.edu/users/pclark/papers/techniques.ps
Refering-URL: http://www.cs.utexas.edu/users/pclark/papers/techniques.abs.html
Root-URL: 
Title: In: Artificial Intelligence: Concepts and Applications in Engineering,  Machine Learning: Techniques and Recent Developments  
Author: pp-, Ed: A. R. Peter Clark, Turing Institute 
Date: 1990  
Web: http://www.cs.utexas.edu/users/pclark/papers/techniques.ps  
Address: London, UK: Chapman and Hall, (1990)  
Affiliation: Mirzai.  
Abstract: The use of expert systems is becoming more and more widespread, making the need for appropriate machine learning techniques more acute to help ease the knowledge aquisition bottleneck. Additionally, the increasing number of large databases offers a vast potential for the automatic generation of new knowledge by machines and its communication to people in a comprehensible form. In response to these events, this paper provides an overview of current machine learning work with a particular emphasis on rule induction techniques. Firstly we provide a summary of existing rule induction techniques, including descriptions of the ID3 and AQ algorithms. Secondly, we review recent developments in rule induction technology which overcome some of the practical limitations of these basic algorithms including noise handling, probabilistic classification, large data sets and incremental learning. Finally, we describe the state of current research in machine learning and the directions in which it is heading, addressing the difficult problems of constructive induction and representation change.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Proc. </editor> <booktitle> First Case-Based Reasoning Workshop Ca, </booktitle> <publisher> Kaufmann. </publisher>
Reference-contexts: Additionally, this technique models the behaviour of much human problem solving and thus contributes towards its psychological validity and comprehensibility by an expert. Examples of recent work in case-based reasoning are Chef [12], by Clark [8, 9], Protos [2] and Nexus [3] (also see <ref> [1] </ref>). A comparison of this approach with rule induction approaches is given in [7].
Reference: [2] <author> Bareiss, E. R., Porter, B. W., and Wier, C. C. </author> <year> (1987). </year> <title> PROTOS: an exemplar-based learning apprentice. </title> <booktitle> In Proc. 4th International Workshop on Machine Learning Ca, </booktitle> <editor> P. Langley, Ed., </editor> <publisher> Kaufmann. </publisher>
Reference-contexts: Additionally, this technique models the behaviour of much human problem solving and thus contributes towards its psychological validity and comprehensibility by an expert. Examples of recent work in case-based reasoning are Chef [12], by Clark [8, 9], Protos <ref> [2] </ref> and Nexus [3] (also see [1]). A comparison of this approach with rule induction approaches is given in [7].
Reference: [3] <author> Bradshaw, G. </author> <year> (1987). </year> <title> Learning about speech sounds: the nexus project. </title> <booktitle> In Proc. 4th International Workshop on Machine Learning Ca, </booktitle> <editor> P. Langley, Ed., </editor> <publisher> Kaufmann, </publisher> <pages> pp. 1-11. </pages>
Reference-contexts: Additionally, this technique models the behaviour of much human problem solving and thus contributes towards its psychological validity and comprehensibility by an expert. Examples of recent work in case-based reasoning are Chef [12], by Clark [8, 9], Protos [2] and Nexus <ref> [3] </ref> (also see [1]). A comparison of this approach with rule induction approaches is given in [7].
Reference: [4] <author> Bundy, A. </author> <year> (1985). </year> <title> Incidence calculus: a mechanism for probabilistic reasoning. </title> <journal> Journal of Automated Reasoning 1, </journal> <volume> 3, </volume> <pages> 263-283. </pages>
Reference-contexts: The aq algorithm can be efficiently implemented using the Incidence Calculus methods of Bundy <ref> [4] </ref>, whereby a bit-string for each attribute test is constructed, each bit representing a different example (`1' if the test is passed and `0' if it is not).
Reference: [5] <author> Cestnik, B., Kononenko, I., and Bratko, I. </author> <year> (1987). </year> <title> Assistant 86: a knowledge-elicitation tool for sophisticated users. </title> <booktitle> In Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <editor> I. Bratko and N. Lavrac, Eds., Sigma, Wilmslow, </editor> <booktitle> UK, </booktitle> <pages> pp. 31-45. </pages>
Reference-contexts: Again in rule induction systems this technique is common, for example in the most recent id3 descendants c4 [40], assistant-86 <ref> [5] </ref> and by Niblett and Bratko [33]. Niblett [32] gives a review of pre- and post-pruning techniques used for decision trees. aq15 [17] employs a post-pruning technique for production rules (termed `rule truncation').
Reference: [6] <author> Clancey, W. J. </author> <year> (1984). </year> <title> Classification problem solving. </title> <booktitle> In AAAI-84 , pp. </booktitle> <pages> 49-55. </pages>
Reference-contexts: not lion furry young large lion not furry young small not lion not furry old large not lion Decision Tree Output by id3 Decision Rules Output by aq if furry=yes and size=large then class=lion. if size=medium then class=lion. if furry=no then class=not lion. if size=small then class=not lion. 4 classification" <ref> [6] </ref>. The systems do not make use of any other domain-specific information beyond that of the training examples themselves. The rules which id3 and aq produce constitute a simple `model' of the world, automatically generated from the observations with which they have been presented.
Reference: [7] <author> Clark, P. </author> <year> (1988). </year> <title> A comparison of rule and exemplar-based learning systems. </title> <booktitle> In International workshop on Machine Learning, Meta-reasoning and Logics Portugal, Portugal, </booktitle> <editor> Ed., Faculdade de Economia, </editor> <publisher> Univ. Porto, </publisher> <pages> pp. 69-81. </pages> <note> (Proceedings to be published in book form in 1989). </note>
Reference-contexts: Examples of recent work in case-based reasoning are Chef [12], by Clark [8, 9], Protos [2] and Nexus [3] (also see [1]). A comparison of this approach with rule induction approaches is given in <ref> [7] </ref>.
Reference: [8] <author> Clark, P. </author> <year> (1988). </year> <note> Exemplar-based Reasoning in Geological Prospect Appraisal. TIRM 034, </note> <institution> Turing Institute, Glasgow, UK. </institution>
Reference-contexts: Additionally, this technique models the behaviour of much human problem solving and thus contributes towards its psychological validity and comprehensibility by an expert. Examples of recent work in case-based reasoning are Chef [12], by Clark <ref> [8, 9] </ref>, Protos [2] and Nexus [3] (also see [1]). A comparison of this approach with rule induction approaches is given in [7].
Reference: [9] <author> Clark, P. </author> <year> (1988). </year> <title> Representing arguments as background knowledge for constraining generalisa-tion. </title> <booktitle> In Proc. Third European Working Session on Learning (EWSL-88) London, </booktitle> <editor> D. Sleeman, Ed., </editor> <publisher> Pitman, </publisher> <pages> pp. 37-44. 18 </pages>
Reference-contexts: Additionally, this technique models the behaviour of much human problem solving and thus contributes towards its psychological validity and comprehensibility by an expert. Examples of recent work in case-based reasoning are Chef [12], by Clark <ref> [8, 9] </ref>, Protos [2] and Nexus [3] (also see [1]). A comparison of this approach with rule induction approaches is given in [7].
Reference: [10] <author> Clark, P., and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <note> Machine Learning Journal 3 , 261-283. </note>
Reference-contexts: These algorithms have been used as the basis of several machine learning systems, for example id3 in assistant [15], acls [34] and c4 [40], and the aq algorithm in aq11 [20], aqr <ref> [10] </ref> and aq15 [17]. An example of the form of the inputs and outputs which these algorithms receive and produce is shown in Figure 1. <p> Existing implementations (e.g. aq11 [20] and aq15 [17]) have adopted this approach. A second approach is to modify the algorithm itself, removing its dependence on specific examples and increasing the space of rules searched. This approach was taken in the CN2 system <ref> [10] </ref>, where instead of conducting a beam search of rules covering a seed but excluding negative examples, the beam search explores (in a general-to-specific manner) the space of all rules, and uses a statistical significance test to prevent statistically insignificant specialisations being made.
Reference: [11] <author> Corlett, R. A. </author> <year> (1983). </year> <title> Explaining induced decision trees. </title> <booktitle> In Expert Systems 83 , pp. </booktitle> <pages> 136-142. </pages>
Reference-contexts: Niblett [32] gives a review of pre- and post-pruning techniques used for decision trees. aq15 [17] employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett <ref> [11] </ref> and Quinlan [37]. Corroborative Rule Application Another technique to prevent unreliable rules degrading performance is to allow all rules to contribute in some way during classification, with different weights attached to their decisions. (This requires rules which `nearly' fire to also contribute towards the final class prediction).
Reference: [12] <author> Hammond, K. </author> <year> (1986). </year> <title> CHEF: a model of case-based planning. </title> <booktitle> In AAAI-86 </booktitle> . 
Reference-contexts: Additionally, this technique models the behaviour of much human problem solving and thus contributes towards its psychological validity and comprehensibility by an expert. Examples of recent work in case-based reasoning are Chef <ref> [12] </ref>, by Clark [8, 9], Protos [2] and Nexus [3] (also see [1]). A comparison of this approach with rule induction approaches is given in [7].
Reference: [13] <author> Hunt, E. B., Marin, J., and Stone, P. T. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> NY. </address>
Reference-contexts: Later in this paper we discuss the possibilities and problems of extending these learning systems to acquire and refine more complex, structured representations of the world. 2.2 The ID3 Algorithm 2.2.1 Knowledge Representation The id3 algorithm [38] is a descendent of Hunt et als' Concept Learning System <ref> [13] </ref>. The `rules' which id3 learns are represented as decision trees. A decision tree is like a flow chart, in which a node of the tree represents a test on an attribute and each outgoing branch corresponds to a possible result of this test.
Reference: [14] <author> Kedar-Cabelli, S. T., and McCarty, L. T. </author> <year> (1987). </year> <title> Explanation-based generalization as resolution theorem proving. </title> <booktitle> In Proc. 4th International Workshop on Machine Learning Ca, </booktitle> <editor> P. Langley, Ed., </editor> <publisher> Kaufmann, </publisher> <pages> pp. 383-389. </pages>
Reference-contexts: The area of explanation-based learning (EBL) covers a variety of techniques, most working with some first order logic variant such as Horn clauses or production rules as a representational language. The most common technique, explanation-based generalisation (EBG), involves generating and storing a general solution to a problem <ref> [27, 14] </ref>. Solving a problem often involves instantiating and applying a number of operators or `rules' - explanation-based `generalisation' (`re-generalisation' is perhaps more appropriate) collects and simplifies the uninstantiated operator sequence, storing it for later use.
Reference: [15] <author> Kononenko, I., Bratko, I., and Roskar, E. </author> <year> (1984). </year> <title> Experiments in Automatic Learning of Medical Diagnostic Rules. </title> <type> Technical Report, </type> <institution> Faculty of Electircal Engineering, E. Kardelj University, Ljubljana. </institution>
Reference-contexts: In this section we first describe two algorithms for learning such rules, namely the id3 and aq algorithms. These algorithms have been used as the basis of several machine learning systems, for example id3 in assistant <ref> [15] </ref>, acls [34] and c4 [40], and the aq algorithm in aq11 [20], aqr [10] and aq15 [17]. An example of the form of the inputs and outputs which these algorithms receive and produce is shown in Figure 1. <p> The id3 algorithm lends itself to easy modification due to the nature of its general-to-specific search. Tree pruning techniques (e.g. [39, 32]), as used for example in the systems c4 [40] and assistant <ref> [15] </ref>, have proved to be effective methods of avoiding overfitting. These use statistical methods to assess confidence in the correlations the decision tree branches represent. The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search.
Reference: [16] <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <booktitle> In Machine Learning, </booktitle> <volume> vol. 1, </volume> <editor> J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, Eds., </editor> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <pages> pp. 83-134. </pages>
Reference-contexts: This is useful for expert system applications based on the production rule paradigm, and is often a more comprehensible representation than a decision tree especially when the decision tree produced by id3 is large. We deviate from the terminology introduced by Michalski <ref> [16] </ref> in order to maintain continuity with the description of id3 above. Each decision rule which aq induces is of the form `if &lt;condition&gt; then predict &lt;class&gt;', where &lt;condition&gt; is a conjunct of attribute tests. There may be more than one rule for each class 1 .
Reference: [17] <author> Michalski, R., Mozetic, I., Hong, J., and Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system aq15 and its testing application to three medical domains. </title> <booktitle> In AAAI-86 Ca, </booktitle> <publisher> Kaufmann, </publisher> <pages> pp. 1041-1045. </pages>
Reference-contexts: These algorithms have been used as the basis of several machine learning systems, for example id3 in assistant [15], acls [34] and c4 [40], and the aq algorithm in aq11 [20], aqr [10] and aq15 <ref> [17] </ref>. An example of the form of the inputs and outputs which these algorithms receive and produce is shown in Figure 1. Each training example is described by giving values for a fixed number of (user-selected) attributes, plus the corresponding class of which the example is a member. <p> There are two possibilities of remedy here. Firstly, the basic algorithm can be left intact and noisy data can be dealt with by using preprocessing (e.g. data filtering, mentioned above) or post-processing techniques (e.g. post-pruning, see below). Existing implementations (e.g. aq11 [20] and aq15 <ref> [17] </ref>) have adopted this approach. A second approach is to modify the algorithm itself, removing its dependence on specific examples and increasing the space of rules searched. <p> Again in rule induction systems this technique is common, for example in the most recent id3 descendants c4 [40], assistant-86 [5] and by Niblett and Bratko [33]. Niblett [32] gives a review of pre- and post-pruning techniques used for decision trees. aq15 <ref> [17] </ref> employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett [11] and Quinlan [37]. <p> This avoids heavy reliance on a specific, possibly unreliable, part of the system, allowing noisy effects to be over-ridden and smoothed out by other components, but also degrades the comprehensibility of the rule set. Statistical methods such as Bayesian techniques (e.g. [22]) can be employed. aq15 <ref> [17] </ref> performs weighted rule application in this way, and Quinlan [35] suggests how decision tree application can be made less brittle by introducing a degree of corroboration between decision tree branches. 2 or as nearly consistent as possible if complete consistency is impossible. 12 13 2.4.3 Probabilistic Classification Another limitation with <p> In fact probabilities and measures of certainty are easily attached to classifications, as probabilities can easily be calculated by examining the performance of the rules on the training data. This technique has been demonstrated in recent systems based on id3 (e.g. [35]) and aq (e.g. <ref> [17] </ref>). 2.4.4 Large Data Sets As computers increase their speed and memory capacity, larger and larger data sets can be handled. However, with data sets containing millions of examples there may still be too much data for induction to be feasible within time and memory constraints of application machines.
Reference: [18] <author> Michalski, R. S. </author> <year> (1985). </year> <title> Knowledge repair mechanisms: evolution vs. revolution. </title> <type> ISG 85-15, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, Urbana. </institution>
Reference-contexts: Given a new training example, existing rules which are inconsistent with it are deleted and the aq algorithm re-run to cover all those examples now without rules covering them. This technique is described by Michalski <ref> [18] </ref> and used, for example, by Mozetic [29]. 14 Table 1: Examples of Binary Numbers and their Associated Parity Attributes Parity No.
Reference: [19] <author> Michalski, R. S., and Chilausky, R. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean diagnosis. </title> <journal> Policy Analysis and Information Systems 4, </journal> <volume> 2, </volume> <pages> 125-160. </pages>
Reference-contexts: Data Filtering One technique is to perform induction using only representative training examples, as selected by the expert or automatically. This technique was used in aq11's application to the task of soya bean diagnosis <ref> [19] </ref>, where the esel system [21] was first used to select representative training examples.
Reference: [20] <author> Michalski, R. S., and Larson, J. </author> <year> (1983). </year> <title> Incremental Generation of VL 1 Hypotheses: the underlying Methodology and the Description of Program AQ11. </title> <type> ISG 83-5, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana. </institution>
Reference-contexts: In this section we first describe two algorithms for learning such rules, namely the id3 and aq algorithms. These algorithms have been used as the basis of several machine learning systems, for example id3 in assistant [15], acls [34] and c4 [40], and the aq algorithm in aq11 <ref> [20] </ref>, aqr [10] and aq15 [17]. An example of the form of the inputs and outputs which these algorithms receive and produce is shown in Figure 1. <p> There are two possibilities of remedy here. Firstly, the basic algorithm can be left intact and noisy data can be dealt with by using preprocessing (e.g. data filtering, mentioned above) or post-processing techniques (e.g. post-pruning, see below). Existing implementations (e.g. aq11 <ref> [20] </ref> and aq15 [17]) have adopted this approach. A second approach is to modify the algorithm itself, removing its dependence on specific examples and increasing the space of rules searched.
Reference: [21] <author> Michalski, R. S., and Larson, J. </author> <year> (1978). </year> <title> Selection of most Representative Training Examples and Incremental Generation of VL 1 Hypotheses: the underlying Methodology and the Description of Programs ESEL and AQ11. </title> <type> UIUCDCS-R 78-867, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Urbana. </institution>
Reference-contexts: Data Filtering One technique is to perform induction using only representative training examples, as selected by the expert or automatically. This technique was used in aq11's application to the task of soya bean diagnosis [19], where the esel system <ref> [21] </ref> was first used to select representative training examples.
Reference: [22] <author> Michie, D. </author> <year> (1982). </year> <title> Bayes, turing and the logic of corroboration. In Machine intelligence and related topics: an information scientist's weekend book, </title> <publisher> Gordon & Breach, </publisher> <address> NY. </address>
Reference-contexts: This avoids heavy reliance on a specific, possibly unreliable, part of the system, allowing noisy effects to be over-ridden and smoothed out by other components, but also degrades the comprehensibility of the rule set. Statistical methods such as Bayesian techniques (e.g. <ref> [22] </ref>) can be employed. aq15 [17] performs weighted rule application in this way, and Quinlan [35] suggests how decision tree application can be made less brittle by introducing a degree of corroboration between decision tree branches. 2 or as nearly consistent as possible if complete consistency is impossible. 12 13 2.4.3
Reference: [23] <author> Michie, D. </author> <year> (1988). </year> <title> Machine learning in the next five years. </title> <booktitle> In Proc. Third European Working Session on Learning (EWSL-88) London, </booktitle> <editor> D. Sleeman, Ed., </editor> <publisher> Pitman, </publisher> <pages> pp. 107-122. </pages>
Reference-contexts: Recently, alternative paradigms of machine learning besides the traditional AI-type symbolic learning have received substantial attention. In particular, increasing power of computers is enabling connectionist and genetic techniques to become feasible areas of study, although their role as artificial intelligence research remains controversial. Michie <ref> [23] </ref> presents three criteria for machine learning. The weak criterion states that machine learning occurs when a "system uses sample data to generate an updated basis for improved performance on subsequent data".
Reference: [24] <author> Michie, D. </author> <year> (1986). </year> <title> The superarticulacy phenomenon in the context of software manufacture. </title> <journal> In Proceedings of the Royal Society (Series A) , pp. </journal> <pages> 185-212. </pages> <note> (Also available as internal report TIRM-85-13, </note> <institution> Turing Institute, Glasgow, UK). </institution>
Reference-contexts: In this way, machines may enhance the global body of scientific and other knowledge. Michie has referred to this phenomenon as `superarticulacy', and it has already been demonstrated in limited domains of application <ref> [24] </ref>. One particular form of learning, that of inducing classification rules from a set of training examples, has received substantial attention within the machine learning field and represents the learning method most frequently and successfully used in expert system applications (e.g. [28]).
Reference: [25] <author> Minton, S. </author> <year> (1988). </year> <title> Quantitative results concerning the utility of explanation-based learning. In AAAI-88 Ca, </title> <publisher> Kaufman, </publisher> <pages> pp. 564-569. </pages>
Reference-contexts: Early work assumed generalising solutions to old problems would yield such an overall benefit (thus assuming new problems will be similar to old problems). However, this is not necessarily the case Minton <ref> [26, 25] </ref> has recently conducted more detailed analyses of the criteria for deciding which generalisations to store in order to achieve an increase in the system's performance. 4 Conclusion Machine learning is becoming an increasingly important area of study within the field of artificial intelligence.
Reference: [26] <author> Minton, S. </author> <year> (1985). </year> <title> Selectively generalizing plans for problem-solving. </title> <booktitle> In IJCAI-85 , pp. </booktitle> <pages> 596-599. </pages>
Reference-contexts: Early work assumed generalising solutions to old problems would yield such an overall benefit (thus assuming new problems will be similar to old problems). However, this is not necessarily the case Minton <ref> [26, 25] </ref> has recently conducted more detailed analyses of the criteria for deciding which generalisations to store in order to achieve an increase in the system's performance. 4 Conclusion Machine learning is becoming an increasingly important area of study within the field of artificial intelligence.
Reference: [27] <author> Mitchell, T. M., Keller, R. M., and Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: a unifying view. </title> <journal> Machine Learning Journal 1, </journal> <volume> 1, </volume> <pages> 47-80. </pages>
Reference-contexts: The area of explanation-based learning (EBL) covers a variety of techniques, most working with some first order logic variant such as Horn clauses or production rules as a representational language. The most common technique, explanation-based generalisation (EBG), involves generating and storing a general solution to a problem <ref> [27, 14] </ref>. Solving a problem often involves instantiating and applying a number of operators or `rules' - explanation-based `generalisation' (`re-generalisation' is perhaps more appropriate) collects and simplifies the uninstantiated operator sequence, storing it for later use.
Reference: [28] <author> Mowforth, P. </author> <year> (1986). </year> <title> Some Applications with Inductive Expert System Shells. </title> <type> TIOP 86-002, </type> <institution> Turing Institute, Glasgow, UK. </institution>
Reference-contexts: One particular form of learning, that of inducing classification rules from a set of training examples, has received substantial attention within the machine learning field and represents the learning method most frequently and successfully used in expert system applications (e.g. <ref> [28] </ref>). <p> thus we focus on systems learning structured, symbolic representations of the world. 2 The Inductive Rule Learning Methodology 2.1 Introduction There are a wide variety of techniques used for machine learning however the technique which has perhaps received the most attention and has been most commercially successful to date (e.g. <ref> [28] </ref>) has been the paradigm of learning classification rules from a set of training examples. In this paradigm the learning system searches a space of rules to find those which `best' classify the training examples, where `best' is defined in terms of accuracy and comprehensibility.
Reference: [29] <author> Mozetic, I. </author> <year> (1987). </year> <title> The role of abstractions in learning qualitative models. </title> <booktitle> In Proc. 4th International Workshop on Machine Learning Ca, </booktitle> <editor> P. Langley, Ed., </editor> <publisher> Kaufmann. </publisher>
Reference-contexts: Given a new training example, existing rules which are inconsistent with it are deleted and the aq algorithm re-run to cover all those examples now without rules covering them. This technique is described by Michalski [18] and used, for example, by Mozetic <ref> [29] </ref>. 14 Table 1: Examples of Binary Numbers and their Associated Parity Attributes Parity No. <p> Controlling this search is a major problem. Some of the most common methods which are used for controlling the search are as follows: 1. Use of representations at multiple levels of abstraction (e.g. <ref> [29, 46] </ref>). Here a representation at a coarse level of detail is first acquired, then a more detailed representation is formed. The abstract (coarse) level serves to constrain search at the more detailed level. 2. Use of an oracle (e.g. [31, 30]).
Reference: [30] <author> Muggleton, S. </author> <year> (1987). </year> <title> Duce: an oracle-based approach to constructive induction. In IJCAI-87 Ca, </title> <editor> J. McDermott, Ed., </editor> <publisher> Kaufmann, </publisher> <pages> pp. 287-292. </pages>
Reference-contexts: Use of representations at multiple levels of abstraction (e.g. [29, 46]). Here a representation at a coarse level of detail is first acquired, then a more detailed representation is formed. The abstract (coarse) level serves to constrain search at the more detailed level. 2. Use of an oracle (e.g. <ref> [31, 30] </ref>). Here, the user is asked to verify the system's operation at each step. 3. Assuming no noise (e.g. [43]).
Reference: [31] <author> Muggleton, S., and Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proc. 5th Int. Conf. on Machine Learning Ca, </booktitle> <editor> J. Laird, Ed., </editor> <publisher> Kaufmann, </publisher> <pages> pp. 339-352. </pages>
Reference-contexts: Use of representations at multiple levels of abstraction (e.g. [29, 46]). Here a representation at a coarse level of detail is first acquired, then a more detailed representation is formed. The abstract (coarse) level serves to constrain search at the more detailed level. 2. Use of an oracle (e.g. <ref> [31, 30] </ref>). Here, the user is asked to verify the system's operation at each step. 3. Assuming no noise (e.g. [43]). <p> An analysis of the importance of constructive induction in 16 the learning of certain classes of concepts is made by Rendell [42]. The importance of structuring problems in this way has also been investigated by Shapiro and Niblett [44]. Research in this area is fairly new; the system cigol <ref> [31] </ref> is one example of recent work, using the principle of inverse resolution to introduce new terms in logic programs. 3.3 Using Additional Domain Knowledge Another limitation of inductive approaches as used in the id3 and aq algorithms is that no additional domain-specific knowledge is employed to control search beyond that
Reference: [32] <author> Niblett, T. </author> <year> (1987). </year> <title> Constructing decision trees in noisy domains. </title> <booktitle> In Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <editor> I. Bratko and N. Lavrac, Eds., Sigma, Wilmslow, </editor> <booktitle> UK, </booktitle> <pages> pp. 67-78. 19 </pages>
Reference-contexts: The id3 algorithm lends itself to easy modification due to the nature of its general-to-specific search. Tree pruning techniques (e.g. <ref> [39, 32] </ref>), as used for example in the systems c4 [40] and assistant [15], have proved to be effective methods of avoiding overfitting. These use statistical methods to assess confidence in the correlations the decision tree branches represent. <p> Again in rule induction systems this technique is common, for example in the most recent id3 descendants c4 [40], assistant-86 [5] and by Niblett and Bratko [33]. Niblett <ref> [32] </ref> gives a review of pre- and post-pruning techniques used for decision trees. aq15 [17] employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett [11] and Quinlan [37].
Reference: [33] <author> Niblett, T., and Bratko, I. </author> <year> (1986). </year> <title> Learning decision rules in noisy domains. </title> <booktitle> In Expert Systems 86, </booktitle> <address> Brighton, UK </address> . 
Reference-contexts: Again in rule induction systems this technique is common, for example in the most recent id3 descendants c4 [40], assistant-86 [5] and by Niblett and Bratko <ref> [33] </ref>. Niblett [32] gives a review of pre- and post-pruning techniques used for decision trees. aq15 [17] employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett [11] and Quinlan [37].
Reference: [34] <author> Paterson, A., and Niblett, T. </author> <title> ACLS Manual. </title> <type> Version 1. </type> <institution> Glasgow, </institution> <year> 1982. </year>
Reference-contexts: In this section we first describe two algorithms for learning such rules, namely the id3 and aq algorithms. These algorithms have been used as the basis of several machine learning systems, for example id3 in assistant [15], acls <ref> [34] </ref> and c4 [40], and the aq algorithm in aq11 [20], aqr [10] and aq15 [17]. An example of the form of the inputs and outputs which these algorithms receive and produce is shown in Figure 1.
Reference: [35] <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Decision trees as probabilistic classifiers. </title> <booktitle> In Proc. 4th International Workshop on Machine Learning Ca, </booktitle> <editor> P. Langley, Ed., </editor> <publisher> Kaufmann. </publisher>
Reference-contexts: Statistical methods such as Bayesian techniques (e.g. [22]) can be employed. aq15 [17] performs weighted rule application in this way, and Quinlan <ref> [35] </ref> suggests how decision tree application can be made less brittle by introducing a degree of corroboration between decision tree branches. 2 or as nearly consistent as possible if complete consistency is impossible. 12 13 2.4.3 Probabilistic Classification Another limitation with simple rule induction methods is that no degree of certainty <p> In fact probabilities and measures of certainty are easily attached to classifications, as probabilities can easily be calculated by examining the performance of the rules on the training data. This technique has been demonstrated in recent systems based on id3 (e.g. <ref> [35] </ref>) and aq (e.g. [17]). 2.4.4 Large Data Sets As computers increase their speed and memory capacity, larger and larger data sets can be handled.
Reference: [36] <author> Quinlan, J. R. </author> <year> (1979). </year> <title> Discovering rules by induction from large collections of examples. In Expert Systems in the Micro-Electronic Age, </title> <editor> D. Michie, Ed., </editor> <publisher> Edinburgh Univ. Press, UK, </publisher> <pages> pp. 168-201. </pages>
Reference-contexts: This technique is described in more detail in <ref> [36] </ref>. 2.4.5 Incremental Learning Another constraint sometimes cited against rule induction systems is their non-incremental nature. The algorithms as described above do not permit modification of existing rule sets/decision trees, but require them to be grown again from scratch.
Reference: [37] <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Generating production rules from decision trees. In IJCAI-87 Ca, </title> <editor> J. McDer-mott, Ed., </editor> <publisher> Kaufmann, </publisher> <pages> pp. 304-307. </pages>
Reference-contexts: Niblett [32] gives a review of pre- and post-pruning techniques used for decision trees. aq15 [17] employs a post-pruning technique for production rules (termed `rule truncation'). The use of post-pruning of decision tree branches to generate production rules has been used by Corlett [11] and Quinlan <ref> [37] </ref>. Corroborative Rule Application Another technique to prevent unreliable rules degrading performance is to allow all rules to contribute in some way during classification, with different weights attached to their decisions. (This requires rules which `nearly' fire to also contribute towards the final class prediction).
Reference: [38] <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess endgames. </title> <booktitle> In Machine Learning, </booktitle> <volume> vol. 1, </volume> <editor> J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, Eds., </editor> <publisher> Tioga, </publisher> <address> Palo Alto, Ca. </address>
Reference-contexts: Later in this paper we discuss the possibilities and problems of extending these learning systems to acquire and refine more complex, structured representations of the world. 2.2 The ID3 Algorithm 2.2.1 Knowledge Representation The id3 algorithm <ref> [38] </ref> is a descendent of Hunt et als' Concept Learning System [13]. The `rules' which id3 learns are represented as decision trees.
Reference: [39] <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> Int. Journal of Man-Machine Studies 27, </journal> <volume> 3, </volume> <pages> 221-234. </pages>
Reference-contexts: The id3 algorithm lends itself to easy modification due to the nature of its general-to-specific search. Tree pruning techniques (e.g. <ref> [39, 32] </ref>), as used for example in the systems c4 [40] and assistant [15], have proved to be effective methods of avoiding overfitting. These use statistical methods to assess confidence in the correlations the decision tree branches represent.
Reference: [40] <author> Quinlan, J. R., Compton, P. J., Horn, K. A., and Lazarus, L. </author> <year> (1987). </year> <title> Inductive knowledge acquisition: a case study. In Applications of Expert Systems, </title> <publisher> Addison-Wesley, Wokingham, UK, </publisher> <pages> pp. 157-173. </pages>
Reference-contexts: In this section we first describe two algorithms for learning such rules, namely the id3 and aq algorithms. These algorithms have been used as the basis of several machine learning systems, for example id3 in assistant [15], acls [34] and c4 <ref> [40] </ref>, and the aq algorithm in aq11 [20], aqr [10] and aq15 [17]. An example of the form of the inputs and outputs which these algorithms receive and produce is shown in Figure 1. <p> The id3 algorithm lends itself to easy modification due to the nature of its general-to-specific search. Tree pruning techniques (e.g. [39, 32]), as used for example in the systems c4 <ref> [40] </ref> and assistant [15], have proved to be effective methods of avoiding overfitting. These use statistical methods to assess confidence in the correlations the decision tree branches represent. The aq algorithm, however, is less easy to modify due to its dependence on specific training examples during its search. <p> The advantage of this is that the quality of pruned and unpruned versions of a rule set can be directly compared rather than having to estimate the latter's quality during search. Again in rule induction systems this technique is common, for example in the most recent id3 descendants c4 <ref> [40] </ref>, assistant-86 [5] and by Niblett and Bratko [33]. Niblett [32] gives a review of pre- and post-pruning techniques used for decision trees. aq15 [17] employs a post-pruning technique for production rules (termed `rule truncation').
Reference: [41] <author> Reddy, R. </author> <year> (1988). </year> <title> Foundations and grand challenges of artificial intelligence. </title> <journal> AI Magazine 9, </journal> <volume> 4, </volume> <pages> 9-21. </pages>
Reference-contexts: A survey in 1988 reported the number of deployed systems had risen sharply from around 50 in the previous year to 1400, and the number under development increased from 2500 to 8500 <ref> [41] </ref>. Expert systems are characterised by the use of a particular programming methodology in which domain-specific knowledge is clearly separated from the more general inference machinery within the system.
Reference: [42] <author> Rendell, L. </author> <year> (1988). </year> <title> Learning hard concepts. </title> <booktitle> In Proc. Third European Working Session on Learning (EWSL-88) London, </booktitle> <editor> D. Sleeman, Ed., </editor> <publisher> Pitman, </publisher> <pages> pp. 177-200. </pages>
Reference-contexts: This automatic introduction of terms is sometimes referred to as `constructive induction'. An analysis of the importance of constructive induction in 16 the learning of certain classes of concepts is made by Rendell <ref> [42] </ref>. The importance of structuring problems in this way has also been investigated by Shapiro and Niblett [44].
Reference: [43] <author> Sammut, C., and Banerji, R. </author> <year> (1986). </year> <title> Learning concepts by asking questions. </title> <booktitle> In Machine Learning, </booktitle> <volume> vol. 2, </volume> <editor> J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, Eds., </editor> <publisher> Tioga, </publisher> <address> Palo Alto, Ca. </address>
Reference-contexts: The abstract (coarse) level serves to constrain search at the more detailed level. 2. Use of an oracle (e.g. [31, 30]). Here, the user is asked to verify the system's operation at each step. 3. Assuming no noise (e.g. <ref> [43] </ref>). By making this assumption, any learned knowledge for which counterexamples exist can immediately be rejected, greatly reducing the search space. 3.2 Constructive Induction We have argued that more structured representations are to be aimed for in order to allow the system to acquire more complex knowledge.
Reference: [44] <author> Shapiro, A. D. </author> <year> (1987). </year> <title> Structured Induction in Expert Systems. </title> <publisher> Turing Inst. Press, in association with Addison-Wesley, </publisher> <address> Wokingham, UK. </address>
Reference-contexts: An analysis of the importance of constructive induction in 16 the learning of certain classes of concepts is made by Rendell [42]. The importance of structuring problems in this way has also been investigated by Shapiro and Niblett <ref> [44] </ref>.
Reference: [45] <author> Utgoff, P. E. </author> <year> (1988). </year> <title> Id5: an incremental id3. </title> <booktitle> In Proc. 5th Int. Conf. on Machine Learning Ca, </booktitle> <editor> J. Laird, Ed., </editor> <publisher> Kaufmann, </publisher> <pages> pp. 107-120. </pages>
Reference-contexts: This technique is described and evaluated by Utgoff in the id5 system <ref> [45] </ref>. aq can similarly be modified to act incrementally. Given a new training example, existing rules which are inconsistent with it are deleted and the aq algorithm re-run to cover all those examples now without rules covering them.
Reference: [46] <author> Van de Velde, W. </author> <year> (1988). </year> <title> Learning through progressive refinement. </title> <booktitle> In Proc. Third European Working Session on Learning (EWSL-88) London, </booktitle> <editor> D. Sleeman, Ed., </editor> <publisher> Pitman, </publisher> <pages> pp. 211-226. </pages>
Reference-contexts: Controlling this search is a major problem. Some of the most common methods which are used for controlling the search are as follows: 1. Use of representations at multiple levels of abstraction (e.g. <ref> [29, 46] </ref>). Here a representation at a coarse level of detail is first acquired, then a more detailed representation is formed. The abstract (coarse) level serves to constrain search at the more detailed level. 2. Use of an oracle (e.g. [31, 30]).
Reference: [47] <author> VanHarmelen, F., and Bundy, A. </author> <year> (1988). </year> <title> Explanation-based generalisation = partial evaluation. </title> <journal> Artificial Intelligence 36, </journal> <volume> 3, </volume> <pages> 401-412. </pages>
Reference-contexts: EBG can in fact be viewed as selectively applying the logic programming method of partial evaluation <ref> [47] </ref>. EBG improves efficiency when the cost of generating, storing and retrieving these learned solution sequences is outweighed by the improved speed they give (a learned solution sequence no longer needs to be recalculated and instead can be immediately applied).
Reference: [48] <author> Watkins, C. J. C. H. </author> <year> (1987). </year> <title> Combining cross-validation and search. </title> <booktitle> In Progress in Machine Learning (proceedings of the 2nd European Working Session on Learning), </booktitle> <editor> I. Bratko and N. Lavrac, Eds., Sigma, Wilmslow, </editor> <booktitle> UK, </booktitle> <pages> pp. 79-87. 20 </pages>
Reference-contexts: As a consequence, the rule set is both large and not of the highest predictive power. This is sometimes referred to as an overfitting of the rules to the data <ref> [48] </ref>. There are several techniques which have been developed for coping with this problem, and we briefly review these here. Data Filtering One technique is to perform induction using only representative training examples, as selected by the expert or automatically.
References-found: 48

