URL: http://www.cs.cmu.edu/afs/cs/usr/akalai/www/PAPERS/blis.ps.gz
Refering-URL: 
Root-URL: 
Email: fsfc,akalai,avrim,ronig@cs.cmu.edu  
Title: On-line Algorithms for Combining Language Models  
Author: Stanley Chen, Adam Kalai, Avrim Blum, Ronald Rosenfeld 
Address: 5000 Forbes Ave. Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: The areas of On-line algorithms and language modeling are both concerned with making decisions from past information. The problem of predicting from expert advice is specifically concerned with combining predictions from a set of models. On-line algorithms developed for this problem have parameters that are updated dynamically to adapt to a data set during evaluation. In this research, we investigate the application of these on-line algorithms to the task of combining multiple language models (such as models trained on text from different domains or topics) into a single composite language model, or supermodel. We use on-line algorithms to automatically adjust model combination parameters such as mixture weights; on-line analysis guarantees that these supermodels will perform almost as well as the best model chosen in hindsight from a large class of supermodels (e.g., the set of all static mixture models). We describe several existing on-line algorithms and present results comparing these techniques with existing language modeling combination methods on the tasks of n-gram model smoothing, domain adaptation, and topic adaptation. We demonstrate that in some situations, on-line techniques are significantly superior to previous methods (i.e., up to 10% better in terms of perplexity), and are especially effective when the nature of the test data is unknown or changes over time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> D. Blackwell. </author> <year> 1956. </year> <title> An analog of the minimax theorem for vector payoffs. </title> <journal> Pacific J. Math., </journal> <volume> 6 </volume> <pages> 1-8. </pages>
Reference-contexts: In the language of competitive analysis, this is the goal of being competitive with respect to the best single expert. This problem and many variations and extensions have been addressed in a number of different communities, under names such as the sequential compound decision problem (Robbins, 1951) <ref> (Blackwell, 1956) </ref>, universal prediction (Feder et al., 1992), universal coding (Shtarkov, 1987), universal portfolios (Cover, 1991), and prediction of individual sequences. A history can be found in (Foster and Vohra, 1995) or (Blum, 1996).
Reference: <author> Avrim Blum and Adam Kalai. </author> <year> 1997. </year> <title> Universal portfolios with and without transaction costs. </title> <booktitle> In Proceedings of the 10th Annual Conference on Computational Learning Theory. </booktitle>
Reference: <author> Avrim Blum. </author> <year> 1996. </year> <title> On-line algorithms in machine learning. </title> <booktitle> In Proceedings of the Workshop on OnLine Algorithms, </booktitle> <publisher> Dagstuhl. </publisher>
Reference: <author> Peter F. Brown, Stephen A. Della Pietra, </author> <note> Vincent J. </note>
Reference: <author> Della Pietra, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1992. </year> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <month> March. </month>
Reference: <author> N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. Schapire, and M.K. Warmuth. </author> <year> 1993. </year> <title> How to use expert advice. </title> <booktitle> In Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-391. </pages>
Reference: <author> Stanley F. Chen, Douglas Beeferman, and Ronald Rosenfeld. </author> <year> 1998. </year> <title> Evaluation metrics for language models. In DARPA Broadcast News Transcription and Understanding Workshop. </title>
Reference: <author> T.M. Cover. </author> <year> 1991. </year> <title> Universal portfolios. </title> <journal> Math. Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <month> January. </month>
Reference-contexts: It is unclear how to set in equation (1) to perform well in this situation (without prior knowledge of the test data). However, using an on-line algorithm for investing in a stock market named the universal algorithm <ref> (Cover, 1991) </ref> that we will refer to as Mixer, we can construct a supermodel that initially distributes its weight evenly among the two models and then updates these weights so that we are guaranteed that it performs almost as well as the best static mixture chosen a posteriori. <p> This problem and many variations and extensions have been addressed in a number of different communities, under names such as the sequential compound decision problem (Robbins, 1951) (Blackwell, 1956), universal prediction (Feder et al., 1992), universal coding (Shtarkov, 1987), universal portfolios <ref> (Cover, 1991) </ref>, and prediction of individual sequences. A history can be found in (Foster and Vohra, 1995) or (Blum, 1996).
Reference: <author> T.M. Cover. </author> <year> 1996. </year> <title> Universal data compression and portfolio selection. </title> <booktitle> In Proceedings of the 37th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 534-538, </pages> <month> October. </month>
Reference: <author> M. Feder, N. Merhav, and M. Gutman. </author> <year> 1992. </year> <title> Universal prediction of individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1258-1270. </pages>
Reference-contexts: This problem and many variations and extensions have been addressed in a number of different communities, under names such as the sequential compound decision problem (Robbins, 1951) (Blackwell, 1956), universal prediction <ref> (Feder et al., 1992) </ref>, universal coding (Shtarkov, 1987), universal portfolios (Cover, 1991), and prediction of individual sequences. A history can be found in (Foster and Vohra, 1995) or (Blum, 1996).
Reference: <author> D.P. Foster and R.V. Vohra. </author> <year> 1995. </year> <title> Regret in the online decision problem. In Something for Nothing Workshop, </title> <month> May. </month>
Reference-contexts: A history can be found in <ref> (Foster and Vohra, 1995) </ref> or (Blum, 1996). In the case of combining language models, the learning algorithm, which is the supermodel, must provide a probability distribution for the next word, rather than a single-bit prediction of that day's weather. <p> Unfortunately, this is not acceptable for evaluation on large corpora. Alternatively, approximations have been suggested which randomly sample of the simplex (Blum and Kalai, 1997) or tile the simplex <ref> (Foster and Vohra, 1995) </ref>, both of which have similar provable guarantees as Mixer. We have implemented the Mixer by tiling the simplex with an interval size that we refer to as the granularity. <p> Guarantees of the performance of Cover's algorithm and the two approximations can be found in (Cover, 1991; Cover, 1996), (Blum and Kalai, 1997), and <ref> (Foster and Vohra, 1995) </ref>, respectively. Cover shows that the worst-case performance ratio associated with this algorithm is 1 t m1 , which corresponds to a regret of (m1) log 2 t t . For large test sets, the regret is small.
Reference: <author> D. Helmbold, R. Schapire, Y. Singer, and M. War-muth. </author> <year> 1996. </year> <title> On-line portfolio selection using multiplicative updates. </title> <booktitle> In Machine Learning: Proceedings of the 13th International Conference. </booktitle>
Reference-contexts: However, in this paper we only considered on-line algorithms with elegant performance guarantees. There exist other algorithms without performance guarantees that often perform well in practice and which are very efficient, e.g., <ref> (Helmbold et al., 1996) </ref>. Another computational consideration with respect to our on-line algorithms is that the updated parameter values j i depend on all previous words, as opposed to a limited history.
Reference: <author> M. Herbster and M.K. Warmuth. </author> <year> 1995. </year> <title> Tracking the best expert. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 286-294. </pages>
Reference-contexts: We also show that the guarantees Switcher achieves are asymptotically optimal. Similar algorithms have been proposed for the general problem of predicting from expert advice <ref> (Herbster and Warmuth, 1995) </ref> as well as for the specific problem of switching investments (Singer, 1998). All three algorithms are the same in the case in which the switching frequency is known in advance. However, Singer provides a clever algorithm that competes even with an unknown switching frequency.
Reference: <author> Frederick Jelinek and Robert L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, The Netherlands: </address> <publisher> North-Holland, </publisher> <month> May. </month>
Reference: <author> Slava M. Katz. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March. </month>
Reference: <author> J. Kelly. </author> <year> 1956. </year> <title> A new interpretation of information rate. </title> <journal> Bell Sys. Tech. Journal, </journal> <volume> 35 </volume> <pages> 917-926. </pages>
Reference: <author> Reinhard Kneser and Hermann Ney. </author> <year> 1995. </year> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 181-184. </pages>
Reference: <author> H. Robbins. </author> <year> 1951. </year> <title> Asymptotically subminimax solutions of compound statistical decision problems. </title> <booktitle> In Proc. 2nd Berkeley Symp. Math. Statist. Prob., </booktitle> <pages> pages 131-148. </pages>
Reference-contexts: In the language of competitive analysis, this is the goal of being competitive with respect to the best single expert. This problem and many variations and extensions have been addressed in a number of different communities, under names such as the sequential compound decision problem <ref> (Robbins, 1951) </ref> (Blackwell, 1956), universal prediction (Feder et al., 1992), universal coding (Shtarkov, 1987), universal portfolios (Cover, 1991), and prediction of individual sequences. A history can be found in (Foster and Vohra, 1995) or (Blum, 1996).
Reference: <author> K. Seymore and R. Rosenfeld. </author> <year> 1997. </year> <title> Using story topics for language model adaptation. </title> <booktitle> In Proceedings of Eurospeech '97. </booktitle>
Reference: <author> J. Shtarkov. </author> <year> 1987. </year> <title> Universal sequential coding of single measures. </title> <booktitle> Problems of Information Transmission, </booktitle> <pages> pages 175-185. </pages>
Reference-contexts: This problem and many variations and extensions have been addressed in a number of different communities, under names such as the sequential compound decision problem (Robbins, 1951) (Blackwell, 1956), universal prediction (Feder et al., 1992), universal coding <ref> (Shtarkov, 1987) </ref>, universal portfolios (Cover, 1991), and prediction of individual sequences. A history can be found in (Foster and Vohra, 1995) or (Blum, 1996).
Reference: <author> Yoram Singer. </author> <year> 1998. </year> <title> Switching portfolios. </title> <booktitle> In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI-98). </booktitle>
Reference: <author> Fuliang Weng, Andreas Stolcke, and Ananth Sankar. </author> <year> 1997. </year> <title> Hub4 language modeling using domain interpolation and data clustering. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> Wash-ington, D.C., </address> <month> February. 11 </month>
References-found: 22

