URL: http://www.cs.brown.edu/people/lpk/ijcai93.ps
Refering-URL: http://www.cs.brown.edu/research/ai/publications/
Root-URL: http://www.cs.brown.edu
Email: lpk@cs.brown.edu  
Title: Learning to Achieve Goals  
Author: Leslie Pack Kaelbling 
Date: 1910  
Address: Box  Providence, RI 02912 USA  
Affiliation: Computer Science Department  Brown University  
Abstract: Temporal difference methods solve the temporal credit assignment problem for reinforcement learning. An important subproblem of general reinforcement learning is learning to achieve dynamic goals. Although existing temporal difference methods, such as Q learning, can be applied to this problem, they do not take advantage of its special structure. This paper presents the DG-learning algorithm, which learns efficiently to achieve dynamically changing goals and exhibits good knowledge transfer between goals. In addition, this paper shows how traditional relaxation techniques can be applied to the problem. Finally, experimental results are given that demonstrate the superiority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.
Abstract-found: 1
Intro-found: 1
Reference: [ Barto et al., 1989 ] <author> A. G. Barto, R. S. Sutton, and C. J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Mas-sachusetts, Amherst, Massachusetts, </institution> <year> 1989. </year> <title> Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks, </title> <editor> Michael Gabriel and John Moore, editors. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: A crucial problem in reinforcement learning is temporal credit assignment: how to choose actions based on good results that happen after (perhaps long after) the action is taken. This problem is solved well in the general case by temporal difference methods, such as Watkins' Q learning <ref> [ Barto et al., 1989; Watkins, 1989 ] </ref> and Sutton's TD algorithm [ Sutton, 1988 ] .
Reference: [ Barto et al., 1991 ] <author> Andrew G. Barto, Steven J. Bradtke, and Satinder Pal Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Department of Computer and Information Science, University of Mas-sachusetts, Amherst, Massachusetts, </institution> <year> 1991. </year>
Reference-contexts: A preliminary implementation shows that this notion has considerable promise. 7 Conclusions Many problems considered in AI and robotics have the nature of dynamically changing goals of achievement. A re-application of the ideas of on-line stochastic dynamic programming <ref> [ Barto et al., 1991 ] </ref> to the problem of calculating shortest paths results in DG learning, which is a very effective learning method.
Reference: [ Bellman, 1957 ] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year>
Reference-contexts: Given definitions of the transition probabilities and the expected reinforcements, it is possible to solve for the optimal policy, using methods from dynamic programming <ref> [ Bellman, 1957; Howard, 1960 ] </ref> . A more interesting case occurs when we wish to simultaneously learn the dynamics of the world and construct the policy. Watkins' Q learning algorithm gives us an elegant and efficient method for doing this.
Reference: [ Cormen et al., 1990 ] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronals L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press / McGraw Hill, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: A much more useful method is provided by the Floyd-Warshall algorithm <ref> [ Cormen et al., 1990 ] </ref> , which converges on the correct function after n 3 relaxations, where n is the size of the state set.
Reference: [ Howard, 1960 ] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Given definitions of the transition probabilities and the expected reinforcements, it is possible to solve for the optimal policy, using methods from dynamic programming <ref> [ Bellman, 1957; Howard, 1960 ] </ref> . A more interesting case occurs when we wish to simultaneously learn the dynamics of the world and construct the policy. Watkins' Q learning algorithm gives us an elegant and efficient method for doing this.
Reference: [ Kaelbling, 1993a ] <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is rarely the case, however, that optimal behavior is actually necessary, and it might be advantageous to trade some degree of performance for increased computational efficiency. In work that builds upon the ideas in this paper, Kael-bling <ref> [ Kaelbling, 1993a ] </ref> introduces a hierarchical model of goal achievement that allows efficient learning of approximately optimal paths.
Reference: [ Kaelbling, 1993b ] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1993. </year> <note> Also available as a PhD Thesis from Stanford University, </note> <year> 1990. </year>
Reference-contexts: But in doing so, we are in danger of violating the requirement that every state-action pair be tried infinitely often. This is an instance of the exploration versus exploitation trade-off; it has been treated extensively elsewhere <ref> [ Kaelbling, 1993b; Thrun, 1992 ] </ref> . In the interest of simplicity in this paper, we generate actions probabilistically based on the Q values using a Boltzmann distribution. <p> The trade-off between exploration and exploitation is important in DG learning as well; we use the Boltzmann distribution to construct action probabilities, though in terval estimation techniques <ref> [ Kaelbling, 1993b ] </ref> could be employed for more careful exploration. 4 Learning From a Model In on-line learning tasks, every step taken by the learning algorithm is a step taken by the agent in the world.
Reference: [ Lin, 1991 ] <author> Long-Ji Lin. </author> <title> Self-improving based on reinforcement learning, planning, and teaching. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Evanston, Illinois, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Experiments have shown this to be a very effective procedure when a forward model of the world (mapping from situations and actions into results) is available. When the model is not made available a priori, it must also be constructed on line. Lin's results <ref> [ Lin, 1991 ] </ref> show that using the model for learning while it is being con-structed is not necessarily advantageous.
Reference: [ Singh, 1992 ] <author> Satinder Pal Singh. </author> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 323-340, </pages> <year> 1992. </year>
Reference-contexts: The work that is done in updating all goals will not speed learning or performance for the particular goal that is being achieved; but it will result in an extremely efficient transfer of knowledge to the achievement of other goals. Singh <ref> [ Singh, 1992 ] </ref> has also addressed this issue, but in a more complex network architecture. 4.3 Relaxation in G Learning Because we know that the problem we are solving is one of finding shortest paths, we can apply basic knowledge about the nature of distance metrics: for any intermediate state
Reference: [ Sutton, 1988 ] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: This problem is solved well in the general case by temporal difference methods, such as Watkins' Q learning [ Barto et al., 1989; Watkins, 1989 ] and Sutton's TD algorithm <ref> [ Sutton, 1988 ] </ref> .
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, Texas, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In many cases, the agent is incurring risk by taking exploratory actions and we would like to minimize such risk. One way to decrease the risk is to employ internal learning from a model rather than direct learning from the world. 4.1 Dyna and Q Learning Sutton <ref> [ Sutton, 1990 ] </ref> and Whitehead and Ballard [ Whitehead and Ballard, 1989 ] have both explored the use of mixing real learning steps in the world with simulated learning steps in an internal model.
Reference: [ Thrun, 1992 ] <author> Sebastian B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In David A. White and Donald A. Sofge, editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: But in doing so, we are in danger of violating the requirement that every state-action pair be tried infinitely often. This is an instance of the exploration versus exploitation trade-off; it has been treated extensively elsewhere <ref> [ Kaelbling, 1993b; Thrun, 1992 ] </ref> . In the interest of simplicity in this paper, we generate actions probabilistically based on the Q values using a Boltzmann distribution.
Reference: [ Watkins, 1989 ] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Cam-bridge, </institution> <year> 1989. </year>
Reference-contexts: A crucial problem in reinforcement learning is temporal credit assignment: how to choose actions based on good results that happen after (perhaps long after) the action is taken. This problem is solved well in the general case by temporal difference methods, such as Watkins' Q learning <ref> [ Barto et al., 1989; Watkins, 1989 ] </ref> and Sutton's TD algorithm [ Sutton, 1988 ] . <p> Given the Q values, there is a policy defined by taking, in any situation s, the action a that maximizes Q (s; a). Watkins showed <ref> [ Watkins, 1989 ] </ref> that, given some assumptions, including that every state-action pair is tried infinitely often on an infinite run, the Q values will converge to the true Q fl values, and hence the induced policy will converge to the optimal one.
Reference: [ Whitehead and Ballard, 1989 ] <author> Steven D. Whitehead and Dana H. Ballard. </author> <title> A role for anticipation in reactive systems that learn. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 354-357, </pages> <address> Ithaca, New York, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One way to decrease the risk is to employ internal learning from a model rather than direct learning from the world. 4.1 Dyna and Q Learning Sutton [ Sutton, 1990 ] and Whitehead and Ballard <ref> [ Whitehead and Ballard, 1989 ] </ref> have both explored the use of mixing real learning steps in the world with simulated learning steps in an internal model.
References-found: 14

