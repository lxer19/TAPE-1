URL: http://www.iscs.nus.sg/~liuh/tkde97.ps
Refering-URL: 
Root-URL: 
Title: Feature Selection via Discretization  
Author: Huan Liu and Rudy Setiono 
Keyword: discretization, feature selection, pattern classification  
Abstract: Discretization can turn numeric attributes into discrete ones. Feature selection can eliminate some irrelevant and/or redundant attributes. Chi2 is a simple and general algorithm that uses the 2 statistic to discretize numeric attributes repeatedly until some inconsistencies are found in the data. It achieves feature selection via dis-cretization. It can handle mixed attributes, work with multi-class data, and remove irrelevant and redundant attributes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69(1-2):279-305, </volume> <month> November </month> <year> 1994. </year>
Reference: [2] <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> In European Working Session on Learning, </booktitle> <year> 1991. </year>
Reference: [3] <author> U.M. Fayyad and K.B. Irani. </author> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 104-110. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference: [4] <author> R. Kerber. ChiMerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 123-128. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The above process is repeated with a decreased ff until the discretized data's inconsistency rate exceeds ffi. Phase 1 is, as a matter of fact, a generalized version of ChiMerge of Kerber <ref> [4] </ref>. Instead of specifying a 2 threshold, Phase 1 of Chi2 wraps up ChiMerge with a loop that automatically increments the 2 threshold (or equivalently decreases ff). A consistency checking is also introduced as a stopping criterion to make sure that the discretized data set accurately represents the original one.
Reference: [5] <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92, Proceedings Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1992. </year>
Reference: [6] <author> H. Liu and W.X. Wen. </author> <title> Concept learning through feature selection. </title> <booktitle> In Proceedings of First Australian and New Zealand Conference on Intelligent Information Systems, </booktitle> <year> 1993. </year>
Reference: [7] <author> J. Murdoch and J.A. Barnes. </author> <title> Statistical tables for science, engineering, management and business studies. </title> <publisher> THE MACMILLAN PRESS LTD, </publisher> <year> 1986. </year>
Reference: [8] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: We want to establish that (1) Chi2 helps improve predictive accuracy; and (2) Chi2 properly and effectively discretizes data as well as eliminates some irrelevant/redundant attributes (this explains why the predictive accuracy of a classifier is improved.) C4.5 <ref> [8] </ref> is used for these purposes.
Reference: [9] <author> H. Ragavan and L. Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Conference, </booktitle> <pages> pages 252-259. </pages> <publisher> Morgan Kaufmann Pub. </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference: [10] <author> H. Liu and R. Setiono. </author> <title> A probabilistic approach to feature selection A filter solution. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 319-327, </pages> <year> 1996. </year>
Reference: [11] <author> I. Sethi, and G. Savarajudu. </author> <title> Hierarchical classifier design using mutual information. </title> <journal> In IEEE Trans. PAMI, </journal> <volume> Vol 4, </volume> <pages> pages 441-445, </pages> <month> July </month> <year> 1982. </year>
Reference: [12] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year> <month> 4 </month>
References-found: 12

