URL: http://www.cs.washington.edu/research/projects/safety/www/papers/glascow.ps
Refering-URL: http://www.cs.washington.edu/research/projects/safety/www/papers.html
Root-URL: 
Email: fleveson,denisep,sds,skoga,jdreeseg@cs.washington.edu  
Title: Analyzing Software Specifications for Mode Confusion Potential  
Author: Nancy G. Leveson L. Denise Pinnel Sean David Sandys Shuichi Koga Jon Damon Reese 
Address: Seattle, WA 98195-2350  
Affiliation: Computer Science and Engineering University of Washington  
Abstract: Increased automation in complex systems has led to changes in the human controller's role and to new types of technology-induced human error. Attempts to mitigate these errors have primarily involved giving more authority to the automation, enhancing operator training, or changing the interface. While these responses may be reasonable under many circumstances, an alternative is to redesign the automation in ways that do not reduce necessary or desirable functionality or to change functionality where the tradeoffs are judged to be acceptable. This paper describes an approach to detecting error-prone automation features early in the development process while significant changes can still be made to the conceptual design of the system. The information about such error-prone features can also be useful in the design of the operator interface, operational procedures, or operator training. 
Abstract-found: 1
Intro-found: 1
Reference: [CO88] <author> Carroll, J.M. and Olson, J.R. </author> <title> Mental models in human-computer interaction. </title> <editor> in M. He-lander (Ed.) </editor> <title> Handbook of Human-Computer Interaction, </title> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 45-65, </pages> <year> 1988. </year>
Reference-contexts: Carroll and Olson define a consistent design as one where a similar task or goal is associated with similar or identical actions <ref> [CO88] </ref>. Consistent behavior makes it easier for the operator to learn how a system works, to build an appropriate mental model of the automation, and to anticipate system behavior. An example of inconsistency was detected in an A-320 simulator study involving a go-around below 100 feet above ground level.
Reference: [CPWM91] <author> Cook, R.I., Potter, S.S., Woods, D.D. and McDonald, J.M. </author> <title> Evaluating the human engineering of microprocessor-controlled operating room devices. </title> <journal> Journal of Clinical Monitoring, </journal> <volume> 7, </volume> <pages> pp. 217-226, </pages> <year> 1991. </year>
Reference-contexts: An example of an output interface mode problem was identified by Cook et.al. in a medical operating room device with two operating modes: warmup and normal <ref> [CPWM91] </ref>. The device starts in warmup mode when turned on and changes from normal mode to warmup mode whenever either of two particular settings are adjusted by the operator.
Reference: [Deg96] <author> Degani, A. </author> <title> Modeling Human-Machine Systems: On Modes, Error, and Patterns of Interaction. </title> <type> Ph. D. thesis, </type> <institution> Georgia Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Sarter and Woods identify some of these error forms. Degani has also identified some features that lead to mode confusion <ref> [Deg96] </ref>, and Jaffe [JL89, Lev95] has identified general requirements completeness criteria to eliminate some types of human computer interaction errors. <p> Mode confusion errors result from divergent controller models. See figure 1. Note that there are several sources of inconsistency due to improper feedback. In attempting to categorize factors that predict mode errors, it is useful to distinguish between different types of modes. Degani classifies modes into three types <ref> [Deg96] </ref>: 1. Interface modes specify the behavior of the interface. They are used to increase the size of the input or output space. 2. Functional modes specify the behavior of the var ious functions of the machine. 3.
Reference: [Hans97] <author> Hansman, John. </author> <type> Personal communication. </type>
Reference-contexts: In addition to providing design guidance, this approach might provide a way of "measuring" or evaluating the cognitive demands involved in working with specific automated devices. Hansman has suggested that automation complexity be defined in terms of the predictability of the automation behavior <ref> [Hans97] </ref>. This predictability can potentially be evaluated using our approach. Analyzing designs requires an appropriate modeling and specification language. This language must be both formally analyzable and readable without advanced mathematical training.
Reference: [HL96] <author> Heimdahl, M. P. E. and N. Leveson. </author> <title> Completeness and consistency analysis of state-based requirements. </title> <journal> Transactions on Software Engineering, </journal> <month> June </month> <year> 1996. </year>
Reference-contexts: In our previous design criteria and analysis tools, we include a check for nondeterminism in the software behavior, that is, we check to determine whether more than one transition can be taken out of a state under the same conditions <ref> [HL96] </ref>. But consistency in this case requires more than simple deterministic behavior on the part of the automation.
Reference: [JLHM91] <author> Jaffe, M.S, Leveson, N.G., Heimdahl, M.P.E., and Melhart, B.E.. </author> <title> Software requirements analysis for real-time process-control systems. </title> <journal> IEEE Transations on Soft ware Engineering, </journal> <volume> SE-17(3):241-258, </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: Using the lessons learned from this experience and others, we are designing a toolkit called SpecTRM (Specification Tools and Requirements Methodology) that includes a requirements specification language SpecTRM-RL. Underneath SpecTRM-RL there is a formal state machine model called RSM <ref> [JLHM91] </ref> upon which we have defined a set of correctness and completeness design criteria for safety-critical process-control system specifications. One of our goals for SpecTRM-RL is to incorporate features to assist in designing less error-prone human-computer interactions and interfaces and in detecting potential communication problems, such as mode confusion. <p> Figure 4 shows an example specification of a transition. Once a blackbox model of the required system behavior has been built, this model can be evaluated as to whether it satisfies design criteria that are known to minimize errors and accidents. Design Criteria and Analysis Jaffe <ref> [JLHM91] </ref> originally identified 26 completeness criteria for requirements specifications, which we have now extended to close to 50 criteria [Lev95].
Reference: [JL89] <author> Jaffe, M.S. and Leveson, N.G. </author> <title> Implications of the man-machine interface for software requirements completeness in real-time, safety-critical software systems. </title> <booktitle> Proceedings of IFAC/IFIP SAFECOMP 89, </booktitle> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Sarter and Woods identify some of these error forms. Degani has also identified some features that lead to mode confusion [Deg96], and Jaffe <ref> [JL89, Lev95] </ref> has identified general requirements completeness criteria to eliminate some types of human computer interaction errors.
Reference: [Lev95] <author> Leveson, N.G. Safeware: </author> <title> System Safety and Computers. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1995. </year>
Reference-contexts: To identify and evaluate potential tradeoffs, we need to understand why the problems occur. Accidents in high-tech systems are related to complexity and coupling <ref> [Per84, Lev95] </ref>. Perrow distinguishes between accidents caused by component failures and those, which he calls system accidents, that are caused by interactive complexity in the presence of tight coupling. High-technology systems are often made up of networks of closely related subsystems (some of which may involve humans). <p> Sarter and Woods identify some of these error forms. Degani has also identified some features that lead to mode confusion [Deg96], and Jaffe <ref> [JL89, Lev95] </ref> has identified general requirements completeness criteria to eliminate some types of human computer interaction errors. <p> The model is updated and kept consistent with the actual system state through various forms of feedback from the system to the controller. When the controller's model of the system diverges from the actual system state, erroneous control commands (based on the incorrect model) can lead to an accident <ref> [Lev95] </ref>. The situation becomes more complicated when there are multiple controllers because the models of the various controllers must also be kept consistent. <p> Design Criteria and Analysis Jaffe [JLHM91] originally identified 26 completeness criteria for requirements specifications, which we have now extended to close to 50 criteria <ref> [Lev95] </ref>. <p> The rest of the paper describes six of our categories of potential design flaws: interface interpretation errors, inconsistent behavior, indirect mode changes, operator authority limits, unintended side effects, and lack of appropriate feedback. Additional criteria can be found in <ref> [Lev95] </ref> and others will be described in future papers.
Reference: [Lev97] <author> Leveson, N.G. </author> <title> Intent Specifications. </title> <note> in preparation. </note>
Reference-contexts: Because consistency may be most important during critical situations or when the behavior is related to a safety design constraint, our hazard analysis tools may be able to assist with these decisions and our new intent specifications <ref> [Lev97] </ref> (a form of Rasmussen's means-ends hierarchy adapted for software) can be used to Enabled ! Disabled Condition: A D Supervisory-Mode Entered Planner T Supervisory-Mode Entered Joystick T Dead-Man-Switch-Open () F Safety-Circuit In-State Unsafe T Robot-Location In Robot-Between-Work-Area T Maps-Operating-Modes In-Mode Off T References: trace such behavior back to its original
Reference: [LHHR94] <author> Leveson, N. G., M. Heimdahl, H. Hildreth, and J. Reese. </author> <title> Requirements specification for process-control systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> September </month> <year> 1994. </year>
Reference-contexts: Our first language, RSML (Requirements State Machine Language), was designed while specifying the system requirements for TCAS II, an airborne aircraft collision avoidance system, for the FAA <ref> [LHHR94] </ref>. Using the lessons learned from this experience and others, we are designing a toolkit called SpecTRM (Specification Tools and Requirements Methodology) that includes a requirements specification language SpecTRM-RL.
Reference: [LP97] <author> Leveson, N.G. and Palmer, E. </author> <title> Identifying Indirect Mode Transitions: `Oops, it didn't arm' as a case study. </title> <note> in preparation. </note>
Reference-contexts: Leveson and Palmer have modeled an example of this problem in SpecTRM-RL and shown how it could be detected and fixed <ref> [LP97] </ref>. Another example of indirect mode change can be found in the MAPS specification. In this scenario, MAPS is in joystick supervisory mode and it receives a message from the planner that the robot has reached the work area.
Reference: [MMR92] <author> Madsen, M., Murphy, J.S., Rosso-Llopart, M. </author> <title> MAPS Software Requirements Specification. </title> <institution> School of Computer Science, Carnegia Mellon University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The software requirements are taken from a master's level project at the CMU Software Engineering Institute for part of a robot that was being designed and constructed in the Robotics Department <ref> [MMR92] </ref>. The system component used as the example in this paper is called MAPS (Mobility and Positioning Software).
Reference: [MLRPS97] <author> Modugno, F., N. Leveson, J. Reese, K. Partridge, and S. Sandys. </author> <title> Integrated safety analysis of requirements specifications. </title> <booktitle> Third IEEE Interational Symposium on Requirements Engineering, </booktitle> <year> 1997. </year>
Reference-contexts: This language must be both formally analyzable and readable without advanced mathematical training. While automated tools may be necessary to analyze some aspects of large and complex models, we believe (and our empirical evidence supports the view) that the most important errors will be found by human experts <ref> [MLRPS97] </ref>. Therefore, one of our goals in the design of our modeling language and tools is to provide support in human navigation and understanding of complex models and specifications. In addition, any potential design flaws detected by automated tools will need to be evaluated by humans.
Reference: [Pal96] <author> Palmer, E. </author> <title> "oops, it didn't arm" a case study of two automation surprises. </title> <type> NASA Technical Report, </type> <year> 1996. </year>
Reference-contexts: As a final example, Palmer has described an example of a common indirect mode transition problem called a "kill-the-capture bust" that has been noted in hundreds of ASRS reports <ref> [Pal96] </ref>. Leveson and Palmer have modeled an example of this problem in SpecTRM-RL and shown how it could be detected and fixed [LP97]. Another example of indirect mode change can be found in the MAPS specification.
Reference: [Per84] <author> Perrow, C. </author> <title> Normal Accidents: Living with High-Risk Technology. </title> <publisher> Basic Books, Inc., </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: To identify and evaluate potential tradeoffs, we need to understand why the problems occur. Accidents in high-tech systems are related to complexity and coupling <ref> [Per84, Lev95] </ref>. Perrow distinguishes between accidents caused by component failures and those, which he calls system accidents, that are caused by interactive complexity in the presence of tight coupling. High-technology systems are often made up of networks of closely related subsystems (some of which may involve humans).
Reference: [Ras90] <author> Rasmussen, J. </author> <title> Human error and the problem of causality in analysis of accidents. </title> <editor> In D.E. Broadbent, J. Reason, and A. Baddeley, editors, </editor> <booktitle> Human Factors in Hazardous Situations, </booktitle> <pages> pages 1-12, </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1990. </year>
Reference-contexts: In addition, the consequences of operator mode awareness problems tended to be minor, partly because feedback about operator errors was fast and complete enough that operators were able to recover before the errors caused serious problems (Rasmussen's concept of error tolerance) <ref> [Ras90] </ref>. Studies of less complex aircraft automation show that pilots sometimes lose track of the automation behavior and experience difficulties with directing the automation, primarily in the context of highly dynamic and/or non-normal situations [SW95].
Reference: [SW95] <author> Sarter, N. D. and D. </author> <title> Woods "How in the world did I ever get into that mode?": Mode error and awareness in supervisory control. </title> <booktitle> Human Factors 37, </booktitle> <pages> 5-19. </pages>
Reference-contexts: Sarter and Woods extend the classic definition of mode error and distinguish between errors of commission (where an operator takes an inappropriate action) and errors of omission (where the operator fails to take a required action) <ref> [SW95] </ref>. The first automated systems tended to have only a small number of independent modes, and functions were associated with one overall mode setting. <p> Studies of less complex aircraft automation show that pilots sometimes lose track of the automation behavior and experience difficulties with directing the automation, primarily in the context of highly dynamic and/or non-normal situations <ref> [SW95] </ref>. Sarter and Woods conclude that in most cases, these problems are associated with errors of commission, that is, with errors that require a pilot action in order for the problem to occur. <p> This type of error is still the prevalent one on relatively simple devices such as word processors. In contrast, studies of more advanced automation in aircraft like the A-320 find that mode errors of omission are the dominant form of error <ref> [SW95] </ref>. In this type of mode error, the operator fails to take an action that is required, perhaps because the automation has done something undesirable (perhaps involving a mode change) and the operator does not notice. <p> As these roles change, the operator tasks and cognitive demands are not necessarily reduced, but instead tend to change in their basic nature. The added or changed cognitive demands tend to congregate at high-tempo, high-criticality periods <ref> [SW95] </ref>. While some types of errors and failures have declined, new error forms and paths to system breakdown have been introduced. Some of these new error forms are a result of mode proliferation without appropriate support. <p> However, the automation and interfaces have been designed assuming conventional monitoring. Simply calling for systems with fewer or less complex modes is unrealistic: Simplifying modes and automation behavior often requires tradeoffs with in creased precision or efficiency and with marketing demands from a diverse set of customers <ref> [SW95] </ref>. However, systems may exhibit accidental complexity where the automation can be redesigned to reduce the potential for human error without sacrificing system capabilities. <p> Some of the increased complexity has been the result of what Sarter, Woods, and Billings have called technology-centered automation <ref> [SW95] </ref>. Too often, the designers of the automation focus on technical aspects and do not devote enough attention to the cognitive and other demands on the operator. <p> what Wiener calls "clumsy automation." If it is true that mode-related problems are caused by clumsy or poorly designed automation, then changing the human interface, training, or operational procedures is not the obvious, or at least the only solution: "Training cannot and should not be the fix for bad design" <ref> [SW95] </ref>. Instead, if we can identify automation design characteristics that lead to mode awareness errors or that increase cognitive demands, then we may be able to redesign the automation without reducing system capabilities. <p> For our proposed analysis approach to work, human errors must be non-random. After studying accidents and incidents in the new, highly automated aircraft, Sarter and Woods have concluded that certain errors are predictable <ref> [SW95] </ref>: They are the regular and predictable consequences of a variety of identifiable factors. <p> This was not intended by the pilots who were not aware of the active "interface mode" and failed to detect the problem. As a consequence of the too steep descent, the airplane crashed into a mountain <ref> [SW95] </ref>. Several design constraints can assist in reducing interface interpretation errors. The first is that any mode used to control interpretation of the supervisory interface should be annunciated to the operator (that is, it should be part of the displays interface in our modeling language). <p> Sarter and Woods found that pilots failed to anticipate and realize that the autothrust system did not arm when they selected TOGA (take off/go around) power under these conditions because it did so under all other circumstances where TOGA power is applied <ref> [SW95] </ref>. Another example of inconsistent automation behavior, which was implicated in an A-320 accident, involves a protection function that is provided in all automation configurations except the altitude acquisition mode in which the autopilot was operating. <p> Even in more low pressure situations, consistency (or predictability) is important in light of the evidence from pilot surveys that their normal monitoring behavior may change on advanced flight decks <ref> [SW95] </ref>. Pilots on conventional aircraft use a highly trained instrument scanning pattern of recurrently sampling a given set of basic flight parameters. <p> A standard instrument scan, on the other hand, serves to ensure that all relevant parameters concerning airplane behavior will be monitored at certain time intervals to make sure that no unexpected and maybe undesirable changes occur <ref> [SW95] </ref>. In our previous design criteria and analysis tools, we include a check for nondeterminism in the software behavior, that is, we check to determine whether more than one transition can be taken out of a state under the same conditions [HL96]. <p> It has been speculated that the pilots did not notice the mode annunciation because the indirect mode change occurred during approach when the pilots were busy and they were not expecting the change <ref> [SW95] </ref>. Another example of such an indirect mode change in the A-320 automation involves an automatic mode transition triggered when the airspeed exceeds a predefined limit. <p> As a consequence of this sudden burst of power, the airplane pitched up to about 50 degrees, entered a sharp left bank, and went into a dive. The pilots eventually disengaged the autothrust system and its associated protection function and regained control of the aircraft <ref> [SW95] </ref>. Various design criteria are related to authority limits. For example, information about any modes or states where the operator input is ignored or limited must be provided in the supervisory interface. <p> Incomplete feedback is often implicated in accident scenarios. For example, in the A-320 Bangalore accident, the pilot flying (PF) had disengaged his flight director during the approach and was assuming that the pilot-not-flying (PNF) would do the same thing <ref> [SW95] </ref>. The result would have been a mode configuration in which airspeed is automatically controlled by the autothrottle (the speed mode), which is the recommended procedure for the approach phase.
Reference: [SW95] <author> Sarter, N. D. and D. Woods Strong, </author> <type> silent, </type> <institution> and out-of-the-loop. CSEL Report 95-TR-01, Ohio State University, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Sarter and Woods extend the classic definition of mode error and distinguish between errors of commission (where an operator takes an inappropriate action) and errors of omission (where the operator fails to take a required action) <ref> [SW95] </ref>. The first automated systems tended to have only a small number of independent modes, and functions were associated with one overall mode setting. <p> Studies of less complex aircraft automation show that pilots sometimes lose track of the automation behavior and experience difficulties with directing the automation, primarily in the context of highly dynamic and/or non-normal situations <ref> [SW95] </ref>. Sarter and Woods conclude that in most cases, these problems are associated with errors of commission, that is, with errors that require a pilot action in order for the problem to occur. <p> This type of error is still the prevalent one on relatively simple devices such as word processors. In contrast, studies of more advanced automation in aircraft like the A-320 find that mode errors of omission are the dominant form of error <ref> [SW95] </ref>. In this type of mode error, the operator fails to take an action that is required, perhaps because the automation has done something undesirable (perhaps involving a mode change) and the operator does not notice. <p> As these roles change, the operator tasks and cognitive demands are not necessarily reduced, but instead tend to change in their basic nature. The added or changed cognitive demands tend to congregate at high-tempo, high-criticality periods <ref> [SW95] </ref>. While some types of errors and failures have declined, new error forms and paths to system breakdown have been introduced. Some of these new error forms are a result of mode proliferation without appropriate support. <p> However, the automation and interfaces have been designed assuming conventional monitoring. Simply calling for systems with fewer or less complex modes is unrealistic: Simplifying modes and automation behavior often requires tradeoffs with in creased precision or efficiency and with marketing demands from a diverse set of customers <ref> [SW95] </ref>. However, systems may exhibit accidental complexity where the automation can be redesigned to reduce the potential for human error without sacrificing system capabilities. <p> Some of the increased complexity has been the result of what Sarter, Woods, and Billings have called technology-centered automation <ref> [SW95] </ref>. Too often, the designers of the automation focus on technical aspects and do not devote enough attention to the cognitive and other demands on the operator. <p> what Wiener calls "clumsy automation." If it is true that mode-related problems are caused by clumsy or poorly designed automation, then changing the human interface, training, or operational procedures is not the obvious, or at least the only solution: "Training cannot and should not be the fix for bad design" <ref> [SW95] </ref>. Instead, if we can identify automation design characteristics that lead to mode awareness errors or that increase cognitive demands, then we may be able to redesign the automation without reducing system capabilities. <p> For our proposed analysis approach to work, human errors must be non-random. After studying accidents and incidents in the new, highly automated aircraft, Sarter and Woods have concluded that certain errors are predictable <ref> [SW95] </ref>: They are the regular and predictable consequences of a variety of identifiable factors. <p> This was not intended by the pilots who were not aware of the active "interface mode" and failed to detect the problem. As a consequence of the too steep descent, the airplane crashed into a mountain <ref> [SW95] </ref>. Several design constraints can assist in reducing interface interpretation errors. The first is that any mode used to control interpretation of the supervisory interface should be annunciated to the operator (that is, it should be part of the displays interface in our modeling language). <p> Sarter and Woods found that pilots failed to anticipate and realize that the autothrust system did not arm when they selected TOGA (take off/go around) power under these conditions because it did so under all other circumstances where TOGA power is applied <ref> [SW95] </ref>. Another example of inconsistent automation behavior, which was implicated in an A-320 accident, involves a protection function that is provided in all automation configurations except the altitude acquisition mode in which the autopilot was operating. <p> Even in more low pressure situations, consistency (or predictability) is important in light of the evidence from pilot surveys that their normal monitoring behavior may change on advanced flight decks <ref> [SW95] </ref>. Pilots on conventional aircraft use a highly trained instrument scanning pattern of recurrently sampling a given set of basic flight parameters. <p> A standard instrument scan, on the other hand, serves to ensure that all relevant parameters concerning airplane behavior will be monitored at certain time intervals to make sure that no unexpected and maybe undesirable changes occur <ref> [SW95] </ref>. In our previous design criteria and analysis tools, we include a check for nondeterminism in the software behavior, that is, we check to determine whether more than one transition can be taken out of a state under the same conditions [HL96]. <p> It has been speculated that the pilots did not notice the mode annunciation because the indirect mode change occurred during approach when the pilots were busy and they were not expecting the change <ref> [SW95] </ref>. Another example of such an indirect mode change in the A-320 automation involves an automatic mode transition triggered when the airspeed exceeds a predefined limit. <p> As a consequence of this sudden burst of power, the airplane pitched up to about 50 degrees, entered a sharp left bank, and went into a dive. The pilots eventually disengaged the autothrust system and its associated protection function and regained control of the aircraft <ref> [SW95] </ref>. Various design criteria are related to authority limits. For example, information about any modes or states where the operator input is ignored or limited must be provided in the supervisory interface. <p> Incomplete feedback is often implicated in accident scenarios. For example, in the A-320 Bangalore accident, the pilot flying (PF) had disengaged his flight director during the approach and was assuming that the pilot-not-flying (PNF) would do the same thing <ref> [SW95] </ref>. The result would have been a mode configuration in which airspeed is automatically controlled by the autothrottle (the speed mode), which is the recommended procedure for the approach phase.
Reference: [SW95] <editor> Sarter, N. D., Woods, D.D. and Billings, C.E. Automation Surprises. in G. Sal-vendy (Ed.) </editor> <booktitle> Handbook of Human Factors/Ergonomics, 2nd Edition, </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <publisher> in press. </publisher>
Reference-contexts: Sarter and Woods extend the classic definition of mode error and distinguish between errors of commission (where an operator takes an inappropriate action) and errors of omission (where the operator fails to take a required action) <ref> [SW95] </ref>. The first automated systems tended to have only a small number of independent modes, and functions were associated with one overall mode setting. <p> Studies of less complex aircraft automation show that pilots sometimes lose track of the automation behavior and experience difficulties with directing the automation, primarily in the context of highly dynamic and/or non-normal situations <ref> [SW95] </ref>. Sarter and Woods conclude that in most cases, these problems are associated with errors of commission, that is, with errors that require a pilot action in order for the problem to occur. <p> This type of error is still the prevalent one on relatively simple devices such as word processors. In contrast, studies of more advanced automation in aircraft like the A-320 find that mode errors of omission are the dominant form of error <ref> [SW95] </ref>. In this type of mode error, the operator fails to take an action that is required, perhaps because the automation has done something undesirable (perhaps involving a mode change) and the operator does not notice. <p> As these roles change, the operator tasks and cognitive demands are not necessarily reduced, but instead tend to change in their basic nature. The added or changed cognitive demands tend to congregate at high-tempo, high-criticality periods <ref> [SW95] </ref>. While some types of errors and failures have declined, new error forms and paths to system breakdown have been introduced. Some of these new error forms are a result of mode proliferation without appropriate support. <p> However, the automation and interfaces have been designed assuming conventional monitoring. Simply calling for systems with fewer or less complex modes is unrealistic: Simplifying modes and automation behavior often requires tradeoffs with in creased precision or efficiency and with marketing demands from a diverse set of customers <ref> [SW95] </ref>. However, systems may exhibit accidental complexity where the automation can be redesigned to reduce the potential for human error without sacrificing system capabilities. <p> Some of the increased complexity has been the result of what Sarter, Woods, and Billings have called technology-centered automation <ref> [SW95] </ref>. Too often, the designers of the automation focus on technical aspects and do not devote enough attention to the cognitive and other demands on the operator. <p> what Wiener calls "clumsy automation." If it is true that mode-related problems are caused by clumsy or poorly designed automation, then changing the human interface, training, or operational procedures is not the obvious, or at least the only solution: "Training cannot and should not be the fix for bad design" <ref> [SW95] </ref>. Instead, if we can identify automation design characteristics that lead to mode awareness errors or that increase cognitive demands, then we may be able to redesign the automation without reducing system capabilities. <p> For our proposed analysis approach to work, human errors must be non-random. After studying accidents and incidents in the new, highly automated aircraft, Sarter and Woods have concluded that certain errors are predictable <ref> [SW95] </ref>: They are the regular and predictable consequences of a variety of identifiable factors. <p> This was not intended by the pilots who were not aware of the active "interface mode" and failed to detect the problem. As a consequence of the too steep descent, the airplane crashed into a mountain <ref> [SW95] </ref>. Several design constraints can assist in reducing interface interpretation errors. The first is that any mode used to control interpretation of the supervisory interface should be annunciated to the operator (that is, it should be part of the displays interface in our modeling language). <p> Sarter and Woods found that pilots failed to anticipate and realize that the autothrust system did not arm when they selected TOGA (take off/go around) power under these conditions because it did so under all other circumstances where TOGA power is applied <ref> [SW95] </ref>. Another example of inconsistent automation behavior, which was implicated in an A-320 accident, involves a protection function that is provided in all automation configurations except the altitude acquisition mode in which the autopilot was operating. <p> Even in more low pressure situations, consistency (or predictability) is important in light of the evidence from pilot surveys that their normal monitoring behavior may change on advanced flight decks <ref> [SW95] </ref>. Pilots on conventional aircraft use a highly trained instrument scanning pattern of recurrently sampling a given set of basic flight parameters. <p> A standard instrument scan, on the other hand, serves to ensure that all relevant parameters concerning airplane behavior will be monitored at certain time intervals to make sure that no unexpected and maybe undesirable changes occur <ref> [SW95] </ref>. In our previous design criteria and analysis tools, we include a check for nondeterminism in the software behavior, that is, we check to determine whether more than one transition can be taken out of a state under the same conditions [HL96]. <p> It has been speculated that the pilots did not notice the mode annunciation because the indirect mode change occurred during approach when the pilots were busy and they were not expecting the change <ref> [SW95] </ref>. Another example of such an indirect mode change in the A-320 automation involves an automatic mode transition triggered when the airspeed exceeds a predefined limit. <p> As a consequence of this sudden burst of power, the airplane pitched up to about 50 degrees, entered a sharp left bank, and went into a dive. The pilots eventually disengaged the autothrust system and its associated protection function and regained control of the aircraft <ref> [SW95] </ref>. Various design criteria are related to authority limits. For example, information about any modes or states where the operator input is ignored or limited must be provided in the supervisory interface. <p> Incomplete feedback is often implicated in accident scenarios. For example, in the A-320 Bangalore accident, the pilot flying (PF) had disengaged his flight director during the approach and was assuming that the pilot-not-flying (PNF) would do the same thing <ref> [SW95] </ref>. The result would have been a mode configuration in which airspeed is automatically controlled by the autothrottle (the speed mode), which is the recommended procedure for the approach phase.
References-found: 19

