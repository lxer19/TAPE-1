URL: ftp://synapse.cs.byu.edu/pub/papers/cgc_94c.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: AN EFFICIENT METRIC FOR HETEROGENEOUS INDUCTIVE LEARNING APPLICATIONS IN THE ATTRIBUTE-VALUE LANGUAGE 1  
Author: Christophe Giraud-Carrier and Tony Martinez 
Address: Provo, UT 84602  
Affiliation: Brigham Young University, Department of Computer Science,  
Abstract: Many inductive learning problems can be expressed in the classical attribute-value language. In order to learn and to generalize, learning systems often rely on some measure of similarity between their current knowledge base and new information. The attribute-value language defines a heterogeneous multidimensional input space, where some attributes are nominal and others linear. Defining similarity, or proximity, of two points in such input spaces is non trivial. We discuss two representative homogeneous metrics and show examples of why they are limited to their own domains. We then address the issues raised by the design of a heterogeneous metric for inductive learning systems. In particular, we discuss the need for normalization and the impact of don't-care values. We propose a heterogeneous metric and evaluate it empirically on a simplified version of ILA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aha, D.W. </author> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks. </title> <type> Technical Report, </type> <institution> University of California, Irvine, </institution> <year> 1991. </year>
Reference-contexts: However, this definition implicitly assumes that all attributes range over the same domain (e.g., reals, integers). If the domains are different, then some attribute distances may dominate others in the overall distance. For example, if x and y are linear attributes ranging over <ref> [0, 1] </ref> in the real numbers and [0, 100] in the natural numbers, respectively, then distances along y are likely to dominate distances along x. The smallest distance in y is equal to the largest distance in x. The problem is one of scale. <p> x and y be two n ensional linear vectors i i Let ndl x y range i Then NDL ndl x y i i i i n ( ) ) = - = x y 2 The division by range (i) causes all attribute distances to fall within the range <ref> [0, 1] </ref>. Hence, all attributes make a normalized contribution to NDL. Again, NDL conveys the intuitive idea that the farther away two vectors are, the less similar they are. So it can be used directly to choose a closest match. However, NDL is inadequate on nominal spaces. <p> Hence, it is not sufficient that each metric be adequate on its own space. They must be somehow normalized with respect to each other. The normalization in ndl causes ndl to have exactly the same range of values as dn, namely <ref> [0, 1] </ref>. Hence, not only is ndl a more accurate linear metric (for linear spaces), it is also "compatible" with dn. There is a possible problem with such a normalization however. Indeed, by normalizing, we artificially force complete mismatches (i.e., distance of 1) to the extrema. <p> Another impact of the variation in generality of different vectors and (1) is the need for some normalization of the overall distance, so as to distinguish a perfect match from a don't-care. Since attributewise distances range over <ref> [0, 1] </ref>, this can be done by dividing the overall distance by the number of asserted input attributes in the vector from which distance is computed. <p> Two modifications are made however. All continuous attributes are made discrete (using a K-means algorithm on each input), and all don't-know values are arbitrarily treated as don't-care values. Other (possibly more appropriate) ways of handling don't-know values are the topic of future research (three of them are discussed in <ref> [1] </ref>). Also, even though the attributes of the application lenses are all claimed to be nominal, the first one (i.e., age) appears to have a definite natural ordering to it. Hence, the dataset lenses is considered to be heterogeneous.
Reference: [2] <author> Aha, D.W., Kibler, D., and Albert, </author> <title> M.K. Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <year> 1991, </year> <pages> 37-66. </pages>
Reference-contexts: Generalization typically consists of either one (or a combination) of two processes: 1) the discovery of subsets of points (or regions) of the input space that map to a given concept [4, 5], and 2) the use of proximity (or similarity) to known points <ref> [2, 6, 7, 12] </ref>. When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points. <p> Section 3 discusses two issues raised by the design of a metric for heterogeneous inductive learning applications. Section 4 proposes a heterogeneous metric that extends the similarity function of <ref> [2] </ref>. Section 5 overviews a simplified version of ILA [6] that serves as a basis for empirical evaluation. Finally, Section 6 concludes the paper. 2. Considerations on the Notion of Similarity We consider here two types of attributes: nominal and linear. <p> Each kind of attribute gives rise to a distinct notion of similarity or distance. We discuss two of them here, and show that they are mutually incompatible. The selected metrics are in no wise unique, only representative. They serve as illustration, are commonly used <ref> [2, 5, 12] </ref>, and have been found to give good empirical results on their respective domains of applications. 2.1. D ISTANCE FOR N OMINAL S PACES For nominal data, the notion of how far apart two values are reduces to a simple binary relation. <p> DN' is the reciprocal of the conceptual similarity measure of [12]. DHET (Distance for HETerogeneous spaces) is an extension of IBL's similarity function <ref> [2] </ref>. The two metrics are equivalent when a purely instance-based learning algorithm is used, and DHET extends IBL's similarity function to inductive learning algorithms that use and/or create general rules. DHET is also similar to the distance metric used by NGE [10]. <p> The results were obtained by executing the same algorithm, only varying the metric it uses. ILA (Incremental Learning Algorithm) was chosen because of its simplicity, execution speed, and the direct impact the metric has over its accuracy. ILA bears resemblance with IBL algorithms <ref> [2] </ref> and NGE [10]. It can be viewed as a nearest-hyperplane algorithm. We first overview a simplified version of ILA. Details of the full algorithm are in [6]. We then present results of simulations on a variety of applications. 5.1.
Reference: [3] <author> Dasarathy, </author> <title> B.V. Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points. Classification by closeness is the essence of most nearest-neighbor algorithms (see <ref> [3] </ref> for a survey) and memory-based reasoning (see for example [11]). To find the "closest" known points to a new point in the input space, the system needs some measure of proximity or similarity, i.e., a distance metric defined on the input space.
Reference: [4] <author> Dietterich, T.G., and Michalski, </author> <title> R.S. A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In Michalski, R.S., Carbonell, J.G., and Mitchell, T.M., (Eds.). </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, CA, </address> <year> 1983, </year> <note> Chapter 3. </note>
Reference-contexts: Generalization typically consists of either one (or a combination) of two processes: 1) the discovery of subsets of points (or regions) of the input space that map to a given concept <ref> [4, 5] </ref>, and 2) the use of proximity (or similarity) to known points [2, 6, 7, 12]. When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points.
Reference: [5] <author> Giraud-Carrier, C., and Martinez, T.R. </author> <title> Using Precepts to Augment Training Set Learning. </title> <booktitle> In Proceedings of the 1993 International Conference on Artificial Neural Networks and Expert Systems (ANNES'93), </booktitle> <year> 1993, </year> <pages> 46-51. </pages>
Reference-contexts: Generalization typically consists of either one (or a combination) of two processes: 1) the discovery of subsets of points (or regions) of the input space that map to a given concept <ref> [4, 5] </ref>, and 2) the use of proximity (or similarity) to known points [2, 6, 7, 12]. When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points. <p> Each kind of attribute gives rise to a distinct notion of similarity or distance. We discuss two of them here, and show that they are mutually incompatible. The selected metrics are in no wise unique, only representative. They serve as illustration, are commonly used <ref> [2, 5, 12] </ref>, and have been found to give good empirical results on their respective domains of applications. 2.1. D ISTANCE FOR N OMINAL S PACES For nominal data, the notion of how far apart two values are reduces to a simple binary relation. <p> Indeed, such rules may also be used as an encoding of prior knowledge, or as a particular instantiation of domain knowledge or commonsense. They then serve as learning biases for the system (see, for example <ref> [5] </ref>). For example, when learning about the flying abilities of birds, one may bias the system with a rule encoding the fact that birds typically fly. The system then only needs to focus on exceptions. 3.2.2. Effects of Don't-Cares.
Reference: [6] <author> Giraud-Carrier, C., and Martinez, T.R. ILA: </author> <title> Combining Inductive Learning with Prior Knowledge and Reasoning. </title> <note> Submitted. </note>
Reference-contexts: Generalization typically consists of either one (or a combination) of two processes: 1) the discovery of subsets of points (or regions) of the input space that map to a given concept [4, 5], and 2) the use of proximity (or similarity) to known points <ref> [2, 6, 7, 12] </ref>. When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points. <p> Section 3 discusses two issues raised by the design of a metric for heterogeneous inductive learning applications. Section 4 proposes a heterogeneous metric that extends the similarity function of [2]. Section 5 overviews a simplified version of ILA <ref> [6] </ref> that serves as a basis for empirical evaluation. Finally, Section 6 concludes the paper. 2. Considerations on the Notion of Similarity We consider here two types of attributes: nominal and linear. <p> ILA bears resemblance with IBL algorithms [2] and NGE [10]. It can be viewed as a nearest-hyperplane algorithm. We first overview a simplified version of ILA. Details of the full algorithm are in <ref> [6] </ref>. We then present results of simulations on a variety of applications. 5.1. ILA - A LGORITHMIC O VERVIEW ILA's representation language is the classical attribute-value language, with the addition of the special don't-care symbol. For convenience, examples and rules are viewed as input-output pairs.
Reference: [7] <author> Kibler, D., and Aha, D.W. </author> <title> Learning Representative Exemplars of Concepts: An Initial Case Study. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <year> 1987, </year> <pages> 24-30. </pages>
Reference-contexts: Generalization typically consists of either one (or a combination) of two processes: 1) the discovery of subsets of points (or regions) of the input space that map to a given concept [4, 5], and 2) the use of proximity (or similarity) to known points <ref> [2, 6, 7, 12] </ref>. When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points.
Reference: [8] <author> Michalski, </author> <title> R.S. A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <year> 1983, </year> <pages> 111-161. </pages>
Reference-contexts: Some of these critical features, or set of attribute-value pairs that are good predictors of the output, may be represented as rules. Such rules are the result of the learning system's ability to generalize. A commonly used generalization rule in learning systems is the dropping-the-condition rule <ref> [8] </ref>. In the attribute-value language, it translates into the setting of one (or more) of the input attributes of a given vector to don't-care. The input attributes that remain asserted can then be viewed as critical features (those that are don't-care do not affect the output value). <p> For convenience, examples and rules are viewed as input-output pairs. Examples have all of their input attributes asserted to some value. ILA discovers new rules by generalizing from examples. The generalization rule it uses is the classical drop the condition rule <ref> [8] </ref>, which causes the value of an input attribute to be set to don't-care. ILA implements a network of simple nodes that adapts to newly acquired knowledge by dynamically adding and/or removing nodes in the network. The network's structure is a balanced binary tree.
Reference: [9] <author> Murphy, P.M., and Aha, D.W. </author> <title> UCI Repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: The output of the winning node is the system's output and prediction of the new pair's output. 5.2. S IMULATION R ESULTS A variety of datasets from the Irvine repository of machine learning datasets <ref> [9] </ref> were chosen, representing a mixture of homogeneous and heterogeneous applications. Two modifications are made however. All continuous attributes are made discrete (using a K-means algorithm on each input), and all don't-know values are arbitrarily treated as don't-care values.
Reference: [10] <author> Salzberg, S. </author> <title> A Nearest Hyperrectangle Learning Method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <year> 1991, </year> <pages> 277-309. </pages>
Reference-contexts: So it can be used directly to choose a closest match. However, DN is inadequate on linear spaces. Consider the following example, where each attribute is linear and ranges over <ref> [0, 10] </ref> in the natural numbers. x z DN x z = fi = ( , , ) ( , , ) ( , ) 3 1 4 2 Though DN (x,y)=DN (x,z), it appears that (given linear attributes) y is closer to x than z is. <p> Consider the following example, where the first attribute is nominal and ranges over the discrete set -0, 1, 2, 3, 4, 5-, and the other two attributes are linear and range over <ref> [0, 10] </ref> in the natural numbers. x z NDL x z DN x z = fi = = ( , , ) ( , , ) ( , ) ( , ) 4 7 0 6 3 0 2 6 The two distances naturally give inconsistent results. <p> The two metrics are equivalent when a purely instance-based learning algorithm is used, and DHET extends IBL's similarity function to inductive learning algorithms that use and/or create general rules. DHET is also similar to the distance metric used by NGE <ref> [10] </ref>. It is somewhat simpler since rules containing don't-cares represent hyperplanes rather than hyperrectangles. It is also inherently heterogeneous whereas NGE's distance is designed for purely continuous domains. Extensions to heterogeneous spaces have been proposed (see for example [13]). <p> Extensions to heterogeneous spaces have been proposed (see for example [13]). NGE's distance is also a weighted sum, where each attributewise distance is assigned its own weight in the computation of the final distance. Mechanisms to assign weights to attributes in distance computation may be found in <ref> [10, 11, 13] </ref>. DHET could be similarly extended. 5. Evaluation In this section, we give empirical evidence of the adequacy of DHET. The results were obtained by executing the same algorithm, only varying the metric it uses. <p> The results were obtained by executing the same algorithm, only varying the metric it uses. ILA (Incremental Learning Algorithm) was chosen because of its simplicity, execution speed, and the direct impact the metric has over its accuracy. ILA bears resemblance with IBL algorithms [2] and NGE <ref> [10] </ref>. It can be viewed as a nearest-hyperplane algorithm. We first overview a simplified version of ILA. Details of the full algorithm are in [6]. We then present results of simulations on a variety of applications. 5.1.
Reference: [11] <author> Stanfill, C., and Waltz, D. </author> <title> Toward Memory-Based Reasoning. </title> <journal> Communications of the ACM, </journal> <volume> Vol. 29, No. 12, </volume> <month> December </month> <year> 1986, </year> <pages> 1213-1228. </pages>
Reference-contexts: Classification by closeness is the essence of most nearest-neighbor algorithms (see [3] for a survey) and memory-based reasoning (see for example <ref> [11] </ref>). To find the "closest" known points to a new point in the input space, the system needs some measure of proximity or similarity, i.e., a distance metric defined on the input space. <p> Extensions to heterogeneous spaces have been proposed (see for example [13]). NGE's distance is also a weighted sum, where each attributewise distance is assigned its own weight in the computation of the final distance. Mechanisms to assign weights to attributes in distance computation may be found in <ref> [10, 11, 13] </ref>. DHET could be similarly extended. 5. Evaluation In this section, we give empirical evidence of the adequacy of DHET. The results were obtained by executing the same algorithm, only varying the metric it uses.
Reference: [12] <author> Sun, R. </author> <title> A Connectionist Model for Commonsense Reasoning Incorporating Rules and Similarities. </title> <journal> Knowledge Acquisition, </journal> <volume> 4, </volume> <year> 1992, </year> <pages> 293-321. </pages>
Reference-contexts: Generalization typically consists of either one (or a combination) of two processes: 1) the discovery of subsets of points (or regions) of the input space that map to a given concept [4, 5], and 2) the use of proximity (or similarity) to known points <ref> [2, 6, 7, 12] </ref>. When a new point is shown, its classification is determined either by its belonging to one of the identified regions, or by the classification of the "closest" known points. <p> Each kind of attribute gives rise to a distinct notion of similarity or distance. We discuss two of them here, and show that they are mutually incompatible. The selected metrics are in no wise unique, only representative. They serve as illustration, are commonly used <ref> [2, 5, 12] </ref>, and have been found to give good empirical results on their respective domains of applications. 2.1. D ISTANCE FOR N OMINAL S PACES For nominal data, the notion of how far apart two values are reduces to a simple binary relation. <p> DN' is the reciprocal of the conceptual similarity measure of <ref> [12] </ref>. DHET (Distance for HETerogeneous spaces) is an extension of IBL's similarity function [2]. The two metrics are equivalent when a purely instance-based learning algorithm is used, and DHET extends IBL's similarity function to inductive learning algorithms that use and/or create general rules.
Reference: [13] <author> Wettschereck, D., and Dietterich, T.G. </author> <title> An Experimental Comparison of the Nearest-Neighbor and Nearest-Hyperrectangle Algorithms. </title> <note> To appear in Machine Learning. </note>
Reference-contexts: DHET is also similar to the distance metric used by NGE [10]. It is somewhat simpler since rules containing don't-cares represent hyperplanes rather than hyperrectangles. It is also inherently heterogeneous whereas NGE's distance is designed for purely continuous domains. Extensions to heterogeneous spaces have been proposed (see for example <ref> [13] </ref>). NGE's distance is also a weighted sum, where each attributewise distance is assigned its own weight in the computation of the final distance. Mechanisms to assign weights to attributes in distance computation may be found in [10, 11, 13]. DHET could be similarly extended. 5. <p> Extensions to heterogeneous spaces have been proposed (see for example [13]). NGE's distance is also a weighted sum, where each attributewise distance is assigned its own weight in the computation of the final distance. Mechanisms to assign weights to attributes in distance computation may be found in <ref> [10, 11, 13] </ref>. DHET could be similarly extended. 5. Evaluation In this section, we give empirical evidence of the adequacy of DHET. The results were obtained by executing the same algorithm, only varying the metric it uses.
References-found: 13

