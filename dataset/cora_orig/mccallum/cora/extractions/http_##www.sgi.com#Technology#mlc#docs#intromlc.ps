URL: http://www.sgi.com/Technology/mlc/docs/intromlc.ps
Refering-URL: http://www.sgi.com/Technology/mlc/docs.html
Root-URL: 
Email: mlc@CS.Stanford.EDU  
Title: MLC A Machine Learning Library in C  
Author: Ron Kohavi George John Richard Long David Manley Karl Pfleger 
Date: August 23, 1994  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: We present MLC ++ , a library of C ++ classes and tools for supervised Machine Learning. While MLC ++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC ++ is an attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC ++ aims to solve, the design of MLC ++ , and the current functionality. 
Abstract-found: 1
Intro-found: 1
Reference: [Aha92] <author> David W. Aha. </author> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36(1) </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: In the future we will provide mappers, a generalization of categorizers where the output is a real number instead of a discrete category label. Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer <ref> [Aha92, AKA91] </ref>, an ID3-like decision tree inducer [Qui86], HOODG for inducing oblivious read-once decision graphs [Koh94b, Koh94a]. learning and plays an important role in the overall decomposition of the MLC ++ library. 4.2 Overview of MLC ++ classes This section provides a more detailed description of the MLC ++ classes. <p> The OODG (Oblivious, read-Once Decision Graph) inducer uses a bottom-up method for building levelled rooted decision graphs [Koh94a, Koh94b]. The HOODG inducer is a subclass of the OODG inducer that provides a hill-climbing implementation of the 7 OODG algorithm. A nearest neighbor inducer uses the IB algorithms described in <ref> [Aha92] </ref>. Test result Test result is a class that provides information about the performance of a categorizer on a test set. It provides the number of instances correctly and incorrectly categorized, the accuracy, and the confusion matrix.
Reference: [AKA91] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: In the future we will provide mappers, a generalization of categorizers where the output is a real number instead of a discrete category label. Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer <ref> [Aha92, AKA91] </ref>, an ID3-like decision tree inducer [Qui86], HOODG for inducing oblivious read-once decision graphs [Koh94b, Koh94a]. learning and plays an important role in the overall decomposition of the MLC ++ library. 4.2 Overview of MLC ++ classes This section provides a more detailed description of the MLC ++ classes.
Reference: [Ang92] <author> Dana Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 351-369. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction In supervised machine learning <ref> [SL92, Ang92, Val84, AS83] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task. <p> In Section 5 we show several examples of the output of the library. In Section 6, we discuss plans for extensions and future work, and conclude with a summary. 2 Problems with the Current Methodology Despite progress on the theoretical front, mostly related to Valiant's PAC model (cf. <ref> [Ang92] </ref> for a survey), most supervised learning algorithms are too complex for formal analysis, and empirical studies retain a central role. Although many experimental results have appeared (and continue to appear), the field seems to be in a state of disarray.
Reference: [AS83] <author> Dana Angluin and Carl H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction In supervised machine learning <ref> [SL92, Ang92, Val84, AS83] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [BaGH93] <author> Pavel Brazdil, Jo ao Gama, and Bob Henery. </author> <title> Comparisons of ML and statistical approaches using meta level learning. </title> <booktitle> In Proceedings of the ECML93 workshop on Real-World Applications of Machine Learning. </booktitle> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: Data generator The data generator should be extended to allow generating non-discrete data, to incorporate different noise models, and varying distributions. Hybrid approaches Hybrid algorithms of different kinds can be tested. One of the more intriguing things we wish to do is meta-level learning <ref> [BGH94, BaGH93] </ref>, i.e., to have a machine learning algorithm induce rules that will define (empirically) when an algorithm is applicable, or which algorithm should be used for a given dataset, and with what parameterization.
Reference: [BFOS84] <author> Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Display of information In order to understand the problem better, and perhaps bias a learning algorithm, it is helpful to view the resulting structures (e.g., decision tree), or the data. Even commercially available programs such as C4.5 [Qui92] and CART <ref> [BFOS84] </ref> give only a rudimentary display of an induced tree. A library providing a common basis would alleviate the pain involved in programming from scratch. Since researchers will need to write less code, they can write higher quality code and do it in shorter time. <p> First, Consultant is a tool for choosing an algorithm, and not a tool for creating variants of algorithms or testing new ones. Second, MLC ++ will be able to suggest an appropriate algorithm to a user, but by employing more general and direct methods such as cross-validation <ref> [BFOS84, WK91] </ref> to estimate the accuracy of the algorithms using only the user-provided training data.
Reference: [BGH94] <author> Pavel Brazdil, Joao Gama, and Bob Henery. </author> <title> Characterizing the applicability of classification algorithms using meta-level learning. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Data generator The data generator should be extended to allow generating non-discrete data, to incorporate different noise models, and varying distributions. Hybrid approaches Hybrid algorithms of different kinds can be tested. One of the more intriguing things we wish to do is meta-level learning <ref> [BGH94, BaGH93] </ref>, i.e., to have a machine learning algorithm induce rules that will define (empirically) when an algorithm is applicable, or which algorithm should be used for a given dataset, and with what parameterization.
Reference: [Bro86] <author> Frederick P. Brooks. </author> <title> No silver bullets. </title> <editor> In H. J. Kugler, editor, </editor> <booktitle> Information Processing. </booktitle> <publisher> Elsevier Science Publishers, North Holland, </publisher> <year> 1986. </year> <note> Reprinted in Unix Review November 1987. 17 </note>
Reference-contexts: We believe that only through projects like MLC ++ and [TMS94] can we reach such a goal. 7 Summary Frederick Brooks in his famous paper "No Silver Bullet" <ref> [Bro86] </ref> wrote The most radical possible solution for constructing software is not to construct it at all. : : : The key issue, of course, is applicability.
Reference: [BU92] <author> Carla E. Brodley and Paul Utgoff. </author> <title> Multivariate decision trees. </title> <type> Technical Report MASSCS 92-93, </type> <institution> University of Massachusetts, Amherst, </institution> <year> 1992. </year>
Reference-contexts: Generate performance statistics such as accuracy, learning rates, and confusion matri ces. 3. Compare algorithms on different datasets (e.g., compare algorithms A; B; C on datasets X; Y; Z). 4. Experiment with hybrid algorithms (e.g., perceptron trees [Utg88], multivariate trees <ref> [BU92] </ref> and entropy nets [KP88]) and hierarchical structures. 1 5. Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known). A useful tool for this purpose is Michalski's General Logic Diagrams [WM94, WSWM90, Mic78]. <p> ID3 [Qui86] always uses attribute categorizers for nominal attributes and threshold categorizers for real attributes. To generate multivariate trees <ref> [BU92, HKS93] </ref> with perceptrons at nodes, the induction algorithm can put perceptron categorizers at the nodes. In the future we will provide mappers, a generalization of categorizers where the output is a real number instead of a discrete category label. Induction algorithms Induction algorithms induce categorizers.
Reference: [Bun90] <author> Wray Buntine. </author> <title> Myths and legends in learning classification rules. </title> <booktitle> In Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 736-742. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: already recognized that learning can be viewed as a type of search when he wrote "The learning process may be regarded as a search for a form of behavior which will satisfy the teacher." While many researchers noted that learning can be viewed as search through the space of hypotheses <ref> [SL92, Bun90, Mit82] </ref>, most algorithms couple the search and the bias of the algorithm. 15 ID3 [Qui86] and many variants are searching for a small tree, guided by the heuristic that splits lowering the entropy should yield small trees.
Reference: [Cog90] <author> James Coggins. </author> <title> Designing C++ libraries. </title> <journal> The C++ Journal, </journal> <volume> 1(1) </volume> <pages> 25-32, </pages> <year> 1990. </year>
Reference-contexts: Algorithms and input/output routines are coded over and over, without using existing code. To quote James Coggins <ref> [Cog90] </ref>, who designed the C ++ COOL library: Newton said he saw farther because he stood on the shoulders of giants. Computer programmers stand on each other's toes.
Reference: [Efr82] <author> Bradley Efron. </author> <title> The jackknife, the bootstrap, and other resampling plans. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1982. </year>
Reference-contexts: Wrappers Given a learning algorithm, we wish to provide more wrappers to evaluate the performance. Variants on cross-validation such as stratification can be useful. Similarly, other methods of estimating accuracy, such as Efron's Bootstrap <ref> [ET93, Efr82] </ref>, are possible. Given different induction algorithms, we would like to add significance tests to test whether an improvement is non-negligible. Comparing on which instances the predictions differ is also important, and the GLDs provide one way of comparing predictions.
Reference: [ET93] <author> Brad Efron and Rob Tibshirani. </author> <title> An introduction to the bootstrap. </title> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference-contexts: Wrappers Given a learning algorithm, we wish to provide more wrappers to evaluate the performance. Variants on cross-validation such as stratification can be useful. Similarly, other methods of estimating accuracy, such as Efron's Bootstrap <ref> [ET93, Efr82] </ref>, are possible. Given different induction algorithms, we would like to add significance tests to test whether an improvement is non-negligible. Comparing on which instances the predictions differ is also important, and the GLDs provide one way of comparing predictions.
Reference: [GKNV93] <author> E. R. Gansner, E. Koutsofios, S. C. North, and K. P. Vo. </author> <title> A technique for drawing directed graphs. </title> <journal> In IEEE Transactions on Software Engineering, </journal> <pages> pages 214-230, </pages> <year> 1993. </year>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher [Nae92], and dot from AT&T <ref> [GKNV93, KN] </ref>. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files. <p> Decision trees and decision graphs are excellent examples of interpretable structures, and MLC ++ interfaces the excellent graph-drawing programs dot and dotty provided by AT&T <ref> [GKNV93, KN] </ref>. Examples of induced structures are shown in Figures 2 and 3. For viewing datasets and induced concepts, we have implemented General Logic Diagrams. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD.
Reference: [HKS93] <author> David Heath, Simon Kasif, and Steven Salzberg. </author> <title> Induction of oblique decision trees. </title> <booktitle> In 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1002-1007. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: ID3 [Qui86] always uses attribute categorizers for nominal attributes and threshold categorizers for real attributes. To generate multivariate trees <ref> [BU92, HKS93] </ref> with perceptrons at nodes, the induction algorithm can put perceptron categorizers at the nodes. In the future we will provide mappers, a generalization of categorizers where the output is a real number instead of a discrete category label. Induction algorithms Induction algorithms induce categorizers.
Reference: [Hol93] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: Comparisons on different datasets Results are given on different datasets, and cannot be compared. The Irvine repository [MA94] partially solves this problem, as it supplies a set of "standard" datasets, but most files do not have test sets, and researchers differ on their evaluation of algorithms. Holte <ref> [Hol93, Appendix C] </ref> collected accuracy results from many papers and experimented on his own, but as he writes, these results should be taken with a grain of salt: The results included in this survey were produced under a very wide variety of experimental conditions, and therefore it is impossible to compare
Reference: [JKP94] <author> George John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: However, we feel that our effort should be concentrated on the core functions as much as possible. The additions to the core functions that we have in mind include: Feature manipulation Functions for discovering irrelevant attributes and ranking the relative importance of attributes <ref> [KR92, JKP94] </ref>, functions to help in constructing new features [PH90], and discretizing real values into ranges. <p> We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already 16 aided some of us in our research <ref> [Koh94a, Koh94b, JKP94, Koh94c] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [KL88] <author> Dennis Kibler and Pat Langley. </author> <title> Machine learning as an experimental science. </title> <booktitle> In Proceedings of the Third European Working Session on Learning. </booktitle> <publisher> Pitman Publishing Ltd., London and Morgan Kaufman Publishing, </publisher> <address> San Mateo, CA., </address> <year> 1988. </year> <note> Also appears in Readings in Machine Learning by Shavlik and Dietterich. </note>
Reference-contexts: There are too many algorithms and variations of algorithms, each claiming to do better on a few datasets. We believe the field has reached a stage of maturity that calls for more systematic experimentation and analysis. Some of the main problems that exist are described below (see <ref> [LK91, KL88] </ref> for further discussions): Comparisons with other algorithms Results are usually given in isolation. When a learning algorithm is presented, comparison is usually limited to a trivial straw algorithm, as opposed to state-of-the-art algorithms.
Reference: [KN] <author> Eleftherios Koutsofios and Stephen C. </author> <title> North. Drawing graphs with dot. </title> <note> Available by anonymous ftp from research.att.com:dist/drawdag/dotdoc.ps.Z. </note>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher [Nae92], and dot from AT&T <ref> [GKNV93, KN] </ref>. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files. <p> Decision trees and decision graphs are excellent examples of interpretable structures, and MLC ++ interfaces the excellent graph-drawing programs dot and dotty provided by AT&T <ref> [GKNV93, KN] </ref>. Examples of induced structures are shown in Figures 2 and 3. For viewing datasets and induced concepts, we have implemented General Logic Diagrams. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD.
Reference: [Koh94a] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious, read-once decision graphs. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <month> April </month> <year> 1994. </year> <note> Paper available by anonymous ftp from starry.Stanford.EDU:pub/ronnyk/euroML94.ps. 18 </note>
Reference-contexts: Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer [Aha92, AKA91], an ID3-like decision tree inducer [Qui86], HOODG for inducing oblivious read-once decision graphs <ref> [Koh94b, Koh94a] </ref>. learning and plays an important role in the overall decomposition of the MLC ++ library. 4.2 Overview of MLC ++ classes This section provides a more detailed description of the MLC ++ classes. <p> ID3 inducer is a subclass of the top-down decision-tree inducer that uses the ID3 algorithm to induce the decision tree [Qui86]. The OODG (Oblivious, read-Once Decision Graph) inducer uses a bottom-up method for building levelled rooted decision graphs <ref> [Koh94a, Koh94b] </ref>. The HOODG inducer is a subclass of the OODG inducer that provides a hill-climbing implementation of the 7 OODG algorithm. A nearest neighbor inducer uses the IB algorithms described in [Aha92]. <p> We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already 16 aided some of us in our research <ref> [Koh94a, Koh94b, JKP94, Koh94c] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [Koh94b] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious, read-once decision graphs : Strengths and limitations. </title> <booktitle> In Twelfth National Conference on Artificial Intelligence, </booktitle> <year> 1994. </year> <note> Paper available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference-contexts: Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer [Aha92, AKA91], an ID3-like decision tree inducer [Qui86], HOODG for inducing oblivious read-once decision graphs <ref> [Koh94b, Koh94a] </ref>. learning and plays an important role in the overall decomposition of the MLC ++ library. 4.2 Overview of MLC ++ classes This section provides a more detailed description of the MLC ++ classes. <p> ID3 inducer is a subclass of the top-down decision-tree inducer that uses the ID3 algorithm to induce the decision tree [Qui86]. The OODG (Oblivious, read-Once Decision Graph) inducer uses a bottom-up method for building levelled rooted decision graphs <ref> [Koh94a, Koh94b] </ref>. The HOODG inducer is a subclass of the OODG inducer that provides a hill-climbing implementation of the 7 OODG algorithm. A nearest neighbor inducer uses the IB algorithms described in [Aha92]. <p> We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already 16 aided some of us in our research <ref> [Koh94a, Koh94b, JKP94, Koh94c] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [Koh94c] <author> Ron Kohavi. </author> <title> Feature subset selection as search with probabilistic estimates. </title> <booktitle> In AAAI Fall Symposium on Relevance, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: We described MLC ++ , our attempt at building such a tool. Much work has already been done on MLC ++ , and it has already 16 aided some of us in our research <ref> [Koh94a, Koh94b, JKP94, Koh94c] </ref>. We trust that other researchers will also enjoy productivity gains when using MLC ++ , and will contribute to this effort. Acknowledgments The MLC ++ project is partly funded by ONR grant N00014-94-1-0448 and NSF grant IRI-9116399.
Reference: [KP88] <author> C. Koutsougeras and C. A. Papachristou. </author> <title> Training of a neural network for pattern classification based on an entropy measure. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <pages> pages 247-254. </pages> <publisher> IEEE, IEEE Press, </publisher> <year> 1988. </year>
Reference-contexts: Generate performance statistics such as accuracy, learning rates, and confusion matri ces. 3. Compare algorithms on different datasets (e.g., compare algorithms A; B; C on datasets X; Y; Z). 4. Experiment with hybrid algorithms (e.g., perceptron trees [Utg88], multivariate trees [BU92] and entropy nets <ref> [KP88] </ref>) and hierarchical structures. 1 5. Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known). A useful tool for this purpose is Michalski's General Logic Diagrams [WM94, WSWM90, Mic78].
Reference: [KR92] <author> Kenji Kira and Larry A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 129-134. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: However, we feel that our effort should be concentrated on the core functions as much as possible. The additions to the core functions that we have in mind include: Feature manipulation Functions for discovering irrelevant attributes and ranking the relative importance of attributes <ref> [KR92, JKP94] </ref>, functions to help in constructing new features [PH90], and discretizing real values into ranges.
Reference: [LK91] <author> Pat Langley and Dennis Kibler. </author> <title> The experimental study of machine learning. </title> <type> Unpublished paper, </type> <year> 1991. </year>
Reference-contexts: There are too many algorithms and variations of algorithms, each claiming to do better on a few datasets. We believe the field has reached a stage of maturity that calls for more systematic experimentation and analysis. Some of the main problems that exist are described below (see <ref> [LK91, KL88] </ref> for further discussions): Comparisons with other algorithms Results are usually given in isolation. When a learning algorithm is presented, comparison is usually limited to a trivial straw algorithm, as opposed to state-of-the-art algorithms.
Reference: [MA94] <author> Patrick M. Murphy and David W. Aha. </author> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu, </title> <year> 1994. </year>
Reference-contexts: Since authors do not use a standard data file format, even those programs which are publicly available often cannot be easily compared on the same datasets. For example, the Irvine repository <ref> [MA94] </ref> does not provide a mechanism for parsing its data files. Comparisons on different datasets Results are given on different datasets, and cannot be compared. The Irvine repository [MA94] partially solves this problem, as it supplies a set of "standard" datasets, but most files do not have test sets, and researchers <p> For example, the Irvine repository <ref> [MA94] </ref> does not provide a mechanism for parsing its data files. Comparisons on different datasets Results are given on different datasets, and cannot be compared. The Irvine repository [MA94] partially solves this problem, as it supplies a set of "standard" datasets, but most files do not have test sets, and researchers differ on their evaluation of algorithms. <p> previous projects addressing these concerns, and attempt to identify how MLC ++ either differs or perhaps solves the same problem in a more general way. 3 3.1 Data Collections An extensive collection of over 80 datasets has been collected by Murphy and Aha at the University of California at Irvine <ref> [MA94] </ref>. We do not intend to duplicate any of this effort; in fact, we use their data formats as much as possible. There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer [Mey]. <p> Sets of Instances Sets (actually multi-sets, or bags) of instances are used for reading, storing, and manipulating instances (such as from a training set or a test set). MLC ++ recognizes files in Quinlan's format [Qui92], which shares the same data file format with the Irvine repository <ref> [MA94] </ref>. Sets can be converted to other formats as well as other encodings (e.g., binary, local). This class provides functions for inserting and deleting instances as well as iterating through all instances in the set.
Reference: [Mey] <author> Mike Meyer. Statlib. </author> <note> Available at lib.stat.cmu.edu. </note>
Reference-contexts: We do not intend to duplicate any of this effort; in fact, we use their data formats as much as possible. There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer <ref> [Mey] </ref>. Not all datasets are for supervised learning|some are intended for clustering or exploratory data analysis. 3.2 Learning Algorithm Collections Mooney [Moo91] has re-implemented many learning algorithms in lisp for the purpose of teaching students about machine learning.
Reference: [Mic78] <author> Ryszard S. Michalski. </author> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions. </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign, </institution> <year> 1978. </year>
Reference-contexts: Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known). A useful tool for this purpose is Michalski's General Logic Diagrams <ref> [WM94, WSWM90, Mic78] </ref>. In the next section, we identify the main problems with the current experimental methodology in the machine learning community. In Section 3, we cover related work. Section 4 is an overview of the MLC ++ objectives, design issues, and implementation. <p> The data can then be plotted using any plot package, such as gnuplot or Mathematica. GLD The GLD class creates General Logic Diagrams, which are graphical projections of multi-dimensional discrete spaces onto two dimensions. They are similar to Karnaugh maps, but are generalized to non Boolean inputs and outputs <ref> [WM94, WSWM90, Mic78] </ref>. A GLD may be generated for a test set, the induced categorizer's predictions for the entire instance space, and an overlay, showing the errors. GLDs can be displayed in X windows, Postscript, and xfig, the X windows drawing editor. <p> Examples of induced structures are shown in Figures 2 and 3. For viewing datasets and induced concepts, we have implemented General Logic Diagrams. After running an induction algorithm, users may gain insight about the induced concept by inspecting the GLD. High dimensional concepts are hard to visualize; Michalski <ref> [Mic78] </ref> introduced General Logic Diagrams as a visualization technique to help understand concepts. As well as a tool for researchers to gain insights into their algorithms, GLDs are a tool for users to test different algorithms on their own data to see which learning algorithm actually induces sensible-looking concepts.
Reference: [Mit82] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: already recognized that learning can be viewed as a type of search when he wrote "The learning process may be regarded as a search for a form of behavior which will satisfy the teacher." While many researchers noted that learning can be viewed as search through the space of hypotheses <ref> [SL92, Bun90, Mit82] </ref>, most algorithms couple the search and the bias of the algorithm. 15 ID3 [Qui86] and many variants are searching for a small tree, guided by the heuristic that splits lowering the entropy should yield small trees.
Reference: [Moo91] <author> Raymond J. Mooney. </author> <note> Ml-code machine learning archive. Available by anonymous ftp in cs.utexas.edu:/pub/mooney, </note> <year> 1991. </year>
Reference-contexts: There is also a large repository of data used by statisticians in the StatLib archive, created and maintained by Meyer [Mey]. Not all datasets are for supervised learning|some are intended for clustering or exploratory data analysis. 3.2 Learning Algorithm Collections Mooney <ref> [Moo91] </ref> has re-implemented many learning algorithms in lisp for the purpose of teaching students about machine learning. This is a nice collection of easy-to-read implementations of common machine learning algorithms.
Reference: [Nae92] <author> Stefan Naeher. LEDA: </author> <title> A Library of Efficient Data Types and Algorithms. </title> <institution> Max-Planck-Institut fuer Informatik, </institution> <address> IM Stadtwald, D-66123 Saarbruecken, FRG, 3.0 edition, </address> <year> 1992. </year> <note> Available by anonymous ftp in ftp.cs.uni-sb.de:LEDA. </note>
Reference-contexts: We attempt to use as much educational and public domain software as possible for this part of MLC ++ . For example, the graph manipulations are done using LEDA (Library of Efficient Data Structures) written by Stefan Naher <ref> [Nae92] </ref>, and dot from AT&T [GKNV93, KN]. Core classes These are the basic tools that are shared by many algorithms in supervised machine learning. They further divide into three types of functionality: Input/Output Classes for reading and writing data files.
Reference: [PH90] <author> Giulia Pagallo and David Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: The additions to the core functions that we have in mind include: Feature manipulation Functions for discovering irrelevant attributes and ranking the relative importance of attributes [KR92, JKP94], functions to help in constructing new features <ref> [PH90] </ref>, and discretizing real values into ranges.
Reference: [PK87] <author> Judea Pearl and Richard E. Korf. </author> <title> Search techniques. </title> <booktitle> Annual Review of Computer Science, </booktitle> <volume> 2 </volume> <pages> 451-467, </pages> <year> 1987. </year> <month> 19 </month>
Reference-contexts: One problem is that since there is no backtracking, decisions cannot be changed (although they can be undone by pruning). By abstracting away the search problem, we can use search algorithms such as beam search and IDA* <ref> [PK87] </ref> to systematically find better trees. Wrappers Given a learning algorithm, we wish to provide more wrappers to evaluate the performance. Variants on cross-validation such as stratification can be useful. Similarly, other methods of estimating accuracy, such as Efron's Bootstrap [ET93, Efr82], are possible.
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: Categorizers are built recursively; for example, in a decision tree categorizer, the branching nodes are categorizers themselves (mapping the set of instances into the set of children of that node), and the induction algorithm can use any categorizer, including the possibility of recursive decision trees. ID3 <ref> [Qui86] </ref> always uses attribute categorizers for nominal attributes and threshold categorizers for real attributes. To generate multivariate trees [BU92, HKS93] with perceptrons at nodes, the induction algorithm can put perceptron categorizers at the nodes. <p> In the future we will provide mappers, a generalization of categorizers where the output is a real number instead of a discrete category label. Induction algorithms Induction algorithms induce categorizers. MLC ++ currently provides a majority inducer, a nearest-neighbor inducer [Aha92, AKA91], an ID3-like decision tree inducer <ref> [Qui86] </ref>, HOODG for inducing oblivious read-once decision graphs [Koh94b, Koh94a]. learning and plays an important role in the overall decomposition of the MLC ++ library. 4.2 Overview of MLC ++ classes This section provides a more detailed description of the MLC ++ classes. <p> ID3 inducer is a subclass of the top-down decision-tree inducer that uses the ID3 algorithm to induce the decision tree <ref> [Qui86] </ref>. The OODG (Oblivious, read-Once Decision Graph) inducer uses a bottom-up method for building levelled rooted decision graphs [Koh94a, Koh94b]. The HOODG inducer is a subclass of the OODG inducer that provides a hill-climbing implementation of the 7 OODG algorithm. <p> learning process may be regarded as a search for a form of behavior which will satisfy the teacher." While many researchers noted that learning can be viewed as search through the space of hypotheses [SL92, Bun90, Mit82], most algorithms couple the search and the bias of the algorithm. 15 ID3 <ref> [Qui86] </ref> and many variants are searching for a small tree, guided by the heuristic that splits lowering the entropy should yield small trees. One problem is that since there is no backtracking, decisions cannot be changed (although they can be undone by pruning).
Reference: [Qui92] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1992. </year>
Reference-contexts: Such hybrid systems are time con suming to program from scratch. Display of information In order to understand the problem better, and perhaps bias a learning algorithm, it is helpful to view the resulting structures (e.g., decision tree), or the data. Even commercially available programs such as C4.5 <ref> [Qui92] </ref> and CART [BFOS84] give only a rudimentary display of an induced tree. A library providing a common basis would alleviate the pain involved in programming from scratch. Since researchers will need to write less code, they can write higher quality code and do it in shorter time. <p> Sets of Instances Sets (actually multi-sets, or bags) of instances are used for reading, storing, and manipulating instances (such as from a training set or a test set). MLC ++ recognizes files in Quinlan's format <ref> [Qui92] </ref>, which shares the same data file format with the Irvine repository [MA94]. Sets can be converted to other formats as well as other encodings (e.g., binary, local). This class provides functions for inserting and deleting instances as well as iterating through all instances in the set.
Reference: [SL92] <author> Jeffrey C. Schlimmer and Pat Langley. </author> <title> Learning, Machine. </title> <editor> In Stuart Shapiro and David Eckroth, editors, </editor> <booktitle> The Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 785-805. </pages> <publisher> Wiley-Interscience, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction In supervised machine learning <ref> [SL92, Ang92, Val84, AS83] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task. <p> already recognized that learning can be viewed as a type of search when he wrote "The learning process may be regarded as a search for a form of behavior which will satisfy the teacher." While many researchers noted that learning can be viewed as search through the space of hypotheses <ref> [SL92, Bun90, Mit82] </ref>, most algorithms couple the search and the bias of the algorithm. 15 ID3 [Qui86] and many variants are searching for a small tree, guided by the heuristic that splits lowering the entropy should yield small trees.
Reference: [Sle] <author> Derek Sleeman. </author> <title> The role of CONSULTANT in helping domain experts use machine learning. </title> <booktitle> Workshop on Fielded Applications of Machine Learning 1993, </booktitle> <institution> University of Massachusetts, Amherst. </institution>
Reference-contexts: Moreover, Mooney's work points out an important benefit of MLC ++ as a tool for teaching machine learning courses. Students using MLC ++ would be able to spend their time doing interesting experimental studies or modifying existing algorithms. The Consultant system <ref> [Sle] </ref>, headed by Derek Sleeman, was created in Europe to help domain experts choose a machine learning algorithm suitable for their needs. Consultant is also a library of learning algorithms, and the selected algorithm is then run on the user's data.
Reference: [Str91] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: Template classes for one and two-dimensional arrays and doubly-linked lists are implemented. Other useful classes include a string class and a random number generator. 8 4.3 The Choice of the C ++ Language C ++ <ref> [Str91] </ref> was chosen as the best language for implementing the library because of its object-oriented capability, good type checking, and efficiency. Inheritance, virtual functions, and overloading make overriding defaults and modifying specific parts of an algorithm effortless. The library has high coding standards.
Reference: [TBB + 91] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Weldel, W. Wenzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: During the last decade, the machine learning community has developed a plethora of algorithms for this task. Regrettably, there is no clear understanding of the circumstances under which specific algorithms work and how they compare to each other. Thrun et al. open the 112-page report <ref> [TBB + 91] </ref> on the Monk's problems with the following: After listening more than one week to a wide variety of learning algorithms, they [the students at the summer school] felt rather confused: Which algorithm would be optimal? And which one to avoid? As a consequence of this dilemma, they created
Reference: [TMS94] <editor> C.C. Taylor, D. Michie, and D.J. Spiegalhalter. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Paramount Publishing International, </publisher> <year> 1994. </year>
Reference-contexts: While more computationally intensive, one would expect such direct data-driven methods to yield better performance than the high-level rules in Consultant. 3.3 Large-Scale Comparisons: StatLog and MLToolbox StatLog <ref> [TMS94] </ref> is an ESPRIT project studying the behavior of over twenty algorithms (mostly in the MLToolbox), on over twenty datasets. The StatLog book gives a nice introduction to a wide variety of statistical and machine learning algorithms and an interesting discussion of the results. <p> We believe that only through projects like MLC ++ and <ref> [TMS94] </ref> can we reach such a goal. 7 Summary Frederick Brooks in his famous paper "No Silver Bullet" [Bro86] wrote The most radical possible solution for constructing software is not to construct it at all. : : : The key issue, of course, is applicability.
Reference: [Tur50] <author> A.M. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. Mind, </journal> <volume> 59, </volume> <year> 1950. </year> <note> Reprinted in 'Computers and Thought', </note> <editor> Feigenbaum and Feldman [eds.], </editor> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: The additions to the core functions that we have in mind include: Feature manipulation Functions for discovering irrelevant attributes and ranking the relative importance of attributes [KR92, JKP94], functions to help in constructing new features [PH90], and discretizing real values into ranges. Search algorithms Turing in <ref> [Tur50] </ref> already recognized that learning can be viewed as a type of search when he wrote "The learning process may be regarded as a search for a form of behavior which will satisfy the teacher." While many researchers noted that learning can be viewed as search through the space of hypotheses
Reference: [Utg88] <author> Paul E. Utgoff. </author> <title> Perceptron trees: a case study in hybrid concept representation. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 601-606. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Implement and test new ideas and variants of supervised learning algorithms. 2. Generate performance statistics such as accuracy, learning rates, and confusion matri ces. 3. Compare algorithms on different datasets (e.g., compare algorithms A; B; C on datasets X; Y; Z). 4. Experiment with hybrid algorithms (e.g., perceptron trees <ref> [Utg88] </ref>, multivariate trees [BU92] and entropy nets [KP88]) and hierarchical structures. 1 5. Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known).
Reference: [Val84] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction In supervised machine learning <ref> [SL92, Ang92, Val84, AS83] </ref>, one tries to find a rule (a categorizer) that can be used to accurately predict the class label of novel instances. During the last decade, the machine learning community has developed a plethora of algorithms for this task.
Reference: [WK91] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: First, Consultant is a tool for choosing an algorithm, and not a tool for creating variants of algorithms or testing new ones. Second, MLC ++ will be able to suggest an appropriate algorithm to a user, but by employing more general and direct methods such as cross-validation <ref> [BFOS84, WK91] </ref> to estimate the accuracy of the algorithms using only the user-provided training data.
Reference: [WM94] <author> Janusz Wnek and Ryszard S. Michalski. </author> <title> Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14(2) </volume> <pages> 139-168, </pages> <year> 1994. </year>
Reference-contexts: Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known). A useful tool for this purpose is Michalski's General Logic Diagrams <ref> [WM94, WSWM90, Mic78] </ref>. In the next section, we identify the main problems with the current experimental methodology in the machine learning community. In Section 3, we cover related work. Section 4 is an overview of the MLC ++ objectives, design issues, and implementation. <p> The data can then be plotted using any plot package, such as gnuplot or Mathematica. GLD The GLD class creates General Logic Diagrams, which are graphical projections of multi-dimensional discrete spaces onto two dimensions. They are similar to Karnaugh maps, but are generalized to non Boolean inputs and outputs <ref> [WM94, WSWM90, Mic78] </ref>. A GLD may be generated for a test set, the induced categorizer's predictions for the entire instance space, and an overlay, showing the errors. GLDs can be displayed in X windows, Postscript, and xfig, the X windows drawing editor.
Reference: [WSWM90] <author> Janusz Wnek, Jayshree Sarma, Ashraf A. Wahab, and Ryszard S. Michalski. </author> <title> Comparing learning paradigms via diagrammatic visualization. </title> <booktitle> In Methodologies for Intelligent Systems, 5. Proceedings of the Fifth International Symposium, </booktitle> <pages> pages 428-437, </pages> <year> 1990. </year> <note> Also technical report MLI90-2, </note> <institution> University of Illinois at Urbaba-Champaign. </institution> <month> 21 </month>
Reference-contexts: Graphically display the learned structure (e.g., display the decision tree). 6. Graphically display the learned concept and deviations from the target concept (when it is known). A useful tool for this purpose is Michalski's General Logic Diagrams <ref> [WM94, WSWM90, Mic78] </ref>. In the next section, we identify the main problems with the current experimental methodology in the machine learning community. In Section 3, we cover related work. Section 4 is an overview of the MLC ++ objectives, design issues, and implementation. <p> The data can then be plotted using any plot package, such as gnuplot or Mathematica. GLD The GLD class creates General Logic Diagrams, which are graphical projections of multi-dimensional discrete spaces onto two dimensions. They are similar to Karnaugh maps, but are generalized to non Boolean inputs and outputs <ref> [WM94, WSWM90, Mic78] </ref>. A GLD may be generated for a test set, the induced categorizer's predictions for the entire instance space, and an overlay, showing the errors. GLDs can be displayed in X windows, Postscript, and xfig, the X windows drawing editor.
References-found: 46

