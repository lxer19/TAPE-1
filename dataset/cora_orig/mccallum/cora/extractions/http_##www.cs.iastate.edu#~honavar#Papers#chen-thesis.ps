URL: http://www.cs.iastate.edu/~honavar/Papers/chen-thesis.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/homepage.html
Root-URL: http://www.cs.iastate.edu
Title: Neural architectures for database query processing, syntax analysis, knowledge representation, and inference  
Author: by Chun-Hsien Chen 
Degree: A dissertation submitted to the graduate faculty in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY Major: Computer Science Major Professor: Vasant Honavar  
Note: Copyright c Chun-Hsien Chen, 1998. All rights reserved.  
Date: 1998  
Address: Ames, Iowa  
Affiliation: Iowa State University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ackley, D. H., Hinton, G. D. and Sejnowski, T. J., </author> <title> A Learning Algorithm for Boltzmann Machines, </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 147-169, </pages> <year> 1985. </year>
Reference-contexts: Note that most of the activation functions produce output in the range of <ref> [0; 1] </ref> for binary signals, and [1; 1] for bipolar signals. This dissertation uses two types of threshold functions: binary hardlimiter and bipolar hardlimiter. <p> Note that most of the activation functions produce output in the range of [0; 1] for binary signals, and <ref> [1; 1] </ref> for bipolar signals. This dissertation uses two types of threshold functions: binary hardlimiter and bipolar hardlimiter. <p> Other representative ANN models in the bright 1980s, to name a few, include Hinton, Sejnowski, and Ackley's Boltzmann machine model <ref> [1, 67] </ref> which can be used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter
Reference: [2] <author> Aho, A. V., Sethi, R. and Ullman, J. D., </author> <booktitle> Compilers: Principles, techniques, and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Such grammars find extensive applications in programming languages and compilers. LR parsing is a linear time table-driven algorithm which is widely used for syntax analysis of computer programs <ref> [2, 19, 170] </ref>. This algorithm involves extensive pattern matching which suggests the consideration of a neural network implementation using associative memories. This section proposes a modular neural network architecture for parsing LR (1) grammars. <p> parsing process in order to realize an LR (1) parser which is characterized by ^ f LRP arser . 6.3.1 Representation of parse table Logically, an LR parser consists of two parts: a driver routine which is the same for all LR parsers and a parse table which is grammar-dependent <ref> [2] </ref>. LR parsing algorithm pre-compiles an LR grammar into a parse table which is referred by the driver routine for deterministically 116 parsing input string of lexical tokens by shift/reduce moves [2, 19]. Such a parsing mechanism can be simulated by a DPDA (deterministic pushdown automata) with *-moves [74]. <p> LR parsing algorithm pre-compiles an LR grammar into a parse table which is referred by the driver routine for deterministically 116 parsing input string of lexical tokens by shift/reduce moves <ref> [2, 19] </ref>. Such a parsing mechanism can be simulated by a DPDA (deterministic pushdown automata) with *-moves [74]. An *-move does not consume the input symbol, and the input head is not advanced after the move. This enables a DPDA to manipulate a stack without reading input symbols. <p> Assuming hardware implementation based on current CMOS VLSI technology, ff = 20 ns (see Section 3.3). Syntax analysis in a conventional computer typically involves: lexical analysis, grammar parsing, parse tree construction and error handling. These four processes are generally coded into two modules <ref> [2] </ref>. Error handling is usually embedded in grammar parsing and lexical analysis respectively, and parse tree construction is often embedded in grammar parsing. The procedure for grammar parsing is the main module.
Reference: [3] <author> Aho, A. V. and Ullman, J. D., </author> <title> Principles of Compiler Design, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: SLR (1) and LALR (1) tables, which are far smaller than LR (1) table, typically have several hundreds of states for the same size of language, and they always have the same number of states for a given grammar <ref> [3] </ref> (The differences among LR, SLR, and LALR parsers are discussed in [19]). <p> The example of LR (1) grammar (G 1 ) used here is taken from <ref> [3] </ref>. <p> Since logically an LR parser consists of two parts, a driver routine which is the same for all LR parsers, and a parse table which varies from one application to the next <ref> [3] </ref>, the proposed NNLR Parser can be used as a general-purpose neural architecture for LR parsing.
Reference: [4] <author> Aitchison, J., </author> <title> Words in the Mind, in: An Introduction to the Mental Lexicon, </title> <publisher> Basil Blackwell, Oxford, </publisher> <year> 1987. </year>
Reference-contexts: For example, the library at Iowa State University has over 2 million volumes, and the number of words a native English speaker knows is estimated to be between 50,000 and 250,000 <ref> [4] </ref>. <p> a 100 MIPS processor with a 32-bit data bus is used, the comparison cycle for every 32 bits takes 5 instructions on average, and there are 2 16 1 = 65; 535 records (the number of words a native English speaker knows is estimated to be between 50,000 and 250,000 <ref> [4] </ref>), then the overhead for locating a desired record pointer is about (16 1) fi d4 fi 15=32e fi 5 fi 10 ns = 1500 ns which compares unfavorably with 100 ns. Note that this is only the cost of locating a record pointer for a single record.
Reference: [5] <author> Ajjanagadde, V. and Shastri, L., </author> <title> Efficient Inference with Multi-Place Predicates and Variables in a Connectionist System, </title> <booktitle> Proceedings of 11th Cognitive Science Society Conference, </booktitle> <pages> pp. 396-403, </pages> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference <ref> [5, 31, 141, 167, 176] </ref>, computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> Many of the systems proposed in the literature are motivated by the need for massively parallel architecture for AI applications, and some of them are proposed to model human cognitive processes robustly. In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic <ref> [5, 31, 95, 175, 176] </ref>; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141].
Reference: [6] <author> Allen, R. B., </author> <title> Connectionist Language Users, </title> <journal> Connection Science, </journal> <volume> vol. 2, no. 4, </volume> <editor> p. </editor> <volume> 279, </volume> <year> 1990. </year>
Reference: [7] <author> Amari, S., </author> <title> Characteristics of Random Nets of Analog Neuron-like Elements, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 2, </volume> <pages> pp. 643-657, </pages> <year> 1972. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari <ref> [7, 8, 9] </ref>, Anderson [10], Fukushima [39], Grossberg [51, 52, 53], Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [8] <author> Amari, S., </author> <title> Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 21, no. 11, </volume> <pages> pp. 1197-1206, </pages> <year> 1972. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari <ref> [7, 8, 9] </ref>, Anderson [10], Fukushima [39], Grossberg [51, 52, 53], Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [9] <author> Amari, S., </author> <title> Neural Theory of Association and Concept-Formation, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 26, </volume> <pages> pp. 175-185, </pages> <year> 1977. </year> <month> 138 </month>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari <ref> [7, 8, 9] </ref>, Anderson [10], Fukushima [39], Grossberg [51, 52, 53], Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s. <p> The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks [75], correlation matrix memories [84], bidirectional associative memories [90], among others <ref> [9, 59, 91, 125] </ref>. A precise definition of binary/bipolar associative memories follows: Let D H (u; u ) denote the Hamming distance between binary (bipolar) vectors u and u 0 Hamming distance is the number of bits that differ between two binary (bipolar) vectors.
Reference: [10] <author> Anderson, J. A., Silverstein, J. W., Ritz, S. A. and Jones, R. S., </author> <title> Distinctive Feature, Categorical Perception, and Probability Learning: Some Applications of a Neural Model, </title> <journal> Psychological Review, </journal> <volume> vol. 84, </volume> <pages> pp. 413-451, </pages> <year> 1977. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson <ref> [10] </ref>, Fukushima [39], Grossberg [51, 52, 53], Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [11] <author> Arbib, M., </author> <title> Schema Theory: Cooperative Computation for Brain Theory and Distributed AI, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 51-74, </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision <ref> [11, 119] </ref>, natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems.
Reference: [12] <author> Bently, J. L., </author> <title> Multidimensional Binary Search Trees Used for Associative Searching, </title> <journal> Communications of the ACM, </journal> <volume> vol. 18, no. 9, </volume> <pages> pp. 507-517, </pages> <year> 1975. </year>
Reference-contexts: The cost of locating several record pointers of related records using user-entered data in an index file containing multiple index fields is examined in next section. 3.3.2.3 The cost of partial-match queries One of the most commonly used data structures for processing partial-match queries on multiple index fields is k-d-tree <ref> [12] </ref>. It can provide approximately satisfactory performance for locating a single record by exact match or several related records by partial match.
Reference: [13] <author> Berg, G., </author> <title> A Connectionist Parser with Recursive Sentence Structure and Lexical Disambiguation, </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 32-37, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> RAAM has been used by several researchers to implement stacks in connectionist designs for parsers <ref> [13, 66, 116] </ref>. A RAAM is a 2-layer Perceptron with recurrent links from hidden neurons to part of input neurons and from part of output neurons to hidden neurons.
Reference: [14] <author> Bookman, L. A., </author> <title> A Framework for Integrating Relational and Associational Knowledge for Comprehension, in: Computational Architectures Integrating Neural and Symbolic Processes : A Perspective on the State of the Art, Sun, </title> <editor> R. and Bookman, L. (Ed.), </editor> <volume> Chapter 9, </volume> <pages> pp. 283-318, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1995. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing <ref> [14, 32] </ref>, learning [46, 69, 168], and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems.
Reference: [15] <author> Butterworth, B., </author> <title> Lexical Representation, in: Language Production Volume 2: Development, Writing and Other Language Processes, Butterworth, </title> <publisher> B. (Ed.), </publisher> <pages> pp. 257-294, </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1983. </year>
Reference-contexts: Typically, a dictionary contains much free text including definitions, examples, cross-reference words, and others. Generally, there are two basic conceptions about the form of the items which serve as access keys in a lexicon. One is minimal listing hypothesis <ref> [15] </ref> which only lists lexemes and results in a root lexicon. A lexeme may have several variants, e.g., in English, the words: produces, produced, producing, producer, productive and production are variants of the lexeme produce, and the words: shorter, shortest and shortly are variants of the lexeme short.
Reference: [16] <author> Carpenter, G. and Grossberg, S., </author> <title> Adaptive Resonance Theory: Stable Self-Organization of Neural Recognition Codes in Response to Arbitrary Lists of Input Patterns, </title> <booktitle> 8th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 45-62, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks <ref> [16, 17, 18] </ref> which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training
Reference: [17] <author> Carpenter, G. and Grossberg, S., </author> <title> A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine, Computer Vision, Graphics, </title> <booktitle> and Image Understanding, </booktitle> <volume> vol. 37, </volume> <pages> pp. 54-116, </pages> <year> 1987. </year> <month> 139 </month>
Reference-contexts: used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks <ref> [16, 17, 18] </ref> which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training
Reference: [18] <author> Carpenter, G. and Grossberg, S., ART2: </author> <title> Self-Organization of Stable Category Recognition Codes for Analog Input Patterns, </title> <journal> Applied Optics, </journal> <volume> vol. 26, </volume> <pages> pp. 4919-4930, </pages> <year> 1987. </year>
Reference-contexts: used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks <ref> [16, 17, 18] </ref> which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training
Reference: [19] <author> Chapman, N. P., </author> <title> LR Parsing : Theory and Practice, </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1987. </year>
Reference-contexts: Such grammars find extensive applications in programming languages and compilers. LR parsing is a linear time table-driven algorithm which is widely used for syntax analysis of computer programs <ref> [2, 19, 170] </ref>. This algorithm involves extensive pattern matching which suggests the consideration of a neural network implementation using associative memories. This section proposes a modular neural network architecture for parsing LR (1) grammars. <p> LR parsing algorithm pre-compiles an LR grammar into a parse table which is referred by the driver routine for deterministically 116 parsing input string of lexical tokens by shift/reduce moves <ref> [2, 19] </ref>. Such a parsing mechanism can be simulated by a DPDA (deterministic pushdown automata) with *-moves [74]. An *-move does not consume the input symbol, and the input head is not advanced after the move. This enables a DPDA to manipulate a stack without reading input symbols. <p> (1) and LALR (1) tables, which are far smaller than LR (1) table, typically have several hundreds of states for the same size of language, and they always have the same number of states for a given grammar [3] (The differences among LR, SLR, and LALR parsers are discussed in <ref> [19] </ref>). The number of states in the parse table of LALR (1) parsers for most programming languages is between about 200 and 450, and the number of symbols (lexical tokens) is around 50 [19], i.e., the number of table entries is between about 10000 and 22500. <p> states for a given grammar [3] (The differences among LR, SLR, and LALR parsers are discussed in <ref> [19] </ref>). The number of states in the parse table of LALR (1) parsers for most programming languages is between about 200 and 450, and the number of symbols (lexical tokens) is around 50 [19], i.e., the number of table entries is between about 10000 and 22500. Typically a parse table is realized as a 2-dimensional array in current computer systems. <p> As described in Section 2.2.5, a BMP module can effectively and efficiently realize such content-based table lookup. LR grammars used in practical applications typically produce parse tables with between 80% and 95% undefined error entries <ref> [19] </ref>. The size of the table is reduced by using lists which can result in a significant performance penalty.
Reference: [20] <author> Chen, C. and Honavar, V., </author> <title> Neural Network Automata, </title> <booktitle> Proc. of World Congress on Neural Networks, </booktitle> <volume> vol. 4, </volume> <pages> pp. 470-477, </pages> <address> San Diego, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This section first briefly reviews the symbolic computing model for deterministic finite automata and then presents a method to systematically design neural network architectures for realizing deterministic finite automata <ref> [20] </ref>. 5.2.1 Deterministic finite automata (DFA) A deterministic finite automaton is a 5-tuple M DF A = (Q; ; ffi; q 0 ; F ) [74], where Q is a finite non-empty set of states, is a finite non-empty input alphabet, q 0 2 Q is the initial state, F Q <p> 0 , q 1 , ..., q n is the sequence of states such that ffi (q i1 ; a i ) = q i for 1 i n. 5.2.2 Architecture of NN DFA A partially recurrent neural network architecture can be used to realize a DFA as shown in <ref> [20] </ref>. Its central concept is to use a BMP module to realize the transition function of a DFA. The neural representation in the BMP module is described as follows. * The input neurons are divided into two groups. <p> The resulting automata can recognize the set of deterministic context-free languages (DCFL), a more complex and widely used class of languages in Chomsky hierarchy [74]. This section describes a method to systematically synthesize neural network architectures for deterministic pushdown finite automata <ref> [20] </ref>. 5.3.1 Deterministic pushdown automata (DPDA) A pushdown automaton M P DA is a 7-tuple (Q; ; ; ffi; q 0 ; ?; F ) [74], where Q is a finite set of states, is a finite input alphabet, is a finite stack alphabet, q 0 2 Q is the initial <p> during the computation of a DPDA, where fl denotes a don't care value, fpop, push, noopg is the set of possible stack operations, and noop denotes no operation. 81 5.3.2 Architecture of NN DPDA A partially recurrent neural network architecture can be used to realize a DPDA as shown in <ref> [20] </ref>. Its central concept is to use a BMP module to realize the transition function of a DPDA. The neural representation in the BMP module is described as follows. * The input neurons are divided into three groups. <p> Notable exceptions are: connectionist realizations of Turing Machines (wherein a stack is simulated using binary representation of a fractional number) [143, 169]; a few neural architectures designed for parsing based on a known grammar [34, 164]; and neural network realizations of finite state automata <ref> [20, 134] </ref>. Nevertheless, it is informative to examine the various proposals for neural architectures for syntax analysis (regardless of whether the grammar is preprogrammed or learned). <p> The conventional approach to implementing a lexical analyzer using a DFA (in particular, a Mealy machine) can be realized quite simply using an NN DFA <ref> [20] </ref>. However, a major drawback of this approach is that all legal transitions have to be exhaustively specified in the DFA.
Reference: [21] <author> Chen, C. and Honavar, V., </author> <title> A Neural Architecture for Content as well as Address-Based Storage and Recall: </title> <journal> Theory and Applications, Connection Science, </journal> <volume> vol. 7, no. 3 & 4, </volume> <pages> pp. 281-300, </pages> <year> 1995a. </year>
Reference-contexts: Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications. <p> The neural associative processor (memory) is essentially a 2-layer perceptron which can store and retrieve arbitrary binary pattern associations <ref> [21] </ref>. It is a cost-effective SIMD (single instruction, multiple data) computer system for massively parallel pattern matching and retrieval. <p> As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications. <p> Fault tolerance capabilities of neural architectures under different fault models (neuron faults, connection faults, etc) has been the topic of considerable research <ref> [21, 165, 180] </ref> and is 135 beyond the scope of this chapter.
Reference: [22] <author> Chen, C. and Honavar, V., </author> <title> A Neural Network Architecture for Syntax Analysis. </title> <note> Submitted. Preliminary version available as Iowa State University Dept. </note> <institution> of Computer Science Tech. </institution> <type> Rep. </type> <address> ISU-CS-TR 95-18, </address> <year> 1995b. </year>
Reference-contexts: Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> Extensions of the proposed ANN memory architecture for efficiently handling database queries and syntax analysis are proposed in Chapters 3 and 6 (also see <ref> [22, 23] </ref>) respectively. It is worth mentioning that the proposed neural memory supports realization of many-to 41 one binary random mappings which is extensively used in the design of digital logic devices such as logic circuitries of AND/OR (or NAND/NOR) gates. <p> Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications. <p> transition will have the next state &lt; 0 m &gt; given our implementation of a BMP module. 5.4 Neural Network Design for Stack (NN Stack) This section first briefly discuss the symbolic computing model for stack and then presents a method to systematically design neural network architectures for realizing stacks <ref> [22] </ref>. 5.4.1 Symbolic representation of stack A stack can be coded as a string over a stack alphabet, with its top element at one end of the string and its bottom element at the other end. Pop and push are the main actions of a stack.
Reference: [23] <author> Chen, C. and Honavar, V., </author> <title> A Neural Network Architecture for High-Speed Database Query Processing System, </title> <journal> Microcomputer Applications vol. </journal> <volume> 15, no. 1, </volume> <pages> pp. 7-13, </pages> <year> 1996. </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems [47, 65, 70, 72, 73, 99, 136, 172, 179, 185]. Examples of such systems include: neural architectures for information retrieval and database query processing <ref> [23, 24] </ref>, generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> With the addition of appropriate control circuitry, this behavior can be modified to yield sequential recall of more than one stored pattern. This has a number of practical applications such as information retrieval, database query processing <ref> [23, 24] </ref> (see Chapter 3), knowledge-based diagnosis systems, etc. This has the effect of searching through memory for patterns that are sufficiently close to a given input pattern and then recall them one after another. <p> proposed content-addressed memory in which data parallelism is achieved and all memory patterns are compared with input pattern in parallel within one step can be far more efficient (in terms of computation time) than searching for data in a key-based organization of the sort commonly used in conventional computer systems <ref> [23] </ref>. With the need for real-time response in language translation and with the increased number of users as well as increased use of large networked databases over the Internet, efficient architectures for high-speed table lookup, message routing and database query processing have assumed great practical significance. <p> Extensions of the proposed ANN memory architecture for efficiently handling database queries and syntax analysis are proposed in Chapters 3 and 6 (also see <ref> [22, 23] </ref>) respectively. It is worth mentioning that the proposed neural memory supports realization of many-to 41 one binary random mappings which is extensively used in the design of digital logic devices such as logic circuitries of AND/OR (or NAND/NOR) gates. <p> Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications. <p> As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [24] <author> Chen, C. and Honavar, V., </author> <title> Neural Architectures for Information Retrieval and Query Processing, in: Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text, </title> <editor> Moisl, H., Dale, R. and Somers, H. (Ed.), </editor> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1998. </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems [47, 65, 70, 72, 73, 99, 136, 172, 179, 185]. Examples of such systems include: neural architectures for information retrieval and database query processing <ref> [23, 24] </ref>, generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> With the addition of appropriate control circuitry, this behavior can be modified to yield sequential recall of more than one stored pattern. This has a number of practical applications such as information retrieval, database query processing <ref> [23, 24] </ref> (see Chapter 3), knowledge-based diagnosis systems, etc. This has the effect of searching through memory for patterns that are sufficiently close to a given input pattern and then recall them one after another. <p> Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications.
Reference: [25] <author> Chen, C. and Honavar, V., </author> <title> A Neural Architecture for Parallel Set Operations. </title> <note> Paper in preparation. </note>
Reference-contexts: Alternatively, one can perform a set difference operation on the hidden neuron outputs from every pair of consecutive time steps before allowing the hidden neurons to influence the 2nd-layer connections and the output neurons. It is rather straightforward to implement such set-theoretic operations using neural networks <ref> [25] </ref>. As already pointed out, the ability to perform multiple recalls is more likely to be useful when dealing with partially specified input patterns.
Reference: [26] <author> Cohen, D., </author> <title> Introduction to Computer Theory, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The computational capability of McCulloch-Pitts neural networks was proved to be equivalent to Turing machines [183] which are the essential model of symbolic computation and can perform any computation that can be described by a finite program in any general purpose language <ref> [26] </ref>. In 1949, Hebb proposed the first learning rule for neurons [63]. In late 1950s and early 1960s, Rosenblatt introduced a class of neural networks [155], called perceptrons, which can learn to classify patterns through supervised learning.
Reference: [27] <author> Das, S., Giles, C. L. and Sun, G. Z., </author> <title> Using Prior Knowledge in a NNDPA to Learn Context-Free Languages, </title> <booktitle> in: Advances in Neural Information Processing Systems 5, </booktitle> <editor> Han-son, S. J., Cowan, J. D. and Giles, C. L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 65-72, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year> <note> 140 0 Autrechy , C. </note> <author> L. and Reggia, J. A., </author> <title> An Overview of Sequence Processing by Connectionist Models, </title> <type> Technical Report UMIACS-TR-89-82, </type> <institution> University of Maryland, College Park, MD, </institution> <year> 1989. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> Augmenting SPEC with a lexical analyzer offers a way around this problem. <ref> [27, 174, 197] </ref> propose higher-order recurrent neural network equipped with an external stack to learn to recognize deterministic CFG, i.e., to learn to simulate a deterministic pushdown automata (DPDA). [27, 174] use an analog network coupled with a continuous stack and use a variant of a real time recurrent network learning <p> Augmenting SPEC with a lexical analyzer offers a way around this problem. [27, 174, 197] propose higher-order recurrent neural network equipped with an external stack to learn to recognize deterministic CFG, i.e., to learn to simulate a deterministic pushdown automata (DPDA). <ref> [27, 174] </ref> use an analog network coupled with a continuous stack and use a variant of a real time recurrent network learning algorithm to train the network. [197] uses a discrete network coupled with a discrete stack and employs a pseudo-gradient learning method to train the network.
Reference: [29] <author> Dayhoff, J., </author> <title> Neural Network Architectures : An Introduction, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: ANN are extremely simplified models of biological neural systems in many aspects such as the structure of basic computational units, the mechanism for information processing, network architecture, etc. Compared to most current digital computer systems, ANN are particularly well-suited for pattern-directed problems pattern completion, pattern classification and pattern association <ref> [29] </ref> which arise frequently in applications such as language processing, speech recognition, and pattern recognition.
Reference: [30] <author> Defiore, C. and Berra, P. B., </author> <title> A Data Management System Utilizing an Associative Memory, </title> <booktitle> AFIPS, Proceedings of the National Computer Conference, </booktitle> <volume> vol. 42, </volume> <pages> pp. 181-185, </pages> <year> 1973. </year>
Reference-contexts: Therefore, many researchers have explored to augment conventional database systems with subsystems which effectively exploit associative processing to enhance the performance of the systems <ref> [30, 101, 121, 135, 157, 162, 196] </ref>. Many applications require associative table lookup mechanism or query processing system to be capable of retrieving items based on partial matches (some features of the input are noisy or missing) or retrieval of multiple records matching the specified query criteria.
Reference: [31] <author> Dolan, C. P. and Smolensky, P., </author> <title> Tensor Product Production System: A Modular Architecture and Representation, </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> pp. 53-58, </pages> <year> 1989. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference <ref> [5, 31, 141, 167, 176] </ref>, computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> Many of the systems proposed in the literature are motivated by the need for massively parallel architecture for AI applications, and some of them are proposed to model human cognitive processes robustly. In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic <ref> [5, 31, 95, 175, 176] </ref>; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141].
Reference: [32] <author> Dyer, M. G., </author> <title> Connectionist Natural Language Processing: A Status Report, in: Computational Architectures Integrating Neural and Symbolic Processes : A Perspective on the State of the Art, Sun, </title> <editor> R. and Bookman, L. (Ed.), </editor> <volume> Chapter 12, </volume> <pages> pp. 389-429, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1995. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing <ref> [14, 32] </ref>, learning [46, 69, 168], and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems.
Reference: [33] <author> Elman, J. L., </author> <title> Finding Structure in Time, </title> <journal> Cognitive Science, </journal> <volume> vol. 14., </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: Usually, the neurons of a feedback net 6 work are classified into input, hidden, and output neurons functionally but not architecturally. Perceptrons [155] and multi-layer Perceptron [156] are two examples of feedforward networks. Elman network <ref> [33] </ref> and Jordan network [81] are two examples of recurrent networks. <p> [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron [40, 41] which can be trained with supervision to recognize handwritten characters; and recurrent neural networks <ref> [33, 81, 144] </ref> which allow recursive processing on input string of variable length.
Reference: [34] <author> Fanty, M. A., </author> <title> Context-free Parsing with Connectionist Networks, </title> <booktitle> Proceedings of AIP Neural Networks for Computing, Conference No. </booktitle> <volume> 151, </volume> <pages> pp. 140-145, </pages> <address> Snowbird, UT, </address> <year> 1986. </year>
Reference-contexts: Notable exceptions are: connectionist realizations of Turing Machines (wherein a stack is simulated using binary representation of a fractional number) [143, 169]; a few neural architectures designed for parsing based on a known grammar <ref> [34, 164] </ref>; and neural network realizations of finite state automata [20, 134]. Nevertheless, it is informative to examine the various proposals for neural architectures for syntax analysis (regardless of whether the grammar is preprogrammed or learned). <p> The remainder of this subsection explores some of the proposed architectures in the literature for syntax analysis in terms of how each of them addresses the key subtasks of syntax analysis. <ref> [34] </ref> proposes a neural network to parse input strings of fixed maximum length for known context-free grammars (CFG).
Reference: [35] <author> Fodor, J. and Pylyshyn, Z., </author> <title> Connectionism and Cognitive Architecture: A Critical Analysis, in: Connections and Symbols, Pinker, </title> <editor> S. and Mehler, J. (Ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: The ability to reliably store and recall associations between arbitrary patterns is regarded by many to be a prerequisite for higher level cognitive activity (e.g., logical inference) <ref> [35] </ref>. The associative memory model proposed in this chapter is designed to reliably store and recall associations between arbitrary pairs of patterns. 18 2.1.3 Address-based memory Address-based memory is extensively used for storing both data as well as programs in current computer systems.
Reference: [36] <author> Forster, K. I., </author> <title> Accessing the Mental Lexicon, in: New Approaches to Language Mechanisms, </title> <editor> Walker, R. and Wales, R. J. </editor> <publisher> (Ed.), </publisher> <pages> pp. 257-287, </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1976. </year>
Reference-contexts: There are several models of lexical access in a computational lexicon. Our ANN-based query system for NLP lexicon is based on the search model of lexical access (indirect access) <ref> [36, 58] </ref>. In such a model, a text-based computational lexicon which associates every access key with its lexical specification contains two organizations: one is called master file which stores entries of lexical specifications, and the other is called access file which consists of pairs of (&lt;access key&gt;, &lt;lexical pointer&gt;).
Reference: [37] <author> Frakes, W. B. and Baeza-Yates R. (Ed.), </author> <title> Information Retrieval: Data Structures & Algorithms, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year> <month> 141 </month>
Reference-contexts: of messages in communication networks, symbol tables used in compiling computer programs written in high level languages, knowledge bases which store facts and rules in relational form, fact and rule tables used in unification process of logic programming systems, keyword tables (inverted and signature files) used in information retrieval applications <ref> [37] </ref>, and machine-readable lexicons, dictionary as well as varieties of tables used in memory-based parsing [82] for natural language processing. In such tables, every table entry is an associated input-output ordered pair. <p> Thus, the performance of the hardware implementation of ANN is likely to improve with technological advances in VLSI. 3.3.2 Analysis of query processing in conventional computer systems Accessing information based on a key is central to information retrieval systems <ref> [37, 157, 158] </ref> and database systems [186]. In relational database systems implemented on conventional computer systems, given the value for a key, a record is located efficiently by using key-based organizations including hashing, index-sequential access files and B-trees [186].
Reference: [38] <author> Frasconi, P., Gori, M., Maggini, M. and Soda, G., </author> <title> Unified Integration of Explicit Rules and Learning by Example in Recurrent Networks, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 340-346, </pages> <year> 1995. </year>
Reference: [39] <author> Fukushima, K., Cognitron: </author> <title> A Self-Organizing Multilayered Neural Network, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 20, </volume> <pages> pp. 121-136, </pages> <month> Nov. </month> <year> 1975. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson [10], Fukushima <ref> [39] </ref>, Grossberg [51, 52, 53], Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [40] <author> Fukushima, K., Miyake, S. and Ito, T., </author> <title> Neocognitron: A Neural Network Model for a Mechanism of Visual Pattern Recognition, </title> <journal> IEEE Transactions on System, Man, and Cybernetics, SMC-13, </journal> <volume> no. 5, </volume> <pages> pp. 826-834, </pages> <address> Sep./Oct., </address> <year> 1983. </year>
Reference-contexts: 146, 147] which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron <ref> [40, 41] </ref> which can be trained with supervision to recognize handwritten characters; and recurrent neural networks [33, 81, 144] which allow recursive processing on input string of variable length.
Reference: [41] <author> Fukushima, K., </author> <title> Neocognitron: A Hierarchical Neural Network Capable of Visual Pattern Recognition, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. </volume> <pages> 1 , pp. 119-130, </pages> <year> 1988. </year>
Reference-contexts: 146, 147] which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron <ref> [40, 41] </ref> which can be trained with supervision to recognize handwritten characters; and recurrent neural networks [33, 81, 144] which allow recursive processing on input string of variable length.
Reference: [42] <author> Gallant, S. I., </author> <title> Connectionist Expert Systems, </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, </volume> <pages> pp. 152-169, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: NEURAL ARCHITECTURES FOR ELEMENTARY LOGICAL INFERENCE 4.1 Introduction Inference often involves tasks which look for interesting situations occurring as patterns in the input or memory data to solve questions such as "What is the most likely answer?", "Is there sufficient evidence to adopt a conclusion or is more evidence needed?" <ref> [42, 61, 191] </ref>, etc. Such tasks are important for inference from partial information, and they generally involve a process of pattern recognition by way of best, partial, and/or exact matches. <p> In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems <ref> [42, 43] </ref>, hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141].
Reference: [43] <author> Gallant, S. I., </author> <title> Neural Network Learning and Expert Systems, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems <ref> [42, 43] </ref>, hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. <p> The input to a Boolean function can be represented as a binary (bipolar) code. Therefore, the evaluation of a Boolean function can be viewed as a process of binary (bipolar) pattern recognition. It is known that every Boolean function can be represented as a DNF expression <ref> [43] </ref>. A DNF expression is a disjunction of conjunctions. The evaluations of conjunction and disjunction can be realized by the proposed AND and OR neural assemblies respectively. <p> Hence, any Boolean function (except the constant 0) can be realized by a 2-layer neural architecture (Perceptron) assembled from a fixed number of AND and OR neural assemblies. Besides, Perceptrons have space and speed advantages over DNF representations for representing and evaluating Boolean functions (see <ref> [43] </ref> for details). <p> We expect that the resulting bipolar AND and OR neural assemblies will be exactly equivalent to those proposed in <ref> [43] </ref>.
Reference: [44] <author> Giles, C. L., Horne, B. W. and Lin, T., </author> <title> Learning a Class of Large Finite State Machines With a Recurrent Neural Network, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 9, </volume> <pages> pp. 1359-1365, </pages> <year> 1995. </year>
Reference: [45] <author> Giles, C. L., Miller, C. B., Chen, D., Sun, G. Z. and Lee, Y. C., </author> <title> Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks, </title> <journal> Neural Computation, </journal> <volume> vol. 4., no. 3., </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1992. </year>
Reference: [46] <author> Goldfarb, L. and Nigam, S., </author> <title> The Unified Learning Paradigm: A Foundation for AI, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 533-559, </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning <ref> [46, 69, 168] </ref>, and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems.
Reference: [47] <editor> Goonatilake, S. and Khebbal, S. (Ed.), </editor> <title> Intelligent Hybrid Systems, </title> <publisher> Wiley, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> However, despite their generality (as computational models) and despite the potential advantages of using them as components in general-purpose artificial intelligence systems which usually involve content-based or memory-based knowledge storing and retrieving <ref> [47, 70, 72, 73, 99, 173, 179] </ref>, detailed design and performance tradeoffs in integrated systems of this sort are yet to be fully understood and working prototypes of such systems are only beginning to be developed. <p> Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications. <p> This latter viewpoint argues for a somewhat systematic exploration of this design space in search of novel and efficient computational architectures for such systems. This dissertation takes a few small steps in this direction and adds to the growing body of literature <ref> [47, 72, 99, 179] </ref> that demonstrates the potential benefits of integrated neural-symbolic architectures that overcome some of the limitations of today's ANN and AI systems.
Reference: [48] <author> Gowda, S. M. et al., </author> <title> Design and Characterization of Analog VLSI Neural Network Modules, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 28, no. 3, </volume> <pages> pp. 301-313, </pages> <year> 1993. </year> <month> 142 </month>
Reference-contexts: Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding. Analog CMOS technology is attractive for realization of ANN because it can yield compact circuits that are capable of high-speed asynchronous operation <ref> [48] </ref>. [184] reports a measured propagation delay of 104ns in a digital circuit with each synapse containing an 8-bit memory, an 8-bit subtractor and an 8-bit adder. [50] reports throughput at the rate of 10MHz (or equivalently, delay of 100ns) in a Hamming Net pattern classifier using analog circuits. [106] describes
Reference: [49] <author> Graf, H. P. and Henderson, D., </author> <title> A Reconfigurable CMOS Neural Network, </title> <booktitle> ISSCC Dig. Tech. Papers, </booktitle> <pages> pp. 144-145, </pages> <address> San Francisco, CA, </address> <year> 1990. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding.
Reference: [50] <author> Grant, D. et al., </author> <title> Design, Implementation and Evaluation of a High-Speed Integrated Hamming Neural Classifier, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 29, no. 9, </volume> <pages> pp. 1154-1157, </pages> <month> Sep. </month> <year> 1994. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding. <p> Analog CMOS technology is attractive for realization of ANN because it can yield compact circuits that are capable of high-speed asynchronous operation [48]. [184] reports a measured propagation delay of 104ns in a digital circuit with each synapse containing an 8-bit memory, an 8-bit subtractor and an 8-bit adder. <ref> [50] </ref> reports throughput at the rate of 10MHz (or equivalently, delay of 100ns) in a Hamming Net pattern classifier using analog circuits. [106] describes a hybrid analog-digital design with 5-bit (4 bits + sign) binary synapse weight values and current-summing circuits that is used to realize a 2-layer feed-forward ANN with
Reference: [51] <author> Grossberg, S., </author> <title> Some Networks That Can Learn, Remember, and Reproduce Any Number of Space-Time Patterns II, </title> <journal> Studies in Applied Mathematics, </journal> <volume> vol. 49, </volume> <pages> pp. 135-166, </pages> <year> 1970. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson [10], Fukushima [39], Grossberg <ref> [51, 52, 53] </ref>, Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [52] <author> Grossberg, S., </author> <title> Contour Enhancement, Short-Term Memory, </title> <booktitle> and Constancies in Reverberating Networks, Studies in Applied Mathematics, vol.52, </booktitle> <pages> pp. 217-257, </pages> <year> 1973. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson [10], Fukushima [39], Grossberg <ref> [51, 52, 53] </ref>, Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [53] <author> Grossberg, S., </author> <title> Adaptive Pattern Classification and Universal Recording II: Feedback, Oscillation, Ilfaction, and Illusions, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 23, </volume> <pages> pp. 187-207, </pages> <year> 1976. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson [10], Fukushima [39], Grossberg <ref> [51, 52, 53] </ref>, Kohonen [84, 85], and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [54] <author> Grosspietsch, K. E., </author> <title> Intelligent Systems by Means of Associative Processing, in: Fuzzy, Holographic, </title> <booktitle> and Parallel Intelligence, Soucek, B. and the IRIS Group (Ed.), </booktitle> <pages> pp. 179-214, </pages> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Pattern-directed associative processing relies on associative pattern matching and retrieval, is central to many problem solving paradigms in AI (e.g., knowledge based expert systems, case based reasoning) as well as computer science (e.g., database query processing, information retrieval) <ref> [54, 97, 181] </ref>, and dominates the computational requirements of many applications in AI and computer science [55, 97, 127]. This dissertation proposes methods to systematically design massively parallel architectures for pattern-directed symbol processing using neural associative memories as key components. <p> A more general goal of this chapter is to explore the design of massively parallel architectures for symbol processing using the neural associative memories proposed in Chapter 2 as key components. Pattern-directed associative inference is an essential part of most AI systems <ref> [54, 97, 181] </ref> and dominates the computational requirements of many AI applications [55, 97, 127].
Reference: [55] <author> Gupta, A., </author> <title> Parallelism in Production Systems, </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, Pittsburgh, </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: on associative pattern matching and retrieval, is central to many problem solving paradigms in AI (e.g., knowledge based expert systems, case based reasoning) as well as computer science (e.g., database query processing, information retrieval) [54, 97, 181], and dominates the computational requirements of many applications in AI and computer science <ref> [55, 97, 127] </ref>. This dissertation proposes methods to systematically design massively parallel architectures for pattern-directed symbol processing using neural associative memories as key components. <p> Pattern-directed associative inference is an essential part of most AI systems [54, 97, 181] and dominates the computational requirements of many AI applications <ref> [55, 97, 127] </ref>. The proposed high performance neural architectures for syntax analysis are systematically (and provably correctly) synthesized through composition of the necessary symbolic functions using a set of component symbolic functions each of which is realized using a neural associative processor (memory).
Reference: [56] <author> Gupta, M. M. and Knopf, G. K., </author> <title> Neuro-Vision Systems: A Tutorial, </title> <booktitle> in: Neuro-Vision Systems: Principles and Applications, </booktitle> <editor> Gupta, M. and Knopf, G. </editor> <publisher> (Ed.), </publisher> <pages> pp. 1-34, </pages> <publisher> IEEE Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Figure 1.1 shows such a node. 4 1.1.2 Activation functions The types of activation functions used by an ANN affect its expressiveness, computational capabilities, and performance. Several typical activation functions are linear, binary sigmoidal, bipolar sigmoidal, binary hardlimiter, bipolar hardlimiter, gaussian, and ramp <ref> [56, 102] </ref> defined as follows: linear : f L (s) = cs, where s = P n binary sigmoidal : f S (s) = 1 1+e cs , where s = P n bipolar sigmoidal : f S (s) = 1e cs 1+e cs , where s = P n binary <p> Their simplicity allows simple and efficient hardware implementation of such threshold functions. 1.1.3 Types of artificial neural networks and their computational capabilities ANN can be mainly classified into three basic categories : feedforward networks, feedback networks and recurrent networks <ref> [28, 56, 131] </ref> according to their architectures, functionalities, and signal propagation direction of their connections. The output of a feedforward network is a function of current input, and its connections are unidirectional. <p> A more detailed taxonomy of most neural network architectures and learning algorithms can be found in <ref> [56, 102, 112] </ref>. 9 1.3 An Overview of the Dissertation Artificial neural networks, due to their inherent parallelism, potential for fault tolerance, and adaptation through learning, offer an attractive computational paradigm for a variety of applications in computer science and engineering, artificial intelligence, robotics, and cognitive modeling.
Reference: [57] <author> Hamilton, A. et al., </author> <title> Integrated Pulse Stream Neural Networks: Results, Issues, and Pointers, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 385-393, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding.
Reference: [58] <author> Handke, I., </author> <title> The Structure of Lexicon: Human versus Machine, </title> <publisher> Mouton de Gruyter, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Basically, the lexical specification for a word in a lexicon contains phonological, morpho-syntactic, syntactic, semantic, and other fields <ref> [58] </ref>. Each field may contain several sub-fields. In a lexical database which realizes a machine-readable lexicon for real-time NLP, the lengths of the fields and sub-fields are usually fixed to allow efficient random access to them. <p> A root lexicon is more compact and requires a rule system to process the variants of lexemes, while a full-form lexicon is more computationally efficient in terms of lexical access and more user-friendly in terms of lexicon editing and extension <ref> [58] </ref>. Therefore, a hybrid of the two conceptions is often adopted in many computational lexicon applications. In the following, the term access key is used to stand for either word or lexeme in a computational lexicon no matter whether it is a root or full-form lexicon. <p> There are several models of lexical access in a computational lexicon. Our ANN-based query system for NLP lexicon is based on the search model of lexical access (indirect access) <ref> [36, 58] </ref>. In such a model, a text-based computational lexicon which associates every access key with its lexical specification contains two organizations: one is called master file which stores entries of lexical specifications, and the other is called access file which consists of pairs of (&lt;access key&gt;, &lt;lexical pointer&gt;).
Reference: [59] <author> Hao, J. and Vandewalle, J., </author> <title> A New Model of Neural Associative Memories, </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 39-47, </pages> <month> Mar. </month> <year> 1994. </year> <month> 143 </month>
Reference-contexts: The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks [75], correlation matrix memories [84], bidirectional associative memories [90], among others <ref> [9, 59, 91, 125] </ref>. A precise definition of binary/bipolar associative memories follows: Let D H (u; u ) denote the Hamming distance between binary (bipolar) vectors u and u 0 Hamming distance is the number of bits that differ between two binary (bipolar) vectors. <p> The activation functions at output neurons are bipolar hardlimiter function f H if the desired output of output neurons is binary. It is worth pointing out that the bipolar associative neural memory model derived here turns out to be exactly equivalent to the memory model proposed by <ref> [59] </ref> which uses real-value neuron thresholds and proves the effectiveness of the bipolar memory model by algebra based on using Hamming distance as difference measurement between input pattern and memory patterns.
Reference: [60] <author> Hassoun, M., </author> <title> Fundamentals of Artificial Neural Networks, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: AI has been successful in applications such as theorem proving, knowledge-based expert systems, mathematical reasoning, syntax analysis, and related applications which mainly involve systematic symbol manipulation. On the other hand, ANN have been particularly successful in applications such as pattern recognition, function approximation, and nonlinear control <ref> [60, 150] </ref> which involve primarily numeric computation. Meyerowitz has suggested that the design of neural architectures capable of supporting dynamic representations for symbol manipulation is one of the grand challenges of neural network research [113].
Reference: [61] <author> Hayes-Roth, F., </author> <title> The Role of Partial and Best Matches in Knowledge Systems, in: Pattern-Directed Inference Systems, </title> <editor> Waterman, D. A. and Hayes-Roth, F. </editor> <publisher> (Ed.), </publisher> <pages> pp. 557-574, </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: NEURAL ARCHITECTURES FOR ELEMENTARY LOGICAL INFERENCE 4.1 Introduction Inference often involves tasks which look for interesting situations occurring as patterns in the input or memory data to solve questions such as "What is the most likely answer?", "Is there sufficient evidence to adopt a conclusion or is more evidence needed?" <ref> [42, 61, 191] </ref>, etc. Such tasks are important for inference from partial information, and they generally involve a process of pattern recognition by way of best, partial, and/or exact matches.
Reference: [62] <author> Haykin, S., </author> <title> Neural Networks, </title> <publisher> MacMillan, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: It also provides for real-time update of memory contents by one-shot learning without interference with other stored patterns. It is generally agreed that artificial neural networks (ANN) have demonstrated success in low-level perceptual tasks (e.g., signal processing, pattern recognition) <ref> [62, 93, 111, 113] </ref>. <p> Similar storage capacity results are known for several content-addressed memory models such as the Hopfield network [75, 109], bi-directional associative memories [90], correlation matrix memories [84], etc. A variety of associative memory models are discussed in <ref> [62, 93] </ref>. As already pointed out, many simple content-addressed memory models studied in the literature are incapable of stable storage and recall of associations between arbitrary pairs of patterns (except under certain restricted circumstances).
Reference: [63] <author> Hebb, D. O., </author> <title> The Organization of Behavior, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1949. </year>
Reference-contexts: In 1949, Hebb proposed the first learning rule for neurons <ref> [63] </ref>. In late 1950s and early 1960s, Rosenblatt introduced a class of neural networks [155], called perceptrons, which can learn to classify patterns through supervised learning. Rosenblatt's work helped produce a large amount of research activities in this early ANN research era.
Reference: [64] <author> Hecht-Nielsen, R., </author> <title> Counterpropagation Networks, </title> <booktitle> Proceedings of IEEE First International Conference on Neural Networks, </booktitle> <volume> vol. II, </volume> <pages> pp. 19-32, </pages> <year> 1987. </year>
Reference-contexts: and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network <ref> [64] </ref> which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron [40, 41] which can be trained with supervision to recognize handwritten characters; and recurrent neural networks [33, 81, 144] which allow recursive
Reference: [65] <editor> Hendler, J., </editor> <booktitle> Beyond the Fifth Generation: Parallel AI Research in Japan, IEEE Expert, </booktitle> <pages> pp. 2-7, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence <ref> [65] </ref>.
Reference: [66] <author> Hester, K. A. et al., </author> <title> The Predictive RAAM: A RAAM That Can Learn to Distinguish Sequences from a Continuous Input Stream, </title> <booktitle> Proceedings of World Congress on Neural Networks, </booktitle> <volume> vol. 4, </volume> <pages> pp. 97-103, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> RAAM has been used by several researchers to implement stacks in connectionist designs for parsers <ref> [13, 66, 116] </ref>. A RAAM is a 2-layer Perceptron with recurrent links from hidden neurons to part of input neurons and from part of output neurons to hidden neurons.
Reference: [67] <author> Hinton, G. D. and Sejnowski, T. J., </author> <title> Learning and Relearning in Boltzmann machines, </title> <booktitle> in: Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations, </booktitle> <editor> Rumelhart, D. E., McClelland, J. L. </editor> <booktitle> and the PDP Research Group, </booktitle> <pages> pp. 282-318, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Other representative ANN models in the bright 1980s, to name a few, include Hinton, Sejnowski, and Ackley's Boltzmann machine model <ref> [1, 67] </ref> which can be used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter
Reference: [68] <author> Hinton, G. E., </author> <title> Implementing Semantic Networks in Parallel Hardware, in: Parallel Models of Associative Memory, </title> <editor> Hinton, G. E. and Anderson, J. A. (updated Ed.), </editor> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks <ref> [68, 167] </ref>, frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic. <p> As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [69] <author> Honavar, V., </author> <title> Toward Learning Systems That Integrate Different Strategies and Representations, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 615-644, </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year> <month> 144 </month>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning <ref> [46, 69, 168] </ref>, and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems. <p> Indeed, many authors have suggested this to be a primary difference between neural networks (or connectionist models) and traditional artificial intelligence systems. However, this perceived difference is rather superficial given the demonstrable Turing-equivalence of sufficiently powerful neural network models <ref> [69, 70] </ref>. Therefore, it is rather straightforward to design neural memories capable of address-based storage and recall of patterns as the following discussion illustrates. <p> The surveys of grammar inference in general can be found in <ref> [69, 96, 115, 137] </ref>, and the recent results on grammar inference using neural networks can be found in [6, 13, 27, 38, 45, 44, 66, 77, 80, 116, 122, 123, 129, 159, 161, 166, 174, 192, 194, 197].
Reference: [70] <author> Honavar, V., </author> <title> Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Toward A Resolution of the Dichotomy, in: Computational Architectures Integrating Symbolic and Neural Processes, Sun, </title> <editor> R. and Bookman, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 351-388, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1995. </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> However, despite their generality (as computational models) and despite the potential advantages of using them as components in general-purpose artificial intelligence systems which usually involve content-based or memory-based knowledge storing and retrieving <ref> [47, 70, 72, 73, 99, 173, 179] </ref>, detailed design and performance tradeoffs in integrated systems of this sort are yet to be fully understood and working prototypes of such systems are only beginning to be developed. <p> Indeed, many authors have suggested this to be a primary difference between neural networks (or connectionist models) and traditional artificial intelligence systems. However, this perceived difference is rather superficial given the demonstrable Turing-equivalence of sufficiently powerful neural network models <ref> [69, 70] </ref>. Therefore, it is rather straightforward to design neural memories capable of address-based storage and recall of patterns as the following discussion illustrates.
Reference: [71] <author> Honavar, V. and Uhr, L., </author> <title> Coordination and Control structures and Processes: Possibilities for Connectionist Networks, </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2: </journal> <pages> 277-302, </pages> <year> 1990. </year>
Reference-contexts: Given the rich panoply of controls found in biological neural networks, there is no reason not to build in a variety of control and coordination structures into neural networks whenever it is beneficial to do so <ref> [71] </ref>).
Reference: [72] <author> Honavar, V. and Uhr, L. (Ed.), </author> <title> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Massively parallel symbol processing architectures for AI systems or highly structured (as opposed to homogeneous, fully connected) ANN are just two examples of a wide range of approaches to designing intelligent systems <ref> [185, 72, 73] </ref>. Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems [47, 65, 70, 72, 73, 99, 136, 172, 179, 185]. <p> Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> However, despite their generality (as computational models) and despite the potential advantages of using them as components in general-purpose artificial intelligence systems which usually involve content-based or memory-based knowledge storing and retrieving <ref> [47, 70, 72, 73, 99, 173, 179] </ref>, detailed design and performance tradeoffs in integrated systems of this sort are yet to be fully understood and working prototypes of such systems are only beginning to be developed. <p> Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications. <p> This latter viewpoint argues for a somewhat systematic exploration of this design space in search of novel and efficient computational architectures for such systems. This dissertation takes a few small steps in this direction and adds to the growing body of literature <ref> [47, 72, 99, 179] </ref> that demonstrates the potential benefits of integrated neural-symbolic architectures that overcome some of the limitations of today's ANN and AI systems.
Reference: [73] <author> Honavar, V. and Uhr, L., </author> <title> Integrating Symbol Processing Systems and Connectionist Networks, in: Intelligent Hybrid Systems, </title> <editor> Goonatilake, S. and Khebbal, S. </editor> <publisher> (Ed.), </publisher> <pages> pp. 177-208. </pages> <publisher> Wiley, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: Massively parallel symbol processing architectures for AI systems or highly structured (as opposed to homogeneous, fully connected) ANN are just two examples of a wide range of approaches to designing intelligent systems <ref> [185, 72, 73] </ref>. Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems [47, 65, 70, 72, 73, 99, 136, 172, 179, 185]. <p> Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> However, despite their generality (as computational models) and despite the potential advantages of using them as components in general-purpose artificial intelligence systems which usually involve content-based or memory-based knowledge storing and retrieving <ref> [47, 70, 72, 73, 99, 173, 179] </ref>, detailed design and performance tradeoffs in integrated systems of this sort are yet to be fully understood and working prototypes of such systems are only beginning to be developed.
Reference: [74] <author> Hopcroft, J. E. and Ullman, J. D., </author> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: reviews the symbolic computing model for deterministic finite automata and then presents a method to systematically design neural network architectures for realizing deterministic finite automata [20]. 5.2.1 Deterministic finite automata (DFA) A deterministic finite automaton is a 5-tuple M DF A = (Q; ; ffi; q 0 ; F ) <ref> [74] </ref>, where Q is a finite non-empty set of states, is a finite non-empty input alphabet, q 0 2 Q is the initial state, F Q is the set of final or accepting states, and ffi : Q fi ! Q is the transition function. <p> A Mealy machine is a DFA augmented with an output function. It is defined by a 6-tuple M Mealy = (Q; ; ; ffi; ; q 0 ) <ref> [74] </ref>, where Q, , ffi, and q 0 are as in the DFA M DF A , is a finite non-empty output alphabet, and is output function mapping from Q fi to . (q; a) is the output associated with the transition from state q on input symbol a. <p> does not need to be implemented using a hidden neuron in the NN DFA transition module. 5.3 Neural Network Design for Deterministic Pushdown Automata (NN DPDA) The capability of DFA is limited to recognition and production of the set of regular languages, the simplest class of languages in Chomsky hierarchy <ref> [74] </ref>. The capability of DFA can be extended by adding a stack. The resulting automata can recognize the set of deterministic context-free languages (DCFL), a more complex and widely used class of languages in Chomsky hierarchy [74]. <p> of the set of regular languages, the simplest class of languages in Chomsky hierarchy <ref> [74] </ref>. The capability of DFA can be extended by adding a stack. The resulting automata can recognize the set of deterministic context-free languages (DCFL), a more complex and widely used class of languages in Chomsky hierarchy [74]. This section describes a method to systematically synthesize neural network architectures for deterministic pushdown finite automata [20]. 5.3.1 Deterministic pushdown automata (DPDA) A pushdown automaton M P DA is a 7-tuple (Q; ; ; ffi; q 0 ; ?; F ) [74], where Q is a finite set of states, <p> widely used class of languages in Chomsky hierarchy <ref> [74] </ref>. This section describes a method to systematically synthesize neural network architectures for deterministic pushdown finite automata [20]. 5.3.1 Deterministic pushdown automata (DPDA) A pushdown automaton M P DA is a 7-tuple (Q; ; ; ffi; q 0 ; ?; F ) [74], where Q is a finite set of states, is a finite input alphabet, is a finite stack alphabet, q 0 2 Q is the initial state, ?2 is a particular stack symbol called stack start symbol, FQ is the set of final states, and ffi is the transition function mapping <p> Thus the concept of nondeter-ministicism in NFA, which plays a central role in both the theory of languages and the theory of computation <ref> [74] </ref>, provides a typical model suitable for the exploration of parallel symbolic 91 computing via neural networks. The reduced operation time complexity of NFA realized by the proposed RNN is due to the parallel operations of the neural assemblies in the RNN. <p> The reduced operation time complexity of NFA realized by the proposed RNN is due to the parallel operations of the neural assemblies in the RNN. It is well known that DFA and NFA are equivalent, and every NFA can be converted into its equivalent DFA <ref> [74] </ref>. NFA seem to be of no practical interest in direct application implementations since they are embedded with nondeterministicism and don't correspond naturally to deterministic algorithms. <p> Therefore, the proposed RNN can be used as a general neural architecture for realizing finite automata including DFA and NFA. 5.5.1 Nondeterministic finite automata (NFA) A nondeterministic finite automaton M NF A is a 5-tuple (Q; ; ffi 0 ; q 0 ; F ) <ref> [74] </ref>, where Q, , q 0 , and F have same meaning as for a DFA, but ffi 0 is a mapping from Qfi to 2 Q . <p> The set of strings 92 accepted by M NF A in fl is denoted as L (M NF A ), called the language accepted by M NF A . 5.5.1.1 Advantages of NFA for applications It is well known that NFA and DFA are equivalent <ref> [74] </ref>. Two automata are said equivalent if they accept the same language. Any language accepted by an NFA can also be accepted by a DFA, and every NFA can be converted into an equivalent DFA [74]. <p> Advantages of NFA for applications It is well known that NFA and DFA are equivalent <ref> [74] </ref>. Two automata are said equivalent if they accept the same language. Any language accepted by an NFA can also be accepted by a DFA, and every NFA can be converted into an equivalent DFA [74]. However, an NFA is usually simpler and more intuitive to design than its equivalent DFA for a given language due to the powerful concept of nondeterministicism inherent in NFA. Figures 5.4 and 5.5 respectively show the state diagrams of an NFA and its equivalent DFA. <p> The whole input string is presented at one time to the neural parser which is a layered network of logical AND and OR nodes with connections set by an algorithm based on CYK algorithm <ref> [74] </ref>. PARSEC [80] is a modular neural parser consisting of six neural network modules. It transforms a semantically rich and therefore fairly complex English sentence into three output representations produced by its respective output modules. <p> ns (at best) to 100 ns (at worst) given the current CMOS technology for implementation of artificial neural networks (see Section 3.3). 115 6.3 A Modular Neural Architecture for LR Parser (NNLR Parser) LR (k) grammars generate the so-called deterministic context-free languages which can be accepted by deterministic push-down automata <ref> [74] </ref>. Such grammars find extensive applications in programming languages and compilers. LR parsing is a linear time table-driven algorithm which is widely used for syntax analysis of computer programs [2, 19, 170]. This algorithm involves extensive pattern matching which suggests the consideration of a neural network implementation using associative memories. <p> LR (k) parsers scan input from left to right and produce a rightmost derivation tree by using lookahead of k unscanned input symbols. Since any LR (k) grammar for k 1 can be transformed into an LR (1) grammar [170], LR (1) parsers are sufficient for practical applications <ref> [74] </ref>. An LR (1) grammar can be defined as G LR (1) = (V; T; ; fi) [74], where V and T are finite sets of variables (nonterminals) and terminals respectively, is a finite set of production rules, and fi 2 V is a special variable called the start symbol. <p> Since any LR (k) grammar for k 1 can be transformed into an LR (1) grammar [170], LR (1) parsers are sufficient for practical applications <ref> [74] </ref>. An LR (1) grammar can be defined as G LR (1) = (V; T; ; fi) [74], where V and T are finite sets of variables (nonterminals) and terminals respectively, is a finite set of production rules, and fi 2 V is a special variable called the start symbol. V and T are disjoint. <p> LR parsing algorithm pre-compiles an LR grammar into a parse table which is referred by the driver routine for deterministically 116 parsing input string of lexical tokens by shift/reduce moves [2, 19]. Such a parsing mechanism can be simulated by a DPDA (deterministic pushdown automata) with *-moves <ref> [74] </ref>. An *-move does not consume the input symbol, and the input head is not advanced after the move. This enables a DPDA to manipulate a stack without reading input symbols.
Reference: [75] <author> Hopfield, J. J., </author> <title> Neural Networks and Physical Systems with Emergent Collective Computational Abilities, </title> <journal> Proc. Natl. Acad. Sci. USA, </journal> <volume> vol. 79, </volume> <pages> pp. 2554-2558, </pages> <month> Apr. </month> <year> 1982. </year>
Reference-contexts: Usually, the neurons of a feedback net 6 work are classified into input, hidden, and output neurons functionally but not architecturally. Perceptrons [155] and multi-layer Perceptron [156] are two examples of feedforward networks. Elman network [33] and Jordan network [81] are two examples of recurrent networks. Hopfield networks <ref> [75] </ref> and BAM [90] two examples of feedback networks. 1.1.4 Implementation of artificial neural networks Due to the computations required by enormous neural nodes to calculate their thresholded activation and weighted sum on the inputs from their associated input connections in an ANN, ANN systems generally require more intensive computational power <p> The tremendous resurgence of ANN research interest in 1980s was mainly due to the invention of Hopfield networks <ref> [75] </ref> which can serve as content-addressable memory or solve combinatorial optimization problems [76], and the introduction of Backpropagation learning algorithm [156] which overcomes the limitation of perceptron's single-layer learning algorithm in linearly separable problems and can be exploited to train multi-layer perceptron to solve nonlinearly separable problems. <p> The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks <ref> [75] </ref>, correlation matrix memories [84], bidirectional associative memories [90], among others [9, 59, 91, 125]. <p> Thus, a linear associative memory with n input neurons can store and recall perfectly at most n pattern associations. Similar storage capacity results are known for several content-addressed memory models such as the Hopfield network <ref> [75, 109] </ref>, bi-directional associative memories [90], correlation matrix memories [84], etc. A variety of associative memory models are discussed in [62, 93].
Reference: [76] <author> Hopfield, J. J. and Tank, D., </author> <title> Neural Computation of Decision in Optimization Problems, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52, </volume> <pages> pp. 141-152, </pages> <year> 1985. </year>
Reference-contexts: The tremendous resurgence of ANN research interest in 1980s was mainly due to the invention of Hopfield networks [75] which can serve as content-addressable memory or solve combinatorial optimization problems <ref> [76] </ref>, and the introduction of Backpropagation learning algorithm [156] which overcomes the limitation of perceptron's single-layer learning algorithm in linearly separable problems and can be exploited to train multi-layer perceptron to solve nonlinearly separable problems.
Reference: [77] <author> Horne, B., Hush, D. R. and Abdallah, C., </author> <title> A State Space Recurrent Neural Network with Application to Regular Grammatical Inference, </title> <type> UNM Tech. Rep. </type> <institution> No. EECE 92-002, Department of Electrical and Computer Engineering, University of New Mexico, </institution> <address> Albuquerque, NM, </address> <year> 1992. </year>
Reference: [78] <author> Howe, D. B. and Asanovic, K., </author> <title> SPACE: </title> <booktitle> Symbolic Processing in Associative Computing Elements, in: VLSI for Neural Networks and Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. </editor> <publisher> (Ed.), </publisher> <pages> pp. 243-252, </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1994 </year> <month> 145 </month>
Reference-contexts: As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [79] <author> Jackson, T. and Austin, J., </author> <title> The Representation of Knowledge and Rules in Hierarchical Neural Networks, in: Neural Networks for Knowledge Representation and Inference, </title> <editor> Levine, D. S. </editor> <booktitle> and Aparicio IV, M.(Ed.), Chapter 8, </booktitle> <pages> pp. 206-238, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year>
Reference-contexts: In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation <ref> [79] </ref>, planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [80] <author> Jain, A. N., Waibel, A. and Touretzky, D. S., </author> <title> PARSEC: A Structured Connectionist Parsing System for Spoken Language, </title> <booktitle> IEEE Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 205-208, </pages> <address> San Francisco, CA, </address> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: The whole input string is presented at one time to the neural parser which is a layered network of logical AND and OR nodes with connections set by an algorithm based on CYK algorithm [74]. PARSEC <ref> [80] </ref> is a modular neural parser consisting of six neural network modules. It transforms a semantically rich and therefore fairly complex English sentence into three output representations produced by its respective output modules.
Reference: [81] <author> Jordan, M., </author> <title> Attractor Dynamics and Parallelism in a Connectionism Sequential Machine, </title> <booktitle> Program of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 531-546, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1986. </year>
Reference-contexts: Usually, the neurons of a feedback net 6 work are classified into input, hidden, and output neurons functionally but not architecturally. Perceptrons [155] and multi-layer Perceptron [156] are two examples of feedforward networks. Elman network [33] and Jordan network <ref> [81] </ref> are two examples of recurrent networks. <p> [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron [40, 41] which can be trained with supervision to recognize handwritten characters; and recurrent neural networks <ref> [33, 81, 144] </ref> which allow recursive processing on input string of variable length.
Reference: [82] <author> Kitano, H., </author> <title> Speech-to-Speech Translation: A Massively Parallel Memory Based Approach, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1994. </year>
Reference-contexts: level languages, knowledge bases which store facts and rules in relational form, fact and rule tables used in unification process of logic programming systems, keyword tables (inverted and signature files) used in information retrieval applications [37], and machine-readable lexicons, dictionary as well as varieties of tables used in memory-based parsing <ref> [82] </ref> for natural language processing. In such tables, every table entry is an associated input-output ordered pair.
Reference: [83] <author> Kleene, S. C., </author> <title> Representation of Events in Nerve Nets and Finite Automata, in: Automata Studies, </title> <editor> Shannon, C. E. and McCarthy, J. </editor> <publisher> (Ed.), </publisher> <pages> pp. 3-42, </pages> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1956. </year>
Reference-contexts: The capabilities of neural network models (in particular, recurrent networks of threshold logic units or McCulloch-Pitts neurons) in processing and generating sequences (strings defined over some finite alphabet) and hence their formal equivalence with finite state automata or regular language generators/recognizers have been known for several decades <ref> [83, 108, 117] </ref>. More recently, recurrent neural network realizations of finite state automata for recognition and learning of finite state (regular) languages have been explored by numerous authors [6, 20, 33, 38, 45, 44, 77, 122, 129, 132, 133, 134, 159, 166, 192].
Reference: [84] <author> Kohonen, T., </author> <title> Correlation matrix memories, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. c-21, no. 4, </volume> <pages> pp. 353-359, </pages> <month> Apr. </month> <year> 1972. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson [10], Fukushima [39], Grossberg [51, 52, 53], Kohonen <ref> [84, 85] </ref>, and many other researchers ultimately brought in the renascence of ANN in 1980s. <p> The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks [75], correlation matrix memories <ref> [84] </ref>, bidirectional associative memories [90], among others [9, 59, 91, 125]. <p> Thus, a linear associative memory with n input neurons can store and recall perfectly at most n pattern associations. Similar storage capacity results are known for several content-addressed memory models such as the Hopfield network [75, 109], bi-directional associative memories [90], correlation matrix memories <ref> [84] </ref>, etc. A variety of associative memory models are discussed in [62, 93]. As already pointed out, many simple content-addressed memory models studied in the literature are incapable of stable storage and recall of associations between arbitrary pairs of patterns (except under certain restricted circumstances).
Reference: [85] <author> Kohonen, T., </author> <title> Associative Memory: A System-Theoretical Approach, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. In the dark ages of the 1970s, the 8 dedicated and everlasting efforts of Amari [7, 8, 9], Anderson [10], Fukushima [39], Grossberg [51, 52, 53], Kohonen <ref> [84, 85] </ref>, and many other researchers ultimately brought in the renascence of ANN in 1980s.
Reference: [86] <author> Kohonen, T., </author> <title> Self-Organization and Associative Memory, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: Other representative ANN models in the bright 1980s, to name a few, include Hinton, Sejnowski, and Ackley's Boltzmann machine model [1, 67] which can be used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map <ref> [86] </ref> which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial
Reference: [87] <editor> Kohonen, T., Content-Addressable Memories, 2nd ed., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: Let us consider the time needed for locating a record pointer using a hash function in current computer systems. Commonly used hash functions are based on multiplication, division and addition operations <ref> [87, 163] </ref>. In hardware implementation addition is faster than multiplication which in turn is far faster than division.
Reference: [88] <author> Kogge, P., Oldfield, J., Brule, M. and Stormon, C., </author> <title> VLSI and Rule-based Systems, </title> <booktitle> in: VLSI for Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. and Moore, W. R. </editor> <publisher> (Ed.), </publisher> <pages> pp. 95-108, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1989. </year> <month> 146 </month>
Reference-contexts: As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [89] <author> Kosko, B., </author> <title> Adaptive Bidirectional Associative Memories, </title> <journal> Applied Optics, </journal> <volume> vol. 26, no. 23, </volume> <pages> pp. 4947-4960, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: to name a few, include Hinton, Sejnowski, and Ackley's Boltzmann machine model [1, 67] which can be used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM <ref> [89, 90] </ref> which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to
Reference: [90] <author> Kosko, B., </author> <title> Bidirectional Associative Memories, </title> <journal> IEEE Transactions on System, Man, and Cybernetics, </journal> <volume> vol. 18, no. 1, </volume> <pages> pp. 49-60, </pages> <address> Jan./Feb. </address> <year> 1988. </year>
Reference-contexts: Perceptrons [155] and multi-layer Perceptron [156] are two examples of feedforward networks. Elman network [33] and Jordan network [81] are two examples of recurrent networks. Hopfield networks [75] and BAM <ref> [90] </ref> two examples of feedback networks. 1.1.4 Implementation of artificial neural networks Due to the computations required by enormous neural nodes to calculate their thresholded activation and weighted sum on the inputs from their associated input connections in an ANN, ANN systems generally require more intensive computational power but simpler types <p> to name a few, include Hinton, Sejnowski, and Ackley's Boltzmann machine model [1, 67] which can be used to find the global optimum solution for a given problem; Kohonen's Self-Organizing Feature Map [86] which can be trained without supervision to find the organization of relationships among training patterns; Kosko's BAM <ref> [89, 90] </ref> which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to <p> The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks [75], correlation matrix memories [84], bidirectional associative memories <ref> [90] </ref>, among others [9, 59, 91, 125]. A precise definition of binary/bipolar associative memories follows: Let D H (u; u ) denote the Hamming distance between binary (bipolar) vectors u and u 0 Hamming distance is the number of bits that differ between two binary (bipolar) vectors. <p> Thus, a linear associative memory with n input neurons can store and recall perfectly at most n pattern associations. Similar storage capacity results are known for several content-addressed memory models such as the Hopfield network [75, 109], bi-directional associative memories <ref> [90] </ref>, correlation matrix memories [84], etc. A variety of associative memory models are discussed in [62, 93]. As already pointed out, many simple content-addressed memory models studied in the literature are incapable of stable storage and recall of associations between arbitrary pairs of patterns (except under certain restricted circumstances).
Reference: [91] <author> Kumagai, Y., Kamruzzaman, J. and Hikita, H., </author> <title> Further Cross Talk Reduction of Associative Memory and Exact Data Retrieval, </title> <booktitle> Proc. of IJCNN, </booktitle> <volume> vol 3, </volume> <pages> pp. 1371-1378, </pages> <address> San Francisco, </address> <year> 1993. </year>
Reference-contexts: The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks [75], correlation matrix memories [84], bidirectional associative memories [90], among others <ref> [9, 59, 91, 125] </ref>. A precise definition of binary/bipolar associative memories follows: Let D H (u; u ) denote the Hamming distance between binary (bipolar) vectors u and u 0 Hamming distance is the number of bits that differ between two binary (bipolar) vectors.
Reference: [92] <author> Kumar, R., NCMOS: </author> <title> A High Performance CMOS Logic, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 29, no. 5, </volume> <pages> pp. 631-633, </pages> <year> 1994. </year>
Reference-contexts: The development of specialized hardware for implementation of ANN is still in its early stages. Conventional CMOS technology that is currently the main technology for VLSI implementation of ANN is known to be slow <ref> [92, 104] </ref>. Other technologies, such as BiCMOS, NCMOS [92], pseudo-NMOS logic, standard N-P domino logic, and quasi N-P domino logic [104], may provide better performance for the realization of ANN. <p> The development of specialized hardware for implementation of ANN is still in its early stages. Conventional CMOS technology that is currently the main technology for VLSI implementation of ANN is known to be slow [92, 104]. Other technologies, such as BiCMOS, NCMOS <ref> [92] </ref>, pseudo-NMOS logic, standard N-P domino logic, and quasi N-P domino logic [104], may provide better performance for the realization of ANN.
Reference: [93] <author> Kung, S. Y., </author> <title> Digital Neural Networks, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: It also provides for real-time update of memory contents by one-shot learning without interference with other stored patterns. It is generally agreed that artificial neural networks (ANN) have demonstrated success in low-level perceptual tasks (e.g., signal processing, pattern recognition) <ref> [62, 93, 111, 113] </ref>. <p> Similar storage capacity results are known for several content-addressed memory models such as the Hopfield network [75, 109], bi-directional associative memories [90], correlation matrix memories [84], etc. A variety of associative memory models are discussed in <ref> [62, 93] </ref>. As already pointed out, many simple content-addressed memory models studied in the literature are incapable of stable storage and recall of associations between arbitrary pairs of patterns (except under certain restricted circumstances).
Reference: [94] <author> Lacher, R. C. and Nguyen, K. D., </author> <title> Hierarchical Architectures for Reasoning, in: Computational Architectures Integrating Neural and Symbolic Processes : A Perspective on the State of the Art, Sun, </title> <editor> R. and Bookman, L. (Ed.), </editor> <volume> Chapter 4, </volume> <pages> pp. 117-150, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1995. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems <ref> [94, 145] </ref>. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems. This dissertation explores to integrate ANN and some essential symbolic computations for content-based associative symbolic processing.
Reference: [95] <author> Lange, T. and Dyer, M., </author> <title> Frame Selection in a Connectionist Model, </title> <booktitle> Proceedings of the 11th Cognitive Science Conference, </booktitle> <pages> pp. 706-713, </pages> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1989. </year>
Reference-contexts: Many of the systems proposed in the literature are motivated by the need for massively parallel architecture for AI applications, and some of them are proposed to model human cognitive processes robustly. In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic <ref> [5, 31, 95, 175, 176] </ref>; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141].
Reference: [96] <author> Langley, P., </author> <title> Elements of Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: The surveys of grammar inference in general can be found in <ref> [69, 96, 115, 137] </ref>, and the recent results on grammar inference using neural networks can be found in [6, 13, 27, 38, 45, 44, 66, 77, 80, 116, 122, 123, 129, 159, 161, 166, 174, 192, 194, 197].
Reference: [97] <author> Lavington, S. H., Wang, C. J., Kasabov, N. and Lin, S., </author> <title> Hardware Support for Data Parallelism in Production Systems, </title> <booktitle> in: VLSI for Neural Networks and Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. </editor> <publisher> (Ed.), </publisher> <pages> pp. 231-242, </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Pattern-directed associative processing relies on associative pattern matching and retrieval, is central to many problem solving paradigms in AI (e.g., knowledge based expert systems, case based reasoning) as well as computer science (e.g., database query processing, information retrieval) <ref> [54, 97, 181] </ref>, and dominates the computational requirements of many applications in AI and computer science [55, 97, 127]. This dissertation proposes methods to systematically design massively parallel architectures for pattern-directed symbol processing using neural associative memories as key components. <p> on associative pattern matching and retrieval, is central to many problem solving paradigms in AI (e.g., knowledge based expert systems, case based reasoning) as well as computer science (e.g., database query processing, information retrieval) [54, 97, 181], and dominates the computational requirements of many applications in AI and computer science <ref> [55, 97, 127] </ref>. This dissertation proposes methods to systematically design massively parallel architectures for pattern-directed symbol processing using neural associative memories as key components. <p> A more general goal of this chapter is to explore the design of massively parallel architectures for symbol processing using the neural associative memories proposed in Chapter 2 as key components. Pattern-directed associative inference is an essential part of most AI systems <ref> [54, 97, 181] </ref> and dominates the computational requirements of many AI applications [55, 97, 127]. <p> Pattern-directed associative inference is an essential part of most AI systems [54, 97, 181] and dominates the computational requirements of many AI applications <ref> [55, 97, 127] </ref>. The proposed high performance neural architectures for syntax analysis are systematically (and provably correctly) synthesized through composition of the necessary symbolic functions using a set of component symbolic functions each of which is realized using a neural associative processor (memory). <p> As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [98] <author> LeCun, Y., </author> <title> Une Procedure D'apprentissage Pour Reseau a Seuil Assymetrique, </title> <booktitle> Proceedings of Cognitiva, </booktitle> <pages> pp. 599-604, </pages> <address> Paris, </address> <year> 1985. </year>
Reference-contexts: Since then, Backpropagation multi-layer perceptron has been successfully applied in a variety of applications and has became the most widely used neural network paradigm. The Backpropagation learning algorithm were independently derived by Werbos [193], Parker [138, 139], and LeCun <ref> [98] </ref>, but its popularity was mainly due to the effort of Rumelhart, McClelland, and the PDP Group.
Reference: [99] <author> Levine, D. S. and Aparicio IV, </author> <title> M.(Ed.), Neural Networks for Knowledge Representation and Inference, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year> <month> 147 </month>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> However, despite their generality (as computational models) and despite the potential advantages of using them as components in general-purpose artificial intelligence systems which usually involve content-based or memory-based knowledge storing and retrieving <ref> [47, 70, 72, 73, 99, 173, 179] </ref>, detailed design and performance tradeoffs in integrated systems of this sort are yet to be fully understood and working prototypes of such systems are only beginning to be developed. <p> Despite a large number of successful applications of ANN in aforementioned areas, their use in complex symbolic computing tasks (including storage and retrieval of records in large databases, and inference in deductive knowledge bases) is only beginning to be explored <ref> [21, 22, 23, 24, 47, 72, 99, 179] </ref>. Database query entails a process of content-based table lookup (associative search and retrieval) which is used in a wide variety of computing applications. <p> This latter viewpoint argues for a somewhat systematic exploration of this design space in search of novel and efficient computational architectures for such systems. This dissertation takes a few small steps in this direction and adds to the growing body of literature <ref> [47, 72, 99, 179] </ref> that demonstrates the potential benefits of integrated neural-symbolic architectures that overcome some of the limitations of today's ANN and AI systems.
Reference: [100] <author> Lewis, H. R. and Papadimitriou, C. H., </author> <title> Elements of the Theory of Computation, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Meyerowitz has suggested that the design of neural architectures capable of supporting dynamic representations for symbol manipulation is one of the grand challenges of neural network research [113]. As shown by Church, Kleene, McCulloch, Post, Turing, and others through their work on the theory of Computation <ref> [117, 100] </ref>, both AI and ANN represent particular realizations of a universal (Turing-equivalent) model of computation [185]. Thus, despite assertions by some to the contrary, any task that can be realized by one can, in principle, be accomplished by the other.
Reference: [101] <author> Linde, R., Gates, R. and Peng, T., </author> <title> Associative Processor Application to Real-time Data Management, </title> <booktitle> AFIPS, Proceedings of the National Computer Conference, </booktitle> <volume> vol. 42, </volume> <pages> pp. 187-195, </pages> <year> 1973. </year>
Reference-contexts: Therefore, many researchers have explored to augment conventional database systems with subsystems which effectively exploit associative processing to enhance the performance of the systems <ref> [30, 101, 121, 135, 157, 162, 196] </ref>. Many applications require associative table lookup mechanism or query processing system to be capable of retrieving items based on partial matches (some features of the input are noisy or missing) or retrieval of multiple records matching the specified query criteria.
Reference: [102] <author> Lippmann, R. P., </author> <title> An Introduction to Computing with Neural Nets, </title> <journal> IEEE ASSP Magazine, </journal> <pages> pp. 4-22, </pages> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: Figure 1.1 shows such a node. 4 1.1.2 Activation functions The types of activation functions used by an ANN affect its expressiveness, computational capabilities, and performance. Several typical activation functions are linear, binary sigmoidal, bipolar sigmoidal, binary hardlimiter, bipolar hardlimiter, gaussian, and ramp <ref> [56, 102] </ref> defined as follows: linear : f L (s) = cs, where s = P n binary sigmoidal : f S (s) = 1 1+e cs , where s = P n bipolar sigmoidal : f S (s) = 1e cs 1+e cs , where s = P n binary <p> A more detailed taxonomy of most neural network architectures and learning algorithms can be found in <ref> [56, 102, 112] </ref>. 9 1.3 An Overview of the Dissertation Artificial neural networks, due to their inherent parallelism, potential for fault tolerance, and adaptation through learning, offer an attractive computational paradigm for a variety of applications in computer science and engineering, artificial intelligence, robotics, and cognitive modeling.
Reference: [103] <author> Lont, J. B. and Guggenbuhl W., </author> <title> Analog CMOS Implementation of a Multilayer Perceptron with Nonlinear Synapses, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 457-465, </pages> <year> 1992. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding.
Reference: [104] <author> Lu, F. and Samueli, H., </author> <title> A 200-MHz CMOS Pipelined Multiplier-Accumulator Using a Quasi-Domino Dynamic Full-Adder Cell Design, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 28, no. 2, </volume> <pages> pp. 123-132, </pages> <year> 1993. </year>
Reference-contexts: The development of specialized hardware for implementation of ANN is still in its early stages. Conventional CMOS technology that is currently the main technology for VLSI implementation of ANN is known to be slow <ref> [92, 104] </ref>. Other technologies, such as BiCMOS, NCMOS [92], pseudo-NMOS logic, standard N-P domino logic, and quasi N-P domino logic [104], may provide better performance for the realization of ANN. <p> Conventional CMOS technology that is currently the main technology for VLSI implementation of ANN is known to be slow [92, 104]. Other technologies, such as BiCMOS, NCMOS [92], pseudo-NMOS logic, standard N-P domino logic, and quasi N-P domino logic <ref> [104] </ref>, may provide better performance for the realization of ANN.
Reference: [105] <author> MacLennan, B. J., </author> <booktitle> Principles of Programming Languages : Design, Evaluation, and Implementation, 2nd edition, </booktitle> <address> CBS College Publishing, New York, NY, </address> <year> 1987. </year>
Reference-contexts: The procedure for grammar parsing is the main module. In single-CPU computer systems, even assuming negligible overhead for parameter passing, a procedure call entails, at the very minimum, (1) saving the context of the caller procedure and activation of the callee procedure which typically requires 6 instructions <ref> [105] </ref>; and (2) context restoration and resumption of caller procedure upon the return (exit) of the callee procedure, which typically requires at least 3 instructions [105]). <p> call entails, at the very minimum, (1) saving the context of the caller procedure and activation of the callee procedure which typically requires 6 instructions <ref> [105] </ref>; and (2) context restoration and resumption of caller procedure upon the return (exit) of the callee procedure, which typically requires at least 3 instructions [105]). Thus, a procedure call entails a penalty of 9 instructions or about 9t ns. 6.4.1 Performance analysis of lexical analyzer Lexical analysis can be performed by a DFA whose transition function can be represented as a 2-dimensional table with current state and current input symbol as indices.
Reference: [106] <author> Masa, P., Hoen, K. and Wallinga, H., </author> <title> 70 Input, 20 Nanosecond Pattern Classifier, </title> <booktitle> IEEE International Joint Conference on Neural Networks, </booktitle> <volume> vol. </volume> <pages> 3, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding. <p> operation [48]. [184] reports a measured propagation delay of 104ns in a digital circuit with each synapse containing an 8-bit memory, an 8-bit subtractor and an 8-bit adder. [50] reports throughput at the rate of 10MHz (or equivalently, delay of 100ns) in a Hamming Net pattern classifier using analog circuits. <ref> [106] </ref> describes a hybrid analog-digital design with 5-bit (4 bits + sign) binary synapse weight values and current-summing circuits that is used to realize a 2-layer feed-forward ANN with a network computation delay of less than 20ns. <p> The 1st-layer and 2nd-layer subnetworks of the proposed neural architecture for database query processing are very similar to the 1st-layer subnetwork of a Hamming Net respectively, and the neural architecture with 2 connection layers in the proposed ANN is exactly same as that implemented by <ref> [106] </ref> except [106] uses discretized inputs, 5-bit synaptic weights, and sigmoid-like activation function. The proposed ANN uses bipolar inputs, weights in f1; 0; 1g 54 and binary hardlimiter as activation function. <p> The 1st-layer and 2nd-layer subnetworks of the proposed neural architecture for database query processing are very similar to the 1st-layer subnetwork of a Hamming Net respectively, and the neural architecture with 2 connection layers in the proposed ANN is exactly same as that implemented by <ref> [106] </ref> except [106] uses discretized inputs, 5-bit synaptic weights, and sigmoid-like activation function. The proposed ANN uses bipolar inputs, weights in f1; 0; 1g 54 and binary hardlimiter as activation function.
Reference: [107] <author> Massengill, L. W. and Mundie, D. B., </author> <title> An Analog Neural Network Hardware Implementation Using Charge-Injection Multipliers and Neuron-Specific Gain Control, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 354-362, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding.
Reference: [108] <author> McCulloch, W. S. and Pitts, W., </author> <title> A Logical Calculus of Ideas Immanent in Nervous Activity, </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> vol. 5, </volume> <pages> pp. 115-133, </pages> <year> 1943. </year>
Reference-contexts: The development of formal mathematical models for ANN can be traced back to early 1940s in the work by McCulloch and Pitts <ref> [108] </ref>, which showed that any logical proposition can be represented by a network of interconnected neurons of two states if enough neurons are provided. <p> The capabilities of neural network models (in particular, recurrent networks of threshold logic units or McCulloch-Pitts neurons) in processing and generating sequences (strings defined over some finite alphabet) and hence their formal equivalence with finite state automata or regular language generators/recognizers have been known for several decades <ref> [83, 108, 117] </ref>. More recently, recurrent neural network realizations of finite state automata for recognition and learning of finite state (regular) languages have been explored by numerous authors [6, 20, 33, 38, 45, 44, 77, 122, 129, 132, 133, 134, 159, 166, 192].
Reference: [109] <author> McEliece, R. J., Posner, E. C., Rodemich, E. R. and Venkatesh, S. S., </author> <title> The Capacity of the Hopfield Associative Memory, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-33, no. 4, </volume> <pages> pp. 461-482, </pages> <month> July </month> <year> 1987. </year> <month> 148 </month>
Reference-contexts: Thus, a linear associative memory with n input neurons can store and recall perfectly at most n pattern associations. Similar storage capacity results are known for several content-addressed memory models such as the Hopfield network <ref> [75, 109] </ref>, bi-directional associative memories [90], correlation matrix memories [84], etc. A variety of associative memory models are discussed in [62, 93].
Reference: [110] <author> McGregor, D., McInnes, S. and Henning, M., </author> <title> An Architecture for Associative Processing of Large Knowledge Bases (LKBs), </title> <journal> Computer Journal, </journal> <volume> vol. 30, no. 5, </volume> <pages> pp. 404-412, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [111] <author> McKenna, T. M., </author> <title> The Role of Interdisciplinary Research Involving Neuroscience in the Development of Intelligent Systems, in: Artificial Intelligence and Neural Networks: Steps Towards Principled Integration, Honavar, </title> <editor> V. and Uhr, L. (Ed.), </editor> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: It also provides for real-time update of memory contents by one-shot learning without interference with other stored patterns. It is generally agreed that artificial neural networks (ANN) have demonstrated success in low-level perceptual tasks (e.g., signal processing, pattern recognition) <ref> [62, 93, 111, 113] </ref>.
Reference: [112] <author> Medsker, L. R., </author> <title> Hybrid Neural Network and Expert Systems, Chapter 1, </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1994. </year>
Reference-contexts: A more detailed taxonomy of most neural network architectures and learning algorithms can be found in <ref> [56, 102, 112] </ref>. 9 1.3 An Overview of the Dissertation Artificial neural networks, due to their inherent parallelism, potential for fault tolerance, and adaptation through learning, offer an attractive computational paradigm for a variety of applications in computer science and engineering, artificial intelligence, robotics, and cognitive modeling.
Reference: [113] <author> Meyerowitz, A. L., </author> <title> Neural Networks: A Computer Science Perspective, </title> <journal> Naval Research Review, </journal> <volume> vol. 43, No. 2. </volume> <pages> pp. 13-18. </pages>
Reference-contexts: Meyerowitz has suggested that the design of neural architectures capable of supporting dynamic representations for symbol manipulation is one of the grand challenges of neural network research <ref> [113] </ref>. As shown by Church, Kleene, McCulloch, Post, Turing, and others through their work on the theory of Computation [117, 100], both AI and ANN represent particular realizations of a universal (Turing-equivalent) model of computation [185]. <p> It also provides for real-time update of memory contents by one-shot learning without interference with other stored patterns. It is generally agreed that artificial neural networks (ANN) have demonstrated success in low-level perceptual tasks (e.g., signal processing, pattern recognition) <ref> [62, 93, 111, 113] </ref>.
Reference: [114] <author> Micchelli, C. A., </author> <title> Interpolation of Scattered Data: Distance Matrices and Conditionally Positive Definite Functions, </title> <booktitle> Constructive Approximation, </booktitle> <pages> pp. 11-22, </pages> <year> 1986. </year>
Reference-contexts: trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method <ref> [114, 146, 147] </ref> which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron
Reference: [115] <author> Miclet, L., </author> <title> Structural Methods in Pattern Recognition, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The surveys of grammar inference in general can be found in <ref> [69, 96, 115, 137] </ref>, and the recent results on grammar inference using neural networks can be found in [6, 13, 27, 38, 45, 44, 66, 77, 80, 116, 122, 123, 129, 159, 161, 166, 174, 192, 194, 197].
Reference: [116] <author> Miikkulainen, </author> <title> R, Subsymbolic Parsing of Embedded Structures, in: Computational Architectures Integrating Neural and Symbolic Processes : A Perspective on the State of the Art, Sun, </title> <editor> R. and Bookman, L. (Ed.), </editor> <volume> Chapter 5, </volume> <pages> pp. 153-186, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1995. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> Both the test and training sets were based on conference registration dialogs from a vocabulary of about 400 words. SPEC <ref> [116] </ref> is a modular neural parser which parses variable-length sentences with embedded clauses and produces case-role representations as output. <p> RAAM has been used by several researchers to implement stacks in connectionist designs for parsers <ref> [13, 66, 116] </ref>. A RAAM is a 2-layer Perceptron with recurrent links from hidden neurons to part of input neurons and from part of output neurons to hidden neurons. <p> The performance of a RAAM stack is known to degrade substantially with increase in depth of the stack, and the number of hidden neurons needed for encoding a stack of a given depth has to be determined through a process of trial and error <ref> [116] </ref>. A RAAM stack has to be trained for each application. Other drawbacks associated with the use of RAAM as a stack are discussed in [174].
Reference: [117] <author> Minsky, M., </author> <title> Computation: Finite and Infinite Machines, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1967. </year>
Reference-contexts: Meyerowitz has suggested that the design of neural architectures capable of supporting dynamic representations for symbol manipulation is one of the grand challenges of neural network research [113]. As shown by Church, Kleene, McCulloch, Post, Turing, and others through their work on the theory of Computation <ref> [117, 100] </ref>, both AI and ANN represent particular realizations of a universal (Turing-equivalent) model of computation [185]. Thus, despite assertions by some to the contrary, any task that can be realized by one can, in principle, be accomplished by the other. <p> The capabilities of neural network models (in particular, recurrent networks of threshold logic units or McCulloch-Pitts neurons) in processing and generating sequences (strings defined over some finite alphabet) and hence their formal equivalence with finite state automata or regular language generators/recognizers have been known for several decades <ref> [83, 108, 117] </ref>. More recently, recurrent neural network realizations of finite state automata for recognition and learning of finite state (regular) languages have been explored by numerous authors [6, 20, 33, 38, 45, 44, 77, 122, 129, 132, 133, 134, 159, 166, 192].
Reference: [118] <author> Minsky, M. and Papert, S., </author> <title> Perceptrons: An Introduction to Computational Geometry, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference-contexts: In 1969, Minsky and Papert showed in their landmark book Perceptrons that the computational power of perceptron's single-layer learning algorithm is only able to solve linearly separable problem but not a large class of other problems <ref> [118] </ref>. With the misinterpretation of such a result, research funding and interest in ANN drastically dropped in the following 1970s. <p> is input value at input neuron j, and f H is binary hardlimiter function, where f H (x) = &gt; &lt; 1 if x 0 0 otherwise (2.5) It is well known that such a 1-layer Perceptron can implement only linearly separable functions from R n to f0; 1g m <ref> [118] </ref>.
Reference: [119] <author> Mjolsness, E., </author> <title> Connectionist Grammars for High-Level Vision, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 423-451, </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year> <month> 149 </month>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision <ref> [11, 119] </ref>, natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems.
Reference: [120] <author> Moon, G. et al., </author> <title> VLSI Implementation of Synaptic Weighting and Summing in Pulse Coded Neural-Type Cells, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 394-403, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding.
Reference: [121] <author> Moulder, R., </author> <title> An Implementation of a Data Management system on associative Processor, </title> <booktitle> AFIPS, Proceedings of the National Computer Conference, </booktitle> <volume> vol. 42, </volume> <pages> pp. 171-179, </pages> <year> 1973. </year>
Reference-contexts: Therefore, many researchers have explored to augment conventional database systems with subsystems which effectively exploit associative processing to enhance the performance of the systems <ref> [30, 101, 121, 135, 157, 162, 196] </ref>. Many applications require associative table lookup mechanism or query processing system to be capable of retrieving items based on partial matches (some features of the input are noisy or missing) or retrieval of multiple records matching the specified query criteria.
Reference: [122] <author> Mozer, M. C. and Bachrach, J., </author> <title> Discovering the Structure of a Reactive Environment by Exploration, </title> <journal> Neural Computation, </journal> <volume> vol. 2., no. 4., </volume> <editor> p. </editor> <volume> 447, </volume> <year> 1990. </year>
Reference: [123] <author> Mozer, M. C. and Das, S., </author> <title> A Connectionist Symbol Manipulator that Discovers the Structure of Context-Free Languages, </title> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> p. 863, </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing.
Reference: [124] <author> Naganuma, J., Ogura, T., Yamada, S. I. and Kimura, T., </author> <title> High-Speed CAM-Based Architecture for a Prolog Machine (ASCA), </title> <journal> IEEE transactions on Computers, </journal> <volume> vol. 37, no. 11, </volume> <pages> pp. 1375-1383, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [125] <author> Nakano, K., </author> <title> Associatron A Model of Associative Memory, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-2, no. 3, </volume> <pages> pp. 380-388, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: The term associative memory (AM) or content-addressed memory refers to a memory system where recall of a stored pattern is accomplished by providing a noisy or partially specified input pattern. Examples of such memory models include Hopfield networks [75], correlation matrix memories [84], bidirectional associative memories [90], among others <ref> [9, 59, 91, 125] </ref>. A precise definition of binary/bipolar associative memories follows: Let D H (u; u ) denote the Hamming distance between binary (bipolar) vectors u and u 0 Hamming distance is the number of bits that differ between two binary (bipolar) vectors.
Reference: [126] <author> Newell, A., </author> <title> Symbol Systems, </title> <journal> Cognitive Science vol. </journal> <volume> 4, </volume> <pages> pp. 135-183, </pages> <year> 1980. </year>
Reference-contexts: In cognitive models and artificial intelligence programs based on Von Neumann model of computation, i.e., models within the so-called symbolic paradigm <ref> [126] </ref>, address-based memory often serves as the working memory (or scratch-pad) for storing intermediate results during the execution of a program.
Reference: [127] <author> Ng, Y. H., Glover, R. J. and Chng, C. L., </author> <title> Unify with active Memory, </title> <booktitle> in: VLSI for Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. and Moore, W. R. </editor> <publisher> (Ed.), </publisher> <pages> pp. 109-118, </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1989. </year>
Reference-contexts: on associative pattern matching and retrieval, is central to many problem solving paradigms in AI (e.g., knowledge based expert systems, case based reasoning) as well as computer science (e.g., database query processing, information retrieval) [54, 97, 181], and dominates the computational requirements of many applications in AI and computer science <ref> [55, 97, 127] </ref>. This dissertation proposes methods to systematically design massively parallel architectures for pattern-directed symbol processing using neural associative memories as key components. <p> Pattern-directed associative inference is an essential part of most AI systems [54, 97, 181] and dominates the computational requirements of many AI applications <ref> [55, 97, 127] </ref>. The proposed high performance neural architectures for syntax analysis are systematically (and provably correctly) synthesized through composition of the necessary symbolic functions using a set of component symbolic functions each of which is realized using a neural associative processor (memory). <p> As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [128] <author> Niranjan, M. and Fallside, F., </author> <title> Neural Networks and Radial Basis Functions in Classifying Static Speech Patterns, </title> <type> Report CUED/FINFENG/TR 22, </type> <institution> University Engineering Department, Cambridge University, </institution> <address> England, </address> <year> 1988. </year>
Reference-contexts: can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to other applications <ref> [128, 149] </ref>; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron [40, 41] which can be trained with supervision to recognize handwritten characters; and recurrent neural networks [33,
Reference: [129] <author> Noda, I. and Nagao, M., </author> <title> A Learning Method for Recurrent Neural Networks Based on Minimization of Finite Automata, </title> <booktitle> Proceedings of International Joint Conference on Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 27-32, </pages> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <year> 1992. </year> <month> 150 </month>
Reference: [130] <author> Norman, D. A., </author> <title> Reflections on Cognition and Parallel Distributed Processing, in: Parallel Distributed Processing, </title> <editor> McClelland, J., Rumelhard, D. and the PDP Research Group (Ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: It is often suggested that traditionally serial symbol processing systems of AI and inherently massively parallel artificial neural networks (ANN) offer two radically, perhaps even irreconcilably different paradigms for modelling minds and brains | both artificial as well as natural <ref> [130, 160] </ref>. AI has been successful in applications such as theorem proving, knowledge-based expert systems, mathematical reasoning, syntax analysis, and related applications which mainly involve systematic symbol manipulation.
Reference: [131] <author> Oh, H., </author> <title> The Relaxation Method for Learning in Artificial Neural Networks, </title> <type> Ph.D. Dissertation, </type> <institution> Iowa State University, </institution> <year> 1992. </year>
Reference-contexts: Their simplicity allows simple and efficient hardware implementation of such threshold functions. 1.1.3 Types of artificial neural networks and their computational capabilities ANN can be mainly classified into three basic categories : feedforward networks, feedback networks and recurrent networks <ref> [28, 56, 131] </ref> according to their architectures, functionalities, and signal propagation direction of their connections. The output of a feedforward network is a function of current input, and its connections are unidirectional.
Reference: [132] <author> Omlin, C. W. and Giles, C. L., </author> <title> Constructing Deterministic Finite-State Automata in Sparse Recurrent Neural Networks, </title> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <volume> vol. 3, </volume> <pages> pp. </pages> <address> 1732- 1737, Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference: [133] <author> Omlin, C. and Giles, C. L., </author> <title> Extraction and Insertion of Symbolic Information in Recurrent Neural Networks, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 271-299, </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference: [134] <author> Omlin, C. and Giles, C. L., </author> <title> Stable Encoding of Large Finite-State Automata in Recurrent Neural Networks with Sigmoid Discriminants, </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 675-696, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Notable exceptions are: connectionist realizations of Turing Machines (wherein a stack is simulated using binary representation of a fractional number) [143, 169]; a few neural architectures designed for parsing based on a known grammar [34, 164]; and neural network realizations of finite state automata <ref> [20, 134] </ref>. Nevertheless, it is informative to examine the various proposals for neural architectures for syntax analysis (regardless of whether the grammar is preprogrammed or learned).
Reference: [135] <author> Ozkarahan, E. A., </author> <title> Evolution and Implementation of the RAP Database Machine, </title> <journal> New Generation Computing, </journal> <volume> 3, </volume> <pages> pp. 237-271, </pages> <year> 1985. </year>
Reference-contexts: Therefore, many researchers have explored to augment conventional database systems with subsystems which effectively exploit associative processing to enhance the performance of the systems <ref> [30, 101, 121, 135, 157, 162, 196] </ref>. Many applications require associative table lookup mechanism or query processing system to be capable of retrieving items based on partial matches (some features of the input are noisy or missing) or retrieval of multiple records matching the specified query criteria.
Reference: [136] <editor> Palm, G. et al., </editor> <booktitle> Knowledge Processing in Neural Architecture, in: VLSI for Neural Networks and Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. </editor> <publisher> (Ed.), </publisher> <pages> pp. 207-216, </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1994 </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system <ref> [136] </ref>, semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [137] <author> Parekh, R. G. and Honavar, V., </author> <title> Automata Induction, Grammar Inference, and Language Acquisition, </title> <booktitle> in: Handbook of Natural Language Processing, </booktitle> <editor> Moisl, H., Dale, R. and Somers, H. (Ed.), </editor> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: The surveys of grammar inference in general can be found in <ref> [69, 96, 115, 137] </ref>, and the recent results on grammar inference using neural networks can be found in [6, 13, 27, 38, 45, 44, 66, 77, 80, 116, 122, 123, 129, 159, 161, 166, 174, 192, 194, 197].
Reference: [138] <author> Parker, D. B., </author> <title> Learning Logic, </title> <type> Invention Report, S81-64, File 1, </type> <institution> Office of Technology Licensing, Stanford University, </institution> <year> 1982. </year> <month> 151 </month>
Reference-contexts: Since then, Backpropagation multi-layer perceptron has been successfully applied in a variety of applications and has became the most widely used neural network paradigm. The Backpropagation learning algorithm were independently derived by Werbos [193], Parker <ref> [138, 139] </ref>, and LeCun [98], but its popularity was mainly due to the effort of Rumelhart, McClelland, and the PDP Group.
Reference: [139] <author> Parker, D. B., </author> <title> Learning Logic, </title> <type> Technical Report TR-47, </type> <institution> Center for Computational Research in Economics and Management Science, MIT, </institution> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: Since then, Backpropagation multi-layer perceptron has been successfully applied in a variety of applications and has became the most widely used neural network paradigm. The Backpropagation learning algorithm were independently derived by Werbos [193], Parker <ref> [138, 139] </ref>, and LeCun [98], but its popularity was mainly due to the effort of Rumelhart, McClelland, and the PDP Group.
Reference: [140] <author> Peter, R., </author> <title> Recursive Functions in Computer Theory, </title> <publisher> Halsted Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Other recursive symbolic functions can also be composed using composition and recursion <ref> [140, 154, 195] </ref>. 76 The operation of a desired composite function on its symbolic input (string) can be fully characterized analytically in terms of its component symbolic functions on their respective symbolic inputs and outputs.
Reference: [141] <author> Pinkas, G., </author> <title> A Fault-Tolerant Connectionist Architecture for Construction of Logic Proofs, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 321-340, </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference <ref> [5, 31, 141, 167, 176] </ref>, computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving <ref> [141] </ref>. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [142] <author> Pinkas, G., </author> <title> Propositional Logic, Nonmonotonic Reasoning, and Symmetric Networks - On Bridging the Gap Between Symbolic and Connectionist Knowledge Representation, in: Neural Networks for Knowledge Representation and Inference, </title> <editor> Levine, D. S. </editor> <booktitle> and Aparicio IV, M.(Ed.), Chapter 7, </booktitle> <pages> pp. 175-203, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year>
Reference-contexts: In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning <ref> [142] </ref>, legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [143] <author> Pollack, J. B., </author> <title> On Connectionist Models of Language Processing, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, University of Illinois, Urbana-Champaign, IL, </institution> <year> 1987. </year>
Reference-contexts: Notable exceptions are: connectionist realizations of Turing Machines (wherein a stack is simulated using binary representation of a fractional number) <ref> [143, 169] </ref>; a few neural architectures designed for parsing based on a known grammar [34, 164]; and neural network realizations of finite state automata [20, 134]. Nevertheless, it is informative to examine the various proposals for neural architectures for syntax analysis (regardless of whether the grammar is preprogrammed or learned).
Reference: [144] <author> Pollack, J. B., </author> <title> Recursive Distributed Representations, </title> <booktitle> Artificial Intelligence 46, </booktitle> <pages> pp. 77-105, </pages> <year> 1990. </year>
Reference-contexts: [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron [40, 41] which can be trained with supervision to recognize handwritten characters; and recurrent neural networks <ref> [33, 81, 144] </ref> which allow recursive processing on input string of variable length. <p> There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> SPEC [116] is a modular neural parser which parses variable-length sentences with embedded clauses and produces case-role representations as output. SPEC consists of a parser which is a simple recurrent network, a stack which is realized using a recursive auto-associative memory (RAAM) <ref> [144] </ref>, and a segmenter which controls the push/pop operations of the stack using a 2-layer Perceptron. RAAM has been used by several researchers to implement stacks in connectionist designs for parsers [13, 66, 116].
Reference: [145] <author> Popescu, I., </author> <title> Hierarchical Neural Networks for Rules Control in Knowledge-Based Expert Systems, </title> <booktitle> Neural, Parallel & Scientific Computations 3, </booktitle> <pages> pp. 379-392, </pages> <year> 1995. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems <ref> [94, 145] </ref>. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems. This dissertation explores to integrate ANN and some essential symbolic computations for content-based associative symbolic processing. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65]. <p> In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning <ref> [145, 188] </ref>, nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [146] <author> Powell, M. J. D., </author> <title> Radial Basis Function for Multi-variable Interpolation: A Review, IMA Conference on Algorithms for the Approximation of Functions and Data, </title> <address> RMCS, Shrivenham, England. </address> <note> Also Report DAMTP/NA12, </note> <institution> Department of Applied Mathematics and Theoretical Physics, University of Cambridge, </institution> <year> 1985. </year>
Reference-contexts: trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method <ref> [114, 146, 147] </ref> which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron
Reference: [147] <author> Powell, M. J. D., </author> <title> Radial Basis Function for Multi-variable Interpolation: A Review, Algorithms for Approximation, </title> <editor> Mason, J. C. and Cox, M. G. </editor> <publisher> (Ed.), </publisher> <pages> pp. 143-167, </pages> <publisher> Oxford: Clarendon Press, </publisher> <year> 1987. </year> <month> 152 </month>
Reference-contexts: trained without supervision to find the organization of relationships among training patterns; Kosko's BAM [89, 90] which can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method <ref> [114, 146, 147] </ref> which was originally used for function interpolation and was also applied to other applications [128, 149]; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron
Reference: [148] <author> Raghupathi, "RP" W. et al., </author> <title> Toward Connectionist Representation of Legal Knowledge, in: Neural Networks for Knowledge Representation and Inference, </title> <editor> Levine, D. S. </editor> <booktitle> and Aparicio IV, M.(Ed.), Chapter 10, </booktitle> <pages> pp. 269-282, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year>
Reference-contexts: explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning <ref> [148] </ref>, commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [149] <author> Renals, S. and Rohwer, R., </author> <title> Phoneme Classification Experiments Using Radial Basis Functions, </title> <booktitle> Proceedings of the IEEE/INNS International Joint Conference on Neural Networks, </booktitle> <volume> vol. I, </volume> <pages> pp. 461-467, </pages> <address> Washington, D. C., </address> <month> June </month> <year> 1989. </year>
Reference-contexts: can serve as hetero-associative memory and temporal associative memory; Carpenter and Gross-berg's ART networks [16, 17, 18] which can be typically used to cluster training patterns via unsupervised training; Radial Basis Function method [114, 146, 147] which was originally used for function interpolation and was also applied to other applications <ref> [128, 149] </ref>; Hecht-Nielsen's Counterpropagation network [64] which has both supervised as well as unsupervised training stages and can be trained to perform pattern mapping, data compression and associative recall; Fukushima, Miyake, and Ito's Neocognitron [40, 41] which can be trained with supervision to recognize handwritten characters; and recurrent neural networks [33,
Reference: [150] <author> Ripley, B. D., </author> <title> Pattern Recognition and Neural Networks, </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: AI has been successful in applications such as theorem proving, knowledge-based expert systems, mathematical reasoning, syntax analysis, and related applications which mainly involve systematic symbol manipulation. On the other hand, ANN have been particularly successful in applications such as pattern recognition, function approximation, and nonlinear control <ref> [60, 150] </ref> which involve primarily numeric computation. Meyerowitz has suggested that the design of neural architectures capable of supporting dynamic representations for symbol manipulation is one of the grand challenges of neural network research [113].
Reference: [151] <author> Robinson, I., </author> <title> The Pattern Addressable Memory: Hardware for Associative Processing, </title> <booktitle> in: VLSI for Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. and Moore, W. R. </editor> <publisher> (Ed.), </publisher> <pages> pp. 119-129., </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1989. </year>
Reference-contexts: As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [152] <author> Robinson, M. E. et al., </author> <title> A Modular CMOS Design of a Hamming Network, </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 444-456, </pages> <year> 1992. </year>
Reference-contexts: Next, the operation of conventional database systems is examined, and their performance is estimated and compared to that of the proposed neural architecture. 3.3.1 Performance of current electronic realization for neural networks Electronic hardware realizations of ANN have been explored by several authors <ref> [49, 50, 57, 103, 106, 107, 120, 152, 184, 190] </ref>. Such implementations typically employ CMOS analog, digital, or hybrid (analog/digital) electronic circuits. Analog circuits typically consist of processing elements for multiplication, summation and thresholding.
Reference: [153] <author> Rodohan, D. and Glover, R., </author> <title> A Distributed Parallel Associative Processor (DPAP) for the Execution of Logic Programs, </title> <booktitle> in: VLSI for Neural Networks and Artificial Intelligence, </booktitle> <editor> Delgado-Frias, J. G. </editor> <publisher> (Ed.), </publisher> <pages> pp. 265-273, </pages> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: As a result, design, VLSI implementation, and applications of associative processors have been studied extensively in the literature <ref> [21, 23, 68, 78, 88, 97, 110, 124, 127, 151, 153] </ref>. The neural network architecture proposed in this chapter for syntax analysis demonstrates the versatility of neural associative processors (memories) as generic building blocks for systematic synthesis of modular massively parallel architectures for symbol processing applications.
Reference: [154] <author> Rogers, Jr., H., </author> <title> Theory of Recursive Functions and Effective Computability, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Other recursive symbolic functions can also be composed using composition and recursion <ref> [140, 154, 195] </ref>. 76 The operation of a desired composite function on its symbolic input (string) can be fully characterized analytically in terms of its component symbolic functions on their respective symbolic inputs and outputs.
Reference: [155] <author> Rosenblatt, F., </author> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms, </title> <publisher> Spartan Books, </publisher> <address> Washington, D.C., </address> <year> 1962. </year>
Reference-contexts: Usually, the neurons of a feedback net 6 work are classified into input, hidden, and output neurons functionally but not architecturally. Perceptrons <ref> [155] </ref> and multi-layer Perceptron [156] are two examples of feedforward networks. Elman network [33] and Jordan network [81] are two examples of recurrent networks. <p> In 1949, Hebb proposed the first learning rule for neurons [63]. In late 1950s and early 1960s, Rosenblatt introduced a class of neural networks <ref> [155] </ref>, called perceptrons, which can learn to classify patterns through supervised learning. Rosenblatt's work helped produce a large amount of research activities in this early ANN research era.
Reference: [156] <author> Rumelhart, D. E., McClelland, J. L. </author> <title> and the PDP Research Group, Parallel Distributed Processing: </title> <journal> Explorations in the Microstructure of Cognition, </journal> <volume> vol. 1: </volume> <booktitle> Foundations, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year> <month> 153 </month>
Reference-contexts: Usually, the neurons of a feedback net 6 work are classified into input, hidden, and output neurons functionally but not architecturally. Perceptrons [155] and multi-layer Perceptron <ref> [156] </ref> are two examples of feedforward networks. Elman network [33] and Jordan network [81] are two examples of recurrent networks. <p> The tremendous resurgence of ANN research interest in 1980s was mainly due to the invention of Hopfield networks [75] which can serve as content-addressable memory or solve combinatorial optimization problems [76], and the introduction of Backpropagation learning algorithm <ref> [156] </ref> which overcomes the limitation of perceptron's single-layer learning algorithm in linearly separable problems and can be exploited to train multi-layer perceptron to solve nonlinearly separable problems.
Reference: [157] <author> Salton, G. and McGill, M. J., </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Therefore, many researchers have explored to augment conventional database systems with subsystems which effectively exploit associative processing to enhance the performance of the systems <ref> [30, 101, 121, 135, 157, 162, 196] </ref>. Many applications require associative table lookup mechanism or query processing system to be capable of retrieving items based on partial matches (some features of the input are noisy or missing) or retrieval of multiple records matching the specified query criteria. <p> Thus, the performance of the hardware implementation of ANN is likely to improve with technological advances in VLSI. 3.3.2 Analysis of query processing in conventional computer systems Accessing information based on a key is central to information retrieval systems <ref> [37, 157, 158] </ref> and database systems [186]. In relational database systems implemented on conventional computer systems, given the value for a key, a record is located efficiently by using key-based organizations including hashing, index-sequential access files and B-trees [186].
Reference: [158] <author> Salton, G., </author> <title> Automatic Text Processing: The Transformation, Analysis, </title> <booktitle> and Retrieval of Information by Computer, </booktitle> <address> Addison-Wesly, Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Thus, the performance of the hardware implementation of ANN is likely to improve with technological advances in VLSI. 3.3.2 Analysis of query processing in conventional computer systems Accessing information based on a key is central to information retrieval systems <ref> [37, 157, 158] </ref> and database systems [186]. In relational database systems implemented on conventional computer systems, given the value for a key, a record is located efficiently by using key-based organizations including hashing, index-sequential access files and B-trees [186].
Reference: [159] <author> Sanfeliu, A. and Alquezar, R., </author> <title> Understanding Neural Networks for Grammatical Inference and Recognition, in: Advances in Structural and Syntactic Pattern Recognition, Bunke, </title> <editor> H. (Ed.), </editor> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1992. </year>
Reference: [160] <author> Schneider, W., </author> <title> Connectionism: Is it a Paradigm Shift for Psychology?, Behavior Research Methods, Instruments, </title> <journal> and Computers, </journal> <volume> 19, </volume> <pages> pp. 73-83, </pages> <year> 1987. </year>
Reference-contexts: It is often suggested that traditionally serial symbol processing systems of AI and inherently massively parallel artificial neural networks (ANN) offer two radically, perhaps even irreconcilably different paradigms for modelling minds and brains | both artificial as well as natural <ref> [130, 160] </ref>. AI has been successful in applications such as theorem proving, knowledge-based expert systems, mathematical reasoning, syntax analysis, and related applications which mainly involve systematic symbol manipulation.
Reference: [161] <author> Schulenburg, D., </author> <title> Sentence Processing with Realistic Feedback, </title> <booktitle> IEEE/INNS International Joint Conference on Neural Networks, </booktitle> <volume> vol. IV, </volume> <pages> pp. 661-666, </pages> <address> Baltimore, MD, </address> <year> 1992. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing.
Reference: [162] <author> Schuster, S. A., Nguyen, H. B., Ozkarahan, E. A. and Smith, K. C., </author> <title> RAP.2 An Associative Processor for Databases and Its Applications, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-28, no. 6, </volume> <pages> pp. 446-458, </pages> <year> 1979. </year>
Reference-contexts: Therefore, many researchers have explored to augment conventional database systems with subsystems which effectively exploit associative processing to enhance the performance of the systems <ref> [30, 101, 121, 135, 157, 162, 196] </ref>. Many applications require associative table lookup mechanism or query processing system to be capable of retrieving items based on partial matches (some features of the input are noisy or missing) or retrieval of multiple records matching the specified query criteria.
Reference: [163] <author> Sedgewick, R., </author> <title> Algorithms, 2nd ed., </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: Let us consider the time needed for locating a record pointer using a hash function in current computer systems. Commonly used hash functions are based on multiplication, division and addition operations <ref> [87, 163] </ref>. In hardware implementation addition is faster than multiplication which in turn is far faster than division. <p> The end-of-input testing module is not shown in Figure 5.6. 5.5.3.7 Operation time complexity of the proposed NN NFA The time complexity of processing an input string of length n by an NFA directly implemented in single-processor computer systems is O (m 2 n) <ref> [163] </ref>, where m is the number of states in the NFA. The proposed NN NFA concurrently tracks all the possible nondeterministic transitions during the processing of an input string for a given NFA by exploiting the inherent parallelism in ANN.
Reference: [164] <author> Selman, B. and Hirst, G., </author> <title> A Rule-based Connectionist Parsing System, </title> <booktitle> Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <address> Irvine, CA, </address> <year> 1985. </year>
Reference-contexts: Notable exceptions are: connectionist realizations of Turing Machines (wherein a stack is simulated using binary representation of a fractional number) [143, 169]; a few neural architectures designed for parsing based on a known grammar <ref> [34, 164] </ref>; and neural network realizations of finite state automata [20, 134]. Nevertheless, it is informative to examine the various proposals for neural architectures for syntax analysis (regardless of whether the grammar is preprogrammed or learned).
Reference: [165] <author> Sequin, C. H. and Clay, R. D., </author> <title> Fault Tolerance in Artificial Neural Networks, </title> <booktitle> Proc. IJCNN, </booktitle> <volume> vol. 1, </volume> <pages> pp. 703-708, </pages> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Fault tolerance capabilities of neural architectures under different fault models (neuron faults, connection faults, etc) has been the topic of considerable research <ref> [21, 165, 180] </ref> and is 135 beyond the scope of this chapter.
Reference: [166] <author> Servan-Schreiber, D., Cleeremans, A. and McClelland, J. L., </author> <title> Graded State Machines: The Representation of Temporal Contingencies in Simple Recurrent Neural Networks, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 241-269, </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference: [167] <author> Shastri, L., </author> <title> A Connectionist Approach to Knowledge Representation and Limited Inference, </title> <journal> Cognitive Science, </journal> <volume> 12, </volume> <pages> pp. 331-392, </pages> <year> 1988. </year> <month> 154 </month>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference <ref> [5, 31, 141, 167, 176] </ref>, computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks <ref> [68, 167] </ref>, frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [168] <author> Shavlik, J. W., </author> <title> A Framework for Combining Symbolic and Neural Learning, in: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, Honavar, </title> <editor> V. and Uhr, L. </editor> <publisher> (Ed.), </publisher> <pages> pp. 561-580, </pages> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference [5, 31, 141, 167, 176], computer vision [11, 119], natural language processing [14, 32], learning <ref> [46, 69, 168] </ref>, and knowledge-based systems [94, 145]. We strongly believe that a judicious and systematic exploration of the design space of such systems is essential for understanding the nature of key cost-performance tradeoffs in the synthesis of intelligent systems.
Reference: [169] <author> Siegelman, H. T. and Sontag, E. D., </author> <title> Turing-Computability with Neural Nets, </title> <journal> Applied Mathematics Letters, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 77-80, </pages> <year> 1991. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> Notable exceptions are: connectionist realizations of Turing Machines (wherein a stack is simulated using binary representation of a fractional number) <ref> [143, 169] </ref>; a few neural architectures designed for parsing based on a known grammar [34, 164]; and neural network realizations of finite state automata [20, 134]. Nevertheless, it is informative to examine the various proposals for neural architectures for syntax analysis (regardless of whether the grammar is preprogrammed or learned).
Reference: [170] <author> Sippu, S. and Soisalon-Soininen, E., </author> <title> Parsing Theory, vol. II : LR(k) nad LL(k) Parsing, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Such grammars find extensive applications in programming languages and compilers. LR parsing is a linear time table-driven algorithm which is widely used for syntax analysis of computer programs <ref> [2, 19, 170] </ref>. This algorithm involves extensive pattern matching which suggests the consideration of a neural network implementation using associative memories. This section proposes a modular neural network architecture for parsing LR (1) grammars. <p> LR (k) parsers scan input from left to right and produce a rightmost derivation tree by using lookahead of k unscanned input symbols. Since any LR (k) grammar for k 1 can be transformed into an LR (1) grammar <ref> [170] </ref>, LR (1) parsers are sufficient for practical applications [74].
Reference: [171] <author> Sloan, M. E., </author> <title> Computer Hardware and Organization, </title> <institution> Science Research Associates, Chicago, </institution> <year> 1976. </year>
Reference-contexts: In this case, hierarchical memory organization using multiple levels of address decoding and multiple memory modules of the type specified above is a more practical alternative <ref> [171] </ref>. 19 2.1.4 Perceptrons A 1-layer Perceptron has n input neurons, m output neurons and one layer of connection weights.
Reference: [172] <author> Soucek, B. </author> <title> and the IRIS Group (Ed.), </title> <booktitle> Neural and Intelligent Systems Integrations: Fifth and Sixth Generation Integrated Reasoning Information Systems, </booktitle> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Of particular interest are alternative designs (including synergistic hybrids of ANN and AI designs) for intelligent systems <ref> [47, 65, 70, 72, 73, 99, 136, 172, 179, 185] </ref>. <p> Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored <ref> [22, 23, 47, 70, 72, 73, 99, 145, 172, 179, 185] </ref> and is currently viewed as one of important research goals in massively parallel computing and artificial intelligence [65].
Reference: [173] <author> Stanfill, C. and Waltz, D., </author> <title> Toward Memory-Based Reasoning, </title> <journal> Communications of the ACM 29, </journal> <pages> pp. 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: However, despite their generality (as computational models) and despite the potential advantages of using them as components in general-purpose artificial intelligence systems which usually involve content-based or memory-based knowledge storing and retrieving <ref> [47, 70, 72, 73, 99, 173, 179] </ref>, detailed design and performance tradeoffs in integrated systems of this sort are yet to be fully understood and working prototypes of such systems are only beginning to be developed.
Reference: [174] <author> Sun, G. Z., Giles, C. L., Chen, H. H. and Lee, Y. C., </author> <title> The Neural Network Pushdown Automation: Model, Stack and Learning Simulations, </title> <type> Technical Report UMIA CS-TR-93-77, </type> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: There has been considerable work on extending the computational capabilities of recurrent neural network models by providing some form of external memory in the form of a tape [194] or a stack <ref> [13, 27, 66, 116, 123, 144, 161, 169, 174, 197] </ref>. To the best of our knowledge, to date, most of the research on neural architectures for sequence processing has focused on the investigation of neural networks that are designed to learn to handle sequence processing. <p> A RAAM stack has to be trained for each application. Other drawbacks associated with the use of RAAM as a stack are discussed in <ref> [174] </ref>. <p> Augmenting SPEC with a lexical analyzer offers a way around this problem. <ref> [27, 174, 197] </ref> propose higher-order recurrent neural network equipped with an external stack to learn to recognize deterministic CFG, i.e., to learn to simulate a deterministic pushdown automata (DPDA). [27, 174] use an analog network coupled with a continuous stack and use a variant of a real time recurrent network learning <p> Augmenting SPEC with a lexical analyzer offers a way around this problem. [27, 174, 197] propose higher-order recurrent neural network equipped with an external stack to learn to recognize deterministic CFG, i.e., to learn to simulate a deterministic pushdown automata (DPDA). <ref> [27, 174] </ref> use an analog network coupled with a continuous stack and use a variant of a real time recurrent network learning algorithm to train the network. [197] uses a discrete network coupled with a discrete stack and employs a pseudo-gradient learning method to train the network.
Reference: [175] <author> Sun, R., </author> <title> On Variable Binding in Connectionist Networks, </title> <journal> Connection Science, </journal> <volume> vol. 4, no. 2, </volume> <pages> pp. 93-124, </pages> <year> 1992. </year>
Reference-contexts: Many of the systems proposed in the literature are motivated by the need for massively parallel architecture for AI applications, and some of them are proposed to model human cognitive processes robustly. In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic <ref> [5, 31, 95, 175, 176] </ref>; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141].
Reference: [176] <author> Sun, R., </author> <title> Logics and Variables in Connectionist Models: A Brief Overview, Symbolic Processors and Connectionist Networks for Artificial Intelligence and Cognitive Modeling, </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Examples of such systems include: neural architectures for information retrieval and database query processing [23, 24], generation of context-free languages [187], rule-based inference <ref> [5, 31, 141, 167, 176] </ref>, computer vision [11, 119], natural language processing [14, 32], learning [46, 69, 168], and knowledge-based systems [94, 145]. <p> Many of the systems proposed in the literature are motivated by the need for massively parallel architecture for AI applications, and some of them are proposed to model human cognitive processes robustly. In particular, they explore neural mechanisms for variable binding to facilitate complex reasoning based on predicate logic <ref> [5, 31, 95, 175, 176] </ref>; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning [177, 178], and logical theorem proving [141].
Reference: [177] <author> Sun, R., </author> <title> Connectionist Models of Commonsense Reasoning, in: Neural Networks for Knowledge Representation and Inference, </title> <editor> Levine, D. S. </editor> <booktitle> and Aparicio IV, M.(Ed.), Chapter 9, </booktitle> <pages> pp. 241-268, </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1994. </year> <month> 155 </month>
Reference-contexts: for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning <ref> [177, 178] </ref>, and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
Reference: [178] <author> Sun, R., </author> <title> A Two-Level Hybrid Architecture for Structuring Knowledge for Commonsense Reasoning, </title> <booktitle> in: Computational Architectures Integrating Neural and Symbolic Processes </booktitle> : 
Reference-contexts: for variable binding to facilitate complex reasoning based on predicate logic [5, 31, 95, 175, 176]; and connectionist realizations of production system [182], expert systems [42, 43], hybrid knowledge processing system [136], semantic networks [68, 167], frame representation [79], planning [145, 188], nonmonotonic reasoning [142], legal reasoning [148], commonsense reasoning <ref> [177, 178] </ref>, and logical theorem proving [141]. This chapter explores how neural architectures for binary partial pattern recognition can be extended for elementary logical inference based on propositional logic.
References-found: 177

