URL: http://www.eecis.udel.edu/~agrawal/papers/ipps95.ps
Refering-URL: http://www.eecis.udel.edu/~agrawal/pub.html
Root-URL: http://www.cis.udel.edu
Email: fedjlali,gagan,als,saltzg@cs.umd.edu  
Title: Data Parallel Programming in An Adaptive Environment  
Author: Guy Edjlali yz Gagan Agrawal Alan Sussman Joel Saltz 
Address: College Park Rocquencourt P.B. 105 MD 20742, USA 78 153 Le Chesnay Cedex, France  
Affiliation: UMIACS and Dept. of Computer Science I.N.R.I.A University of Maryland Domaine de Voluceau  
Abstract: For better utilization of computing resources, it is important to consider parallel programming environments in which the number of available processors varies at runtime. In this paper, we discuss run-time support for data parallel programming in such an adaptive environment. Executing data parallel programs in an adaptive environment requires redistributing data when the number of processors changes, and also requires determining new loop bounds and communication patterns for the new set of processors. We have developed a runtime library to provide this support. We also present performance results for a multi-block Navier-Stokes solver run on a network of workstations using PVM for message passing. Our experiments show that if the number of processors is not varied frequently, the cost of data redistribution is not significant compared to the time required for the actual computations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: This library was also integrated with the HPF/Fortran90D compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. We discuss the functionality of the existing library and then present the extensions that were implemented to support adaptive parallelism.
Reference: [2] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Efficient runtime support for parallelizing block structured applications. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 158-167. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: The runtime library has been developed on top of an existing runtime library for structured and block structured applications. This library is called Multiblock PARTI <ref> [2, 13] </ref>, since it was initially used to parallelize multiblock applications. We have developed our runtime support for adaptive parallelism on top of Multiblock PARTI because this runtime library provides much of the run--time support required for forall loops and array expressions in data parallel languages like HPF.
Reference: [3] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> An integrated runtime and compile-time approach for parallelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1994. To appear. Also available as University of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. </note>
Reference-contexts: This library was also integrated with the HPF/Fortran90D compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. We discuss the functionality of the existing library and then present the extensions that were implemented to support adaptive parallelism.
Reference: [4] <author> R. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1991. </year>
Reference-contexts: Each of these tasks can be migrated from one machine to another, but, only coarse grained load balancing can be obtained when a parallel program needs to be executed on a smaller number of processors. Piranha [7] is a system developed on top of Linda <ref> [4] </ref>. In this system, the application programmer has to write functions for adapting to a change in the number of available processors. Programs written in this system use a master-slave model and the master coordinates relocation of slaves.
Reference: [5] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This library was also integrated with the HPF/Fortran90D compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. We discuss the functionality of the existing library and then present the extensions that were implemented to support adaptive parallelism.
Reference: [6] <author> Guy Edjlali, Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Data parallel programming in an adaptive environment. </title> <institution> Technical Report CS-TR-3350 and UMIACS-TR-94-109, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: In this paper we describe our runtime library. In a separate report, we also discuss how this library can be used by compilers of High Performance Fortran (HPF) like languages to provide automatic support for adaptive parallelism <ref> [6] </ref>. We present experimental results on an application parallelized for adaptive execution by inserting our runtime support by hand. The rest of this paper is organized as follows. In Section 2, we discuss the programming model and model of execution we are targeting.
Reference: [7] <author> David Gelernter and David Kaminsky. </author> <title> Supercomputing out of recycled garbage: Preliminary experience with Piranha. </title> <booktitle> In Proceedings of the Sixth International Conference on Supercomputing, </booktitle> <pages> pages 417-427. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: This system provides light-weight user level tasks. Each of these tasks can be migrated from one machine to another, but, only coarse grained load balancing can be obtained when a parallel program needs to be executed on a smaller number of processors. Piranha <ref> [7] </ref> is a system developed on top of Linda [4]. In this system, the application programmer has to write functions for adapting to a change in the number of available processors. Programs written in this system use a master-slave model and the master coordinates relocation of slaves.
Reference: [8] <author> J.Casas, R.Konuru, S.Otto, R.Prouty, and J.Walpole. </author> <title> Adaptative load migration systems for PVM. </title> <type> Technical report, </type> <institution> Dept. of Computer Science and Engineering,Oregon Graduate Institute of Science and Technology, </institution> <year> 1994. </year>
Reference-contexts: However, this system does not support parallel programs; it considers only programs that will be executed on a single processor. In a version of PVM called Migratable PVM (MPVM) <ref> [8] </ref>, a process or a task running on a machine can be migrated to other machines or processors. However, MPVM does not provide any mechanism for redistribution of data across the remaining processors when a data parallel program has to be withdrawn from one of the processors.
Reference: [9] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: To illustrate the functionality of the runtime routines for communication analysis, consider a single statement forall loop as specified in HPF. This is a parallel loop in which loop bounds and strides associated with any loop variable cannot be functions of any other loop variable <ref> [9] </ref>.
Reference: [10] <author> M.Litzkow and M.Solomon. </author> <title> Supporting check-pointing and process migration outside the Unix kernel. </title> <booktitle> Usenix Winter Conference, </booktitle> <year> 1992. </year>
Reference-contexts: This figure shows that if the program will continue to run for several more time-steps, remapping from almost any configuration to any other larger configuration is likely to be profitable. 5 Related Work In this section, we compare our approach to other efforts on similar problems. Condor <ref> [10] </ref> is a system that supports transparent migration of a process (through checkpointing) from No. of No. of Time-steps for Amortizing Proc. when remapped to 12 proc. 8 proc. 4 proc. 1 proc. 12 - 4 2.3 3.6 - Remapping one workstation to another.
Reference: [11] <author> N.Nedeljkovic and M.J.Quinn. </author> <title> Data-parallel programming on a network of heterogeneous workstations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(4), </volume> <year> 1993. </year>
Reference-contexts: In this system, the application programmer has to write functions for adapting to a change in the number of available processors. Programs written in this system use a master-slave model and the master coordinates relocation of slaves. Data Parallel C and its compilation system <ref> [11] </ref> have been designed for load balancing on a network of heterogeneous machines. The system requires continuous monitoring of the progress of the programs executing on each machine. Experimental results have shown that this involves a significant overhead, even when no load balancing is required [11]. 6 Conclusions In this paper <p> C and its compilation system <ref> [11] </ref> have been designed for load balancing on a network of heterogeneous machines. The system requires continuous monitoring of the progress of the programs executing on each machine. Experimental results have shown that this involves a significant overhead, even when no load balancing is required [11]. 6 Conclusions In this paper we have addressed the problem of developing applications for execution in an adaptive parallel programming environment, meaning an environment in which the number of processors available varies at runtime. We have considered only Single Program Multiple Data (SPMD) parallel programs.
Reference: [12] <author> R.Konuru, J.Casa, R.Prouty, and J.Walpole. </author> <title> A user-level process package for PVM. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 48-55. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: However, MPVM does not provide any mechanism for redistribution of data across the remaining processors when a data parallel program has to be withdrawn from one of the processors. Another system called User Level Processes (ULP) <ref> [12] </ref> has also been developed. This system provides light-weight user level tasks. Each of these tasks can be migrated from one machine to another, but, only coarse grained load balancing can be obtained when a parallel program needs to be executed on a smaller number of processors.
Reference: [13] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The runtime library has been developed on top of an existing runtime library for structured and block structured applications. This library is called Multiblock PARTI <ref> [2, 13] </ref>, since it was initially used to parallelize multiblock applications. We have developed our runtime support for adaptive parallelism on top of Multiblock PARTI because this runtime library provides much of the run--time support required for forall loops and array expressions in data parallel languages like HPF.
Reference: [14] <author> V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based multiblock flow solver; AIAA-93-0677. </title> <booktitle> In Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: This template is extracted from a multiblock computational fluid dynamics application that solves the thin-layer Navier-Stokes equations over a 3D surface (multiblock TLNS3D) <ref> [14] </ref>. In this code, the major computation is performed inside a (sequential) time-step loop.
References-found: 14

