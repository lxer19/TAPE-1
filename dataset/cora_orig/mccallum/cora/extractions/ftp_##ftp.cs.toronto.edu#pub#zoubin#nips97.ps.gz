URL: ftp://ftp.cs.toronto.edu/pub/zoubin/nips97.ps.gz
Refering-URL: http://www.cs.utoronto.ca/~zoubin/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fzoubin,hintong@cs.toronto.edu  
Title: Hierarchical Non-linear Factor Analysis and Topographic Maps  
Author: Zoubin Ghahramani and Geoffrey E. Hinton 
Web: http://www.cs.toronto.edu/neuron/  
Address: Toronto, Ontario, M5S 3H5, Canada  
Affiliation: Dept. of Computer Science, University of Toronto  
Note: To appear in Jordan, MI, Kearns MJ, and Solla, SA Advances in Neural Information Processing Systems 10. MIT Press: Cambridge, MA, 1998.  
Abstract: We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topo graphically organised local feature detectors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bell & T. J. Sejnowski. </author> <title> The 'Independent components' of natural scenes are edge filters. Vision Research, </title> <publisher> In Press. </publisher>
Reference-contexts: The emergence of topography depended on the strength of the MRF and on the speed with which it decayed. Results were relatively insensitive to other parametric changes. We also presented image patches taken from natural images <ref> [1] </ref> to a network with units in the first hidden layer arranged in laterally-connected 2D grid. The network developed local feature detectors, with nearby units responding to similar features (Fig. 5).
Reference: [2] <author> R. Durbin & D. Willshaw. </author> <title> An analogue approach to the travelling salesman problem using an elastic net method. </title> <journal> Nature, </journal> <volume> 326(16) </volume> <pages> 689-691, </pages> <year> 1987. </year>
Reference-contexts: The network developed local feature detectors, with nearby units responding to similar features (Fig. 5). Not all units were used, but the unused units all clustered into one area. 6 Discussion Classical models of topography formation such as Kohonen's self-organising map [6] and the elastic net <ref> [2, 4] </ref> can be thought of as variations on mixture models where additional constraints have been placed to encourage neighboring hidden units to have similar generative weights. The problem with a mixture model is that it cannot handle images in which there are several things going on at once.
Reference: [3] <author> Z. Ghahramani & G. E. Hinton. </author> <title> The EM algorithm for mixtures of factor analyzers. </title> <institution> Univ. Toronto Technical Report CRG-TR-96-1, </institution> <year> 1996. </year>
Reference-contexts: One way to make factor analysis non-linear is to use a mixture of factor analyser modules, each of which captures a different linear regime in the data <ref> [3] </ref>. We can view the factors of all of the modules as a large set of basis functions for describing the data and the process of selecting one module then corresponds to selecting an appropriate subset of the basis functions.
Reference: [4] <author> G. J. Goodhill & D. J. Willshaw. </author> <title> Application of the elatic net algorithm to the formation of ocular dominance stripes. Network: </title> <journal> Comp. in Neur. Sys., </journal> <volume> 1 </volume> <pages> 41-59, </pages> <year> 1990. </year>
Reference-contexts: The network developed local feature detectors, with nearby units responding to similar features (Fig. 5). Not all units were used, but the unused units all clustered into one area. 6 Discussion Classical models of topography formation such as Kohonen's self-organising map [6] and the elastic net <ref> [2, 4] </ref> can be thought of as variations on mixture models where additional constraints have been placed to encourage neighboring hidden units to have similar generative weights. The problem with a mixture model is that it cannot handle images in which there are several things going on at once.
Reference: [5] <author> G. E. Hinton & Z. Ghahramani. </author> <title> Generative models for discovering sparse distributed representations. </title> <journal> Philos. Trans. Roy. Soc. B, </journal> <volume> 352 </volume> <pages> 1177-1190, </pages> <year> 1997. </year>
Reference-contexts: This can be achieved by using a generative model in which there is a high probability of generating factor activations of exactly zero. 2 Rectified Gaussian Belief Nets The Rectified Gaussian Belief Net (RGBN) uses multiple layers of units with states that are either positive real values or zero <ref> [5] </ref>. Its main disadvantage is that computing the posterior distribution over the factors given a data vector involves Gibbs sampling. <p> If units at one layer are driven by unit-variance, independent Gaussian noise, and these in turn drive units in the layer below using the generative weights, then Hebbian learning between the two layers will learn the correct recognition weights <ref> [5] </ref>. 4 Lateral Connections in the Generative Model When the generative model contains only top-down connections, lateral connections make it possible to do perceptual inference using locally available information. But it is also possible, and often desirable, to have lateral connections in the generative model.
Reference: [6] <author> T. Kohonen. </author> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: The network developed local feature detectors, with nearby units responding to similar features (Fig. 5). Not all units were used, but the unused units all clustered into one area. 6 Discussion Classical models of topography formation such as Kohonen's self-organising map <ref> [6] </ref> and the elastic net [2, 4] can be thought of as variations on mixture models where additional constraints have been placed to encourage neighboring hidden units to have similar generative weights.
Reference: [7] <author> D. D. Lee & H. S. Seung. </author> <title> Unsupervised learning by convex and conic coding. </title> <editor> In M. Mozer, M. Jordan, & T. Petsche, eds., </editor> <booktitle> NIPS 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference: [8] <author> M. S. Lewicki & T. J. Sejnowski. </author> <title> Bayesian unsupervised learning of higher order structure. </title> <booktitle> In NIPS 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: We first describe the RGBN without considering neural plausibility. Then we show how lateral interactions within a layer can be used to perform probabilistic inference correctly using locally available information. This makes the RGBN far more plausible as a neural model than a sigmoid belief net <ref> [9, 8] </ref> because it means that Gibbs sampling can be performed without requiring units in one layer to see the total top-down input to units in the layer below.
Reference: [9] <author> R. M. Neal. </author> <title> Connectionist learning of belief networks. </title> <journal> Artif. Intell., </journal> <volume> 56 </volume> <pages> 71-113, </pages> <year> 1992. </year>
Reference-contexts: We first describe the RGBN without considering neural plausibility. Then we show how lateral interactions within a layer can be used to perform probabilistic inference correctly using locally available information. This makes the RGBN far more plausible as a neural model than a sigmoid belief net <ref> [9, 8] </ref> because it means that Gibbs sampling can be performed without requiring units in one layer to see the total top-down input to units in the layer below.
Reference: [10] <author> R M Neal & G E Hinton. </author> <title> A new view of the EM algorithm that justifies incremental, sparse, and other variants. </title> <editor> In M Jordan, ed. </editor> <title> Learning in Graphical Models, </title> <year> 1998. </year>
Reference-contexts: In general, Gibbs sampling can be very time consuming, but in practice 10 to 20 samples per unit have proved adequate and there are theoretical reasons for believing that learning can work well even when the Gibbs sampling fails to reach equilibrium <ref> [10] </ref>. We first describe the RGBN without considering neural plausibility. Then we show how lateral interactions within a layer can be used to perform probabilistic inference correctly using locally available information. <p> Assuming the local noise models for the lower layer units all have unit variance, and 2 If Gibbs sampling has not been run long enough to reach equilibrium, the delta rule follows the gradient of the penalized log probability of the data <ref> [10] </ref>. The penalty term is the Kullback-Liebler divergence between the equilibrium distribution and the distribution produced by Gibbs sampling.
References-found: 10

