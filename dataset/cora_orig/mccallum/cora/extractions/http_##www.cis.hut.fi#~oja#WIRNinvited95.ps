URL: http://www.cis.hut.fi/~oja/WIRNinvited95.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: Erkki.Oja@hut.fi  
Title: Principal and Independent Components in Neural Networks Recent Developments  
Author: E. Oja, J. Karhunen, L. Wang, and R. Vigario 
Address: Rakentajanaukio 2C, 02150 Espoo, Finland  
Affiliation: Laboratory of Computer and Information Science Helsinki University of Technology  
Abstract: Nonlinear extensions of one-unit and multi-unit Principal Component Analysis (PCA) neural networks, introduced earlier by the authors, are reviewed. The networks and their nonlinear Hebbian learning rules are related to other signal expansions like Projection Pursuit (PP) and Independent Component Analysis (ICA). Separation results for mixtures of real world signals and im ages are given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Baldi and K. Hornik, </author> <title> "Neural networks for principal component analysis: learning from examples without local minima," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 53-58, </pages> <year> 1989. </year>
Reference: [2] <author> A. Bell and T. Sejnowski, </author> <title> "Blind separation and blind deconvolution: an information-theoretic approach," </title> <booktitle> in Proc. 1995 IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> Detroit, USA, </address> <month> May </month> <year> 1995, </year> <pages> pp. 3415-3418. </pages>
Reference-contexts: However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld [4, 21], can be interpreted as learning algorithms of a neural network. Quite recently, some authors <ref> [2, 9] </ref> have derived unsupervised "neural" learning rules from information-theoretic measures. The resulting algorithms show good separation performance, but are not truly realizable in neural networks because they are based on relatively complicated numerical operations, requiring for example the inversion of a matrix.
Reference: [3] <author> G. Burel, </author> <title> "Blind separation of sources: a nonlinear neural algorithm," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 937-947, </pages> <year> 1992. </year>
Reference: [4] <author> J.-F. Cardoso and B. Laheld, </author> <title> "Equivariant adaptive source separation," </title> <note> manuscript submitted to IEEE Trans. on Signal Processing, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: The following assumptions are typically made <ref> [4, 21] </ref>: 1. A is an unknown constant matrix with full column rank. Thus the number of sources M L. Usually M is assumed to be known in advance. 2. <p> The noise term n k is often omitted from (23), because in fact it can be incorporated in the sum as one of the source signals. The source separation problem <ref> [15, 4, 21] </ref> is now to find an M fi L separating matrix B so that the M -vector y k = Bx k (24) is an estimate y k = ^ s k of the original independent source signals. <p> Most of the approaches dealing with signal separation and ICA are non-neural, and are based on some batch type or adaptive signal processing algorithm. However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld <ref> [4, 21] </ref>, can be interpreted as learning algorithms of a neural network. Quite recently, some authors [2, 9] have derived unsupervised "neural" learning rules from information-theoretic measures.
Reference: [5] <author> Y. Chauvin, </author> <title> "Principal component analysis by gradient descent on a constrained linear Hebbian cell", </title> <booktitle> in Proc. Int. Joint Conf. on Neural Networks, </booktitle> <address> Washington DC, USA, </address> <month> June </month> <year> 1989, </year> <pages> pp. 1373 - 1380. </pages>
Reference-contexts: following we consider a different approach, where the orthonormality constraints are not taken into account via Lagrange multipliers, but by minimizing another criterion simultaneously. 6 3.1.2 The bigradient algorithm In the context of PCA type neural networks, the basic idea of optimizing two criteria simultaneously has been introduced earlier in <ref> [5] </ref> for a somewhat different single neuron problem. First, the criterion J 1 (w (i)) = Eff (x T w (i))g (12) which is the first part in the criterion (11) must be maximized or minimized by the i-th neuron.
Reference: [6] <author> A. Cichocki and R. Unbehauen, </author> <title> Neural Networks for Optimization and Signal Processing. </title> <address> New York: </address> <publisher> John Wiley, </publisher> <year> 1993. </year>
Reference-contexts: Typically Hebbian type learning rules are used based on the one-unit learning algorithm originally proposed by one of the authors in [24]. Many different versions and extensions of this basic algorithm have been proposed during the recent years; for reviews and introductions, see e.g. <ref> [29, 6, 13] </ref>. PCA networks are useful in signal characterization, optimal feature extraction, and data compression. <p> First, if f (y) grows less than quadratically, (11) is more robust than standard variances against outliers and long-tailed noise. Examples of such robust cost functions are f (y) = lncosh (y) and f (y) = jyj; see also <ref> [6, 17] </ref>. Second, if f (y) is non-quadratic, (11) introduces higher-order statistics into computations, while the variance criterion f (y) = y 2 would measure second-order statistics (covariances) only. This is sufficient for Gaussian data, but usually not sufficient for inputs with non-Gaussian densities [19]. <p> x (1); x (2) are rotated to y (1); y (2). 4 Relation of nonlinear PCA to the source sepa- ration problem and ICA In source separation for linear memoryless channels and the related technique of Independent Component Analysis (ICA), the following basic data model is usually assumed (see e.g. <ref> [15, 8, 6] </ref>). There are M scalar-valued signals s k (1); :::; s k (M ) indexed by an index k; if k is one-dimensional, then it usually signifies discrete time. We assume that the signals have zero mean and they are mutually statistically independent.
Reference: [7] <author> P. Comon, </author> <title> "Separation of stochastic processes," </title> <booktitle> in Proc. of Workshop on Higher-Order Spectral Analysis, </booktitle> <address> Vail, Colorado, </address> <month> June </month> <year> 1989, </year> <pages> pp. 174-179. 17 </pages>
Reference: [8] <author> P. Comon, </author> <title> "Independent component analysis anew concept?," </title> <journal> Signal Pro--cessing, </journal> <volume> vol. 36, </volume> <pages> pp. 287-314, </pages> <year> 1994. </year>
Reference-contexts: Recently, some authors have discussed the relation of these rules to Projection Pursuit (PP) [12, 36]. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) [15] and the source separation problem. The source separation problem <ref> [15, 8] </ref>, to be discussed in more detail in Section 4, is important in certain applications of signal processing and communications. The problem is to find a set of source signals, when only linear combinations or mixtures of them are given. <p> However, in addition to the uncorrelatedness, the output signals must be mutually independent (or as independent as possible). This leads in a natural way from PCA to Independent Component Analysis. A precise definition of ICA together with a detailed analysis is given in a recent fundamental paper by Comon <ref> [8] </ref>. Generally, source separation and ICA require higher than second-order statistics, not provided by standard PCA. Such higher-order statistics can be incorporated into the computations either explicitly or by using suitable nonlinearities. We favor the latter approach because it suits better to a neural network environment. <p> Thus z = tanh (fiw T x) will have an almost uniform density if w T x is Gaussian. In Projection Pursuit, such a direction should be avoided. Friedman's index in PP (see Comon <ref> [8] </ref>) is based precisely on the deviation of z from the uniform density. <p> x (1); x (2) are rotated to y (1); y (2). 4 Relation of nonlinear PCA to the source sepa- ration problem and ICA In source separation for linear memoryless channels and the related technique of Independent Component Analysis (ICA), the following basic data model is usually assumed (see e.g. <ref> [15, 8, 6] </ref>). There are M scalar-valued signals s k (1); :::; s k (M ) indexed by an index k; if k is one-dimensional, then it usually signifies discrete time. We assume that the signals have zero mean and they are mutually statistically independent. <p> In this case, B is an orthogonal matrix. It is not easy to verify the independence condition exactly in practice because the involved probability densities are unknown. Therefore, approximating contrast functions which are maximized for a separating matrix have been introduced <ref> [8] </ref>. Even these often lead to relatively intensive batch type computations.
Reference: [9] <author> G. Deco and W. </author> <title> Brauer, "Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures," </title> <note> to appear in Neural Networks, 1995 (in press). </note>
Reference-contexts: However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld [4, 21], can be interpreted as learning algorithms of a neural network. Quite recently, some authors <ref> [2, 9] </ref> have derived unsupervised "neural" learning rules from information-theoretic measures. The resulting algorithms show good separation performance, but are not truly realizable in neural networks because they are based on relatively complicated numerical operations, requiring for example the inversion of a matrix.
Reference: [10] <author> Friedman, J. and Tukey, J., </author> <title> "A projection pursuit algorithm for exploratory data analysis," </title> <journal> IEEE Trans. Comput. </journal> <volume> C-23, </volume> <year> 1974, </year> <pages> pp. 881 - 889. </pages>
Reference-contexts: Several possible cost functions are listed in [28]. 2.2 Connection to Exploratory Projection Pursuit The Exploratory Projection Pursuit (PP) technique was introduced by Friedman and Tukey <ref> [10] </ref> for visualization purposes, especially clustering. The purpose is to find directions w 0 in the input space that are somehow interesting, for instance, such that projections y = w 0T x of the data on the direction w 0 is multimodal or in general non-Gaussian.
Reference: [11] <author> J. Friedman, </author> <title> "Exploratory projection pursuit," </title> <journal> J. Amer. Statistical Assoc., </journal> <volume> vol. 82, no. 397, </volume> <pages> pp. 249-266, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Therefore a direction that has low kurtosis is often interesting for visualization and clustering; see e.g. [36]. Good review papers on the PP technique are given by Huber [14] and by Friedman <ref> [11] </ref>. The constrained Hebbian learning rule of Section 2.1 was used by Fyfe et al [12] in exploring neural learning for exploratory projection pursuit.
Reference: [12] <author> C. Fyfe, D.R. McGregor, and R. Baddeley, </author> <title> "Exploratory projection pursuit: an artificial neural network approach," </title> <institution> Dep. Computer Sci., Univ. of Strathclyde (Glasgow, Scotland), Res. </institution> <type> Rep. /94/160, </type> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: One of the present authors generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in [28]. Recently, some authors have discussed the relation of these rules to Projection Pursuit (PP) <ref> [12, 36] </ref>. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) [15] and the source separation problem. The source separation problem [15, 8], to be discussed in more detail in Section 4, is important in certain applications of signal processing and communications. <p> Therefore a direction that has low kurtosis is often interesting for visualization and clustering; see e.g. [36]. Good review papers on the PP technique are given by Huber [14] and by Friedman [11]. The constrained Hebbian learning rule of Section 2.1 was used by Fyfe et al <ref> [12] </ref> in exploring neural learning for exploratory projection pursuit. <p> However, let us explicitly prevent this by sphering the data first (see <ref> [12] </ref>): by linear preprocessing, the covariance C = Efxx T g can be made equal to the unit matrix. Let us assume that the input vector has been sphered: C = I.
Reference: [13] <author> S. Haykin, </author> <title> Neural Networks: A Comprehensive Foundation. </title> <address> New York: </address> <publisher> IEEE Computer Society Press and Macmillan, </publisher> <year> 1994. </year>
Reference-contexts: Typically Hebbian type learning rules are used based on the one-unit learning algorithm originally proposed by one of the authors in [24]. Many different versions and extensions of this basic algorithm have been proposed during the recent years; for reviews and introductions, see e.g. <ref> [29, 6, 13] </ref>. PCA networks are useful in signal characterization, optimal feature extraction, and data compression.
Reference: [14] <author> P. Huber, </author> <title> "Projection pursuit," </title> <journal> The Annals of Statistics, </journal> <volume> vol. 13, no. 2, </volume> <pages> pp. 435-475, </pages> <year> 1985. </year>
Reference-contexts: Therefore a direction that has low kurtosis is often interesting for visualization and clustering; see e.g. [36]. Good review papers on the PP technique are given by Huber <ref> [14] </ref> and by Friedman [11]. The constrained Hebbian learning rule of Section 2.1 was used by Fyfe et al [12] in exploring neural learning for exploratory projection pursuit.
Reference: [15] <author> C. Jutten and J. Herault, </author> <title> "Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture," </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 24, no. 1, </volume> <pages> pp. 1-10, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: One of the present authors generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in [28]. Recently, some authors have discussed the relation of these rules to Projection Pursuit (PP) [12, 36]. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) <ref> [15] </ref> and the source separation problem. The source separation problem [15, 8], to be discussed in more detail in Section 4, is important in certain applications of signal processing and communications. <p> Recently, some authors have discussed the relation of these rules to Projection Pursuit (PP) [12, 36]. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) [15] and the source separation problem. The source separation problem <ref> [15, 8] </ref>, to be discussed in more detail in Section 4, is important in certain applications of signal processing and communications. The problem is to find a set of source signals, when only linear combinations or mixtures of them are given. <p> x (1); x (2) are rotated to y (1); y (2). 4 Relation of nonlinear PCA to the source sepa- ration problem and ICA In source separation for linear memoryless channels and the related technique of Independent Component Analysis (ICA), the following basic data model is usually assumed (see e.g. <ref> [15, 8, 6] </ref>). There are M scalar-valued signals s k (1); :::; s k (M ) indexed by an index k; if k is one-dimensional, then it usually signifies discrete time. We assume that the signals have zero mean and they are mutually statistically independent. <p> The noise term n k is often omitted from (23), because in fact it can be incorporated in the sum as one of the source signals. The source separation problem <ref> [15, 4, 21] </ref> is now to find an M fi L separating matrix B so that the M -vector y k = Bx k (24) is an estimate y k = ^ s k of the original independent source signals. <p> Various more or less heuristic approaches have been proposed for achieving separation. The learning algorithms are constructed in such a way that they should satisfy some kind of independence condition after convergence. An example is the seminal Herault-Jutten (HJ) algorithm <ref> [15] </ref>, which uses a neural-like network structure. This algorithm is simple and elegant, but may fail in separating more than two independent sources. Most of the approaches dealing with signal separation and ICA are non-neural, and are based on some batch type or adaptive signal processing algorithm.
Reference: [16] <author> J. Karhunen, </author> <title> "Adaptive algorithms for estimating eigenvectors of correlation type matrices," </title> <booktitle> in Proc. 1984 IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <address> San Diego, California, </address> <month> March </month> <year> 1984, </year> <pages> pp. </pages> <month> 14.6.1-14.6.4. </month>
Reference: [17] <author> J. Karhunen and J. Joutsensalo, </author> <title> "Representation and separation of signals using nonlinear PCA type learning," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 1, </volume> <pages> pp. 113-127, </pages> <year> 1994. </year>
Reference-contexts: These learning rules were related to optimization criteria by Karhunen and Jout-sensalo <ref> [17] </ref>. Let us consider each algorithm in turn, and also introduce a third possibility closely related to the above, the bigradient algorithm. <p> In this section, the learning rules are discussed from a general optimization point of view, and they will be consequently applied to the Independent Component Analysis problem in Section 4. 3.1.1 Robust PCA learning rule Consider first algorithm (9). It was shown by Karhunen and Joutsensalo <ref> [17] </ref> that it is a stochastic approximation algorithm for the minimization problems J 0 (w (i)) = Eff (x T w (i))g + k where ij are Lagrangian coefficients. Each neuron is maximizing its objective function under the constraint that the weight vectors are mutually orthonormal. <p> First, if f (y) grows less than quadratically, (11) is more robust than standard variances against outliers and long-tailed noise. Examples of such robust cost functions are f (y) = lncosh (y) and f (y) = jyj; see also <ref> [6, 17] </ref>. Second, if f (y) is non-quadratic, (11) introduces higher-order statistics into computations, while the variance criterion f (y) = y 2 would measure second-order statistics (covariances) only. This is sufficient for Gaussian data, but usually not sufficient for inputs with non-Gaussian densities [19]. <p> It was shown by Karhunen and Joutsensalo in <ref> [17] </ref> that it approximates a stochastic gradient descent algorithm for the minimization problem J 3 (W) = Efkx Wg (W T x)k 2 g: (19) This is a least mean square criterion in which the input vector x is represented by a nonlinear expansion of the weight vectors; for details, see <p> that it approximates a stochastic gradient descent algorithm for the minimization problem J 3 (W) = Efkx Wg (W T x)k 2 g: (19) This is a least mean square criterion in which the input vector x is represented by a nonlinear expansion of the weight vectors; for details, see <ref> [17] </ref>. Since this criterion is not explicitly minimizing or maximizing any quality criteria Eff (x T w (i))g for the neuron weights, it may be difficult to see whether it has any connections to our basic problems of independent component analysis or source separation.
Reference: [18] <author> J. Karhunen, </author> <title> "Optimization criteria and nonlinear PCA neural networks," </title> <booktitle> in Proc. 1994 IEEE Int. Conf. on Neural Networks, </booktitle> <address> Orlando, Florida, </address> <month> June </month> <year> 1994, </year> <pages> pp. 1241-1246. </pages>
Reference: [19] <author> J. Karhunen and J. Joutsensalo, </author> <title> "Generalizations of principal component analysis, optimization problems, and neural networks," </title> <note> to appear in Neural Networks (in press). </note>
Reference-contexts: The subspace spanned by the principal eigenvectors c (1); : : : ; c (M ) (M &lt; L) is called the PCA subspace (of dimensionality M ). PCA is used in many applications because of its optimality properties in data compression and information representation <ref> [19] </ref>. It is now well-known that PCA can be realized neurally in various ways. A Principal Component Analysis (PCA) network [26] is a one-layer feedforward neural network which is able to extract the principal components of the stream of input vectors. <p> Second, if f (y) is non-quadratic, (11) introduces higher-order statistics into computations, while the variance criterion f (y) = y 2 would measure second-order statistics (covariances) only. This is sufficient for Gaussian data, but usually not sufficient for inputs with non-Gaussian densities <ref> [19] </ref>. For example, if f (y) = y 4 (suitably scaled), the first term in (11) measures the kurtosis of the input data provided that the data vectors are whitened (sphered) prior to inputting them to the network, as explained above in Section 2.2.
Reference: [20] <author> J. Karhunen, L. Wang, and J. Joutsensalo, </author> <title> "Neural estimation of basis vectors in independent component analysis," </title> <note> to appear. </note>
Reference-contexts: Generally, source separation and ICA require higher than second-order statistics, not provided by standard PCA. Such higher-order statistics can be incorporated into the computations either explicitly or by using suitable nonlinearities. We favor the latter approach because it suits better to a neural network environment. Recently, Karhunen et al <ref> [20] </ref> proposed the multilayer ICA network based on linear and nonlinear PCA layers as a neural realization of ICA. These developments will be reviewed here. As the starting point, we use the nonlinear constrained Hebbian rules introduced by one the authors in Oja et al [28]. <p> Note that with these steps, even if we obtain the matrix BVA, we cannot yet estimate the reconstruction matrix B itself because A is unknown. This must be done separately as shown by Karhunen et al. <ref> [20] </ref>. Because both kurtosis maximization/minimization and the sphering can be performed adaptively in an on-line process receiving a sequence of mixture signals x k , a neural network is a possible implementation for the algorithms.
Reference: [21] <author> B. Laheld and J.-F. Cardoso, </author> <title> "Adaptive source separation with uniform performance," </title> <booktitle> in Signal Processing VII: Theories and Applications (Proc. </booktitle> <editor> EUSIPCO-94), M. Holt et al. (Eds.). Lausanne: EURASIP, </editor> <booktitle> 1994, </booktitle> <volume> vol. 2, </volume> <pages> pp. 183-186. 18 </pages>
Reference-contexts: The following assumptions are typically made <ref> [4, 21] </ref>: 1. A is an unknown constant matrix with full column rank. Thus the number of sources M L. Usually M is assumed to be known in advance. 2. <p> The noise term n k is often omitted from (23), because in fact it can be incorporated in the sum as one of the source signals. The source separation problem <ref> [15, 4, 21] </ref> is now to find an M fi L separating matrix B so that the M -vector y k = Bx k (24) is an estimate y k = ^ s k of the original independent source signals. <p> Most of the approaches dealing with signal separation and ICA are non-neural, and are based on some batch type or adaptive signal processing algorithm. However, some of these adaptive algorithms, such as the HJ algorithm and its modifications as well as the PFS/EASI algorithm proposed by Cardoso and Laheld <ref> [4, 21] </ref>, can be interpreted as learning algorithms of a neural network. Quite recently, some authors [2, 9] have derived unsupervised "neural" learning rules from information-theoretic measures.
Reference: [22] <author> K. Matsuoka, M. Ohya, and M. Kawamoto, </author> <title> "A neural net for blind separation of nonstationary signals," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 3, </volume> <pages> pp. 411-419, </pages> <year> 1995. </year>
Reference: [23] <author> E. Moreau and O. Macchi, </author> <title> "New self-adaptive algorithms for source separation based on contrast functions," </title> <booktitle> in Proc. IEEE Signal Proc. Workshop on Higher Order Statistics, </booktitle> <address> Lake Tahoe, USA, </address> <month> June </month> <year> 1993, </year> <pages> pp. 215-219. </pages>
Reference-contexts: Therefore, approximating contrast functions which are maximized for a separating matrix have been introduced [8]. Even these often lead to relatively intensive batch type computations. However, for prewhitened input vectors it can be shown <ref> [23] </ref> that the simpler contrast function, the sum of absolute values of the fourth order cumulants (kurtoses) J kurt (y) = M X j cum (y (i) 4 ) j = i=1 is maximized by a separating matrix B under certain conditions. <p> This provides 11 an interesting connection between the techiques of ICA and Projection Pursuit, in which kurtosis also can play an important role as shown in Section 2. The following result (in a slightly different formulation) was proven by Moreau and Macchi <ref> [23] </ref>: Theorem 1. Assume that the fourth order cumulants of all the original statistically independent source signals s (i) have the same sign, and consider vectors y = Hs with s = (s (1); :::; s (M )) T the source signal vector and H an invertible matrix.
Reference: [24] <author> E. Oja, </author> <title> "A simplified neuron model as a Principal Components Analyzer," </title> <journal> J. Math. Biol. </journal> <volume> 15, </volume> <year> 1982, </year> <pages> pp. 267 - 273. </pages>
Reference-contexts: A Principal Component Analysis (PCA) network [26] is a one-layer feedforward neural network which is able to extract the principal components of the stream of input vectors. Typically Hebbian type learning rules are used based on the one-unit learning algorithm originally proposed by one of the authors in <ref> [24] </ref>. Many different versions and extensions of this basic algorithm have been proposed during the recent years; for reviews and introductions, see e.g. [29, 6, 13]. PCA networks are useful in signal characterization, optimal feature extraction, and data compression.
Reference: [25] <author> E. Oja and J. Karhunen, </author> <title> "On stochastic approximation of eigenvectors and eigenvalues of the expectation of a random matrix", </title> <journal> J. Math. Anal. Appl. </journal> <volume> 106, </volume> <year> 1985, </year> <pages> pp. 69 - 84. </pages>
Reference-contexts: functions is then ~ w k+1 = w k + ff k dw k k x k ); w k+1 = k ~ w k+1 k where ff k is the learning rate, usually a sequence of positive numbers decreasing slowly to zero; for details on the learning rates, see <ref> [25] </ref>. The first equation in eq. (1) is a pure gradient ascent step, and the other is a normalizing step.
Reference: [26] <author> E. Oja, </author> <title> "Neural networks, principal components, and subspaces". </title> <booktitle> Int. J. of Neural Systems 1, </booktitle> <year> 1989, </year> <pages> pp. 61 - 68. </pages>
Reference-contexts: PCA is used in many applications because of its optimality properties in data compression and information representation [19]. It is now well-known that PCA can be realized neurally in various ways. A Principal Component Analysis (PCA) network <ref> [26] </ref> is a one-layer feedforward neural network which is able to extract the principal components of the stream of input vectors. Typically Hebbian type learning rules are used based on the one-unit learning algorithm originally proposed by one of the authors in [24].
Reference: [27] <author> E. Oja and T. Kohonen, </author> <title> "The subspace learning algorithm as a formalism for Pattern Recognition and Neural Networks," </title> <booktitle> Proc. IEEE 1988 Int. Conf. on Neural Networks, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1988, </year> <pages> pp. 277-284. </pages>
Reference-contexts: PCA networks are useful in signal characterization, optimal feature extraction, and data compression. PCA is also the basis of subspace classifiers that have been used e.g. in speech and texture classification <ref> [27] </ref>. 1 In the field of neural networks, there has been growing interest in nonlinear extensions of the PCA. One of the present authors generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in [28].
Reference: [28] <author> E. Oja, H. Ogawa, and J. Wangviwattana, </author> <title> "Learning in nonlinear constrained Hebbian networks," </title> <booktitle> in Artificial Neural Networks (Proc. ICANN-91), </booktitle> <editor> T. Ko-honen et al. (Eds.). </editor> <publisher> Amsterdam: North-Holland, </publisher> <year> 1991, </year> <pages> pp. 385-390. </pages>
Reference-contexts: One of the present authors generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in <ref> [28] </ref>. Recently, some authors have discussed the relation of these rules to Projection Pursuit (PP) [12, 36]. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) [15] and the source separation problem. <p> Recently, Karhunen et al [20] proposed the multilayer ICA network based on linear and nonlinear PCA layers as a neural realization of ICA. These developments will be reviewed here. As the starting point, we use the nonlinear constrained Hebbian rules introduced by one the authors in Oja et al <ref> [28] </ref>. These will be reviewed in Section 2 for the one-unit case and Section 3 for the multi-unit case, and they are related to optimization and Exploratory Projection Pursuit. Also, the bigradient learning rule of Wang et al [35] is reviewed here. <p> Section 5 is a review of the full ICA network, and applications to speech and image separation are given in Section 6. 2 Nonlinear Hebbian learning: the one-unit case 2.1 Learning rule for one neuron Consider first a single artificial neuron receiving an L-dimensional input vector x <ref> [28] </ref>. The neuron is trying to adapt its weight vector w so that a function Eff (w T x)g is maximized, where E is the expectation with respect to the (unknown) density of x and f (:) is a continuous objective function. <p> T x) = g (w T x)x: (2) For small values of ff k , eq. (1) can be approximated by w k+1 = w k + ff k (I w k w T k x k ) (3) in which terms proportional to ff 2 k have been dropped <ref> [28] </ref>. Depending on the function f (w T x), several cases are covered by this formalism. <p> For instance, if the objective function is f (y) = lncosh (y); y = w T x; (4) then g (y) = (d=dy)f (y) = tanh (y), the activation function commonly used in Per ceptron networks. Several possible cost functions are listed in <ref> [28] </ref>. 2.2 Connection to Exploratory Projection Pursuit The Exploratory Projection Pursuit (PP) technique was introduced by Friedman and Tukey [10] for visualization purposes, especially clustering. <p> This was one of the criteria proposed in <ref> [28] </ref>, and the derivative to be used in the learning rule (3) is then g (y) = tanh (y)(1 tanh 2 (y)). <p> Consider a one-layer neural network of M parallel units. Each 5 unit i has the same L-element input vextor x and its own L-dimensional weight vector w (i), which together comprise an L fi M weight matrix W = [w (1):::w (M )]. Following Oja et al <ref> [28] </ref>, the following learning rules are generalizations of the linear PCA learning rule: W k+1 = W k + ff k (I W k W T k W k ); (9) k W k ) W k g (W T k W k )]: (10) The nonlinear vector function g (x
Reference: [29] <author> E. Oja, </author> <title> "Principal components, minor components, and linear neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 927-935, </pages> <year> 1992. </year>
Reference-contexts: Typically Hebbian type learning rules are used based on the one-unit learning algorithm originally proposed by one of the authors in [24]. Many different versions and extensions of this basic algorithm have been proposed during the recent years; for reviews and introductions, see e.g. <ref> [29, 6, 13] </ref>. PCA networks are useful in signal characterization, optimal feature extraction, and data compression.
Reference: [30] <author> J. Platt and F. Faggin, </author> <title> "Networks for the separation of sources that are superimposed and delayed," </title> <booktitle> in Advances in Neural Processing Systems 4, </booktitle> <editor> J. Moody et al. (Eds.). </editor> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991, </year> <pages> pp. 730-737. </pages>
Reference: [31] <author> M. Plumbley, </author> <title> "A Hebbian/anti-Hebbian network which optimizes information capacity by orthonormalizing the principal subspace, </title> <booktitle> in Proc. IEE Conf. on Artificial Neural Networks, </booktitle> <address> Brighton, UK, </address> <month> May </month> <year> 1993, </year> <pages> pp. 86-90. </pages>
Reference-contexts: The elements of v have variances equal to 1 and are uncorrelated but in general not independent. The sphering transformation V is implemented by the weights of the linear layer and can be learned in a neural learning algorithm introduced by Plumbley <ref> [31] </ref>: V k+1 = V k + k (V k x k x T k I)V k : (33) In the second layer, one of the nonlinear Hebbian learning rules (9), (10), or (18) is used, with the sphered vectors v k taken as inputs instead of the original 13 Weight
Reference: [32] <author> A. Sudjianto and M. Hassoun, </author> <title> "Nonlinear Hebbian rule: a statistical interpretation", </title> <booktitle> in Proc. IEEE Int. Conf. on Neural Networks, </booktitle> <address> Orlando, Fla., </address> <month> July </month> <year> 1994, </year> <pages> pp. 1247 - 1252. </pages>
Reference-contexts: It is seen that this objective function also measures deviation from normality much in the same way as the kurtosis. 4 function lncosh (y) (curve 2) for a two-peaked density, as functions of the distance of the peaks from the origin Sudjianto and Hassoun <ref> [32] </ref> made the interesting observation that the tanh (fiy) function, with a suitable parameter fi, follows very closely the function 2 (y) 1 where (y) is the cumulative distribution function of the Gaussian density.
Reference: [33] <author> J. Taylor and M. Plumbley, </author> <title> "Information theory and neural networks," in Mathematical Approaches to Neural Networks, </title> <editor> J. Taylor (Ed.). </editor> <publisher> Amsterdam: Elsevier Science Publ., </publisher> <year> 1993, </year> <pages> pp. 307-340. </pages>
Reference: [34] <author> C. Therrien, </author> <title> Discrete Random Signals and Statistical Signal Processing. </title> <address> Engle-wood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference: [35] <author> L. Wang, J. Karhunen, and E. Oja, </author> <title> "A bigradient optimization approach for robust PCA, MCA, and source separation," </title> <booktitle> submitted to 1995 IEEE Int. Conf. on Neural Networks, </booktitle> <address> Perth, Australia, </address> <month> November </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: These will be reviewed in Section 2 for the one-unit case and Section 3 for the multi-unit case, and they are related to optimization and Exploratory Projection Pursuit. Also, the bigradient learning rule of Wang et al <ref> [35] </ref> is reviewed here. In Section 4, the source separation problem and the ICA expansion are reviewed and a two stage solution is analyzed in which the first stage normalizes the input data by whitening or sphering, and the second stage performs nonlinear PCA.
Reference: [36] <author> M. sterberg and R. Lenz, </author> <title> "Unsupervised parallel feature extraction from first principles, </title> <booktitle> in Advances in Neural Processing Systems 6, </booktitle> <editor> J. Cowan et al. (Eds.). </editor> <address> San Francisco: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994, </year> <pages> pp. 136-143. 20 </pages>
Reference-contexts: One of the present authors generalized the one-unit and multi-unit PCA learning rules to nonlinear versions in [28]. Recently, some authors have discussed the relation of these rules to Projection Pursuit (PP) <ref> [12, 36] </ref>. Another interesting development is the relation of neural networks to Independent Component Analysis (ICA) [15] and the source separation problem. The source separation problem [15, 8], to be discussed in more detail in Section 4, is important in certain applications of signal processing and communications. <p> A high kurtosis (a positive value) signifies a peaked unimodal density, sharper than a Gaussian, while a low kurtosis (a high negative value) indicates multimodality. Therefore a direction that has low kurtosis is often interesting for visualization and clustering; see e.g. <ref> [36] </ref>. Good review papers on the PP technique are given by Huber [14] and by Friedman [11]. The constrained Hebbian learning rule of Section 2.1 was used by Fyfe et al [12] in exploring neural learning for exploratory projection pursuit.
References-found: 36

