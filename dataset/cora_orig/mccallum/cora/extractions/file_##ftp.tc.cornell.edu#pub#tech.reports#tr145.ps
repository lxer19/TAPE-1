URL: file://ftp.tc.cornell.edu/pub/tech.reports/tr145.ps
Refering-URL: http://www.tc.cornell.edu/~liao/papers.html
Root-URL: http://www.tc.cornell.edu
Title: SOLVING LP PROBLEMS VIA WEIGHTED CENTERS  
Author: AIPING LIAO AND MICHAEL J. TODD 
Keyword: Key words. weighted center, the ellipsoid method, Newton's method, linear programming.  
Note: AMS subject classifications. 65K, 90C  
Date: July 12, 1993  
Abstract: The feasibility problem for a system of linear inequalities can be converted into an unconstrained optimization problem using ideas from the ellipsoid method, which can be viewed as a very simple minimization technique for the resulting nonlinear function. Using more sophisticated algorithms, we develop and investigate more efficient methods, which lead to two kinds of weighted centers for the feasible set. With these centers, we develop new algorithms for solving linear programming problems. 1. Introduction and history of centers. In this paper we will consider linear 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Akg ul, </author> <title> Topics in relaxation and ellipsoidal methods, </title> <publisher> Pitman Advanced Publishing Program, </publisher> <year> 1984. </year>
Reference-contexts: Therefore, the solution set of (P f+h ) is (at least) fd = d + (1 ) dj 2 <ref> [0; 1] </ref>g. We note that any point used as a "center" must be uniquely defined. Although the optimal solution set S f+h may not be a singleton, it turns out that x c (d fl ) is unique, so it can be defined as a "center". <p> We need only to show that for any d; d 2 S f+h , we have ADA T = A DA T : By Lemma 2.12, det (A (D + "( D D))A T ) = det (ADA T ); 8" 2 <ref> [0; 1] </ref>:(14) On the other hand, det (A (D + "( D D))A T ) = det (ADA T + "A ( D D)A T ) 1 2 A ( D D)A T (ADA T ) 1 1 Thus, (14) implies det (I + "(ADA T ) 1 2 ) = <p> We therefore change the factors to n, dn, lk, and dl. The problems for the experimental design are as follows. For given factors n, dn, lk, and dl, we take m = n + dn, then generate randomly, according to a uniform distribution on the interval <ref> [1; 1] </ref>, an n by m matrix A; then we generate, according the same distribution, a column and multiply this column by lk and take it as the lower bound l; the upper bound u is simply a column obtained by adding dl to each component of l. <p> special case of the ellipsoids that we are concerned with, therefore, volume (E k+1 ) volume (E k ) volume ( ~ E k+1 ) volume (E k ) = n 1 1 n 2 n exp ( 2n with the last inequality following by, e.g., Corollary 3.7.3 of Akgul <ref> [1] </ref>. The above theorem gives us the convergence rate of the algorithm measured by volume of corresponding ellipsoids. As for the convergence rate measured by the objective value, we have, similar to Theorem 6.1 in Goldfarb and Todd [10], 26 Theorem 4.2.
Reference: [2] <author> D. Avis and V. Chvatal, </author> <title> Notes on Bland's pivoting rule, Mathematical Programming Study, </title> <booktitle> 8 (1978), </booktitle> <pages> pp. 24-34. </pages>
Reference-contexts: Note that most "bound-update" methods, e.g., Levin [16], Newman [21] and Renegar [22], decrease an appropriate bound so as to "push" the current center to approach the optimal solution. Our limited test results are reported in Table 4. The test problems we use are those in Avis and Chvatal <ref> [2] </ref> but with upper bounds on each component of x, namely, max e T x 0 x 10e: 33 Table 4 Computational results of Algorithm 3.3.
Reference: [3] <author> R. M. Bethea, B. S. Duran, and T. L. Boullion, </author> <title> Statistical Methods, </title> <publisher> Marcel Dekker, Inc., </publisher> <year> 1985. </year>
Reference-contexts: Here we have 4 factors, so a suitable model is the Graeco-Latin square design. The idea of this design can be found, for example, in Devore [8] and Bethea, Duran and Boullion <ref> [3] </ref>. Usually, the use of the Graeco-Latin square design can obtain a tremendous saving in time and money in that the total number of observations is greatly reduced.
Reference: [4] <author> R. G. Bland, D. Goldfarb, and M. J. Todd, </author> <title> The ellipsoid method: a survey, </title> <journal> Operations Research, </journal> <volume> 29 (1981), </volume> <pages> pp. 1039-1091. </pages>
Reference-contexts: By scaling and changing variables we can assume, without loss of generality, AA T = I. Thus Algorithm 3.1 is a kind of "ball method" <ref> [4] </ref> for which Todd [28] and Goffin [9] show that an exponential number of iterations may be required. <p> This makes the code simpler yet will not affect the statistical analysis much, since we are mainly interested in the number of iterations and the time is a constant multiple of that using Cholesky factorization. There is an example in Bland, Goldfarb and Todd <ref> [4] </ref>, showing that convergence of the standard ellipsoid algorithm can be extremely slow. The example is A T = I, l = u = 0; by using our models, as well as Burrell and Todd's algorithm [5], the solution can be obtained in just one iteration. <p> Since model I is closely related to the ellipsoid method it can be adapted to a polynomial algorithm for (LP) without getting the exact center, see, e.g., Bland, Goldfarb and Todd <ref> [4] </ref>. We are now going to discuss a practical stopping criterion. <p> To overcome this disadvantage we replace h (d) by a strictly convex function B (d) which forms our second model, model II. Similarly, we propose two algorithms for solving this model: a coordinate descent algorithm and a Newton's algorithm. The first one is actually a ball-like method <ref> [4] </ref> which has been shown to be not a polynomial time algorithm. The second algorithm is a combination of Newton's algorithm and a scaling technique, which works well in practice. The statistical analysis shows that this algorithm is very robust numerically. <p> Or in other words, if x c does not approach the optimal set, x c approaches the optimal set linearly. In addition to the pushing technique similar to that using by the sliding objective function method <ref> [4] </ref> and Renegar [22] we also propose a "pulling" technique: we let the lower bound corresponding to the objective be a very large negative number, i.e., l 0 ~ 1 (or u 0 ~ 1 for the maximizing linear program).
Reference: [5] <author> B. P. Burrell and M. J. Todd, </author> <title> The ellipsoid method generates dual variables, </title> <journal> Mathematics of Operations Research, </journal> <volume> 10 (1985), </volume> <pages> pp. 688-700. </pages>
Reference-contexts: x 0: If the input data are all integers, (ILP) is equivalent to min ~c T x (l =)0 x u: (ILPB) where u = 2 L e with L the input length of (ILP), which can be easily adapted to a form of (LP) (see, e.g., Burrell and Todd <ref> [5] </ref>). So, theoretically speaking, our form (LP) is not restrictive. <p> We also denote by e the all-one vector and e j the j-th column of the identity matrix. In <ref> [5] </ref> Burrell and Todd proposed a parallel-cut ellipsoid algorithm based on the results of Todd [29]. <p> There is an example in Bland, Goldfarb and Todd [4], showing that convergence of the standard ellipsoid algorithm can be extremely slow. The example is A T = I, l = u = 0; by using our models, as well as Burrell and Todd's algorithm <ref> [5] </ref>, the solution can be obtained in just one iteration. This comparison might be not appropriate, but it does show the difference between our model and the ellipsoid method. In the following we investigate the numerical behavior of model II by using the Newton-scaling algorithm, i.e., Algorithm 3.3. <p> We are now going to discuss a practical stopping criterion. The dual of (LP) is: max l T y 1 u T y 2 y 1 ; y 2 0: By the results of Burrell and Todd <ref> [5] </ref>, (DP) is equivalent to max (y) := l T y + u T y (DP 1 ) where y := (maxf0; y i g), y + := (maxf0; y i g), and so y = y + y . <p> By taking y = (c T (ADA T ) 1 c) 2 D (A T z r) where z = x c (c T (ADA T ) 1 c) 1 2 (ADA T ) 1 c and d is such that f (d) = 1, Burrell and Todd <ref> [5] </ref> show that (y) c T z = c T x c (c T (ADA T ) 1 c) 2 : We can use (y) as a lower bound on c T x. <p> Summary and conclusions. In this paper we have analyzed, and tested, some generalized models for linear systems and, later, proposed some algorithms for solving linear programming problems via these models. Foremost among these is the idea of generalizing Burrell and Todd's approach <ref> [5] </ref> which is closely related to the coordinate descent method so that Newton's method can be employed. We develop our models in sections 2 and 3. <p> The advantage of this model is that it is a convex program as well as preserving the advantages of Burrell and Todd's approach <ref> [5] </ref>. We prove the existence and uniqueness of the smallest ellipsoid of the form (3) which contains the feasible region P . The center of this smallest ellipsoid is proved to be an interior point of P ; it is our first kind of center.
Reference: [6] <author> V. Chv atal, </author> <title> Linear Programming, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980. </year>
Reference-contexts: So, theoretically speaking, our form (LP) is not restrictive. On the other hand, since "many linear programming problems involve explicit upper bounds on individual variables [in the standard form]" (Chvatal <ref> [6] </ref>) especially, the upper bounds may stand for limits on resources we thus often, in practice, are confronted with linear programming problems with the following form: min c T x (l =)0 x u fl Advanced Computing Research Institute and Center for Applied Mathematics, Cornell University, Ithaca, New York 14853, partially
Reference: [7] <author> T. F. Coleman and C. V. Loan, </author> <title> Handbook for Matrix Computations, </title> <publisher> SIAM, </publisher> <year> 1988. </year>
Reference-contexts: The third method is QR de composition; using Householder orthogonalization, the QR decomposition of D 1 needs n 2 (m n 3 ) flops. Among these three possible choices the Cholesky Decomposition seems the best. Our computer code is written in MATLAB ([19], <ref> [7] </ref>) and uses LU decomposition because MATLAB finds x = Anb by using the LU decomposition for A.
Reference: [8] <author> J. L. Devore, </author> <title> Probability and Statistics, </title> <publisher> Brooks/Cole Publishing Company, </publisher> <year> 1991. </year>
Reference-contexts: Here we have 4 factors, so a suitable model is the Graeco-Latin square design. The idea of this design can be found, for example, in Devore <ref> [8] </ref> and Bethea, Duran and Boullion [3]. Usually, the use of the Graeco-Latin square design can obtain a tremendous saving in time and money in that the total number of observations is greatly reduced.
Reference: [9] <author> J. L. Goffin, </author> <title> On the non-polynomiality of the relaxation method for system of inequalities, </title> <type> Technical Report, </type> <institution> Faculty of Management, McGill University, </institution> <address> Montreal, Quebec, </address> <year> 1979. </year>
Reference-contexts: By scaling and changing variables we can assume, without loss of generality, AA T = I. Thus Algorithm 3.1 is a kind of "ball method" [4] for which Todd [28] and Goffin <ref> [9] </ref> show that an exponential number of iterations may be required.
Reference: [10] <author> D. Goldfarb and M. J. Todd, </author> <title> Modifications and implementation of the ellipsoid algorithm for linear programming, </title> <journal> Mathematical Programming, </journal> <volume> 23 (1982), </volume> <pages> pp. 1-19. </pages>
Reference-contexts: The above theorem gives us the convergence rate of the algorithm measured by volume of corresponding ellipsoids. As for the convergence rate measured by the objective value, we have, similar to Theorem 6.1 in Goldfarb and Todd <ref> [10] </ref>, 26 Theorem 4.2. Suppose there is a ball B with a radius of ffi 0 contained in the feasible region P of (LP).
Reference: [11] <author> G. H. Golub and C. V. Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <year> 1989. </year>
Reference-contexts: For the general case, instead of calculating (ADA T ) 1 directly, an equivalent linear system is solved. There are many methods for solving linear systems, see, for example, Golub and Van Loan <ref> [11] </ref>. Usually, some decomposition methods are used to reduce the linear system to a simpler system and the solution of the original system can be obtained from that of this simpler system. We would like to mention here three possible decomposition methods.
Reference: [12] <author> R. A. Horn and C. R. Johnson, </author> <title> Matrix Analysis, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: The following result can be found, for example, in <ref> [12] </ref>. 4 Lemma 2.4. If B and C are m by m positive semidefinite matrices, so is B ffi C. Proposition 2.5. f and h are convex functions over D, and hence so is F = f + h. Proof.
Reference: [13] <author> F. John, </author> <title> Extremum problems with inequalities as subsidiary conditions, </title> <booktitle> in Studies and Essays(Courant Anniversary Volume), </booktitle> <publisher> Interscience, </publisher> <address> New York, </address> <year> 1948. </year>
Reference-contexts: Shor [25] independently developed the ellipsoid method. Later, in 1979, Khachiyan [14] showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems. We note that John's result <ref> [13] </ref> shows that for every convex polytope P R n , the minimum volume ellipsoid containing P exists and is unique.
Reference: [14] <author> L. G. Khachiyan, </author> <title> A polynomial algorithm for linear programming, </title> <journal> Doklady Akad. Nauk. USSR, </journal> <volume> 244 (1979), </volume> <pages> pp. 1093-1096. </pages>
Reference-contexts: This modified method is computationally implementable. They also point out that this ellipsoid method is a special case of Shor's algorithm [24] with space dilation in the direction of the subgradient. Shor [25] independently developed the ellipsoid method. Later, in 1979, Khachiyan <ref> [14] </ref> showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems. We note that John's result [13] shows that for every convex polytope P R n , the minimum volume ellipsoid containing P exists and is unique.
Reference: [15] <author> L. G. Khachiyan and M. J. Todd, </author> <title> On the complexity of approximating the maximal inscribed ellipsoid for a polytope, </title> <type> Technical Report 893, </type> <institution> School of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1990. </year>
Reference-contexts: However, the smallest ellipsoid containing P is hard to find in general, and so is its center. Tarasov, Khachiyan and Erlich [27] study the method of inscribed ellipsoids. Actually, the center of the maximal inscribed ellipsoid for a polytope can be regarded as a "center". In <ref> [15] </ref>, Khachiyan and Todd discuss the problem of approximating the maximal inscribed ellipsoid and related problems. They also propose algorithms for finding these ellipsoids. Renegar [22] uses the "analytical center" (see also Sonnevend [26]) to develop his algorithm for linear programming problems, which is polynomial time bounded.
Reference: [16] <author> A. Y. Levin, </author> <title> On an algorithm for the minimization of convex functions, </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 6 (1965), </volume> <pages> pp. 286-290. </pages>
Reference-contexts: The first kind of "center" that was used in optimization is the center of gravity or centroid used by Levin <ref> [16] </ref> in his algorithm, the method of central sections, for minimization of a convex function over a convex polytope P (see also Newman [21]). In that paper the centroid is used as the test point. <p> For example, we can cut the current polytope by a hyperplane through the current center and throw away the part that does not contain the optimal solution and repeat this procedure; we thus get a sequence of shrinking polytopes which contain the optimal solution, as does Levin <ref> [16] </ref> as well as Newman [21]; or push the center towards the optimum by adjusting the appropriate bound as done by Renegar [22]. In section 4.1 and 4.2 we will use the same strategies to develop our algorithms via model I and model II respectively. <p> We call this technique the "pulling" technique, since it pulls the current center to the optimal solution. Note that most "bound-update" methods, e.g., Levin <ref> [16] </ref>, Newman [21] and Renegar [22], decrease an appropriate bound so as to "push" the current center to approach the optimal solution. Our limited test results are reported in Table 4.
Reference: [17] <author> A. Liao, </author> <title> Algorithms for Linear Programming via Weighted Centers, </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <year> 1992. </year>
Reference-contexts: In term of the shrinkage of the volumes of the corresponding ellipsoids, we have volume (E + ) volume (E) ffi n = (1 16n 2 ) n exp ( 16n Although, from the above results, coordinate descent algorithm is polynomial, it converges slowly. Liao <ref> [17] </ref> shows that, although F may not be strictly convex over D as shown earlier in an example, F is strictly convex on fi: fi := R m where &lt; 0 &gt; ? is the orthogonal complement of &lt; 0 &gt; and &lt; 0 &gt;= fd : ADA T = 0g <p> Based on these observations Liao <ref> [17] </ref> proposes a partial Newton-step algorithm which has a stronger convergence property: under certain conditions, certain components of the iterates produced by this algorithm converge quadratically. However, the partial Newton-step algorithm is still not an efficient model because Newton's method cannot be used without restriction. <p> We should mention that this phenomenon did not occur in our computational experiments; in particular, our computational results always gave a feasible solution for this example. We note that Liao <ref> [17] </ref> discusses some other possible models for solving the feasibility problem (1). 3.3. Computational results. In this section we discuss our computational experience with Algorithm 3.3. Obviously, the main computational effort is spent in calculating (ADA T ) 1 .
Reference: [18] <author> A. Liao and M. J. Todd, </author> <title> The ellipsoid algorithm using parallel cuts, </title> <type> Technical Report 997, </type> <institution> School of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1992. </year>
Reference-contexts: Liao and Todd <ref> [18] </ref> show that the Burrell-Todd algorithm is basically 3 the coordinate descent method applied to (P fh ), together with rules for updating the bounds l and u. They also propose a simpler way to perform the updating. <p> As we know Newton's method is fast in practice while the coordinate descent algorithm is usually considered not fast enough for practical use. But here the function v (d) is a homogeneous function of degree 0, and it can be shown (see, for example, Liao and Todd <ref> [18] </ref>) that rv (d) + r 2 v (d) T d = 0; so d is the Newton direction, as well as a direction of constancy for the function, and therefore useless. <p> a T where ~ d k+1 0 = 1 Then, scale d k+1 so that f (d k+1 ) = h (d k+1 ), where f is computed with the updated j-th bounds; set k k + 1, and repeat. 11 2 The difference from the Liao and Todd algorithm <ref> [18] </ref> is that here the objective function is F (d) := f (d) + h (d) instead of v (d) := f (d) h (d) in the Liao and Todd algorithm. Theorem 2.17. Suppose that fd k g is the sequence generated by the algorithm. <p> Let ~ d k+1 be d k + k e j (the next iterate before scaling). Also let = fl and 0 := j x c l j )(a T s 2 0. Then the same analysis as in Liao and Todd <ref> [18] </ref> leads to F ( ~ d k+1 ) = 1 + F (d k ) ( 0 ) (1 (1 + ) 1 = 1 f (d k ) + h (d k ) n fi 1 + = 1 2 n fi 1 + Let n fi 1 + <p> F ( ~ d k+1 ) = 1 + F (d k ) ( 0 ) (1 (1 + ) 1 = 1 f (d k ) + h (d k ) n fi 1 + = 1 2 n fi 1 + Let n fi 1 + As in <ref> [18] </ref>, it is enough to deal with the canonical case, i.e., fi 1 4 . <p> For simplicity we write d for d k , for k and d + = d + k e j . Then B (d + ) = B (d) d j (d j + ) and from <ref> [18] </ref> f (d + ) = f (d) + f (d) 1 + fi j 2 ); where fi j = fi j (d), = fl j , fl j = a T j (ADA T ) 1 a j and 0 := j x c l j )(a T s <p> The center of this smallest ellipsoid is proved to be an interior point of P ; it is our first kind of center. For solving model I we propose two algorithms: the first one is a coordinate descent method obtained by modifying the Liao and Todd algorithm <ref> [18] </ref>. It is proved to have a polynomial time bound. Since the objective function of model I might be not strictly convex, it thus prevents the use of a full version of Newton's method.
Reference: [19] <author> C. B. Moler, J. Little, S. Bangret, and S. Kleiman, </author> <title> Pro-Matlab User's Guide, The MathWorks, </title> <publisher> Inc., </publisher> <year> 1987. </year>
Reference: [20] <author> J. L. Nazareth, </author> <title> Pricing criteria in linear programming, </title> <booktitle> in Progress in Mathematical Programming, </booktitle> <editor> N. Meggiddo, ed., </editor> <address> New York, 1989, </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 105-130. </pages>
Reference-contexts: We reduce (32) to our form (LP): max e T x (33) with l = 0; ~ N = N We choose q = 10 8 . The data in the column corresponding to the simplex method are from Nazareth <ref> [20] </ref>; these are average numbers of iterations for a number of random problems. := f (d)c T (ADA T ) 1 c is the width of the ellipsoid in the direction c after the final iteration and it serves here as the stopping criterion (we terminate if 10 3 ).
Reference: [21] <author> D. J. Newman, </author> <title> Location of the maximum on unimodal surfaces, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 12 (1965), </volume> <pages> pp. 395-398. </pages>
Reference-contexts: The first kind of "center" that was used in optimization is the center of gravity or centroid used by Levin [16] in his algorithm, the method of central sections, for minimization of a convex function over a convex polytope P (see also Newman <ref> [21] </ref>). In that paper the centroid is used as the test point. <p> we can cut the current polytope by a hyperplane through the current center and throw away the part that does not contain the optimal solution and repeat this procedure; we thus get a sequence of shrinking polytopes which contain the optimal solution, as does Levin [16] as well as Newman <ref> [21] </ref>; or push the center towards the optimum by adjusting the appropriate bound as done by Renegar [22]. In section 4.1 and 4.2 we will use the same strategies to develop our algorithms via model I and model II respectively. <p> We call this technique the "pulling" technique, since it pulls the current center to the optimal solution. Note that most "bound-update" methods, e.g., Levin [16], Newman <ref> [21] </ref> and Renegar [22], decrease an appropriate bound so as to "push" the current center to approach the optimal solution. Our limited test results are reported in Table 4.
Reference: [22] <author> J. Renegar, </author> <title> A polynomial-time algorithm based on Newton's method for linear programming, </title> <journal> Mathematical Programming, </journal> <volume> 40 (1988), </volume> <pages> pp. 59-93. </pages>
Reference-contexts: Actually, the center of the maximal inscribed ellipsoid for a polytope can be regarded as a "center". In [15], Khachiyan and Todd discuss the problem of approximating the maximal inscribed ellipsoid and related problems. They also propose algorithms for finding these ellipsoids. Renegar <ref> [22] </ref> uses the "analytical center" (see also Sonnevend [26]) to develop his algorithm for linear programming problems, which is polynomial time bounded. The analytical center is easier to approximate compared with the centroid. Actually, in his algorithm, Renegar showed that an "*-analytical center" is enough. <p> The analytical center is easier to approximate compared with the centroid. Actually, in his algorithm, Renegar showed that an "*-analytical center" is enough. Unlike the centroid, the analytical center is not analytically independent; it depends on the way 2 in which the polytope P is represented. Renegar <ref> [22] </ref> makes use of this property to improve the convergence of his algorithm by adding some extra constraints. <p> part that does not contain the optimal solution and repeat this procedure; we thus get a sequence of shrinking polytopes which contain the optimal solution, as does Levin [16] as well as Newman [21]; or push the center towards the optimum by adjusting the appropriate bound as done by Renegar <ref> [22] </ref>. In section 4.1 and 4.2 we will use the same strategies to develop our algorithms via model I and model II respectively. <p> We call this technique the "pulling" technique, since it pulls the current center to the optimal solution. Note that most "bound-update" methods, e.g., Levin [16], Newman [21] and Renegar <ref> [22] </ref>, decrease an appropriate bound so as to "push" the current center to approach the optimal solution. Our limited test results are reported in Table 4. <p> Or in other words, if x c does not approach the optimal set, x c approaches the optimal set linearly. In addition to the pushing technique similar to that using by the sliding objective function method [4] and Renegar <ref> [22] </ref> we also propose a "pulling" technique: we let the lower bound corresponding to the objective be a very large negative number, i.e., l 0 ~ 1 (or u 0 ~ 1 for the maximizing linear program).
Reference: [23] <author> R. T. Rockafellar, </author> <title> Convex Analysis, </title> <publisher> Princeton University Press, </publisher> <year> 1970. </year>
Reference-contexts: Theorem 2.7. If l &lt; u, and A is of full rank, then P = ; () f (d) &lt; 0; for some d 2 D: Proof. " =)". First of all, we recall Helly's Theorem which can be found, e.g., in Rockafellar <ref> [23] </ref>. Let fC i ji 2 Ig be a finite collection of convex sets in R n . If every subcollection consisting of n + 1 or fewer sets has a non-empty intersection, then the entire collection has a non-empty intersection. <p> We have thus proved that ~ F has no recession direction in R m . Therefore, by Theorem 27.3 of Rockafellar <ref> [23] </ref>, ~ F attains its minimum in R m . Equivalently, F attains its minimum over D, i.e., S f+h is not empty. The following corollaries show that, under assumption (A1), (P f+h ) and (P fh ) are essentially the same. Corollary 2.9.
Reference: [24] <author> N. Z. Shor, </author> <title> Utilization of the operation of space dilatation in the minimization of convex functions, </title> <journal> Cybernetics, </journal> <volume> 6 (1970), </volume> <pages> pp. </pages> <month> 7-15. </month> <title> [25] , Cut-off method with space extension in convex programming problems, </title> <journal> Cybernetics, </journal> <volume> 13 (1977), </volume> <pages> pp. 94-96. </pages>
Reference-contexts: Yudin and Nemirovskii [31] discuss the computational difficulties of Levin's method and propose a modified method of centered cross-sections, using ellipsoids instead of polyhedra. This modified method is computationally implementable. They also point out that this ellipsoid method is a special case of Shor's algorithm <ref> [24] </ref> with space dilation in the direction of the subgradient. Shor [25] independently developed the ellipsoid method. Later, in 1979, Khachiyan [14] showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems.
Reference: [26] <author> G. Sonnevend, </author> <title> An analytical center for polyhedrons and new classes of global algorithms for linear (smooth, convex) programming, </title> <booktitle> in Lecture Notes in Control and Information Sciences, </booktitle> <volume> No. </volume> <pages> 84, </pages> <address> Berlin, 1986, </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 866-875. </pages>
Reference-contexts: In [15], Khachiyan and Todd discuss the problem of approximating the maximal inscribed ellipsoid and related problems. They also propose algorithms for finding these ellipsoids. Renegar [22] uses the "analytical center" (see also Sonnevend <ref> [26] </ref>) to develop his algorithm for linear programming problems, which is polynomial time bounded. The analytical center is easier to approximate compared with the centroid. Actually, in his algorithm, Renegar showed that an "*-analytical center" is enough.
Reference: [27] <author> S. P. Tarasov, L. G. Khachiyan, and I. I. Erlich, </author> <title> The method of inscribed ellipsoids, </title> <journal> Soviet Math. Dokl., </journal> <month> 37 </month> <year> (1988). </year>
Reference-contexts: Thus, the center of this ellipsoid can be used as a "center". However, the smallest ellipsoid containing P is hard to find in general, and so is its center. Tarasov, Khachiyan and Erlich <ref> [27] </ref> study the method of inscribed ellipsoids. Actually, the center of the maximal inscribed ellipsoid for a polytope can be regarded as a "center". In [15], Khachiyan and Todd discuss the problem of approximating the maximal inscribed ellipsoid and related problems. They also propose algorithms for finding these ellipsoids.
Reference: [28] <author> M. J. Todd, </author> <title> Some remarks on the relaxation method for linear inequalities, </title> <type> Technical Report 419, </type> <institution> School of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1979. </year> <title> [29] , On minimum volume ellipsoids containing part of a given ellipsoid, </title> <journal> Mathematics of Operations Research, </journal> <volume> 7 (1980), </volume> <pages> pp. 253-261. </pages>
Reference-contexts: By scaling and changing variables we can assume, without loss of generality, AA T = I. Thus Algorithm 3.1 is a kind of "ball method" [4] for which Todd <ref> [28] </ref> and Goffin [9] show that an exponential number of iterations may be required.
Reference: [30] <author> P. M. Vaidya, </author> <title> A new algorithm for minimizing convex functions over convex sets, </title> <type> Technical Report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1989. </year>
Reference-contexts: Unlike the centroid, the analytical center is not analytically independent; it depends on the way 2 in which the polytope P is represented. Renegar [22] makes use of this property to improve the convergence of his algorithm by adding some extra constraints. In the paper <ref> [30] </ref> Vaidya introduces the "volumetric center" which is the center of the ellipsoid with largest volume among a certain set of ellipsoids that are contained in P and proposes an algorithm with a better global convergence rate and time complexity than the ellipsoid method.
Reference: [31] <author> D. B. Yudin and A. S. Nemirovskii, </author> <title> Informational complexity and efficient methods for the solution of convex extremal problems, </title> <journal> Matekon, </journal> <volume> 13 (1976), </volume> <pages> pp. 3-25. 37 </pages>
Reference-contexts: This method is very concise and 1 exp (1) is a guaranteed reduction of the volume of successive polytopes. The disadvantage is the difficulty of calculating the centroid. Yudin and Nemirovskii <ref> [31] </ref> discuss the computational difficulties of Levin's method and propose a modified method of centered cross-sections, using ellipsoids instead of polyhedra. This modified method is computationally implementable. <p> In section 4.3 we will propose a pulling technique so that, empirically, solving linear programming problem can be reduced to solving a linear system without the above disadvantages. 4.1. Sliding method via model I. The sliding objective function method was first proposed by Yudin and Nemirovskii <ref> [31] </ref> and Shor [25]. The idea is to reduce the linear programming problem to a sequence of feasibility problems formed by letting the objective be an extra constraint and decreasing the bound corresponding to the objective function as long as it is possible. Suppose we are to solve (LP).
References-found: 29

