URL: http://www.cs.rutgers.edu/pub/technical-reports/lcsr-tr-229.ps.Z
Refering-URL: http://athos.rutgers.edu/~arunava/resource.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: arunava@cs.rutgers.edu  
Title: Initializing Neural Networks using Decision Trees  
Author: Arunava Banerjee 
Address: New Brunswick, NJ 08903  
Affiliation: Department of Computer Science Rutgers University  
Abstract: Existing approaches to the inductive learning problem include Symbolic and Connectionist algorithms. While the Symbolic approach is generally found to run significantly faster during learning, the Connectionist algorithms are often more accurate at classifying novel examples in the presence of noisy data. This paper presents a technique that determines the topology and initial weights of a neural network using a decision tree, thus combining both approaches. Experimental results on benchmark real-world datasets indicate that this technique outperforms the above mentioned approaches both in efficiency and accuracy.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Breiman, L., Friedman, J., Olshen, R., and Stone, C. </author> <title> (1984) Classification and Regression Trees. </title> <publisher> Wadsworth, Monterrey, </publisher> <address> Ca. </address>
Reference: [2] <author> Brent, R., P. </author> <title> (1991) Fast Training Algorithms for Neural Nets. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Volume 2. </volume>
Reference: [3] <author> Cios, K., J., and Liu, N. </author> <title> (1992) A Machine Learning Method for Generation of a Neural Network Architecture: A Continuous ID3 Algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Volume 3. </volume>
Reference: [4] <author> Mooney, R., J., Shavlik, J., W., Towell, G., G., and Gove A. </author> <title> (1989) An Experimental Comparison of Symbolic and Connectionist Learning Algorithms. </title> <booktitle> Proceedings of the 11th IJCAI. </booktitle>
Reference: [5] <author> Quinlan, J., R. </author> <title> (1986) Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> Volume 1. </volume>
Reference: [6] <author> Rosenblatt, F. </author> <title> (1958) The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> Volume 65. </volume> <pages> 13 </pages>
Reference: [7] <author> Rumelhart, D., E., Hinton, G., E., and Williams, R., J., </author> <title> (1986) Learning Internal Representations by Error Propagation. Parallel Distributed Processing Volume 1. </title> <editor> Editors D. E. Rumelhart, and J. L. McClelland, </editor> <publisher> MIT Press. </publisher>
Reference: [8] <author> Sethi, I., K. </author> <title> (1990) Entropy Nets: From Decision Trees to Neural Networks. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> Volume 78. </volume>
Reference: [9] <author> Towell, G., G., and Shavlik, J., W. </author> <title> (1993) Extracting Refined Rules from Knowledge-Based Neural Networks. </title> <journal> Machine Learning, </journal> <volume> Volume 13. </volume>
Reference: [10] <author> Utgoff, P., E., </author> <title> (1989) Perceptron Trees: A case study in Hybrid Concept Representations. </title> <journal> Connection Science, </journal> <volume> Volume 1. </volume>
Reference: [11] <author> Weiss, S., and Kapouleas, I. </author> <title> (1989) An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods. </title> <booktitle> Proceedings of the 11th IJCAI. </booktitle> <pages> 14 </pages>
References-found: 11

