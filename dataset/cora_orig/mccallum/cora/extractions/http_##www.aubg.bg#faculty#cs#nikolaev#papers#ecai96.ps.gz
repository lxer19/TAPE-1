URL: http://www.aubg.bg/faculty/cs/nikolaev/papers/ecai96.ps.gz
Refering-URL: http://www.aubg.bg/faculty/cs/nikolaev/pbl.html
Root-URL: 
Title: Stochastically Guided Disjunctive Version Space Learning  
Author: Nikolay I. Nikolaev and Evgueni N. Smirnov 
Abstract: This paper presents an incremental concept learning approach to identiflcation of concepts with high overall accuracy. The main idea is to address the concept overlap as a central problem when learning multiple descriptions. Many traditional inductive algorithms, as those from the disjunctive version space family considered here, face this problem. The approach focuses on combinations of confldent, possibly overlapping, concepts with an original stochastic complexity formula. The focusing is e-cient because it is organized as a simulated annealing-based beam search. The experiments show that the approach is especially suitable for developing incremental learning algorithms with the following advantages: flrst, it generates highly accurate concepts; second, it overcomes to a certain degree the sensitivity to the order of examples; and third, it handles noisy examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali, and, M. Pazzani, </author> <title> 'Error Reduction through Learning Multiple Models', </title> <journal> Machine Learning, </journal> <note> 1996 ( to appear </note> ). 
Reference-contexts: This paper explores the possibility for overfltting avoidance in incremental learning using multiple concept descriptions. Researchers already studied that learning multiple descriptions is increasingly better than learning a single description from the same data <ref> [1] </ref>. Within the multiple descriptions paradigm, however, the concepts are often overlapping and this inuences the overall accuracy [14]. <p> Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics <ref> [1] </ref>, [3], [7], [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. <p> Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics <ref> [1] </ref>, [3], [7], [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7].
Reference: [2] <author> W. Buntine, 'Classiflers: </author> <title> A Theoretical and Empirical Study', </title> <booktitle> Proc. of the Twelfth Int. Joint Conf. on Artiflcial Intelligence, IJCAI-91, </booktitle> <address> Australia, </address> <year> 1991. </year>
Reference-contexts: Moreover, the incremental inductive process depends on the presentation order of the examples [5], [11]. These aspects, essential for making better incremental learners, could be handled with appropriate overfltting avoidance metrics <ref> [2] </ref>. This paper explores the possibility for overfltting avoidance in incremental learning using multiple concept descriptions. Researchers already studied that learning multiple descriptions is increasingly better than learning a single description from the same data [1]. <p> Nikolaev and E. Smirnov ECAI 96. 12th European Conference on Artiflcial Intelligence Edited by W. Wahlster Published in 1996 by John Wiley & Sons, Ltd. The stochastic formula proved to be a remedy for avoid-ing overfltting with the sequentially provided examples <ref> [2] </ref>. The experiments show that ISF has three advantages: flrst, it produces concepts with high overall accuracy compared to the primary method and several other incremental learners; second, it overcomes to a certain degree the sensitivity to the order of the examples; and third, it handles noisy examples.
Reference: [3] <author> P. Clark, and R. Boswell, </author> <title> 'Rule Induction with CN2: Recent Improvements', </title> <booktitle> Proc. European Working Session on Learning, EWSL 91, Porto, Portugal, </booktitle> <pages> 151-163, </pages> <year> 1991. </year>
Reference-contexts: Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], <ref> [3] </ref>, [7], [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. <p> Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], <ref> [3] </ref>, [7], [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. The third strategy flnds maximally accurate homogenous concepts whose accuracy does not change in the context of the flnal concept [14].
Reference: [4] <author> P. Clark, and T. Niblett, </author> <title> 'The CN2 Induction Algorithm', </title> <journal> Machine Learning, </journal> <volume> 3:4, </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: The concept overlap problem has been investigated only in batch inductive learning. One of the batch approaches to this problem is to learn a description, then to discard the examples covered by it so that subsequent descriptions are isolated from the remaining examples, and next to collect all descriptions <ref> [4] </ref>, [12]. Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], [7], [8], [14]. There are several such strategies [1], [8].
Reference: [5] <author> A. Cornuejols, </author> <title> 'Getting Order Independence in Incremental Learning', </title> <booktitle> Proc. 1993 Spring Symp. Training Issues in Incremental Learning, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, 43-53, </address> <year> 1993. </year>
Reference-contexts: The incremental induction is expected to increase at each step the belief in some of the concept descriptions and to eliminate redundant ones, which do not contribute substantially for modeling the examples [9]. Moreover, the incremental inductive process depends on the presentation order of the examples <ref> [5] </ref>, [11]. These aspects, essential for making better incremental learners, could be handled with appropriate overfltting avoidance metrics [2]. This paper explores the possibility for overfltting avoidance in incremental learning using multiple concept descriptions.
Reference: [6] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt and M.P. Vecci, 'Optimization by Simulated Annealing', </title> <journal> Science, </journal> <volume> 220, </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: For comparison, the worst case time complexity of Space Fragmenting is O (( E +E ) n 2E ) [15]. Machine Learning 3 N. Nikolaev and E. Smirnov 3.3 Simulated annealing-based search The focusing is developed as a simulated annealing-based <ref> [6] </ref> search guided by the stochastic complexity formula.
Reference: [7] <author> I. Kononenko, </author> <title> 'ID3, Sequential Bayes, Nave Bayes and Bayesian Neural Networks', </title> <booktitle> Proc. 4th European Working Eu-ropean Working Session on Learning, </booktitle> <address> EWSL 91, Montpellier, France, 91-98, </address> <year> 1989. </year>
Reference-contexts: Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], <ref> [7] </ref>, [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. <p> <ref> [7] </ref>, [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. The third strategy flnds maximally accurate homogenous concepts whose accuracy does not change in the context of the flnal concept [14]. A difierent approach to handling multiple overlapping concept descriptions especially suitable for incremental learners is explored here.
Reference: [8] <author> I. Kononenko, and M. </author> <title> Kovacic, 'Learning as Optimization: Stochastic Generation of Multiple Knowledge', </title> <booktitle> Proc. of the Ninth International Workshop on Machine Learning, ML92, Aberdeen, Scotland, </booktitle> <pages> 257-262, </pages> <year> 1992. </year>
Reference-contexts: Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], [7], <ref> [8] </ref>, [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. <p> Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], [7], <ref> [8] </ref>, [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. <p> Therefore the computation of the probability estimates becomes easier with the development of the learning process. 5 RELEVANCE TO OTHER WORKS The ISF algorithm could be compared to some batch approaches. The work in <ref> [8] </ref> presents stochastic search algorithms for learning multiple descriptions of several concepts. Nevertheless an heuristic for combining descriptions covering disjoint sets of examples is used, flnally only one description for each concept is produced which restricts the reduction of the classiflcation errors.
Reference: [9] <author> M. Li and P. Vitanyi, </author> <title> An Introduction to Kolmogorov Complexity and Its Applications, </title> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1993. </year>
Reference-contexts: The incremental induction is expected to increase at each step the belief in some of the concept descriptions and to eliminate redundant ones, which do not contribute substantially for modeling the examples <ref> [9] </ref>. Moreover, the incremental inductive process depends on the presentation order of the examples [5], [11]. These aspects, essential for making better incremental learners, could be handled with appropriate overfltting avoidance metrics [2]. This paper explores the possibility for overfltting avoidance in incremental learning using multiple concept descriptions.
Reference: [10] <author> T. M. Mitchell, </author> <title> 'Version Spaces: An Approach to Concept Learning', </title> <type> PhD Thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1978. </year>
Reference-contexts: In addition, at least one maximally general consistent concept description that covers only this subset of positive examples should exist. The maximally complete concept descriptions corresponding to the same subset of positive examples form a classical version space <ref> [10] </ref>. Every classical version space could be considered a set of fragments. A fragment F i consists of concept descriptions restricted by one maximally general and one maximally speciflc description in the partial ordering structure of the concept description language. <p> When a positive example comes, the fragments F i that cover it are isolated. They are generalized and together with the uncovered old fragments form the updated disjunctive version space F . The disjunctive version space is processed by manipulating only its boundary sets <ref> [10] </ref>, [15]. The algorithm always produces a correct description, as the fulflllment of the incompleteness and inconsistency conditions is avoided [15]. The learning terminates when each fragment consists of only one element.
Reference: [11] <author> K. Murray, </author> <title> 'Multiple Convergence: An Approach to Disjunctive Concept Acquisition', </title> <booktitle> Proc. Int. Joint Conf. on Artiflcial Intelligence, IJCAI-87, vol.1, Italy, </booktitle> <pages> 297-300, </pages> <year> 1987. </year>
Reference-contexts: The incremental induction is expected to increase at each step the belief in some of the concept descriptions and to eliminate redundant ones, which do not contribute substantially for modeling the examples [9]. Moreover, the incremental inductive process depends on the presentation order of the examples [5], <ref> [11] </ref>. These aspects, essential for making better incremental learners, could be handled with appropriate overfltting avoidance metrics [2]. This paper explores the possibility for overfltting avoidance in incremental learning using multiple concept descriptions. <p> In addition, the incremental ID5 carries out more exible search as it can heuristically reconstruct its decision tree, but requires keeping of both types of examples and always produces disjoint concept descriptions. Like ISF, the incremental Multiple Convergence (MC) <ref> [11] </ref> saves only the positive examples. This helps to recover the positive experience and thus to achieve examples order independence.
Reference: [12] <author> B. Pfahringer, </author> <title> 'A New MDL Measure for Robust Rule Induction', </title> <booktitle> Proc. 8th European Conference on Machine Learning, </booktitle> <address> ECML-95, Crete, Grece, 331-334, </address> <year> 1995. </year>
Reference-contexts: One of the batch approaches to this problem is to learn a description, then to discard the examples covered by it so that subsequent descriptions are isolated from the remaining examples, and next to collect all descriptions [4], <ref> [12] </ref>. Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples [14]. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], [7], [8], [14]. There are several such strategies [1], [8].
Reference: [13] <author> J. Rissanen, </author> <title> 'Stochastic Complexity', </title> <journal> Journal of the Royal Statistical Society, series B, </journal> <volume> 49, </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: The learning terminates when each fragment consists of only one element. Then a complete concept deflnition is generated, which is a disjunction of all such elements from the fragments. 3 INCREMENTAL STOCHASTIC FOCUSING 3.1 The stochastic complexity formula 3.1.1 Syntactic complexity and predictive accuracy The stochastic complexity <ref> [13] </ref> (or minimum description length) principle provides the syntactic complexity and predictive accuracy as integrated measures for fragments justiflcation. The syntactic complexity is an estimation of the syntactic potential of the fragments.
Reference: [14] <author> R. Segal and O. Etzioni, </author> <title> 'Learning Decision Lists Using Homogeneous Rules', </title> <booktitle> Proc. of the 12th Nat. Conf. on Artiflcial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Researchers already studied that learning multiple descriptions is increasingly better than learning a single description from the same data [1]. Within the multiple descriptions paradigm, however, the concepts are often overlapping and this inuences the overall accuracy <ref> [14] </ref>. <p> not only a function of the accuracy 1 Dept. of Computer Science, American University in Bulgaria, Blagoevgrad 2700, Bulgaria, nikolaev@nws.aubg.bg 2 Institute for Information Technologies, Bulgarian Academy of Sciences, Acad.G.Bonchev Str., blok 29A, Sofla 1113, Bulgaria, smirnov@iinf.bg of its constituent descriptions but also a function of the overlappings between them <ref> [14] </ref>. Many inductive learners identify descriptions of the concept separately, in isolation, and fail to recognize their interdependencies. That is why the heuristics for combining the descriptions usually amalgamate the accuracy of the constituents without improving considerably the overall rate. <p> Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples <ref> [14] </ref>. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], [7], [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. <p> Although accurate results are produced in this way, such algorithms are restricted to learn from reduced sets of examples <ref> [14] </ref>. Another batch approach is to produce multiple descriptions and to combine them using heuristics [1], [3], [7], [8], [14]. There are several such strategies [1], [8]. The flrst strategy examines sums of distributions of all concepts that cover a given instance for selecting the best majority [3]. The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. <p> The second strategy uses nave Bayesian formulas for identifying the best combination of concepts that cover a given example [7]. The third strategy flnds maximally accurate homogenous concepts whose accuracy does not change in the context of the flnal concept <ref> [14] </ref>. A difierent approach to handling multiple overlapping concept descriptions especially suitable for incremental learners is explored here. The idea is to justify the confl-dent from the multiple descriptions by a general stochastic complexity formula that accounts for the overlap-pings between them. <p> The work in [8] presents stochastic search algorithms for learning multiple descriptions of several concepts. Nevertheless an heuristic for combining descriptions covering disjoint sets of examples is used, flnally only one description for each concept is produced which restricts the reduction of the classiflcation errors. The batch approach BruteDL <ref> [14] </ref> resembles to ISF in that it performs a probabilistically search for solving the overlap problem. The concept overlap problem however is only expected to be solved implicitly without real accounting for the relations between the constituents.
Reference: [15] <author> E. Smirnov, </author> <title> 'Space Fragmenting AMethod for Disjunctive Concept Acquisition', </title> <booktitle> Proc. Artiflcial Intelligence V: Methodology, Systems, Applications, </booktitle> <editor> B. du Boulay and V.Sgurev (eds.), </editor> <publisher> Elsevier Science Publ., Amsterdam, </publisher> <pages> 97-104, </pages> <year> 1992. </year>
Reference-contexts: This formula combines the empirical evidence, using the remembered positive examples. In the proposed framework, the formula enables focusing on confldent descriptions in the incremental induction performed by a disjunctive version space learner. The Space Fragmenting algorithm <ref> [15] </ref> is employed for generation of multiple descriptions from the disjunctive version space of the unknown concept. The novel Incremental Stochastic Focusing (ISF) is an improvement of Space Fragmenting because it organizes a simulated annealing-based beam search guided by the stochastic complexity formula. <p> Section three ofiers the ISF in three subsections: the stochastic complexity formula, the learning scenario, and the annealing-based search. The performance of ISF is given in section four. Finally comparisons with related works are made and conclusions are derived. 2 SPACE FRAGMENTING Space Fragmenting (SF) <ref> [15] </ref> is a strongly incremental algorithm for disjunctive concept learning. <p> When a positive example comes, the fragments F i that cover it are isolated. They are generalized and together with the uncovered old fragments form the updated disjunctive version space F . The disjunctive version space is processed by manipulating only its boundary sets [10], <ref> [15] </ref>. The algorithm always produces a correct description, as the fulflllment of the incompleteness and inconsistency conditions is avoided [15]. The learning terminates when each fragment consists of only one element. <p> The disjunctive version space is processed by manipulating only its boundary sets [10], <ref> [15] </ref>. The algorithm always produces a correct description, as the fulflllment of the incompleteness and inconsistency conditions is avoided [15]. The learning terminates when each fragment consists of only one element. <p> For comparison, the worst case time complexity of Space Fragmenting is O (( E +E ) n 2E ) <ref> [15] </ref>. Machine Learning 3 N. Nikolaev and E. Smirnov 3.3 Simulated annealing-based search The focusing is developed as a simulated annealing-based [6] search guided by the stochastic complexity formula.
Reference: [16] <editor> S.B. Thrun, et al., </editor> <title> 'The MONKs Problems APerformance Comparison of Difierent Learning Algorithms', </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year> <title> Machine Learning 5 N. </title> <editor> Nikolaev and E. </editor> <publisher> Smirnov </publisher>
Reference-contexts: They demonstrate that ISF outperforms some popular incremental learners (Table 1) tested on two benchmark datasets <ref> [16] </ref>. The results of ID5R, IDL, ID5R-hat are taken from [16], while these of MC, SF and ISF (Table 2) are derived with implementations of the authors. MONKS-1 MONKS-2 ID5R 81.7% 61.8% ID5R-hat 90.3% 65.7% SF 74.2% 5:2 58.5% 7:1 Table 1. <p> They demonstrate that ISF outperforms some popular incremental learners (Table 1) tested on two benchmark datasets <ref> [16] </ref>. The results of ID5R, IDL, ID5R-hat are taken from [16], while these of MC, SF and ISF (Table 2) are derived with implementations of the authors. MONKS-1 MONKS-2 ID5R 81.7% 61.8% ID5R-hat 90.3% 65.7% SF 74.2% 5:2 58.5% 7:1 Table 1.
References-found: 16

