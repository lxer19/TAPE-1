URL: http://http.cs.berkeley.edu/~jjones/Work/memband.ps
Refering-URL: http://http.cs.berkeley.edu/~jjones/Work/
Root-URL: http://www.cs.berkeley.edu
Title: Quantifying the Memory Bandwidth Requirements of Scientific Applications  
Author: Nelson Chow Jeff Jones 
Date: June 3, 1994  
Abstract: As microprocessors become faster and faster relative to memory, the memory system becomes, more and more, the limiting factor of a computer system's performance. In particular, David Bailey [3] points out the importance of main memory bandwidth for scientific applications. Our goal is to characterize the memory bandwidth requirements of scientific applications, and provide a method which will enable designers to determine quantitatively an appropriate memory system design. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.M. </author> <title> Amdahl. Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities. Proceedings of AFIPS 1967 Spring Joint Computer Con 10 Application Problem Size Matching Points (bytes/cycle) LINPACK 200 fi 200 1.7@64KB LINPACK, blocked 200 fi 200 0.4(?)@64KB Bucket Sort 512K keys, maxkey = 256K 9.0,10.5@64KB Bucket Sort, modified 512K keys, maxkey = 256K 3.5,10.5@64KB Conjugate Gradient Vector size = 1400, </title> <note> Nonzeros = 103000 1.9@256KB Embarassingly Parallel 2 22 trials 0.6@8KB 3-D FFT 64 fi 64 fi 32 1.9(?),5.5@64KB Multigrid 64 fi 64 fi 64 0.7@256KB ference, 30 April, </note> <institution> Atlantic City, </institution> <address> NJ, USA : 1967, p. </address> <pages> 483-485. </pages>
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide, Release 1.0. </title> <address> Philadelphia, PA, USA: </address> <publisher> Society for Industrial and Applied Mathematics Press, </publisher> <year> 1992. </year>
Reference-contexts: Another approach to the problem is to reduce the demands of applications. Reducing the memory bandwidth requirements of scientific applications is an area of intense ongoing research. The most common technique for improving the locality of algorithms, specifically scientific algorithms, is blocking, also known as tiling. <ref> [2, 8, 15, 16] </ref> Improving the locality of an algorithm increases the performance of the memory hierarchy, by increasing the probability that a needed datum will be near the processor. <p> We first divide the matrix into m n fi n b column blocks, where m = n=n b , so A = [A (1) : : : A (m) ]. To factor A, we factor each of the column blocks in turn, updating the rest of the matrix. See <ref> [2] </ref> and [8] for details. for l = 1 : m Factor P A (l) = LU k b = k + n b Apply P to prior columns A (1 : n; 1 : k 1) and later columns A (1 : n; k b : n) Update block row
Reference: [3] <author> D.H. Bailey. </author> <title> RISC Microprocessors and Scientific Computing. </title> <booktitle> Proceedings of Supercomputing '93 : Portland, </booktitle> <address> Oregon, Novem-ber 15-19. Los Alamitos, CA, </address> <publisher> USA : IEEE Computer Society Press, </publisher> <year> 1993, </year> <pages> p. 645-654. </pages>
Reference: [4] <author> D.H. Bailey, E. Barszcz, J.T. Barton, D.S. Browning, R.L. Carter, L. Dagum, R.A. Fatoohi, P.O. Frederickson, T.A. Lasinski, R.S. Schreiber, H.D. Simon, V. Venkatakr-ishnan, and S.K. Weeratunga. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <note> v5, n3 (Fall 1991), </note> <author> p. </author> <month> 63-73. </month>
Reference-contexts: More importantly, 8 9 this small change resulted in a 30% improve-ment in execution time. 4.5 Interpretation Table 4.5 shows the results from interpreting the bandwidth profiles and speedup curves for the LINPACK benchmarks and for five of the NAS benchmarks <ref> [4] </ref>, as well as the problem sizes used in our simulations. Note that the problem sizes were limited both by memory requirements and by simulation time.
Reference: [5] <author> J. Barreh, S. Dhawan, T. Hicks, D. Shippy. </author> <title> The POWER2 Processor. </title> <booktitle> Proceedings of COMPCON '94, </booktitle> <address> San Francisco, CA, USA, 28 February - 4 March 1994). Los Alamitos, CA, USA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994, </year> <pages> p. 389-98. </pages>
Reference-contexts: But this requires a matched width throughout the memory system, and the buses can only be widened to a certain extent before they become too large, unwieldy, and costly. This is the approach taken by IBM's POWER2 architecture, which supports dual quad-word buses <ref> [5] </ref>. These dual units, wide buses, and supporting quad-word storage instructions give this architecture a significant advantage over others in main memory bandwidth. Another common approach to increased throughput is to interleave.
Reference: [6] <author> D. Bursky. </author> <title> Synchronous DRAMs clock at 100 MHz. </title> <booktitle> Electronic Design v41, n4 (Febru-ary 18, 1993), p. </booktitle> <pages> 45-49 </pages>
Reference-contexts: Historically, DRAMs have been controlled asynchronously. By controlling a DRAM synchronously, all DRAM inputs are sampled at the positive edge of the input clock, and all DRAM outputs are valid on that positive edge as well <ref> [6, 21] </ref>. This allows I/O transactions to take place every clock cycle. By referencing all inputs and outputs to the rising edge of the clock pulse, this method can supply a peak bandwidth equal to the system's clock frequency multiplied by the number of lines in the system's bus.
Reference: [7] <author> S. Carr and K. Kennedy. </author> <title> Compiler Blocka-bility of Numerical Algorithms. </title> <booktitle> Proceedings of Supercomputing '92 : Washington, </booktitle> <address> D.C., </address> <month> July 19-23. </month> <institution> Baltimore, MD, USA: Association for Computing Machinery, </institution> <year> 1992, </year> <pages> p. 114-124. </pages>
Reference-contexts: All references to that block of the result are made during this period, so the locality of reference has improved. Currently, automatic methods for blocking have not proved especially effective, although this is a significant amount of ongoing research in this field <ref> [7, 12, 22] </ref>. As a case study, we will examine the LIN-PACK benchmark, which consists of Gaussian elimination with partial pivoting [9].
Reference: [8] <author> J.W. Demmel, M.T. Heath, and H.A. van der Vorst. </author> <title> Parallel Numerical Linear Algebra. </title> <editor> In A. Iserles, ed., </editor> <booktitle> Acta Numerica, Volume 2. </booktitle> <address> Cambridge, England: </address> <publisher> Cambridge University Press, </publisher> <year> 1993, </year> <pages> p. 1-88. </pages>
Reference-contexts: Another approach to the problem is to reduce the demands of applications. Reducing the memory bandwidth requirements of scientific applications is an area of intense ongoing research. The most common technique for improving the locality of algorithms, specifically scientific algorithms, is blocking, also known as tiling. <ref> [2, 8, 15, 16] </ref> Improving the locality of an algorithm increases the performance of the memory hierarchy, by increasing the probability that a needed datum will be near the processor. <p> As a case study, we will examine the LIN-PACK benchmark, which consists of Gaussian elimination with partial pivoting [9]. The model implementation is unblocked, and the code is similar to the following (the matrix A is n fi n) <ref> [8] </ref>: f choose l so jA lk j = max kin jA ik j, 3 swap A lk and A kk g for i = k + 1 : n for j = k + 1 : n fswap A lj and A kj g for i = k + 1 <p> To factor A, we factor each of the column blocks in turn, updating the rest of the matrix. See [2] and <ref> [8] </ref> for details. for l = 1 : m Factor P A (l) = LU k b = k + n b Apply P to prior columns A (1 : n; 1 : k 1) and later columns A (1 : n; k b : n) Update block row of U
Reference: [9] <author> J. Dongarra, J. Bunch, C. Moler, and G.W. Stewart. </author> <title> LINPACK User's Guide. </title> <institution> Philadel-phia, PA, USA: Society for Industrial and Applied Mathematics, </institution> <year> 1979. </year>
Reference-contexts: Currently, automatic methods for blocking have not proved especially effective, although this is a significant amount of ongoing research in this field [7, 12, 22]. As a case study, we will examine the LIN-PACK benchmark, which consists of Gaussian elimination with partial pivoting <ref> [9] </ref>.
Reference: [10] <author> M. Farnwald and D. Mooring. </author> <title> A Fast Path to One Memory. </title> <journal> IEEE Spectrum v29, </journal> <note> n10 (October 1992), </note> <author> p. </author> <month> 50-51 </month>
Reference-contexts: Also, it takes advantage of the fact that the internal bus connecting this SRAM and DRAM is very wide, so the cache misses can be serviced very quickly. The Rambus DRAM <ref> [10] </ref> shares many characteristics with the cache and synchronous DRAMs. It uses custom DRAMs and an extremely high-speed channel, incorporated into a fully developed memory system environment in which both transitions of a 250 MHz clock is used.
Reference: [11] <author> B. Furht and R. Luken. </author> <title> The Space Shuttle Launch Computer Control System at NASA Kennedy Space Center. </title> <booktitle> Proceedings of EUROMICRO '91. Workshop on Real-Time Systems, </booktitle> <address> Paris-Orsay, France, 12-14 June 1991). Los Alamitos, CA, USA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year> <pages> p. 184-92. </pages>
Reference: [12] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for Cache and Local Memory Management by Global Program Transformation. Journal of Parallel and Distributed Programming, </title> <booktitle> v1, </booktitle> <address> n5. New York, NY, USA: </address> <publisher> Academic Press, </publisher> <year> 1988, </year> <pages> p. 587-616. 11 </pages>
Reference-contexts: All references to that block of the result are made during this period, so the locality of reference has improved. Currently, automatic methods for blocking have not proved especially effective, although this is a significant amount of ongoing research in this field <ref> [7, 12, 22] </ref>. As a case study, we will examine the LIN-PACK benchmark, which consists of Gaussian elimination with partial pivoting [9].
Reference: [13] <author> G.H. Golub and C.F. Van Loan. </author> <title> Ma--trix Computations. 2nd ed. </title> <address> Baltimore, MD, USA: </address> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference: [14] <author> F. Jones. </author> <title> A New Era of Fast Dynamic RAMs. </title> <journal> IEEE Spectrum v29, </journal> <note> n10 (October 1992), </note> <author> p. </author> <month> 43-49 </month>
Reference-contexts: Cached DRAMs have a small SRAM cache between its external pins and an internal DRAM <ref> [14] </ref>. This cache, which may replace the external cache in a hierarchical memory system, 2 makes access to the DRAM much faster if an ad-dress hits in this cache.
Reference: [15] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, April 8-11, 1991. New York, NY, </address> <publisher> USA : ACM, </publisher> <year> 1991, </year> <pages> p. 63-74 </pages>
Reference-contexts: Another approach to the problem is to reduce the demands of applications. Reducing the memory bandwidth requirements of scientific applications is an area of intense ongoing research. The most common technique for improving the locality of algorithms, specifically scientific algorithms, is blocking, also known as tiling. <ref> [2, 8, 15, 16] </ref> Improving the locality of an algorithm increases the performance of the memory hierarchy, by increasing the probability that a needed datum will be near the processor. <p> But, each iteration works on more than just a single block. In fact, most of the time is spent in the last two operations (updating the block row and the matrix multiplication), where the data sets do not fit in the cache. Also, it is demonstrated in <ref> [15] </ref> that for fixed block sizes, the optimal block size tends to be small compared to the cache size, because of self-interference. An interesting feature of the graph is the fact that the peak efficiencies shift to the left for increasing problem sizes.
Reference: [16] <author> A.C. McKeller and E.G. Coffman. </author> <title> The Organization of Matrices and Matrix Operations in a Paged Multiprogramming Environment. </title> <journal> Communications of the ACM, </journal> <volume> 12(3), </volume> <year> 1969, </year> <pages> p. 153-165. </pages>
Reference-contexts: Another approach to the problem is to reduce the demands of applications. Reducing the memory bandwidth requirements of scientific applications is an area of intense ongoing research. The most common technique for improving the locality of algorithms, specifically scientific algorithms, is blocking, also known as tiling. <ref> [2, 8, 15, 16] </ref> Improving the locality of an algorithm increases the performance of the memory hierarchy, by increasing the probability that a needed datum will be near the processor.
Reference: [17] <author> R. Ng. </author> <title> Fast Computer Memories. </title> <journal> IEEE Spectrum v29, </journal> <note> n10 (October 1992), </note> <author> p. </author> <month> 36-39 </month>
Reference-contexts: Then the column address is presented, and data returns within 20-40 ns. Also included in the read/write operation is a pre-charge time and other overhead, which accounts for about 30-50 ns. Thus, the cycle time is about 110-150 ns <ref> [17] </ref>, which corresponds to a rate of about 8 MHz. If a byte is transferred at a time, then the main memory can supply data to the processor at a rate of only 8 MB/s.
Reference: [18] <author> D.A. Patterson and J.L. Hennessy. </author> <title> Computer Architecture: A Quantitative Approach. </title> <address> San Mateo, CA, USA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference: [19] <author> E. Rothberg, J.P. Singh, and A. Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture ; May 16-19, 1993, </booktitle> <address> San Diego, Califor-nia. Los Alamitos, CA, </address> <publisher> USA : IEEE Computer Society Press, </publisher> <year> 1993, </year> <pages> p. 14-25. </pages>
Reference-contexts: Clearly, the majority of the bandwidth requirements are met by this bandwidth. Therefore, increasing available bandwidth will not result in any significant performance improvement. This knee is reminiscent of the cache miss curves used to determine working set sizes in <ref> [19] </ref>. We suggest the term "matching point," for the point at which the available bandwidth matches the demanded bandwidth. The blocked LINPACK curves have a similar shape to the unblocked curves, but the vertical scale is much smaller.
Reference: [20] <author> G. S. Sohi and M. Franklin. </author> <title> High-Bandwidth Data Memory Systems for Superscalar Processors. </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, April 8-11, 1991. New York, NY, </address> <publisher> USA : ACM, </publisher> <year> 1991, </year> <pages> p. 53-62. </pages>
Reference: [21] <author> B. Vogley. </author> <title> 800 Megabyte per Second Systems via Use of Synchronous DRAM. </title> <booktitle> Proceedings of COMPCON '94, </booktitle> <address> San Francisco, CA, USA, 28 February - 4 March 1994). Los Alamitos, CA, USA: </address> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <pages> p. 255-60. </pages>
Reference-contexts: Historically, DRAMs have been controlled asynchronously. By controlling a DRAM synchronously, all DRAM inputs are sampled at the positive edge of the input clock, and all DRAM outputs are valid on that positive edge as well <ref> [6, 21] </ref>. This allows I/O transactions to take place every clock cycle. By referencing all inputs and outputs to the rising edge of the clock pulse, this method can supply a peak bandwidth equal to the system's clock frequency multiplied by the number of lines in the system's bus.
Reference: [22] <author> M.E. Wolf and M.S.Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> Proceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> 26-28 June, </month> <year> 1991. </year> <journal> SIGPLAN Notices, </journal> <month> June </month> <year> 1991, </year> <title> v26, </title> <address> n6. New York, NY, USA: </address> <publisher> ACM Press, </publisher> <year> 1991, </year> <pages> p. 30-44. </pages>
Reference-contexts: All references to that block of the result are made during this period, so the locality of reference has improved. Currently, automatic methods for blocking have not proved especially effective, although this is a significant amount of ongoing research in this field <ref> [7, 12, 22] </ref>. As a case study, we will examine the LIN-PACK benchmark, which consists of Gaussian elimination with partial pivoting [9].
References-found: 22

