URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94534.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Automatic Differentiation: Obtaining Fast and Reliable Derivatives Fast  
Author: Christian H. Bischof, Alan Carle, Peyvand M. Khademi, Gordon Pusch 
Date: July 1994  
Note: to appear in Proc. of the SIAM Symposium on Control Problems in Industry San Diego,  
Abstract: In this paper, we introduce automatic differentiation as a method for computing derivatives of large computer codes. After a brief discussion of methods of differentiating codes, we review automatic differentiation and introduce the ADIFOR automatic differentiation tool. We highlight some applications of ADIFOR to large industrial and scientific codes, and discuss the effectiveness and performance of our approach. Finally, we discuss sparsity in automatic differen tiation and introduce the SparsLinC library.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. More, and G. L. Xue. </author> <title> The 13 MINPACK-2 test problem collection. </title> <type> Preprint ANL-MCS-P153-0692, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Full-Precision Support: single- and double-precision computations are provided for both real- and complex-valued computations. MINPACK-2 test problem collection <ref> [1] </ref> of large-scale optimization problems. Both problems are partially separable functions. We are interested in computing gradients for a range of grid sizes. Note that in Figure 3, which is a square "log-log" plot, the nonsparse gradients exhibit linear behavior with respect to p.
Reference: [2] <author> Brett Averick, Jorge More, Christian Bischof, Alan Carle, and An-dreas Griewank. </author> <title> Computing large sparse Jacobian matrices using automatic differentiation. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 15(2) </volume> <pages> 285-294, </pages> <year> 1994. </year>
Reference-contexts: The full Jacobian is then mapped onto a corresponding compressed Jacobian (in ADIFOR this mapping is implemented via the initialization of the seed matrix). This effective reduction of p results in reduced runtimes and memory requirements <ref> [2] </ref>. Given some code rewriting (which in some cases can be nontrivial), this approach is also applicable to the computation of gradients of partially separable functions [13].
Reference: [3] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. ADIFOR: </author> <title> Generating derivative codes from Fortran programs. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: In either case, automatic differentiation produces code that computes the values of the derivatives accurate to machine precision. Here, we discuss briefly issues impacting the computational complexity of 4 each mode, and refer the reader to <ref> [3, 12] </ref> for a detailed treatment of both these modes. The Forward Mode: The forward mode accumulates the derivatives of intermediate variables with respect to the independent variables, corresponding to the forward sensitivity formalism [14, 15]. <p> In this section, we briefly introduce the ADIFOR tool and highlight three applications. A "source transformation" approach to automatic differentiation has been explored in the ADIFOR <ref> [3, 5] </ref>, ADIC [10], and Odyssee [26, 27] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code that, when executed, computes derivatives without the overhead associated with trace interpretation schemes.
Reference: [4] <author> Christian Bischof, Alan Carle, and Peyvand Khademi. </author> <title> Fortran 77 interface specification to the SparsLinC library. </title> <type> Technical Report ANL/MCS-TM-196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: That is, by employing algorithms and data structures tailored for sparsity, sparsity in derivative calculations can be exploited transparently. Note that the sparsity structure of J or rf i is 11 computed as a by-product of the derivative computation. The SparsLinC (Sparse Linear Combination) library <ref> [4, 12] </ref> addresses the case in which p is large, and most of the vectors involved in vector linear combination are sparse. It provides support for sparse vector linear combination in a fashion that is well suited to the use of this operation in the context of automatic differentiation.
Reference: [5] <author> Christian Bischof, Alan Carle, Peyvand Khademi, and Andrew Mauer. </author> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, 1994. </title> <type> Preprint MCS-P481-1194, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, and CRPC-TR94491, Center for Research on Parallel Computation, Rice University. </institution>
Reference-contexts: In this section, we briefly introduce the ADIFOR tool and highlight three applications. A "source transformation" approach to automatic differentiation has been explored in the ADIFOR <ref> [3, 5] </ref>, ADIC [10], and Odyssee [26, 27] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code that, when executed, computes derivatives without the overhead associated with trace interpretation schemes. <p> ADIFOR has been successfully applied to codes from various domains of science and engineering, an extensive list of which can be found in <ref> [5] </ref>. We highlight three of them here. <p> Finally, we reviewed exploitation of sparsity in automatic differentiation and briefly introduced the SparsLinC library in this connection. At the time of this writing, we anticipate the forthcoming release of ADIFOR 2.0 <ref> [5] </ref>, our newest version of ADIFOR in which the SparsLinC library is fully integrated.
Reference: [6] <author> Christian Bischof, George Corliss, Larry Green, Andreas Griewank, Kara Haigler, and Perry Newman. </author> <title> Automatic differentiation of advanced CFD codes for multidisciplinary design. </title> <journal> Journal on Computing Systems in Engineering, </journal> <volume> 3(6) </volume> <pages> 625-638, </pages> <year> 1992. </year>
Reference-contexts: Hand-coding of derivatives for a large code is a tedious and error-prone process, in particular as "real" codes are often not well documented. In fact, the effort can take months or years, and in some cases may even be considered prohibitive <ref> [6] </ref>. However, depending on the skill of the implementer, hand-coding may lead to the most efficient code possible.
Reference: [7] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> Computing second- and higher-order derivatives through univariate Taylor series. </title> <journal> Optimization Methods and Software, </journal> <volume> 2 </volume> <pages> 211-232, </pages> <year> 1993. </year>
Reference-contexts: initializing rx (i) to the i-th canonical unit vector of length n, on exit each ry (i) contains the gradient @ y (i) Forward-mode code is easy to generate, for the most part preserves any parallelizable or vectorizable structures within the original code, and is readily generalized to higher-order derivatives <ref> [7] </ref> (in this paper, however, our discussions are restricted to first-order derivatives). If we wish to compute n directional derivatives, then running forward-mode code requires at 5 most on the order of n times as much time and memory as the original code.
Reference: [8] <author> Christian Bischof and Paul Hovland. </author> <title> Using ADIFOR to compute dense and sparse Jacobians. </title> <type> Technical Report ANL/MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: We see that ADIFOR-generated code provides a directional derivative computation capability <ref> [8] </ref>: Instead of simply producing code to compute the Jacobian J , ADIFOR produces code to compute J fl S, where the "seed matrix" S is initialized by the user.
Reference: [9] <author> Christian Bischof, Peyvand Khademi, and Timothy Knauff. </author> <title> ADI-FOR strategies related to pointer usage in MM5. </title> <type> Technical Report ANL/MCS-TM-187, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year> <month> 14 </month>
Reference-contexts: MM5 does not comply with this standard; in particular, it makes much use of "pointer variables." We circumvented this difficulty by developing an MM5-specific tool to map the pointer handling to standard-conforming Fortran77 code acceptable to ADIFOR, and to remap ADIFOR's output to obtain the desired sensitivity-enhanced code <ref> [9] </ref>. Given the size and complexity of the code, automatic differentiation is the only viable approach for doing this sensitivity study. Our work has demonstrated that automatic differentiation can generate results equivalent to a tangent linear model for sophisticated weather models, with minimal recourse to laborious and error-prone hand-coding.
Reference: [10] <author> Christian Bischof and Andrew Mauer. </author> <title> ADIC A tool for the auto-matic differentiation of C programs. </title> <type> Preprint MCS-P499-0295, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: In this section, we briefly introduce the ADIFOR tool and highlight three applications. A "source transformation" approach to automatic differentiation has been explored in the ADIFOR [3, 5], ADIC <ref> [10] </ref>, and Odyssee [26, 27] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code that, when executed, computes derivatives without the overhead associated with trace interpretation schemes.
Reference: [11] <author> Christian Bischof, Greg Whiffen, Christine Shoemaker, Alan Carle, and Aaron Ross. </author> <title> Application of automatic differentiation to groundwater transport models. </title> <editor> In Alexander Peters et al., editor, </editor> <booktitle> Computational Methods in Water Resources X, </booktitle> <pages> pages 173-182, </pages> <address> Dordrecht, 1994. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: order to evaluate the accuracy and runtime performance of ADIFOR-generated derivative codes in comparison with divided differences and hand-coded derivatives, a comparative study was done on two groundwater codes developed at Cornell University: ISOQUAD, a two-dimensional finite-element model of groundwater transient flow and transport, and TLS3D, a three-dimensional advection/diffusion model <ref> [11] </ref>. Each code was over 2,000 lines long. The hand-derived derivative code of ISOQUAD 8 took several months to develop; no hand-coded derivative of TLS3D was available for comparison.
Reference: [12] <author> Christian H. Bischof. </author> <title> Automatic differentiation, tangent linear models and pseudo-adjoints. </title> <type> Preprint MCS-P472-1094, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: In either case, automatic differentiation produces code that computes the values of the derivatives accurate to machine precision. Here, we discuss briefly issues impacting the computational complexity of 4 each mode, and refer the reader to <ref> [3, 12] </ref> for a detailed treatment of both these modes. The Forward Mode: The forward mode accumulates the derivatives of intermediate variables with respect to the independent variables, corresponding to the forward sensitivity formalism [14, 15]. <p> That is, by employing algorithms and data structures tailored for sparsity, sparsity in derivative calculations can be exploited transparently. Note that the sparsity structure of J or rf i is 11 computed as a by-product of the derivative computation. The SparsLinC (Sparse Linear Combination) library <ref> [4, 12] </ref> addresses the case in which p is large, and most of the vectors involved in vector linear combination are sparse. It provides support for sparse vector linear combination in a fashion that is well suited to the use of this operation in the context of automatic differentiation.
Reference: [13] <author> Christian H. Bischof and Moe El-Khadiri. </author> <title> Extending compile-time reverse mode and exploiting partial separability in ADIFOR. </title> <type> Technical Report ANL/MCS-TM-163, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: This effective reduction of p results in reduced runtimes and memory requirements [2]. Given some code rewriting (which in some cases can be nontrivial), this approach is also applicable to the computation of gradients of partially separable functions <ref> [13] </ref>. An alternative approach exploits sparsity in a transparent fashion, that is, without the a priori knowledge of the sparsity pattern of the Jacobian required for graph coloring.
Reference: [14] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems, I: Nonlinear functional analysis approach. </title> <journal> Journal of Mathematical Physics, </journal> <volume> 22(12) </volume> <pages> 2794-2802, </pages> <year> 1981. </year>
Reference-contexts: The Forward Mode: The forward mode accumulates the derivatives of intermediate variables with respect to the independent variables, corresponding to the forward sensitivity formalism <ref> [14, 15] </ref>. Here, derivatives are computed much in the way that the chain rule of differential calculus is usually taught. <p> The Reverse Mode: In contrast to the forward mode, the reverse mode propagates adjoints, that is, the derivatives of the final values with respect to intermediate variables, corresponding to the adjoint sensitivity formalism <ref> [14, 15] </ref>. To propagate adjoints, we have to be able to reverse the flow of the program and remember or recompute any intermediate value that nonlinearly impacts the final result. The reverse mode is difficult to implement owing to memory requirements.
Reference: [15] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems, II: Extension to additional classes of responses. </title> <journal> Journal of Mathematical Physics, </journal> <volume> 22(12) </volume> <pages> 2803-2812, </pages> <year> 1981. </year>
Reference-contexts: The Forward Mode: The forward mode accumulates the derivatives of intermediate variables with respect to the independent variables, corresponding to the forward sensitivity formalism <ref> [14, 15] </ref>. Here, derivatives are computed much in the way that the chain rule of differential calculus is usually taught. <p> The Reverse Mode: In contrast to the forward mode, the reverse mode propagates adjoints, that is, the derivatives of the final values with respect to intermediate variables, corresponding to the adjoint sensitivity formalism <ref> [14, 15] </ref>. To propagate adjoints, we have to be able to reverse the flow of the program and remember or recompute any intermediate value that nonlinearly impacts the final result. The reverse mode is difficult to implement owing to memory requirements.
Reference: [16] <author> Alan Carle, Lawrence Green, Christian Bischof, and Perry Newman. </author> <title> Applications of automatic differentiation in CFD. </title> <booktitle> In Proceedings of the 25th AIAA Fluid Dynamics Conference, </booktitle> <institution> AIAA Paper 94-2197. American Institute of Aeronautics and Astronautics, </institution> <year> 1994. </year>
Reference-contexts: Neither of these codes provide hand-coded derivatives|the code being deemed too complicated to differentiate by hand|and divided differences were shown to be numerically inaccurate, despite several attempts with different perturbation sizes; hence ADIFOR was used to generate the desired sensitivities <ref> [16] </ref>. In the case of the iterative solver, a post-ADIFOR modification to the derivative code was needed in order for the stopping criterion in the sensitivity code to monitor not only function convergence, but also sensitivity convergence [21].
Reference: [17] <author> Phillip E. Gill, Walter Murray, and Margaret H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: However, the accuracy of divided differences is hard to assess, and numerical errors tend to grow with problem complexity (see, e.g., <ref> [17] </ref>). Further, the computational complexity of the method has a lower bound of n times the time to compute F . These factors make divided differences impractical for the computation of large derivative matrices and gradients.
Reference: [18] <author> G. A. Grell, J. Dudhia, and D. R. Stauffer. </author> <title> A description of the fifth-generation Penn State/NCAR mesoscale weather model (MM5). </title> <type> Technical Report NCAR/TN-398+STR, </type> <institution> National Center for Atmospheric Research, Boulder, Colorado, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Sensitivities computed by ADIFOR were validated, thus showing the effectiveness of automatic differentiation in computing derivatives of iterative solvers. 9 Sensitivity-Enhanced MM5 Mesoscale Weather Model: The Fifth- Generation PSU/NCAR mesoscale weather model (MM5) <ref> [18] </ref> incorporates most processes known to be relevant in meteorology. We are working on the development of a sensitivity-enhanced version of the code, which may be used to investigate, for example, the sensitivity of model behavior with respect to sensor placement, data coverage, or model resolution.
Reference: [19] <author> Andreas Griewank. </author> <title> The chain rule revisited in scientific computing. </title> <type> Preprint MCS-P27-0491, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year> <month> 15 </month>
Reference-contexts: Automatic Differentiation: Automatic differentiation techniques rely on the fact that every function, no matter how complicated, is executed on a computer as a (potentially very long) sequence of elementary operations such as additions, multiplications, and elementary functions such as sin and cos (see, for example, <ref> [19, 25] </ref>). By repeated application of the chain rule of derivative calculus to the composition of those elementary operations, one can compute, in a completely mechanical fashion, derivatives of F that are correct up to machine precision [22].
Reference: [20] <author> Andreas Griewank. </author> <title> Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. </title> <journal> Optimization Methods and Software, </journal> <volume> 1(1) </volume> <pages> 35-54, </pages> <year> 1992. </year>
Reference-contexts: We have expressed compute time as a ratio of gradient to function runtimes. Provided memory constraints are not exceeded, a hand-coded gradient can be computed in a constant multiple of the function run-time <ref> [20] </ref>, whereas a straightforward implementation of divided differences would have a linear dependency on n. In contrast to these, there is a large range for runtimes of derivative codes generated by automatic differentiation. This variance is due to a number of factors which will be discussed in the ensuing sections.
Reference: [21] <author> Andreas Griewank, Christian Bischof, George Corliss, Alan Carle, and Karen Williamson. </author> <title> Derivative convergence of iterative equation solvers. </title> <journal> Optimization Methods and Software, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1993. </year>
Reference-contexts: In the case of the iterative solver, a post-ADIFOR modification to the derivative code was needed in order for the stopping criterion in the sensitivity code to monitor not only function convergence, but also sensitivity convergence <ref> [21] </ref>. Sensitivities computed by ADIFOR were validated, thus showing the effectiveness of automatic differentiation in computing derivatives of iterative solvers. 9 Sensitivity-Enhanced MM5 Mesoscale Weather Model: The Fifth- Generation PSU/NCAR mesoscale weather model (MM5) [18] incorporates most processes known to be relevant in meteorology.
Reference: [22] <author> Andreas Griewank and Shawn Reese. </author> <title> On the calculation of Jacobian matrices by the Markowitz rule. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 126-135. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: By repeated application of the chain rule of derivative calculus to the composition of those elementary operations, one can compute, in a completely mechanical fashion, derivatives of F that are correct up to machine precision <ref> [22] </ref>. The techniques of automatic differentiation are directly applicable to computer programs of arbitrary length containing branches, loops, and subroutines. the gradient of a scalar-valued function.
Reference: [23] <author> Andreas Griewank and Philippe L. Toint. </author> <title> On the unconstrained optimization of partially separable objective functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, </booktitle> <pages> pages 301-312, </pages> <address> London, 1981. </address> <publisher> Academic Press. </publisher>
Reference-contexts: Two classes of such problems that arise in large-scale optimization are computing sparse Jacobians and computing gradients of partially separable functions. Sparse Jacobians, as the name suggests, occur where many of the dependent variables are expected to have a zero dependency on the independent variables. Partially separable functions <ref> [23] </ref> can be represented in the form f (x) = i=1 where each of the component functions f i has limited support. This implies that the gradients rf i are sparse even though the final gradient rf is dense. It can be shown [23] that any function with a sparse Hessian <p> Partially separable functions <ref> [23] </ref> can be represented in the form f (x) = i=1 where each of the component functions f i has limited support. This implies that the gradients rf i are sparse even though the final gradient rf is dense. It can be shown [23] that any function with a sparse Hessian is partially separable. One approach for exploiting sparsity is the "compressed Jacobian" approach. Given the sparsity pattern of the Jacobian, this approach derives a graph coloring that identifies which columns of the Jacobian can be computed with the same directional derivative.
Reference: [24] <author> David Juedes. </author> <title> A taxonomy of automatic differentiation tools. </title> <editor> In An-dreas Griewank and George Corliss, editors, </editor> <booktitle> Proceedings of the Workshop on Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </booktitle> <pages> pages 315-330, </pages> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: Hence, in the case of gradient computations, the reverse mode provides a lower bound on runtime complexity. 3 The ADIFOR (Automatic DIfferentiation of FORtran) Tool There have been various implementations of automatic differentiation, an extensive survey of which can be found in <ref> [24] </ref>. In this section, we briefly introduce the ADIFOR tool and highlight three applications. A "source transformation" approach to automatic differentiation has been explored in the ADIFOR [3, 5], ADIC [10], and Odyssee [26, 27] tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code.
Reference: [25] <author> Louis B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> volume 120 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year>
Reference-contexts: Automatic Differentiation: Automatic differentiation techniques rely on the fact that every function, no matter how complicated, is executed on a computer as a (potentially very long) sequence of elementary operations such as additions, multiplications, and elementary functions such as sin and cos (see, for example, <ref> [19, 25] </ref>). By repeated application of the chain rule of derivative calculus to the composition of those elementary operations, one can compute, in a completely mechanical fashion, derivatives of F that are correct up to machine precision [22].
Reference: [26] <author> Nicole Rostaing, Stephane Dalmas, and Andre Galligo. </author> <title> Automatic differentiation in Odyssee. </title> <address> Tellus, 45a(5):558-568, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: In this section, we briefly introduce the ADIFOR tool and highlight three applications. A "source transformation" approach to automatic differentiation has been explored in the ADIFOR [3, 5], ADIC [10], and Odyssee <ref> [26, 27] </ref> tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code that, when executed, computes derivatives without the overhead associated with trace interpretation schemes. ADIFOR and ADIC mainly use the forward mode.
Reference: [27] <author> Nicole Rostaing-Schmidt and Eric Hassold. </author> <title> Basic functional representation of programs for automatic differentiation in the Odyssee system. </title> <editor> In Francois-Xavier Le Dimet, editor, </editor> <booktitle> High-Performance Computing in the Geosciences, </booktitle> <address> Dordrecht, 1994. </address> <publisher> Kluwer Academic Publishers. </publisher> <pages> 16 </pages>
Reference-contexts: In this section, we briefly introduce the ADIFOR tool and highlight three applications. A "source transformation" approach to automatic differentiation has been explored in the ADIFOR [3, 5], ADIC [10], and Odyssee <ref> [26, 27] </ref> tools. ADIFOR and Odyssee transform Fortran 77 code and ADIC transforms ANSI-C code. By applying the rules of automatic differentiation, these tools generate new code that, when executed, computes derivatives without the overhead associated with trace interpretation schemes. ADIFOR and ADIC mainly use the forward mode.
References-found: 27

