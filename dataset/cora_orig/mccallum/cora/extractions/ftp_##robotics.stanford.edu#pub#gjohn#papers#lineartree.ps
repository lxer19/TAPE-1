URL: ftp://robotics.stanford.edu/pub/gjohn/papers/lineartree.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Title: Robust Linear Discriminant Trees  
Author: George H. John 
Keyword: Decision Trees, Learning Algorithms, Regularization  
Address: Stanford, CA 94305  
Affiliation: Artificial Intelligence Statistics  Computer Science Department Stanford University  
Note: Extended Abstract submitted to  Category: Learning From Data  
Email: gjohn@cs.Stanford.EDU  
Date: June 30, 1994  
Abstract-found: 0
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference-contexts: All of the splits shown have equivalent entropy, but clearly the hyperplane that divides the space down the middle is preferred. Thus we believe that in continuous spaces a good splitting criterion requires more granularity than entropy provides. The GINI criterion used in linear splits in CART <ref> (Breiman, Friedman, Olshen & Stone 1984) </ref> also suffers from this problem. A further advantage is that soft entropy is differentiable and thus amenable to the efficient use of a wider variety of function optimizers. <p> Note that the approach here is more general than only defining a soft version of entropy any splitting criterion which depends only on the statistics p; p i ; n; n i (e.g., GINI <ref> (Breiman et al. 1984) </ref>, C-SEP (Fayyad & Irani 1992)) can be similarly "softened" by taking discr (x) to be continuous in [0; 1] and using the above equations for these statistics. 3 Regularization: Pruning and Filtering While studying the trees induced by the algorithm, we found that typical leaf subsets contained
Reference: <author> Brent, R. P. </author> <year> (1991), </year> <title> "Fast training algorithms for neural networks", </title> <journal> IEEE Transactions on Neural Networks 2(3), </journal> <pages> 346-354. </pages>
Reference: <author> Brodley, C. E. & Utgoff, P. E. </author> <year> (1992), </year> <title> Multivariate versus univariate decision trees, </title> <type> Technical Report COINS TR 92-8, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> 01003. </month>
Reference-contexts: Previous work in multivariate decision trees has used off-the-shelf training algorithms for "perceptrons" (linear discriminants), thus minimizing a mean squared error criterion <ref> (Brodley & Utgoff 1992) </ref>. Soft entropy is to be preferred over MSE for several reasons. Figure 1 shows a dataset and the linear discriminants minimizing MSE and soft entropy.
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1990), </year> <title> The cascade-correlation learning architecture, </title> <editor> in D. Touretzky, ed., </editor> <booktitle> "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 2, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We thus hope to build more robust trees (Hubel 1977). 4 Experiments We ran DT-SE, DT-SEP (pruned), DT-SEPIR (pruned + iterative refiltering), OC1 (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a), and Cascade Correlation <ref> (Fahlman & Lebiere 1990) </ref> on several datasets (Thrun et al. 1991, Murphy & Aha 1994), and results appear in Table 1. DT-SEPIR is the best or second best algorithm on both real datasets. OC1 is similar to CART with a slight enhancement to the optimization method.
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992), </year> <title> The attribute selection problem in decision tree generation, </title> <booktitle> in "AAAI-92: Proceedings of the Tenth National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 104-110. </pages>
Reference-contexts: Note that the approach here is more general than only defining a soft version of entropy any splitting criterion which depends only on the statistics p; p i ; n; n i (e.g., GINI (Breiman et al. 1984), C-SEP <ref> (Fayyad & Irani 1992) </ref>) can be similarly "softened" by taking discr (x) to be continuous in [0; 1] and using the above equations for these statistics. 3 Regularization: Pruning and Filtering While studying the trees induced by the algorithm, we found that typical leaf subsets contained around 50 to 100 instances,
Reference: <author> Hubel, P. J. </author> <year> (1977), </year> <title> Robust Statistical Procedures, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Pitts-burgh, PA. </address>
Reference-contexts: A novelty in our iterative re-filtering approach is to rebuild the tree using the reduced training set, the training set with outliers removed. The hypothesis is thus that data which is locally uninformative is also globally uninformative. We thus hope to build more robust trees <ref> (Hubel 1977) </ref>. 4 Experiments We ran DT-SE, DT-SEP (pruned), DT-SEPIR (pruned + iterative refiltering), OC1 (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a), and Cascade Correlation (Fahlman & Lebiere 1990) on several datasets (Thrun et al. 1991, Murphy & Aha 1994), and results appear in Table 1.
Reference: <author> John, G. </author> <year> (1994), </year> <title> "Annotated bibliography of linear discriminant decision trees", Posted to the comp.ai.neural-nets Newsgroup. Author's electronic mail: </title> <publisher> gjohn@cs.stanford.edu. </publisher>
Reference: <author> Loh, W.-Y. & Vanichsetakul, N. </author> <year> (1988), </year> <title> "Tree-structured classification via generalized discriminant analysis", </title> <journal> Journal of the American Statistical Association 83(403), </journal> <pages> 715-725. </pages>
Reference: <author> Morgan, J. N. & Messenger, R. C. </author> <year> (1973), </year> <title> THAID: a sequential analysis program for the analysis of nominal scale dependent variables, </title> <institution> University of Michigan. </institution>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference: <author> Murthy, S. K., Salzberg, S. & Kasif, S. </author> <year> (1993a), </year> <note> "OC1", Software available by ftp to blaze.cs.jhu.edu. </note>
Reference: <author> Murthy, S., Kasif, S., Salzberg, S. & Beigel, R. </author> <year> (1993b), </year> <title> OC1: Randomized induction of oblique decision trees, </title> <booktitle> in "AAAI-93: Proceedings of the Eleventh National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 322-327. </pages>
Reference-contexts: The hypothesis is thus that data which is locally uninformative is also globally uninformative. We thus hope to build more robust trees (Hubel 1977). 4 Experiments We ran DT-SE, DT-SEP (pruned), DT-SEPIR (pruned + iterative refiltering), OC1 <ref> (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a) </ref>, and Cascade Correlation (Fahlman & Lebiere 1990) on several datasets (Thrun et al. 1991, Murphy & Aha 1994), and results appear in Table 1. DT-SEPIR is the best or second best algorithm on both real datasets.
Reference: <author> Thrun, S. B. et al. </author> <year> (1991), </year> <title> The monk's problems a performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> CMU School of Computer Science. </institution>
Reference-contexts: We thus hope to build more robust trees (Hubel 1977). 4 Experiments We ran DT-SE, DT-SEP (pruned), DT-SEPIR (pruned + iterative refiltering), OC1 (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a), and Cascade Correlation (Fahlman & Lebiere 1990) on several datasets <ref> (Thrun et al. 1991, Murphy & Aha 1994) </ref>, and results appear in Table 1. DT-SEPIR is the best or second best algorithm on both real datasets. OC1 is similar to CART with a slight enhancement to the optimization method.
References-found: 13

