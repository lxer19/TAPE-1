URL: http://ptolemy.eecs.berkeley.edu/~parks/papers/ee225b-f89.ps.gz
Refering-URL: http://ptolemy.eecs.berkeley.edu/~parks/resume.html
Root-URL: 
Title: Vector Quantization Code Book Design Using Neural Networks  
Author: Thomas M. Parks 
Date: 3 December 1990  
Abstract: The Kohonen Self-Organizing Feature Map algorithm is compared to the K-Means vector quantization algorithm. Computation and storage requirements are calculated for both algorithms. A new algorithm which takes advantage of the structured code book produced by Kohonen's algorithm is introduced. This algorithm offers a significant computational savings over full-search vector quantization without imposing a storage cost penalty. The results of simulation studies are presented and the performance of the algorithms is compared. 
Abstract-found: 1
Intro-found: 1
Reference: [KMS84] <author> T. Kohonen, K. Makisara, and T. Saramaki. </author> <title> Phonotopic maps: Insightful representation of phonological features for speech recognition. </title> <booktitle> In IEEE Seventh International Conference on Pattern Recognition, </booktitle> <month> August </month> <year> 1984. </year>
Reference-contexts: Thus multiple code books can be designed with increasing M , using the previ ous result for the initial code book. 2 4 Kohonen's Self Organiz- ing Feature Map The neural network algorithm described here is the feature map algorithm developed by Kohonen <ref> [KMS84, Lip87, Lip88] </ref>. The network consists of N input nodes, corresponding to the components x k of the input vector ~x, and M output nodes. The output nodes are arranged in a two-dimensional array, with neighboring nodes designed to have similar responses to the inputs.
Reference: [Lip87] <author> R. P. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <month> April </month> <year> 1987. </year>
Reference-contexts: Thus multiple code books can be designed with increasing M , using the previ ous result for the initial code book. 2 4 Kohonen's Self Organiz- ing Feature Map The neural network algorithm described here is the feature map algorithm developed by Kohonen <ref> [KMS84, Lip87, Lip88] </ref>. The network consists of N input nodes, corresponding to the components x k of the input vector ~x, and M output nodes. The output nodes are arranged in a two-dimensional array, with neighboring nodes designed to have similar responses to the inputs.
Reference: [Lip88] <author> R. P. Lippmann. </author> <title> Neural network classifiers for speech recognition. </title> <journal> The Lincoln Laboratory Journal, </journal> <volume> 1(1), </volume> <year> 1988. </year>
Reference-contexts: Thus multiple code books can be designed with increasing M , using the previ ous result for the initial code book. 2 4 Kohonen's Self Organiz- ing Feature Map The neural network algorithm described here is the feature map algorithm developed by Kohonen <ref> [KMS84, Lip87, Lip88] </ref>. The network consists of N input nodes, corresponding to the components x k of the input vector ~x, and M output nodes. The output nodes are arranged in a two-dimensional array, with neighboring nodes designed to have similar responses to the inputs.
Reference: [LRCG89] <author> T. Lookabaugh, E.A. Riskin, P.A. Chou, and R.M. Gray. </author> <title> Variable rate vector quantization for speech, image, and video compression. </title> <type> Unpublished, </type> <month> April </month> <year> 1989. </year>
Reference-contexts: Vector dimensionality allows the classes C i associated with the code vectors ~y i to vary both in size and in shape. 3 K-Means Clustering Al gorithm The K-Means clustering algorithm is also known as the generalized Lloyd algorithm or the LBG algorithm <ref> [MRG85, LRCG89] </ref>. It is an iterative method for designing a code book which converges to a local minimum. After each iteration, it is necessary to perform a termination test. Typically this involves measuring the decrease in overall distortion. 1. Initialize code vectors by a suitable method. 2. <p> The number of distortion computa tions required has been reduced from O (M ) to O ( M ). 5.2 Using a Tree-Structured Code Book A common way to reduce the computational cost of a full search vector quantizer is to use a tree-structured code book <ref> [MRG85, LRCG89] </ref>. Using the K-Means algorithm, for example, the training set is first divided into P classes using K-Means. Each of the resulting classes is then subdivided into P subclasses, again using K-Means. <p> Thus it is possible to form subclasses which have few or even no training samples associated with them. Pruning away such classes from the tree can reduce the code book size with only a minor increase in distortion. The methods used by Gray and Riskin <ref> [LRCG89] </ref> require the design of a full tree-structured code book, followed by a pruning process which trades distortion for code book size. If neural network techniques could be used to design a tree-structured code book, then the same pruning methods could be applied 12 off-line after training was complete.
Reference: [MRG85] <author> J. Makhoul, S. Roucos, and H. Gish. </author> <title> Vector quantization in speech coding. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(11), </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: Sec fl This report has been sponsored in part by the Air Force Office of Scientific Research (AFOSR/JSEP) under Contract Number F49620-90-C-0029. tion 7 concludes with a discussion of issues that deserve additional consideration. 2 Vector Quantization A review of vector quantization techniques appears in <ref> [MRG85] </ref>. Vector quantization involves the mapping of a continuous valued input vector ~x of dimension N to a discrete valued reconstruction vector ~y of the same dimension. The reconstruction vector typically can only take on a finite number of values, M . <p> Vector dimensionality allows the classes C i associated with the code vectors ~y i to vary both in size and in shape. 3 K-Means Clustering Al gorithm The K-Means clustering algorithm is also known as the generalized Lloyd algorithm or the LBG algorithm <ref> [MRG85, LRCG89] </ref>. It is an iterative method for designing a code book which converges to a local minimum. After each iteration, it is necessary to perform a termination test. Typically this involves measuring the decrease in overall distortion. 1. Initialize code vectors by a suitable method. 2. <p> The number of distortion computa tions required has been reduced from O (M ) to O ( M ). 5.2 Using a Tree-Structured Code Book A common way to reduce the computational cost of a full search vector quantizer is to use a tree-structured code book <ref> [MRG85, LRCG89] </ref>. Using the K-Means algorithm, for example, the training set is first divided into P classes using K-Means. Each of the resulting classes is then subdivided into P subclasses, again using K-Means.
Reference: [TM90] <author> K. Truong and R. Mersereau. </author> <title> Structural image codebooks and the self-organizing feature map algorithm. </title> <booktitle> In ICASSP, </booktitle> <year> 1990. </year> <month> 13 </month>
Reference-contexts: The gradient search requires the more restrictive sorted order, and reduces the number of required computations to O ( M ). 5.1.1 Decimated Search Truong and Mersereau <ref> [TM90] </ref> have proposed a decimated search algorithm which uses M 2 P two-level search trees. For example, when P = 2, the distortion is first computed only for the nodes in the even numbered rows and columns of the output array. These nodes are the roots of the search trees.
References-found: 6

