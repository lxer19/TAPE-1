URL: http://www.cs.vt.edu/~nrg/roland/thesis/thesis.ps.gz
Refering-URL: http://www.cs.vt.edu/~nrg/roland/thesis/
Root-URL: http://www.cs.vt.edu
Title: Optimizing Response Time, Rather than Hit Rates, of WWW Proxy Caches  
Author: by Roland Peter Wooster Dr. Edward Fox Dr. Scott Midkiff 
Degree: Thesis submitted to the faculty of the  in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE in Computer Science c flRoland Peter Wooster and VPI SU 1996 APPROVED: Advisor: Dr. Marc Abrams  
Date: December, 1996  
Affiliation: Virginia Polytechnic Institute and State University  Blacksburg, Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ghaleb Abdulla, Marc Abrams, and Edward A. Fox. </author> <title> Scaling the World Wide Web. </title> <address> &lt;URL: http://ei.cs.vt.edu/ ~ succeed/96ieeeAAF/&gt;, </address> <month> April </month> <year> 1996. </year> <note> Unpublished paper (5 Dec. </note> <year> 1996). </year>
Reference-contexts: If in the client caching system each client uses 5MB of disk space then 200 users will use 1GB of disk space. One study has shown that about 17% of the space is wasted by caching multiple copies of the same file <ref> [1] </ref>. In the above example 170MB would have been wasted. <p> Research groups [2, 19] have shown that 30% to 50% hit rates can be achieved by a proxy cache. This not only reduces latency for the clients, but significantly reduces the load on the network beyond the proxy <ref> [1] </ref>. The reason the server cache's hit rate is higher than a proxy cache's is simple; the server already has a hit on half of the fserver, documentg pair specified by a URL, whereas the proxy cache must hit on both of the pair to yield an actual hit.
Reference: [2] <author> Marc Abrams, Charles Standridge, Ghaleb Abdulla, Stephen Williams, and Edward Fox. </author> <title> Caching Proxies: Limitations and Potentials. </title> <booktitle> 4th Inter. World-Wide Web Conference, </booktitle> <pages> pages 119-133, </pages> <month> December </month> <year> 1995. </year> <note> &lt;URL: http://www.w3.org/pub/WWW/- Journal/1/abrams.155/paper/155.html&gt; Also in WWW Journal Vol. 1 (5 Dec. </note> <year> 1996). </year>
Reference-contexts: If this 1GB of disk space is used for a shared proxy cache that all the clients can access, the hit rate will increase enormously, at little cost in latency (assuming the network connecting the client to the proxy is lightly loaded). Research groups <ref> [2, 19] </ref> have shown that 30% to 50% hit rates can be achieved by a proxy cache. This not only reduces latency for the clients, but significantly reduces the load on the network beyond the proxy [1]. <p> Quite obviously the hit rate will be significantly reduced if there are only a few huge documents in the cache. If, however, these audio or video documents are used a large number of times then the weighted hit rate will be extraordinary. This, however, does not often happen <ref> [2, 31] </ref>. The proxy server will require different, maximum cachable file size settings depending upon whether the weighted hit rate or the request hit rate is more important. In the workloads 2 used by the VT-NRG group less than 0.01% of files were over 4MB [2, 31]. <p> however, does not often happen <ref> [2, 31] </ref>. The proxy server will require different, maximum cachable file size settings depending upon whether the weighted hit rate or the request hit rate is more important. In the workloads 2 used by the VT-NRG group less than 0.01% of files were over 4MB [2, 31]. This insignificantly small percentage of files allows a maximum file size of 4MB to be used for "cachable" files in the Harvest Cache. Table 3.1 illustrates the enormous difference between the percentage of requests and percentage of bytes transferred in one workload at NCSA.
Reference: [3] <author> Marc Abrams, Charles Standridge, Stephen Williams, Ghaleb Abdulla, Edward Fox, Shashin Patel, and Randy Ribler. </author> <title> Multimedia Traffic Analysis Using Chitra95. </title> <booktitle> ACM Multimedia '95, </booktitle> <address> San Francisco CA, </address> <pages> pages 267-276, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: This would of course be included within the caching of documents from the last 24 hour period. Wessel's results indicate that documents that have not been accessed for a month are only rarely accessed again. Similarly the VT-NRG Group <ref> [3] </ref> found that for a server cache one hour and 28 hours are times at which there is a significant decrease in hit rate. This would indicate that if a user does not use a file within one hour they are unlikely to access it again in the same session.
Reference: [4] <author> Martin F. Arlitt and Carey L. Williamson. </author> <title> Web Server Workload Characterization: The Search for Invariants. </title> <booktitle> In Proc. SIGMETRICS, </booktitle> <address> Philadelphia, PA, </address> <month> April </month> <year> 1996. </year> <institution> ACM. University of Saskatchewan. </institution>
Reference-contexts: Some servers receive up to a million requests per day [21]. A single machine may struggle to perform this many connections, negotiations, and replies to HTTP requests. The best example of a server cache is the NCSA server <ref> [4, 16, 17, 18] </ref>, which in August 1995 consisted of eight workstations. NCSA's solution to an over-burdened single server was initially to increase the performance of the server; when this again became over-burdened they developed a new architecture for a high performance WWW server. <p> Arlitt et al. found this to be a significant proportion of the documents in a cache: "This one-time referencing behavior means that, on average, one-third of a server cache could be cluttered with useless documents <ref> [4] </ref>". The VT-NRG group [30], in a sample of 50,000 URL accesses, found that more than 90% of the URLs were only referenced once. Obviously to cache these documents is wasteful for a cache with limited disk space.
Reference: [5] <author> Jean-Chrysostome Bolot and Philipp Hoschka. </author> <title> Performance Engineering of the World Wide Web. The World Wide Web Journal, </title> <booktitle> 1(3) </booktitle> <pages> 185-195, </pages> <month> Summer </month> <year> 1996. </year> <note> &lt;URL: http://www.w3.org/pub/WWW/Journal/3/s3.bolot.html&gt; (14 Dec. </note> <year> 1996). </year>
Reference-contexts: of bytes transferred: HR = Number of URLs loaded from the cache Total number of URLs accessed (1.1) Number of bytes loaded from the cache Total number of bytes accessed : (1.2) 1.4 Problem Statement Almost all published research on proxy caching, except one recent paper by Bolot and Hoschka <ref> [5] </ref>, is devoted to maximization of hit rate or weighted hit rate. However, this may not yield the optimal response time. There is no benefit in caching local documents which may, in some instances, be retrieved faster from the origin server than from the proxy server. <p> REMOVAL ALGORITHMS TESTED the average latency per byte, documents that download slowly are cached while quickly downloadable documents are unlikely to be cached, and local documents not cached at all. Only recently has research using latency as a factor in the removal algorithm been investigated <ref> [5] </ref>. As mentioned previously, the latency to download a file is the product of its size divided by the link's bandwidth and the propagation delay in the network. <p> Bolot and Hoschka suggest that removal algorithms should consider many more factors than just the download time in ranking documents for replacement <ref> [5] </ref>. Their algorithm, on an artificial workload, illustrates that using latency with other factors can perform better than SIZE or LFU, see equation (4.5).
Reference: [6] <author> Anawat Chankhuthod, Peter Danzig, Chuck Neerdaels, Michael Schwartz, and Kurt Worrel. </author> <title> A Hierarchical Internet Object Cache. </title> <institution> &lt;URL: ftp://ftp.cs.colorado.edu /pub/cs/techreports/schwartz/HarvestCache.ps.Z&gt;. University of Southern Califor-nia, University of Colorado, </institution> <address> Boulder (5 Dec. </address> <year> 1996). </year>
Reference-contexts: RELATED WORK 2.2 Proxy Design The CERN proxy cache, being the first to be widely deployed, is old and slow when compared to newer proxies such as the Harvest Cache <ref> [6] </ref>. Many research groups have analyzed the performance difference between proxies; all of them come to the same conclusion that the CERN cache is slow and inefficient. The "CERN proxy software can probably handle a sustained load of no more than 5 connections per second" [22]. <p> Studies have shown that using the CERN proxy will slow down a cache miss by an average of an extra 69% over the average direct latency 1 [20]. Collaborative research groups <ref> [6, 29] </ref> at the University of Colorado at Boulder and the University of Southern California have created a number of vastly improved caching proxies. The most significant of these is the Harvest Cache. <p> Thus processes do not have to be continually forked and terminated with every HTTP request. This enables the Harvest Cache to serve up to 200 "small" requests per second on a Sparc 20 model 61, as compared to the maximum of 5 requests per second for the CERN cache <ref> [6] </ref>. Its method of document removal is discussed in section 4.3. <p> A total of 2000 objects were faulted into the cache in this way. Once the cache was warm, all ten clients concurrently referenced all 2000 objects, in random order" <ref> [6] </ref>. 11 CHAPTER 2. RELATED WORK be considered in the design of a proxy cache. 2.3 Summary Of the three caching types, client, proxy, and server, only proxy caching is investigated in this thesis. <p> The "last-modified" value of the cached copy is sent within the request. If the header data returned with the document does not include a "last-modified" field then the cache server will use a default TTL. The Harvest cache <ref> [6] </ref>, other proxies, and HTTP/1.1 [11] significantly reduce the number 15 CHAPTER 3. CACHING ISSUES of requests because they do not check with the server on every retrieval. Occasionally stale documents will be sent to the client. <p> Clearly a fixed TTL is not appropriate for weather maps or many other situations. 3.4 Staleness HTTP/1.1 [11] and the proxies by CERN [12] and Harvest <ref> [6] </ref> calculate the TTL to be a fraction of the "last-modified" date. <p> CACHING ISSUES between 5 and 7 days will return approximately 20% <ref> [6, 12] </ref> stale documents. Wessels [29] states that twenty percent is considered unacceptable. <p> This is discussed in section 4.3.4 3.9 Negative Caching WWW browser average latency is greatly reduced by documents that are not available because of server failures and name changes. A solution by the Harvest group <ref> [6] </ref> is that of "negative-caching." This is where the cost of repeated failures is reduced to that of a cache hit.
Reference: [7] <author> Carlos Cunha, Azer Bestavros, and Mark Crovella. </author> <title> Characteristics of WWW Client-based Traces. </title> <type> Technical Report BU-CS-95-010, </type> <institution> Computer Science Department, Boston University, Computer Science Department, Boston University, </institution> <address> 111 Cummington St, Boston, MA 02215, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: They found that using SIZE for the removal algorithm achieved up to 50% better hit rates than using NREF, achieving hit rates of 45% and 30% respectively. NREF (same as LFU), however, yields the highest weighted hit rate. Cunha et al.'s <ref> [7] </ref> research found that there is a fixed latency due to the setup of an HTTP request; thus it is much more effective to cache small documents. The total latency to download a small file is affected more by the fixed overhead than the latency to download the actual file.
Reference: [8] <author> Peter Danzig, Richard Hall, and Michael Schwartz. </author> <title> A Case for Caching File Objects Inside Internetworks. </title> <type> Technical Report CU-CS-642-93, </type> <institution> U. of Southern California, U. of Colorado at Boulder, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> This yields many possibilities for caching. Changes in the protocols used by WWW browsers could also improve performance. 1.3.1 Compression Not all the documents on the Internet are compressed. Studies of the data transferred on the NSFnet were performed which indicated that 31% of FTP data was not compressed <ref> [8] </ref>. These studies were performed, during September 1992, at a time when FTP traffic used the greatest percentage of the NSFnet's bandwidth. FTP is now only a small percentage of the traffic. <p> Popular Removal Algorithms 4.1.1 LRU and LFU Least Recently Used (LRU) would appear to be intuitive to use; the longer it has been since the file was used the less likely it is to be used again. "Duplicate transmissions tend to occur within a small time window of one another" <ref> [8] </ref>. Likewise, Least Frequently Used (LFU) is intuitive because a file which has yielded many cache hits is likely to yield many more cache hits. This also benefits the cache for documents that are only used once, which are loaded into the cache and never used again. <p> These documents displace other documents that may have resulted in cache hits. Studies have shown that as many as half of the referenced documents are never referenced again <ref> [8] </ref>. This would indicate that up to half the documents in a cache are useless, using a significant fraction of the cache and never yielding a cache hit. Danzig et al. [8] found that LFU and LRU yield increasingly similar hit rates as the size of the cache increases: " : <p> Studies have shown that as many as half of the referenced documents are never referenced again <ref> [8] </ref>. This would indicate that up to half the documents in a cache are useless, using a significant fraction of the cache and never yielding a cache hit. Danzig et al. [8] found that LFU and LRU yield increasingly similar hit rates as the size of the cache increases: " : : : the probability of seeing the same duplicate-transmitted file within 48 hours is nearly 90%. For this reason, LRU and LFU replacement policies are 23 CHAPTER 4.
Reference: [9] <author> Jennie File. </author> <title> NCSA Web Server Activity: Total Connections. </title> <address> &lt;URL: http://www.ncsa.- uiuc.edu/SDG/Presentations/Stats/WebServer.html&gt;, </address> <month> November </month> <year> 1995. </year> <note> Unpublished slides (5 Dec. 1996). 96 REFERENCES </note>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> This is an increase of 125,000 times, equal to a doubling every 50 days. 1 CHAPTER 1. INTRODUCTION The number of connections, where a connection is a single TCP request for a URL, are also increasing exponentially. The National Center for Supercomputing Applications (NCSA) reported <ref> [9] </ref> 92,000 connections to its systems for the week ending July 3, 1993. Their peak reported load was 4,100,000 for the week ending March 4, 1995. This is a doubling of load every 110 days.
Reference: [10] <author> Jennie File. </author> <title> NSFnet Backbone Byte Traffic. </title> <address> &lt;URL: http://www.ncsa.uiuc.edu/SDG/- Presentations/Stats/NSFnetStats.html&gt;, </address> <month> April </month> <year> 1995. </year> <note> Unpublished slides (5 Dec. </note> <year> 1996). </year>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. In November 1992 the total NSFnet backbone data traffic <ref> [10] </ref> was 28GB per month, and only 0.1% was WWW traffic. By March 1995 the traffic was 200 times greater, at 5.3TB per month with 90% WWW traffic. The actual WWW traffic on the NSFnet backbone increased from 39MB to 4.8TB during this period.
Reference: [11] <institution> HTTP Working Group. </institution> <note> Hypertext Transfer Protocol | HTTP/1.1. &lt;URL: http://- www.w3.org/pub/WWW/Protocols/HTTP/1.1/draft-ietf-http-v11-spec-07.txt&gt;, August 1996. (18 Dec. </note> <year> 1996). </year>
Reference-contexts: To exaggerate the problem HTTP sends every possible option supported by the client in each new connection request packet. Many new protocols [17, 20, 25] have been developed to solve one or both of the above problems. HTTP-NG [25] and HTTP/1.1 <ref> [11] </ref> use long lived connections. Thus a TCP connection can serve all the images and documents within a WWW page, rather than making multiple TCP connections for a single WWW page containing multiple images or documents. <p> The "last-modified" value of the cached copy is sent within the request. If the header data returned with the document does not include a "last-modified" field then the cache server will use a default TTL. The Harvest cache [6], other proxies, and HTTP/1.1 <ref> [11] </ref> significantly reduce the number 15 CHAPTER 3. CACHING ISSUES of requests because they do not check with the server on every retrieval. Occasionally stale documents will be sent to the client. A stale file is one which is cached but has more recently been modified at the server. <p> There is, however, no way of telling the difference between the GIF photograph of someone on their WWW page, which almost never changes, and a dynamically created GIF weather map. Clearly a fixed TTL is not appropriate for weather maps or many other situations. 3.4 Staleness HTTP/1.1 <ref> [11] </ref> and the proxies by CERN [12] and Harvest [6] calculate the TTL to be a fraction of the "last-modified" date.
Reference: [12] <author> James Gwertzman and Margo Seltzer. </author> <title> World Wide Web Cache Consistency. </title> <booktitle> 1996 Proceedings of the Usenix Technical Conference, 1996. </booktitle> <publisher> Microsoft Corporation, Harvard University. </publisher>
Reference-contexts: Clearly a fixed TTL is not appropriate for weather maps or many other situations. 3.4 Staleness HTTP/1.1 [11] and the proxies by CERN <ref> [12] </ref> and Harvest [6] calculate the TTL to be a fraction of the "last-modified" date. <p> CACHING ISSUES between 5 and 7 days will return approximately 20% <ref> [6, 12] </ref> stale documents. Wessels [29] states that twenty percent is considered unacceptable. <p> Gwertzman and Seltzer <ref> [12] </ref> took 5% staleness as their goal for the development of their Alex server, stating: "The Alex protocol provides the best of all worlds in that it can be tuned to: * reduce network bandwidth consumption by an order of magnitude over an invalidation protocol 5 , * produce a stale <p> CACHING ISSUES same query may be different because the database on the server may have been updated or it may be time dependent. No research seems to agree exactly on what fraction of documents are of this type. Gwertzman et al. <ref> [12] </ref> mention 10% of the documents as a typical figure. The VT-NRG Group [30] found the maximum percentage in any of their workloads was 40% of the documents.
Reference: [13] <author> Darren Hardy, Michael Schwartz, and Duane Wessels. </author> <title> Harvest User's Manual. </title> <type> Technical Report CU-CS-743-94, </type> <institution> University of Colorado at Boulder, Department of Computer Science, University of Colorado, Boulder, Colorado 80309-0430, </institution> <month> September </month> <year> 1995. </year> <note> Version 1.3. </note>
Reference-contexts: Following this the experiment order and the analysis methods used for testing the removal algorithms are discussed. 5.1 System Requirements All experiments in this thesis were performed using a modified version of the Harvest Cache, which is one of a group of five applications. The Harvest manual <ref> [13] </ref> does not, however, indicate what type of machine is required if only one of the applications is to be executed.
Reference: [14] <author> Van Jacobson. </author> <title> Congestion Avoidance and Control. </title> <booktitle> In Proc. SIGCOMM, </booktitle> <pages> pages 314-329, </pages> <address> Stanford, CA, </address> <month> August </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: If the document is smaller than CONN, the time to download the document is used only for the CLAT calculations, otherwise it is used for the CBW calculations. This estimate is smoothed in a similar manner to TCP RTT latency estimates <ref> [14, 26] </ref>, using both old and new requests to the server as factors of the new estimate. Equation (4.4) shows the LAT algorithm's formula, the variable lat i is 29 CHAPTER 4. REMOVAL ALGORITHMS TESTED the LAT value for a particular file.
Reference: [15] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The method of obtaining replications for the significance tests is different for each of the different experiments. In experiment one a two factor F-test <ref> [15] </ref> was performed. No replications were used, and the two factors were 39 CHAPTER 5. METHODOLOGY the removal algorithm and the hour the data was recorded at. In the second and third experiment only data after the cache has filled up and started to remove files is used. <p> Then the log file is tested with another four parallel proxies, each running the HYB algorithm but with four different sets of constants (W B , W N , and CONN) in the HYB formula. A 2 kp r <ref> [15] </ref> design was used to test the performance of the HYB algorithm with different values for each of the three constants used in the removal algorithm, shown in equations (4.2, 4.3, and 4.6).
Reference: [16] <author> Eric Dean Katz, Michell Butler, and Robert McGrath. </author> <title> A stable HTTP Server: The NCSA prototype. Computer Networks and ISDN Systems, </title> <address> 27(0169-7552(94)00068-9):155-164, </address> <year> 1994. </year>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> NCSA's servers have crashed many times due to excessive load <ref> [16] </ref>, due to exhaustion of resources in the operating system. When an HTTP page is requested the cost to the network is the product of the number of bytes and the hop count. The hop count is the number of network segments between client and server [27]. <p> Some servers receive up to a million requests per day [21]. A single machine may struggle to perform this many connections, negotiations, and replies to HTTP requests. The best example of a server cache is the NCSA server <ref> [4, 16, 17, 18] </ref>, which in August 1995 consisted of eight workstations. NCSA's solution to an over-burdened single server was initially to increase the performance of the server; when this again became over-burdened they developed a new architecture for a high performance WWW server. <p> NCSA's solution to an over-burdened single server was initially to increase the performance of the server; when this again became over-burdened they developed a new architecture for a high performance WWW server. Their server works with the Andrew File System (AFS) as its distributed file system <ref> [16] </ref>. NCSA's entire local network is served by three Sparc 10 servers, each with 120GB of disk space. To provide caching for remote HTTP requests they use a cluster of Hewlett-Packard (HP735) workstations which operate in a round robin fashion.
Reference: [17] <author> Thomas Kwan, Robert McGrath, and Daniel Reed. </author> <title> NCSA's World Wide Web Server: Design and Performance. </title> <journal> IEEE Computer, </journal> <volume> 1995(0018-9162):68-74, </volume> <year> 1995. </year>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> This means that for each image within a WWW page a TCP connection is made, rather than one connection for the whole page. To exaggerate the problem HTTP sends every possible option supported by the client in each new connection request packet. Many new protocols <ref> [17, 20, 25] </ref> have been developed to solve one or both of the above problems. HTTP-NG [25] and HTTP/1.1 [11] use long lived connections. <p> Some servers receive up to a million requests per day [21]. A single machine may struggle to perform this many connections, negotiations, and replies to HTTP requests. The best example of a server cache is the NCSA server <ref> [4, 16, 17, 18] </ref>, which in August 1995 consisted of eight workstations. NCSA's solution to an over-burdened single server was initially to increase the performance of the server; when this again became over-burdened they developed a new architecture for a high performance WWW server.
Reference: [18] <author> Thomas Kwan, Robert McGrath, and Daniel Reed. </author> <title> User Access Patterns to NCSA's World Wide Web Server. </title> <type> Technical Report UIUCDCS-R-95-1934, </type> <institution> Dept. of Comp. Sci., Univ. of Illinois, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> One of the three, network caching, is the subject of this thesis. A brief synopsis of the other two, client and server caching, is included in this section. If the WWW is to continue growing "both clients and servers must aggressively exploit caching" <ref> [18] </ref>. 2.1.1 Client Caches All recent WWW browsers offer non-persistent and possibly persistent caching. The former type only stores recent documents in memory during the browser session in which they are loaded. In the latter type WWW pages are stored to disk ready for use in subsequent sessions. <p> Some servers receive up to a million requests per day [21]. A single machine may struggle to perform this many connections, negotiations, and replies to HTTP requests. The best example of a server cache is the NCSA server <ref> [4, 16, 17, 18] </ref>, which in August 1995 consisted of eight workstations. NCSA's solution to an over-burdened single server was initially to increase the performance of the server; when this again became over-burdened they developed a new architecture for a high performance WWW server. <p> It was found that the 50 most requested documents only required 600KB of memory to cache and that they satisfied 70% of all the requests <ref> [18] </ref>. To increase the hit rate to 95%, 9 CHAPTER 2. RELATED WORK 800 documents had to be kept in each of the cache servers memories, requiring 60MB of memory. Although the hit rate is 95%, the weighted hit rate is only 80%. <p> The VT-NRG Group, however, found that there was no significant correlation with the day of the week and the hit rate achieved by a proxy cache [31]. Other studies <ref> [18] </ref> have shown that certain types of documents are more likely to be loaded in the daytime and others at nighttime. Again it might be worth researching this phenomena, using a tape drive to up-load and download documents at an appropriate time. 3.7 Periodic vs.
Reference: [19] <author> Ari Luotonen and Kevin Altis. </author> <title> World-Wide Web Proxies. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2), </volume> <year> 1994. </year> <note> &lt;URL: http://www1.cern.ch/PapersWWW94/- luotonen.ps&gt; (5 Dec. </note> <year> 1996). </year>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> This will lead to either reduced hit rates or a requirement for increasing cache server memory sizes and disk space. 2.1.3 Proxy Caches Luotonen and Altis claim that caching within the network "is more effective on the proxy server than on each client" <ref> [19] </ref>. It should be clear that the more disk space that is available for a cache the more effective it will be. <p> If this 1GB of disk space is used for a shared proxy cache that all the clients can access, the hit rate will increase enormously, at little cost in latency (assuming the network connecting the client to the proxy is lightly loaded). Research groups <ref> [2, 19] </ref> have shown that 30% to 50% hit rates can be achieved by a proxy cache. This not only reduces latency for the clients, but significantly reduces the load on the network beyond the proxy [1].
Reference: [20] <author> Radhika Malpani, Jacob Lorch, and David Berger. </author> <title> Making World Wide Web Caching Servers Cooperate. </title> <booktitle> In 4th Inter. World-Wide Web Conference, </booktitle> <pages> pages 107-117, </pages> <address> Boston, MA, </address> <month> December </month> <year> 1995. </year> <note> &lt;URL: http://www.w3.org/pub/WWW/Journal/1/lorch.059/- paper/059.html&gt; (18 Dec. </note> <year> 1996). </year>
Reference-contexts: This means that for each image within a WWW page a TCP connection is made, rather than one connection for the whole page. To exaggerate the problem HTTP sends every possible option supported by the client in each new connection request packet. Many new protocols <ref> [17, 20, 25] </ref> have been developed to solve one or both of the above problems. HTTP-NG [25] and HTTP/1.1 [11] use long lived connections. <p> Studies have shown that using the CERN proxy will slow down a cache miss by an average of an extra 69% over the average direct latency 1 <ref> [20] </ref>. Collaborative research groups [6, 29] at the University of Colorado at Boulder and the University of Southern California have created a number of vastly improved caching proxies. The most significant of these is the Harvest Cache.
Reference: [21] <author> Jeffrey Mogul. </author> <title> The Case for Persistent Connection HTTP. ACM, </title> <booktitle> SIGCOMM 95(0 89791-711-1/95/0008):299-313, </booktitle> <year> 1995. </year> <title> REFERENCES </title>
Reference-contexts: Not only is the Internet overloaded but so are many of the servers placed upon it. Some servers receive up to a million requests per day <ref> [21] </ref>. A single machine may struggle to perform this many connections, negotiations, and replies to HTTP requests. The best example of a server cache is the NCSA server [4, 16, 17, 18], which in August 1995 consisted of eight workstations.
Reference: [22] <author> Danial O'Callaghan. </author> <title> A Central Caching proxy Server for the WWW. Asia Pacific WWW Conference 1995, </title> <address> Sydney Australia, </address> <month> August </month> <year> 1995. </year> <institution> University of Melbourne, Australia. </institution>
Reference-contexts: Also each of the documents cached may have taken different amounts of time to download. Consequentially simple CPU cache removal policies are inappropriate for proxy caches. 1.1 World Wide Web Growth Statistics Every research group that has analyzed WWW traffic growth has reported exponential growth <ref> [8, 9, 10, 16, 17, 18, 19, 22] </ref>. Extensive analysis of the Internet's growth used to be calculated by the National Software Foundation (NSF), however, their studies only include traffic until November 1994, when the Internet backbone was privatized. NSF statistics, as current as they are, are daunting. <p> The drop might be due to users switching to Netscape browsers, therefore decreasing the number of hits to the Mosaic home page. The University of Melbourne <ref> [22] </ref> reported an even faster rate of increase of HTTP connections to their servers, doubling in quantity every 38 days for a six month period. <p> If this cache does not satisfy the request (a cache miss) then the request is forwarded to the network and onto the server. A study completed by O'Callaghan <ref> [22] </ref> at the University of Melbourne in Australia found that only 6.5% of the requests actually hit the client cache. This small hit rate could probably be improved significantly by increasing the size of the cache, but this is at the expense of disk space. 8 CHAPTER 2. <p> Many research groups have analyzed the performance difference between proxies; all of them come to the same conclusion that the CERN cache is slow and inefficient. The "CERN proxy software can probably handle a sustained load of no more than 5 connections per second" <ref> [22] </ref>. The CERN proxy uses the UNIX fork () procedure for every proxy request. The forked process terminates after the HTTP request has been satisfied, either via a cache hit or from the actual server. <p> of magnitude faster than the CERN cache. 12 Chapter 3 Caching Issues This chapter discusses which protocols to cache, the types of documents to cache, object lifetimes, uncachable documents, periodic trends, and one versus two way caching. 3.1 Which Protocols | HTTP, FTP, and Gopher | to Cache O'Callaghan's research <ref> [22] </ref> shows that using a shared cache space to store HTTP, FTP, and Gopher does not maximize cache hit ratio for a fixed disk size. <p> In contrast with other studies Wessel's proxy cache size was a mere 15MB, yielding an explainably low 23.5% hit rate. In comparison to the 5MB Netscape persistent client cache hit rates of approximately 6.5% <ref> [22] </ref> this is a large improvement; for only a three fold increase in cache size, there is an almost four fold increase in the cache hit rate. However, the difference may be due in part to a different workload.
Reference: [23] <author> James E. Pitkow and Margaret M. Recker. </author> <title> A Simple yet Robust Caching Algorithm Based on Dynamic Access Patterns. </title> <booktitle> The Second International WWW Conference '94, </booktitle> <volume> 2 </volume> <pages> 1039-46, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The proxy will not attempt to cache dynamic files. It determines whether a file is dynamic by looking for certain characters or character strings in the URL. Files containing the following: "cgi", "bin", "pl", "?", and "map" are considered dynamic. 3.6 Day of the Week Pitkow and Recker <ref> [23] </ref> found that the hit rate of a server cache containing all the documents accessed within the last 24 hours could be increased by using a five day week rather than a seven day week. When using a seven day week the mean hit rate was 67%. <p> On-demand Removal There are two methods of cache removal. The first is the on-demand removal policy, as advocated by the VT-NRG Group [31] for proxies. The second as, advocated by Pitkow and Recker <ref> [23] </ref> for servers, is periodic removal. In the periodic removal system the cache is periodically emptied to a comfort level, such as 80% of the disk space. This has attractive 19 CHAPTER 3. <p> This is, however, contradictory to the VT-NRG Group's [31] discoveries (described in section 4.1.2). This contradiction in their results may be due to the simple fact that Danzig et al. studied server caching and the VT-NRG group proxy caching. Also they used different workloads. Pitkow and Recker <ref> [23] </ref> in their studies of server caching found that " : : : recency proved to be a stronger predictor than frequency." Their studies showed that a mean hit rate of between 67% and 69% could be achieved by simply caching only the documents accessed one day ago.
Reference: [24] <author> Neil Smith. </author> <title> What Can Archives Offer the World-wide Web, </title> <month> May </month> <year> 1994. </year> <note> &lt;URL: http:- //www.hensa.ac.uk/www94/wwwcache.ps.Z&gt; (5 Dec. </note> <year> 1996). </year>
Reference-contexts: This indicates that a global TTL is difficult to set without either serving many stale documents or making many unnecessary server requests. The problem with the Lagoon cache <ref> [24] </ref> is that it does not use the "Expires" or "Last-Modified" fields, but relies upon a statically defined TTL. Different TTL's can be set for different types of documents or different domains.
Reference: [25] <author> Simon Spero. </author> <title> Next Generation Hypertext Transport Protocol. </title> <address> &lt;URL: http://sunsite .unc.edu/ses/ng-notes.txt&gt;, </address> <month> March </month> <year> 1995. </year> <note> Unpublished Paper (5 Dec. </note> <year> 1996). </year>
Reference-contexts: The term "absorption" is used to reinforce the idea that a significant amount of network bandwidth is used for headers rather than data. Tests have shown that 95% of the data of some HTTP requests is redundant <ref> [25] </ref>. The problem with the HTTP/1.0 protocol is that it handles connection negotiation on a per URL basis. This means that for each image within a WWW page a TCP connection is made, rather than one connection for the whole page. <p> This means that for each image within a WWW page a TCP connection is made, rather than one connection for the whole page. To exaggerate the problem HTTP sends every possible option supported by the client in each new connection request packet. Many new protocols <ref> [17, 20, 25] </ref> have been developed to solve one or both of the above problems. HTTP-NG [25] and HTTP/1.1 [11] use long lived connections. <p> To exaggerate the problem HTTP sends every possible option supported by the client in each new connection request packet. Many new protocols [17, 20, 25] have been developed to solve one or both of the above problems. HTTP-NG <ref> [25] </ref> and HTTP/1.1 [11] use long lived connections. Thus a TCP connection can serve all the images and documents within a WWW page, rather than making multiple TCP connections for a single WWW page containing multiple images or documents. <p> The initialization data is only sent in the first request; the server stores this information and associates it with the TCP connection. Successive requests to that server need only send option information if the options are changed by the client. Simon Spero <ref> [25] </ref> proposes a predictive form of HTTP with his HTTP-NG. Although prediction would reduce average response time for the client in theory, this idea may lead 4 CHAPTER 1. INTRODUCTION to significantly more inefficient use of network resources.
Reference: [26] <editor> W. Richard Stevens. TCP/IP Illustrated, </editor> <volume> volume 3. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1996. </year> <note> Section 10.5, Retransmission Timeout Calculations. </note>
Reference-contexts: Equation (4.3) uses the variable cbw j to represent the CBW for a particular server. The value used for ff, 1 8 , is the same as in the TCP smoothed estimation of Round Trip Time (RTT) <ref> [26] </ref>. The effect of this constant is that the new value reflects seven eighthes of the original value and one eighth of the most recently calculated value. <p> If the document is smaller than CONN, the time to download the document is used only for the CLAT calculations, otherwise it is used for the CBW calculations. This estimate is smoothed in a similar manner to TCP RTT latency estimates <ref> [14, 26] </ref>, using both old and new requests to the server as factors of the new estimate. Equation (4.4) shows the LAT algorithm's formula, the variable lat i is 29 CHAPTER 4. REMOVAL ALGORITHMS TESTED the LAT value for a particular file.
Reference: [27] <author> W3C. </author> <title> Propagation, Replication and Caching. </title> <journal> &lt;URL: </journal> <note> http://www.w3.org/hypertext /WWW/Propagation/Activity.html&gt;. Unpublished paper (5 Dec. </note> <year> 1996). </year>
Reference-contexts: When an HTTP page is requested the cost to the network is the product of the number of bytes and the hop count. The hop count is the number of network segments between client and server <ref> [27] </ref>. It is also important to consider what fraction of the bandwidth of a link is used by a connection.
Reference: [28] <author> W3C. </author> <title> Internet Survey Methodology and Web Demographics. </title> <address> Boston, MA, </address> <booktitle> Workshop, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Solutions to this problem are 18 CHAPTER 3. CACHING ISSUES being discussed. One suggestion is for a cache to maintain a count of the number of hits and to increment the origin server with the number of hits received on a regular basis <ref> [28] </ref>. The proxy will not attempt to cache dynamic files. It determines whether a file is dynamic by looking for certain characters or character strings in the URL.
Reference: [29] <author> Duane Wessel. </author> <title> Intelligent caching for World Wide Web Objects. </title> <booktitle> INET 95 Proceedings, </booktitle> <year> 1995. </year> <institution> University of Colorado, Boulder. </institution>
Reference-contexts: This saves bandwidth on all the networks between the cache and the origin server indicating that it is better to position the cache as close to the client as possible. Unfortunately there are disadvantages to caching data <ref> [29] </ref>; depending on the system the cache can become a single point of failure. It can also serve out of date documents to the user and can not cache dynamic documents. There are three locations for caches: at the client, at the server, and at the network. <p> The UNIX fork () procedure is very expensive, which is the primary reason for the poor performance of this proxy. "The operating system of a busy proxy site will spend too much time spawning new processes and not enough time relaying data" <ref> [29] </ref>. Studies have shown that using the CERN proxy will slow down a cache miss by an average of an extra 69% over the average direct latency 1 [20]. <p> Studies have shown that using the CERN proxy will slow down a cache miss by an average of an extra 69% over the average direct latency 1 [20]. Collaborative research groups <ref> [6, 29] </ref> at the University of Colorado at Boulder and the University of Southern California have created a number of vastly improved caching proxies. The most significant of these is the Harvest Cache. <p> A response to an HTTP request includes metadata about the object, such as an "Expires" field and a "last-modified" field. FTP and Gopher do not provide this information, making the choice of TTL much harder. Wessels <ref> [29] </ref> found that in one workload only six out of 28,000 HTTP replies included a complete "Expires" field. Thus the HTTP "last-modified" header field is what most cache servers use to estimate the TTL. A proxy cache stores the "last-modified" value with the object. <p> CACHING ISSUES between 5 and 7 days will return approximately 20% [6, 12] stale documents. Wessels <ref> [29] </ref> states that twenty percent is considered unacceptable. <p> In fact the company would save itself time and a significant amount of money, which over time could perhaps pay for the proxy server itself! One growing commercial issue is that many organizations' funding depends upon the number of times their WWW page is hit <ref> [29] </ref>, advertising being one key example. If a page is cached, then not all the cache hits will be recorded by the actual server, and consequently the server will be deemed far less popular than it really is. Solutions to this problem are 18 CHAPTER 3. CACHING ISSUES being discussed. <p> Their statistics are, however, for server caching and not proxy caching. Wessel <ref> [29] </ref> came to the same conclusion for proxy caching, that one day is a key caching point. He found that 10 minutes is also an important caching point. This would of course be included within the caching of documents from the last 24 hour period.
Reference: [30] <author> Stephen Williams, </author> <month> February </month> <year> 1996. </year> <type> Personal Communications. </type>
Reference-contexts: No research seems to agree exactly on what fraction of documents are of this type. Gwertzman et al. [12] mention 10% of the documents as a typical figure. The VT-NRG Group <ref> [30] </ref> found the maximum percentage in any of their workloads was 40% of the documents. <p> Arlitt et al. found this to be a significant proportion of the documents in a cache: "This one-time referencing behavior means that, on average, one-third of a server cache could be cluttered with useless documents [4]". The VT-NRG group <ref> [30] </ref>, in a sample of 50,000 URL accesses, found that more than 90% of the URLs were only referenced once. Obviously to cache these documents is wasteful for a cache with limited disk space. <p> This is included in the testing as a baseline. It is expected that the algorithms discussed in the following sections should perform better. 4.3.2 LFU The VT-NRG group have found in their simulations that LFU often yields the highest weighted hit rate of all the removal algorithms <ref> [30] </ref>. This is included to see if LFU will provide an improvement in response time over LRU. 4.3.3 SIZE SIZE yielded the highest hit rate of all the algorithms tested by the VT-NRG group [31]. <p> Figures 7.3 and 7.4 show the disappointingly poor results that LAT is always worst. The results with respect to the other three algorithms are consistent with the VT-NRG group's findings <ref> [31, 30] </ref>. The SIZE algorithm yielded the highest HR, and LFU yielded the highest WHR. The other aspect that can be seen in the two graphs is that LFU and LRU are very similar.
Reference: [31] <author> Stephen Williams, Marc Abrams, Charles Standridge, Ghaleb Abdulla, and Edward A Fox. </author> <title> Removal Policies in Network Caches for World-Wide Web Documents. </title> <booktitle> In Proc. SIGCOMM, </booktitle> <pages> pages 293-305, </pages> <month> August </month> <year> 1996. </year> <note> &lt;URL: http://ei.cs.vt.edu/ ~ succeed/- 96sigcomm/&gt; (5 Dec. 1996). 98 </note>
Reference-contexts: Quite obviously the hit rate will be significantly reduced if there are only a few huge documents in the cache. If, however, these audio or video documents are used a large number of times then the weighted hit rate will be extraordinary. This, however, does not often happen <ref> [2, 31] </ref>. The proxy server will require different, maximum cachable file size settings depending upon whether the weighted hit rate or the request hit rate is more important. In the workloads 2 used by the VT-NRG group less than 0.01% of files were over 4MB [2, 31]. <p> however, does not often happen <ref> [2, 31] </ref>. The proxy server will require different, maximum cachable file size settings depending upon whether the weighted hit rate or the request hit rate is more important. In the workloads 2 used by the VT-NRG group less than 0.01% of files were over 4MB [2, 31]. This insignificantly small percentage of files allows a maximum file size of 4MB to be used for "cachable" files in the Harvest Cache. Table 3.1 illustrates the enormous difference between the percentage of requests and percentage of bytes transferred in one workload at NCSA. <p> The VT-NRG Group found that for some of their workloads almost none of the actual references were for video, but between 26% and 39% of the bytes transferred were video documents <ref> [31] </ref>. Although not typical, a similar trait for audio was exhibited in one of their 2 The workloads used were the "local clients" and the "graduate clients". 14 CHAPTER 3. <p> The VT-NRG Group, however, found that there was no significant correlation with the day of the week and the hit rate achieved by a proxy cache <ref> [31] </ref>. Other studies [18] have shown that certain types of documents are more likely to be loaded in the daytime and others at nighttime. Again it might be worth researching this phenomena, using a tape drive to up-load and download documents at an appropriate time. 3.7 Periodic vs. <p> Again it might be worth researching this phenomena, using a tape drive to up-load and download documents at an appropriate time. 3.7 Periodic vs. On-demand Removal There are two methods of cache removal. The first is the on-demand removal policy, as advocated by the VT-NRG Group <ref> [31] </ref> for proxies. The second as, advocated by Pitkow and Recker [23] for servers, is periodic removal. In the periodic removal system the cache is periodically emptied to a comfort level, such as 80% of the disk space. This has attractive 19 CHAPTER 3. <p> There is, however, a serious drawback overlooked by this system: the cache is never full. Thus a great deal of disk space is wasted which could be caching documents that may be generating cache hits. The VT-NRG Group <ref> [31] </ref> argue that SIZE is the best key; a sorted list can be kept, maintaining at the head of the list the largest file. <p> One study has shown that a weighted hit rate of an amazing 95% can be achieved for caching the incoming traffic from clients outside a campus site <ref> [31] </ref>. Clearly, this reduces the load on the campus or business subnet by a factor of 20, which is quite extraordinary. Although the researchers claim that this is not typical, their test was for a period of 57 days, during which 227,210 documents were requested. <p> For this reason, LRU and LFU replacement policies are 23 CHAPTER 4. REMOVAL ALGORITHMS TESTED nearly indistinguishable." They showed that with a cache of 4GB both LRU and LFU yield a hit rate of 45%. This is, however, contradictory to the VT-NRG Group's <ref> [31] </ref> discoveries (described in section 4.1.2). This contradiction in their results may be due to the simple fact that Danzig et al. studied server caching and the VT-NRG group proxy caching. Also they used different workloads. <p> There are two distinct disadvantages of proxy caching over client caching, the latency is increased and the scalability decreased. 24 CHAPTER 4. REMOVAL ALGORITHMS TESTED 4.1.2 SIZE SIZE, although not an acronym, is written in upper case to be consistent with the other removal algorithm names. The VT-NRG Group <ref> [31] </ref> analyzed the performance of many different types of removal policy. They ranked hit rate performance in the following order (best first): SIZE, log (SIZE), LRU-MIN (which uses log (SIZE) as a primary key and LRU as a secondary key), LRU, Number of References (NREF). <p> This is included to see if LFU will provide an improvement in response time over LRU. 4.3.3 SIZE SIZE yielded the highest hit rate of all the algorithms tested by the VT-NRG group <ref> [31] </ref>. SIZE is included to test its performance in response time experiments. 4.3.4 Latency, LAT The Harvest Cache is user-configurable to prevent caching local documents, from machines in the same domain, such as ".vt.edu" or ".cs.vt.edu". There is sound reasoning behind this policy. <p> Using a second removal algorithm to decide which file to remove seems appealing. No study published, however, shows results that would conclude that using a second removal algorithm improves the hit rate significantly <ref> [31] </ref>. What is significant, however, is the cost of implementing a second sorting algorithm for the marginal performance improvement. The objective of this thesis is to improve response time for a large user base, where the CPU cost of removal is significant and thus must be minimized. <p> Figures 7.3 and 7.4 show the disappointingly poor results that LAT is always worst. The results with respect to the other three algorithms are consistent with the VT-NRG group's findings <ref> [31, 30] </ref>. The SIZE algorithm yielded the highest HR, and LFU yielded the highest WHR. The other aspect that can be seen in the two graphs is that LFU and LRU are very similar. <p> EXPERIMENT ONE: LRU, LFU, SIZE, AND LAT not work. The LAT algorithm, for a single server, favored the largest documents from that server. This resulted in a removal algorithm that worked in the opposite manner to the SIZE algorithm! As the SIZE algorithm was shown in <ref> [31] </ref> to be the best performer for at least HR it is logical to conclude that the LAT algorithm should do very badly. The LFU algorithm was the best performer for WHR. The LAT algorithm does not take into consideration how many times a file has been hit.
References-found: 31

