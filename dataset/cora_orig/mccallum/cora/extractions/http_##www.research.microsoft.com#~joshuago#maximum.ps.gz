URL: http://www.research.microsoft.com/~joshuago/maximum.ps.gz
Refering-URL: http://www.research.microsoft.com/~joshuago/
Root-URL: http://www.research.microsoft.com
Email: goodman@das.harvard.edu  
Title: Parsing Algorithms and Metrics  
Author: Joshua Goodman 
Address: 33 Oxford St. Cambridge, MA 02138  
Affiliation: Harvard University  
Abstract: Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others. However, most parsing algorithms, including the Viterbi algorithm, attempt to optimize the same metric, namely the probability of getting the correct labelled tree. By choosing a parsing algorithm appropriate for the evaluation metric, better performance can be achieved. We present two new algorithms: the "Labelled Recall Algorithm," which maximizes the expected Labelled Recall Rate, and the "Bracketed Recall Algorithm," which maximizes the Bracketed Recall Rate. Experimental results are given, showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria, especially the ones that they optimize. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baker, J.K. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the Spring Conference of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <address> Boston, MA, </address> <month> June. </month>
Reference: <author> Bod, Rens. </author> <year> 1993. </year> <title> Using an annotated corpus as a stochastic grammar. </title> <booktitle> In Proceedings of the Sixth Conference of the European Chapter of the ACL, </booktitle> <pages> pages 37-44. </pages>
Reference-contexts: We have used the technique outlined in this paper in other work (Goodman, 1996) to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the possible derivations was a slow Monte Carlo algorithm <ref> (Bod, 1993) </ref>.
Reference: <author> Brill, Eric. </author> <year> 1993. </year> <title> A Corpus-Based Approach to Language Learning. </title> <type> Ph.D. thesis, </type> <institution> University of Penn-sylvania. </institution>
Reference-contexts: Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed. One notable exception is Brill's Transformation-Based Error Driven system <ref> (Brill, 1993) </ref>, which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate. However, Brill's system is not probabilistic. Intuitively, if one were to match the parsing algorithm to the evaluation criterion, better performance should be achieved.
Reference: <author> Goodman, Joshua. </author> <year> 1996. </year> <title> Efficient algorithms for parsing the DOP model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing. To appear. </booktitle>
Reference-contexts: Thus, these algorithms improve performance not only on the measures that they were designed for, but also on related criteria. Furthermore, in some cases these techniques can make parsing fast when it was previously impractical. We have used the technique outlined in this paper in other work <ref> (Goodman, 1996) </ref> to efficiently parse the DOP model; in that model, the only previously known algorithm which summed over all the possible derivations was a slow Monte Carlo algorithm (Bod, 1993).
Reference: <author> Lari, K. and S.J. Young. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56. </pages>
Reference: <author> Magerman, David. </author> <year> 1994. </year> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University University, </institution> <month> February. </month>
Reference-contexts: Even in probabilistic models not closely related to PCFGs, such as Spatter parsing <ref> (Magerman, 1994) </ref>, expression (1) is still computed. One notable exception is Brill's Transformation-Based Error Driven system (Brill, 1993), which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate. However, Brill's system is not probabilistic.
Reference: <author> Magerman, D.M. and C. Weir. </author> <year> 1992. </year> <title> Efficiency, robustness, and accuracy in picky chart parsing. </title> <booktitle> In Proceedings of the Association for Computational Linguistics. </booktitle>
Reference-contexts: the following expression, where E denotes the expected value operator: T G = arg max E ( 1 if L = N C ) (1) This is true of the Labelled Tree Algorithm and stochastic versions of Earley's Algorithm (Stolcke, 1993), and variations such as those used in Picky parsing <ref> (Magerman and Weir, 1992) </ref>. Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed. One notable exception is Brill's Transformation-Based Error Driven system (Brill, 1993), which induces a set of transformations designed to maximize the Consistent Brackets Recall Rate.
Reference: <author> Pereira, Fernando and Yves Schabes. </author> <year> 1992. </year> <title> Inside-Outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 128-135, </pages> <address> Newark, Delaware. </address>
Reference: <author> Stolcke, Andreas. </author> <year> 1993. </year> <title> An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference-contexts: corpus was generated by the model, and then attempt to evaluate the following expression, where E denotes the expected value operator: T G = arg max E ( 1 if L = N C ) (1) This is true of the Labelled Tree Algorithm and stochastic versions of Earley's Algorithm <ref> (Stolcke, 1993) </ref>, and variations such as those used in Picky parsing (Magerman and Weir, 1992). Even in probabilistic models not closely related to PCFGs, such as Spatter parsing (Magerman, 1994), expression (1) is still computed.
References-found: 9

