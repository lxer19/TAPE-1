URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-035.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: HiPNeT-1: A Highly Pipelined Architecture for Neural Network Training  
Author: Krste Asanovic Brian E. D. Kingsbury Nelson Morgan John Wawrzynek 
Address: Berkeley, Berkeley, CA 94720  
Affiliation: Computer Science Division, EECS Department, University of California at  
Date: October 1991  
Pubnum: TR-91-035  
Abstract: Current artificial neural network (ANN) algorithms require extensive computational resources. However, they exhibit massive fine-grained parallelism and require only moderate arithmetic precision. These properties make possible custom VLSI implementations for high performance, low cost systems. This paper describes one such system, a special purpose digital VLSI architecture to implement neural network training in a speech recognition application. The network algorithm has a number of atypical features. These include: shared weights, sparse activation, binary inputs, and a serial training input stream. The architecture illustrates a number of design techniques to exploit these algorithm-specific features. The result is a highly pipelined system which sustains a learning rate of one pattern per clock cycle. At a clock rate of 20MHz each "neuron" site performs 200 million connection updates per second. Multiple such neurons can be integrated onto a modestly sized VLSI die. fl International Computer Science Institute, 1947 Center Street, Berkeley, CA 94704
Abstract-found: 1
Intro-found: 1
Reference: [Asa] <author> K. Asanovic. OctC++: </author> <title> A C++ interface to the Oct database. </title> <type> ICSI Technical Report. </type> <note> In preparation. </note>
Reference-contexts: Fast and flexible CAD techniques are also being developed to make this kind of design a practical solution to systems that require this throughput [MAKW90] <ref> [Asa] </ref>. 10 Acknowledgements Phil Kohn ran many of the simulations discussed in the paper. We also thank James Beck for his constructive criticism and advice throughout this work. The National Science Foundation has provided explicit support for this project with Grant No.
Reference: [BH88] <author> T. Baker and D. Hammerstrom. </author> <title> Modifications to artificial neural network models for digital hardware implementation. </title> <type> Technical Report CS/E 88-035, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Center, </institution> <year> 1988. </year>
Reference-contexts: Our simulations indicate that a weight precision of 12-16 bits is sufficient for learning our speech task with back-propagation. Output values require 6-8 bits. These findings are consistent with those discovered by other researchers working with back propagation <ref> [BH88] </ref>. The systems we describe here use 12 bit weights and 6 bit output values. We use only the most significant 6 bits of the stored weight in calculating the output value, and restrict weight increments w ij to 6 bits.
Reference: [Hin87] <author> G. Hinton. </author> <title> Connectionist learning procedures. </title> <type> Technical Report CMU-CS-87-115, </type> <institution> Carnegie Mellon, </institution> <year> 1987. </year>
Reference-contexts: The output of neuron j is given by o j = f (s j ) where s j = i and 1 The training algorithm derives a weight update value w ij using a cross entropy error criterion <ref> [Hin87] </ref>. where d j is the desired output value for neuron j, and ff is the learning rate. 1 3 Algorithm Modifications for Digital Hardware Implemen tation We require no multiplications, since input values are either 1 or 0. Within each frame only a single input is active.
Reference: [HP90] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture | a quantative approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: For other algorithms and for deeper pipelines this overwriting of weight updates may still be a problem. We can avoid this consequence of the Read After Write hazard by using a technique analogous to register forwarding in pipelined register-register architectures <ref> [HP90] </ref>. We store recently updated weight values together with their RAM addresses in an associatively searchable shift register. The shift register can be searched using the address of an incoming weight update.
Reference: [HTCB89] <author> M. Holler, S. Tam, H. Castro, and R. Benson. </author> <title> An electrically trainable artificial neural network (ETANN) with 10240 "Floating Gate" synapses. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages II-191-196, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Many neural network algorithms, including the popular error back-propagation algorithm, require a range of arithmetic precisions for the variables used. Although many of the proposed analog neural network implementations promise excellent performance in certain applications <ref> [Mea89, HTCB89] </ref>, their relatively low precision limits their general applicability. Further, in general an artificial neural network will form part of a larger information processing system and so ease of system integration will be an important design criterion.
Reference: [Koh] <author> P. Kohn. </author> <type> Personal communication. </type>
Reference-contexts: In general, weight updates are lost for all but the last of a set of matching features in a time window equal to the depth of the pipeline. Our simulations indicate that this weight update hazard is not detrimental to learning performance <ref> [Koh] </ref>. This is perhaps surprising, but one explanation is that successive updates to the same feature would tend to move the network too far towards one direction in the search space. For other algorithms and for deeper pipelines this overwriting of weight updates may still be a problem.
Reference: [MAKW90] <author> N. Morgan, K. Asanovic, B. Kingsbury, and J. Wawrzynek. </author> <title> Developments in digital VLSI design for artificial neural networks. </title> <type> Technical Report TR-90-065, </type> <institution> International Computer Science Institute, </institution> <year> 1990. </year>
Reference-contexts: Part of our research is the investigation of general architectural techniques which may be applied to such systems. We are also working on the development of CAD tools and library cells to support rapid VLSI layout generation <ref> [MAKW90] </ref>. A number of example systems are being implemented to guide our work. The first of these is an architecture to perform neural network training for a speech recognition system being developed at ICSI. In this paper, we first describe the speech recognition application. <p> Fast and flexible CAD techniques are also being developed to make this kind of design a practical solution to systems that require this throughput <ref> [MAKW90] </ref> [Asa]. 10 Acknowledgements Phil Kohn ran many of the simulations discussed in the paper. We also thank James Beck for his constructive criticism and advice throughout this work. The National Science Foundation has provided explicit support for this project with Grant No.
Reference: [MB90] <author> N. Morgan and H. Bourlard. </author> <title> Continuous speech recognition using Multilayer Perceptrons with Hidden Markov models. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 413-416, </pages> <address> Albuquerque, USA, </address> <year> 1990. </year>
Reference-contexts: The ANN can also combine multiple sources of evidence, such as multiple features and contextual windows, in a straightforward and efficient manner. Initial experiments indicate that this method compares favorably with conventional HMM speech recognition methods <ref> [MB90] </ref>. In this system, continuous features (such as spectra) are extracted from speech input and passed to a vector quantizer that maps the input speech frame into one of a set of prototype vectors or features. <p> Each vector quantized frame is represented using unary encoding, that is, using a binary input neuron for each possible feature value, only one of which can be active at a time. For the networks described in <ref> [MB90] </ref>, the vector quantizer selects one from 132 features and the ANN is fed a window of 9 frames. This gives 132 fi 9 = 1188 input layer neurons, of which only 9 will be active in a given pattern.
Reference: [MBAB90] <author> N. Morgan, J. Beck, E. Allman, and J. Beer. </author> <title> RAP: A Ring Array Processor for Multilayer Perceptron applications. </title> <booktitle> In Proc. IEEE Intl. Conf. on Acoustics, Speech, & Signal Processing, </booktitle> <pages> pages 1005-1008, </pages> <address> Albuquerque, New Mexico, </address> <year> 1990. </year>
Reference-contexts: 10 6 200 fi 10 6 200 fi 10 6 12.5 12.5 ETANN 300 fi 10 6 2000 fi 10 6 - 6.7 - TMS320C30 700 fi 10 6 16 fi 10 6 4 fi 10 6 0.02 0.006 ray Processor (RAP) using 4 TMS320C30 chips achieves about 13 MCUPS <ref> [MBAB90] </ref>, while a Sun SparcStation-1 attains around 0.4 CUPS. For the sparse activation case treated in this design, each of these figures is several times lower. The special purpose designs can sustain their maximum throughput on this algorithm.
Reference: [Mea89] <author> C.A. Mead. </author> <title> Analog VLSI and neural systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction Many neural network algorithms, including the popular error back-propagation algorithm, require a range of arithmetic precisions for the variables used. Although many of the proposed analog neural network implementations promise excellent performance in certain applications <ref> [Mea89, HTCB89] </ref>, their relatively low precision limits their general applicability. Further, in general an artificial neural network will form part of a larger information processing system and so ease of system integration will be an important design criterion. <p> Given that high performance is readily obtainable, it is necessary to account for cost when comparing alternative architectures. One technology-independent way of expressing silicon resource consumption is to consider the die area expressed as the square of the minimum length unit, or parameter <ref> [Mea89] </ref>. In Table 9 we have abbreviated Connections Per Second as CPS, while the unit performance normalized by the scalable area (using 2 ) is called the COnnection Rate Density (CORD).
Reference: [Mor90] <author> N. Morgan, </author> <title> editor. Artificial Neural Networks: Electronic Implementations. </title> <publisher> Computer Society Press Technology Series. Computer Society Press of the IEEE, </publisher> <address> Washington, D.C., </address> <year> 1990. </year>
Reference-contexts: Further, in general an artificial neural network will form part of a larger information processing system and so ease of system integration will be an important design criterion. For these and other reasons <ref> [Mor90] </ref>, our group at ICSI is concentrating on the development of purely digital VLSI connectionist hardware. Part of our research is the investigation of general architectural techniques which may be applied to such systems. <p> Analog circuitry can be designed with such precision, but such design is difficult and the results are unlikely to be compact or to scale well with shrinking process geometries. Most analog neural net implementations have implemented non-sparse activation with a multiplier per synapse, using a simple 2-dimensional connection pattern <ref> [Mor90] </ref>. The speech algorithm targeted here has sparse activation, and so this simple approach would be extremely inefficient with less than 1% of the synapses active at a time. Further, only a similar small fraction of the system I/O will be usefully employed.
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. In Parallel Distributed Processing. </title> <journal> Exploration of the Microstructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The output layer consists of 50-64 neurons, corresponding to the number of phonemes to be recognized. The best experimental results for this problem were obtained without hidden units, and so the input layer is directly and fully connected to the output layer. Error back-propagation training is used <ref> [Wer74, RHW86] </ref>. The network algorithms are summarized in the following equations. Input neuron i is connected to output neuron j by a weight w ij .
Reference: [Wer74] <author> P.J. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Dept. of Applied Mathematics, Harvard University, </institution> <year> 1974. </year> <month> 10 </month>
Reference-contexts: The output layer consists of 50-64 neurons, corresponding to the number of phonemes to be recognized. The best experimental results for this problem were obtained without hidden units, and so the input layer is directly and fully connected to the output layer. Error back-propagation training is used <ref> [Wer74, RHW86] </ref>. The network algorithms are summarized in the following equations. Input neuron i is connected to output neuron j by a weight w ij .
References-found: 13

