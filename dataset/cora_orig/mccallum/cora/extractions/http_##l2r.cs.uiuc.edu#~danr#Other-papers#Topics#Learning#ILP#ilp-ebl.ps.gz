URL: http://l2r.cs.uiuc.edu/~danr/Other-papers/Topics/Learning/ILP/ilp-ebl.ps.gz
Refering-URL: http://l2r.cs.uiuc.edu/~danr/Teaching/CS491-98/491-list.html
Root-URL: http://www.cs.uiuc.edu
Email: mooney@cs.utexas.edu, zelle@cs.utexas.edu  
Title: Integrating ILP and EBL  
Author: Raymond J. Mooney and John M. Zelle 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Abstract: This paper presents a review of recent work that integrates methods from Inductive Logic Programming (ILP) and Explanation-Based Learning (EBL). ILP and EBL methods have complementary strengths and weaknesses and a number of recent projects have effectively combined them into systems with better performance than either of the individual approaches. In particular, integrated systems have been developed for guiding induction with prior knowledge (ML-Smart, Focl, Gren-del) refining imperfect domain theories (Forte, Audrey, Rx), and learning effective search-control knowledge (AxA-EBL, Dolphin). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Bergadano, A. Giodana, L. Saitta, D. De Marchi, and F. Brancadori. </author> <title> Integrated learning in a real domain. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 322-328, </pages> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: A variant of ML-Smart was applied to learning an expert system for a real-world electromechanical fault diagnosis problem <ref> [1] </ref>.
Reference: [2] <author> F. Bergadano and A. Giordana. </author> <title> A knowledge intensive approach to concept induction. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 305-317, </pages> <address> Ann Arbor, MI, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Two general approaches to inte grating ILP and EBL in concept learning have been explored. One approach, knowledge-guided induction, uses the existing domain theory to guide the learning of a separate concept definition, e.g. <ref> [2, 34, 7] </ref>. The other approach, theory refinement, uses the examples to modify the existing domain theory in an attempt to improve its accuracy, e.g. [39, 52, 47]. <p> This flexibility derives from viewing the background theory as a specification for the inductive hypothesis space rather than as a concept definition per se. This shift in viewpoint underlies the explicit grammatical bias used in Gren-del, which is discussed below. 3.1.2 ML-Smart ML-Smart <ref> [2] </ref> illustrates another approach to integrating EBL and inductive techniques to construct operational concept definitions. Whereas theory specialization systems can only deal with overly-general background theories, ML-Smart is able to handle both overly-general and overly-specific theories.
Reference: [3] <author> I. Bratko. </author> <title> Prolog Programming for Artificial Intelligence. </title> <publisher> Ad-dison Wesley, </publisher> <address> Reading:MA, </address> <year> 1990. </year>
Reference-contexts: Forte correctly debugged all 23 programs (see Table 3). Forte has also been used to debug a version of the decision-tree induction program from Bratko's Prolog text <ref> [3] </ref>, and to revise a qualitative model of a portion of the Reaction Control System of the NASA Space Shuttle. 3.2.2 Audrey and A3 Audrey [52], Audrey II [54], and A3 [53] are a series of first-order theory refinement systems that also integrate ideas from ILP and EBL.
Reference: [4] <author> W. W. Cohen. </author> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 268-276, </pages> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Sample problems are used to generate a separate set of control examples describing appropriate and inappropriate contexts for applying operators in the domain theory. A combination of ILP and EBL methods are then used to learn control rules for deciding when to apply the existing operators, e.g. <ref> [4, 55] </ref>. Control learning is traditionally used to improve the efficiency of a problem solver as a form of speedup learning [46]; however, it can also be used to improve accuracy [56]. This paper presents a review of recent research that integrates ILP and EBL methods. <p> considering the problem of control-rule learning within the context of logic programming. 4.2 Controlling Search in Logic Programs Although most EBL research in logic programming has generally focussed on learning macros, [25, 10, 36], Recent work has shown the utility of learning explicit search-control rules within a logic programming framework <ref> [4, 55, 56] </ref>. The execution of a logic program can be viewed as a problem solving process with a search strategy based on resolution theorem proving. A program executes by finding a constructive proof of a partially instantiated goal given as input. <p> Prolog provides a particular implementation of logic programming using a very simple control strategy incorpo rating depth-first search with simple backtracking. Controlling search in this framework can be viewed as a clause selection problem <ref> [4] </ref>. Clause selection is the process of deciding which of several applicable program clauses should be used to solve a particular subgoal during the course of a proof. <p> This type of problem illustrates the need to consider conditions of the top-level proof that lie outside of the subproof of the immediate goal for which clause selection rules are being learned. 4.4 AxA-EBL Cohen <ref> [4] </ref> attacks the utility problem for clause selection by combining EBL with induction to learn a small set of "approximate" control rules with reduced match cost. His algorithm, AxA-EBL (Approximating Abductive EBL), is a variant of the A-EBL concept learning algorithm applied to the problem of learning control rules.
Reference: [5] <author> W. W. Cohen. </author> <title> The generality of over-generality. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 490-495, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Cohen <ref> [5] </ref> shows that ANA-EBL, a variant of A-EBL that allows macros to retain some non-operational conditions, is able to exploit many different kinds of background knowledge expressed as overly-general theories. <p> Careful inspection shows that this is a version of the insertion sort algorithm, and an O (n!) sort has been "optimized" into an O (n 2 ) version by learning and incorporating suitable control rules. 4.3 An EBL Approach One approach to learning search-control in a logic program is EBL-control <ref> [5] </ref>, an application of standard EBL methods to the clause selection problem. Each clause application in a successful proof is "explained" by compiling a macro-rule for the subgoal which extracts the generalized operational conditions that allowed the the clause to successfully solve the subgoal.
Reference: [6] <author> W. W. Cohen. </author> <title> Abductive explanation-based learning: A solution to the multiple inconsistent explanation problem. </title> <journal> Machine Learning, </journal> <volume> 8(2) </volume> <pages> 167-219, </pages> <year> 1992. </year>
Reference-contexts: We call such an approach knowledge-guided induction. 3.1.1 Theory specialization One of the earliest uses of EBL techniques to guide concept induction in structured domains was through theory specialization <ref> [14, 6] </ref>. These systems utilize background knowledge represented as an overly-general domain theory. That is, the background theory defines a concept that is a superset of the concept to be learned.
Reference: [7] <author> W.W. Cohen. </author> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 102-110, </pages> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Two general approaches to inte grating ILP and EBL in concept learning have been explored. One approach, knowledge-guided induction, uses the existing domain theory to guide the learning of a separate concept definition, e.g. <ref> [2, 34, 7] </ref>. The other approach, theory refinement, uses the examples to modify the existing domain theory in an attempt to improve its accuracy, e.g. [39, 52, 47]. <p> In another experiment, these systems were shown to significantly outperform pure induction in a real-world task customizing an existing expert system for locating phone-line faults. 3.1.4 Grendel Grendel <ref> [7] </ref> is another Foil-based system that incorporates domain knowledge. Unlike, the previously discussed systems where background knowledge is provided in the form of a Horn-clause theory that somehow approximates the concept, Grendel uses an antecedent description grammar (ADG) to explicitly represent the inductive hypothesis space.
Reference: [8] <author> G. DeJong. </author> <title> Generalizations based on explanations. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 67-70, </pages> <address> Vancouver, BC, </address> <year> 1981. </year>
Reference-contexts: EBL [25, 10] focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning [13], natural language understanding <ref> [8] </ref>, and mathematical problem solving [23]. Despite these differences, the approaches also have two important similarities. First, both ILP and EBL methods employ background knowledge in learning. ILP methods generalize an existing domain theory by inductively adding new rules that utilize existing concepts in the theory. <p> Second, was the desire to employ learning to improve problem solving as opposed to the traditional focus on classification. Historically, EBL methods were developed to improve planning [13], mathematical problem solving [23, 45], and story understanding <ref> [8] </ref>. The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches [25, 10]. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from [25]).
Reference: [9] <author> G. F. DeJong, </author> <title> editor. Investigating Explanation-Based Learning. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1993. </year>
Reference-contexts: 4 reviews ILP/EBL research in learning control rules and Section 5 presents our conclusions. 2 Background on EBL The goal of explanation-based learning is to acquire an efficient concept definition from a single example by using existing background knowledge to explain the example and thereby focus on its important features <ref> [9] </ref>. The development of EBL was driven by two general goals. First, was the desire to make greater use of background knowledge in learning as opposed to resorting to "tabula rasa" induction.
Reference: [10] <author> G. F. DeJong and R. J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: ILP [31] focuses on induction of logic programs from examples and has roots in early work in generalization in logic [35] and logic-program synthesis [44]. EBL <ref> [25, 10] </ref> focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning [13], natural language understanding [8], and mathematical problem solving [23]. Despite these differences, the approaches also have two important similarities. <p> Historically, EBL methods were developed to improve planning [13], mathematical problem solving [23, 45], and story understanding [8]. The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches <ref> [25, 10] </ref>. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from [25]). The task involves using a single example and a domain theory to transform an abstract definition of a concept into an operational definition that is useful for efficient classification. <p> A unifying framework that cleanly integrates these techniques can be achieved by considering the problem of control-rule learning within the context of logic programming. 4.2 Controlling Search in Logic Programs Although most EBL research in logic programming has generally focussed on learning macros, <ref> [25, 10, 36] </ref>, Recent work has shown the utility of learning explicit search-control rules within a logic programming framework [4, 55, 56]. The execution of a logic program can be viewed as a problem solving process with a search strategy based on resolution theorem proving.
Reference: [11] <author> L. DeRaedt and M. Bruynooghe. </author> <title> An overview of the interactive concept learner and theory revisor CLINT. </title> <editor> In S. Mug-gleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 163-192. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This paper presents a review of recent research that integrates ILP and EBL methods. We intentionally focus on ILP work that is clearly influenced by ideas from EBL and do not attempt to review other ILP research in the areas of knowledge-based induction and theory refinement, e.g. <ref> [11, 44] </ref>. The remainder of the paper is organized as follows. Section 2 presents a brief review of traditional EBL for the uninitiated reader. Section 3 reviews ILP/EBL research in concept learning, including work in knowledge-guided induction and theory refinement.
Reference: [12] <author> O. Etzioni. </author> <title> STATIC: A problem-space compiler for PRODIGY. </title> <booktitle> In Proceedings of National Conference on Artificial Intelligence, </booktitle> <pages> pages 533-540, </pages> <address> Anaheim, CA, </address> <year> 1991. </year>
Reference-contexts: Such techniques formed the basis of the Lex2 system [23]. Subsequent speedup learning research in planning (e.g. Prodigy [21], Static <ref> [12] </ref>) and production sys tems (e.g. Soar [18]) has focussed on these analytic methods. Arguably, one of the reasons for the ascent of EBL over inductive techniques in control-rule learning was the relative lack of good tools for performing induction in structured domains.
Reference: [13] <author> R. E. Fikes, P. E. Hart, and N. J. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <journal> Artificial Intelligence, </journal> <volume> 3(4) </volume> <pages> 251-288, </pages> <year> 1972. </year>
Reference-contexts: EBL [25, 10] focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning <ref> [13] </ref>, natural language understanding [8], and mathematical problem solving [23]. Despite these differences, the approaches also have two important similarities. First, both ILP and EBL methods employ background knowledge in learning. ILP methods generalize an existing domain theory by inductively adding new rules that utilize existing concepts in the theory. <p> First, was the desire to make greater use of background knowledge in learning as opposed to resorting to "tabula rasa" induction. Second, was the desire to employ learning to improve problem solving as opposed to the traditional focus on classification. Historically, EBL methods were developed to improve planning <ref> [13] </ref>, mathematical problem solving [23, 45], and story understanding [8]. The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches [25, 10]. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from [25]). <p> In addition to EBL-control and AxA-EBL which are the control-rule learning algorithms discussed above, the figure also shows a comparison to a macro-based speedup strategy, EBL-macro. EBL-macro uses a macro-operator approach reminiscent of the original STRIPS macro-operator learning mechanism <ref> [13] </ref>. EBL-macro collects a set of top-level macros from the training problems and attempts to solve new problems by direct application of a previously learned macro before resorting to normal planning.
Reference: [14] <author> N. S. Flann and T. G. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 187-226, </pages> <year> 1989. </year>
Reference-contexts: We call such an approach knowledge-guided induction. 3.1.1 Theory specialization One of the earliest uses of EBL techniques to guide concept induction in structured domains was through theory specialization <ref> [14, 6] </ref>. These systems utilize background knowledge represented as an overly-general domain theory. That is, the background theory defines a concept that is a superset of the concept to be learned.
Reference: [15] <author> A. Ginsberg. </author> <title> Theory reduction, theory revision, </title> <booktitle> and retrans-lation. In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 777-782, </pages> <address> Detroit, MI, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Rx initially learns an operational concept definition in a manner very similar to Focl and then "unoperational izes" this definition back into a revised theory. The general approach is very similar to the reduce, revise, and retrans-late method used in the RTLS propositional theory reviser <ref> [15] </ref>. Latex is a simple hill-climbing revision system that uses a method based on minimum description length (MDL) to handle noisy data. The theory revision process in Rx consists of four steps: 1) Operationalization, 2) Specialization, 3) Rule creation, and 4) Unoperationalization.
Reference: [16] <author> J. Gratch and G. DeJong. COMPOSER: </author> <title> A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 235-240, </pages> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: A number of techniques have been subsequently developed to help insure the utility of EBL, including simplifying and selectively retaining and utilizing learned rules <ref> [22, 26, 16] </ref>. As discussed in section 4, combining EBL with ILP is another important approach to addressing the utility problem. Another well-recognized problem with traditional EBL is the requirement that the domain theory be correct and complete [25].
Reference: [17] <author> S. T. Kedar-Cabelli and L. T. McCarty. </author> <title> Explanation-based generalization as resolution theorem proving. </title> <booktitle> In Proceedings of the 1987 Machine Learning Workshop, </booktitle> <pages> pages 383-389, </pages> <address> Irvine, CA, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Generalize: Determine a set of sufficient conditions under which the explanation structure holds, stated in terms of the operationality criterion. Standard Prolog deduction is generally used to construct explanations. Several algorithms have been developed for correctly performing the generalization step <ref> [28, 17] </ref>. These procedures use unification to properly variablize the explanation and thereby generalize the proof as far as possible while maintaining it's correctness. For the example in Table 2, the proof and the generalization are straight-forward. The generalized proof for this example is shown in Figure 1.
Reference: [18] <author> J. Laird, P. Rosenbloom, and A. Newell. </author> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: Such techniques formed the basis of the Lex2 system [23]. Subsequent speedup learning research in planning (e.g. Prodigy [21], Static [12]) and production sys tems (e.g. Soar <ref> [18] </ref>) has focussed on these analytic methods. Arguably, one of the reasons for the ascent of EBL over inductive techniques in control-rule learning was the relative lack of good tools for performing induction in structured domains. The recent emergence of new, efficient ILP algorithms (e.g.
Reference: [19] <author> P. Langley. </author> <title> Learning to search: From weak methods to domain specific heuristics. </title> <journal> Cognitive Science, </journal> <volume> 9(2) </volume> <pages> 217-260, </pages> <year> 1985. </year>
Reference-contexts: The notion of intelligence as controlled search pervades AI. Learning rules to control search can im-prove both the efficiency and accuracy of a problem solver by eliminating search along paths that do not lead to correct solutions. 4.1 Background Early systems such as Lex [24] and Sage <ref> [19] </ref> used inductive methods to learn operator-selection rules. Using experience from a set of training problems, these systems collected examples where the application of each operator led to successful problem solutions. Induction over the problem-solving states in which various operators were successful or unsuccessful produced operator-selection rules.
Reference: [20] <author> C. Leckie and I. Zukerman. </author> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In Proceedings of the Thirteenth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 1100-1105, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The recent emergence of new, efficient ILP algorithms (e.g. Foil and Golem), has led to renewed interest in inductive methods for control-rule learning. Leckie and Zuckerman's Grasshopper <ref> [20] </ref> uses a modification of Foil to learn search-control rules in a Prodigy framework. Grasshopper was shown to outperform Prodigy/EBL on several standard planning problems. The inductive process was able to learn rules that were relatively simple and covered multiple examples.
Reference: [21] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 564-569, </pages> <address> St. Paul, MN, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: It is also closely related to chunking as applied to production rules in systems like Soar [43]. Adding macro rules can result in dramatic improvements in efficiency although on many tasks the overhead of matching many specific rules can eventually degrade overall performance <ref> [21] </ref>. The fact that adding compiled rules can have a negative as well as a positive effect on efficiency is generally referred to as the utility problem. <p> Such techniques formed the basis of the Lex2 system [23]. Subsequent speedup learning research in planning (e.g. Prodigy <ref> [21] </ref>, Static [12]) and production sys tems (e.g. Soar [18]) has focussed on these analytic methods. Arguably, one of the reasons for the ascent of EBL over inductive techniques in control-rule learning was the relative lack of good tools for performing induction in structured domains. <p> These two factors serve to increase the utility of the resulting rules by lowering the cost of matching the rules to new problem states and insuring that the learned rules have broad applicability. Thus induction helps tame the utility problem of control-learning <ref> [21] </ref> wherein the acquisition of control rules can sometimes degrade the overall performance of a system. Of course, the purely inductive approach of Grasshopper does not take advantage of the information provided by analytic methods. It would seem that a combination of EBL and ILP might provide the best alternative.
Reference: [22] <author> S. Minton. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference-contexts: A number of techniques have been subsequently developed to help insure the utility of EBL, including simplifying and selectively retaining and utilizing learned rules <ref> [22, 26, 16] </ref>. As discussed in section 4, combining EBL with ILP is another important approach to addressing the utility problem. Another well-recognized problem with traditional EBL is the requirement that the domain theory be correct and complete [25].
Reference: [23] <author> T. Mitchell. </author> <title> Learning and problem solving. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1139-1151, </pages> <address> Karlsruhe, West Germany, </address> <month> August </month> <year> 1983. </year>
Reference-contexts: EBL [25, 10] focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning [13], natural language understanding [8], and mathematical problem solving <ref> [23] </ref>. Despite these differences, the approaches also have two important similarities. First, both ILP and EBL methods employ background knowledge in learning. ILP methods generalize an existing domain theory by inductively adding new rules that utilize existing concepts in the theory. <p> Second, was the desire to employ learning to improve problem solving as opposed to the traditional focus on classification. Historically, EBL methods were developed to improve planning [13], mathematical problem solving <ref> [23, 45] </ref>, and story understanding [8]. The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches [25, 10]. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from [25]). <p> However, utilizing a simple domain theory stating that an operator is useful when it leads to a solution allows the learning of this generalization from a single example using EBL techniques [25]. Such techniques formed the basis of the Lex2 system <ref> [23] </ref>. Subsequent speedup learning research in planning (e.g. Prodigy [21], Static [12]) and production sys tems (e.g. Soar [18]) has focussed on these analytic methods.
Reference: [24] <author> T. Mitchell, T. Utgoff, and R. Banerji. </author> <title> Learning problem solving heuristics by experimentation. </title> <editor> In R. Michalski, T. Mitchell, and J. Carbonell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: The notion of intelligence as controlled search pervades AI. Learning rules to control search can im-prove both the efficiency and accuracy of a problem solver by eliminating search along paths that do not lead to correct solutions. 4.1 Background Early systems such as Lex <ref> [24] </ref> and Sage [19] used inductive methods to learn operator-selection rules. Using experience from a set of training problems, these systems collected examples where the application of each operator led to successful problem solutions. Induction over the problem-solving states in which various operators were successful or unsuccessful produced operator-selection rules.
Reference: [25] <author> Tom M. Mitchell, Richard M. Keller, and Smadar T. Kedar--Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: ILP [31] focuses on induction of logic programs from examples and has roots in early work in generalization in logic [35] and logic-program synthesis [44]. EBL <ref> [25, 10] </ref> focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning [13], natural language understanding [8], and mathematical problem solving [23]. Despite these differences, the approaches also have two important similarities. <p> Historically, EBL methods were developed to improve planning [13], mathematical problem solving [23, 45], and story understanding [8]. The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches <ref> [25, 10] </ref>. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from [25]). The task involves using a single example and a domain theory to transform an abstract definition of a concept into an operational definition that is useful for efficient classification. <p> The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches [25, 10]. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from <ref> [25] </ref>). The task involves using a single example and a domain theory to transform an abstract definition of a concept into an operational definition that is useful for efficient classification. <p> Table 2 shows one of the standard examples of this task, learning a structural definition of a cup from a functional definition, a single example, and a domain theory relating form to function <ref> [51, 25] </ref>. The basic method for solving the explanation-based generalization problem consists of the fol Table 1: The Explanation-Based Generalization Problem. <p> As discussed in section 4, combining EBL with ILP is another important approach to addressing the utility problem. Another well-recognized problem with traditional EBL is the requirement that the domain theory be correct and complete <ref> [25] </ref>. A number of researchers have combined EBL with various forms of induction in order to address this issue. For example, EBL has been combined with decision-tree induction [32] and neural-network learning [49] in order to refine incomplete and/or incorrect propositional domain theories. <p> However, utilizing a simple domain theory stating that an operator is useful when it leads to a solution allows the learning of this generalization from a single example using EBL techniques <ref> [25] </ref>. Such techniques formed the basis of the Lex2 system [23]. Subsequent speedup learning research in planning (e.g. Prodigy [21], Static [12]) and production sys tems (e.g. Soar [18]) has focussed on these analytic methods. <p> A unifying framework that cleanly integrates these techniques can be achieved by considering the problem of control-rule learning within the context of logic programming. 4.2 Controlling Search in Logic Programs Although most EBL research in logic programming has generally focussed on learning macros, <ref> [25, 10, 36] </ref>, Recent work has shown the utility of learning explicit search-control rules within a logic programming framework [4, 55, 56]. The execution of a logic program can be viewed as a problem solving process with a search strategy based on resolution theorem proving.
Reference: [26] <author> R. J. Mooney. </author> <title> The effect of rule use on the utility of explanation-based learning. </title> <booktitle> In Proceedings of the Eleventh International Joint conference on Artificial intelligence, </booktitle> <pages> pages 725-730, </pages> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: A number of techniques have been subsequently developed to help insure the utility of EBL, including simplifying and selectively retaining and utilizing learned rules <ref> [22, 26, 16] </ref>. As discussed in section 4, combining EBL with ILP is another important approach to addressing the utility problem. Another well-recognized problem with traditional EBL is the requirement that the domain theory be correct and complete [25].
Reference: [27] <author> R. J. Mooney. </author> <title> A preliminary PAC analysis of theory revision. </title> <editor> In T. Petsche, S. Judd, and S. Hanson, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Vol. 3. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <publisher> in press. </publisher>
Reference-contexts: A notion of the syntactic distance between two theories can be defined based on the number of primitive edit operations required to transform one theory into the other <ref> [27, 54] </ref>. Unfortunately, it is computationally intractable to minimize such measures when revising a theory. Therefore, implemented systems must resort to some form of heuristic search in an attempt to minimize syntactic change to the theory.
Reference: [28] <author> R. J. Mooney and S. W. Bennett. </author> <title> A domain independent explanation-based generalizer. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 551-555, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Generalize: Determine a set of sufficient conditions under which the explanation structure holds, stated in terms of the operationality criterion. Standard Prolog deduction is generally used to construct explanations. Several algorithms have been developed for correctly performing the generalization step <ref> [28, 17] </ref>. These procedures use unification to properly variablize the explanation and thereby generalize the proof as far as possible while maintaining it's correctness. For the example in Table 2, the proof and the generalization are straight-forward. The generalized proof for this example is shown in Figure 1.
Reference: [29] <author> S. Muggleton and W. Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 339-352, </pages> <address> Ann Arbor, MI, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Program # of Training Train % Programs Set Size Time Correct directed path 4 121 87 secs 100% insert after 9 35 82 secs 100% merge sort 10 60 199 secs 100% first order induction [38], and inverse resolution <ref> [29] </ref>. First, Forte attempts to prove all positive and negative examples using the current theory. When a positive example is unprovable, some clause in the theory needs to be generalized. All clauses that were backtracked over during the attempted proof are candidates for generalization. <p> Antecedents are chosen for deletion using a greedy algorithm that attempts to maximize the number of additional provable positive examples without causing additional provable negatives. New rules are learned using Foil and relational pathfinding. Forte also includes two additional generalization operators (identification and absorption) based on inverse resolution <ref> [29] </ref>. These operators introduce new rules based on repeated patterns of literals found in existing rules. In one test, Forte was used to automatically debug Pro-log programs written by undergraduates for an assignment in a class on programming languages.
Reference: [30] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: However, many systems are only concerned with the meanings of the background predicates, not with an intensional representation that might be considered a domain theory. In fact, many popular induction algorithms (e.g. Golem <ref> [30] </ref> and Foil [38]) represent background theories extensionally as a set of ground facts. A number of recent systems have attempted to improve the performance of inductive concept learning by utilizing the analytic techniques of EBL and intensional domain theories to provide a bias for induction. <p> The standard logical definition of ILP only considers adding clauses to a theory in order to allow it to derive a set of positive examples without deriving a set of negative examples <ref> [30] </ref>. Theory refinement allows for the possibility that the existing background knowledge is incorrect, and therefore also allows existing clauses to be generalized, specialized, and deleted.
Reference: [31] <editor> S. H. Muggleton, editor. </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Inductive Logic Programming (ILP) and Explanation-Based Learning (EBL) are two subtopics of Machine Learning that have some important commonalities despite their different histories and methods. ILP <ref> [31] </ref> focuses on induction of logic programs from examples and has roots in early work in generalization in logic [35] and logic-program synthesis [44].
Reference: [32] <author> D. Ourston and R. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <address> Detroit, MI, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Another well-recognized problem with traditional EBL is the requirement that the domain theory be correct and complete [25]. A number of researchers have combined EBL with various forms of induction in order to address this issue. For example, EBL has been combined with decision-tree induction <ref> [32] </ref> and neural-network learning [49] in order to refine incomplete and/or incorrect propositional domain theories. <p> Forte's revision operators include methods from propositional theory refinement <ref> [32] </ref>, Table 3: Forte's program debugging results. Program # of Training Train % Programs Set Size Time Correct directed path 4 121 87 secs 100% insert after 9 35 82 secs 100% merge sort 10 60 199 secs 100% first order induction [38], and inverse resolution [29]. <p> These systems share features with both Forte and the propositional theory refinement system Either <ref> [32] </ref> as well as incorporating several unique features. Like Either and unlike Forte, Audrey II employs two separate phases. First the theory is specialized to remove any proofs of negatives, and next it is generalized to prove all of the positives.
Reference: [33] <author> M. Pazzani and C. Brunk. </author> <title> Finding accurate frontiers: A knowledge-intensive approach to relational learning. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 328-334, </pages> <address> Amherst, MA, </address> <year> 1993. </year>
Reference-contexts: Choices as to which of several competing rules should be used to reduce a particular literal are made using the Foil information-gain metric. In this way, the inductive and analytic components are cleanly integrated with a single unified evaluation metric. Systematic evaluations of Focl and Focl-Frontier <ref> [33] </ref>, an extension that allows the learning of definitions containing non-operational literals, have been carried out in a number of domains.
Reference: [34] <author> M. Pazzani and D. Kibler. </author> <title> The utility of background knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: Two general approaches to inte grating ILP and EBL in concept learning have been explored. One approach, knowledge-guided induction, uses the existing domain theory to guide the learning of a separate concept definition, e.g. <ref> [2, 34, 7] </ref>. The other approach, theory refinement, uses the examples to modify the existing domain theory in an attempt to improve its accuracy, e.g. [39, 52, 47]. <p> This experiment demonstrated that the use of such techniques could reduce the development time for expert systems and obtain a level of performance comparable to systems developed with classical knowledge-engineering methods. 3.1.3 Focl Focl (First Order Combined Learner) <ref> [34] </ref> is similar in spirit to ML-Smart, but employs a hill-climbing search based on Foil. As such, it is most easily understood as method for biasing Foil with background knowledge. Foil constructs a concept definition using a basic covering strategy.
Reference: [35] <author> G. D. Plotkin. </author> <title> A note on inductive generalization. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: 1 Introduction Inductive Logic Programming (ILP) and Explanation-Based Learning (EBL) are two subtopics of Machine Learning that have some important commonalities despite their different histories and methods. ILP [31] focuses on induction of logic programs from examples and has roots in early work in generalization in logic <ref> [35] </ref> and logic-program synthesis [44]. EBL [25, 10] focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning [13], natural language understanding [8], and mathematical problem solving [23]. Despite these differences, the approaches also have two important similarities.
Reference: [36] <author> A. Prieditis and J. Mostow. Prolearn: </author> <title> Towards a Prolog interpreter that learns. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> Jul </month> <year> 1987. </year>
Reference-contexts: A unifying framework that cleanly integrates these techniques can be achieved by considering the problem of control-rule learning within the context of logic programming. 4.2 Controlling Search in Logic Programs Although most EBL research in logic programming has generally focussed on learning macros, <ref> [25, 10, 36] </ref>, Recent work has shown the utility of learning explicit search-control rules within a logic programming framework [4, 55, 56]. The execution of a logic program can be viewed as a problem solving process with a search strategy based on resolution theorem proving.
Reference: [37] <author> J. R. Quinlan and R. L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: The normal approach to using MDL in inductive learning with noisy data is to attempt to induce a theory that minimizes the sum of the number of bits required to represent the theory plus the number of bits required to encode the classification of the training examples given the theory <ref> [37] </ref>. This allows one to resolve the trade-off between making a theory more complex in order to account for the remaining misclassified data or accepting the simpler theory and treating the remaining data as noise.
Reference: [38] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: However, many systems are only concerned with the meanings of the background predicates, not with an intensional representation that might be considered a domain theory. In fact, many popular induction algorithms (e.g. Golem [30] and Foil <ref> [38] </ref>) represent background theories extensionally as a set of ground facts. A number of recent systems have attempted to improve the performance of inductive concept learning by utilizing the analytic techniques of EBL and intensional domain theories to provide a bias for induction. <p> Systematic evaluations of Focl and Focl-Frontier [33], an extension that allows the learning of definitions containing non-operational literals, have been carried out in a number of domains. In the standard ILP domain of classifying illegal king-rook-king chess configurations <ref> [38] </ref>, Focl was shown capable of learning accurate definitions in the presence of a wide variety of errors that were artificially introduced in the initial domain knowledge. <p> Program # of Training Train % Programs Set Size Time Correct directed path 4 121 87 secs 100% insert after 9 35 82 secs 100% merge sort 10 60 199 secs 100% first order induction <ref> [38] </ref>, and inverse resolution [29]. First, Forte attempts to prove all positive and negative examples using the current theory. When a positive example is unprovable, some clause in the theory needs to be generalized. All clauses that were backtracked over during the attempted proof are candidates for generalization. <p> Forte's specialization operators include rule deletion and antecedent addition. Several methods are used to determine appropriate additional antecedents to add to an overly-general clause. One is a hill-climbing method based on Foil <ref> [38] </ref>. Another is called relational pathfinding [41] and adds a sequence of literals that form a relational path linking all of the arguments of the goal predicate. Since it adds multiple literals at once, relational pathfinding helps overcome local minima problems in Foil. <p> Latex uses a very simple hill-climbing revision algorithm that tries all possible single-literal additions and deletions anywhere in the theory and picks the one that minimizes the description length metric given above. Experiments in the standard ILP domain of determining illegal king-rook-king chess configurations <ref> [38] </ref> demonstrated that this approach could effectively revise theories even in the presence of data with 10% classification noise. 4 ILP/EBL in Control Learning As previously mentioned, EBL research is rooted in the use of learning to improve the performance of problem-solving systems. <p> Dolphin improves on AxA-EBL in two significant ways. First, like Grasshopper, it employs Foil, a powerful ILP induction algorithm <ref> [38] </ref>. Second, Dolphin explicitly considers the surrounding proof context during control-rule induction. Dolphin begins by applying EBL techniques to the entire proof-tree of each training example to produce a set of generalized proofs.
Reference: [39] <author> B. Richards and R. Mooney. </author> <title> First-order theory revision. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 447-451, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: One approach, knowledge-guided induction, uses the existing domain theory to guide the learning of a separate concept definition, e.g. [2, 34, 7]. The other approach, theory refinement, uses the examples to modify the existing domain theory in an attempt to improve its accuracy, e.g. <ref> [39, 52, 47] </ref>. ILP/EBL research in control learning specifically focuses on learning search-control knowledge that improves the performance of an existing logic program or problem solver. Sample problems are used to generate a separate set of control examples describing appropriate and inappropriate contexts for applying operators in the domain theory. <p> Researchers familiar with EBL have generally approached the problem of theory refinement by using failed attempts to explain positive examples and incorrect explanations of negative examples to suggest potential revisions to the domain theory. This section reviews several theory refinement systems that take this approach. 3.2.1 Forte Forte <ref> [39, 40] </ref> performs a hill-climbing search through a space of specializing and generalizing operators in an attempt to find a minimal revision to a theory that makes it consistent with the training examples. Forte's revision operators include methods from propositional theory refinement [32], Table 3: Forte's program debugging results.
Reference: [40] <author> B. L. Richards and R. J. Mooney. </author> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: Researchers familiar with EBL have generally approached the problem of theory refinement by using failed attempts to explain positive examples and incorrect explanations of negative examples to suggest potential revisions to the domain theory. This section reviews several theory refinement systems that take this approach. 3.2.1 Forte Forte <ref> [39, 40] </ref> performs a hill-climbing search through a space of specializing and generalizing operators in an attempt to find a minimal revision to a theory that makes it consistent with the training examples. Forte's revision operators include methods from propositional theory refinement [32], Table 3: Forte's program debugging results.
Reference: [41] <author> B.L. Richards and R. J. Mooney. </author> <title> Learning relations by pathfinding. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Forte's specialization operators include rule deletion and antecedent addition. Several methods are used to determine appropriate additional antecedents to add to an overly-general clause. One is a hill-climbing method based on Foil [38]. Another is called relational pathfinding <ref> [41] </ref> and adds a sequence of literals that form a relational path linking all of the arguments of the goal predicate. Since it adds multiple literals at once, relational pathfinding helps overcome local minima problems in Foil. Forte's generalization operators include deleting antecedents and adding rules.
Reference: [42] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Auto-matica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Latex is a quite different approach to theory refinement that employs a unique application of the minimum description length (MDL) principle <ref> [42] </ref> to effectively learn from an approximate theory and noisy data.
Reference: [43] <author> P. Rosenbloom and J. Laird. </author> <title> Mapping explanation-based generalization onto Soar. </title> <booktitle> In Proceedings of National Conference on Artificial Intelligence, </booktitle> <pages> pages 561-567, </pages> <address> Philadelphia, PA, </address> <month> Aug </month> <year> 1986. </year>
Reference-contexts: Explanation-based generalization is closely related to partial evaluation as applied to logic programs and can be viewed as a form of example-guided partial evaluation [50]. It is also closely related to chunking as applied to production rules in systems like Soar <ref> [43] </ref>. Adding macro rules can result in dramatic improvements in efficiency although on many tasks the overhead of matching many specific rules can eventually degrade overall performance [21].
Reference: [44] <author> E.Y. Shapiro. </author> <title> Algorithmic Program Debugging. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: ILP [31] focuses on induction of logic programs from examples and has roots in early work in generalization in logic [35] and logic-program synthesis <ref> [44] </ref>. EBL [25, 10] focuses on improving the efficiency of a problem solver using deductive methods and has roots in early work in learning in planning [13], natural language understanding [8], and mathematical problem solving [23]. Despite these differences, the approaches also have two important similarities. <p> This paper presents a review of recent research that integrates ILP and EBL methods. We intentionally focus on ILP work that is clearly influenced by ideas from EBL and do not attempt to review other ILP research in the areas of knowledge-based induction and theory refinement, e.g. <ref> [11, 44] </ref>. The remainder of the paper is organized as follows. Section 2 presents a brief review of traditional EBL for the uninitiated reader. Section 3 reviews ILP/EBL research in concept learning, including work in knowledge-guided induction and theory refinement.
Reference: [45] <author> B. Silver. </author> <title> Precondition analysis: Learning control information. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An AI Approach, </booktitle> <volume> Vol. II, </volume> <pages> pages 647-670. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: Second, was the desire to employ learning to improve problem solving as opposed to the traditional focus on classification. Historically, EBL methods were developed to improve planning [13], mathematical problem solving <ref> [23, 45] </ref>, and story understanding [8]. The variety of related EBL techniques developed in the early 1980's eventually lead to an effort to unify the various approaches [25, 10]. The resulting definition of the general problem addressed by EBL is shown in Table 1 (from [25]).
Reference: [46] <editor> P. Tadepalli, editor. </editor> <booktitle> Proceedings of the ML92 Workshop on Knowledge Compilation and Speedup Learning, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: A combination of ILP and EBL methods are then used to learn control rules for deciding when to apply the existing operators, e.g. [4, 55]. Control learning is traditionally used to improve the efficiency of a problem solver as a form of speedup learning <ref> [46] </ref>; however, it can also be used to improve accuracy [56]. This paper presents a review of recent research that integrates ILP and EBL methods.
Reference: [47] <author> S. Tangkitvanich and M. Shimura. </author> <title> Refining a relational theory with multiple faults in the concept and subconcepts. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 436-444, </pages> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: One approach, knowledge-guided induction, uses the existing domain theory to guide the learning of a separate concept definition, e.g. [2, 34, 7]. The other approach, theory refinement, uses the examples to modify the existing domain theory in an attempt to improve its accuracy, e.g. <ref> [39, 52, 47] </ref>. ILP/EBL research in control learning specifically focuses on learning search-control knowledge that improves the performance of an existing logic program or problem solver. Sample problems are used to generate a separate set of control examples describing appropriate and inappropriate contexts for applying operators in the domain theory. <p> Since theory refinement attempts to preserve the structure and content of the domain theory as much as possible, it can have an advantage over knowledge-guided induction as illustrated in this domain. 3.2.3 Rx and Latex Rx <ref> [47] </ref> and Latex [48] are two recent first-order theory refinement systems developed at the Tokyo Institute of Technology. Rx initially learns an operational concept definition in a manner very similar to Focl and then "unoperational izes" this definition back into a revised theory.
Reference: [48] <author> S. Tangkitvanich and M. Shimura. </author> <title> Learning from an approximate theory and noisy examples. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 466-471, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Since theory refinement attempts to preserve the structure and content of the domain theory as much as possible, it can have an advantage over knowledge-guided induction as illustrated in this domain. 3.2.3 Rx and Latex Rx [47] and Latex <ref> [48] </ref> are two recent first-order theory refinement systems developed at the Tokyo Institute of Technology. Rx initially learns an operational concept definition in a manner very similar to Focl and then "unoperational izes" this definition back into a revised theory.
Reference: [49] <author> G. G. Towell, J. W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, MA, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: Another well-recognized problem with traditional EBL is the requirement that the domain theory be correct and complete [25]. A number of researchers have combined EBL with various forms of induction in order to address this issue. For example, EBL has been combined with decision-tree induction [32] and neural-network learning <ref> [49] </ref> in order to refine incomplete and/or incorrect propositional domain theories.
Reference: [50] <author> F. van Harmelen and A. Bundy. </author> <title> Explanation-based general-isation = partial evaluation. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 401-412, </pages> <year> 1988. </year>
Reference-contexts: Therefore, instead of performing complex inferencing, direct pattern matching can be used to classify future examples as cups. Explanation-based generalization is closely related to partial evaluation as applied to logic programs and can be viewed as a form of example-guided partial evaluation <ref> [50] </ref>. It is also closely related to chunking as applied to production rules in systems like Soar [43]. Adding macro rules can result in dramatic improvements in efficiency although on many tasks the overhead of matching many specific rules can eventually degrade overall performance [21].
Reference: [51] <author> P. H. Winston, T. O. Binford, B. Katz, and M. Lowry. </author> <title> Learning physical descriptions from functional definitions, examples, and precedents. </title> <booktitle> In Proceedings of the Third National Conference on Artificial Intelligence, </booktitle> <pages> pages 433-439, </pages> <address> Wash-ington, D.C., </address> <month> Aug </month> <year> 1983. </year>
Reference-contexts: Table 2 shows one of the standard examples of this task, learning a structural definition of a cup from a functional definition, a single example, and a domain theory relating form to function <ref> [51, 25] </ref>. The basic method for solving the explanation-based generalization problem consists of the fol Table 1: The Explanation-Based Generalization Problem.
Reference: [52] <author> J. Wogulis. </author> <title> Revising relational domain theories. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 462-466, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: One approach, knowledge-guided induction, uses the existing domain theory to guide the learning of a separate concept definition, e.g. [2, 34, 7]. The other approach, theory refinement, uses the examples to modify the existing domain theory in an attempt to improve its accuracy, e.g. <ref> [39, 52, 47] </ref>. ILP/EBL research in control learning specifically focuses on learning search-control knowledge that improves the performance of an existing logic program or problem solver. Sample problems are used to generate a separate set of control examples describing appropriate and inappropriate contexts for applying operators in the domain theory. <p> Forte has also been used to debug a version of the decision-tree induction program from Bratko's Prolog text [3], and to revise a qualitative model of a portion of the Reaction Control System of the NASA Space Shuttle. 3.2.2 Audrey and A3 Audrey <ref> [52] </ref>, Audrey II [54], and A3 [53] are a series of first-order theory refinement systems that also integrate ideas from ILP and EBL. These systems share features with both Forte and the propositional theory refinement system Either [32] as well as incorporating several unique features.
Reference: [53] <author> J. Wogulis and M. Pazzani. </author> <title> Handling negation in first-order theory revision. </title> <booktitle> In Proceedings of the IJCAI-93 Workshop on Inductive Logic Programming, </booktitle> <pages> pages 36-46, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Forte has also been used to debug a version of the decision-tree induction program from Bratko's Prolog text [3], and to revise a qualitative model of a portion of the Reaction Control System of the NASA Space Shuttle. 3.2.2 Audrey and A3 Audrey [52], Audrey II [54], and A3 <ref> [53] </ref> are a series of first-order theory refinement systems that also integrate ideas from ILP and EBL. These systems share features with both Forte and the propositional theory refinement system Either [32] as well as incorporating several unique features. Like Either and unlike Forte, Audrey II employs two separate phases.
Reference: [54] <author> J. Wogulis and M. Pazzani. </author> <title> A methodology for evaluating theory revision systems: Results with Audrey II. </title> <booktitle> In Proceedings of the Thirteenth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 1128-1134, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: A notion of the syntactic distance between two theories can be defined based on the number of primitive edit operations required to transform one theory into the other <ref> [27, 54] </ref>. Unfortunately, it is computationally intractable to minimize such measures when revising a theory. Therefore, implemented systems must resort to some form of heuristic search in an attempt to minimize syntactic change to the theory. <p> Forte has also been used to debug a version of the decision-tree induction program from Bratko's Prolog text [3], and to revise a qualitative model of a portion of the Reaction Control System of the NASA Space Shuttle. 3.2.2 Audrey and A3 Audrey [52], Audrey II <ref> [54] </ref>, and A3 [53] are a series of first-order theory refinement systems that also integrate ideas from ILP and EBL. These systems share features with both Forte and the propositional theory refinement system Either [32] as well as incorporating several unique features.
Reference: [55] <author> J. M. Zelle and R. J. Mooney. </author> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 1106-1111, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Sample problems are used to generate a separate set of control examples describing appropriate and inappropriate contexts for applying operators in the domain theory. A combination of ILP and EBL methods are then used to learn control rules for deciding when to apply the existing operators, e.g. <ref> [4, 55] </ref>. Control learning is traditionally used to improve the efficiency of a problem solver as a form of speedup learning [46]; however, it can also be used to improve accuracy [56]. This paper presents a review of recent research that integrates ILP and EBL methods. <p> considering the problem of control-rule learning within the context of logic programming. 4.2 Controlling Search in Logic Programs Although most EBL research in logic programming has generally focussed on learning macros, [25, 10, 36], Recent work has shown the utility of learning explicit search-control rules within a logic programming framework <ref> [4, 55, 56] </ref>. The execution of a logic program can be viewed as a problem solving process with a search strategy based on resolution theorem proving. A program executes by finding a constructive proof of a partially instantiated goal given as input. <p> AxA-EBL is also limited by efficiency considerations; the number of candidate rules is exponential in k, and hence only relatively simple approximations may be considered. 4.5 Dolphin Dolphin, (Dynamic Optimization of Logic Programs through Heuristics INduction) <ref> [55] </ref> is an integration of ILP and EBL that extends the AxA-EBL approach. Dolphin improves on AxA-EBL in two significant ways. First, like Grasshopper, it employs Foil, a powerful ILP induction algorithm [38]. Second, Dolphin explicitly considers the surrounding proof context during control-rule induction.
Reference: [56] <author> J. M. Zelle and R. J. Mooney. </author> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 817-822, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Control learning is traditionally used to improve the efficiency of a problem solver as a form of speedup learning [46]; however, it can also be used to improve accuracy <ref> [56] </ref>. This paper presents a review of recent research that integrates ILP and EBL methods. We intentionally focus on ILP work that is clearly influenced by ideas from EBL and do not attempt to review other ILP research in the areas of knowledge-based induction and theory refinement, e.g. [11, 44]. <p> considering the problem of control-rule learning within the context of logic programming. 4.2 Controlling Search in Logic Programs Although most EBL research in logic programming has generally focussed on learning macros, [25, 10, 36], Recent work has shown the utility of learning explicit search-control rules within a logic programming framework <ref> [4, 55, 56] </ref>. The execution of a logic program can be viewed as a problem solving process with a search strategy based on resolution theorem proving. A program executes by finding a constructive proof of a partially instantiated goal given as input.
References-found: 56

