URL: http://theory.lcs.mit.edu/~karger/Papers/web.ps.gz
Refering-URL: http://www.cs.washington.edu/homes/yasushi/porc-internal/LMM.html
Root-URL: 
Title: Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World
Author: David Karger Eric Lehman Tom Leighton ; Matthew Levine Daniel Lewin Rina Panigrahy 
Abstract: We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anawat Chankhunthod, Peter Danzig, Chuck Neerdaels, Michael Schwartz and Kurt Worrell. </author> <title> A Hierarchical Internet Object Cache. </title> <booktitle> In USENIX Proceedings, </booktitle> <year> 1996. </year>
Reference-contexts: A tool that we develop in this paper, consistent hashing, gives a way to implement such a distributed cache without requiring that the caches communicate all the time. We discuss this in Section 4. Chankhunthod et al. <ref> [1] </ref> developed the Harvest Cache, a more scalable approach using a tree of caches. A user obtains a page by asking a nearby leaf cache. If neither this cache nor its siblings have the page, the request is forwarded to the cache's parent. <p> In order to economize on the space to represent a function in the family, and on the use of random bits, we only demand that the functions r B and r I map points log (C)- way independently and uniformly to <ref> [0; 1] </ref>. Note that for each point we pick in the unit interval, we need only pick enough random bits to distinguish the point from all other points. Thus it is unlikely that we need more than log (number of points) bits for each point.
Reference: [2] <author> Robert Devine. </author> <title> Design and Implementation of DDH: A Distributed Dynamic Hashing Algorithm. </title> <booktitle> In Proceedings of 4th International Conference on Foundations of Data Organizations and Algorithms, </booktitle> <year> 1993. </year>
Reference-contexts: In [5] Litwin et al proposes a hash function that allows buckets to be added one at a time sequentially. However our hash function allows the buckets to be added in an arbitrary order. Another scheme that we can improve on is given by Devine <ref> [2] </ref>.
Reference: [3] <author> M. J. Feeley, W. E. Morgan, F. P. Pighin, A. R. Karlin, H. M. Levy and C. A. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <year> 1995. </year>
Reference-contexts: This makes standard hashing useless since it relies on clients agreeing on which caches are responsible for serving a particular page. For example, Feeley et al <ref> [3] </ref> implement a distributed global shared memory system for a network of workstations that uses a hash table distributed among the machines to resolve references. Each time a new machine joins the network, the require a central server to redistribute a completely updated hash table to all the machines.
Reference: [4] <author> Sally Floyd, Van Jacobson, Steen McCanne, Ching-Gung Liu and Lixia Zhang. </author> <title> A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing, </title> <type> SIGCOMM' 95 </type>
Reference-contexts: In particular, consistent hashing may be a useful tool for distributing information from name servers such as DNS and label servers such as PICS in a load-balanced and fault-tolerant fashion. Our two schemes may together provide an interesting method for constructing multicast trees <ref> [4] </ref>. Another important way in which our ideas could be extended is in handling pages whose information changes over time, due to either server or client activity. <p> If we augment our protocol to let the server know which machines are currently caching its page, then the server can notify such caches whenever the data on its pages changes. This might work particularly well in conjunction with the currently under development multicast protocols <ref> [4] </ref> that broadcast information from a server to all the client members of a multicast group. Our protocol can be mapped into this model if we assume that every machine caching a page joins a multicast group for that page.
Reference: [5] <author> Witold Litwin, Marie-Anne Neimat and Donovan A. Schneider. </author> <title> LH fl -A Scalable, Distributed Data Structure. </title> <journal> ACM Transactions on Database Systems, </journal> <month> Dec. </month> <year> 1996 </year>
Reference-contexts: We apply consistent hashing to our tree-of-caches scheme, and show how this makes the scheme work well even if each client is aware of only a constant fraction of all the caching machines. In <ref> [5] </ref> Litwin et al proposes a hash function that allows buckets to be added one at a time sequentially. However our hash function allows the buckets to be added in an arbitrary order. Another scheme that we can improve on is given by Devine [2].
Reference: [6] <author> Radhika Malpani, Jacob Lorch and David Berger. </author> <title> Making World Wide Web Caching Servers Cooperate. </title> <booktitle> In Proceedings of World Wide Web Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The dilemma in this scheme is that there is more benefit if more users share the same cache, but then the cache itself is liable to get swamped. Malpani et al. <ref> [6] </ref> work around this problem by making a group of caches function as one. A user's request for a page is directed to an arbitrary cache. If the page is stored there, it is returned to the user.
Reference: [7] <author> M. Naor and A. Wool. </author> <title> The load, capacity, and availability of quorum systems. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 214-225, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: However our hash function allows the buckets to be added in an arbitrary order. Another scheme that we can improve on is given by Devine [2]. In addition, we believe that consistent hashing will be useful in other applications (such as quorum systems <ref> [7] </ref> [8] or distributed name servers) where multiple machines with different views of the network must agree on a common storage location for an object without communication. 1.3 Presentation In Section 2 we describe our model of the Web and the hot spot problem.
Reference: [8] <author> D. Peleg and A. Wool. </author> <title> The availability of quorum systems. </title> <booktitle> Information and Computation 123(2) </booktitle> <pages> 210-233, </pages> <year> 1995. </year>
Reference-contexts: However our hash function allows the buckets to be added in an arbitrary order. Another scheme that we can improve on is given by Devine [2]. In addition, we believe that consistent hashing will be useful in other applications (such as quorum systems [7] <ref> [8] </ref> or distributed name servers) where multiple machines with different views of the network must agree on a common storage location for an object without communication. 1.3 Presentation In Section 2 we describe our model of the Web and the hot spot problem.
Reference: [9] <author> Greg Plaxton and Rajmohan Rajaraman. </author> <title> Fast Fault-Tolerant Concurrent Access to Shared Objects. </title> <booktitle> In Proceedings of 37th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1996. </year>
Reference-contexts: This can swamp the root if the number of distinct page requests grows too large, meaning that this scheme also suffers from potential scaling problems. Plaxton and Rajaraman <ref> [9] </ref> show how to balance the load among all caches by using randomization and hashing.
Reference: [10] <author> M. O. Rabin. </author> <title> Efficient dispersal of Information for Security, Load Balancing, and Fault Tolerance. </title> <journal> Journal of the ACM 36:335348, </journal> <year> 1989. </year>
Reference-contexts: Under our protocol, no preemptive caching of pages is done. Thus, if a server goes down, all pages that it has not distributed become inaccessible to any algorithm. This problem can be eliminated using standard techniques, such as Rabin's Information Dispersal Algorithm <ref> [10] </ref>. So we ignore server faults. Observe that a request is satisfied if and only if all the caches serving for the nodes of the tree path are not down.
Reference: [11] <author> Jeanette Schmidt, Alan Siegel and Aravind Srinivasan. </author> <title> Chernoff-Hoeffding Bounds for Applications with Limited Independence. </title> <booktitle> In Proc. 4th ACS-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1993. </year>
Reference-contexts: In practice, hash functions with limited independence are more plausible since they economize on space and randomness. We have proven all theorems of this paper with only limited independence using methods similar to those in <ref> [11] </ref>. However, in this extended abstract we only state the degree of independence required for results to hold. Proofs assuming limited independence will appear in the full version of this paper. 2 Model This section presents our model of the Web and the hotspot problem. <p> To bound the projected weight, we first give a bound for the case where each node is assigned to a random machine. This is a weighted version of the familiar balls-in-bins type of analysis. Our analysis gives a bound with an exponential tail. We can therefore argue as in <ref> [11] </ref> that it applies even when the balls are assigned to bins only k = O (log N )- way independently. This can be achieved by using a k-universal hash function to map the abstract tree nodes to machines. We will now analyze our protocol under the simplified model. <p> Proof: The full proof is deferred to the final version of the paper. This result does not follow immediately from the results of <ref> [11] </ref>, but involves a similar argument. Setting k = log N we get the following corollary. Corollary 3.5 The high probability bound proved in theorem 3.1 for the number of requests a cache gets holds even if h is selected from a log N -universal hash family.
References-found: 11

