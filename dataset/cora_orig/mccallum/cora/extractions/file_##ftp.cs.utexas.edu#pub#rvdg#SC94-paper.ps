URL: file://ftp.cs.utexas.edu/pub/rvdg/SC94-paper.ps
Refering-URL: http://www.cs.utexas.edu/users/rvdg/conference.html
Root-URL: http://www.cs.utexas.edu
Title: Building a High-Performance Collective Communication Library  
Author: Mike Barnett Satya Gupta and David G. Payne Lance Shuler Robert van de Geijn Jerrell Watts 
Address: Moscow, Idaho 83844-1010 15201 N.W. Greenbrier Pkwy Beaverton, Oregon 97006  Albuquerque, New Mexico 87185-1109 Austin, Texas 78712-1188  Pasadena, California 91125  
Affiliation: Department of Computer Science Supercomputer Systems Division University of Idaho Intel Corporation  Parallel Computing Sciences Department, 1424 Department of Computer Sciences Sandia National Laboratory The University of Texas at Austin  Computer Science Department California Institute of Technology  
Abstract: In this paper, we report on a project to develop a unified approach for building a library of collective communication operations that performs well on a cross-section of problems encountered in real applications. The target architecture is a two-dimensional mesh with worm-hole routing, but the techniques are more general. The approach differs from traditional library implementations in that we address the need for implementations that perform well for various sized vectors and grid dimensions, including non-power-of-two grids. We show how a general approach to hybrid algorithms yields performance across the entire range of vector lengths. Moreover, many scalable implementations of application libraries require collective communication within groups of nodes. Our approach yields the same kind of performance for group collective communication. Results from the Intel Paragon system are included. To obtain this library for Intel systems contact intercom@cs.utexas.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Barnett, S. Gupta, D. Payne, L. Shuler, R. van de Geijn and J. Watts. </author> <title> Interprocessor Collective Communication Library (InterCom). </title> <booktitle> Proceedings of Scalable High Performance Computing Conference, pg. </booktitle> <pages> 357-364, </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Knoxville, TN, </address> <month> May 23-24, </month> <year> 1994. </year>
Reference-contexts: For a general purpose library, it is crucial that an implementation performs well for all vector lengths. In an earlier paper <ref> [1] </ref>, we have presented a general approach to building collective communication libraries.
Reference: [2] <author> M. Barnett, R. Littlefield, D.G. Payne and R. van de Geijn. </author> <title> Efficient Communication Primitives on Mesh Architectures with Hardware Routing. </title> <booktitle> Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Norfolk, VA, </address> <month> Mar. </month> <pages> 22-24, </pages> <year> 1993. </year>
Reference: [3] <author> M. Barnett, R. Littlefield, D.G. Payne and R. van de Geijn. </author> <title> Global Combine on Mesh Architectures with Wormhole Routing. </title> <booktitle> 7th International Parallel Processing Symposium, </booktitle> <pages> pages 156-162, </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Newport Beach, CA, </address> <month> Apr. </month> <pages> 13-16, </pages> <year> 1993. </year>
Reference-contexts: On meshes, the use of long vector primitives can be enhanced by alternating directions within the mesh <ref> [3] </ref>. Also, the model for communication is considerably more complex: details of how messages are sent greatly affects the parameters in the model, ff and fi. Furthermore, there is an excess of bandwidth on each link of the network compared to the bandwidth from a node to the network.
Reference: [4] <author> M. Barnett, D. Payne and R. van de Geijn. </author> <title> Optimal broadcasting in mesh-connected architectures. </title> <institution> University of Texas Department of Computer Science TR-91-38, </institution> <month> Dec. </month> <year> 1991. </year>
Reference: [5] <author> M. Barnett, D.G. Payne, R. van de Geijn and J. Watts. </author> <title> Broadcasting on Meshes with WormHole Routing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> submitted. </note> <institution> (Currently University of Texas Department of Computer Sciences TR-93-24.) </institution>
Reference-contexts: On hypercubes, this can be easily accomplished by staging the algorithms as log p steps during which communication is performed in each hypercube dimension. For meshes, this idea can be utilized as well, provided some care is taken at each stage <ref> [5] </ref>. All our target short vector collective communication operations can be built from four primitives. These are broadcast, combine-to-one, scatter, and gather. Consider the broadcast.
Reference: [6] <author> J.-C. Bermond, P. Michallon and D. Trystram. </author> <title> Broadcasting in Wraparound Meshes with Parallel Monodirectional Links. </title> <journal> Parallel Computing, </journal> <volume> 18(6) </volume> <pages> 639-648, </pages> <month> June </month> <year> 1992. </year>
Reference: [7] <author> C.-T. Ho and S. L. Johnsson. </author> <title> Distributed Rout--ing Algorithms for Broadcasting and Personalized Communication in Hypercubes. </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, pg. </booktitle> <pages> 640-648, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference-contexts: For example, on hypercubes Ho and Johnsson's EDST broadcast <ref> [7] </ref> will outperform our scatter/collect broadcast by a factor of two for long vectors. However, it is our experience that such pipelined algorithms are generally difficult to implement and are extremely architecture dependent.
Reference: [8] <author> S. L. Lillevik. </author> <booktitle> The Touchstone 30 Gigaflop Delta Prototype Sixth Distributed Memory Computing Conference Proceedings, pg. </booktitle> <pages> 671-677, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference: [9] <author> R. Littlefield. </author> <title> Characterizing and Tuning Communications Perfomance on the Touchstone Delta and iPSC/860. </title> <booktitle> Proceedings of the 1992 Intel User's Group Meeting, </booktitle> <address> Dallas, Texas, </address> <month> Oct. </month> <pages> 4-7, </pages> <year> 1992. </year>
Reference: [10] <author> L. M. Ni and P. K McKinley. </author> <title> A Survey of Wormhole Routing Techniques in Direct Networks. </title> <journal> IEEE Computer, </journal> <volume> 26(2) </volume> <pages> 62-76, </pages> <month> Feb. </month> <year> 1993. </year>
Reference: [11] <author> Y. Saad and M. H. Schultz. </author> <title> Data Communication in Parallel Architectures. </title> <journal> Parallel Computing, </journal> <volume> 11(2) </volume> <pages> 131-150, </pages> <month> Aug. </month> <year> 1989. </year>
Reference: [12] <author> S. R. Seidel. </author> <title> Broadcasting on Linear Arrays and Meshes. </title> <institution> Oak Ridge National Laboratory Technical Report ORNL/TM-12356, </institution> <month> Mar. </month> <year> 1993. </year>
Reference: [13] <author> M. Simmen. </author> <title> Comments on Broadcast Algorithms for Two-Dimensional Grids Parallel Computing, </title> <booktitle> 17(1) </booktitle> <pages> 109-112, </pages> <month> Apr. </month> <year> 1991. </year>
Reference: [14] <author> R. A. van de Geijn. </author> <title> Efficient Global Combine Operations. </title> <booktitle> Sixth Distributed Memory Computing Conference Proceedings, pg. </booktitle> <pages> 291-294, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference: [15] <author> R. van de Geijn and J. Watts. </author> <title> A Pipelined Broadcast for Multidimensional Meshes. </title> <note> Parallel Processing Letters, to appear. </note>
Reference: [16] <author> D. W. Walker. </author> <title> The Design of a Standard Message Passing Interface for Distributed Memory Concurrent Computers. </title> <booktitle> Parallel Computing, </booktitle> <month> Apr. </month> <year> 1994. </year> <title> (Up to date information about the MPI standard is available from netlib, directory mpi.) </title>
Reference-contexts: 1 Introduction The Interprocessor Collective Communication (InterCom) Project is a comprehensive study of techniques for a high-performance implementation of com monly used collective communication algorithms. It is this emphasis on a high-performance implementation that sets it aside from the MPI effort <ref> [16] </ref>, which tries to standardize the interface to communication libraries. Indeed, we expect the fruits of our efforts to be incorporated into implementations of the MPI standard. The following collective communication operations have been identified as being useful in many applications: broadcast, scatter, gather, collect and global combine.
References-found: 16

