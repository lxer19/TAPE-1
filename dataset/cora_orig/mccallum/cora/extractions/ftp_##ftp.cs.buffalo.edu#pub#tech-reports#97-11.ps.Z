URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/97-11.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Aarts and J. Kost. </author> <title> Simulated Annealing and Bolzmann Machine: A Stochastic Approach to Combinatorial Optimization and Neural Computing. </title> <publisher> John Wiley & Sons Ltd., </publisher> <address> Chichester UK, </address> <year> 1989. </year>
Reference-contexts: The algorithm starts from an initial configuration with a relatively large value of T . For a given value of T a number of transitions are considered and then T is lowered until it reaches zero. General information about simulated annealing and it various applications can be found in <ref> [97, 1] </ref>. Forsyth and Safavi-Naini [48, 49] proposed an elegant formulation of decoding a substitution cipher as a combinatorial optimization problem and suggested using a simulated annealing algorithm to find the optimal solution that corresponds to the breaking of the cipher. <p> SIMULATED ANNEALING ALGORITHM193 Simulated Annealing () BEGIN initialize (F start ; T start ); for (k = 1; k &lt; max iterations;k + +) for (l = 0; l &lt; max chain length; l + +) BEGIN generate A j from N i ; if (F ij 0 or random <ref> [0; 1] </ref> exp (F ij =T k1 )) then update (A i ); END if (stop criterion==true) then exit (); END 194 CHAPTER 7. SUMMARY AND CONCLUSIONS
Reference: [2] <author> M.R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1973. </year>
Reference-contexts: The existence of visually similar characters with different identities makes it extremely difficult to achieve good clustering results. Image degradations that cause character pattern deformations aggravate this problem. There have been many research efforts that have tried to improve clustering performance by optimizing a chosen mathematical objective function <ref> [80, 2, 38] </ref>. Nevertheless, achieving accurate clustering results especially on degraded images is still a challenging issue. Two problems need to be solved. First of all, a clustering algorithm for character patterns needs to be developed so that good clustering performance can be obtained in the presence of image degradation.
Reference: [3] <author> S. Ariyoshi. </author> <title> A character segmentation method for japanese printed documents coping with touching character problems. </title> <booktitle> In 11th IAPR Int. Conf. on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> pages 313-316, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [4] <author> L. Bahl. </author> <title> A algorithm for solving simple substitution cryptograms. </title> <booktitle> In Proc. IEEE Int. Symp. Information Theory, </booktitle> <address> Ithaca, NY, </address> <year> 1977. </year>
Reference-contexts: A segmentation-free classifier for machine-printed numeric fields that used a neural network approach can be found in [54] 46 CHAPTER 2. BACKGROUND 2.3 Character Recognition as Substitution Cipher Decoding Efforts to design autonomous reading machines motivated the early application of substitution cipher decoding to OCR <ref> [20, 21, 4] </ref>. The basic idea with this approach is to make use of language statistics and assign alphabetic labels to visual character patterns so that the visual character repetition pattern in the input text passage best matches the letter repetition pattern provided by a language model.
Reference: [5] <author> Henry S. Baird. </author> <title> Document image defect models and their uses. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <year> 1993. </year>
Reference-contexts: Image degradation in documents makes effective font training even more difficult. The need for large amounts of font training is a significant impediment to the development of high performance commercial recognition systems <ref> [84, 7, 5, 8, 174] </ref>. 4 CHAPTER 1. INTRODUCTION 1.1. THE MOTIVATION 5 6 CHAPTER 1. INTRODUCTION There have been extensive research efforts over the past three decades in seeking solutions to these two problems.
Reference: [6] <author> H.S. Baird. </author> <title> Accurate skew estimation and the top-down analysis of document images. </title> <booktitle> In Proceedings of the 1st Int. Conf. on Computer Vision, </booktitle> <year> 1987. </year>
Reference-contexts: Columns where the projection fell below a predefined threshold were considered candidates for segmentation points. In the case of touching characters or text lines where neighboring characters overlap horizontally, the projection often contains a minimum at the proper segmentation points. Baird <ref> [6] </ref> proposed a method in which the ratio of the second derivative of the projection to its height was used as a criterion for choosing segmentation points. This criterion avoids considering points along a thin horizontal line as potential segmentation points. <p> Moreover, the upper and lower profiles of the patterns were also used in combination with the projection to get improved segmentation point candidates <ref> [6] </ref>. Applications of projection analysis for character segmentation on italic fonts or on text that involves slanted characters can be found in [50], where the projections were implemented at two-degree increments between 16 and 16 degrees from the vertical.
Reference: [7] <author> H.S. Baird. </author> <title> Document image defect models. </title> <editor> In H. S. Baird, H. Bunke, and K. Yamamoto, editors, </editor> <booktitle> Structural Document Image Analysis, </booktitle> <pages> pages 546-556. </pages> <publisher> Springer Verlag, </publisher> <address> N.Y., </address> <year> 1992. </year> <note> 195 196 BIBLIOGRAPHY </note>
Reference-contexts: Image degradation in documents makes effective font training even more difficult. The need for large amounts of font training is a significant impediment to the development of high performance commercial recognition systems <ref> [84, 7, 5, 8, 174] </ref>. 4 CHAPTER 1. INTRODUCTION 1.1. THE MOTIVATION 5 6 CHAPTER 1. INTRODUCTION There have been extensive research efforts over the past three decades in seeking solutions to these two problems.
Reference: [8] <author> H.S. Baird and G. Nagy. </author> <title> A self-correcting 100-font classifier. </title> <booktitle> In Proceedings of the Conference on Document Recognition of 1994 S&T/SPIE Symposium, </booktitle> <month> February 6-10 </month> <year> 1994. </year>
Reference-contexts: Image degradation in documents makes effective font training even more difficult. The need for large amounts of font training is a significant impediment to the development of high performance commercial recognition systems <ref> [84, 7, 5, 8, 174] </ref>. 4 CHAPTER 1. INTRODUCTION 1.1. THE MOTIVATION 5 6 CHAPTER 1. INTRODUCTION There have been extensive research efforts over the past three decades in seeking solutions to these two problems.
Reference: [9] <author> D.H. Ballard and C.M. Brown. </author> <title> Computer Vision. </title> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference-contexts: They often reflect other problems familiar to researchers in a few related branches of science. For decades figure-ground segregation (or image segmentation) has been an elusive problem that perplexed researchers in the fields of computer vision, cognitive science and psychology <ref> [107, 9, 145, 85, 35] </ref>. This difficulty in separating what is meaningful to recognition from what is irrelevant background noise lies at the core of the computational model that forms the base of today's computer vision systems [109]. <p> INTRODUCTION greatly increased the performance of document recognition systems when applied to degraded documents. The improved understanding of the importance of linguistic processing in document recognition has also paralleled the recognition of the importance of using contextual information in many other aspects of computer vision and image understanding <ref> [11, 9, 158, 159] </ref>. Research in using contextual information in pattern recognition had resulted in new classification frameworks that incorporated different levels of contextual constraints inside the discrimination function for high classification accuracy and more meaningful interpretation of results [52, 164].
Reference: [10] <author> H.B. Barlow. </author> <title> The mechanical mind. </title> <journal> Annual Review of Neuroscience, </journal> <volume> 13 </volume> <pages> 15-24, </pages> <year> 1990. </year>
Reference-contexts: We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision [166, 165, 131, 169], as well as on neural physiological studies of features detection and representation in primate visual cortex <ref> [72, 71, 10, 18, 37] </ref>. This hierarchical feature representation is computed from the skeleton representation of a character. It includes local features such as isolated spots, line segments, curve segments, line (or curve) terminators and line crossings, as well as global features such as holes.
Reference: [11] <editor> J. Beck, B. Hope, and A. Rosenfeld, editors. </editor> <booktitle> Human and Machine Vision. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: INTRODUCTION greatly increased the performance of document recognition systems when applied to degraded documents. The improved understanding of the importance of linguistic processing in document recognition has also paralleled the recognition of the importance of using contextual information in many other aspects of computer vision and image understanding <ref> [11, 9, 158, 159] </ref>. Research in using contextual information in pattern recognition had resulted in new classification frameworks that incorporated different levels of contextual constraints inside the discrimination function for high classification accuracy and more meaningful interpretation of results [52, 164].
Reference: [12] <author> S. Bercu and G. Lorette. </author> <title> On-line handwritten word recognition: an approach based on hidden markov models. </title> <booktitle> In Proceedings IWFHR III, </booktitle> <pages> page 385, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An HMM can be trained to learn the letter segmentation behavior because contextual constraints, such as word and letter frequencies, and syntactic rules, can be supported by transition probabilities between letter states. Examples of using HMMs for the optimal segmentation and recognition of text were reported in <ref> [60, 31, 151, 12, 96] </ref>. Non-HMM methods such as probabilistic relaxation and hypothesis testing were also used in recognition-based segmentation. Hayes used a probabilistic relaxation method to read off-line handwritten words [65]. The model worked on a hierarchical description of words derived from a skeletal representation.
Reference: [13] <author> M. Berthod and S. Ahyan. </author> <title> On line cursive script recognition: A structural approach with learning. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> page 723, </pages> <year> 1980. </year>
Reference-contexts: Relaxation was performed on the stroke graph and on the letter graph where all the possible segmentations were kept. Letter n-gram statistics were used to rule out unlikely combinations. Examples of using a hypothesis testing and verification algorithm to recognized hand-printed or on-line cursive words were reported in <ref> [75, 75, 13, 121] </ref> 2.2. WORD RECOGNITION WITHOUT CHARACTER SEGMENTATION 41 2.2 Word Recognition Without Character Segmentation The extensiveness of research reported in designing competent character segmentation algorithms to deal with degraded documents also reflects the difficulty in achieving this goal.
Reference: [14] <author> I. Biederman. </author> <title> Human image understanding: Recent research and a theory. </title> <journal> Comp. Vision Graphics Image Process., </journal> <volume> 32 </volume> <pages> 29-73, </pages> <year> 1985. </year>
Reference-contexts: Extensive research efforts related to the computational simulation of figure-ground segregation have been reported in the areas of perceptual organization, texture discrimination and shape decomposition <ref> [108, 128, 101, 14, 88] </ref>. But even today, knowledge of the mechanisms as well as their computational formulation are still at the very primitive stage. To make things even less optimistic, sometimes even the human brain 2.4.
Reference: [15] <editor> Mindy Bokser. </editor> <booktitle> Omni-document technologies. Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1066-1078, </pages> <year> 1992. </year>
Reference-contexts: Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. The first is the difficulty in dealing with touching characters and character fragmentation that are abundant in real-world documents as a result of document degradations <ref> [26, 15, 30, 142, 113, 22] </ref>. Examples of touching characters and character fragmentation in documents are shown in Figure 1.2 and Figure 1.3.
Reference: [16] <author> R. Bozinovic and S.N. Srihari. </author> <title> A string correction algorithm for cursive script recognition. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> PAMI-14(6):655-663, </volume> <year> 1982. </year>
Reference-contexts: These contextual constraints can be used to re-evaluate the segmentation decisions made earlier. Bozinovic <ref> [16] </ref> used a Markov model to represent splitting, merging as well as misclassification in the recognition process. The system seeks to correct the errors by minimizing an edit distance between recognition output and words in a given lexicon.
Reference: [17] <author> T. Breuel. </author> <title> Design and implementation of a system for recognition of handwritten responses on us census forms. </title> <booktitle> In Proceedings of IAPR workshop on Document Analysis systems, </booktitle> <year> 1994. </year>
Reference-contexts: Many algorithms had been proposed in the past for recognizing degraded words. The more recent and advanced algorithms in this area are based on integrating character segmentation with character classification and using the character or word recognition results as feedbacks to alter or improve segmentation decisions made earlier <ref> [53, 17, 25, 167, 149] </ref>.
Reference: [18] <author> V. Bruce and P. Green. </author> <title> Visula Perception: Physiology, Psychology, and Ecology, 2nd ed. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1990. </year> <note> BIBLIOGRAPHY 197 </note>
Reference-contexts: We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision [166, 165, 131, 169], as well as on neural physiological studies of features detection and representation in primate visual cortex <ref> [72, 71, 10, 18, 37] </ref>. This hierarchical feature representation is computed from the skeleton representation of a character. It includes local features such as isolated spots, line segments, curve segments, line (or curve) terminators and line crossings, as well as global features such as holes.
Reference: [19] <author> C.J.C. Burges, J.I. Be, and C.R. Nohl. </author> <title> Recognition of handwritten cursive postal words using neural networks. </title> <booktitle> In USPA 5th Advanced Technology Conference, </booktitle> <pages> pages A-177, </pages> <year> 1992. </year>
Reference-contexts: It generates a lattice of possible feature-to-letter combinations. The final decision is found by choosing an optimal path through the graph. A method that combined dynamic programming and a neural network for the optimal selection of segmentation hypotheses was reported in <ref> [19] </ref>. In another research, a neural network model was used to implement the concept of "selective attention" in human attention and reading, and was applied to compute the optimal segmentation [83]. Character or word-level linguistic constraints are usually used in the recognition-segmentation 40 CHAPTER 2. BACKGROUND stage.
Reference: [20] <author> R. Casey and G. Nagy. </author> <title> An autonomous reading machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-17:492-503, </volume> <year> 1968. </year>
Reference-contexts: Introduction Ever since the emergence of computer technology in the 1940's, a long-standing objective has been to build automatic reading machines with the power, tolerance and efficiency of a human reader <ref> [20, 156, 64, 111, 106, 63] </ref>. Research on computer document recognition has been an important part in the efforts to understand human reading and build automatic systems with matching performance. <p> A segmentation-free classifier for machine-printed numeric fields that used a neural network approach can be found in [54] 46 CHAPTER 2. BACKGROUND 2.3 Character Recognition as Substitution Cipher Decoding Efforts to design autonomous reading machines motivated the early application of substitution cipher decoding to OCR <ref> [20, 21, 4] </ref>. The basic idea with this approach is to make use of language statistics and assign alphabetic labels to visual character patterns so that the visual character repetition pattern in the input text passage best matches the letter repetition pattern provided by a language model. <p> This allows the deciphering algorithm to recognize touching characters and to detect and reverse clustering mistakes. This results in a deciphering algorithm that has robust performance under image degradation. 68 CHAPTER 4. CHARACTER LEVEL DECIPHERING 4.1 Introduction Character-level deciphering algorithms have been proposed for OCR in the past <ref> [20, 23, 112, 114, 127] </ref>. These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code [175, 24].
Reference: [21] <author> R. Casey and G. Nagy. </author> <title> Advance in pattern recognition. </title> <journal> Scientific America, </journal> <volume> 224 </volume> <pages> 56-71, </pages> <year> 1971. </year>
Reference-contexts: A segmentation-free classifier for machine-printed numeric fields that used a neural network approach can be found in [54] 46 CHAPTER 2. BACKGROUND 2.3 Character Recognition as Substitution Cipher Decoding Efforts to design autonomous reading machines motivated the early application of substitution cipher decoding to OCR <ref> [20, 21, 4] </ref>. The basic idea with this approach is to make use of language statistics and assign alphabetic labels to visual character patterns so that the visual character repetition pattern in the input text passage best matches the letter repetition pattern provided by a language model.
Reference: [22] <author> R. Casey and K. Wong. </author> <title> Document-analysis system and techniques. </title> <editor> In R. Kasturi and M. Trivedi, editors, </editor> <booktitle> Image Analysis and Applications, </booktitle> <pages> pages 1-35. </pages> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. The first is the difficulty in dealing with touching characters and character fragmentation that are abundant in real-world documents as a result of document degradations <ref> [26, 15, 30, 142, 113, 22] </ref>. Examples of touching characters and character fragmentation in documents are shown in Figure 1.2 and Figure 1.3. <p> Despite many satisfactory results reported for constrained domains, character segmentation in the presence of image degradation still remains one of the most difficult problems in document recognition and the weakest link in a practical OCR system <ref> [30, 141, 113, 22] </ref>. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 11 12 CHAPTER 1. INTRODUCTION acter over-segmentation. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 13 character under-segmentation. 14 CHAPTER 1. INTRODUCTION 1.3 Contextual Linguistic Information in Document Recogni tion Document recognition has been considered a sub-area in the broader field of computer vision. <p> Another major problem in recognizing degraded words is caused by the occurrence of touching characters and character fragmentation, which usually pose extreme difficulty to recognition algorithms that are based on character segmentation <ref> [113, 26, 22, 163, 39] </ref>. Many algorithms had been proposed in the past for recognizing degraded words.
Reference: [23] <author> R. G. Casey. </author> <title> Text OCR by solving a cryptogram. </title> <booktitle> In IEEE Proceedings, </booktitle> <pages> pages 349-351, </pages> <year> 1986. </year>
Reference-contexts: The system seeks to correct the errors by minimizing an edit distance between recognition output and words in a given lexicon. Casey proposed a non-Markov system that uses a spell-checker to correct repeatedly made merge and split errors in a complete text, rather than in individual words <ref> [23] </ref>. The "grapheme" methods first segment an input image into "graphemes" which are sub-images that are not necessarily individual characters. <p> In this deciphering framework, breaking a substitution cipher corresponds to a particular mapping from the set of character pattern clusters to the alphabetic letter set such that all the cipher-words are legitimate words in the dictionary. Among the published works that use dictionary lookup <ref> [23, 112, 114, 104, 134] </ref>, Casey [23] presented an algorithm based on adaptive optimization of the assignment with the help of pattern matching provided by an on-line spelling checker. <p> Among the published works that use dictionary lookup [23, 112, 114, 104, 134], Casey <ref> [23] </ref> presented an algorithm based on adaptive optimization of the assignment with the help of pattern matching provided by an on-line spelling checker. This algorithm showed some limited ability to handle two-character touching patterns, but it lacks the support for handling more extensive multi-character touching patterns. <p> This allows the deciphering algorithm to recognize touching characters and to detect and reverse clustering mistakes. This results in a deciphering algorithm that has robust performance under image degradation. 68 CHAPTER 4. CHARACTER LEVEL DECIPHERING 4.1 Introduction Character-level deciphering algorithms have been proposed for OCR in the past <ref> [20, 23, 112, 114, 127] </ref>. These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code [175, 24].
Reference: [24] <author> R. G. Casey, S.K. Chai, and K.Y. Wong. </author> <title> Un-supervised construction of decision networks for pattern recognition. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <month> July </month> <year> 1984. </year>
Reference-contexts: These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code <ref> [175, 24] </ref>. A deciphering algorithm is then applied that uses a language model to assign alphabetic labels to the cipher codes so that the character repetition pattern in the input text passage best matches the letter repetition pattern provided by the language model.
Reference: [25] <author> R. G. Casey and G. Nagy. </author> <title> Recursive segmentation and classification of composite character patterns. </title> <booktitle> In Proceedings of 6th International Conference on Pattern Recognition, </booktitle> <pages> pages 1023-1026, </pages> <year> 1982. </year>
Reference-contexts: No feature-based dissection algorithm is employed. The image is divided systematically into many tentative overlapping pieces, from which the optimal character segmentation is to be derived from the recognition results. Early work on this method was described in <ref> [92, 25] </ref>, in which the concept of a moving window of variable width was used to produce tentative segmentation candidates, which were then confirmed or rejected based on character recognition results. Both serial and parallel optimization schemes were used. The serial implementation recognizes words iteratively in a left-to-right fashion. <p> Many algorithms had been proposed in the past for recognizing degraded words. The more recent and advanced algorithms in this area are based on integrating character segmentation with character classification and using the character or word recognition results as feedbacks to alter or improve segmentation decisions made earlier <ref> [53, 17, 25, 167, 149] </ref>.
Reference: [26] <author> R.G. Casey. </author> <title> Character segmentation in document ocr: Progress and hope. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 13-40, </pages> <year> 1995. </year>
Reference-contexts: Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. The first is the difficulty in dealing with touching characters and character fragmentation that are abundant in real-world documents as a result of document degradations <ref> [26, 15, 30, 142, 113, 22] </ref>. Examples of touching characters and character fragmentation in documents are shown in Figure 1.2 and Figure 1.3. <p> Another major problem in recognizing degraded words is caused by the occurrence of touching characters and character fragmentation, which usually pose extreme difficulty to recognition algorithms that are based on character segmentation <ref> [113, 26, 22, 163, 39] </ref>. Many algorithms had been proposed in the past for recognizing degraded words.
Reference: [27] <author> R.G. Casey and E. Lecolinet. </author> <title> Strategies in character segmentation: A survey. </title> <booktitle> In Proceedings of the International Conference on Document Analysis and Recognition, </booktitle> <address> ICDAR-95, </address> <year> 1995. </year>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [28] <author> R.G. Casey and E. Lecolinet. </author> <title> A survey of methods and strategies in character segmentation. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <note> PAMI-18:690-706, 1996. 198 BIBLIOGRAPHY </note>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [29] <author> M. Cesar and R. Shinghal. </author> <title> Algorithm for segmenting handwritten postal codes. </title> <journal> Int. J. Man Machine Study, </journal> <volume> 33(1) </volume> <pages> 63-80, </pages> <year> 1990. </year>
Reference-contexts: By using information regarding the size, aspect ratios, and adjacency relationships of the bounding boxes to trigger the merging or splitting of connected components, this method can usually achieve accurate character segmentation. One example was given in <ref> [29] </ref> where knowledge of the symbols was also used to help segment handwritten post codes. Experimental comparison of character segmentation by projection analysis vs. segment 36 CHAPTER 2. BACKGROUND 2.1. CHARACTER SEGMENTATION 37 ation by connected component analysis is reported in [173].
Reference: [30] <author> C. Chen and J. Decurtins. </author> <title> A segmentation-free approach to OCR. </title> <booktitle> In Proceedings of IEEE Workshop on Applications of Computer Vision, </booktitle> <pages> pages 190-196, </pages> <year> 1992. </year>
Reference-contexts: Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. The first is the difficulty in dealing with touching characters and character fragmentation that are abundant in real-world documents as a result of document degradations <ref> [26, 15, 30, 142, 113, 22] </ref>. Examples of touching characters and character fragmentation in documents are shown in Figure 1.2 and Figure 1.3. <p> Despite many satisfactory results reported for constrained domains, character segmentation in the presence of image degradation still remains one of the most difficult problems in document recognition and the weakest link in a practical OCR system <ref> [30, 141, 113, 22] </ref>. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 11 12 CHAPTER 1. INTRODUCTION acter over-segmentation. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 13 character under-segmentation. 14 CHAPTER 1. INTRODUCTION 1.3 Contextual Linguistic Information in Document Recogni tion Document recognition has been considered a sub-area in the broader field of computer vision. <p> They differ from the holistic recognition methods in that the actual recognition and representation of features are on the sub-character level. The recognition of a character is triggered by detecting a combination of sub-character features at the correct locations. Chen <ref> [30] </ref> proposed an approach that was based on the concept of occluding object recognition in computer vision [34], in which objects are recognized and then segmented from the image. Characters are treated as touching or occluding objects that are subject to special constraints on their poses.
Reference: [31] <author> M.Y. Chen. </author> <title> An alternative to variable duration hmm in handwritten word recognition. </title> <booktitle> In Proceedings IWFHR III, </booktitle> <pages> page 82, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An HMM can be trained to learn the letter segmentation behavior because contextual constraints, such as word and letter frequencies, and syntactic rules, can be supported by transition probabilities between letter states. Examples of using HMMs for the optimal segmentation and recognition of text were reported in <ref> [60, 31, 151, 12, 96] </ref>. Non-HMM methods such as probabilistic relaxation and hypothesis testing were also used in recognition-based segmentation. Hayes used a probabilistic relaxation method to read off-line handwritten words [65]. The model worked on a hierarchical description of words derived from a skeletal representation.
Reference: [32] <author> M. Cheriet, Y.S. Huang, and C.Y. Suen. </author> <title> Background region-based algorithm for the segmentation of connected digits. </title> <booktitle> In 11th IAPR Int. Conf. on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> page 619, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Pattern splitting is then carried out on connected components classified as touching characters by locating characteristic landmarks indicating likely segmentation points. Methods for finding the optimal segmentation 38 CHAPTER 2. BACKGROUND path were also reported in <ref> [32, 55, 172] </ref>. 2.1.1.5 Landmark Detection Image features such as character ascenders and descenders usually can serve as landmarks for segmentation of word images. Methods of character segmentation based on detection of ascenders and descenders has been applied to both printed text as well as cursive writings.
Reference: [33] <author> M.B. Clowes. </author> <title> On seeing things. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 79-116, </pages> <year> 1971. </year>
Reference-contexts: As a result, each object in the scene are frequently associated with multiple labels representing different classification results. An early example of scene labeling was the interpretation of line segments in a line drawing of a set of polyhedra. This problem domain was also investigated in <ref> [33, 73, 171] </ref>. The ambiguities in individual object identification are difficult to resolve if each object is considered in isolation. However, the relationships among the objects can be helpful in resolving the ambiguities and achieving a scene labeling that is consistent with the scene context.
Reference: [34] <author> M. Cooper. </author> <title> Visual Occlusion and the Interpretation of Ambiguous Pictures. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1992. </year>
Reference-contexts: The recognition of a character is triggered by detecting a combination of sub-character features at the correct locations. Chen [30] proposed an approach that was based on the concept of occluding object recognition in computer vision <ref> [34] </ref>, in which objects are recognized and then segmented from the image. Characters are treated as touching or occluding objects that are subject to special constraints on their poses.
Reference: [35] <author> T.N. Cornsweet. </author> <title> Visual Perception. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: They often reflect other problems familiar to researchers in a few related branches of science. For decades figure-ground segregation (or image segmentation) has been an elusive problem that perplexed researchers in the fields of computer vision, cognitive science and psychology <ref> [107, 9, 145, 85, 35] </ref>. This difficulty in separating what is meaningful to recognition from what is irrelevant background noise lies at the core of the computational model that forms the base of today's computer vision systems [109].
Reference: [36] <author> R.G. Crowder. </author> <title> The Psychology of Reading: An Introduction. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired <ref> [130, 82, 36, 146, 150] </ref>. Developments in the field of OCR, computer vision, patterns recognition, as well as human reading have shown that the use of contextual information is more than an ad-hoc remedy to deficiencies in visual recognition.
Reference: [37] <author> R. Desimone and L.G. Ungerleider. </author> <title> Neural mechanisms of visula processing in monkeys. </title> <editor> In F. Boller and J. Grafman, editors, </editor> <booktitle> Handbook of Neuropsychology, </booktitle> <volume> vol. 2, </volume> <pages> pages 267-299. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision [166, 165, 131, 169], as well as on neural physiological studies of features detection and representation in primate visual cortex <ref> [72, 71, 10, 18, 37] </ref>. This hierarchical feature representation is computed from the skeleton representation of a character. It includes local features such as isolated spots, line segments, curve segments, line (or curve) terminators and line crossings, as well as global features such as holes.
Reference: [38] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The existence of visually similar characters with different identities makes it extremely difficult to achieve good clustering results. Image degradations that cause character pattern deformations aggravate this problem. There have been many research efforts that have tried to improve clustering performance by optimizing a chosen mathematical objective function <ref> [80, 2, 38] </ref>. Nevertheless, achieving accurate clustering results especially on degraded images is still a challenging issue. Two problems need to be solved. First of all, a clustering algorithm for character patterns needs to be developed so that good clustering performance can be obtained in the presence of image degradation.
Reference: [39] <author> C.E. Dunn and P.S.P. Wang. </author> <title> Character segmentation techniques for handwritten text - a survey. </title> <booktitle> In 11th IAPR Int. Conf. on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> page 577, </pages> <month> August </month> <year> 1992. </year> <note> BIBLIOGRAPHY 199 </note>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions. <p> Another major problem in recognizing degraded words is caused by the occurrence of touching characters and character fragmentation, which usually pose extreme difficulty to recognition algorithms that are based on character segmentation <ref> [113, 26, 22, 163, 39] </ref>. Many algorithms had been proposed in the past for recognizing degraded words.
Reference: [40] <author> L.D. Earnest. </author> <title> Machine recognition of cursive writing. </title> <editor> In C. Cherry, editor, </editor> <booktitle> Information Processing. Butterworth, </booktitle> <address> London, </address> <year> 1962. </year>
Reference-contexts: Another approach to word recognition is the holistic approach which is based on extracting features that represent the whole word and comparing the feature representation of the unknown word with those references stored in a lexicon <ref> [40, 75, 68] </ref>. Later improvements in holistic recognition restricted the possible match to a reduced neighborhood containing words that are visually similar to the unknown word. For example, the neighborhood for the word "word" might also contain the word "work" and the word "ward".
Reference: [41] <author> S. Edelman, T. Flash, and S. Ullman. </author> <title> Reading cursive handwriting by alignment of letter prototypes. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 5(3) </volume> <pages> 303-331, </pages> <year> 1990. </year>
Reference-contexts: The method allows the recognition of characters that overlap, or that are underlined. A final search for the optimal path under certain criteria gives the best interpretation of the word features. A different approach using letter prototype alignment was proposed for cursive handwriting recognition <ref> [41] </ref>. The basic idea of the alignment approach is to decompose the recognition process into two stages: First, the transformations between the viewed object and each one of the candidate models or prototypes are determined. This is the alignment stage.
Reference: [42] <author> R.W. Ehrich and K.J. Koehler. </author> <title> Experiments in the contextual recognition of cursive script. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 24(2):182, </volume> <year> 1975. </year>
Reference-contexts: Most dissection techniques for cursive script are based on the fact that lowercase characters are usually linked by lower ligatures. A simple way to locate these ligatures is to detect the minima of the upper outline of words. Examples of techniques along this line are given in <ref> [42, 105, 70, 89, 47] </ref>. 2.1. CHARACTER SEGMENTATION 39 Sennhauser [152] proposed a blackboard-based system which integrated three contextual knowledge sources to provide feedback and generate hypotheses for recognition and re-segmentation.
Reference: [43] <author> L.J. Evett et al. </author> <title> Using linguistic information to aid handwriting recognition. </title> <editor> In S. Impe-dovo and J.C. Simon, editors, </editor> <title> From Pixels to Features III: </title> <booktitle> Frontiers in Handwriting Recognition, </booktitle> <pages> pages 339-348. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: Character n-gram statistics or a dictionary were usually used to model the linguistic constraints. Later developments along this line included using linguistic constraints at the word and sentence levels such as word n-gram statistics, word collocation statistics, and the transition probabilities of word categories <ref> [162, 160, 43] </ref>. New statistical tools and language models such as the Hidden Markov Model and grammatical parsers that were developed and achieved successes in speech recognition and natural language understanding were also introduced into OCR [133, 135].
Reference: [44] <author> R.J. Evey. </author> <title> Use of a computer to design character recognition logic. </title> <booktitle> In Proceedings of the Estern Jt. Comp. Conf., </booktitle> <pages> pages 205-211, </pages> <year> 1959. </year>
Reference-contexts: Examples were bank check fonts that were designed with strong leading edge features to indicate the accurate position of each character <ref> [44] </ref>, as well as forms with printed boxes where hand-printed characters are supposed to be enclosed (Figure 2.2). Therefore, character segmentation was a relatively easy task and considered only a secondary issue in an OCR system.
Reference: [45] <author> C. Fang and J.J. Hull. </author> <title> A modified character level deciphering algorithm for OCR in degraded documents. </title> <booktitle> In Proceedings of the Conference on Document Recognition of 1995 IS&T/SPIE Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: The work presented in Chapter 4 is intented to improve the power of character-level deciphering algorithms in this respect. The modified character-level deciphering algorithm is enhanced to handle touching characters and is tolerant to clustering mistakes <ref> [45] </ref>. Nevertheless, these modifications are not able to solve the character fragmentation problem. Sensitivity to document image degradation still remains an issue for character-level deciphering algorithms. 116 CHAPTER 5.
Reference: [46] <author> C. Fang and J.J. Hull. </author> <title> A word-level deciphering algorithm for degraded document recognition. </title> <booktitle> In Proceedings of DAIR 95, </booktitle> <year> 1995. </year>
Reference-contexts: It is assumed that a partial font base that represents font information of at least part of the character alphabet is available either through an explicit font learning step [87] or from the results of a word-level deciphering algorithm that has recognized a portion of the input text <ref> [46] </ref>. Figure 5.9 in Chapter 5 illustrates the components involved in the proposed method.
Reference: [47] <author> R. Fenrich. </author> <title> Segmentation of automatically located handwritten numeric strings. </title> <editor> In S. Impedovo and J.C. Simon, editors, </editor> <title> From Pixels to Features III: Frontiers in Handwriting Recognition, page 47. </title> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: Most dissection techniques for cursive script are based on the fact that lowercase characters are usually linked by lower ligatures. A simple way to locate these ligatures is to detect the minima of the upper outline of words. Examples of techniques along this line are given in <ref> [42, 105, 70, 89, 47] </ref>. 2.1. CHARACTER SEGMENTATION 39 Sennhauser [152] proposed a blackboard-based system which integrated three contextual knowledge sources to provide feedback and generate hypotheses for recognition and re-segmentation.
Reference: [48] <author> W. Forsyth. </author> <title> Solving substitution ciphers using the method of simulated annealing. </title> <type> PhD thesis, Honors Thesis, </type> <institution> Dept. of Math, Statistics and Computing Science, The University of New England, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: A simulated annealing algorithm can be used to solve the optimization. Examples can be found in <ref> [48, 49] </ref>. An instance of a combinatorial optimization problem can be described as a pair (R; F ), where R the configuration space, is a finite set of configurations, and F is a cost function that attaches a real number (cost) to each configuration A 2 R. <p> For a given value of T a number of transitions are considered and then T is lowered until it reaches zero. General information about simulated annealing and it various applications can be found in [97, 1]. Forsyth and Safavi-Naini <ref> [48, 49] </ref> proposed an elegant formulation of decoding a substitution cipher as a combinatorial optimization problem and suggested using a simulated annealing algorithm to find the optimal solution that corresponds to the breaking of the cipher.
Reference: [49] <author> W. Forsyth and R. Safavi-Naini. </author> <title> Automated cryptanalysis of substitution ciphers. </title> <journal> Cryptologia, </journal> <note> XVII(4):407-418, 1993. 200 BIBLIOGRAPHY </note>
Reference-contexts: A simulated annealing algorithm can be used to solve the optimization. Examples can be found in <ref> [48, 49] </ref>. An instance of a combinatorial optimization problem can be described as a pair (R; F ), where R the configuration space, is a finite set of configurations, and F is a cost function that attaches a real number (cost) to each configuration A 2 R. <p> For a given value of T a number of transitions are considered and then T is lowered until it reaches zero. General information about simulated annealing and it various applications can be found in [97, 1]. Forsyth and Safavi-Naini <ref> [48, 49] </ref> proposed an elegant formulation of decoding a substitution cipher as a combinatorial optimization problem and suggested using a simulated annealing algorithm to find the optimal solution that corresponds to the breaking of the cipher.
Reference: [50] <author> P.D. Friday and C.G. Leedham. </author> <title> A pre-segmenter for separating characters in unconstrained hand-printed text. </title> <booktitle> In Proc. Int. Conf. on Image Processing, </booktitle> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: Moreover, the upper and lower profiles of the patterns were also used in combination with the projection to get improved segmentation point candidates [6]. Applications of projection analysis for character segmentation on italic fonts or on text that involves slanted characters can be found in <ref> [50] </ref>, where the projections were implemented at two-degree increments between 16 and 16 degrees from the vertical. Segmentation cuts were also implemented along the projection angle. 2.1.1.3 Connected Components Analysis Pitch-based methods usually cannot be applied when the width of characters is variable.
Reference: [51] <author> V. Fromkin, </author> <title> editor. Errors in Linguistic Performance: Slips of the Tongue, Ear, Pen and Hand. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: CHARACTER LEVEL DECIPHERING Research in spelling correction and OCR post-processing algorithms can provide useful hints about the construction of character confusion heuristics [94, 161, 86]. Most of the techniques used for text correction are based on studies of characteristics of spelling errors made by human beings <ref> [51] </ref>, and therefore are not directly applicable to the correction of character identities achieved by character pattern clustering and deciphering. However, some techniques that were especially designed for the correction of OCR errors might be helpful.
Reference: [52] <author> K-S Fu and T.S. Yu. </author> <title> Statistical Pattern Classification using Contextual Information. </title> <publisher> John Wiley & Sons Ltd., </publisher> <address> Chichester, England, </address> <year> 1980. </year>
Reference-contexts: Research in using contextual information in pattern recognition had resulted in new classification frameworks that incorporated different levels of contextual constraints inside the discrimination function for high classification accuracy and more meaningful interpretation of results <ref> [52, 164] </ref>. Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired [130, 82, 36, 146, 150].
Reference: [53] <author> H. Fujisawa and K. Kurino Y. Nakano. </author> <title> Segmentation methods for character recognition: from segmentation to document structure analysis. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1079-1092, </pages> <year> 1992. </year>
Reference-contexts: Many algorithms had been proposed in the past for recognizing degraded words. The more recent and advanced algorithms in this area are based on integrating character segmentation with character classification and using the character or word recognition results as feedbacks to alter or improve segmentation decisions made earlier <ref> [53, 17, 25, 167, 149] </ref>.
Reference: [54] <author> P. Gader et al. </author> <title> A segmentation-free neural network classifier for machine-printed numeric fields. </title> <booktitle> In Proceedings of the 5th USPS Advanced Technology Conference, </booktitle> <month> Decem-ber </month> <year> 1992. </year>
Reference-contexts: In addition to the true instances, several false ones are usually found. Finally, a best-first search is performed to compose admissible strings out of the set of all detected letter instances. A segmentation-free classifier for machine-printed numeric fields that used a neural network approach can be found in <ref> [54] </ref> 46 CHAPTER 2. BACKGROUND 2.3 Character Recognition as Substitution Cipher Decoding Efforts to design autonomous reading machines motivated the early application of substitution cipher decoding to OCR [20, 21, 4].
Reference: [55] <author> P. Gader, M. Magdi, and J-H. Chiang. </author> <title> Segmentation-based handwritten word recognition. </title> <booktitle> In Proceedings of the 5th USPS Advanced Technology Conference, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Pattern splitting is then carried out on connected components classified as touching characters by locating characteristic landmarks indicating likely segmentation points. Methods for finding the optimal segmentation 38 CHAPTER 2. BACKGROUND path were also reported in <ref> [32, 55, 172] </ref>. 2.1.1.5 Landmark Detection Image features such as character ascenders and descenders usually can serve as landmarks for segmentation of word images. Methods of character segmentation based on detection of ascenders and descenders has been applied to both printed text as well as cursive writings.
Reference: [56] <author> R. Ganesan and A. Sherman. </author> <title> Statistical techniques for language recognition: An empirical study using real and simulated english. </title> <note> in preparation. </note>
Reference-contexts: BACKGROUND 2.3.1 Character n-grams and Dictionary Lookup One group of methods are based on using character n-gram statistics as constraints. Examples that use character bigram and trigram statistics are [157, 120]. Some other works also used a stationary Markov model, examples are <ref> [155, 57, 56, 95] </ref>. Two problems exist with using character n-gram statistics. First, a substantial amount of cipher text is needed for statistically reliable n-gram estimation.
Reference: [57] <author> R. Ganesan and A. Sherman. </author> <title> Statistical techniques for language recognition: An introduction and guide for cryptanalysis. </title> <journal> Cryptologia, </journal> <volume> XVII(4), </volume> <year> 1993. </year>
Reference-contexts: BACKGROUND 2.3.1 Character n-grams and Dictionary Lookup One group of methods are based on using character n-gram statistics as constraints. Examples that use character bigram and trigram statistics are [157, 120]. Some other works also used a stationary Markov model, examples are <ref> [155, 57, 56, 95] </ref>. Two problems exist with using character n-gram statistics. First, a substantial amount of cipher text is needed for statistically reliable n-gram estimation.
Reference: [58] <author> J.J. Gibson. </author> <title> The Senses Considered as Perceptual Systems. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1966. </year>
Reference-contexts: Gibson's ecological approach to vision represents a systematic effort to address the problem of visual representation. Rather than focusing on detecting and responding to sensory variances, the function of vision in Gibson's view was to detect and recover the basic "invariants" of the external world <ref> [59, 58] </ref>. In other words, representation of the underlying invariant properties of the perceived subject is the core of visual representation and perception. Gibson's theory is directly relevant to the task of document recognition.
Reference: [59] <author> J.J. Gibson. </author> <title> The Ecological Approach to Visual Perception. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1979. </year> <note> BIBLIOGRAPHY 201 </note>
Reference-contexts: Gibson's ecological approach to vision represents a systematic effort to address the problem of visual representation. Rather than focusing on detecting and responding to sensory variances, the function of vision in Gibson's view was to detect and recover the basic "invariants" of the external world <ref> [59, 58] </ref>. In other words, representation of the underlying invariant properties of the perceived subject is the core of visual representation and perception. Gibson's theory is directly relevant to the task of document recognition.
Reference: [60] <author> A.M. Gillies. </author> <title> Cursive word recognition using hidden markov models. </title> <booktitle> In USPS 5th Advanced Technology Conference, </booktitle> <year> 1992. </year>
Reference-contexts: An HMM can be trained to learn the letter segmentation behavior because contextual constraints, such as word and letter frequencies, and syntactic rules, can be supported by transition probabilities between letter states. Examples of using HMMs for the optimal segmentation and recognition of text were reported in <ref> [60, 31, 151, 12, 96] </ref>. Non-HMM methods such as probabilistic relaxation and hypothesis testing were also used in recognition-based segmentation. Hayes used a probabilistic relaxation method to read off-line handwritten words [65]. The model worked on a hierarchical description of words derived from a skeletal representation.
Reference: [61] <author> M. Gilloux, J.M. Bertille, and M. Leroux. </author> <title> Recognition of handwritten words in a limited dynamic vocabulary. </title> <booktitle> In Proceedings IWFHR III, </booktitle> <pages> page 417, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Whereas features used in Leroux's method were derived from the contours of word image patterns. Hidden Markov Models are also used in holistic word recognition <ref> [115, 61] </ref>. Angular representation is used in Nagy's system to represent the feature, while structural image primitives are used in Gilloux's system. Hull [74] proposed a holistic approach to recognize machine-printed text. Here the method is used to recognize commonly occuring words in a document.
Reference: [62] <author> A. Goshtasby and R. W. Ehrich. </author> <title> Contextual word recognition using probabilistic relaxation labeling. </title> <journal> Pattern Recognition, </journal> <volume> 21(5) </volume> <pages> 455-462, </pages> <year> 1988. </year>
Reference-contexts: CHARACTER RECOGNITION AS SUBSTITUTION CIPHER DECODING 51 also applied to contextual word recognition where initial probabilities of character patterns being certain alphabetic letters are provided by a character recognizer and relaxation labeling is used for post-processing by subjecting character identity probabilities to letter n-gram constraints provided by a language model <ref> [62] </ref>. Relaxation labeling algorithms for decoding a substitution cipher rely on using character level language constraints in decoding character identities. <p> And the relaxation process can be applied so that a recognition result representing the most context-compatible interpretation can be achieved. 5.2.3.2 Adaptation to the word-level The relaxation updating scheme in our word-level deciphering algorithm is based on Gos-htasby's relaxation procedure on the character level <ref> [62] </ref>. We changed the updating scheme so that it works on the word level by using word transition probabilities within each word bigram. The relaxation updating process is applied to each of the selected cipher-word bigrams.
Reference: [63] <author> V.K. Govindan and A.P. Shivaprasad. </author> <title> Character recognition a review. </title> <journal> Pattern Recognition, </journal> <volume> 23(7) </volume> <pages> 671-683, </pages> <year> 1990. </year>
Reference-contexts: Introduction Ever since the emergence of computer technology in the 1940's, a long-standing objective has been to build automatic reading machines with the power, tolerance and efficiency of a human reader <ref> [20, 156, 64, 111, 106, 63] </ref>. Research on computer document recognition has been an important part in the efforts to understand human reading and build automatic systems with matching performance.
Reference: [64] <author> L.D. Harmon. </author> <title> Automatic recognition of print and script. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 60(10) </volume> <pages> 1165-1177, </pages> <year> 1972. </year>
Reference-contexts: Introduction Ever since the emergence of computer technology in the 1940's, a long-standing objective has been to build automatic reading machines with the power, tolerance and efficiency of a human reader <ref> [20, 156, 64, 111, 106, 63] </ref>. Research on computer document recognition has been an important part in the efforts to understand human reading and build automatic systems with matching performance. <p> Methods of character segmentation based on detection of ascenders and descenders has been applied to both printed text as well as cursive writings. An example of applying landmark detection in character segmentation is given in <ref> [64] </ref>. 2.1.2 Re-segmentation after Classification Using Context The identities of characters or words in the text are subject to contextual constraints on different linguistic levels. These contextual constraints can be used to re-evaluate the segmentation decisions made earlier.
Reference: [65] <author> K.C. Hayes. </author> <title> Reading handwritten words using hierarchical relaxation. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 14 </volume> <pages> 344-364, </pages> <year> 1980. </year>
Reference-contexts: Examples of using HMMs for the optimal segmentation and recognition of text were reported in [60, 31, 151, 12, 96]. Non-HMM methods such as probabilistic relaxation and hypothesis testing were also used in recognition-based segmentation. Hayes used a probabilistic relaxation method to read off-line handwritten words <ref> [65] </ref>. The model worked on a hierarchical description of words derived from a skeletal representation. Relaxation was performed on the stroke graph and on the letter graph where all the possible segmentations were kept. Letter n-gram statistics were used to rule out unlikely combinations.
Reference: [66] <author> H.S. </author> <title> Heaps. Information Retrieval: Computational and Theoretical Aspects. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The reasons for this are that more font information can be learned from the re-recognition stage, and also the most frequent 21 lower-case characters which are likely to be learned account for 96% of all text <ref> [93, 66] </ref>. Based on this observation, we propose a novel word-level deciphering algorithm that first solves a selected portion of the input text that has relatively more reliable word bigram statistics by using a word-level relaxation deciphering algorithm.
Reference: [67] <author> R.B. Hennis. </author> <title> The ibm 1975 optical page reader: system design. </title> <journal> IBM J. of Res. and Dev., </journal> <pages> pages 346-353, </pages> <month> Sept. </month> <year> 1968. </year>
Reference-contexts: Pitch estimation also permits correct segmentation in case of touching characters and character fragmentation. Combining white space location and pitch estimation can usually produce reliable segmentation results on prints of well-spaced font with fixed width. Examples of commercial applications can be found in <ref> [67, 69] </ref>. 2.1.1.2 Projection Analysis The vertical projection of a word image consists of a simple running count of black pixels in each column. It can help detect white space between characters. It can also help to locate vertical strokes.
Reference: [68] <author> Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari. </author> <title> A word shape analysis approach to lexicon based word recognition. </title> <journal> Pattern Recognition letters, </journal> <volume> 13 </volume> <pages> 821-826, </pages> <year> 1992. </year>
Reference-contexts: The document image was generated artificially using a text formatting package. Random noise that simulates image degradation was added to the clean image. As a result of the image degradation introduced, the preliminary word recognizer we used to generate the top ten neighborhood <ref> [68] </ref> only got a 79% correct rate for the top choice, the correct rate for top ten choices was 99.6%. A portion of the degraded document image is shown in Figure 5.10. <p> Another approach to word recognition is the holistic approach which is based on extracting features that represent the whole word and comparing the feature representation of the unknown word with those references stored in a lexicon <ref> [40, 75, 68] </ref>. Later improvements in holistic recognition restricted the possible match to a reduced neighborhood containing words that are visually similar to the unknown word. For example, the neighborhood for the word "word" might also contain the word "work" and the word "ward".
Reference: [69] <author> Hoffman. </author> <title> Segmentation methods for recognition of machine-printed characters. </title> <journal> IBM J. of Res. and Dev., </journal> <pages> pages 153-165, </pages> <month> March </month> <year> 1971. </year>
Reference-contexts: Pitch estimation also permits correct segmentation in case of touching characters and character fragmentation. Combining white space location and pitch estimation can usually produce reliable segmentation results on prints of well-spaced font with fixed width. Examples of commercial applications can be found in <ref> [67, 69] </ref>. 2.1.1.2 Projection Analysis The vertical projection of a word image consists of a simple running count of black pixels in each column. It can help detect white space between characters. It can also help to locate vertical strokes.
Reference: [70] <author> M. Holt, M. Beglou, and S. Datta. </author> <title> Slant-independent letter segmentation for off-line cursive script recognition. </title> <editor> In S. Impedovo and J.C. Simon, editors, </editor> <title> From Pixels to Features III: Frontiers in Handwriting Recognition, page 41. </title> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1992. </year> <note> 202 BIBLIOGRAPHY </note>
Reference-contexts: Most dissection techniques for cursive script are based on the fact that lowercase characters are usually linked by lower ligatures. A simple way to locate these ligatures is to detect the minima of the upper outline of words. Examples of techniques along this line are given in <ref> [42, 105, 70, 89, 47] </ref>. 2.1. CHARACTER SEGMENTATION 39 Sennhauser [152] proposed a blackboard-based system which integrated three contextual knowledge sources to provide feedback and generate hypotheses for recognition and re-segmentation.
Reference: [71] <author> D.H. Hubel. </author> <title> Eye, Brain, and Vision. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision [166, 165, 131, 169], as well as on neural physiological studies of features detection and representation in primate visual cortex <ref> [72, 71, 10, 18, 37] </ref>. This hierarchical feature representation is computed from the skeleton representation of a character. It includes local features such as isolated spots, line segments, curve segments, line (or curve) terminators and line crossings, as well as global features such as holes.
Reference: [72] <author> D.H. Hubel and T.N. Wiesel. </author> <title> Receptive fields and functional architecture of monkey striate cortex. </title> <journal> J. Physiology, London, </journal> <volume> 195 </volume> <pages> 215-243, </pages> <year> 1968. </year>
Reference-contexts: We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision [166, 165, 131, 169], as well as on neural physiological studies of features detection and representation in primate visual cortex <ref> [72, 71, 10, 18, 37] </ref>. This hierarchical feature representation is computed from the skeleton representation of a character. It includes local features such as isolated spots, line segments, curve segments, line (or curve) terminators and line crossings, as well as global features such as holes.
Reference: [73] <author> D.A. Huffman. </author> <title> Impossible objects as nonsense sentences. </title> <editor> In B. Melzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 6, </volume> <pages> pages 295-323. </pages> <publisher> Edinburgh Univ. Press, </publisher> <year> 1971. </year>
Reference-contexts: As a result, each object in the scene are frequently associated with multiple labels representing different classification results. An early example of scene labeling was the interpretation of line segments in a line drawing of a set of polyhedra. This problem domain was also investigated in <ref> [33, 73, 171] </ref>. The ambiguities in individual object identification are difficult to resolve if each object is considered in isolation. However, the relationships among the objects can be helpful in resolving the ambiguities and achieving a scene labeling that is consistent with the scene context.
Reference: [74] <author> J. Hull et al. </author> <title> Combination of segmentation-based and holistic handwritten word recognition algorithms. </title> <editor> In S. Impedovo and J.C. Simon, editors, </editor> <title> From Pixels to Features III: Frontiers in Handwriting Recognition, page 261. </title> <publisher> Elsevier Science Publishers, </publisher> <address> Am-sterdam, </address> <year> 1992. </year>
Reference-contexts: Whereas features used in Leroux's method were derived from the contours of word image patterns. Hidden Markov Models are also used in holistic word recognition [115, 61]. Angular representation is used in Nagy's system to represent the feature, while structural image primitives are used in Gilloux's system. Hull <ref> [74] </ref> proposed a holistic approach to recognize machine-printed text. Here the method is used to recognize commonly occuring words in a document. Clustering is used to separate groups of matching words from the text. Conventional OCR and linguistic analysis can then be applied to recognize the groups.
Reference: [75] <author> J.J. Hull. </author> <title> Hypothesis generation in a computational model for visual word recognition. </title> <journal> IEEE Expert, </journal> <pages> pages 63-70, </pages> <year> 1986. </year>
Reference-contexts: Relaxation was performed on the stroke graph and on the letter graph where all the possible segmentations were kept. Letter n-gram statistics were used to rule out unlikely combinations. Examples of using a hypothesis testing and verification algorithm to recognized hand-printed or on-line cursive words were reported in <ref> [75, 75, 13, 121] </ref> 2.2. WORD RECOGNITION WITHOUT CHARACTER SEGMENTATION 41 2.2 Word Recognition Without Character Segmentation The extensiveness of research reported in designing competent character segmentation algorithms to deal with degraded documents also reflects the difficulty in achieving this goal. <p> Another approach to word recognition is the holistic approach which is based on extracting features that represent the whole word and comparing the feature representation of the unknown word with those references stored in a lexicon <ref> [40, 75, 68] </ref>. Later improvements in holistic recognition restricted the possible match to a reduced neighborhood containing words that are visually similar to the unknown word. For example, the neighborhood for the word "word" might also contain the word "work" and the word "ward".
Reference: [76] <author> J.J. Hull. </author> <title> Hypothesis testing in a computational theory of visual word recognition. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 718-722, </pages> <year> 1987. </year>
Reference-contexts: These algorithms effectively compensate for image degradation by choosing the optimal neighborhood size to guarantee that the correct word is included. The reduction of the neighborhood to a single 6.1. INTRODUCTION 159 optimal decision that best matches the word image is performed by a hypothesis testing algorithm <ref> [76] </ref>. In this chapter, a novel hypothesis testing algorithm is proposed that achieves recognition by reducing the candidate neighborhood of a given word image to a single choice.
Reference: [77] <author> Jonathan J. Hull and Y. Li. </author> <title> Interpreting word recognition decisions with a document database graph. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition, </booktitle> <pages> pages 488-492, </pages> <address> Tsukuba Science City, Japan, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: The matching language database is acquired by a vocabulary matching algorithm from a database of pre-classified text documents based on the sets of word candidate neighborhoods generated for each word in the input document <ref> [77, 78] </ref>. This vocabulary matching algorithm is executed on the sets of word candidate neighborhoods of an input document. Similarities between the input document and each document in the database are determined by using a modified vector space model [147] for information retrieval. <p> A portion of the degraded document image is shown in Figure 5.10. The matching language base that provides word bigram constraints consists of 31 recent news reports on the Korean Peninsula nuclear crisis which was generated by a vocabulary matching system <ref> [77] </ref> from a comprehensive language corpus acquired from Clarinet. The word candidate neighborhoods generated by the preliminary word recognizer were used by the vocabulary matching system to correctly locate the language base that matches the input text in content. <p> Three matching language databases that provide word bigram constraints for each of the subject group were constructed separately using a vocabulary matching algorithm <ref> [77, 78] </ref>. Each constructed language database consists of recent news reports from the corresponding Clarinet newsgroups. The test documents are not included in the document database from which the language bases were compiled.
Reference: [78] <author> Jonathan J. Hull and Y. Li. </author> <title> Word recognition result interpretation using the vector space model for information retrieval. </title> <booktitle> In Proceedings of the Second Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 147-155, </pages> <year> 1993. </year>
Reference-contexts: The matching language database is acquired by a vocabulary matching algorithm from a database of pre-classified text documents based on the sets of word candidate neighborhoods generated for each word in the input document <ref> [77, 78] </ref>. This vocabulary matching algorithm is executed on the sets of word candidate neighborhoods of an input document. Similarities between the input document and each document in the database are determined by using a modified vector space model [147] for information retrieval. <p> Three matching language databases that provide word bigram constraints for each of the subject group were constructed separately using a vocabulary matching algorithm <ref> [77, 78] </ref>. Each constructed language database consists of recent news reports from the corresponding Clarinet newsgroups. The test documents are not included in the document database from which the language bases were compiled.
Reference: [79] <author> D. Hunter and A. McKenzie. </author> <title> Experiments with relaxation algorithms for breaking simple substitution cipher. </title> <journal> The Comput. J., </journal> <volume> 26(1) </volume> <pages> 68-71, </pages> <year> 1983. </year>
Reference-contexts: Iterating the updating scheme results in improved estimates that finally lead to the breaking of the cipher (Figure 2.6). Later similar studies in applying a relaxation algorithm for breaking a substitution cipher resulted in improved updating rules <ref> [91, 79] </ref> and deciphering algorithms for substitution ciphers where word boundaries may have been concealed [79, 90]. Relaxation labeling was 2.3. <p> Later similar studies in applying a relaxation algorithm for breaking a substitution cipher resulted in improved updating rules [91, 79] and deciphering algorithms for substitution ciphers where word boundaries may have been concealed <ref> [79, 90] </ref>. Relaxation labeling was 2.3.
Reference: [80] <author> A.K. Jian and R.C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year> <note> BIBLIOGRAPHY 203 </note>
Reference-contexts: The existence of visually similar characters with different identities makes it extremely difficult to achieve good clustering results. Image degradations that cause character pattern deformations aggravate this problem. There have been many research efforts that have tried to improve clustering performance by optimizing a chosen mathematical objective function <ref> [80, 2, 38] </ref>. Nevertheless, achieving accurate clustering results especially on degraded images is still a challenging issue. Two problems need to be solved. First of all, a clustering algorithm for character patterns needs to be developed so that good clustering performance can be obtained in the presence of image degradation.
Reference: [81] <author> M.A. Jones, G.A. Story, and B.W. Ballard. </author> <title> Integrating multiple knowledge sources in a bayesian ocr post-processor. </title> <booktitle> In Proceedings of the First International Conference on Document Analysis and Recognition(ICDAR-91), </booktitle> <pages> pages 925-933, </pages> <year> 1991. </year>
Reference-contexts: However, some techniques that were especially designed for the correction of OCR errors might be helpful. These techniques usually assume that most of the errors are caused by the substitution of one character for another <ref> [81, 136] </ref>, and both rule-based and probabilistic approaches have been applied to represent character confusion characteristics. Usually a training phase is used to estimate a probabilistic model for character confusion by supplying an OCR device with sample texts and tabulating error statistics.
Reference: [82] <author> M.A. </author> <title> Just and P.A. Carpenter. The psychology of reading and language comprehension. </title> <publisher> Allyn and Bacon, </publisher> <address> Newton, MA, </address> <year> 1987. </year>
Reference-contexts: Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired <ref> [130, 82, 36, 146, 150] </ref>. Developments in the field of OCR, computer vision, patterns recognition, as well as human reading have shown that the use of contextual information is more than an ad-hoc remedy to deficiencies in visual recognition.
Reference: [83] <author> Fukushima K and T. Imagawa. </author> <title> Recognition and segmentation of connected characters with selective attention. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(1) </volume> <pages> 33-41, </pages> <year> 1993. </year>
Reference-contexts: In another research, a neural network model was used to implement the concept of "selective attention" in human attention and reading, and was applied to compute the optimal segmentation <ref> [83] </ref>. Character or word-level linguistic constraints are usually used in the recognition-segmentation 40 CHAPTER 2. BACKGROUND stage. Contextual constraints could be represented in the form of a lexicon.
Reference: [84] <author> S. Kahan and T. Pavlidis. </author> <title> On the recognition of printed characters of any font and size. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> PAMI-9:274-287, </volume> <year> 1987. </year>
Reference-contexts: Image degradation in documents makes effective font training even more difficult. The need for large amounts of font training is a significant impediment to the development of high performance commercial recognition systems <ref> [84, 7, 5, 8, 174] </ref>. 4 CHAPTER 1. INTRODUCTION 1.1. THE MOTIVATION 5 6 CHAPTER 1. INTRODUCTION There have been extensive research efforts over the past three decades in seeking solutions to these two problems.
Reference: [85] <author> D. Kahneman and A. Henik. </author> <title> Perceptual organization and attention. </title> <editor> In M. Kubovy, B.R. Pomerantz, and D.R. Davis, editors, </editor> <title> Perceptual Organization. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year>
Reference-contexts: They often reflect other problems familiar to researchers in a few related branches of science. For decades figure-ground segregation (or image segmentation) has been an elusive problem that perplexed researchers in the fields of computer vision, cognitive science and psychology <ref> [107, 9, 145, 85, 35] </ref>. This difficulty in separating what is meaningful to recognition from what is irrelevant background noise lies at the core of the computational model that forms the base of today's computer vision systems [109].
Reference: [86] <author> R.L. Kashyap and B.J. Oommen. </author> <title> An effective algorithm for string correction using generalized edit distance. </title> <journal> Inf. Sci., </journal> <volume> 23 </volume> <pages> 123-142, </pages> <year> 1981. </year>
Reference-contexts: CHARACTER LEVEL DECIPHERING Research in spelling correction and OCR post-processing algorithms can provide useful hints about the construction of character confusion heuristics <ref> [94, 161, 86] </ref>. Most of the techniques used for text correction are based on studies of characteristics of spelling errors made by human beings [51], and therefore are not directly applicable to the correction of character identities achieved by character pattern clustering and deciphering.
Reference: [87] <author> S. Khoubyari and J.J. Hull. </author> <title> Font and function word identification in document recognition. Computer Vision, Graphics, </title> <booktitle> and Image Processing, </booktitle> <year> 1994. </year>
Reference-contexts: It is assumed that a partial font base that represents font information of at least part of the character alphabet is available either through an explicit font learning step <ref> [87] </ref> or from the results of a word-level deciphering algorithm that has recognized a portion of the input text [46]. Figure 5.9 in Chapter 5 illustrates the components involved in the proposed method.
Reference: [88] <author> H.S. Kim and K.H.Park. </author> <title> Shape decomposition based on perceptual structure. </title> <editor> In L. Shapiro and A. Rosenfeld, editors, </editor> <booktitle> Computer Vision and Image Processing, </booktitle> <pages> pages 363-383. </pages> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: Extensive research efforts related to the computational simulation of figure-ground segregation have been reported in the areas of perceptual organization, texture discrimination and shape decomposition <ref> [108, 128, 101, 14, 88] </ref>. But even today, knowledge of the mechanisms as well as their computational formulation are still at the very primitive stage. To make things even less optimistic, sometimes even the human brain 2.4.
Reference: [89] <author> F. Kimura, S. Tsuruoka, M. Shridhar, and Z. Chen. </author> <title> Lexicon directed segmentation-recognition procedure for unconstrained handwritten words. </title> <booktitle> In Proceedings IWFHR III, </booktitle> <pages> page 122, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Most dissection techniques for cursive script are based on the fact that lowercase characters are usually linked by lower ligatures. A simple way to locate these ligatures is to detect the minima of the upper outline of words. Examples of techniques along this line are given in <ref> [42, 105, 70, 89, 47] </ref>. 2.1. CHARACTER SEGMENTATION 39 Sennhauser [152] proposed a blackboard-based system which integrated three contextual knowledge sources to provide feedback and generate hypotheses for recognition and re-segmentation.
Reference: [90] <author> J. King and D. Bahler. </author> <title> An implementation of probabilistic relaxation in the cryptanalysis of simple substitution cipher. </title> <journal> Cryptologia, </journal> <note> XVI(3):215-225, 1992. 204 BIBLIOGRAPHY </note>
Reference-contexts: Later similar studies in applying a relaxation algorithm for breaking a substitution cipher resulted in improved updating rules [91, 79] and deciphering algorithms for substitution ciphers where word boundaries may have been concealed <ref> [79, 90] </ref>. Relaxation labeling was 2.3.
Reference: [91] <author> R. Kirby. </author> <title> A product rule relaxation method. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 13 </volume> <pages> 158-189, </pages> <year> 1982. </year>
Reference-contexts: Iterating the updating scheme results in improved estimates that finally lead to the breaking of the cipher (Figure 2.6). Later similar studies in applying a relaxation algorithm for breaking a substitution cipher resulted in improved updating rules <ref> [91, 79] </ref> and deciphering algorithms for substitution ciphers where word boundaries may have been concealed [79, 90]. Relaxation labeling was 2.3.
Reference: [92] <author> V.A. Kovalevsky. </author> <title> Character Readers and Pattern Recognition. </title> <publisher> Spartan Books, </publisher> <address> Wash. D.C., </address> <year> 1968. </year>
Reference-contexts: No feature-based dissection algorithm is employed. The image is divided systematically into many tentative overlapping pieces, from which the optimal character segmentation is to be derived from the recognition results. Early work on this method was described in <ref> [92, 25] </ref>, in which the concept of a moving window of variable width was used to produce tentative segmentation candidates, which were then confirmed or rejected based on character recognition results. Both serial and parallel optimization schemes were used. The serial implementation recognizes words iteratively in a left-to-right fashion.
Reference: [93] <author> H. Kucera and W. N. Francis. </author> <title> Computational Analysis of Present-day American English. </title> <publisher> Brown University Press, </publisher> <year> 1967. </year>
Reference-contexts: The language model for the deciphering algorithm consists of a dictionary and a set of character bigram statistics. The dictionary contained all the 53121 unique English words from the Brown Corpus, which is an English text corpus that contains about one million words of running text <ref> [93] </ref>. The character bigram statistics database was also compiled from the Brown Corpus. 4.6.2 Baseline for Performance Comparison The original Nagy's algorithm was implemented and run with input documents that contained no touching characters and no clustering mistakes. <p> The reasons for this are that more font information can be learned from the re-recognition stage, and also the most frequent 21 lower-case characters which are likely to be learned account for 96% of all text <ref> [93, 66] </ref>. Based on this observation, we propose a novel word-level deciphering algorithm that first solves a selected portion of the input text that has relatively more reliable word bigram statistics by using a word-level relaxation deciphering algorithm.
Reference: [94] <author> K. Kukich. </author> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference-contexts: CHARACTER LEVEL DECIPHERING Research in spelling correction and OCR post-processing algorithms can provide useful hints about the construction of character confusion heuristics <ref> [94, 161, 86] </ref>. Most of the techniques used for text correction are based on studies of characteristics of spelling errors made by human beings [51], and therefore are not directly applicable to the correction of character identities achieved by character pattern clustering and deciphering.
Reference: [95] <author> Solomon Kullback. </author> <title> Statistical methods in cryptanalysis. </title> <publisher> Aegean Park Press, </publisher> <address> Laguna Hills, California, </address> <year> 1976. </year>
Reference-contexts: BACKGROUND 2.3.1 Character n-grams and Dictionary Lookup One group of methods are based on using character n-gram statistics as constraints. Examples that use character bigram and trigram statistics are [157, 120]. Some other works also used a stationary Markov model, examples are <ref> [155, 57, 56, 95] </ref>. Two problems exist with using character n-gram statistics. First, a substantial amount of cipher text is needed for statistically reliable n-gram estimation.
Reference: [96] <author> A. Kundu, Y. He, and P. Bahl. </author> <title> Recognition of handwritten word: First and second order hidden markov model based approach. </title> <journal> Pattern Recognition, </journal> <volume> 22(3):283, </volume> <year> 1989. </year>
Reference-contexts: An HMM can be trained to learn the letter segmentation behavior because contextual constraints, such as word and letter frequencies, and syntactic rules, can be supported by transition probabilities between letter states. Examples of using HMMs for the optimal segmentation and recognition of text were reported in <ref> [60, 31, 151, 12, 96] </ref>. Non-HMM methods such as probabilistic relaxation and hypothesis testing were also used in recognition-based segmentation. Hayes used a probabilistic relaxation method to read off-line handwritten words [65]. The model worked on a hierarchical description of words derived from a skeletal representation.
Reference: [97] <author> P.T. Van Larrhoven and E.H. Aarts. </author> <title> Simulated Annealing: Theory and Practice. </title> <publisher> Kluwer Acedemic, </publisher> <address> Dordrecht, </address> <year> 1988. </year>
Reference-contexts: The algorithm starts from an initial configuration with a relatively large value of T . For a given value of T a number of transitions are considered and then T is lowered until it reaches zero. General information about simulated annealing and it various applications can be found in <ref> [97, 1] </ref>. Forsyth and Safavi-Naini [48, 49] proposed an elegant formulation of decoding a substitution cipher as a combinatorial optimization problem and suggested using a simulated annealing algorithm to find the optimal solution that corresponds to the breaking of the cipher.
Reference: [98] <author> E. Lecolinet. </author> <title> Segmentation d'images de mots manuscripts. </title> <type> PhD thesis, </type> <institution> Universite Pierre et Marie Curie, </institution> <year> 1990. </year>
Reference-contexts: These different points have been addressed by a number of authors. Parui [123] used contour analysis to help detect the likely segmentation points. Shridhar [153] proposed an algorithm for digit segmentation which not only detected likely segmentation points, but also computed the appropriate segmentation path. Lecolinet <ref> [99, 98] </ref> proposed a technique to detect connected components that have to be further segmented.
Reference: [99] <author> E. Lecolinet and J-V. Moreau. </author> <title> A new system for automatic segmentation and recognition of unconstrained zip codes. </title> <booktitle> In Proceedings of the 6th Scandinavian Conference on Image Analysis, </booktitle> <year> 1989. </year>
Reference-contexts: These different points have been addressed by a number of authors. Parui [123] used contour analysis to help detect the likely segmentation points. Shridhar [153] proposed an algorithm for digit segmentation which not only detected likely segmentation points, but also computed the appropriate segmentation path. Lecolinet <ref> [99, 98] </ref> proposed a technique to detect connected components that have to be further segmented.
Reference: [100] <author> M. Leroux, J-C. Salome, and J. Badard. </author> <title> Recognition of cursive script words in a small lexicon. </title> <booktitle> In Proc. Int. Conf. Document Analysis and Recognition, </booktitle> <pages> page 774, </pages> <year> 1991. </year>
Reference-contexts: Dynamic programming was employed in [129] for check recognition. Words were represented by a list of features indicating the presence of ascenders, descenders, directional strokes and closed loops. Paquet [122] and Leroux <ref> [100] </ref> used a similar scheme, but with different features. Features used in Paquet's method were based on the notion of "guiding points," which are the intersection of letters and the median line of the word. Whereas features used in Leroux's method were derived from the contours of word image patterns.
Reference: [101] <author> M.D. Levine. </author> <title> Vision in Man and Machine. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1985. </year> <note> BIBLIOGRAPHY 205 </note>
Reference-contexts: Extensive research efforts related to the computational simulation of figure-ground segregation have been reported in the areas of perceptual organization, texture discrimination and shape decomposition <ref> [108, 128, 101, 14, 88] </ref>. But even today, knowledge of the mechanisms as well as their computational formulation are still at the very primitive stage. To make things even less optimistic, sometimes even the human brain 2.4.
Reference: [102] <author> S. Liang, M. Ahmadi, and M. Shridhar. </author> <title> Segmentation of touching characters in printed document recognition. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition(ICDAR-93), </booktitle> <pages> pages 569-572, </pages> <year> 1993. </year>
Reference-contexts: This criterion avoids considering points along a thin horizontal line as potential segmentation points. Lu's algorithm [103] improved on this method by using a peak-to-valley function that favors low valleys with high peaks on both sides. 2.1. CHARACTER SEGMENTATION 35 Different pre-filtering methods were proposed by Tsujimoto and Liang <ref> [168, 102] </ref> that were intended to produce better projection for locating boundaries between touching characters. Moreover, the upper and lower profiles of the patterns were also used in combination with the projection to get improved segmentation point candidates [6].
Reference: [103] <author> Y. Lu. </author> <title> On the segmentation of touching characters. </title> <booktitle> In Proceedings of the Second International Conference on Document Analysis and Recognition ICDAR-93, </booktitle> <pages> pages 440-443, </pages> <year> 1993. </year>
Reference-contexts: Baird [6] proposed a method in which the ratio of the second derivative of the projection to its height was used as a criterion for choosing segmentation points. This criterion avoids considering points along a thin horizontal line as potential segmentation points. Lu's algorithm <ref> [103] </ref> improved on this method by using a peak-to-valley function that favors low valleys with high peaks on both sides. 2.1. CHARACTER SEGMENTATION 35 Different pre-filtering methods were proposed by Tsujimoto and Liang [168, 102] that were intended to produce better projection for locating boundaries between touching characters.
Reference: [104] <author> M. Lucks. </author> <title> A constraint satisfaction algorithm for the automated decryption of simple substitution ciphers. </title> <booktitle> In Advance in Cryptology, CRYPTO'88, </booktitle> <pages> pages 132-144, </pages> <year> 1988. </year>
Reference-contexts: In this deciphering framework, breaking a substitution cipher corresponds to a particular mapping from the set of character pattern clusters to the alphabetic letter set such that all the cipher-words are legitimate words in the dictionary. Among the published works that use dictionary lookup <ref> [23, 112, 114, 104, 134] </ref>, Casey [23] presented an algorithm based on adaptive optimization of the assignment with the help of pattern matching provided by an on-line spelling checker. <p> This algorithm showed some limited ability to handle two-character touching patterns, but it lacks the support for handling more extensive multi-character touching patterns. Lucks <ref> [104] </ref> proposed an automated algorithm that employs an exhaustive search in a large on-line dictionary for words that satisfy constraints on word length, letter position and letter repetition pattern.
Reference: [105] <author> M. Maier. </author> <title> Separating character in scripted documents. </title> <booktitle> In 8th IAPR Int. Conf. on Pattern Recognition, </booktitle> <pages> page 1056, </pages> <year> 1986. </year>
Reference-contexts: Most dissection techniques for cursive script are based on the fact that lowercase characters are usually linked by lower ligatures. A simple way to locate these ligatures is to detect the minima of the upper outline of words. Examples of techniques along this line are given in <ref> [42, 105, 70, 89, 47] </ref>. 2.1. CHARACTER SEGMENTATION 39 Sennhauser [152] proposed a blackboard-based system which integrated three contextual knowledge sources to provide feedback and generate hypotheses for recognition and re-segmentation.
Reference: [106] <author> J. Mantas. </author> <title> An overview of character recognition methodologies. </title> <journal> Pattern Recognition, </journal> <volume> 19(6) </volume> <pages> 425-430, </pages> <year> 1986. </year>
Reference-contexts: Introduction Ever since the emergence of computer technology in the 1940's, a long-standing objective has been to build automatic reading machines with the power, tolerance and efficiency of a human reader <ref> [20, 156, 64, 111, 106, 63] </ref>. Research on computer document recognition has been an important part in the efforts to understand human reading and build automatic systems with matching performance.
Reference: [107] <author> David Marr. </author> <title> Vision. </title> <address> W.H.Freeman, San Francisco, </address> <year> 1982. </year>
Reference-contexts: They often reflect other problems familiar to researchers in a few related branches of science. For decades figure-ground segregation (or image segmentation) has been an elusive problem that perplexed researchers in the fields of computer vision, cognitive science and psychology <ref> [107, 9, 145, 85, 35] </ref>. This difficulty in separating what is meaningful to recognition from what is irrelevant background noise lies at the core of the computational model that forms the base of today's computer vision systems [109]. <p> long been troubled by the fundamental difficulty of having a generic representation for a certain object that is concise so that it is computationally feasible, while at the same time is general and sufficient enough to capture the rich varieties of visual patterns of the object in the real world <ref> [107, 110] </ref>. This is an issue of concern not only for designing general theoretical frameworks in computer vision, but also for developing commercial computer vision systems for specific task domains. <p> The two major difficult problems of character segmentation and font training, for which we seek to find solutions in this thesis, also find close counterparts in computational vision. In his formulation of a computational approach to vision, Marr <ref> [107] </ref> proposed a three-level methodology for the understanding of any machine system that carries out an information processing task. As shown in Figure 1.11, when trying to understand a computational task, it is important to seek answers separately at different levels of abstraction. <p> It will be interesting to look at the issues at a higher level and ask ourself the question of what this research has contributed to the general understanding of document recognition as a computational task. Using the term of the three-level methodology proposed by Marr <ref> [107] </ref>, the contributions of this thesis to the understanding of document recognition as a computational process are mainly on the representational and algorithm level. The uniqueness of considering document recognition as a deciphering process is that visual information and linguistic constraints are treated with equal significance in the recognition task. <p> It has to do with the fundamental difficulty of having a generic representation that is concise so that it is computationally feasible, while at the same time is also powerful and sufficient enough to cover the rich varieties of different visual patterns and their variations in the real world <ref> [107, 110] </ref>. Visual representation has been a challenging issue in vision research. Gibson's ecological approach to vision represents a systematic effort to address the problem of visual representation.
Reference: [108] <author> J.D. McCafferty. </author> <title> Human and Machine Vision: Computing Perceptual Organization. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1990. </year>
Reference-contexts: Extensive research efforts related to the computational simulation of figure-ground segregation have been reported in the areas of perceptual organization, texture discrimination and shape decomposition <ref> [108, 128, 101, 14, 88] </ref>. But even today, knowledge of the mechanisms as well as their computational formulation are still at the very primitive stage. To make things even less optimistic, sometimes even the human brain 2.4.
Reference: [109] <author> Marvin Minsky. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1968. </year>
Reference-contexts: This difficulty in separating what is meaningful to recognition from what is irrelevant background noise lies at the core of the computational model that forms the base of today's computer vision systems <ref> [109] </ref>. Character segmentation is just a reflection of this fundamental difficulty in the task of document recognition.
Reference: [110] <author> Marvin Minsky. </author> <title> A framework for representing knowledge. </title> <editor> In P.H Winston, editor, </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: long been troubled by the fundamental difficulty of having a generic representation for a certain object that is concise so that it is computationally feasible, while at the same time is general and sufficient enough to capture the rich varieties of visual patterns of the object in the real world <ref> [107, 110] </ref>. This is an issue of concern not only for designing general theoretical frameworks in computer vision, but also for developing commercial computer vision systems for specific task domains. <p> It has to do with the fundamental difficulty of having a generic representation that is concise so that it is computationally feasible, while at the same time is also powerful and sufficient enough to cover the rich varieties of different visual patterns and their variations in the real world <ref> [107, 110] </ref>. Visual representation has been a challenging issue in vision research. Gibson's ecological approach to vision represents a systematic effort to address the problem of visual representation.
Reference: [111] <author> G. Nagy. </author> <title> Optical character recognition: theory and practice. </title> <editor> In P.R.Krishnaiah and L.N.Kanal, editors, </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> volume 2, </volume> <pages> pages 621-649. </pages> <year> 1982. </year>
Reference-contexts: Introduction Ever since the emergence of computer technology in the 1940's, a long-standing objective has been to build automatic reading machines with the power, tolerance and efficiency of a human reader <ref> [20, 156, 64, 111, 106, 63] </ref>. Research on computer document recognition has been an important part in the efforts to understand human reading and build automatic systems with matching performance.
Reference: [112] <author> G. Nagy. </author> <title> Efficient algorithms to decode substitution cipher with application to OCR. </title> <booktitle> In IEEE Proceedings, </booktitle> <pages> pages 352-355, </pages> <year> 1986. </year>
Reference-contexts: In this deciphering framework, breaking a substitution cipher corresponds to a particular mapping from the set of character pattern clusters to the alphabetic letter set such that all the cipher-words are legitimate words in the dictionary. Among the published works that use dictionary lookup <ref> [23, 112, 114, 104, 134] </ref>, Casey [23] presented an algorithm based on adaptive optimization of the assignment with the help of pattern matching provided by an on-line spelling checker. <p> They also suggested the use of letter frequencies to further cut down candidates for cipher words after the dictionary search. Nagy's work on decoding a substitution cipher using dictionary lookup represents the most systematic treatment of this problem <ref> [112, 114] </ref>. The concept of "cover set" was proposed to 2.3. CHARACTER RECOGNITION AS SUBSTITUTION CIPHER DECODING 49 exploit the powerful constraints of character repetition patterns distributed across the text passage. <p> COMPUTATIONAL FRAMEWORK 3.1 Character Level Deciphering A modified character-level deciphering algorithm is proposed that recognizes characters or touching characters by combining character level linguistic constraints with visual constraints from the input document (Figure 3.2). The modifications are based on an Breadth-First Search deciphering algorithm proposed by Nagy <ref> [112] </ref>. <p> This allows the deciphering algorithm to recognize touching characters and to detect and reverse clustering mistakes. This results in a deciphering algorithm that has robust performance under image degradation. 68 CHAPTER 4. CHARACTER LEVEL DECIPHERING 4.1 Introduction Character-level deciphering algorithms have been proposed for OCR in the past <ref> [20, 23, 112, 114, 127] </ref>. These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code [175, 24]. <p> A technique that is representative of the many proposed character-level deciphering algorithms is that presented by Nagy <ref> [112] </ref>. This algorithm solves a substitution cipher by extending a tentative assignment of letters to character patterns according to the degree of match between the decrypted portion of the cipher text and a vocabulary of common words. <p> No attempt has been made to also use visual constraints in the deciphering process to improve the performance. Our modifications to a character-level deciphering algorithm are intended to address the above issues. Starting with the algorithm NONMATCHES proposed by Nagy in <ref> [112] </ref>, the proposed modifications integrate visual constraints of character patterns extracted from the text page and linguistic constraints provided by a language model to decode the identities of characters and touching patterns. <p> Short touching patterns are solved by matching the current cipher word against the dictionary with the assumption that it might include touching patterns each consisting of an estimated number of single characters, and resolving the multiple matches with constraints from the cover set <ref> [112] </ref>. The cover set of a cipher word consists of a group of other unsolved cipher words from the same document page whose identities are dependent on the decryption of the current cipher word. <p> HANDLING CLUSTERING MISTAKES 89 90 CHAPTER 4. CHARACTER LEVEL DECIPHERING 4.5 The Modified Algorithm An outline of the modified character-level deciphering algorithm is shown in Figure 4.11. The overall framework of a character level deciphering approach is inherited from the algorithm NONMATCHES originally proposed by Nagy in <ref> [112] </ref>. In addition to the functionalities of the original algorithm, this modified algorithm includes the ability to detect and label potential touching patterns and unusual long touching patterns. The dictionary matching algorithm is generalized to handle touching patterns as well. <p> A program was developed that implemented the Nagy NONMATCHES deciphering algorithm on a text input. It includes clustering based on character identities followed by the original NONMATCHES method. We added a few things that were not included in the NON-MATCHES algorithm presented in <ref> [112] </ref>. The character bigram information compiled from the language base was used to resolve multiple competing results from dictionary matching. Multiple matching results happen when there is no cover set constraint or not enough cover set constraints are available. This often happens when deciphering is starting.
Reference: [113] <author> G. Nagy. </author> <booktitle> At the frontiers of OCR. Proceedings of the IEEE, </booktitle> <volume> 80(7), </volume> <year> 1992. </year> <note> 206 BIBLIOGRAPHY </note>
Reference-contexts: Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. The first is the difficulty in dealing with touching characters and character fragmentation that are abundant in real-world documents as a result of document degradations <ref> [26, 15, 30, 142, 113, 22] </ref>. Examples of touching characters and character fragmentation in documents are shown in Figure 1.2 and Figure 1.3. <p> Despite many satisfactory results reported for constrained domains, character segmentation in the presence of image degradation still remains one of the most difficult problems in document recognition and the weakest link in a practical OCR system <ref> [30, 141, 113, 22] </ref>. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 11 12 CHAPTER 1. INTRODUCTION acter over-segmentation. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 13 character under-segmentation. 14 CHAPTER 1. INTRODUCTION 1.3 Contextual Linguistic Information in Document Recogni tion Document recognition has been considered a sub-area in the broader field of computer vision. <p> Another major problem in recognizing degraded words is caused by the occurrence of touching characters and character fragmentation, which usually pose extreme difficulty to recognition algorithms that are based on character segmentation <ref> [113, 26, 22, 163, 39] </ref>. Many algorithms had been proposed in the past for recognizing degraded words.
Reference: [114] <author> G. Nagy, S. Seth, and K. Einspahr. </author> <title> Decoding substitution cipher by means of word matching with application to OCR. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> PAMI-9(5):710-715, </volume> <year> 1987. </year>
Reference-contexts: In this deciphering framework, breaking a substitution cipher corresponds to a particular mapping from the set of character pattern clusters to the alphabetic letter set such that all the cipher-words are legitimate words in the dictionary. Among the published works that use dictionary lookup <ref> [23, 112, 114, 104, 134] </ref>, Casey [23] presented an algorithm based on adaptive optimization of the assignment with the help of pattern matching provided by an on-line spelling checker. <p> They also suggested the use of letter frequencies to further cut down candidates for cipher words after the dictionary search. Nagy's work on decoding a substitution cipher using dictionary lookup represents the most systematic treatment of this problem <ref> [112, 114] </ref>. The concept of "cover set" was proposed to 2.3. CHARACTER RECOGNITION AS SUBSTITUTION CIPHER DECODING 49 exploit the powerful constraints of character repetition patterns distributed across the text passage. <p> This allows the deciphering algorithm to recognize touching characters and to detect and reverse clustering mistakes. This results in a deciphering algorithm that has robust performance under image degradation. 68 CHAPTER 4. CHARACTER LEVEL DECIPHERING 4.1 Introduction Character-level deciphering algorithms have been proposed for OCR in the past <ref> [20, 23, 112, 114, 127] </ref>. These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code [175, 24].
Reference: [115] <author> R. Nagy, K.H. Wong, and F. Fallside. </author> <title> Script recognition using hidden markov models. </title> <booktitle> In IEEE ICASSP, </booktitle> <pages> pages 2071-2074, </pages> <year> 1986. </year>
Reference-contexts: Whereas features used in Leroux's method were derived from the contours of word image patterns. Hidden Markov Models are also used in holistic word recognition <ref> [115, 61] </ref>. Angular representation is used in Nagy's system to represent the feature, while structural image primitives are used in Gilloux's system. Hull [74] proposed a holistic approach to recognize machine-printed text. Here the method is used to recognize commonly occuring words in a document.
Reference: [116] <author> T.A. Nartker. </author> <note> 1992 Annual Report of ISRI. </note> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <year> 1992. </year>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [117] <author> T.A. Nartker. </author> <note> 1993 Annual Report of ISRI. </note> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <year> 1993. </year>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [118] <author> T.A. Nartker. </author> <note> 1994 Annual Report of ISRI. </note> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <year> 1994. </year>
Reference-contexts: The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [119] <author> K. Ohta, I. Kaneko, Y. Itamoto, and Y. Nishijima. </author> <title> Character segmentation of address reading/letter sorting machine. </title> <journal> NEC Research and Development, </journal> <volume> 34(2) </volume> <pages> 248-256, </pages> <year> 1993. </year>
Reference-contexts: It can help detect white space between characters. It can also help to locate vertical strokes. For example, it was used in <ref> [119] </ref> to segment Kanji hand-printed address. Columns where the projection fell below a predefined threshold were considered candidates for segmentation points. In the case of touching characters or text lines where neighboring characters overlap horizontally, the projection often contains a minimum at the proper segmentation points.
Reference: [120] <author> B. Oommen and J. Zgierski. </author> <title> Breaking substitution ciphers using Stochastic Automata. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> PAMI-15(2), </volume> <year> 1993. </year>
Reference-contexts: CHARACTER RECOGNITION AS SUBSTITUTION CIPHER DECODING 47 48 CHAPTER 2. BACKGROUND 2.3.1 Character n-grams and Dictionary Lookup One group of methods are based on using character n-gram statistics as constraints. Examples that use character bigram and trigram statistics are <ref> [157, 120] </ref>. Some other works also used a stationary Markov model, examples are [155, 57, 56, 95]. Two problems exist with using character n-gram statistics. First, a substantial amount of cipher text is needed for statistically reliable n-gram estimation.
Reference: [121] <author> H. Ouladj, G. Lorette, and E. Petit. </author> <title> From primitives to letters: A structural method to automatic cursive handwriting recognition. </title> <booktitle> In Proceedings of the 6th Scandinavian Conference on Image Analysis, </booktitle> <year> 1989. </year>
Reference-contexts: Relaxation was performed on the stroke graph and on the letter graph where all the possible segmentations were kept. Letter n-gram statistics were used to rule out unlikely combinations. Examples of using a hypothesis testing and verification algorithm to recognized hand-printed or on-line cursive words were reported in <ref> [75, 75, 13, 121] </ref> 2.2. WORD RECOGNITION WITHOUT CHARACTER SEGMENTATION 41 2.2 Word Recognition Without Character Segmentation The extensiveness of research reported in designing competent character segmentation algorithms to deal with degraded documents also reflects the difficulty in achieving this goal.
Reference: [122] <author> T. Paquet and Y. Lecourtier. </author> <title> Handwriting recognition: Application on bank cheques. </title> <booktitle> In Proc. Int. Conf. Document Analysis and Recognition, </booktitle> <pages> page 749, </pages> <year> 1991. </year>
Reference-contexts: Dynamic programming was employed in [129] for check recognition. Words were represented by a list of features indicating the presence of ascenders, descenders, directional strokes and closed loops. Paquet <ref> [122] </ref> and Leroux [100] used a similar scheme, but with different features. Features used in Paquet's method were based on the notion of "guiding points," which are the intersection of letters and the median line of the word.
Reference: [123] <author> S.K. Parui, B.B. Chaudhri, </author> <title> and D.D. Majumder. A procedure for recognition of connected handwritten numerals. </title> <journal> Int. J. System Science, </journal> <volume> 13(9) </volume> <pages> 1019-1029, </pages> <year> 1982. </year>
Reference-contexts: Accurate segmentation for these cases needs more detailed analysis and knowledge of the shape of the patterns to be split to determine the appropriate segmentation path. These different points have been addressed by a number of authors. Parui <ref> [123] </ref> used contour analysis to help detect the likely segmentation points. Shridhar [153] proposed an algorithm for digit segmentation which not only detected likely segmentation points, but also computed the appropriate segmentation path. Lecolinet [99, 98] proposed a technique to detect connected components that have to be further segmented.
Reference: [124] <author> T. Pavlidis. </author> <title> Problems in the recognition of poorly printed text. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 162-173, </pages> <year> 1992. </year>
Reference-contexts: Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents <ref> [124, 138, 140, 139, 137] </ref>. Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. <p> RE-RECOGNITION BY HYPOTHESIS TESTING 6.1 Introduction Recognition of degraded words has been a difficult problem in OCR. One of the major problems caused by image degradation is the increase in character shape deformation that makes character classification more difficult <ref> [124] </ref>. Another major problem in recognizing degraded words is caused by the occurrence of touching characters and character fragmentation, which usually pose extreme difficulty to recognition algorithms that are based on character segmentation [113, 26, 22, 163, 39]. Many algorithms had been proposed in the past for recognizing degraded words.
Reference: [125] <author> S. Peleg. </author> <title> Ambiguity reduction in handwriting with ambiguous segmentation and uncertain interpretation. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 10 </volume> <pages> 235-245, </pages> <month> July </month> <year> 1979. </year> <note> BIBLIOGRAPHY 207 </note>
Reference-contexts: The process is iterated until a satisfactory classification is achieved. A new formulation of relaxation based on probability theory <ref> [126, 125] </ref> paves the way for more general application of relaxation algorithms. 50 CHAPTER 2. BACKGROUND Peleg and Rosenfeld developed an approach that uses a relaxation labeling algorithm to break a substitution cipher with application to OCR [127].
Reference: [126] <author> S. Peleg. </author> <title> A new probabilistic relaxation scheme. </title> <booktitle> In IEEE Conf. on Pattern Recognition and Image Processing, </booktitle> <pages> pages 337-343, </pages> <address> Chicago, </address> <month> August </month> <year> 1979. </year>
Reference-contexts: Our proposed approaches are aimed at investigating solutions to the above problems. 2.3.2 Deciphering as Relaxation Labeling and Application to OCR Relaxation algorithms have been applied to image processing in the past <ref> [126, 144, 176] </ref>. They are iterative parallel classification algorithms that estimate the class membership probabilities of each element in a graph structure based on those of its neighbors and the constraints that exist among the neighbors. The process is iterated until a satisfactory classification is achieved. <p> The process is iterated until a satisfactory classification is achieved. A new formulation of relaxation based on probability theory <ref> [126, 125] </ref> paves the way for more general application of relaxation algorithms. 50 CHAPTER 2. BACKGROUND Peleg and Rosenfeld developed an approach that uses a relaxation labeling algorithm to break a substitution cipher with application to OCR [127].
Reference: [127] <author> S. Peleg and A. Rosenfeld. </author> <title> Breaking substitution ciphers using a relaxation algorithm. </title> <journal> Communications of the ACM, </journal> <volume> 22 </volume> <pages> 598-605, </pages> <year> 1979. </year>
Reference-contexts: A new formulation of relaxation based on probability theory [126, 125] paves the way for more general application of relaxation algorithms. 50 CHAPTER 2. BACKGROUND Peleg and Rosenfeld developed an approach that uses a relaxation labeling algorithm to break a substitution cipher with application to OCR <ref> [127] </ref>. In this approach, every cipher text code is assigned probabilities of representing plain-text letters. These probabilities are updated in parallel for all cipher-text codes, using joint letter probabilities (trigrams). Iterating the updating scheme results in improved estimates that finally lead to the breaking of the cipher (Figure 2.6). <p> This allows the deciphering algorithm to recognize touching characters and to detect and reverse clustering mistakes. This results in a deciphering algorithm that has robust performance under image degradation. 68 CHAPTER 4. CHARACTER LEVEL DECIPHERING 4.1 Introduction Character-level deciphering algorithms have been proposed for OCR in the past <ref> [20, 23, 112, 114, 127] </ref>. These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code [175, 24].
Reference: [128] <author> A.P. Pentland. </author> <title> Perceptual organization and the representation of natural form. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 293-331, </pages> <year> 1986. </year>
Reference-contexts: Extensive research efforts related to the computational simulation of figure-ground segregation have been reported in the areas of perceptual organization, texture discrimination and shape decomposition <ref> [108, 128, 101, 14, 88] </ref>. But even today, knowledge of the mechanisms as well as their computational formulation are still at the very primitive stage. To make things even less optimistic, sometimes even the human brain 2.4.
Reference: [129] <author> B. Plessis et al. </author> <title> Isolated handwritten word recognition for contextual address reading. </title> <booktitle> In USPS 5th Advanced Technology Conference, </booktitle> <pages> page 579, </pages> <year> 1992. </year>
Reference-contexts: BACKGROUND between hypotheses and references are performed. Recent comparison techniques are more flexible and account the word pattern variabilities better. These techniques are generally based on dynamic programming algorithms with optimization criteria based either on distance measurements or on a probabilistic framework. Dynamic programming was employed in <ref> [129] </ref> for check recognition. Words were represented by a list of features indicating the presence of ascenders, descenders, directional strokes and closed loops. Paquet [122] and Leroux [100] used a similar scheme, but with different features.
Reference: [130] <author> A. Pollatsek and K. Rayner. </author> <title> Reading. In M.I. Posner, editor, </title> <booktitle> Foundations of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired <ref> [130, 82, 36, 146, 150] </ref>. Developments in the field of OCR, computer vision, patterns recognition, as well as human reading have shown that the use of contextual information is more than an ad-hoc remedy to deficiencies in visual recognition.
Reference: [131] <author> W. Prinzmetal. </author> <title> Principles of feature integration in visula perception. </title> <journal> Perception and Psychophysics, </journal> <volume> 30 </volume> <pages> 330-340, </pages> <year> 1981. </year>
Reference-contexts: CONCLUSIONS AND FUTURE DIRECTIONS 109 they could be very useful in differentiating similar character patterns when the pixel representation produces conflicting results. We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision <ref> [166, 165, 131, 169] </ref>, as well as on neural physiological studies of features detection and representation in primate visual cortex [72, 71, 10, 18, 37]. This hierarchical feature representation is computed from the skeleton representation of a character.
Reference: [132] <author> Z.W. </author> <title> Pylyshyn. </title> <booktitle> Computing in cognitive science. In M.I. Posner, editor, Foundations of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The seemingly effortlessness nature of human reading has allured many scholars into grossly under-estimating the complexity and difficulties in building machine systems for document recognition <ref> [154, 132] </ref>. Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents [124, 138, 140, 139, 137].
Reference: [133] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <year> 1986. </year>
Reference-contexts: New statistical tools and language models such as the Hidden Markov Model and grammatical parsers that were developed and achieved successes in speech recognition and natural language understanding were also introduced into OCR <ref> [133, 135] </ref>. More recent developments include incorporating linguistic processing within the recognition process itself, not only to post-process results produced by visual character classification, but also to generate new character or word candidates [152].
Reference: [134] <author> R. Ramesh, G. Athithan, and K. Thiruvengadam. </author> <title> An automated approach to solve simple substitution ciphers. </title> <journal> Cryptologia, </journal> <volume> XVII(2):202-217, </volume> <year> 1993. </year>
Reference-contexts: In this deciphering framework, breaking a substitution cipher corresponds to a particular mapping from the set of character pattern clusters to the alphabetic letter set such that all the cipher-words are legitimate words in the dictionary. Among the published works that use dictionary lookup <ref> [23, 112, 114, 104, 134] </ref>, Casey [23] presented an algorithm based on adaptive optimization of the assignment with the help of pattern matching provided by an on-line spelling checker. <p> Lucks [104] proposed an automated algorithm that employs an exhaustive search in a large on-line dictionary for words that satisfy constraints on word length, letter position and letter repetition pattern. A similar method was presented by Ramesh et al. <ref> [134] </ref> that exploits letter repetition patterns in cipher words and uses a dictionary search that matches the repetition patterns. They also suggested the use of letter frequencies to further cut down candidates for cipher words after the dictionary search.
Reference: [135] <author> J. Raviv. </author> <title> Decision making in markov chains applied to the problem of pattern recognition. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-13(3):536-551, </volume> <year> 1967. </year>
Reference-contexts: New statistical tools and language models such as the Hidden Markov Model and grammatical parsers that were developed and achieved successes in speech recognition and natural language understanding were also introduced into OCR <ref> [133, 135] </ref>. More recent developments include incorporating linguistic processing within the recognition process itself, not only to post-process results produced by visual character classification, but also to generate new character or word candidates [152].
Reference: [136] <author> J.R. Rhyne and C.G. Wolf. </author> <title> Recognition-based user interfaces. </title> <editor> In H.R. Hartson and D. Hix, editors, </editor> <booktitle> Advances in Human-Computer Interaction. </booktitle> <publisher> Ablex, </publisher> <address> Norwood, N.J., </address> <year> 1993. </year> <note> 208 BIBLIOGRAPHY </note>
Reference-contexts: However, some techniques that were especially designed for the correction of OCR errors might be helpful. These techniques usually assume that most of the errors are caused by the substitution of one character for another <ref> [81, 136] </ref>, and both rule-based and probabilistic approaches have been applied to represent character confusion characteristics. Usually a training phase is used to estimate a probabilistic model for character confusion by supplying an OCR device with sample texts and tabulating error statistics.
Reference: [137] <author> S. V. Rice, J. Kanai, and T. A. Nartker. </author> <title> A report on the accuracy of OCR devices. </title> <type> ISRI Technical Report TR-92-02, </type> <institution> University of Nevada, </institution> <year> 1992. </year>
Reference-contexts: Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents <ref> [124, 138, 140, 139, 137] </ref>. Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons.
Reference: [138] <author> S.V. Rice, F.R. Jenkins, and T.A. Nartker. </author> <booktitle> The fourth annual test of OCR accuracy. In 1995 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 11-50, </pages> <year> 1995. </year>
Reference-contexts: Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents <ref> [124, 138, 140, 139, 137] </ref>. Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons.
Reference: [139] <author> S.V. Rice, J. Kanai, and T.A. Nartker. </author> <title> An evaluation of OCR accuracy. </title> <booktitle> In 1993 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 9-34, </pages> <year> 1993. </year>
Reference-contexts: Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents <ref> [124, 138, 140, 139, 137] </ref>. Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons.
Reference: [140] <author> S.V. Rice, J. Kanai, and T.A. Nartker. </author> <booktitle> The third annual test of OCR accuracy. In 1994 Annual Report of ISRI, </booktitle> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <pages> pages 11-40, </pages> <year> 1994. </year>
Reference-contexts: Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents <ref> [124, 138, 140, 139, 137] </ref>. Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons.
Reference: [141] <author> J. Rocha and T. Pavlidis. </author> <title> A new method for word recognition without segmentation. </title> <booktitle> In Proceedings of EI, </booktitle> <year> 1993. </year>
Reference-contexts: Despite many satisfactory results reported for constrained domains, character segmentation in the presence of image degradation still remains one of the most difficult problems in document recognition and the weakest link in a practical OCR system <ref> [30, 141, 113, 22] </ref>. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 11 12 CHAPTER 1. INTRODUCTION acter over-segmentation. 1.2. DIFFICULTY IN CHARACTER SEGMENTATION 13 character under-segmentation. 14 CHAPTER 1. INTRODUCTION 1.3 Contextual Linguistic Information in Document Recogni tion Document recognition has been considered a sub-area in the broader field of computer vision. <p> Confidence in each recognized character is based on the number and nature of the features found in the image. Figure 2.4 illustrates the basic concepts in this approach. A similar approach was proposed by Rocha <ref> [141, 143] </ref> in which each subgraph of features that matches a previously defined character prototype is recognized anywhere in the word even if it corresponds to a broken character or to a character touching another one.
Reference: [142] <author> J. Rocha and T. Pavlidis. </author> <title> A solution to the problem of touching and broken characters. </title> <booktitle> In Proceedings of the International Conference on Document Analysis and Recognition(ICDAR-93), </booktitle> <pages> pages 602-605, </pages> <year> 1993. </year>
Reference-contexts: Research and experiments have shown that the performance breakdown of commercial document recognition systems under real application situations is caused mainly by two reasons. The first is the difficulty in dealing with touching characters and character fragmentation that are abundant in real-world documents as a result of document degradations <ref> [26, 15, 30, 142, 113, 22] </ref>. Examples of touching characters and character fragmentation in documents are shown in Figure 1.2 and Figure 1.3. <p> The recent years have also witnessed the emergence of a growing number of commercial recognition systems that deliver limited solutions to these two problems to a certain extent. Nevertheless, breakthroughs in the general solution to these two problems have been few <ref> [27, 28, 142, 39, 3, 116, 117, 118] </ref>. Most of the past efforts were focused on exploring alternative solutions to character segmentation, trying to improve segmentation results in some special application areas or under specific conditions.
Reference: [143] <author> J. Rocha and T. Pavlidis. </author> <title> Character recognition without segmentation. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> 17(9) </volume> <pages> 903-909, </pages> <year> 1995. </year>
Reference-contexts: Confidence in each recognized character is based on the number and nature of the features found in the image. Figure 2.4 illustrates the basic concepts in this approach. A similar approach was proposed by Rocha <ref> [141, 143] </ref> in which each subgraph of features that matches a previously defined character prototype is recognized anywhere in the word even if it corresponds to a broken character or to a character touching another one.
Reference: [144] <author> A. Rosenfeld, R. A. Hummel, and S. W. Zucker. </author> <title> Scene labeling by relaxation operations. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-6(6):420-433, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Our proposed approaches are aimed at investigating solutions to the above problems. 2.3.2 Deciphering as Relaxation Labeling and Application to OCR Relaxation algorithms have been applied to image processing in the past <ref> [126, 144, 176] </ref>. They are iterative parallel classification algorithms that estimate the class membership probabilities of each element in a graph structure based on those of its neighbors and the constraints that exist among the neighbors. The process is iterated until a satisfactory classification is achieved. <p> Probabilistic relaxation algorithms are iterative parallel classification algorithms where every element tries to estimate its class membership probabilities based on those of its neighbors. The process is iterated until a satisfactory classification is achieved. Detailed discussion of probabilistic relaxation algorithms can be found in <ref> [144] </ref>. The initial membership probabilities can usually be provided by preliminary recognition of each individual object. Because of its ability of exploiting contextual constraints, probabilistic relaxation algorithms have also shown their effectiveness in OCR.
Reference: [145] <author> A. Rosenfeld and A. Kak. </author> <title> Digital Picture Processing. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: They often reflect other problems familiar to researchers in a few related branches of science. For decades figure-ground segregation (or image segmentation) has been an elusive problem that perplexed researchers in the fields of computer vision, cognitive science and psychology <ref> [107, 9, 145, 85, 35] </ref>. This difficulty in separating what is meaningful to recognition from what is irrelevant background noise lies at the core of the computational model that forms the base of today's computer vision systems [109].
Reference: [146] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> An interactive activation model of context effects in letter perception: Part 2. </title> <journal> Psychological Review, </journal> <volume> 89 </volume> <pages> 60-94, </pages> <year> 1982. </year>
Reference-contexts: Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired <ref> [130, 82, 36, 146, 150] </ref>. Developments in the field of OCR, computer vision, patterns recognition, as well as human reading have shown that the use of contextual information is more than an ad-hoc remedy to deficiencies in visual recognition.
Reference: [147] <author> G. Salton. </author> <title> Automatic text processing. </title> <publisher> Addison Wesley, </publisher> <year> 1988. </year>
Reference-contexts: This vocabulary matching algorithm is executed on the sets of word candidate neighborhoods of an input document. Similarities between the input document and each document in the database are determined by using a modified vector space model <ref> [147] </ref> for information retrieval. The top N documents in the database that are most similar to the input document are selected to form the matching language database of the input document, from which the word bigram statistics is compiled and used by the word-level probabilistic relaxation algorithm.
Reference: [148] <author> K.M. Sayre. </author> <title> Machine recognition of handwritten words: a project report. </title> <journal> Pattern Recognition, </journal> <volume> 5 </volume> <pages> 213-228, </pages> <year> 1973. </year> <note> BIBLIOGRAPHY 209 </note>
Reference-contexts: A following recognition stage modifies the pre-segmentation decisions by using a mapping function that uses contextual constraints to combine or split the pre-segmented "graphemes." An example of applying grapheme method for the segmentation of off-line cursive script is reported in <ref> [148] </ref>. Most dissection techniques for cursive script are based on the fact that lowercase characters are usually linked by lower ligatures. A simple way to locate these ligatures is to detect the minima of the upper outline of words.
Reference: [149] <author> J. Schurmann et al. </author> <title> From pixels to contents. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1101-1119, </pages> <year> 1992. </year>
Reference-contexts: Many algorithms had been proposed in the past for recognizing degraded words. The more recent and advanced algorithms in this area are based on integrating character segmentation with character classification and using the character or word recognition results as feedbacks to alter or improve segmentation decisions made earlier <ref> [53, 17, 25, 167, 149] </ref>.
Reference: [150] <author> M.W. Schustack, S.F. Ehrlich, and K. Rayner. </author> <title> The complexity of contextual facilitation in reading: Local and global influences. </title> <journal> Journal of Memory and Language, </journal> <volume> 26 </volume> <pages> 322-340, </pages> <year> 1987. </year>
Reference-contexts: Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired <ref> [130, 82, 36, 146, 150] </ref>. Developments in the field of OCR, computer vision, patterns recognition, as well as human reading have shown that the use of contextual information is more than an ad-hoc remedy to deficiencies in visual recognition.
Reference: [151] <author> A.W. Senior and F. Fallside. </author> <title> An off-line cursive script recognition system using recurrent error propagation networks. </title> <booktitle> In Proceedings IWFHR III, </booktitle> <pages> page 132, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An HMM can be trained to learn the letter segmentation behavior because contextual constraints, such as word and letter frequencies, and syntactic rules, can be supported by transition probabilities between letter states. Examples of using HMMs for the optimal segmentation and recognition of text were reported in <ref> [60, 31, 151, 12, 96] </ref>. Non-HMM methods such as probabilistic relaxation and hypothesis testing were also used in recognition-based segmentation. Hayes used a probabilistic relaxation method to read off-line handwritten words [65]. The model worked on a hierarchical description of words derived from a skeletal representation.
Reference: [152] <author> R. Sennhauser. </author> <title> Integration of contextual knowledge sources into a blackboard-based text recognition system. </title> <booktitle> In Proceedings of IAPR workshop on Document Analysis systems, </booktitle> <pages> pages 211-228, </pages> <year> 1994. </year>
Reference-contexts: More recent developments include incorporating linguistic processing within the recognition process itself, not only to post-process results produced by visual character classification, but also to generate new character or word candidates <ref> [152] </ref>. Using linguistic constraints and incorporating them into visual recognition has helped to disambiguate similar candidates and achieved better segmentation and recognition results, and has therefore 16 CHAPTER 1. INTRODUCTION greatly increased the performance of document recognition systems when applied to degraded documents. <p> A simple way to locate these ligatures is to detect the minima of the upper outline of words. Examples of techniques along this line are given in [42, 105, 70, 89, 47]. 2.1. CHARACTER SEGMENTATION 39 Sennhauser <ref> [152] </ref> proposed a blackboard-based system which integrated three contextual knowledge sources to provide feedback and generate hypotheses for recognition and re-segmentation. The three knowledge sources include a spelling checker, a general letter n-gram statistics model, and another specific n-gram model for the processing of numbers and digits.
Reference: [153] <author> M. Shridhar and A. Badreldin. </author> <title> Recognition of connected and simple connected handwritten numerals. </title> <journal> Pattern Recognition, </journal> <volume> 19(1):1, </volume> <year> 1986. </year>
Reference-contexts: These different points have been addressed by a number of authors. Parui [123] used contour analysis to help detect the likely segmentation points. Shridhar <ref> [153] </ref> proposed an algorithm for digit segmentation which not only detected likely segmentation points, but also computed the appropriate segmentation path. Lecolinet [99, 98] proposed a technique to detect connected components that have to be further segmented.
Reference: [154] <author> H.A. Simon and C.A. Kaplan. </author> <booktitle> Foundations of cognitive science. In M.I. Posner, editor, Foundations of Cognitive Science. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The seemingly effortlessness nature of human reading has allured many scholars into grossly under-estimating the complexity and difficulties in building machine systems for document recognition <ref> [154, 132] </ref>. Despite three decades of extensive research, solid progress has only been made in limited areas such as developing recognition algorithms for isolated machine-printed characters. Machine systems for document recognition still have limited capabilities for recognizing degraded documents [124, 138, 140, 139, 137].
Reference: [155] <author> Abraham Sinkov. </author> <title> Elementary Cryptanalysis: A Mathematical Approach. Random House, </title> <address> New York, </address> <year> 1968. </year>
Reference-contexts: BACKGROUND 2.3.1 Character n-grams and Dictionary Lookup One group of methods are based on using character n-gram statistics as constraints. Examples that use character bigram and trigram statistics are [157, 120]. Some other works also used a stationary Markov model, examples are <ref> [155, 57, 56, 95] </ref>. Two problems exist with using character n-gram statistics. First, a substantial amount of cipher text is needed for statistically reliable n-gram estimation.
Reference: [156] <institution> British Computer Society. Character Recognition. British Computer Society, </institution> <address> London, England, </address> <year> 1971. </year>
Reference-contexts: Introduction Ever since the emergence of computer technology in the 1940's, a long-standing objective has been to build automatic reading machines with the power, tolerance and efficiency of a human reader <ref> [20, 156, 64, 111, 106, 63] </ref>. Research on computer document recognition has been an important part in the efforts to understand human reading and build automatic systems with matching performance.
Reference: [157] <author> R. Spillman, M. Janssen, B. Nelson, and M. Kepner. </author> <title> Use of a generic algorithm in the cryptanalysis of simple substitution ciphers. </title> <journal> Cryptologia, </journal> <volume> XVII(1):31-44, </volume> <year> 1993. </year>
Reference-contexts: CHARACTER RECOGNITION AS SUBSTITUTION CIPHER DECODING 47 48 CHAPTER 2. BACKGROUND 2.3.1 Character n-grams and Dictionary Lookup One group of methods are based on using character n-gram statistics as constraints. Examples that use character bigram and trigram statistics are <ref> [157, 120] </ref>. Some other works also used a stationary Markov model, examples are [155, 57, 56, 95]. Two problems exist with using character n-gram statistics. First, a substantial amount of cipher text is needed for statistically reliable n-gram estimation.
Reference: [158] <author> R.K. Srihari. </author> <title> Integrating textual and visual information. </title> <booktitle> In Proceedings of AAAI-91: National Artificial Intelligence Conference, </booktitle> <year> 1991. </year>
Reference-contexts: INTRODUCTION greatly increased the performance of document recognition systems when applied to degraded documents. The improved understanding of the importance of linguistic processing in document recognition has also paralleled the recognition of the importance of using contextual information in many other aspects of computer vision and image understanding <ref> [11, 9, 158, 159] </ref>. Research in using contextual information in pattern recognition had resulted in new classification frameworks that incorporated different levels of contextual constraints inside the discrimination function for high classification accuracy and more meaningful interpretation of results [52, 164].
Reference: [159] <author> R.K. Srihari. </author> <title> Linguistic context in vision. </title> <booktitle> In Proceedings of the IEEE Context-Based Vision Workshop, </booktitle> <year> 1995. </year> <note> 210 BIBLIOGRAPHY </note>
Reference-contexts: INTRODUCTION greatly increased the performance of document recognition systems when applied to degraded documents. The improved understanding of the importance of linguistic processing in document recognition has also paralleled the recognition of the importance of using contextual information in many other aspects of computer vision and image understanding <ref> [11, 9, 158, 159] </ref>. Research in using contextual information in pattern recognition had resulted in new classification frameworks that incorporated different levels of contextual constraints inside the discrimination function for high classification accuracy and more meaningful interpretation of results [52, 164].
Reference: [160] <author> S. Srihari. </author> <title> Computer Text Recognition And Error Correction. </title> <publisher> IEEE Computer Science Press, </publisher> <address> Silver Spring, MD, </address> <year> 1985. </year>
Reference-contexts: Character n-gram statistics or a dictionary were usually used to model the linguistic constraints. Later developments along this line included using linguistic constraints at the word and sentence levels such as word n-gram statistics, word collocation statistics, and the transition probabilities of word categories <ref> [162, 160, 43] </ref>. New statistical tools and language models such as the Hidden Markov Model and grammatical parsers that were developed and achieved successes in speech recognition and natural language understanding were also introduced into OCR [133, 135].
Reference: [161] <author> Sargur N. Srihari, </author> <title> editor. Computer Text Recognition and Error Correction. </title> <publisher> IEEE Computer Software Press, </publisher> <address> Piscataway, N.J., </address> <year> 1984. </year>
Reference-contexts: CHARACTER LEVEL DECIPHERING Research in spelling correction and OCR post-processing algorithms can provide useful hints about the construction of character confusion heuristics <ref> [94, 161, 86] </ref>. Most of the techniques used for text correction are based on studies of characteristics of spelling errors made by human beings [51], and therefore are not directly applicable to the correction of character identities achieved by character pattern clustering and deciphering.
Reference: [162] <author> Sargur N. Srihari. </author> <title> From pixels to paragraphs: the use of models in text recognition. </title> <booktitle> In Symposium on Document Analysis and Information Retrievals, </booktitle> <pages> pages 47-64, </pages> <year> 1993. </year>
Reference-contexts: Character n-gram statistics or a dictionary were usually used to model the linguistic constraints. Later developments along this line included using linguistic constraints at the word and sentence levels such as word n-gram statistics, word collocation statistics, and the transition probabilities of word categories <ref> [162, 160, 43] </ref>. New statistical tools and language models such as the Hidden Markov Model and grammatical parsers that were developed and achieved successes in speech recognition and natural language understanding were also introduced into OCR [133, 135].
Reference: [163] <author> C.C. Tappert, C.Y. Suen, and T. Wakahara. </author> <title> The state of the art in on-line handwriting recognition. </title> <journal> IEEE Transactions on pattern analysis and machine intelligence, </journal> <volume> PAMI-12:787, </volume> <year> 1990. </year>
Reference-contexts: Another major problem in recognizing degraded words is caused by the occurrence of touching characters and character fragmentation, which usually pose extreme difficulty to recognition algorithms that are based on character segmentation <ref> [113, 26, 22, 163, 39] </ref>. Many algorithms had been proposed in the past for recognizing degraded words.
Reference: [164] <author> G.T. Toussaint. </author> <title> The use of context in pattern recognition. </title> <journal> Pattern Recognition, </journal> <volume> 10 </volume> <pages> 189-204, </pages> <year> 1979. </year>
Reference-contexts: Research in using contextual information in pattern recognition had resulted in new classification frameworks that incorporated different levels of contextual constraints inside the discrimination function for high classification accuracy and more meaningful interpretation of results <ref> [52, 164] </ref>. Psychological studies in human reading have also shown the critical role of linguistic context in human reading, especially when visual information is im 1.3. CONTEXTUAL LINGUISTIC INFORMATION IN DOCUMENT RECOGNITION17 paired [130, 82, 36, 146, 150].
Reference: [165] <author> A. Treisman. </author> <title> Features and objects in visual processing. </title> <journal> Scientific American, </journal> <volume> 254 </volume> <pages> 114-124, </pages> <year> 1980. </year>
Reference-contexts: CONCLUSIONS AND FUTURE DIRECTIONS 109 they could be very useful in differentiating similar character patterns when the pixel representation produces conflicting results. We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision <ref> [166, 165, 131, 169] </ref>, as well as on neural physiological studies of features detection and representation in primate visual cortex [72, 71, 10, 18, 37]. This hierarchical feature representation is computed from the skeleton representation of a character.
Reference: [166] <author> A. Treisman and G. Gelade. </author> <title> A feature integration theory of attention. </title> <journal> Cognitive Psychology, </journal> <volume> 12 </volume> <pages> 97-136, </pages> <year> 1980. </year>
Reference-contexts: CONCLUSIONS AND FUTURE DIRECTIONS 109 they could be very useful in differentiating similar character patterns when the pixel representation produces conflicting results. We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision <ref> [166, 165, 131, 169] </ref>, as well as on neural physiological studies of features detection and representation in primate visual cortex [72, 71, 10, 18, 37]. This hierarchical feature representation is computed from the skeleton representation of a character.
Reference: [167] <author> S. Tsujimoto and H. Asada. </author> <title> Resolving ambiguity in segmenting touching characters. </title> <booktitle> In Proceedings of the First International Conference on Document Analysis and Recognition(ICDAR-91), </booktitle> <year> 1991. </year>
Reference-contexts: Many algorithms had been proposed in the past for recognizing degraded words. The more recent and advanced algorithms in this area are based on integrating character segmentation with character classification and using the character or word recognition results as feedbacks to alter or improve segmentation decisions made earlier <ref> [53, 17, 25, 167, 149] </ref>.
Reference: [168] <author> S. Tsujimoto and H. Asada. </author> <title> Major components of a complex text reading system. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1133-1149, </pages> <year> 1992. </year>
Reference-contexts: This criterion avoids considering points along a thin horizontal line as potential segmentation points. Lu's algorithm [103] improved on this method by using a peak-to-valley function that favors low valleys with high peaks on both sides. 2.1. CHARACTER SEGMENTATION 35 Different pre-filtering methods were proposed by Tsujimoto and Liang <ref> [168, 102] </ref> that were intended to produce better projection for locating boundaries between touching characters. Moreover, the upper and lower profiles of the patterns were also used in combination with the projection to get improved segmentation point candidates [6].
Reference: [169] <author> S. Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year>
Reference-contexts: CONCLUSIONS AND FUTURE DIRECTIONS 109 they could be very useful in differentiating similar character patterns when the pixel representation produces conflicting results. We have experimented with devising a hierarchical feature representation of characters based on psychological studies of feature detection and representation in human preattentive-attentive vision and attentive vision <ref> [166, 165, 131, 169] </ref>, as well as on neural physiological studies of features detection and representation in primate visual cortex [72, 71, 10, 18, 37]. This hierarchical feature representation is computed from the skeleton representation of a character.
Reference: [170] <author> S. Ullman. </author> <title> An approach to object recognition: aligning pictorial descriptions. mit ai memo. </title> <type> Technical report, </type> <institution> MIT, </institution> <month> December </month> <year> 1986. </year> <note> BIBLIOGRAPHY 211 </note>
Reference-contexts: This is the alignment stage. The alignment transformation may be computed from a small number of corresponding anchor features in the image and the model. In the second stage, the model that minimize the distance measure is found <ref> [170] </ref>. For character recognition, the anchor points are first extracted by tracing the image contour. Then strokes are detected and recognized by prototype alignment, using affine transformations computed from anchor point correspondence. Letter hypotheses are formed by detecting potential instances of each of the character prototypes.
Reference: [171] <author> D.L. Waltz. </author> <title> Generating semantic description from drawings of scenes with shadows. </title> <publisher> MIT Technical Report AI271 AI271, MIT, </publisher> <year> 1972. </year>
Reference-contexts: As a result, each object in the scene are frequently associated with multiple labels representing different classification results. An early example of scene labeling was the interpretation of line segments in a line drawing of a set of polyhedra. This problem domain was also investigated in <ref> [33, 73, 171] </ref>. The ambiguities in individual object identification are difficult to resolve if each object is considered in isolation. However, the relationships among the objects can be helpful in resolving the ambiguities and achieving a scene labeling that is consistent with the scene context.
Reference: [172] <author> J. Wang and J. Jean. </author> <title> Segmentation of merged characters by neural networks and shortest path. </title> <journal> Pattern Recognition, </journal> <volume> 27(5) </volume> <pages> 649-658, </pages> <year> 1994. </year>
Reference-contexts: Pattern splitting is then carried out on connected components classified as touching characters by locating characteristic landmarks indicating likely segmentation points. Methods for finding the optimal segmentation 38 CHAPTER 2. BACKGROUND path were also reported in <ref> [32, 55, 172] </ref>. 2.1.1.5 Landmark Detection Image features such as character ascenders and descenders usually can serve as landmarks for segmentation of word images. Methods of character segmentation based on detection of ascenders and descenders has been applied to both printed text as well as cursive writings.
Reference: [173] <author> R. A. Wilkinson. </author> <title> Comparison of massively parallel segmenters. </title> <type> NIST internal report, U.S. </type> <institution> National Institute of Standards and Technology, Gaithersburg, Maryland, </institution> <year> 1992. </year>
Reference-contexts: One example was given in [29] where knowledge of the symbols was also used to help segment handwritten post codes. Experimental comparison of character segmentation by projection analysis vs. segment 36 CHAPTER 2. BACKGROUND 2.1. CHARACTER SEGMENTATION 37 ation by connected component analysis is reported in <ref> [173] </ref>. Both segmenters were tested on a large database of hand-printed digits (272870 in total).
Reference: [174] <author> R. A. Wilkinson et al. </author> <title> The first census optical character recognition system conference. </title> <type> NIST internal report, U.S. </type> <institution> National Institute of Standards and Technology, Gaithers-burg, Maryland, </institution> <year> 1992. </year>
Reference-contexts: Image degradation in documents makes effective font training even more difficult. The need for large amounts of font training is a significant impediment to the development of high performance commercial recognition systems <ref> [84, 7, 5, 8, 174] </ref>. 4 CHAPTER 1. INTRODUCTION 1.1. THE MOTIVATION 5 6 CHAPTER 1. INTRODUCTION There have been extensive research efforts over the past three decades in seeking solutions to these two problems.
Reference: [175] <author> K.Y. Wong, R.G. Casey, </author> <title> and F.M. Wahl. Document analysis systems. </title> <journal> IBM J. of Res. and Dev., </journal> <volume> 26(6) </volume> <pages> 656-757, </pages> <year> 1982. </year>
Reference-contexts: These techniques recognize character images in a printed text by solving a substitution cipher (Figure 2.5). An image pattern clustering step first converts each character on the input page into a computer readable code so that every shape corresponds to a distinct code <ref> [175, 24] </ref>. A deciphering algorithm is then applied that uses a language model to assign alphabetic labels to the cipher codes so that the character repetition pattern in the input text passage best matches the letter repetition pattern provided by the language model.
Reference: [176] <author> S. W. Zucker, E.V. Krishnammarthy, </author> <title> and R.L. Haar. Relaxation process for scene labeling: convergence, speed and stability. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-8:41-48, </volume> <year> 1978. </year>
Reference-contexts: Our proposed approaches are aimed at investigating solutions to the above problems. 2.3.2 Deciphering as Relaxation Labeling and Application to OCR Relaxation algorithms have been applied to image processing in the past <ref> [126, 144, 176] </ref>. They are iterative parallel classification algorithms that estimate the class membership probabilities of each element in a graph structure based on those of its neighbors and the constraints that exist among the neighbors. The process is iterated until a satisfactory classification is achieved.
References-found: 176

