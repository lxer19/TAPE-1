URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/complex_systems.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: E-mail: sontag@fermat.rutgers.edu, sussmann@math.rutgers.edu  
Phone: (201)932-3072 (Sontag), (201)932-5407 (Sussmann)  
Title: BACKPROPAGATION CAN GIVE RISE TO SPURIOUS LOCAL MINIMA EVEN FOR NETWORKS WITHOUT HIDDEN LAYERS  
Author: Eduardo D. Sontag Hector J. Sussmann 
Note: Sussmann's work was partially supported by NSF grant DMS83-01678-01, and by the CAIP Center, Rutgers University, with funds provided by the New Jersey Commission on Science and Technology and by CAIP's industrial members Sontag's work was partially supported by NSF grant DMS88-03396, by US Air Force grants AFOSR-85-0247 and AFOSR-88-0235, and by the CAIP Center, Rutgers University, with funds provided by the New Jersey Commission on Science and Technology and by CAIP's industrial members.  
Address: New Brunswick, NJ 08903  
Affiliation: Department of Mathematics Rutgers University  
Abstract: We give an example of a neural net without hidden layers and with a sigmoid transfer function, together with a training set of binary vectors, for which the sum of the squared errors, regarded as a function of the weights, has a local minimum which is not a global minimum. The example consists of a set of 125 training instances, with four weights and a threshold to be learnt. We do not know if substantially smaller binary examples exist. 
Abstract-found: 1
Intro-found: 1
Reference: [BRS88] <author> Brady, M., R. Raghavan and J. Slawny, </author> <title> "Gradient descent fails to separate," </title> <booktitle> in Proc. IEEE International Conference on Neural Networks, </booktitle> <address> San Diego, California, </address> <month> July </month> <year> 1988, </year> <note> Vol. I, pp.649-656. </note>
Reference-contexts: In [So88] we remarked that even for the case of no hidden neurons there may be "bad" solutions of the gradient descent algorithm, and this point was also raised by Brady, Raghavan, and Slawny in the last section of <ref> [BRS88] </ref>. The main point of the latter reference is to deal with the second of the above assertions.
Reference: [Hi87] <author> Hinton, G.E., </author> <title> "Connectionist learning procedures," </title> <type> Technical Report CMU-CS-87-115, </type> <institution> Comp.Sci. Dept., Carnegie-Mellon University, </institution> <month> June </month> <year> 1987. </year> <month> 14 </month>
Reference-contexts: 1 Introduction Backpropagation (bp) is one of the most widely used techniques for neural net learning. (Cf. Rumelhart and Hinton [PDP], Hinton <ref> [Hi87] </ref> for an introduction and references to current work.) The method attempts to obtain interconnection weights that minimize missclassifications in pattern recognition problems.
Reference: [PDP] <author> Rumelhart, D.E., and J.L. McClelland, </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <publisher> MIT Press, </publisher> <year> 1986, </year> <note> volume 1. </note>
Reference-contexts: 1 Introduction Backpropagation (bp) is one of the most widely used techniques for neural net learning. (Cf. Rumelhart and Hinton <ref> [PDP] </ref>, Hinton [Hi87] for an introduction and references to current work.) The method attempts to obtain interconnection weights that minimize missclassifications in pattern recognition problems.
Reference: [So88] <author> Sontag. E.D., </author> <title> Some remarks on the backpropogation algorithm for neural net learning, </title> <institution> Rutgers Center for Systems and Control Technical Report 88-07, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: In <ref> [So88] </ref> we remarked that even for the case of no hidden neurons there may be "bad" solutions of the gradient descent algorithm, and this point was also raised by Brady, Raghavan, and Slawny in the last section of [BRS88].
Reference: [SoSu88] <author> E.D. Sontag and H.J. Sussmann, </author> <title> Backpropagation Separates when Perceptrons Do, </title> <institution> Rutgers Center for Systems and Control Technical Report 88-12, </institution> <month> Novem-ber </month> <year> 1988. </year>
Reference-contexts: Through a careful and rigorous analysis they show that even if a training set is separable, that is, if it is recognizable by a perceptron, weights obtained through bp may not classify the data correctly. (A modification of the cost function, as described in <ref> [SoSu88] </ref> and discussed below, allows one to avoid this problem, however.) In addition, the domain of attraction of such "bad" weight configurations is very large. So neither of the above three assertions is in fact correct in general. <p> This latter remark is of interest because of the recent result obtained by the authors (see <ref> [SoSu88] </ref>) where it is proved that if (1) one modifies the cost function to be of a threshold-LMS type, i.e. one does not penalize "overclassification," and (2) the training data is separable in the sense of perceptrons, then it is true indeed that there are no local minima that are not
Reference: [Su88] <author> Sussmann, H.J., </author> <title> On the convergence of learning algorithms for Boltzmann machines, </title> <institution> Rutgers Center for Systems and Control Technical Report 88-03, </institution> <month> August </month> <year> 1988. </year> <note> Submitted to Neural Networks. </note>
Reference-contexts: This situation is in marked contrast with the case of Boltzmann machines, where the "folk fact" that, if there are no hidden neurons, then there are no spurious local minima, actually is a true theorem. (Cf., e.g. <ref> [Su88] </ref>.) In our example, all the components of the a j , and all the b j , will be binary (i.e. equal to 1 or 1).
References-found: 6

