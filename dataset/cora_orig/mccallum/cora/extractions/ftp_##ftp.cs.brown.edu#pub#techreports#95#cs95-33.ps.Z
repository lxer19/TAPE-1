URL: ftp://ftp.cs.brown.edu/pub/techreports/95/cs95-33.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-95-33.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Alefeld and J. Herzberger. </author> <title> Introduction to Interval Computations. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1983. </year>
Reference-contexts: array [1..2]; Body: unique solve system Circle: x <ref> [1] </ref>^2 + x [2]^2 = 1; Parabola: x [1]^2 - x [2] = 0; The variable section declares an array of two variables and the body section specifies the two constraints. Helios returns the two boxes (i.e., two tuples of intervals) x [1] in [-0.78615138,-0.78615137] x [1] in [+0.78615138,+0.78615137] in about 0.1 second. Some more interesting multivariate problems come from the area of robot kinematics, where the goal is to find the angles for the joints of a robot arm so that the robot hand ends up in a specified position. <p> solve system Circle: x <ref> [1] </ref>^2 + x [2]^2 = 1; Parabola: x [1]^2 - x [2] = 0; The variable section declares an array of two variables and the body section specifies the two constraints. Helios returns the two boxes (i.e., two tuples of intervals) x [1] in [-0.78615138,-0.78615137] x [1] in [+0.78615138,+0.78615137] in about 0.1 second. Some more interesting multivariate problems come from the area of robot kinematics, where the goal is to find the angles for the joints of a robot arm so that the robot hand ends up in a specified position. <p> 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y <ref> [1] </ref>*(y [2] + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] | j &lt;&gt; i g; Variable: x : array [idx] in [-10^8..10^8]; Body: unique <p> Constants are assumed to denote real numbers unless specified otherwise by an integer declaration. Helios guarantees that a constant is evaluated only once. The compilation time for this example is about 0.6 second and the running time to obtain the unique solution y <ref> [1] </ref> in [0:00311409; 0:00311411] y [3] in [0:06504177; 0:06504178] y [5] in [0:03695185; 0:03695186] is about 5 seconds. 8 Our next example is the well-known Broyden Banded function problem (e.g., [8]) which amounts to finding the zeros of the system of n equations f i (x 1 ; : : :; <p> n n 2 Compilation Time (ms) Running Time (ms) Growth Factor 5 25 480 1190 10 100 480 8350 2.77 20 400 480 59490 2.86 40 1600 480 535700 3.02 10 Set: Variable: x : array [idx] in [-10^8..10^8]; Body: minimize 100 * (x [2] - x <ref> [1] </ref>^2)^2 + (x [1] - 1)^2; Input: int n : "Number of variables"; Set: Variable: x : array [idx] in [-10..10]; Function: y (i in idx) = 1 + 0.25 * (x [i]-1); Body: minimize 10 * sin (pi*y (1))^2 + (y (n) - 1)^2 + (y (i) -1)^2 * (1 + 10 * <p> On unconstrained optimization problems, Helios returns an interval bounding the value of the global optimum as well as boxes enclosing each of the optima. For instance, in the Rosenbrock problem, Helios returns optimum in [0:00000000; 0:00000001] x <ref> [1] </ref> in [0:99999999; 1:00000001] The compilation and running times of Helios are 0.15 and 0.4 second respectively. It is important to stress that Helios bounds the global optimum value, not a local minimum value. In addition, Helios returns all the global optima. <p> specify the region in which 3 Note that, in unconstrained optimization, no such issue arises since there are no constraints. 13 Set: Variable: x : array [idx] in [-10..10]; Body: minimize Prod (k in [1..2]) (Sum (i in [1..5]) i * cos ((i+1)*x [k] + i)) with range constraints x <ref> [1] </ref> &gt;= x [2]; Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + <p> The range constraint exploits the fact that the variables play a symmetric role and constrains x <ref> [1] </ref> to be no smaller than x [2]. The constraint reduces the execution time by a factor 2. Finally, there is another functionality that is worth mentioning: the display section. The display section makes it possible to specify the information to be produced. <p> The second constraint can be used to reduce further the interval of x 2 by searching for the leftmost and rightmost zeros of <ref> [1; 1] </ref> 2 X 2 = 0 producing the interval [0,1] for x 2 . No more reduction is obtained by Helios and branching is needed to make progress. <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 23, 24] </ref>). Our definitions are slightly non-standard. 19 5.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> For instance, the interval function is the most precise interval extension of addition (i.e., it returns the smallest possible interval containing all real results) while a function always returning <ref> [1; 1] </ref> would be the least accurate. In the following, we assume fixed interval extensions for Helios operators and relations (for instance, the interval extension of + is defined by ). In addition, we overload the real symbols and use them for their interval extensions. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 4 Consider the constraint x 1 + x 2 x 1 = 0. The constraint is not arc-consistent wrt h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1]+[1; 1 + ][1; 1])"[0; 0] and ([1; <p> Example 4 Consider the constraint x 1 + x 2 x 1 = 0. The constraint is not arc-consistent wrt h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1]+[1; 1 + ][1; 1])"[0; 0] and ([1; 1] + <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1]+[1; 1 + ][1; 1])"[0; 0] and ([1; 1] + <ref> [1 ; 1] </ref> [1; 1]) " [0; 0] are non-empty. 6.2 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since (<ref> [1; 1] </ref>+[1; 1 + ][1; 1])"[0; 0] and ([1; 1] + [1 ; 1] [1; 1]) " [0; 0] are non-empty. 6.2 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 <ref> [0; 1] </ref> 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 2: Summary of the Experimental Results on Unconstrained Optimization.
Reference: [2] <author> F. Benhamou, D. McAllester, and P. Van Hentenryck. </author> <title> CLP(Intervals) Revisited. </title> <booktitle> In Proceedings of the International Symposium on Logic Programming (ILPS-94), </booktitle> <pages> pages 124-138, </pages> <address> Ithaca, NY, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: We now consider a simple multivariate problem: the intersection of a circle and a parabola. The problem can be stated as follows: Variable: x : array [1..2]; Body: unique solve system Circle: x [1]^2 + x <ref> [2] </ref>^2 = 1; Parabola: x [1]^2 - x [2] = 0; The variable section declares an array of two variables and the body section specifies the two constraints. Helios returns the two boxes (i.e., two tuples of intervals) x [1] in [-0.78615138,-0.78615137] x [1] in [+0.78615138,+0.78615137] in about 0.1 second. <p> s [i]^2 + c [i]^2 = 1; C1 : s <ref> [2] </ref>*c [5]*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; <p> [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c <ref> [2] </ref>*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); <p> [4]*c [5]*s [6] + C2 : c [1]*c <ref> [2] </ref>*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 <p> c [1]*c <ref> [2] </ref>*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system <p> + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c <ref> [2] </ref> + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] <p> s [1]*c [5] = 1.9115; C4 : c [1]*c <ref> [2] </ref> + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: 0 = y <p> <ref> [2] </ref> + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; <p> r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y <ref> [2] </ref> + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] | j &lt;&gt; i g; <p> = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y <ref> [2] </ref> + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] | j &lt;&gt; i g; Variable: x : array [idx] in [-10^8..10^8]; Body: unique solve system f <p> in [1..k]) t [j]*p (j) + n n 2 Compilation Time (ms) Running Time (ms) Growth Factor 5 25 480 1190 10 100 480 8350 2.77 20 400 480 59490 2.86 40 1600 480 535700 3.02 10 Set: Variable: x : array [idx] in [-10^8..10^8]; Body: minimize 100 * (x <ref> [2] </ref> - x [1]^2)^2 + (x [1] - 1)^2; Input: int n : "Number of variables"; Set: Variable: x : array [idx] in [-10..10]; Function: y (i in idx) = 1 + 0.25 * (x [i]-1); Body: minimize 10 * sin (pi*y (1))^2 + (y (n) - 1)^2 + (y (i) <p> in which 3 Note that, in unconstrained optimization, no such issue arises since there are no constraints. 13 Set: Variable: x : array [idx] in [-10..10]; Body: minimize Prod (k in [1..2]) (Sum (i in [1..5]) i * cos ((i+1)*x [k] + i)) with range constraints x [1] &gt;= x <ref> [2] </ref>; Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c <p> s [i]^2 + c [i]^2 = 1; C1 : s <ref> [2] </ref>*c [5]*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 <p> [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c <ref> [2] </ref>*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving <p> [4]*c [5]*s [6] + C2 : c [1]*c <ref> [2] </ref>*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios <p> c [1]*c <ref> [2] </ref>*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios statement for Levy 3, including range <p> + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c <ref> [2] </ref> + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios statement for Levy 3, including range constraints, is depicted in Figure 12. <p> s [1]*c [5] = 1.9115; C4 : c [1]*c <ref> [2] </ref> + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios statement for Levy 3, including range constraints, is depicted in Figure 12. <p> The range constraint exploits the fact that the variables play a symmetric role and constrains x [1] to be no smaller than x <ref> [2] </ref>. The constraint reduces the execution time by a factor 2. Finally, there is another functionality that is worth mentioning: the display section. The display section makes it possible to specify the information to be produced. Figure 13 describes the Helios statement for the kinematics example with a display section. <p> Section 6.4 specifies the pruning in Helios. Section 6.5 describes the algorithm. Recall that we assume that all constraints are defined over variables x 1 ; : : : ; x n . 6.1 Box Consistency Box-consistency <ref> [2] </ref> is an approximation of arc-consistency, a notion well-known in artificial intelligence [20] which states a simple local condition on a constraint c and the set of possible values for each of its variables, say D 1 ; : : : ; D n . <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h [1; 1] ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; <ref> [2; 2] </ref> i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Moreover, decomposing complex constraints into simple constraints entails a substantial loss in pruning, making this approach unpractical on many applications. See <ref> [2] </ref> for experimental results on this approach and their comparison with the approach presented in this paper. The notion of box-consistency introduced in [2] is a coarser approximation of arc-consistency which provides a much better trade-off between efficiency and pruning. <p> Moreover, decomposing complex constraints into simple constraints entails a substantial loss in pruning, making this approach unpractical on many applications. See <ref> [2] </ref> for experimental results on this approach and their comparison with the approach presented in this paper. The notion of box-consistency introduced in [2] is a coarser approximation of arc-consistency which provides a much better trade-off between efficiency and pruning. It consists in replacing the existential quantification in the above condition by the evaluation of an interval extension of the constraint on the intervals of the existential variables.
Reference: [3] <author> F. Benhamou and W. </author> <title> Older. Applying Interval Arithmetic to Real, Integer and Boolean Constraints. </title> <journal> Journal of Logic Programming, </journal> <note> 1995. To appear. </note>
Reference-contexts: However, these matrices are low-level, computer-centered, specifications of nonlinear applications; they are still far from the mathematical descriptions typically found in scientific publications. Constraint programming languages based on interval analysis (e.g., BNR-Prolog [31], CLP (BNR) <ref> [3] </ref>, and Newton [41]) are a recent addition to the set of tools supporting interval analysis. These languages also bridge some of the gap between interval libraries and nonlinear applications. <p> [i]^2 = 1; C1 : s [2]*c [5]*s [6] - s <ref> [3] </ref>*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; <p> [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c <ref> [3] </ref>*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 <p> [5] + c [1]*c <ref> [3] </ref>*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = <p> [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c <ref> [3] </ref> + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: <p> Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: 0 = y <ref> [3] </ref>*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in <p> array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: 0 = y <ref> [3] </ref>*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] <p> 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: 0 = y <ref> [3] </ref>*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] | j &lt;&gt; i g; Variable: x : array [idx] in [-10^8..10^8]; Body: unique solve system f (i in idx): 0 = x [i] * <p> Constants are assumed to denote real numbers unless specified otherwise by an integer declaration. Helios guarantees that a constant is evaluated only once. The compilation time for this example is about 0.6 second and the running time to obtain the unique solution y [1] in [0:00311409; 0:00311411] y <ref> [3] </ref> in [0:06504177; 0:06504178] y [5] in [0:03695185; 0:03695186] is about 5 seconds. 8 Our next example is the well-known Broyden Banded function problem (e.g., [8]) which amounts to finding the zeros of the system of n equations f i (x 1 ; : : :; x n ) = x <p> [i]^2 = 1; C1 : s [2]*c [5]*s [6] - s <ref> [3] </ref>*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search <p> [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c <ref> [3] </ref>*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. <p> [5] + c [1]*c <ref> [3] </ref>*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios statement for Levy 3, including range constraints, is depicted <p> [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c <ref> [3] </ref> + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios statement for Levy 3, including range constraints, is depicted in Figure 12. <p> j 9r 1 2 I 1 ; : : : ; 9r i1 2 I i1 ; : : :; 9r i+1 2 I i+1 ; 9r n 2 I n : c (r 1 ; : : : ; r n ) gg: This condition, used in systems like <ref> [31, 3] </ref>, is easily enforced on simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 but it is also computationally very expensive for complex constraints with multiple occurrences of the same <p> Early languages based on intervals such as BNR-Prolog [31] and CLP (BNR) <ref> [3] </ref> did not use some of the advanced techniques from interval analysis (e.g., the interval Newton method). As a consequence, we investigated how to introduce these techniques in a declarative way and proposed the concept of box-consistency which subsumes several techniques from interval analysis and artificial intelligence. 12 .
Reference: [4] <author> R. Hammer, M. Hocks, M. Kulisch, and D. Ratz. </author> <title> Numerical Toolbox for Verified Computing I Basic Numerical Problems, Theory, Algorithms, and PASCAL-XSC Programs. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1993. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> : s [2]*c [5]*s [6] - s [3]*c [5]*s [6] - s <ref> [4] </ref>*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); <p> [1]*c [3]*s [5] + c [1]*c <ref> [4] </ref>*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y [5] - <p> solve system C1: 0 = 3*y [5] - y [1]*(y [2] + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y <ref> [4] </ref>) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] | j &lt;&gt; i g; Variable: x : array [idx] in [-10^8..10^8]; Body: unique solve system f (i in idx): 0 = x [i] * (2 + 5 <p> : s [2]*c [5]*s [6] - s [3]*c [5]*s [6] - s <ref> [4] </ref>*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are <p> [1]*c [3]*s [5] + c [1]*c <ref> [4] </ref>*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; Display: variables: c,s; constraints : C1,C2,C3,C4,C5,C6; 14 15 16 to search for solutions: they are not used for proving the existence of solutions. 4 A new Helios statement for Levy 3, including range constraints, is depicted in Figure 12. <p> is optimistic in this case. 29 Benchmarks v d range Helios HRB CONT Broyden 10 3 10 [-1,1] 1.78 18.23 Broyden 20 3 20 [-1,1] 5.36 ? Broyden 160 3 160 [-1,1] 95.09 ? Broyden 160 3 160 [10 8 ; 10 8 ] 105.610 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 59.49 968.25 More-Cosnard 40 3 40 [4; 5] 535.70 ? More-Cosnard 40 3 40 [10 8 ; 0] 538.41 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 <p> d range Helios HRB CONT Broyden 10 3 10 [-1,1] 1.78 18.23 Broyden 20 3 20 [-1,1] 5.36 ? Broyden 160 3 160 [-1,1] 95.09 ? Broyden 160 3 160 [10 8 ; 10 8 ] 105.610 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 59.49 968.25 More-Cosnard 40 3 40 [4; 5] 535.70 ? More-Cosnard 40 3 40 [10 8 ; 0] 538.41 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 ] 353.06 4730.34 35.61 eco 5 54 [10 <p> [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 <ref> [4; 4] </ref> 10.04 5 More2 4 [25; 25] 189.56 32 Table 2: Summary of the Experimental Results on Unconstrained Optimization.
Reference: [5] <author> E. Hansen. </author> <title> Global Optimization Using Interval Analysis. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> Interval methods are generally robust and have been shown to be competitive with continuation methods on a variety of benchmarks [40]. Interval methods can also be applied to global optimization as advocated in numerous papers (e.g., <ref> [6, 5, 16, 26, 33] </ref>). Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. However, few software tools are available to help engineers and scientists applying this body of knowledge. <p> s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: unique solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] <p> [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: unique solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s <p> in [-10^8..10^8]; Body: unique solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + <p> solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; <p> s [4] + s [2] + s [3] + s [2] = 3.9701; Variable: y : array [1..5] in [0..10^8]; Constant: r = 10; r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y <ref> [5] </ref> - y [1]*(y [2] + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : <p> r6 = 0.002597/sqrt (40); r7 = 0.003448/sqrt (40); r8 = 0.00001799/40; r9 = 0.0002155/sqrt (40); r10 = 0.00003846/40; Body: unique solve system C1: 0 = 3*y <ref> [5] </ref> - y [1]*(y [2] + 1); C3: 0 = y [3]*(2*y [2]*y [3] + 2*r5*y [3] + r6 + r7*y [2]) - 8*y [5]; C5: 0 = y [2]*(y [1] + r10*y [2] + y [3]^2 + r8 + r7*y [3] + r9*y [4]) + 7 Input: int n : "Number of variables: "; Set: sidx (i in idx) = f j in [max (1,i-5)..min (n,i+1)] | j &lt;&gt; i g; Variable: x : <p> Helios guarantees that a constant is evaluated only once. The compilation time for this example is about 0.6 second and the running time to obtain the unique solution y [1] in [0:00311409; 0:00311411] y [3] in [0:06504177; 0:06504178] y <ref> [5] </ref> in [0:03695185; 0:03695186] is about 5 seconds. 8 Our next example is the well-known Broyden Banded function problem (e.g., [8]) which amounts to finding the zeros of the system of n equations f i (x 1 ; : : :; x n ) = x i (2 + 5x 2 <p> s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] <p> [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s <p> in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + <p> system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c <ref> [5] </ref>*s [6] - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; C6 : s [2] + s [3] + s [4] + s [2] + s [3] + s [2] = 3.9701; <p> A proof of this result can be found in [30] where credit is given to Moore and Nickel. 7 Unconstrained Optimization Helios uses a branch and bound algorithm for solving unconstrained optimization problems. The branch and bound algorithm is closely related to previous interval-based algorithms (e.g., <ref> [5, 16] </ref>) for global optimization. The main difference comes from the constraint-solving algorithm in Helios which is more effective in many cases than the algorithms traditionally used. The algorithm is also very close to the branch and prune algorithm. <p> There are two significant differences however: 1. The optimality conditions such as the Fritz-John conditions (e.g., <ref> [5] </ref>) involve both the con straint system and the objective function. In addition, they introduce new variables. 2. Probing is not as easy, since the constraints must be satisfied. When the constraint system consists only of inequalities, probing can still be performed by using the center of the intervals. <p> It is however necessary to verify that the point satisfies the inequalities. In presence of equations, probing is essentially impossible. However, techniques for proving the existence of a solution in a box (such as those of Section 6.6) can be used (see for instance <ref> [5, 16] </ref> for more discussion of these techniques). If it can be shown that ~ I contains a solution, then right ( b f ( ~ I)) may be used as an upper bound. <p> is optimistic in this case. 29 Benchmarks v d range Helios HRB CONT Broyden 10 3 10 [-1,1] 1.78 18.23 Broyden 20 3 20 [-1,1] 5.36 ? Broyden 160 3 160 [-1,1] 95.09 ? Broyden 160 3 160 [10 8 ; 10 8 ] 105.610 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 59.49 968.25 More-Cosnard 40 3 40 [4; 5] 535.70 ? More-Cosnard 40 3 40 [10 8 ; 0] 538.41 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 <p> d range Helios HRB CONT Broyden 10 3 10 [-1,1] 1.78 18.23 Broyden 20 3 20 [-1,1] 5.36 ? Broyden 160 3 160 [-1,1] 95.09 ? Broyden 160 3 160 [10 8 ; 10 8 ] 105.610 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 59.49 968.25 More-Cosnard 40 3 40 [4; 5] 535.70 ? More-Cosnard 40 3 40 [10 8 ; 0] 538.41 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 ] 353.06 4730.34 35.61 eco 5 54 [10
Reference: [6] <author> E.R. Hansen. </author> <title> Global Optimization Using Interval Analysis: the Multi-Dimensional Case. </title> <journal> Numer. Math, </journal> <volume> 34 </volume> <pages> 247-270, </pages> <year> 1980. </year>
Reference-contexts: Interval methods are generally robust and have been shown to be competitive with continuation methods on a variety of benchmarks [40]. Interval methods can also be applied to global optimization as advocated in numerous papers (e.g., <ref> [6, 5, 16, 26, 33] </ref>). Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. However, few software tools are available to help engineers and scientists applying this body of knowledge. <p> It illustrates the use of 6 Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: unique solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s <ref> [6] </ref> - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] <p> It illustrates the use of 6 Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: unique solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s <ref> [6] </ref> - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + <p> illustrates the use of 6 Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: unique solve system trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s <ref> [6] </ref> - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616; <p> i * cos ((i+1)*x [k] + i)) with range constraints x [1] &gt;= x [2]; Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s <ref> [6] </ref> - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] <p> + i)) with range constraints x [1] &gt;= x [2]; Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s <ref> [6] </ref> - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + <p> x [1] &gt;= x [2]; Set: Variable: s : array [idx] in [-10^8..10^8]; c : array [idx] in [-10^8..10^8]; Body: solve system once trigo (i in idx) : s [i]^2 + c [i]^2 = 1; C1 : s [2]*c [5]*s <ref> [6] </ref> - s [3]*c [5]*s [6] - s [4]*c [5]*s [6] + C2 : c [1]*c [2]*s [5] + c [1]*c [3]*s [5] + c [1]*c [4]*s [5] + s [1]*c [5] = 1.9115; C4 : c [1]*c [2] + c [1]*c [3] + c [1]*c [4] + c [1]*c [2] + c [1]*c [3] + c [1]*c [2] = 4.0616;
Reference: [7] <author> E.R. Hansen and R.I. Greenberg. </author> <title> An Interval Newton Method. </title> <journal> Appl. Math. Comput., </journal> <volume> 12 </volume> <pages> 89-98, </pages> <year> 1983. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 23, 24] </ref>). Our definitions are slightly non-standard. 19 5.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> In addition, the final intervals have widths smaller than 10 8 . The results are summarized in Table 1. For each benchmark, we give the number of variables (n), the total degree 11 Some interval methods such as <ref> [7] </ref> are more sophisticated than HRB but the sophistication aims at speeding up the computation near a solution.
Reference: [8] <author> E.R. Hansen and S. Sengupta. </author> <title> Bounding Solutions of Systems of Equations Using Interval Analysis. </title> <journal> BIT, </journal> <volume> 21 </volume> <pages> 203-211, </pages> <year> 1981. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> Interestingly, Helios solves the Broyden banded function problem <ref> [8] </ref> and More-Cosnard discretization of a nonlinear integral equation [27] for several hundred variables. 3 A Tour of Helios This section gives a gentle introduction to Helios through a number of examples. <p> The compilation time for this example is about 0.6 second and the running time to obtain the unique solution y [1] in [0:00311409; 0:00311411] y [3] in [0:06504177; 0:06504178] y [5] in [0:03695185; 0:03695186] is about 5 seconds. 8 Our next example is the well-known Broyden Banded function problem (e.g., <ref> [8] </ref>) which amounts to finding the zeros of the system of n equations f i (x 1 ; : : :; x n ) = x i (2 + 5x 2 X x j (1 + x j ) (1 i n) where J i = fj j max (1; i <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 23, 24] </ref>). Our definitions are slightly non-standard. 19 5.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> 1 ; : : :; m n ) + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using automatic differentiation [32]. 6.3 Conditioning It is interesting to note that box-consistency on the Taylor interval extension is closely related to the Hansen-Segupta's operator <ref> [8] </ref>, which is an improvement over Krawczyk's operator [18]. <p> We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis <ref> [8, 11, 25] </ref>, and continuation methods [21, 28, 29, 43]. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [9] <author> E.R. Hansen and R.R. Smith. </author> <title> Interval Arithmetic in Matrix Computation: Part II. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 4 </volume> <pages> 1-9, </pages> <year> 1967. </year>
Reference-contexts: Hansen and Smith <ref> [9] </ref> also argued that these operators are more effective for a system ff 1 = 0; : : : ; f n = 0g wrt a box hI 1 ; : : : ; I n i when the interval Jacobian M ij = @x j is diagonally dominant, i.e., mig
Reference: [10] <author> W. Hock and K. Schittkowski. </author> <title> Test Examples for Nonlinear Programming Codes. </title> <booktitle> Lecture Notes in Economics and Mathematical Systems. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1981. </year> <month> 34 </month>
Reference-contexts: The Helios statement is shown in minima in about 20 seconds. 3.3 Constrained Optimization in Helios Constrained optimization problems can also be stated in Helios. Figure 11 depicts the Helios statement of a constrained optimization problem taken from <ref> [10] </ref>. <p> 29.88 5.87 eco 7 486 [10 8 ; 10 8 ] 127.65 ? 991.45 eco 9 4374 [10 8 ; 10 8 ] 8600.28 ? combustion 10 96 [10 8 ; 10 8 ] 9.94 ? 57.40 chemistry 5 108 [0; 10 8 ] 6.32 ? 56.55 neuro 6 1024 <ref> [10; 10] </ref> 0.91 28.84 5.02 neuro 6 1024 [1000; 1000] 172.71 ? 5.02 Table 1: Summary of the Experimental Results on Equation Solving. 9 Experimental Results We now turn to the experimental results of Helios on traditional benchmarks. <p> Future versions of the system will include pragmas to let users control these features. 31 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 <p> Future versions of the system will include pragmas to let users control these features. 31 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 <p> system will include pragmas to let users control these features. 31 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 <p> users control these features. 31 Benchmarks v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 <p> v Range Time Splits Hump 2 [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 <p> [10 7 ; 10 8 ] 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 <p> 0.17 3 Levy1 1 [10 7 ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 <p> ; 10 7 ] 0.09 2 Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 <p> Levy2 1 <ref> [10; 10] </ref> 0.61 4 Levy3 2 [10; 10] 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; <p> Levy3 2 <ref> [10; 10] </ref> 16.14 30 Levy4 2 [10; 10] 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale <p> Levy4 2 <ref> [10; 10] </ref> 2.13 7 Levy5 3 [10; 10] 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 <p> Levy5 3 <ref> [10; 10] </ref> 0.75 1 Levy5 5 [10; 10] 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 <p> Levy5 5 <ref> [10; 10] </ref> 1.04 1 Levy5 10 [10; 10] 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 <p> Levy5 10 <ref> [10; 10] </ref> 3.34 1 Levy5 20 [10; 10] 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; <p> Levy5 20 <ref> [10; 10] </ref> 11.82 1 Levy5 40 [10; 10] 45.89 1 Levy5 80 [10; 10] 235.22 1 Levy6 3 [10; 10] 0.96 1 Levy6 5 [10; 10] 1.57 1 Levy6 10 [10; 10] 4.29 1 Levy6 20 [10; 10] 14.86 1 Levy6 40 [10; 10] 64.11 1 Levy6 80 [10; 10] 372.39 1 Beale 2 [4:5; 4:5] 2.50 3 Beale 2 [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; 10 7 ] 0.24 0 Booth <p> [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; 10 7 ] 0.24 0 Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 <ref> [10; 20] </ref> 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; <p> ; 10 7 ] 0.24 0 Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 <ref> [0; 10] </ref> 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 2: Summary of the Experimental Results on Unconstrained Optimization. <p> Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 <ref> [0; 10] </ref> 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 2: Summary of the Experimental Results on Unconstrained Optimization. <p> 7 ] 0.11 0 Powell 4 [10; 20] 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 <ref> [0; 10] </ref> 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 [25; 25] 189.56 32 Table 2: Summary of the Experimental Results on Unconstrained Optimization. <p> Table 3 summarizes some of our computation results on some of the toughest problems from <ref> [10] </ref>. We give the number of variables in the initial statement (v), the number of constraints (c), the CPU time, and the number of splits.
Reference: [11] <author> H. Hong and V. Stahl. </author> <title> Safe Starting Regions by Fixed Points and Tightening. </title> <booktitle> Computing, </booktitle> <address> 53(3-4):323-335, </address> <year> 1994. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> Some more interesting multivariate problems come from the area of robot kinematics, where the goal is to find the angles for the joints of a robot arm so that the robot hand ends up in a specified position. The Helios statement to solve a problem given in <ref> [11] </ref> is depicted in Figure 1. There are several novel features in the statement. First, the set section declares a set idx that is used subsequently to define arrays and to specify constraints. Second, the example illustrates how constraints can be stated generically. <p> We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis <ref> [8, 11, 25] </ref>, and continuation methods [21, 28, 29, 43]. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> From a language standpoint, Helios is a modeling language allowing mathematical descriptions of nonlinear applications to be written 12 A related technique was also proposed by <ref> [11] </ref>. See [40] for a comparison of the two approaches 33 almost as in scientific publications. As a consequence, it dramatically simplifies the solving of these applications.
Reference: [12] <author> More. J., B. Garbow, and K. Hillstrom. </author> <title> Testing Unconstrained Optimization Software. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(1) </volume> <pages> 17-41, </pages> <year> 1981. </year>
Reference-contexts: Helios is very fast when the initial intervals are small, but its computation times increase significantly when the intervals get larger. 9.2 Unconstrained Optimization Table 2 describes the results of Helios on unconstrained optimization. The benchmarks were taken mainly from <ref> [19, 12, 34, 36] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. Full details on the benchmarks are given in the appendix. The experimental results once again exhibit a number of interesting facts.
Reference: [13] <author> R.B. Kearfott. INTBIS, </author> <title> A Portable Interval Newton/Bisection Package (Algorithm 681). </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16(2) </volume> <pages> 152-157, </pages> <year> 1990. </year>
Reference-contexts: Several interval libraries (e.g., [17]) offer interval extensions of many real functions but there is a considerable gap between these libraries and practical algorithms. Some interval packages for solving systems of polynomial equations are also available (e.g., <ref> [13] </ref>). In general, these packages only require the specification of a coefficient matrix for the constraints and hence they reduce the distance between 1 interval libraries and practical applications substantially.
Reference: [14] <author> R.B. Kearfott. </author> <title> Preconditioners for the Interval Gauss-Seidel Method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 27, </volume> <year> 1990. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> The resulting system is generally solved through Gauss-Seidel iterations, giving the Hansen-Segupta's operator. See also <ref> [14, 15] </ref> for an extensive coverage of conditioners. Helios exploits this idea to improve the effectiveness of box-consistency on the Taylor interval extension. The conditioning of Helios is abstracted by the following definition.
Reference: [15] <author> R.B. Kearfott. </author> <title> A Review of Preconditioners for the Interval Gauss-Seidel Method. Interval Computations 1, </title> <booktitle> 1 </booktitle> <pages> 59-85, </pages> <year> 1991. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> The resulting system is generally solved through Gauss-Seidel iterations, giving the Hansen-Segupta's operator. See also <ref> [14, 15] </ref> for an extensive coverage of conditioners. Helios exploits this idea to improve the effectiveness of box-consistency on the Taylor interval extension. The conditioning of Helios is abstracted by the following definition.
Reference: [16] <author> R.B. Kearfott. </author> <title> A Review of Techniques in the Verified Solution of Constrained Global Optimization Problems. </title> <note> (To Appear), </note> <year> 1994. </year>
Reference-contexts: Interval methods are generally robust and have been shown to be competitive with continuation methods on a variety of benchmarks [40]. Interval methods can also be applied to global optimization as advocated in numerous papers (e.g., <ref> [6, 5, 16, 26, 33] </ref>). Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. However, few software tools are available to help engineers and scientists applying this body of knowledge. <p> A proof of this result can be found in [30] where credit is given to Moore and Nickel. 7 Unconstrained Optimization Helios uses a branch and bound algorithm for solving unconstrained optimization problems. The branch and bound algorithm is closely related to previous interval-based algorithms (e.g., <ref> [5, 16] </ref>) for global optimization. The main difference comes from the constraint-solving algorithm in Helios which is more effective in many cases than the algorithms traditionally used. The algorithm is also very close to the branch and prune algorithm. <p> It is however necessary to verify that the point satisfies the inequalities. In presence of equations, probing is essentially impossible. However, techniques for proving the existence of a solution in a box (such as those of Section 6.6) can be used (see for instance <ref> [5, 16] </ref> for more discussion of these techniques). If it can be shown that ~ I contains a solution, then right ( b f ( ~ I)) may be used as an upper bound. <p> No special effort has been devoted to make constrained optimization run fast in Helios at this point, although these results are satisfactory for a global search method. Note also that there are almost no published results on the behaviour of interval methods for constrained optimization problems <ref> [16] </ref>. 10 Discussion The research described in this paper originated in our attempt to design a constraint programming language based on interval analysis.
Reference: [17] <author> V. Knueppel. </author> <title> PROFIL/BIAS-A Fast Interval Library. </title> <booktitle> Computing, </booktitle> <address> 53(3-4):323-335, </address> <year> 1994. </year>
Reference-contexts: Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. However, few software tools are available to help engineers and scientists applying this body of knowledge. Several interval libraries (e.g., <ref> [17] </ref>) offer interval extensions of many real functions but there is a considerable gap between these libraries and practical algorithms. Some interval packages for solving systems of polynomial equations are also available (e.g., [13]).
Reference: [18] <author> R. Krawczyk. </author> <title> Newton-Algorithmen zur Bestimmung von Nullstellen mit Fehlerschranken. </title> <journal> Computing, </journal> <volume> 4 </volume> <pages> 187-201, </pages> <year> 1969. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using automatic differentiation [32]. 6.3 Conditioning It is interesting to note that box-consistency on the Taylor interval extension is closely related to the Hansen-Segupta's operator [8], which is an improvement over Krawczyk's operator <ref> [18] </ref>.
Reference: [19] <author> A.V. Levy and A. Montalvo. </author> <title> The Tunnelling Algorithm for the Global Minimization of Functions. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 6(1) </volume> <pages> 15-29, </pages> <year> 1985. </year>
Reference-contexts: Our first example is the Rosenbrock function <ref> [19] </ref> which consists of minimizing the function f (x 1 ; x 2 ) = 100 (x 2 x 2 The Helios statement is depicted in Figure 7. <p> It is important to stress that Helios bounds the global optimum value, not a local minimum value. In addition, Helios returns all the global optima. A nice example from <ref> [19] </ref> illustrating this fact is the minimization of the function f (x 1 ; : : : ; x n ) = 10 sin (y 1 ) 2 + (y n 1) 2 + i=1 For n = 10, the function has 10 10 local minima but a single global optimum. <p> Helios is very fast when the initial intervals are small, but its computation times increase significantly when the intervals get larger. 9.2 Unconstrained Optimization Table 2 describes the results of Helios on unconstrained optimization. The benchmarks were taken mainly from <ref> [19, 12, 34, 36] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. Full details on the benchmarks are given in the appendix. The experimental results once again exhibit a number of interesting facts.
Reference: [20] <author> A.K. Mackworth. </author> <title> Consistency in Networks of Relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 99-118, </pages> <year> 1977. </year>
Reference-contexts: We start with some constraint-solving examples, continue with unconstrained and constrained optimization problems and conclude with a discussion of some additional facilities. 2 Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [20, 22] </ref> and used to solve discrete combinatorial problems in several systems (e.g., [38, 39]). 4 3.1 Constraint Solving in Helios We start with a simple univariate problem which consists of finding the roots of the function x 4 12x 3 + 47x 2 60x in the interval [10 8 ; <p> The pruning step ensures that some local consistency conditions are satisfied. It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Helios is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [20, 22] </ref> and used in many systems (e.g., [39, 42, 37]) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied. <p> Section 6.4 specifies the pruning in Helios. Section 6.5 describes the algorithm. Recall that we assume that all constraints are defined over variables x 1 ; : : : ; x n . 6.1 Box Consistency Box-consistency [2] is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [20] </ref> which states a simple local condition on a constraint c and the set of possible values for each of its variables, say D 1 ; : : : ; D n . <p> [10 2 ; 10 2 ] 3.31 12 Beale 2 [10 4 ; 10 4 ] 5.52 31 Beale 2 [10 7 ; 10 7 ] 23.29 61 Schwefel1 3 [10 7 ; 10 7 ] 0.24 0 Booth 2 [10 7 ; 10 7 ] 0.11 0 Powell 4 <ref> [10; 20] </ref> 6.69 267 Schwefel3 2 [10 7 ; 10 7 ] 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0;
Reference: [21] <author> K. Meintjes and A.P. Morgan. </author> <title> Chemical Equilibrium Systems as Numerical test Problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 143-151, </pages> <year> 1990. </year>
Reference-contexts: Chemistry is the source of many interesting nonlinear problems and our next application of Helios is a chemical equilibrium problem for the propulsion of propane into the air. The problem is taken from <ref> [21] </ref> and the Helios statement is depicted in Figure 2. <p> We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis [8, 11, 25], and continuation methods <ref> [21, 28, 29, 43] </ref>. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [22] <author> U. Montanari. </author> <title> Networks of Constraints : Fundamental Properties and Applications to Picture Processing. </title> <journal> Information Science, </journal> <volume> 7(2) </volume> <pages> 95-132, </pages> <year> 1974. </year>
Reference-contexts: We start with some constraint-solving examples, continue with unconstrained and constrained optimization problems and conclude with a discussion of some additional facilities. 2 Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [20, 22] </ref> and used to solve discrete combinatorial problems in several systems (e.g., [38, 39]). 4 3.1 Constraint Solving in Helios We start with a simple univariate problem which consists of finding the roots of the function x 4 12x 3 + 47x 2 60x in the interval [10 8 ; <p> The pruning step ensures that some local consistency conditions are satisfied. It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Helios is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [20, 22] </ref> and used in many systems (e.g., [39, 42, 37]) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [23] <author> R.E. Moore. </author> <title> Interval Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1966. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 23, 24] </ref>). Our definitions are slightly non-standard. 19 5.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> This extension is an example of centered forms which are interval extensions introduced by Moore <ref> [23] </ref> 7 It is interesting to note that this definition is also related to the theorem of Miranda [30].
Reference: [24] <author> R.E. Moore. </author> <title> Methods and Applications of Interval Analysis. </title> <publisher> SIAM Publ., </publisher> <year> 1979. </year>
Reference-contexts: More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 23, 24] </ref>). Our definitions are slightly non-standard. 19 5.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set.
Reference: [25] <author> R.E. Moore and S.T. Jones. </author> <title> Safe Starting Regions for Iterative Methods. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 14 </volume> <pages> 1051-1065, </pages> <year> 1977. </year>
Reference-contexts: We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis <ref> [8, 11, 25] </ref>, and continuation methods [21, 28, 29, 43]. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> 0.03 0 Rosenbrock 2 [10 7 ; 10 7 ] 0.33 10 Ratz1 5 [500; 600] 1.19 0 Ratz25 4 [0; 10] 2.86 0 Ratz27 4 [0; 10] 4.44 0 Ratz210 4 [0; 10] 7.42 0 Ratz3 6 [0; 1] 9.13 2 More1 3 [4; 4] 10.04 5 More2 4 <ref> [25; 25] </ref> 189.56 32 Table 2: Summary of the Experimental Results on Unconstrained Optimization.
Reference: [26] <author> R.E. Moore and H. Ratschek. </author> <title> Inclusion Functions and Global Optimization II. </title> <journal> Mathematical Programming, </journal> <volume> 41(3) </volume> <pages> 341-356, </pages> <year> 1988. </year>
Reference-contexts: Interval methods are generally robust and have been shown to be competitive with continuation methods on a variety of benchmarks [40]. Interval methods can also be applied to global optimization as advocated in numerous papers (e.g., <ref> [6, 5, 16, 26, 33] </ref>). Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. However, few software tools are available to help engineers and scientists applying this body of knowledge.
Reference: [27] <author> J.J. More and M.Y. Cosnard. </author> <title> Numerical Solution of Nonlinear Equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 64-85, </pages> <year> 1979. </year>
Reference-contexts: Interestingly, Helios solves the Broyden banded function problem [8] and More-Cosnard discretization of a nonlinear integral equation <ref> [27] </ref> for several hundred variables. 3 A Tour of Helios This section gives a gentle introduction to Helios through a number of examples. Its purpose is to describe informally the syntax of Helios, as well as its functionality and performance on a number of well-known problems. <p> Another interesting fact is that Helios is essentially linear in the number of variables on this benchmark. The performance results are given in Figure 4. Our last example of equation solving is another traditional benchmark from numerical analysis: the discretization of a nonlinear integral equation <ref> [27] </ref>. <p> We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis <ref> [27] </ref>, interval analysis [8, 11, 25], and continuation methods [21, 28, 29, 43]. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [28] <author> A.P. Morgan. </author> <title> Computing All Solutions To Polynomial Systems Using Homotopy Continuation. </title> <journal> Appl. Math. Comput., </journal> <volume> 24 </volume> <pages> 115-138, </pages> <year> 1987. </year> <month> 35 </month>
Reference-contexts: We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis [8, 11, 25], and continuation methods <ref> [21, 28, 29, 43] </ref>. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [29] <author> A.P. Morgan. </author> <title> Solving Polynomial Systems Using Continuation for Scientific and Engineering Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35]) and continuation methods (e.g., <ref> [29, 43] </ref>). Continuation methods have been shown to be effective for problems for which the total degree is not too high, since the number of paths explored depends on the estimation of the number of solutions. <p> We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis [8, 11, 25], and continuation methods <ref> [21, 28, 29, 43] </ref>. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [30] <author> A. Neumaier. </author> <title> Interval Methods for Systems of Equations. </title> <booktitle> PHI Series in Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]). <p> This extension is an example of centered forms which are interval extensions introduced by Moore [23] 7 It is interesting to note that this definition is also related to the theorem of Miranda <ref> [30] </ref>. In this case, box-consistency can be seen as replacing universally quantified variables by the intervals on which they range. 8 Helios used a third interval extension but it is only a way to speed up the computation. 23 and studied by many authors, since they have important properties. <p> If I 0 then there exists a unique zero in hI 0 1 ; : : :; I 0 n i. A proof of this result can be found in <ref> [30] </ref> where credit is given to Moore and Nickel. 7 Unconstrained Optimization Helios uses a branch and bound algorithm for solving unconstrained optimization problems. The branch and bound algorithm is closely related to previous interval-based algorithms (e.g., [5, 16]) for global optimization.
Reference: [31] <author> W. Older and A. Vellino. </author> <title> Extending Prolog with Constraint Arithmetics on Real Intervals. </title> <booktitle> In Canadian Conference on Computer & Electrical Engineering, </booktitle> <address> Ottawa, </address> <year> 1990. </year>
Reference-contexts: However, these matrices are low-level, computer-centered, specifications of nonlinear applications; they are still far from the mathematical descriptions typically found in scientific publications. Constraint programming languages based on interval analysis (e.g., BNR-Prolog <ref> [31] </ref>, CLP (BNR) [3], and Newton [41]) are a recent addition to the set of tools supporting interval analysis. These languages also bridge some of the gap between interval libraries and nonlinear applications. <p> j 9r 1 2 I 1 ; : : : ; 9r i1 2 I i1 ; : : :; 9r i+1 2 I i+1 ; 9r n 2 I n : c (r 1 ; : : : ; r n ) gg: This condition, used in systems like <ref> [31, 3] </ref>, is easily enforced on simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 but it is also computationally very expensive for complex constraints with multiple occurrences of the same <p> Early languages based on intervals such as BNR-Prolog <ref> [31] </ref> and CLP (BNR) [3] did not use some of the advanced techniques from interval analysis (e.g., the interval Newton method).
Reference: [32] <author> L.B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications. </title> <booktitle> Springer Lectures Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: interval extension of c wrt ~ I, denoted by c t ( ~ I) , is the interval constraint b f (m 1 ; : : :; m n ) + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using automatic differentiation <ref> [32] </ref>. 6.3 Conditioning It is interesting to note that box-consistency on the Taylor interval extension is closely related to the Hansen-Segupta's operator [8], which is an improvement over Krawczyk's operator [18].
Reference: [33] <author> H. Ratschek and J. Rokne. </author> <title> New Computer Methods for Global Optimization. </title> <publisher> Ellis Horwood Limited, </publisher> <address> Chichester, </address> <year> 1988. </year>
Reference-contexts: Interval methods are generally robust and have been shown to be competitive with continuation methods on a variety of benchmarks [40]. Interval methods can also be applied to global optimization as advocated in numerous papers (e.g., <ref> [6, 5, 16, 26, 33] </ref>). Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. However, few software tools are available to help engineers and scientists applying this body of knowledge.
Reference: [34] <author> D. Ratz. </author> <title> Box-Splitting Strategies for the Interval Gauss-Seidel Step in a Global Optimization Method. </title> <booktitle> Computing, </booktitle> <address> 53(3-4):337-353, </address> <year> 1994. </year>
Reference-contexts: Helios is very fast when the initial intervals are small, but its computation times increase significantly when the intervals get larger. 9.2 Unconstrained Optimization Table 2 describes the results of Helios on unconstrained optimization. The benchmarks were taken mainly from <ref> [19, 12, 34, 36] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. Full details on the benchmarks are given in the appendix. The experimental results once again exhibit a number of interesting facts. <p> The experimental results once again exhibit a number of interesting facts. Helios is able to solve problems such as Levy5 and Levy6 in essentially linear time in the number of variables. Helios also solves the problems Ratz25, Ratz27, and Ratz210 without splitting. These problems were used in <ref> [34] </ref> to study splitting strategies. Finally, Helios does not exhibit the behaviour of traditional interval methods on problems such as the Rosenbrock function.
Reference: [35] <author> S.M. Rump. </author> <title> Verification Methods for Dense and Sparse Systems of Equations. </title> <editor> In J. (Ed.) Herzberger, editor, </editor> <booktitle> Topics in Validated Computations, </booktitle> <pages> pages 217-231. </pages> <publisher> Elsevier, </publisher> <year> 1988. </year>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35] </ref>) and continuation methods (e.g., [29, 43]).
Reference: [36] <author> H. Schwefel. </author> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Helios is very fast when the initial intervals are small, but its computation times increase significantly when the intervals get larger. 9.2 Unconstrained Optimization Table 2 describes the results of Helios on unconstrained optimization. The benchmarks were taken mainly from <ref> [19, 12, 34, 36] </ref> and, for each of them, we give the number of variables, the range of the variables, the CPU time, and the number of splits. Full details on the benchmarks are given in the appendix. The experimental results once again exhibit a number of interesting facts.
Reference: [37] <author> J. Siskind and D. McAllester. </author> <title> Nondeterministic Lisp as a Substrate for Constraint Logic Programming. </title> <booktitle> In AAAI-93, </booktitle> <pages> pages 133-138, </pages> <year> 1993. </year>
Reference-contexts: It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Helios is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence [20, 22] and used in many systems (e.g., <ref> [39, 42, 37] </ref>) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [38] <author> P. Van Hentenryck. </author> <title> A Logic Language for Combinatorial Optimization. </title> <journal> Annals of Operations Research, </journal> <volume> 21 </volume> <pages> 247-274, </pages> <year> 1989. </year>
Reference-contexts: We start with some constraint-solving examples, continue with unconstrained and constrained optimization problems and conclude with a discussion of some additional facilities. 2 Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence [20, 22] and used to solve discrete combinatorial problems in several systems (e.g., <ref> [38, 39] </ref>). 4 3.1 Constraint Solving in Helios We start with a simple univariate problem which consists of finding the roots of the function x 4 12x 3 + 47x 2 60x in the interval [10 8 ; 10 8 ].
Reference: [39] <author> P. Van Hentenryck. </author> <title> Constraint Satisfaction in Logic Programming. Logic Programming Series, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: We start with some constraint-solving examples, continue with unconstrained and constrained optimization problems and conclude with a discussion of some additional facilities. 2 Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence [20, 22] and used to solve discrete combinatorial problems in several systems (e.g., <ref> [38, 39] </ref>). 4 3.1 Constraint Solving in Helios We start with a simple univariate problem which consists of finding the roots of the function x 4 12x 3 + 47x 2 60x in the interval [10 8 ; 10 8 ]. <p> It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Helios is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence [20, 22] and used in many systems (e.g., <ref> [39, 42, 37] </ref>) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [40] <author> P. Van Hentenryck, D. McAllister, and D. Kapur. </author> <title> Solving Polynomial Systems Using a Branch and Prune Approach. </title> <note> SIAM Journal on Numerical Analysis, 1995. (to appear). </note>
Reference-contexts: Interval methods are generally robust and have been shown to be competitive with continuation methods on a variety of benchmarks <ref> [40] </ref>. Interval methods can also be applied to global optimization as advocated in numerous papers (e.g., [6, 5, 16, 26, 33]). Interval methods have been investigated extensively in the last decades, leading to numerous theoretical results and to the implementation of many algorithms. <p> Note also that Helios is similar in philosophy to modeling languages such as AMPL, although their design and implementation differ because of the targeted application areas. Helios statements are used as input to state-of-the-art interval algorithms. In particular, Helios is based on a branch and prune algorithm described in <ref> [40] </ref> which combines techniques from numerical analysis and artificial intelligence. This branch and prune algorithm has been extended into a branch and bound algorithm for solving global optimization problems. Helios has been evaluated on a variety of benchmarks from kinematics, chemistry, combustion, economics, and mechanics. <p> As mentioned, the kernel of Helios is a global search algorithm (fully described in <ref> [40] </ref>) which solves a constraint solving problem by dividing it into subproblems which are solved recursively. <p> For simplicity of exposition, we restrict attention to equations. It is straightforward to generalize our results to inequalities. 6 Constraint Solving This section gives a high-level overview of the branch & prune algorithm of Helios. It is based on <ref> [40] </ref>, where all details can be found. Section 6.1 defines box-consistency, the key concept behind our algorithm. Section 6.2 shows how box-consistency can be instantiated to produce various pruning operators achieving various tradeoffs between accuracy and efficiency. <p> An implementation of BOX-NARROW using the interval Newton method is described in <ref> [40] </ref>. We are now in a position to define the pruning algorithm which consists essentially of applying the narrowing operators of each projection until no further reduction occurs. The pruning algorithm is depicted in Figure 16. <p> From a language standpoint, Helios is a modeling language allowing mathematical descriptions of nonlinear applications to be written 12 A related technique was also proposed by [11]. See <ref> [40] </ref> for a comparison of the two approaches 33 almost as in scientific publications. As a consequence, it dramatically simplifies the solving of these applications. From an algorithm standpoint, Helios is based on a state-of-the-art algorithms, combining techniques from numerical analysis (i.e., interval analysis) and artificial intelligence (i.e., consistency techniques).
Reference: [41] <author> P. Van Hentenryck and L. Michel. </author> <title> Newton: Constraint Programming over Nonlinear Constraints. </title> <journal> Science of Computer Programming. </journal> <note> To appear, </note> <year> 1996. </year>
Reference-contexts: However, these matrices are low-level, computer-centered, specifications of nonlinear applications; they are still far from the mathematical descriptions typically found in scientific publications. Constraint programming languages based on interval analysis (e.g., BNR-Prolog [31], CLP (BNR) [3], and Newton <ref> [41] </ref>) are a recent addition to the set of tools supporting interval analysis. These languages also bridge some of the gap between interval libraries and nonlinear applications. In addition, some of them (e.g., Newton [41]) are based on state-of-the-art interval algorithms, which make them appealing for computer scientists familiar with this <p> Constraint programming languages based on interval analysis (e.g., BNR-Prolog [31], CLP (BNR) [3], and Newton <ref> [41] </ref>) are a recent addition to the set of tools supporting interval analysis. These languages also bridge some of the gap between interval libraries and nonlinear applications. In addition, some of them (e.g., Newton [41]) are based on state-of-the-art interval algorithms, which make them appealing for computer scientists familiar with this class of languages. Nevertheless, constraint programs are still far from traditional statements of these applications. As a consequence, there is a need for software supporting interval analysis at a higher, human-centered, level. <p> As a consequence, we investigated how to introduce these techniques in a declarative way and proposed the concept of box-consistency which subsumes several techniques from interval analysis and artificial intelligence. 12 . This resulted in the design and implementation of Newton <ref> [41] </ref> which was used to solve many practical applications with an efficiency comparable or better to existing methods with a similar functionality. In developing these applications, it became clear that most programs had a similar structure and that a modeling language could possibly be compiled down to Newton.
Reference: [42] <author> P. Van Hentenryck, V. Saraswat, and Y. Deville. </author> <title> The Design, Implementation, and Evaluation of the Constraint Language cc(FD). In Constraint Programming: Basics and Trends. </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Helios is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence [20, 22] and used in many systems (e.g., <ref> [39, 42, 37] </ref>) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [43] <author> J Verschelde, P. Verlinden, and R. Cools. </author> <title> Homotopies Exploiting Newton Polytopes For Solving Sparse Polynomial Systems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 31(3) </volume> <pages> 915-930, </pages> <year> 1994. </year> <month> 36 </month>
Reference-contexts: Several interesting methods have been proposed in the past for the former task, including two fundamentally different methods: interval methods (e.g., [4, 5, 7, 8, 11, 14, 15, 18, 23, 30, 35]) and continuation methods (e.g., <ref> [29, 43] </ref>). Continuation methods have been shown to be effective for problems for which the total degree is not too high, since the number of paths explored depends on the estimation of the number of solutions. <p> We consider successively equation solving, unconstrained optimization, and constrained optimization. 9.1 Equation Solving We start with experimental results of Helios on a variety of standard benchmarks for equation solving. The benchmarks were taken from papers on numerical analysis [27], interval analysis [8, 11, 25], and continuation methods <ref> [21, 28, 29, 43] </ref>. Complete details on the benchmarks can be found in the appendix. We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> We also compare Helios with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. This method uses the same implementation technology as Helios and is denoted by HRB in the following. 11 Finally, we compare Helios with a state-of-the-art continuation method <ref> [43] </ref>, denoted by CONT in the following. Note that all results given in this section were obtained by running Helios on a Sun Sparc 10 workstation to obtain all solutions. In addition, the final intervals have widths smaller than 10 8 . The results are summarized in Table 1. <p> For a given dimension n, the problem can be stated as the system ( (x k + i=1 x i x i+k )x n c k = 0 (1 k n 1) l=1 x l + 1 = 0 and the constants can be chosen at random. <ref> [43] </ref> reports times (on a DEC-5000/200) of about 1 second for n = 4, 6 seconds for n = 5, 50 seconds for n = 6 and 990 seconds for n = 7.
References-found: 43

