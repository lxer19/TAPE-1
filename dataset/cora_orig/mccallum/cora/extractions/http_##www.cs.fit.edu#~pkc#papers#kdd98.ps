URL: http://www.cs.fit.edu/~pkc/papers/kdd98.ps
Refering-URL: http://www.cs.fit.edu/~pkc/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: pkc@cs.fit.edu  sal@cs.columbia.edu  
Title: Toward Scalable Learning with Non-uniform Class and Cost Distributions: A Case Study in Credit Card
Author: Philip K. Chan Salvatore J. Stolfo 
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Abstract: Very large databases with skewed class distributions and non-uniform cost per error are not uncommon in real-world data mining tasks. We devised a multi-classifier meta-learning approach to address these three issues. Our empirical results from a credit card fraud detection task indicate that the approach can significantly reduce loss due to illegitimate transactions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: In order to vary the fraud distribution from 10% to 90% for each month, we limit the size of the training sets to 6,400 transactions, which are sampled randomly without replacement. Four learning algorithms (C4.5 (Quinlan 1993), CART <ref> (Breiman et al. 1984) </ref>, RIPPER (Cohen 1995), and BAYES (Clark & Niblett 1989)) were used in our experiments. The results are plotted in Figure 1 (due to space limitations, only results from C4.5 are shown|the other algorithms behave similarly). <p> Not only is our method efficient, it is also scalable to larger amounts of data. Although downsampling instances of the majority class is not new for handling skewed distributions <ref> (Breiman et al. 1984) </ref>, our approach does not discard any data, allows parallelism for processing large amounts of data efficiently, and permits the usage of multiple "off-the-shelf" learning algorithms to increase diversity among the learned classifiers.
Reference: <author> Cardie, C., and Howe, N. </author> <year> 1997. </year> <title> Improving minority class prediction using case-specific feature weights. </title> <booktitle> In Proc. 14th Intl. Conf. Mach. Learning, </booktitle> <pages> 57-65. </pages>
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> Megainduction: A test flight. </title> <booktitle> In Proc. Eighth Intl. Work. Machine Learning, </booktitle> <pages> 596-599. </pages>
Reference-contexts: Until recently, researchers in machine learning have been focusing on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., <ref> (Catlett 1991) </ref>) and parallelism (e.g., (Provost & Aronis 1996)) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a specific parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993. </year> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. Multistrategy Learning, </booktitle> <pages> 150-165. </pages>
Reference-contexts: We devised a multi-classifier meta-learning approach to address these three issues. Our approach is based on creating data subsets with the appropriate class distribution, applying learning algorithms to the subsets independently and in parallel, and integrating to optimize cost performance of the classifiers by learning (meta-learning <ref> (Chan & Stolfo 1993) </ref>) from their classification behavior. That is, our method utilizes all available training examples and does not change the underlying learning algorithms. It also handles non-uniform cost per error and is cost-sensitive during the learning process. <p> For massive amounts of data, substantial improvement in speed can be achieved for super-linear-time learning algorithms. The generated classifiers are combined by learning (meta-learning) from their classification behavior. Several meta-learning strategies are described in <ref> (Chan & Stolfo 1993) </ref>. To simplify our discussion, we only describe the class-combiner (or stacking (Wolpert 1992)) strategy. In this strategy a meta-level training set is composed by using the (base) classifiers' predictions on a validation set as attribute values and the actual classification as the class label.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1995. </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> 90-98. </pages>
Reference-contexts: This training set is then used to train a meta-classifier. For integrating subsets, class combiner can be more effective than the voting-based techniques <ref> (Chan & Stolfo 1995) </ref>.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 261-285. </pages>
Reference-contexts: In order to vary the fraud distribution from 10% to 90% for each month, we limit the size of the training sets to 6,400 transactions, which are sampled randomly without replacement. Four learning algorithms (C4.5 (Quinlan 1993), CART (Breiman et al. 1984), RIPPER (Cohen 1995), and BAYES <ref> (Clark & Niblett 1989) </ref>) were used in our experiments. The results are plotted in Figure 1 (due to space limitations, only results from C4.5 are shown|the other algorithms behave similarly). Each data point is an average of 10 classifiers, each of which is generated from a separate month.
Reference: <author> Cohen, W. </author> <year> 1995. </year> <title> Fast effective rule induction. </title> <booktitle> In Proc. 12th Intl. Conf. Machine Learning, </booktitle> <pages> 115-123. </pages>
Reference-contexts: In order to vary the fraud distribution from 10% to 90% for each month, we limit the size of the training sets to 6,400 transactions, which are sampled randomly without replacement. Four learning algorithms (C4.5 (Quinlan 1993), CART (Breiman et al. 1984), RIPPER <ref> (Cohen 1995) </ref>, and BAYES (Clark & Niblett 1989)) were used in our experiments. The results are plotted in Figure 1 (due to space limitations, only results from C4.5 are shown|the other algorithms behave similarly).
Reference: <author> Fawcett, T., and Provost, F. </author> <year> 1997. </year> <title> Adaptive fraud detection. </title> <booktitle> Data Mining and Knowledge Discovery 1 </booktitle> <pages> 291-316. </pages>
Reference-contexts: A similar task is cellular phone fraud detection <ref> (Fawcett & Provost 1997) </ref>. Each of these three issues has not been widely studied in the machine learning research community. Fawcett (1996) summarized the responses to his inquiry on learning with skewed class distributions. The number of responses was few given skewed distributions are not rare in practice.
Reference: <author> Fawcett, T. </author> <year> 1996. </year> <title> Learning with skewed class distributions-summary of responses. </title> <journal> Machine Learning List, </journal> <volume> Vol. 8, No. </volume> <pages> 20. </pages>
Reference: <author> Kubat, M., and Matwin, S. </author> <year> 1997. </year> <title> Addressing the curse of imbal-anaced training sets: One sided selection. </title> <booktitle> In Proc. 14th Intl. Conf. Machine Learning, </booktitle> <pages> 179-186. </pages>
Reference: <author> Pazzani, M.; Merz, C.; Murphy, P.; Ali, K.; Hume, T.; and Brunk, C. </author> <year> 1994. </year> <title> Reducing misclassification costs. </title> <booktitle> In Proc. 11th Intl. Conf. Machine Learning, </booktitle> <pages> 217-225. </pages>
Reference-contexts: Error rate is commonly used in evaluating learning algorithms; cost-sensitive learning has not been widely investigated. Assuming the errors can be grouped into a few types and each type incurs the same cost, some Copyright c fl1998, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. studies (e.g., <ref> (Pazzani et al. 1994) </ref>) proposed algorithms that aim to reduce the total cost. Another line of cost-sensitive work tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain (Tan 1993).
Reference: <author> Provost, F., and Aronis, J. </author> <year> 1996. </year> <title> Scaling up inductive learning with massive parallelism. </title> <booktitle> Machine Learning 23 </booktitle> <pages> 33-46. </pages>
Reference-contexts: Until recently, researchers in machine learning have been focusing on small data sets. Efficiently learning from large amounts of data has been gaining attention due to the fast growing field of data mining, where data are abundant. Sampling (e.g., (Catlett 1991)) and parallelism (e.g., <ref> (Provost & Aronis 1996) </ref>) are the two main directions in scalable learning. Much of the parallelism work focuses on parallelizing a particular algorithm on a specific parallel architecture. That is, a new algorithm or architecture requires substantial amount of parallel programming work.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In order to vary the fraud distribution from 10% to 90% for each month, we limit the size of the training sets to 6,400 transactions, which are sampled randomly without replacement. Four learning algorithms (C4.5 <ref> (Quinlan 1993) </ref>, CART (Breiman et al. 1984), RIPPER (Cohen 1995), and BAYES (Clark & Niblett 1989)) were used in our experiments. The results are plotted in Figure 1 (due to space limitations, only results from C4.5 are shown|the other algorithms behave similarly).
Reference: <author> Tan, M. </author> <year> 1993. </year> <title> Cost-sensitive learning of classification knowledge and its applications in robotics. </title> <booktitle> Machine Learning 13:7. </booktitle>
Reference-contexts: All rights reserved. studies (e.g., (Pazzani et al. 1994)) proposed algorithms that aim to reduce the total cost. Another line of cost-sensitive work tries to reduce the cost in using a classifier. For instance, some sensing devices are costlier in the robotics domain <ref> (Tan 1993) </ref>. Fawcett and Provost (1997) considered non-uniform cost per error in their cellular phone fraud detection task and exhaustively searched (with a fixed increment) for the Linear Threshold Unit's threshold that minimizes the total cost. Until recently, researchers in machine learning have been focusing on small data sets.
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
Reference-contexts: For massive amounts of data, substantial improvement in speed can be achieved for super-linear-time learning algorithms. The generated classifiers are combined by learning (meta-learning) from their classification behavior. Several meta-learning strategies are described in (Chan & Stolfo 1993). To simplify our discussion, we only describe the class-combiner (or stacking <ref> (Wolpert 1992) </ref>) strategy. In this strategy a meta-level training set is composed by using the (base) classifiers' predictions on a validation set as attribute values and the actual classification as the class label. This training set is then used to train a meta-classifier.
References-found: 15

