URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/uai97b.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: email: cebly@cs.ubc.ca  
Title: Correlated Action Effects in Decision Theoretic Regression  
Author: Craig Boutilier 
Address: Vancouver, BC, CANADA, V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia  
Date: July, 1997  
Note: To appear, Proc. Thirteenth Conf. on Uncertainty in AI (UAI-97),Providence,  
Abstract: Much recent research in decision theoretic planning has adopted Markov decision processes (MDPs) as the model of choice, and has attempted to make their solution more tractable by exploiting problem structure. One particular algorithm, structured policy construction achieves this by means of a decision theoretic analog of goal regression, using action descriptions based on Bayesian networks with tree-structured conditional probability tables. The algorithm as presented is not able to deal with actions with correlated effects. We describe a new decision theoretic regression operator that corrects this weakness. While conceptually straightforward, this extension requires a somewhat more complicated technical approach.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1-2):81-138, </volume> <year> 1995. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit enumeration of the state space while constructing optimal or approximately optimal policies. Such techniques include the use of reachability analysis to eliminate (approximately) unreachable states <ref> [9, 1] </ref>, and state aggregation, whereby various states are grouped together and each aggregate state or cluster is treated as a single state. Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable [8, 3, 11, 5, 16].
Reference: [2] <author> Richard E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference-contexts: One of the key drawbacks of classic algorithms such as policy iteration [13] or value iteration <ref> [2] </ref> is the need to explicitly sweep through state space some number of times to determine the values of various actions at different states. <p> This connection has lead to the insight that the basic operations in computing optimal policies for MDPs can be viewed as a generalization of goal regression [5]. More specifically, a Bellman backup <ref> [2] </ref> for a specific action a is essentially a regression step where, instead of determining the the conditions under which one specific goal proposition will be achieved when a is executed, we determine the conditions under which a will lead to a number of different goal regions (each having different value) <p> We conclude in Section 5 with some remarks on future research and related work. 2 MDPs and Their Representation 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [2, 13, 15] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> The optimal value function V fl is the same as the value function for any optimal policy. A number of techniques for constructing optimal policies exist. An especially simple algorithm is value iteration <ref> [2] </ref>. We produce a sequence of n-step optimal value functions V n by setting V 0 = R, and defining V i+1 (s) = max fR (s) + fi t2S 2 More general formulations of reward (e.g., adding action costs) offer no special complications.
Reference: [3] <author> Craig Boutilier and Richard Dearden. </author> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1016-1022, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable <ref> [8, 3, 11, 5, 16] </ref>. In some of these aggregation techniques, the use of standard AI representations like STRIPS or Bayesian networks to represent actions in an MDP can be exploited to help construct the aggregations. <p> Fortunately, several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators <ref> [14, 3] </ref> and dynamic Bayes nets [10, 5]. We will use the latter. We assume that a set of variables V describes our system.
Reference: [4] <author> Craig Boutilier and Richard Dearden. </author> <title> Approximating value trees in structured dynamic programming. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 54-62, </pages> <address> Bari, Italy, </address> <year> 1996. </year>
Reference-contexts: A decision theoretic regression operator of this form is developed in [5]. The value functions being regressed are represented using decision trees, and the actions that are regressed through are represented using Bayes nets with tree-structured conditional probability tables. As shown there (see also <ref> [4] </ref>), classic algorithms for solving MDPs, such as value iteration or modified policy iteration, can be expressed purely in terms of decision theoretic regression, together with some tree manipulation. <p> We note that this paper does not offer much in the way of a conceptual advance in the understanding of the decision theoretic regression, and builds directly on the observations in <ref> [5, 4] </ref>. However, the modifications of these approaches to handle correlations are substantial enough, both in technical detail and in spirit, to warrant special attention. We review MDPs and their representation using Bayes nets and decision trees in Section 2. <p> decision trees and the DBN representation of the MDP is exploited to build these compact policies. 4 This 3 To simplify the presentation, we restric our attention to binary variables in our examples. 4 See [12] for a similar, though less general, method in the con technique is applied in <ref> [4] </ref> to value iteration, and dynamic approximation methods are considered as well. Roughly, if one has a tree representation of a value function, only certain variables will be mentioned as being relevant (under certain conditions) to value. <p> We focus here only on the construction of Q-treesthe remaining parts of the algorithms are straightforward. As in <ref> [5, 4] </ref>, we assume that no action has correlated effects (all have the form illustrated in Figure 1 (a)): this simplifies the algorithm considerably. <p> At each leaf of Tree (Q a ), replace probabilities labeling leaf with P c Pr (c)V (c), using these probabilities to determine Pr (c) for any context (branch) of Tree (V ). This decision theoretic regression algorithm forms the core of the policy construction techniques of <ref> [5, 4] </ref>. We illustrate the algorithm on the example above. We will regress the variables of Tree (V ) through action a in the order Y; W (generally, we want to respect the ordering within the tree as much as possible). <p> One concern about such approaches is the overhead involved in constructing appropriate trees. We note that this algorithm will behave exactly as the algorithms discussed in <ref> [5, 4] </ref> if there are no correlations. While we expect MDPs to often contain actions that exhibit correlations, it seems likely that many of these correlations will be localized.
Reference: [5] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable <ref> [8, 3, 11, 5, 16] </ref>. In some of these aggregation techniques, the use of standard AI representations like STRIPS or Bayesian networks to represent actions in an MDP can be exploited to help construct the aggregations. <p> This connection has lead to the insight that the basic operations in computing optimal policies for MDPs can be viewed as a generalization of goal regression <ref> [5] </ref>. <p> Classical goal regression can be viewed as a special case of this, where the action is deterministic and the value distinction is binary (goal states versus nongoal states). A decision theoretic regression operator of this form is developed in <ref> [5] </ref>. The value functions being regressed are represented using decision trees, and the actions that are regressed through are represented using Bayes nets with tree-structured conditional probability tables. <p> We note that this paper does not offer much in the way of a conceptual advance in the understanding of the decision theoretic regression, and builds directly on the observations in <ref> [5, 4] </ref>. However, the modifications of these approaches to handle correlations are substantial enough, both in technical detail and in spirit, to warrant special attention. We review MDPs and their representation using Bayes nets and decision trees in Section 2. <p> However, the modifications of these approaches to handle correlations are substantial enough, both in technical detail and in spirit, to warrant special attention. We review MDPs and their representation using Bayes nets and decision trees in Section 2. We briefly describe the basic decision theoretic regression operator of <ref> [5] </ref> in Section 3. In Section 4, we illustrate the challenges posed by correlated action effects for decision theoretic regression with several examples and describe an algorithm that meets these challenges. <p> Fortunately, several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [14, 3] and dynamic Bayes nets <ref> [10, 5] </ref>. We will use the latter. We assume that a set of variables V describes our system. <p> Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [5, 7] </ref> for a more detailed discussion To appear, Proc. Thirteenth Conf. on Uncertainty in AI (UAI-97),Providence, July, 1997 network (correlations); and (c) Reward Tree of this representation). 3 Figures 1 (a) and (b) illustrate this representation for two different actions. <p> Methods for optimal policy construction can use compact representations of policies and value functions in order to prevent enumeration of the state space. In <ref> [5] </ref>, a structured version of modified policy iteration is developed, in which value functions and policies are represented using decision trees and the DBN representation of the MDP is exploited to build these compact policies. 4 This 3 To simplify the presentation, we restric our attention to binary variables in our <p> We focus here only on the construction of Q-treesthe remaining parts of the algorithms are straightforward. As in <ref> [5, 4] </ref>, we assume that no action has correlated effects (all have the form illustrated in Figure 1 (a)): this simplifies the algorithm considerably. <p> At each leaf of Tree (Q a ), replace probabilities labeling leaf with P c Pr (c)V (c), using these probabilities to determine Pr (c) for any context (branch) of Tree (V ). This decision theoretic regression algorithm forms the core of the policy construction techniques of <ref> [5, 4] </ref>. We illustrate the algorithm on the example above. We will regress the variables of Tree (V ) through action a in the order Y; W (generally, we want to respect the ordering within the tree as much as possible). <p> One concern about such approaches is the overhead involved in constructing appropriate trees. We note that this algorithm will behave exactly as the algorithms discussed in <ref> [5, 4] </ref> if there are no correlations. While we expect MDPs to often contain actions that exhibit correlations, it seems likely that many of these correlations will be localized.
Reference: [6] <author> Craig Boutilier, Nir Friedman, Moises Goldszmidt, and Daphne Koller. </author> <title> Context-specific independence in Bayesian networks. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 115-123, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: We capture additional independence by assuming structured CPTs; that is, we exploit context-specific independence (CSI) as defined in <ref> [6] </ref>. In particular, we use a decision tree to represent the function that maps parent variable values to (conditional) probabilities.
Reference: [7] <author> Craig Boutilier and Moises Goldszmidt. </author> <title> The frame problem and Bayesian network action representations. </title> <booktitle> In Proceedings of the Eleventh Biennial Canadian Conference on Artificial Intelligence, </booktitle> <pages> pages 69-83, </pages> <address> Toronto, </address> <year> 1996. </year>
Reference-contexts: Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [5, 7] </ref> for a more detailed discussion To appear, Proc. Thirteenth Conf. on Uncertainty in AI (UAI-97),Providence, July, 1997 network (correlations); and (c) Reward Tree of this representation). 3 Figures 1 (a) and (b) illustrate this representation for two different actions.
Reference: [8] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the Twelfth International Joint Conferenceon Artificial Intelligence, </booktitle> <pages> pages 726-731, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable <ref> [8, 3, 11, 5, 16] </ref>. In some of these aggregation techniques, the use of standard AI representations like STRIPS or Bayesian networks to represent actions in an MDP can be exploited to help construct the aggregations.
Reference: [9] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 574-579, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: Recent research on the use of MDPs for DTP has focussed on methods for solving MDPs that avoid explicit enumeration of the state space while constructing optimal or approximately optimal policies. Such techniques include the use of reachability analysis to eliminate (approximately) unreachable states <ref> [9, 1] </ref>, and state aggregation, whereby various states are grouped together and each aggregate state or cluster is treated as a single state. Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable [8, 3, 11, 5, 16].
Reference: [10] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: Fortunately, several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators [14, 3] and dynamic Bayes nets <ref> [10, 5] </ref>. We will use the latter. We assume that a set of variables V describes our system.
Reference: [11] <author> Richard Dearden and Craig Boutilier. </author> <title> Abstraction and approximate decision theoretic planning. </title> <journal> Artificial Intelligence, </journal> <note> 1996. To appear. </note>
Reference-contexts: Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable <ref> [8, 3, 11, 5, 16] </ref>. In some of these aggregation techniques, the use of standard AI representations like STRIPS or Bayesian networks to represent actions in an MDP can be exploited to help construct the aggregations.
Reference: [12] <author> Thomas G. Dietterich and Nicholas S. Flann. </author> <title> Explanation-based learning and reinforcement learning: A unified approach. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 176-184, </pages> <address> Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: of modified policy iteration is developed, in which value functions and policies are represented using decision trees and the DBN representation of the MDP is exploited to build these compact policies. 4 This 3 To simplify the presentation, we restric our attention to binary variables in our examples. 4 See <ref> [12] </ref> for a similar, though less general, method in the con technique is applied in [4] to value iteration, and dynamic approximation methods are considered as well.
Reference: [13] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: One of the key drawbacks of classic algorithms such as policy iteration <ref> [13] </ref> or value iteration [2] is the need to explicitly sweep through state space some number of times to determine the values of various actions at different states. <p> We conclude in Section 5 with some remarks on future research and related work. 2 MDPs and Their Representation 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [2, 13, 15] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> In order to compare policies, we adopt expected total discounted reward as our optimality criterion; future rewards are discounted by rate 0 fi &lt; 1. The value of a policy can be shown to satisfy <ref> [13] </ref>: V (s) = R (s) + fi t2S The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 .
Reference: [14] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1073-1078, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Fortunately, several good representations for MDPs, suitable for DTP, have been proposed. These include stochastic STRIPS operators <ref> [14, 3] </ref> and dynamic Bayes nets [10, 5]. We will use the latter. We assume that a set of variables V describes our system.
Reference: [15] <author> Martin L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: We conclude in Section 5 with some remarks on future research and related work. 2 MDPs and Their Representation 2.1 Markov Decision Processes We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [2, 13, 15] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s.
Reference: [16] <author> John H. Tsitsiklis and Benjamin Van Roy. </author> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 59-94, </pages> <year> 1996. </year>
Reference-contexts: Recently, methods for automatic aggregation have been developed in which cer tain problem features are ignored, making certain states indistinguishable <ref> [8, 3, 11, 5, 16] </ref>. In some of these aggregation techniques, the use of standard AI representations like STRIPS or Bayesian networks to represent actions in an MDP can be exploited to help construct the aggregations.
Reference: [17] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Finally, given a value function V , we define the Q-function <ref> [17] </ref>, mapping state-action pairs into values, as follows: Q (s; a) = fR (s) + fi t2S This denotes the value of performing action a at state s assuming that value V is attained at future states (e.g., if we acted optimally after performing a and attained V fl subsequently).
References-found: 17

