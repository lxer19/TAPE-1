URL: http://www.cs.arizona.edu/people/jhh/papers/thesis.ps
Refering-URL: http://www.cs.arizona.edu/people/jhh/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Zebra Striped Network File System  
Author: by John Henry Hartman Sc. B. (Brown 
Degree: 1987 M.S. (University of California at Berkeley) 1990 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor John Ousterhout, Chair Professor Randy Katz Professor Ray Larson Dr. Felipe Cabrera  
Date: 1994  
Affiliation: University)  
Abstract-found: 0
Intro-found: 1
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout, </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 198-212. </pages> <note> Published as ACM SIGOPS Operating Systems Review 25, 5. </note>
Reference-contexts: Zebra is designed to support UNIX workloads as found in office/engineering environments. These workloads are characterized by short file lifetimes, sequential file accesses, infrequent write-sharing of a file by different clients, and many small files <ref> [Baker91] </ref>. This environment is also notable because of the behavior it does not exhibit, namely random accesses to existing files. Zebra is therefore designed to handle sequential file accesses well, perhaps at the expense of random file accesses. <p> If the application does indeed read from those blocks then it can do so without having to wait for the disk. Read-ahead requires predicting what blocks will be accessed in the future; fortunately, most UNIX applications read files sequentially and in their entirety <ref> [Baker91] </ref>, so read-ahead can be easily done. Write-behind allows application programs to write a block into the cache without waiting for it to be written to disk. The application can continue processing while the disk access occurs. <p> This is called write-through-on-close. This improves the performance of applications that write the same file blocks many times in a single open session, but unfortunately this is an uncommon activity <ref> [Baker91] </ref>. Another scheme writes the data to disk only when the application program makes a special fsync request, or when the new data has reached an age limit (typically 30 seconds). This type of cache is called a write-back cache. <p> One of the effects of needing to eventually write dirty blocks to the disk is that file caches are more effective at filtering out read requests than write requests <ref> [Baker91] </ref>. A larger cache can be expected to satisfy a larger number of read requests since there are more blocks in the cache. <p> The same isnt true for writes because a dirty block that lives 11 longer than the caches write-back time interval will have to be written to disk, independent of the size of the cache. The study of cache behavior by Baker et al. <ref> [Baker91] </ref> found that almost all disk writes were due to the 30-second writeback, and were therefore independent of the cache size. The result is that the read/write ratio seen by the disk has been skewed by the cache towards writes. <p> Unfortunately there are a number of factors that conspire to ensure that this will be the case. First, the best size for a striping unit appears to be tens of kilobytes or more [Chen90], which is larger than the average file size in many environments <ref> [Baker91] </ref>, so that even writes of entire files are not likely to fill an entire stripe. Second, when a file is written the file system must update its metadata. <p> Sequential write-sharing is the most common form of sharing, accounting for at least 80% of all write-sharing <ref> [Baker91] </ref>, and it is also the easiest to handle since clients only need to verify that the cached copy of a file is current when they open it. Concurrent write-sharing, on the other hand, occurs infrequently yet is expensive to handle since clients are simultaneously reading and writing a file. <p> The advantage of this scheme is that it simplifies the token implementation since clients only have to obtain tokens when a file is opened, and it has little effect on system performance since concurrent write-sharing is infrequent <ref> [Baker91] </ref>. 2.3.5 Server Crash Recovery A network file system is a collaboration between clients and servers: each performs some of the functions required for applications to access files. This collaboration leads to dependencies in the states of the machines involved. <p> The file server will not know that the file has been modified and the second client will read stale file data. Second, it reduces the effectiveness of the client cache at filtering short-lived file data because most files are open a very short period of time <ref> [Baker91] </ref> and it is unlikely that the data will be deleted before it is written to the server. Third, it reduces application performance because the application must wait for dirty file blocks to be written to the server when the file is closed. <p> In many cases files are created and deleted before the threshold age is reached so their data never need to be written at all <ref> [Baker91] </ref>. When information does need to be written to disk, the client forms the new data into one or more stripe fragments and writes them to the storage servers.
Reference: [Baker92a] <author> Mary Baker and Mark Sullivan, </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment, </title> <booktitle> Proceedings of the Summer 1992 USENIX Conference, </booktitle> <month> June </month> <year> 1992, </year> <pages> 31-43. </pages>
Reference: [Baker92b] <author> Mary Baker, Satoshi Asami, Etienne Deprit, and John Ousterhout, </author> <title> NonVolatile Memory for Fast, Reliable File Systems, </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992, </year> <pages> 10-22. </pages>
Reference-contexts: The rate at which applications invoke fsync will have a large impact on Zebras performance (or any other file systems) because fsyncs require synchronous disk operations. Baker et. al <ref> [Baker92b] </ref> found that under a transaction processing workload up to 90% of the segments written on an LFS file system were partial segments caused by an fsync. Such a workload would have poor performance on Zebra as well.
Reference: [Baker94] <author> Mary Baker, </author> <title> Fast Crash Recovery in Distributed File Systems, </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> January </month> <year> 1994. </year> <note> Also available as Technical Report UCB/CSD 94/787. </note>
Reference-contexts: The server learns from the clients which files they have cached, and which files they have open. The server uses this information to initialize its cache consistency mechanism before resuming file service to the clients. Another approach which promises higher performance and security has been proposed by Baker <ref> [Baker94] </ref>. The servers distributed state is stored in the volatile main memory of the server in such a way that it can be reused after a crash. <p> The issues involved in recovering the distributed state of a network file system are outside the scope of this thesis, but are covered in great detail in Bakers thesis <ref> [Baker94] </ref>. 65 The first Zebra-related issue in file manager recovery is to recover the contents of the virtual disk file at the time of the crash.
Reference: [Bernstein81] <author> Philip A. Bernstein and Nathan Goodman, </author> <title> Concurrency Control in Distributed Database Systems, </title> <journal> ACM Computing Surveys 13, </journal> <month> 2 (June </month> <year> 1981), </year> <pages> 185-222. </pages>
Reference-contexts: Unlike a RAID, the storage devices involved are not connected to the same computer, requiring an atomic commit protocol between the computers involved. There exist protocols for ensuring that two writes to two different servers are carried out atomically <ref> [Bernstein81] </ref>, but they are complex and expensive. Despite the disadvantages of file-based striping, its conceptual simplicity and ease of implementation have caused it to be used in all existing striped file systems.
Reference: [Bhide91a] <author> Anupam Bhide, Elmootazbellah N. Elnozahy, and Stephen P. Morgan, </author> <title> A Highly Available Network File Server, </title> <booktitle> Proceedings of the Winter 1991 USENIX Conference, </booktitle> <address> Dallas, TX, </address> <month> January </month> <year> 1991, </year> <pages> 199-205. </pages>
Reference: [Bhide91b] <author> Anupam Bhide, Elmootazbellah N. Elnozahy, Stephen P. Morgan, and Alex Siegel, </author> <title> A Comparison of Two Approaches to Build Reliable Distributed File Servers, </title> <booktitle> International Conference on Distributed Computing Systems (ICDCS), </booktitle> <year> 1991. </year>
Reference: [Birman84] <author> Kenneth P. Birman, Amr El Abbadi, Wally Dietrich, Thomas Joseph, and Thomas Raeuchle, </author> <title> An Overview of the Isis Project, </title> <type> Technical Report 84-642, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> October </month> <year> 1984. </year>
Reference-contexts: If a client contacts a server that does 33 not store a replica of the desired file the request is forwarded to a server that does. Updates to replicas are propagated using the ISIS distributed programming toolkit <ref> [Birman84] </ref>. 2.5.2 Dual-Ported Disks Another means of providing highly available file service is to connect dual-ported disks to two servers, as shown in Figure 2-6. Each disk can be accessed by either server, preventing a server failure from making a disk inaccessible.
Reference: [Brown85] <author> Mark R. Brown, Karen N. Koling, and Edward A. Taft, </author> <title> The Alpine File System, </title> <journal> ACM Transactions on Computer Systems 3, </journal> <volume> 4 (1985), </volume> <pages> 261-293. 146 </pages>
Reference-contexts: The advantage of using a log is that the recovery program need only check the metadata referred to by the records in the log, greatly reducing the recovery time. Examples of file systems that use logging in this way are Alpine <ref> [Brown85] </ref> and Cedar [Hagmann87]. 2.2 Disk Storage Systems Despite the benefits of caching file data in main memory, doing so cannot eliminate disk accesses completely.
Reference: [Cabrera91] <author> Luis-Felipe Cabrera and Darrell D. E. Long, Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <booktitle> Computing Systems 4, 4 (Fall 1991), </booktitle> <pages> 405-436. </pages>
Reference-contexts: For example, copying a file can be implemented by doing a block copy locally on each node storing a block. Bridge reports nearly linear speedup in applications that access file blocks in this manner. 2.6.3 Swift Swift <ref> [Cabrera91] </ref> improves the performance of network file systems by striping files across its file servers. Clients can access a file in parallel from the servers, improving the performance of file access. <p> If the client that is running the file manager should suffer a hardware failure another client can easily take over as the file manager since it too can access the metadata. A similar approach has been proposed by Cabrera and Long for the Swift file system <ref> [Cabrera91] </ref> to make its storage mediator highly available. The metadata is stored on the storage servers in a virtual disk implemented as a Zebra file.
Reference: [Chen90] <author> Peter M. Chen and David A. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of the 17th Annual International Symposium of Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 322-331. </pages>
Reference-contexts: Unfortunately there are a number of factors that conspire to ensure that this will be the case. First, the best size for a striping unit appears to be tens of kilobytes or more <ref> [Chen90] </ref>, which is larger than the average file size in many environments [Baker91], so that even writes of entire files are not likely to fill an entire stripe. Second, when a file is written the file system must update its metadata.
Reference: [Dibble90] <author> Peter C. Dibble, Michael L. Scott, and Carla Schlatter Ellis, </author> <title> Bridge: A High-Performance File System for Parallel Processors, </title> <booktitle> Proceedings of the 8th International Conference on Distributed Computing Systems (ICDCS), </booktitle> <year> 1988, </year> <pages> 154-161. </pages>
Reference-contexts: Parallel file systems do not typically worry about failures in the interconnect, processors, or storage systems. Examples of parallel file systems are CFS [Pierce89], sfs [LoVerso93], and Bridge <ref> [Dibble90] </ref>. All three support standard UNIX semantics as well as parallel access modes that allow multiple processors to access a file concurrently. CFS (Concurrent File System) is designed for the Intel iPSC/2 parallel computer. CFS has a single name process that manages the file system name space. <p> Data are then transferred between the I/O nodes and the processing nodes. The file system node is notified when the I/Os are complete so it can update the file metadata. Bridge <ref> [Dibble90] </ref> is designed to allow multiple processes in a parallel application to access a file concurrently. A centralized process called the Bridge server maintains the file system metadata. One common mode of operation uses the Bridge server to initiate parallel transfers.
Reference: [Drapeau94] <author> Ann L. Drapeau, Ken Shirriff, John H. Hartman, Ethan L. Miller, Srinivasan Seshan, Randy H. Katz, Ken Lutz, David A. Patterson, Edward K. Lee, Peter M. Chen, and Garth A. Gibson, </author> <title> RAID-II: A High-Bandwidth Network File Server, </title> <booktitle> Proceedings of the 21st Annual International Symposium of Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: On a DECstation 5000/200 workstation, for example, these copies only proceed at about 6-8 Mbytes/second. The Berkeley RAID project has built a special-purpose memory system with a dedicated high-bandwidth path between the network and the disks <ref> [Drapeau94] </ref> but even this system can support only a few dozen disks at full speed. A write that does not fill a whole stripe requires a parity update. The old data and parity must be read, the new parity computed, and the new data and parity written. <p> The division of tasks between separate processors allows overlapping of the tasks so that system performance can be improved in a balanced manner. For example, network bottlenecks can be alleviated by adding networks and network processors without affecting the rest of the server configuration. 2.4.1.2 RAID-II RAID-II <ref> [Drapeau94] </ref> is a research project at U.C. Berkeley to build a high-performance network file server from a disk array and a host workstation. <p> In addition, since the server does not interpret the contents of the fragments it stores there is no need for the fragments to actually cross the host backplane at all. Some server architectures, such as RAID-II <ref> [Drapeau94] </ref>, implement a high-performance data path between the network and the disk subsystem. This data path allows data to ow between the networks and the disks without being copied to the host memory system across the host backplane, as would be the case in a traditional file server architecture.
Reference: [Guy90] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier, </author> <title> Implementation of the Ficus Replicated File System, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 63-71. </pages>
Reference: [Hagmann87] <author> Robert Hagmann, </author> <title> Reimplementing the Cedar File System Using Logging and Group Commit, </title> <booktitle> Proceedings of the 131h Symposium on Operating Systems Principles (SOSP), </booktitle> <month> November, </month> <year> 1987, </year> <pages> 155-162. </pages> <note> Published as ACM SIGOPS Operating Systems Review 21, 5. </note>
Reference-contexts: The advantage of using a log is that the recovery program need only check the metadata referred to by the records in the log, greatly reducing the recovery time. Examples of file systems that use logging in this way are Alpine [Brown85] and Cedar <ref> [Hagmann87] </ref>. 2.2 Disk Storage Systems Despite the benefits of caching file data in main memory, doing so cannot eliminate disk accesses completely.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout, </author> <title> Letter to the Editor, </title> <booktitle> ACM SIGOPS Operating Systems Review 27, </booktitle> <month> 1 (January </month> <year> 1993), </year> <pages> 7-10. </pages>
Reference: [Hisgen89] <author> Andy Hisgen, Andrew Birrell, Timothy Mann, Michael Schroeder, and Garret Swart, </author> <title> Availability and Consistency Tradeoffs in the Echo Distributed File System, </title> <booktitle> Proceedings of the Second Workshop on Workstation Operating Systems, </booktitle> <month> September </month> <year> 1989, </year> <pages> 49-54. </pages>
Reference-contexts: Some examples of network file systems or file server architectures that provide replicated copies are Locus [Walker83], Coda [Satyanarayanan90], Ficus [Guy90][Page91], Echo <ref> [Hisgen89] </ref>, Harp [Liskov91], and Deceit [Siegel90]. Locus, Coda, and Echo are complete network file systems in the sense that they define a client/ server protocol for accessing files. <p> Care must be taken that only one server at a time thinks it is the owner of a disk or chaos will ensue. Two examples of highly-available file servers that use dual-ported disks are Echo <ref> [Hisgen89] </ref> and HA-NFS [Bhide91a][Bhide91b]. Echo uses a combination of replication and dual-ported disks to provide highly available file service. HA-NFS uses dual-ported disks to provide highly available NFS file service.
Reference: [Hitz94] <author> Dave Hitz, James Lau, and Michael Malcolm, </author> <title> File System Design for an NFS File Server Appliance, </title> <booktitle> Proceedings of the Winter 1994 USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1994, </year> <pages> 235-246. </pages>
Reference-contexts: By using non-overwrite to modify file blocks, clients can write unrelated file blocks to the same stripe and perform a single parity update that covers all of the blocks. This same technique used in the write-anywhere file layout (WAFL) <ref> [Hitz94] </ref> to reduce the cost of partial stripe writes.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West, </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: All subsequent accesses to the file are sent through to the server, ensuring that the clients always access the most recent contents of the file, even if the file is being concurrently write-shared. 2.3.8 AFS/DEcorum The Andrew File System (AFS) <ref> [Howard88] </ref> is a distributed file system intended to support very large numbers of clients. Clients cache file data on their local disks to improve performance and to reduce the load on the servers. <p> Second, clients can cache naming information so that the file manager need not be contacted for most opens and closes. Client-level name caching has been used successfully in the AFS file system <ref> [Howard88] </ref> and Shirriff found that a name cache occupying only 40 Kbytes of a clients memory can produce a hit rate of 97% [Shirriff92].
Reference: [Kazar90] <author> Michael L. Kazar, Bruce W. Leverett, Owen T. Anderson, Vasilis Apostolides, Beth A. Bottos, Sailesh Chutani, Craig F. Everhart, W. Anthony Mason, Shu-Tsui Tu, Edward R. Zayas, </author> <title> DEcorum File System Architectural Overview, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 151-163. </pages>
Reference-contexts: The server does not notice that a file has been modified until it is closed, so that it cannot invalidate other cached copies of a file while the file is being modified. During that time the other clients will use out-of-date data from their caches. Transarcs DEcorum <ref> [Kazar90] </ref> is a commercial version of AFS that provides better consistency guarantees and higher performance. Perfect client cache consistency is guaranteed through the use of tokens that are explicitly passed between the clients and the server.
Reference: [Koch87] <author> Philip D. L. Koch, </author> <title> Disk File Allocation Based on the Buddy System, </title> <journal> ACM Transactions on Computer Systems 4, </journal> <volume> 5 (1987), </volume> <pages> 32-370. </pages>
Reference-contexts: Fragmentation is reduced by reorganizing the disk during off-peak hours, or as necessary. During reorganization files are moved around on the disk to eliminate any space lost due to external fragmentation. The drawbacks of contiguous allocation have led to the development of extent-based file systems. Examples include DTSS <ref> [Koch87] </ref> and EFS [McVoy91]. An extent is a fixed-sized contiguous region of the disk. The idea is that an extent is large enough so that the cost to seek to its start it negligible when amortized over all of the bytes that are subsequently transferred.
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams, </author> <title> Replication in the Harp File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 226-238. </pages> <note> Published as ACM SIGOPS Operating Systems Review 25, 5. 147 </note>
Reference-contexts: Some examples of network file systems or file server architectures that provide replicated copies are Locus [Walker83], Coda [Satyanarayanan90], Ficus [Guy90][Page91], Echo [Hisgen89], Harp <ref> [Liskov91] </ref>, and Deceit [Siegel90]. Locus, Coda, and Echo are complete network file systems in the sense that they define a client/ server protocol for accessing files.
Reference: [Long94] <author> Darrell D. E. Long, Bruce R. Montague, and Luis-Felipe Cabrera, </author> <title> Swift/ RAID: A Distributed RAID System, </title> <booktitle> Computing Systems 7, 3 (Summer 1994), </booktitle> <pages> 333-359. </pages>
Reference-contexts: Files were striped uniformly across the storage agents. This prototype has shown nearly linear speedup of file reads and writes when the number of storage agents is increased from one to three. Recently the Swift prototype has been reimplemented to incorporate the reliability mechanisms <ref> [Long94] </ref>. The prototype can now support a variety of parity organizations. Measurements indicate the parity computation incurs a significant overhead, so that the resulting performance of a five-server system with parity enabled is only 53% of the original Swift prototype with the same number of servers.
Reference: [Lo Verso93] <author> Susan J. Lo Verso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler, sfs: </author> <title> A Parallel File System for the CM-5, </title> <booktitle> Proceedings of the Summer 1993 USENIX Conference, </booktitle> <address> Cincinnati, OH, </address> <month> June </month> <year> 1993, </year> <pages> 291-305. </pages>
Reference-contexts: Interestingly, CFS does allow processing nodes to cache copies of the file structure to avoid fetching it on every file access. These copies are kept adequately up to date through simple lazy conventions [Pierce89]. The sfs file system <ref> [Lo Verso93] </ref> provides parallel file access on the CM-5 parallel computer. The NFS protocol is used to communicate between the processing nodes and a centralized processor that runs the file system.
Reference: [McKusick84] <author> Marshall K. McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry, </author> <title> A Fast File System for UNIX, </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 3 (August </month> <year> 1984), </year> <pages> 181-197. </pages>
Reference-contexts: On average one-half of the last extent allocated to a file will be left unused. This space cannot be used by another file because it is smaller than an extent. The UNIX Fast File System (FFS) <ref> [McKusick84] </ref> strikes a compromise between contiguous allocation and extent-based allocation by allocating the disk in units of file blocks, but allocating blocks contiguously when possible. When a block is appended to a file its location is chosen based upon the location of the previous block.
Reference: [McVoy91] <author> Larry W. McVoy and Steve R. Kleiman, </author> <title> Extent-like Performance from a UNIX File System, </title> <booktitle> Proceedings of the Winter 1991 USENIX Conference, </booktitle> <address> Dallas, TX, </address> <month> January </month> <year> 1991, </year> <pages> 33-43. </pages>
Reference-contexts: During reorganization files are moved around on the disk to eliminate any space lost due to external fragmentation. The drawbacks of contiguous allocation have led to the development of extent-based file systems. Examples include DTSS [Koch87] and EFS <ref> [McVoy91] </ref>. An extent is a fixed-sized contiguous region of the disk. The idea is that an extent is large enough so that the cost to seek to its start it negligible when amortized over all of the bytes that are subsequently transferred.
Reference: [Moran90] <author> J. Moran, R. Sandberg, D. Coleman, J. Kepecs, and B. Lyon, </author> <title> Breaking Through the NFS Performance Barrier, </title> <booktitle> Proceedings of EUUG Spring 1990, </booktitle> <address> Munich, Germany, </address> <month> April </month> <year> 1990, </year> <pages> 199-206. </pages>
Reference-contexts: Modified inodes and indirect blocks are buffered in the NVRAM before being written to disk. This allows multiple modifications to the same file to be filtered by the NVRAM, and allows the inodes and indirect blocks be written to disk in an efficient order <ref> [Moran90] </ref>. 2.3.7 Sprite The Sprite [Ousterhout88] network file system is designed to provide both high performance and perfect cache consistency. Both of these goals require file servers to be stateful, i.e. to keep track of which clients are caching which files.
Reference: [Nelson93] <author> Bruce Nelson and Raphael Frommer, </author> <title> An Overview of Functional Multiprocessing for NFS Network Servers, </title> <type> Technical Report 1, Seventh Edition, </type> <institution> Auspex Systems Inc., </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Once a file server saturates the addition of more or faster clients will result in lower performance for each client. If more performance is needed then a faster server must be purchased. This lack of scalability has led to larger and larger file servers, such as the Auspex <ref> [Nelson93] </ref>. These special-purpose machines are tailored to provide file service to many more clients than a mere workstation-based file server can support. <p> These machines are tailored to their file server task by having high-bandwidth data paths connecting the network to the disks and multiple processors for handling client requests. The following sections give some examples of high-performance file servers. 2.4.1.1 Auspex NS 6000 The Auspex NS 6000 <ref> [Nelson93] </ref> is a special-purpose computer designed to provide high-performance NFS file service. The main focus is on supporting high aggregate loads generated by large numbers of clients and networks.
Reference: [Nelson88] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout, </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988), </year> <pages> 134-154. </pages>
Reference-contexts: It can continue processing and the data will be written back later, improving the application performance. A comparison of Sprite and NFS performance made several years ago found that Sprite was 30-40% faster than NFS <ref> [Nelson88] </ref>. A more recent study on faster workstations found that Sprites performance improved to 50-100% faster than NFS [Ousterhout90]. This is because NFSs write-through-on-close policy ties application speed to disk speed, whereas Sprites write-back policy decouples the two. <p> For example, a client could write a file that is cached on another client; if the second client subsequently reads the file, it must discard its stale cached data and fetch the new data. Zebras cache consistency mechanism is similar to that used in Sprite <ref> [Nelson88] </ref>. Clients notify the file manager when they open and close files. This allows the file manager to keep track of which clients are caching which files, and whether or not they have any dirty blocks for the file.
Reference: [Ousterhout88] <author> John Ousterhout, Andrew Cherenson, Fred Douglis, Mike Nelson, and Brent Welch, </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: This is analogous to losing the data stored in a UNIX file system cache when the machine crashes. 1.3 Prototype I have implemented a Zebra prototype in the Sprite operating system <ref> [Ousterhout88] </ref>, and I have run a variety of benchmarks that demonstrate the advantages of Zebra over existing network file systems. Although Sprite was used as a vehicle for developing the Zebra prototype, the Zebra architecture is not dependent on Sprite in any way. <p> The file server returns a file handle for the entry. The client repeats the operation until the end of the path is reached. In Sprite <ref> [Ousterhout88] </ref>, the clients send the entire pathname to the file server, which does the name lookup and returns the resulting file handle. <p> After a server crash its token state must be recovered. The details of server crash recovery are covered in the next section. One variation on the token-based approach is to handle concurrent write-sharing by revoking all tokens, as is done in Sprite <ref> [Ousterhout88] </ref>. In this scheme concurrent write-sharing causes the server to revoke all tokens for the file, which in turn causes the clients to forward to the server all application read and write requests to the file. <p> Modified inodes and indirect blocks are buffered in the NVRAM before being written to disk. This allows multiple modifications to the same file to be filtered by the NVRAM, and allows the inodes and indirect blocks be written to disk in an efficient order [Moran90]. 2.3.7 Sprite The Sprite <ref> [Ousterhout88] </ref> network file system is designed to provide both high performance and perfect cache consistency. Both of these goals require file servers to be stateful, i.e. to keep track of which clients are caching which files. This allows clients to use write-back caches while avoiding stale data errors.
Reference: [Ousterhout90] <author> John Ousterhout, </author> <title> Why Arent Operating Systems Getting Faster As Fast As Hardware?, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 247-256. </pages>
Reference-contexts: A comparison of Sprite and NFS performance made several years ago found that Sprite was 30-40% faster than NFS [Nelson88]. A more recent study on faster workstations found that Sprites performance improved to 50-100% faster than NFS <ref> [Ousterhout90] </ref>. This is because NFSs write-through-on-close policy ties application speed to disk speed, whereas Sprites write-back policy decouples the two. Processor speed is increasing at a faster rate than disk speed, allowing Sprite to make better use of newer hardware.
Reference: [Page91] <author> Thomas W. Page, Jr., Richard G. Guy, John S. Heidemann, Gerald J. Popek, Wai Mak, and Dieter Rothmeier, </author> <title> Management of Replicated Volume Location Data in the Ficus Replicated File System, </title> <booktitle> Proceedings of the Summer 1991 USENIX Conference, </booktitle> <address> Nashville, TN, </address> <month> June </month> <year> 1991, </year> <pages> 17-29. </pages>
Reference: [Patterson88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988, </year> <pages> 109-116. </pages>
Reference-contexts: There are more servers to fail and therefore a higher probability that at any given time a server will be down and file data will be unavailable. Failures can be masked either by replicating the file data or by using parity in the style of RAID <ref> [Patterson88] </ref> disk arrays. <p> most file systems already cache file data in the main memory of the computer, reducing the locality of disk accesses and reducing the effectiveness of a cache on the disk itself. 2.2.1.1 RAID The difficulties in improving disk performance led to the development of RAID (Redundant Array of Inexpensive Disks) <ref> [Patterson88] </ref>, in which many small disks work together to provide increased performance and data availability. A RAID appears to higher-level software as a single very large and fast disk. Transfers to or from the disk array are divided into blocks called striping units. <p> of this size is only half that of writing full segments, so that fsync reduces the Zebra write bandwidth by less than 10%. 4.3.3 Storage Server Crashes Zebras parity mechanism allows the clients to tolerate the failure of a single storage server using algorithms similar to those described for RAIDs <ref> [Patterson88] </ref>. To read a file while a storage server is down, a client must reconstruct any stripe fragment that was stored on the down server. This is done by computing the parity of all the other fragments in the same stripe; the result is the missing fragment.
Reference: [Pierce89] <author> Paul Pierce, </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem, </title> <booktitle> Proceedings of the Fourth Conference on Hypercubes, </booktitle> <address> Monterey CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Parallel file systems do not typically worry about failures in the interconnect, processors, or storage systems. Examples of parallel file systems are CFS <ref> [Pierce89] </ref>, sfs [LoVerso93], and Bridge [Dibble90]. All three support standard UNIX semantics as well as parallel access modes that allow multiple processors to access a file concurrently. CFS (Concurrent File System) is designed for the Intel iPSC/2 parallel computer. <p> Interestingly, CFS does allow processing nodes to cache copies of the file structure to avoid fetching it on every file access. These copies are kept adequately up to date through simple lazy conventions <ref> [Pierce89] </ref>. The sfs file system [Lo Verso93] provides parallel file access on the CM-5 parallel computer. The NFS protocol is used to communicate between the processing nodes and a centralized processor that runs the file system.
Reference: [Rosenblum91] <author> Mendel Rosenblum and John K. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating Systems Principles (SOSP), Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 1-15. </pages> <note> Published as ACM SIGOPS Operating Systems Review 25, 5. 148 </note>
Reference-contexts: The desire to allow many files to be accessed in a single transfer led to the development of the log-structured file system (LFS) <ref> [Rosenblum91] </ref>, which is one of the underlying technologies in Zebra. A log-structured file system treats the disk like an append-only log. <p> The inherent limitations of file-based striping led to the development in Zebra of an alternative striping scheme called log-based striping. Log-based striping borrows ideas from log-structured file systems (LFS) <ref> [Rosenblum91] </ref>, so that instead of striping individual files, as is done in file-based striping, Zebra interposes a log abstraction between the files and the disks and stripes the logs. <p> If there are no empty stripes and more free space is needed then the cleaner chooses one or more stripes to clean from the set of stripes that are cleanable and contain live data. The policy it uses for this is identical to the one described by Rosenblum <ref> [Rosenblum91] </ref>, i.e., a cost-benefit analysis is done for each stripe, which considers both the amount of live data in the stripe and the age of the data. <p> There are two ways to avoid this race condition: locking the files to ensure exclusive access, as was done in Sprite LFS <ref> [Rosenblum91] </ref>, and an optimistic approach pioneered by Seltzer et al. [Seltzer93] and used in Zebra. The most straight-forward way of avoiding a cleaning race is to lock a file when it is being cleaned. <p> Rosenblum measured production usage of LFS on Sprite for several months and found that only 2-7% of the data in stripes that were cleaned were live and needed to be copied <ref> [Rosenblum91] </ref>. <p> Under this workload the cleaner bandwidth can support an infinite system bandwidth. The answer to the more relevant question of what cleaning bandwidth is required to support a real workload can be estimated from the LFS studies. Rosenblum <ref> [Rosenblum91] </ref> found that only 2-7% of the bytes in stripes that were cleaned were live and needed to be copied.
Reference: [Rosenblum92] <author> Mendel Rosenblum, </author> <title> The Design and Implementation of a Log-structured File System, </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> June </month> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 92/696. </note>
Reference-contexts: By checking only the portion of the log created since the last checkpoint LFS is able to recover from a crash in significantly shorter time than traditional UNIX file systems. Rosenblum reported recovery times on the order of one second for LFS <ref> [Rosenblum92] </ref>, as compared to many minutes for UNIX file systems. LFS has two features that make it especially well-suited for use on a RAID: large writes, and the ability to find the end of the log after a crash. LFS writes to the disk in large transfers.
Reference: [Ruemmler93] <author> Chis Ruemmler and John Wilkes, </author> <title> UNIX disk access patterns, </title> <booktitle> Proceedings of the Winter 1993 USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993, </year> <pages> 405-420. </pages>
Reference-contexts: Higher transfer rates require either higher rotational speed or higher bit density. Higher bit density is achieved through reducing the ying height of the head or improving the head sensitivity. The net result is that raw disk performance isnt improving very rapidly, only about 7% per year <ref> [Ruemmler93] </ref>. Advances have been made, however, in improving the effective disk performance by using caching to take advantage of locality in the workload. Most disks now contain a track buffer, which is used to store the contents of the track currently being accessed. <p> Systems with many users and many running applications per disk might generate lots of simultaneous disk accesses that can be effectively scheduled, but it has been shown that in the UNIX environment 70% of the disk accesses encounter an idle disk, and the average queue length is less than ten <ref> [Ruemmler93] </ref>. Maximum queue lengths of over 1000 were measured on a file server serving 200 users, but queues this long were seen by less than 1% of the accesses. Maximum queue lengths on workstations were less than 100.
Reference: [Sandberg85] <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon, </author> <title> Design and Implementation of the Sun Network Filesystem, </title> <booktitle> Proceedings of the Summer 1985 USENIX Conference, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1985, </year> <pages> 119-130. </pages>
Reference-contexts: The migration out of the machine room was not universal, however. Left behind were the file servers. These machines stored the users files and made those files accessible to other computers (clients) via the network. Network file systems, such as NFS 2 <ref> [Sandberg85] </ref>, were developed to define the interface between the clients and the servers and to allow a single server to handle many clients. File servers were originally ordinary workstations, outfitted with more disks on which to store their files. <p> In a network file system the application is separated from the disk by a network, and thus there are two places in which the name lookup can logically occur: on the client or on the file server. In the NFS <ref> [Sandberg85] </ref> network file system the clients do the name lookup by traversing the path themselves. <p> The disadvantage is that a client may occasionally read stale file data from its cache. An example of a network file system that uses a time-based approach is NFS <ref> [Sandberg85] </ref>. An alternative is to use tokens to ensure that clients never cache obsolete versions of files. Each file in use has two types of tokens associated with it: read tokens and write tokens. <p> This allows the server to recover its state without interacting with the clients. 2.3.6 NFS The de facto standard network file system in the UNIX workstation environment is Suns Network File System (NFS) <ref> [Sandberg85] </ref>. NFS was designed to be simple, and as a consequence uses stateless file servers to avoid the complexity associated with maintaining and recovering distributed state. For example, servers do not keep track of the contents of the clients caches.
Reference: [Satyanarayanan90] <author> Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. Okasaki, E. H. Siegel, and D. C. Steere, Coda: </author> <title> a highly available file system for a distributed workstation environment, </title> <journal> IEEE Transactions on Computers 39, </journal> <month> 4 (April </month> <year> 1990), </year> <pages> 447-459. </pages>
Reference-contexts: Some examples of network file systems or file server architectures that provide replicated copies are Locus [Walker83], Coda <ref> [Satyanarayanan90] </ref>, Ficus [Guy90][Page91], Echo [Hisgen89], Harp [Liskov91], and Deceit [Siegel90]. Locus, Coda, and Echo are complete network file systems in the sense that they define a client/ server protocol for accessing files.
Reference: [Seltzer90] <author> Margo Seltzer, Peter Chen, and John Ousterhout, </author> <title> Disk Scheduling Revisited, </title> <booktitle> Proceedings of the Winter 1990 USENIX Conference, </booktitle> <month> January </month> <year> 1990, </year> <pages> 313-324. </pages>
Reference-contexts: Accessing them in the reverse order results in seek distances of 999 cylinders and 998 cylinders (1997 total), roughly doubling the amount of seek time required to access the sectors. A recent study <ref> [Seltzer90] </ref> found that by intelligently scheduling long sequences of random requests the disk bandwidth can be improved from about 7% to 25% of the disks raw bandwidth. Disk scheduling works best in environments where there are many pending disk accesses to be scheduled.
Reference: [Seltzer93] <author> Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, and Carl Staelin, </author> <title> An Implementation of a Log-Structured File System for UNIX, </title> <booktitle> Proceedings of the Winter 1993 USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1993, </year> <pages> 307-326. </pages>
Reference-contexts: There are two ways to avoid this race condition: locking the files to ensure exclusive access, as was done in Sprite LFS [Rosenblum91], and an optimistic approach pioneered by Seltzer et al. <ref> [Seltzer93] </ref> and used in Zebra. The most straight-forward way of avoiding a cleaning race is to lock a file when it is being cleaned. The cleaner simply locks the file, cleans the desired blocks, and unlocks the a 1 m ( ) m a m 2m 70 file. <p> them results in additional messages to the file manager, reducing the performance of both the cleaner and the file manager. 4.5.4 Optimistic Cleaning To avoid the performance problems associated with locking files to clean them the Zebra stripe cleaner uses an optimistic approach similar to that of Seltzer et al. <ref> [Seltzer93] </ref>. The idea behind optimistic cleaning is that blocks are cleaned without any synchronization with other applications. Applications may therefore cause a race by modifying a block at the same time it is cleaned. <p> While I have not measured Zebras cleaning overhead under real workloads, it should be comparable to those for other log-structured file systems. In a synthetic transaction-processing benchmark on a nearly full disk, Seltzer found that cleaning accounted for 60-80% of all write traffic and significantly affected system throughput <ref> [Seltzer93] </ref>. However, Seltzer found cleaning costs to be negligible in a software development benchmark that is more typical of workstation workloads. <p> In a transaction processing environment, on the other hand, there is typically a single large long-lived file (the database) that is overwritten and accessed randomly. There is little experience with LFS in such an environment, and Seltzers measurements <ref> [Seltzer93] </ref> suggest that there may be performance problems. More work is needed to understand the problems and see if there are simple solutions. Zebra may also suffer performance problems under workloads characterized by reading many small files.
Reference: [Shirriff92] <author> Ken Shirriff and John Ousterhout, </author> <title> A Trace-driven Analysis of Name and Attribute Caching in a Distributed File System, </title> <booktitle> Proceedings of the Winter 1992 USENIX Conference, </booktitle> <month> January </month> <year> 1992, </year> <pages> 315-331. </pages>
Reference-contexts: Client-level name caching has been used successfully in the AFS file system [Howard88] and Shirriff found that a name cache occupying only 40 Kbytes of a clients memory can produce a hit rate of 97% <ref> [Shirriff92] </ref>. Client name caching has not been implemented in the Zebra prototype described in Chapter 5 because of the difficulty in adding it to the Sprite operating system, but I would expect that a production version of Zebra would do so. <p> Shirriff <ref> [Shirriff92] </ref> has shown that name caching can be very effective at reducing the load on the file server (a 40-Kbyte name cache on a client produces a hit rate of 97%), making the load on the file manager in the prototype very different from its load in a real system.
Reference: [Siegel90] <author> Alex Siegel, Kenneth Birman, and Keith Marzullo, Deceit: </author> <title> A Flexible Distributed File System, </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June </month> <year> 1990, </year> <pages> 51-61. </pages>
Reference-contexts: Some examples of network file systems or file server architectures that provide replicated copies are Locus [Walker83], Coda [Satyanarayanan90], Ficus [Guy90][Page91], Echo [Hisgen89], Harp [Liskov91], and Deceit <ref> [Siegel90] </ref>. Locus, Coda, and Echo are complete network file systems in the sense that they define a client/ server protocol for accessing files.
Reference: [van Renesse88] <author> Robbert van Renesse, Andrew S. Tanenbaum, and Annita Wilschut, </author> <title> The Design of a High-Performance File Server, </title> <institution> IR-178, Vrije Universiteit Amsterdam, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Eventually the file system may find itself unable to store a file because there isnt a big enough free region, even though there is plenty of free space on the disk. An example of an existing file system that uses contiguous allocation is Bullet <ref> [van Renesse88] </ref>. Bullet does not provide UNIX semantics for the files it stores, which makes it easier to implement contiguous allocation. Space is preallocated for file data by specifying the ultimate size of a file when it is created. Files cannot be grown by appending, nor can they be modified. <p> First, some network file systems, such as Bullet <ref> [van Renesse88] </ref>, require clients to read and write whole files. The problem with this approach is that clients cannot access files that are larger than they can store. Second, Zebra uses a log abstraction similar to that used in LFS between the clients and servers.
Reference: [Walker83] <author> Bruce Walker, Gerald Popek, Robert English, Charles Kline, and Greg Thiel, </author> <title> The LOCUS Distributed Operating System, </title> <booktitle> Proceedings of the 9th Symposium on Operating Systems Principles (SOSP), </booktitle> <month> November </month> <year> 1983, </year> <pages> 49-70. </pages> <note> Published as ACM SIGOPS Operating Systems Review 17, 5. </note>
Reference-contexts: Some examples of network file systems or file server architectures that provide replicated copies are Locus <ref> [Walker83] </ref>, Coda [Satyanarayanan90], Ficus [Guy90][Page91], Echo [Hisgen89], Harp [Liskov91], and Deceit [Siegel90]. Locus, Coda, and Echo are complete network file systems in the sense that they define a client/ server protocol for accessing files.
Reference: [Welch86] <author> Brent B. Welch, </author> <title> The Sprite Remote Procedure Call System, </title> <type> Technical Report UCB/CSD 86/302, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> June </month> <year> 1986. </year>
Reference-contexts: This section describes how data are transferred between the clients and the servers in the prototype, allowing data transfer to be both efficient and scalable. 5.3.1 Remote Procedure Calls The clients and servers in the Zebra prototype communicate via Sprite remote procedure calls (RPC) <ref> [Welch86] </ref>. A remote procedure call emulates a procedure call over the network. The client sends parameters to the server, and the server sends a reply.
Reference: [Wilkes89] <author> John Wilkes, </author> <title> DataMesh -- Scope and Objectives: A Commentary, </title> <type> Technical Report HPL-DSD-89-44, </type> <institution> Hewlett-Packard Company, </institution> <address> Palo Alto, CA, </address> <month> July 19 </month> <year> 1989. </year>
Reference: [Wilkes91] <author> John Wilkes, </author> <title> DataMesh -- Parallel Storage Systems for the 1990s, </title> <booktitle> Proceedings of the Eleventh IEEE Symposium on Mass Storage Systems, </booktitle> <address> Monterey, CA, </address> <month> October </month> <year> 1991, </year> <pages> 131-136. </pages>
References-found: 48

