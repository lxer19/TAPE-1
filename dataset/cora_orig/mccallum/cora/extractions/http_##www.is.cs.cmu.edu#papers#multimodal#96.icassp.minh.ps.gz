URL: http://www.is.cs.cmu.edu/papers/multimodal/96.icassp.minh.ps.gz
Refering-URL: http://www.cs.cmu.edu:80/afs/cs.cmu.edu/user/tue/WWW/resume.html
Root-URL: 
Title: ABSTRACT  
Abstract: While significant advances have been made in recent years to improve speech recognition performance, and more recently, gesture and handwriting recognition as well, speech and pen-based systems have still not found broad acceptance in everyday life. One reason for this is the inexibility of each input modality when used alone. Human communication is very natural and exible because we can take advantage of a multiplicity of communication signals working in concert to supply complementary information or increase robustness with redundancy. In this paper we present a multimodal interface capable of jointly interpreting speech, pen-based gestures, and handwriting in the context of an appointment scheduling application. The interpretation engine based on semantic frame merging correctly interprets 80% of a multimodal data set assuming perfect speech and gesture/handwriting recognition; in the presence of recognition errors the interpretation performance is in the range of 35-62%. A dialog processing scheme uses task domain knowledge to guide the user in supplying information and permits human-computer interactions to span several related multimodal input events. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Hauptmann, </author> <title> A.G., Speech and Gestures for Graphic Image Manipulation, </title> <booktitle> Proc. </booktitle> <address> CHI89 (Austin, Texas). </address>
Reference-contexts: These intuitive assertions were confirmed by a user study <ref> [1] </ref> conducted at Carnegie Mellon University, in which people interacting with a computer much preferred a combination of both speech and gestures over speech or gestures alone.
Reference: [2] <author> Vo, M.T. and Waibel, A., </author> <title> A Multimodal Human-Computer Interface: Combination of Speech and Gesture Recognition, </title> <booktitle> Adjunct Proc. </booktitle> <address> InterCHI93 (Amsterdam, The Netherlands). </address>
Reference-contexts: On a small data set of 128 utterances collected in user study experiments, the word recognition rate was 82%; if we take into account insertion, substitution, and deletion errors, the word accuracy is 76%. Gesture. In earlier implementations we employed a gesture recognition module <ref> [2] </ref> based on a TDNN classifier [10]. The present implementation increases exibility by decomposing gestures into sequences of strokes recognized as basic shapes such as lines, arcs, arrows, circles, crosses... <p> The underlying algorithm is domain-independent although the encoding of information in frames must necessarily depend on the task domain. This frame merging technique represents a much extended and improved implementation of the approach sketched in <ref> [2] </ref>. It leads to uniform handling of high-level information from all input sources, which is very important for modularity and extensibility. To add another input modality we need only provide a module to convert low-level recognizer output to a partially filled frame to be merged with others.
Reference: [3] <author> Vo, M.T. and Waibel, A., </author> <title> Multimodal Human-Computer Interaction, </title> <booktitle> Proc. </booktitle> <address> ISSD93 (Waseda, Japan). </address>
Reference: [4] <author> Vo, M.T., Houghton, R., Yang, J., Bub, U., Meier, U., Waibel, A., and Duchnowski, P., </author> <title> Multimodal Learning Interfaces, </title> <booktitle> Proc. ARPA SLT Workshop 95 (Austin, </booktitle> <address> Texas). </address>
Reference-contexts: An earlier version of this system was described in <ref> [4] </ref>. The interpretation engine in that system was based on an information-theoretic connectionist network [7] capable of incremental learning during use.
Reference: [5] <author> Waibel, A., Vo, M.T., Duchnowski, P., and Manke, S., </author> <title> Multimodal Interfaces, </title> <booktitle> Artificial Intelligence Review, Special Volume on Integration of Natural Language and Vision Processing, </booktitle> <editor> McKevitt, P. (Ed.), </editor> <volume> Vol. 10, </volume> <pages> Nos. 3-4, </pages> <year> 1995. </year>
Reference: [6] <author> Salber, D. and Coutaz, J., </author> <title> Applying the Wizard of Oz Technique to the Study of Multimodal Systems, </title> <booktitle> Proc. </booktitle> <address> EWHCI93 (Moscow, Russia). </address>
Reference-contexts: A multimodal interpretation engine jointly interprets information from all input sources by merging semantic frames. A domain-independent dialog processor maintains context information across input events. We evaluated the system with data collected in a user study conducted using the Wizard-of-Oz paradigm <ref> [6] </ref>. 2. JEANIE: A MULTIMODAL CALENDAR We have developed a prototype of a multimodal interface for an appointment scheduling program. A person using our Jeanie multimodal calendar can employ any combination of spoken input, gesturing with a pen on a touch-sensitive screen, or handwritten words to interact with the system. <p> USER STUDY AND DATA COLLECTION In order to create a useful and robust multimodal system, we need to find out how people would use a system with such capabilities. We have begun a series of experiments following the Wizard-of-Oz paradigm <ref> [6] </ref>, in which the test subjects were presented with FIGURE 2: Example of interpretation using frames Spoken utterance: IM NOT MEETING WITH SAM Parser =&gt; [i_not_meet] (... [attendee] ([person] SAM)) Sam Speech Operation Delete (0.5) RemoveAttendee (0.5) TargetAttendee Sam (0.5) ParamAttendee Sam (0.5) Operation Delete (1.0) TargetItem h239 (1.0) Combined Operation
Reference: [7] <author> Gorin, A.L., Levinson, S., Gertner, A., and Goldman, E., </author> <title> Adaptive Acquisition of Language, </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> Vol. 5, No. 2, </volume> <pages> pp. 101-132, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: An earlier version of this system was described in [4]. The interpretation engine in that system was based on an information-theoretic connectionist network <ref> [7] </ref> capable of incremental learning during use. Although that approach worked well for the preliminary system designed to explore multimodal interpretation, we found that the connectionist network was difficult to scale up when the task domain representation was significantly expanded in the new version.
Reference: [8] <author> Woszczyna, M., Aoki-Waibel, N., Bu, F.D., Coccaro, N., Horiguchi, K., Kemp, T., Lavie, A., McNair, A., Polzin, T., Rogina, I., Rose, C.P., Schultz, T., Suhm, B., Tomita, M., and Waibel, A., </author> <title> JANUS 93: Towards Spontaneous Speech Translation, </title> <booktitle> Proc. </booktitle> <address> ICASSP94 (Adelaide, Australia). </address>
Reference: [9] <author> Suhm, B., Geutner, P., Kemp, T., Lavie, A., Mayfield, L., McNair, A., Rogina, I., Schultz, T., Sloboda, T., Ward, W., Woszczyna, M., and Waibel, A., </author> <title> JANUS: Towards Multilingual Spoken Language Translation, </title> <booktitle> Proc. ARPA SLT Workshop 95 (Austin, </booktitle> <address> Texas). </address>
Reference: [10] <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K., </author> <title> Phoneme Recognition Using Time-Delay Neural Networks, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 37, No. 3, </volume> <year> 1989, </year> <pages> pp. 328-339. </pages>
Reference-contexts: Gesture. In earlier implementations we employed a gesture recognition module [2] based on a TDNN classifier <ref> [10] </ref>. The present implementation increases exibility by decomposing gestures into sequences of strokes recognized as basic shapes such as lines, arcs, arrows, circles, crosses... Each gesture component is augmented by gesture contexts indicating spatial relationships between the gesture and nearby objects in the calendar interface.
Reference: [11] <author> Haffner, P. and Waibel, A., </author> <title> Multi-State Time Delay Neural Networks for Continuous Speech Recognition, </title> <booktitle> Advances in Neural Network Information Processing Systems 4, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992, </year> <pages> pp. 135-142. </pages>
Reference-contexts: A version based on the handwriting recognition algorithm described below is being developed and should greatly improve recognition performance. Handwriting. Our handwriting recognizer developed by Stefan Manke at University of Karlsruhe based on the MS-TDNN <ref> [11] </ref> is capable of processing writer-independent, continuous (cursive) handwriting [12] at a recognition rate of over 90% on a 20,000-word vocabulary. Handwriting recognition is performed only when the gesture recognizer cannot identify the input strokes as basic shapes.
Reference: [12] <author> Manke, S., Finke, M., and Waibel, A., </author> <title> The Use of Dynamic Writing Information in a Connectionist On-Line Cursive Handwriting Recognition System, </title> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: A version based on the handwriting recognition algorithm described below is being developed and should greatly improve recognition performance. Handwriting. Our handwriting recognizer developed by Stefan Manke at University of Karlsruhe based on the MS-TDNN [11] is capable of processing writer-independent, continuous (cursive) handwriting <ref> [12] </ref> at a recognition rate of over 90% on a 20,000-word vocabulary. Handwriting recognition is performed only when the gesture recognizer cannot identify the input strokes as basic shapes.
Reference: [13] <author> Ward, W., </author> <title> Understanding Spontaneous Speech: </title> <booktitle> the Phoenix System, Proc. </booktitle> <address> ICASSP91 (Toronto, Canada). </address>
Reference-contexts: In addition, context information can be retained across input events by merging with previous interpretation frames as implemented in our dialog processor (Section 3.3). 3.1. Parsing Inputs From Individual Modalities The text string output from the speech recognizer is processed by the Phoenix semantic parser developed by Ward <ref> [13] </ref>. The parser compiles a grammar specifying semantically meaningful fragments of text into an efficient recurrent transition network that identifies these fragments in the recognized utterance. It is capable of skipping unknown words and unmatched fragments and can therefore deal gracefully with ungrammatical sentences.
References-found: 13

