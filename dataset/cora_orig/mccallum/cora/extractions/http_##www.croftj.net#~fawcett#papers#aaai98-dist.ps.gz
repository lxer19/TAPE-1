URL: http://www.croftj.net/~fawcett/papers/aaai98-dist.ps.gz
Refering-URL: 
Root-URL: 
Email: foster@basit.com  fawcett@basit.com  
Title: Robust classification systems for imprecise environments  
Author: Foster Provost Tom Fawcett 
Address: 400 Westchester Avenue White Plains, New York 10604  400 Westchester Avenue White Plains, New York 10604  
Affiliation: Bell Atlantic Science and Technology  Bell Atlantic Science and Technology  
Abstract: In real-world environments it is usually difficult to specify target operating conditions precisely. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. In some cases, the performance of the hybrid can actually surpass that of the best known classifier. The hybrid is also efficient to build, to store, and to update. Finally, we provide empirical evidence that a robust hybrid classifier is needed for many real-world problems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M., and Pazzani, M. J. </author> <year> 1996. </year> <title> Error reduction through learning multiple descriptions. </title> <booktitle> Machine Learning 24(3) </booktitle> <pages> 173-202. </pages>
Reference-contexts: It is efficient to build in terms of time and space, and can be updated incrementally. Therefore, we conclude that it is an elegant, robust classification system. The motivation for this work is fundamentally different from recent machine learning work on combining multiple models <ref> (Ali & Pazzani 1996) </ref>. That work combines models in order to boost performance for a fixed cost and class distribution. The rocch-hybrid combines models for robustness across different cost and class distributions. These methods should be independent|multiple-model classifiers are candidates for extending the rocch.
Reference: <author> Barber, C.; Dobkin, D.; and Huhdanpaa, H. </author> <year> 1993. </year> <title> The quickhull algorithm for convex hull. </title> <type> Technical Report GCG53, </type> <institution> University of Min-nesota. </institution> <note> Available from ftp://geom.umn.edu/pub/ software/qhull.tar.Z. </note>
Reference-contexts: As described in detail by Provost and Fawcett (1997) the rocch method takes as input a set of classifiers, along with their classification performance statistics, and plots them in ROC space. Then it finds the convex hull <ref> (Barber, Dobkin, & Huhdanpaa 1993) </ref> of the set of points in ROC space (the rocch). The convex hull of a set of points is the smallest convex set that contains the points. The rocch is the "northwest boundary" of the points in ROC space. <p> Once again, we presume that this is work that would be done anyway (more on this in Limitations). Finally, the time efficiency of the rocch-hybrid depends on the efficiency of building the rocch, which can be done in O (N log N ) time using the Quick-Hull algorithm <ref> (Barber, Dobkin, & Huhdanpaa 1993) </ref> where N is the number of classifiers. The rocch is space efficient, too, because it comprises only classifiers that might be optimal under some target conditions.
Reference: <author> Berry, M. J. A., and Linoff, G. </author> <year> 1997. </year> <title> Data Mining Techniques: For Marketing, Sales, and Customer Support. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The rest follows from Theorem 7. 2 Similar arguments hold for many other comparison metrics. It can be shown that for maximizing lift <ref> (Berry & Linoff 1997) </ref>, precision or recall, subject to absolute or percentage cutoffs on case presentation, the rocch-hybrid will provide the best set of cases. As with minimizing expected cost, imprecision in the environment forces us to favor a robust solution for these other comparison frameworks.
Reference: <author> Bradley, A. P. </author> <year> 1997. </year> <title> The use of the area under the ROC curve in the evaluation of machine learning algorithms. </title> <booktitle> Pattern Recognition 30(7) </booktitle> <pages> 1145-1159. </pages>
Reference-contexts: Corollary 5 The rocch-hybrid minimizes error rate (maximizes accuracy) for any target class distribution. Proof: Error rate minimization is cost minimization with uniform error costs. 2 An alternative metric, used often in ROC analysis to compare models, is the area under the ROC curve (AUC) <ref> (Bradley 1997) </ref>. It is especially useful for situations where either the target cost distribution or class distribution is completely unknown.
Reference: <author> Catlett, J. </author> <year> 1995. </year> <title> Tailoring rulesets to misclassifica-tioin costs. </title> <booktitle> In Proceedings of the 1995 Conference on AI and Statistics, </booktitle> <pages> 88-94. </pages>
Reference: <author> Dietterich, T. G. </author> <year> 1998. </year> <title> Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation. </title> <note> to appear. </note>
Reference: <author> Egan, J. P. </author> <year> 1975. </year> <title> Signal Detection Theory and ROC Analysis. </title> <booktitle> Series in Cognitition and Perception. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: We want to find the classifier with the highest possible true positive rate, T P = p (Yjp), that does not exceed the F P limit. This is the Neyman-Pearson decision criterion <ref> (Egan 1975) </ref>. Three classifiers, under three such F P limits, are shown in Figure 1. A different classifier is best for each F P limit; any system built with a single "best" classifier is brittle if the F P requirement can change.
Reference: <author> Hanley, J. A., and McNeil, B. J. </author> <year> 1982. </year> <title> The meaning and use of the area under a receiver operating characteristic (roc) curve. </title> <booktitle> Radiology 143 </booktitle> <pages> 29-36. </pages>
Reference-contexts: It is especially useful for situations where either the target cost distribution or class distribution is completely unknown. The AUC represents the probability that a randomly chosen positive instance will be rated higher than a negative instance, and thereby is also estimated by the Wilcoxon test of ranks <ref> (Hanley & McNeil 1982) </ref>. A criticism of the use of AUC for model choice is that for specific target conditions, the classifier with the maximum AUC may be suboptimal (as we will see below).
Reference: <author> Provost, F., and Fawcett, T. </author> <year> 1997. </year> <title> Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97), </booktitle> <pages> 43-48. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: In the subsequent sections we prove that the ROC Convex Hull, which is a method for comparing and visualizing classifier behavior in imprecise environments <ref> (Provost & Fawcett 1997) </ref>, is also an elegant solution to the problem of building a robust classification system. The solution is elegant because the resulting hybrid classifier is robust for a wide variety of problem formulations, and it is efficient to build, to store, and to update.
Reference: <author> Provost, F.; Fawcett, T.; and Kohavi, R. </author> <year> 1998. </year> <title> Building the case against accuracy estimation for comparing induction algorithms. </title> <note> Submitted to IMLC-98. AAAI Press. Available from http://www.croftj. net/~fawcett/papers/ICML98-submitted.ps.gz. </note>
Reference-contexts: In many cases, such a problem can be transformed into an equivalent problem with uniform intra-type error costs by duplicating 1 More recently, an independent study showed a dominating classifier for only one of ten standard machine learning benchmarks <ref> (Provost, Fawcett, & Kohavi 1998) </ref>. heart disease data instances in proportion to their costs for evaluation. We have also assumed for this paper that the estimates of the classifiers' performance statistics (F P and T P ) are very good. <p> It should be noted that, as with simpler statistics, care should be taken to avoid over-fitting the training data and to ensure that differences between ROC curves are meaningful. Cross-validation with averaging of ROC curves is one possible solution <ref> (Provost, Fawcett, & Kohavi 1998) </ref>. Finally, we have addressed predictive performance and computational performance. These are not the only concerns in choosing a classification model.
Reference: <author> Salzberg, S. L. </author> <year> 1997. </year> <title> On comparing classifiers: Pitfalls to avoid and a recommended approach. </title> <booktitle> Data Mining and Knowledge Discovery 1 </booktitle> <pages> 317-328. </pages>
Reference: <author> Swets, J. </author> <year> 1988. </year> <title> Measuring the accuracy of diagnostic systems. </title> <booktitle> Science 240 </booktitle> <pages> 1285-1293. </pages>
Reference-contexts: At this point, it is necessary to review briefly some of the basics of Receiver Operating Characteristic (ROC) analysis, a classic methodology from signal detection theory that is now common in medical diagnosis and has recently begun to be used more generally in AI classifier work <ref> (Swets 1988) </ref>. ROC space denotes the coordinate system used for visualizing classifier performance. In ROC space, T P is represented on the Y axis and F P is represented on the X axis.
Reference: <author> Weinstein, M. C., and Fineberg, H. V. </author> <year> 1980. </year> <title> Clinical Decision Analysis. </title> <address> Philadelphia, PA: </address> <publisher> W. B. Saunders Company. </publisher>
Reference-contexts: Specifically, costs and benefits usually are not known precisely, and class distributions often are known only approximately as well. This fact has been pointed out by many authors (Bradley 1997; Catlett 1995), and is in fact the concern of a large subfield of decision analysis <ref> (Weinstein & Fineberg 1980) </ref>. Imprecision also arises because the environment may change between the time the system is conceived and the time it is used, and even as it is used. For example, levels of fraud and levels of customer responsiveness change continually over time and from place to place. <p> After that we discuss how the rocch can be used in imprecise environments. Finally, we show that for some problems the rocch-hybrid can actually do better than all known classifiers. Minimizing expected cost Decision analysis <ref> (Weinstein & Fineberg 1980) </ref> provides us with a method for determining when one classification model is better than another.
References-found: 13

