URL: ftp://ftp.aic.nrl.navy.mil/pub/schultz/papers/isram96.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/perception-action-learning.html
Root-URL: 
Email: schultz@aic.nrl.navy.mil  
Title: ROBO-SHEPHERD: LEARNING COMPLEX ROBOTIC BEHAVIORS  
Author: Alan C. Schultz, John J. Grefenstette, and William Adams 
Keyword: Genetic Algorithms, Evolutionary Computation, Robot Learning  
Address: DC 20375-5337, U.S.A.  
Affiliation: Naval Research Laboratory, Washington,  
Abstract: This paper reports on recent results using genetic algorithms to learn decision rules for complex robot behaviors. The method involves evaluating hypothetical rule sets on a simulator and applying simulated evolution to evolve more effective rules. The main contributions of this paper are (1) the task learned is a complex behavior involving multiple mobile robots, and (2) the learned rules are verified through experiments on operational mobile robots. The case study involves a shepherding task in which one mobile robot attempts to guide another robot to a specified area. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dorigo, M. </author> <year> (1993). </year> <title> "Genetic and Non-Genetic Operators in Alecsys," </title> <journal> Evolutionary Computation, </journal> <volume> 1(2): </volume> <pages> 151-164. </pages>
Reference-contexts: We start with designed parts, and with a specific decomposition of behaviors, and let the system learn the rules for each behavior. Although similar in that robot behaviors are being learned, Dorigo <ref> [1] </ref> takes an entirely different architectural approach to evolution, using classifier systems to learn behaviors. Here the entire population of the genetic algorithm is taken as the behavior. In our work, each individual in the population is a behavior, and the population consists of competing behaviors.
Reference: [2] <author> Grefenstette, J. J., Ramsey, Connie L., and Schultz, Alan C., </author> <year> (1990). </year> <title> "Learning sequential decision rules using simulation models and competition," </title> <journal> Machine Learning, </journal> <volume> 5(4), </volume> <pages> 355-381 </pages>
Reference-contexts: Conflicts are resolved in favor of rules with higher strength. Rule strengths are updated based on rewards received after each training episode. See <ref> [2] </ref> for further details. LEARNING UNDER SIMULATION The rule set for the shepherd is learned under simulation while the sheep's rule set is fixed.
Reference: [3] <author> Grefenstette, J.J. </author> <year> (1991). </year> <title> "Lamarckian learning in multi-agent environments," </title> <booktitle> Proc. Fourth International Conference of Genetic Algorithms, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 303-310. </pages>
Reference: [4] <author> Cliff, D., I. Harvey, and P. </author> <note> Husbands (1991). Cognitive Science Research Paper No. 318, </note> <institution> School of Cognitive and Computer Science, University of Sussex. </institution>
Reference-contexts: Videos of the operational robots will be shown at the Conference. RELATED WORK Other approaches to evolutionary robotics have been used before. The work of (Harvey, Cliff and Husbands) <ref> [4] </ref> has concentrated on bottom-up learning; for example, the perception system is learned as a neural network via evolutionary algorithms. Our approach differs in that we take an engineering view, and do not insist that all components must be learned.
Reference: [5] <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> Univ. Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference: [6] <author> Ram, Ashwin, R. Arkin, G. Boone, and M. </author> <title> Pearce (1994). "Using Genetic Algorithms to Learn Reactive Control Parameters for Autonomous Robotic Navigation," Adaptive Behavior, </title> <type> 2(3), </type> <year> 1994 </year>
Reference-contexts: Here the entire population of the genetic algorithm is taken as the behavior. In our work, each individual in the population is a behavior, and the population consists of competing behaviors. In the system GA-Robot by Ram Et. Al. <ref> [6] </ref>, a genetic algorithm with a floating point representation is used to optimize parameters that effect the behavior. In Samuel, the entire behavior is learned in a high-level stimulus-response language.
Reference: [7] <author> Schultz, Alan C. and Grefenstette, John J. </author> <year> (1992). </year> <title> "Using a genetic algorithm to learn behaviors for autonomous vehicles," </title> <booktitle> Proceedings of the of the AIAA Guidance, Navigation and Control Conference, </booktitle> <address> Hilton Head, SC, </address> <month> August 10-12, </month> <year> 1992. </year>
Reference: [8] <author> Schultz, Alan C. </author> <year> (1994). </year> <title> "Learning robot behaviors using genetic algorithms," </title> <booktitle> Intelligent Automation and Soft Computing: Trends in Research, Development, and Applications, v1, </booktitle> <editor> Mohammad Jamshidi and Charles Nguyen, editors, </editor> <booktitle> Proceedings of the First World Automation Congress (WAC '94), </booktitle> <pages> 607-612, </pages> <publisher> TSI Press: </publisher> <address> Albuquerque. </address>
References-found: 8

