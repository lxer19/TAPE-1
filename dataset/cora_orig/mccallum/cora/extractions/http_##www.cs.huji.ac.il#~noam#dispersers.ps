URL: http://www.cs.huji.ac.il/~noam/dispersers.ps
Refering-URL: http://www.cs.huji.ac.il/~noam/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Extracting Randomness: How and Why A survey  
Author: Noam Nisan 
Note: This manuscript surveys extractors and dispersers: what they are, how they can be designed, and some of their applications. The work described is due to of a long list of research papers by various authors most notably by David Zuckerman.  
Address: Jerusalem, Israel  
Affiliation: Institute of Computer Science Hebrew University  
Abstract: Extractors are boolean functions that allow, in some precise sense, extraction of randomness from somewhat random distributions. Extractors, and the closely related "Dispersers", exhibit some of the most "random-like" properties of explicitly constructed combinatorial structures. In turn, extractors and dispersers have many applications in "removing randomness" in various settings, and in making randomized constructions explicit. 
Abstract-found: 1
Intro-found: 1
Reference: [AKS87] <author> Ajtai, Komlos, and Szemeredi. </author> <title> Deterministic simulation in LOGSPACE. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1987. </year>
Reference-contexts: On the other hand, due to these strong requirements, extractors and dispersers cannot have constant degree like expanders can. Before getting into known constructions of extractors and dispersers, it is perhaps instructive to see the best construction of dispersers which is implied by expanders. This construction is due to <ref> [AKS87] </ref> (implicitly) and appears explicitly in [IZ89, CW89]. Take any expander on M vertices with degree C = 2 c , and pick any l = (m). The vertices on the right hand side of our disperser will simply be the vertices of the expander. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied [KPS85, CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> Pippenger [Pip87] showed that good explicit highly expanding graphs, yield good algorithms for "sorting in rounds" and for "selecting in rounds". 4.4 Pseudo-random Generators Extractors can also be used to construct pseudo-random generators which fool certain classes of algorithms. The following theorem of [NZ93] improves on previous results of <ref> [AKS87] </ref>. Theorem: [NZ93] There exists a pseudorandom generator which converts O (S) truly random bits into poly (S) bits which look random to any algorithm which runs in space S. The generator runs in O (S) space and poly (S) time.
Reference: [ALM + 92] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and hardness of approximation problems. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 14-23, </pages> <year> 1992. </year>
Reference-contexts: In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see [Zuc93, SZ94]. The proof itself builds on the known hardness results for approximating clique <ref> [AS92b, ALM + 92, FGL + 91] </ref>, and is best understood when viewing it as deterministic amplification for P CP systems. 4.2.4 Time vs. Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]).
Reference: [Alo85] <author> Alon. Expanders, </author> <title> sorting in rounds and superconcentrators of limited depth. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1985. </year>
Reference: [AS92a] <author> N. Alon and J. H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year>
Reference-contexts: The "probabilistic method" is many times used to non-constructively prove the existence of these sought after objects. Again, in many cases it is known how to "De-randomized" these probabilistic proofs, and achieve an explicit construction. We refer the reader to <ref> [AS92a] </ref> for a survey of the probabilistic method. Derandomization Techniques It is possible to roughly categorize the techniques used for De-randomization according to their generality. On one extreme are techniques which relate very strongly to the problem and algorithm at hand. <p> It is probably fair to say that there are only two or three basic types of tools which are commonly used in the construction of small sample spaces for De-randomizations: 1. Pairwise (and k-wise) independence and Hashing. 2. Small Bias Spaces. 3. Expanders. We refer the reader, again, to <ref> [AS92a, MR95] </ref> for further information as well as for references. Dispersers and Extractors This survey explores a fourth general type of tool: a family a graphs called Dispersers and Extractors. These graphs have certain strong "random-like" properties hence they can be used in many cases where "random-like" properties are needed.
Reference: [AS92b] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs; a new characterization of NP. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 2-13, </pages> <year> 1992. </year>
Reference-contexts: In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see [Zuc93, SZ94]. The proof itself builds on the known hardness results for approximating clique <ref> [AS92b, ALM + 92, FGL + 91] </ref>, and is best understood when viewing it as deterministic amplification for P CP systems. 4.2.4 Time vs. Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]).
Reference: [AW95] <author> R. Armoni and A. Wigderson. </author> <type> manuscript. </type> <year> 1995. </year>
Reference-contexts: To get any poly (S) factor gain in the number of bits simply compose the pseudo-random generators. A more careful type of iteration with improved parameters can be found in <ref> [AW95] </ref>. The Pseudo-random Generator Input: x, y 1 :::y t .
Reference: [Blu86] <author> M. Blum. </author> <title> Independent unbiased coin flips from a correlated biased source: a finite markov chain. </title> <journal> Combinator-ica, </journal> <volume> 6(2) </volume> <pages> 97-108, </pages> <year> 1986. </year>
Reference-contexts: A natural idea is to, deterministically, convert this source into truly random bits. For certain types of sources this can indeed be done. E.g. <ref> [Blu86] </ref> shows how it can be done if the source is a (known) Markov chain. For more general sources it can be shown that this cannot be done [SV86]. Instead, we may use the somewhat random source to indirectly simulate a given randomized algorithm.
Reference: [BR94] <author> Bellare and Rompel. </author> <title> Randomness-efficient oblivious sampling. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <year> 1994. </year>
Reference-contexts: These are sampling procedures that give a good estimate for the expected value of a real valued function on some finite domain. Definition 4.2 <ref> [BR94] </ref> An oblivious (ffi; *)- sampler is a deterministic function that for each x 2 [N ] produces a set (x) = fz 1 :::z D g [M ] such that for every function f : [M ] ! [0; 1], (where [0; 1] is the real interval between 0 and <p> For given *, ffi, and m, our wish is to have D be polynomial and n be as small as possible. Oblivious samplers were constructed in <ref> [BR94] </ref> who used them for interactive proof systems. The best known results to date, which use extractors, appears in [Zuc96]. Theorem: There exist explicitly constructible (ffi; *) oblivious samplers where ffi = 2K=N .
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: The roots of the research surveyed here lie mostly in the work on "somewhat random sources" done in the late 1980's, by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. <p> If they are allowed to be correlated then the previous statement may not hold. The following key definition, first formulated in <ref> [CG88] </ref>, turns out to suffice instead of total independence. Definition 3.4 Let X 1 ; X 2 be (possibly correlated) random variables taking values, respectively, in [N 1 ] and [N 2 ]. <p> The requirement that is needed for the output to be close to uniform is: Definition 3.5 <ref> [CG88] </ref> Let B 1 ; :::; B t be correlated random variables taking values, respectively, in [N 1 ]; :::; [N t ]. <p> Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani [Vaz87a, Vaz86, Vaz87b] and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldre-ich <ref> [CG88] </ref> generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. [CW89, CGH + 85, LLS89]).
Reference: [CG89] <author> Chor and Goldreich. </author> <title> On the power of two-point based sampling. </title> <journal> Journal of Complexity, </journal> <volume> 5, </volume> <year> 1989. </year>
Reference-contexts: We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [KPS85, CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification.
Reference: [CGH + 85] <author> B. Chor, O. Goldreich, J. Has-tad, J. Friedman, S. Rudich, and R. Smolensky. </author> <title> The bit extraction problem and t-resilient functions. </title> <booktitle> In Proceedings of the 26th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 396-407, </pages> <year> 1985. </year>
Reference-contexts: Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. <ref> [CW89, CGH + 85, LLS89] </ref>).
Reference: [CW89] <author> A. Cohen and A. Wigderson. Dis--persers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 14-19, </pages> <year> 1989. </year>
Reference-contexts: Before getting into known constructions of extractors and dispersers, it is perhaps instructive to see the best construction of dispersers which is implied by expanders. This construction is due to [AKS87] (implicitly) and appears explicitly in <ref> [IZ89, CW89] </ref>. Take any expander on M vertices with degree C = 2 c , and pick any l = (m). The vertices on the right hand side of our disperser will simply be the vertices of the expander. <p> This limit is reached since the important parameter in the above analysis is the second largest eigenvalue of the expander, which is constrained by the lower bound ( p <ref> [CW89] </ref>). In the rest of this survey we will be interested in constructions which work for smaller values of k. Known Results The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. <ref> [CW89, CGH + 85, LLS89] </ref>). <p> [Zuc90] suggested a general model generalizing 1 Thus, in order for the whole simulation to be polynomial in terms of the original algorithm it is also required that n = poly (m), and that z 1 :::z D can be computed in polynomial time which is the definition used in <ref> [CW89] </ref>. all the previous models. His model was sim-ply all random sources having high min-entropy. Later [Zuc91] he showed that BP P can be simulated given such a source (with enough min-entropy). In a very basic sense, Zuckerman's model is the most general source we can think off. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [KPS85, CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied [KPS85, CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification.
Reference: [FGL + 91] <author> U. Feige, S. Goldwasser, L. Lovasz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 2-12, </pages> <year> 1991. </year>
Reference-contexts: In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see [Zuc93, SZ94]. The proof itself builds on the known hardness results for approximating clique <ref> [AS92b, ALM + 92, FGL + 91] </ref>, and is best understood when viewing it as deterministic amplification for P CP systems. 4.2.4 Time vs. Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]).
Reference: [GG81] <author> Gabber and Galil. </author> <title> Explicit constructions of linear-sized superconcentra-tors. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22, </volume> <year> 1981. </year>
Reference-contexts: The parameters of interest are, first, the size which is the number of edges in H, and second, the depth which is the length of the longest directed path in it. Gabber and Galil <ref> [GG81] </ref> constructed, using expanders, the first explicit linear size super concentrators. The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82].
Reference: [GW94] <author> O. Goldreich and A. Wigderson. </author> <title> Tiny families of functions with random properties: A quality-size trade-off for hashing. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 574-583, </pages> <year> 1994. </year>
Reference-contexts: On the other hand, our purposes do not quite require 0-collision error but only small collision error. This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L = 2 l N = 2 n and * &gt; 0, There exist explicit families H of hash functions from [N ] to [L], with * collision error, and size with jHj = poly (n; <p> This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L = 2 l N = 2 n and * &gt; 0, There exist explicit families H of hash functions from [N ] to [L], with * collision error, and size with jHj = poly (n; *; L).
Reference: [ILL89] <author> R. Impagliazzo, L. Levin, and M. Luby. </author> <title> Pseudo-random generation from one-way functions. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 12-24, </pages> <year> 1989. </year>
Reference-contexts: is a family of hash functions with collision error ffi, if for any x 1 6= x 2 2 [N ], P rob h2H [h (x 1 ) = h (x 2 )] The following lemma is a variant of the of leftover hash lemma of Impagliazzo, Levin, and Luby <ref> [ILL89] </ref>, stated in our terms: Lemma 3.1 [ILL89] Let H be a family of hash functions from [N ] to [L] with collision error ffi. <p> collision error ffi, if for any x 1 6= x 2 2 [N ], P rob h2H [h (x 1 ) = h (x 2 )] The following lemma is a variant of the of leftover hash lemma of Impagliazzo, Levin, and Luby <ref> [ILL89] </ref>, stated in our terms: Lemma 3.1 [ILL89] Let H be a family of hash functions from [N ] to [L] with collision error ffi.
Reference: [IZ89] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to recycle random bits. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 248-253, </pages> <year> 1989. </year>
Reference-contexts: Before getting into known constructions of extractors and dispersers, it is perhaps instructive to see the best construction of dispersers which is implied by expanders. This construction is due to [AKS87] (implicitly) and appears explicitly in <ref> [IZ89, CW89] </ref>. Take any expander on M vertices with degree C = 2 c , and pick any l = (m). The vertices on the right hand side of our disperser will simply be the vertices of the expander. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [KPS85, CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied [KPS85, CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification.
Reference: [KPS85] <author> R. Karp, N. Pippernger, and M. Sipser. </author> <title> A time randomness tradeoff. </title> <booktitle> In AMS Conference on Probabilistic Computational Complexity, </booktitle> <year> 1985. </year>
Reference-contexts: We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [KPS85, CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification.
Reference: [LLS89] <author> Lichtenstein, Linial, and Saks. </author> <title> Some extremal problems arising from discrete control processes. </title> <journal> Combinator-ica, </journal> <volume> 9, </volume> <year> 1989. </year>
Reference-contexts: Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. <ref> [CW89, CGH + 85, LLS89] </ref>).
Reference: [Mes84] <author> R. Meshulam. </author> <title> A geometric construction of a superconcentrator of depth 2. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 215-219, </pages> <year> 1984. </year>
Reference-contexts: Gabber and Galil [GG81] constructed, using expanders, the first explicit linear size super concentrators. The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82]. Meshu-lam <ref> [Mes84] </ref> showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). [WZ93] give a construction based on extractors. <p> Current explicit extractors [TaS96] imply depth 2 super concentrators of size N 2 polyloglog (N) . Proof: Meshulam <ref> [Mes84] </ref> showed that H is a super concentrator of depth 2, iff for any 1 K 0 N and any two sets W I,Z O of size K 0 each, there are at least K 0 common neighbors. We will build a depth 2 graph with this property.
Reference: [MR95] <author> Rajeev Motwani and Prabhakar Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: There are many examples of randomized algorithms for various problems which are better than any known deterministic algorithm for the problem. The randomized algorithms may be faster, more space-efficient, use less communication, allow parallelization, or may be simply simpler than the deterministic counterparts. We refer the reader e.g. to <ref> [MR95] </ref> for a textbook on randomized algorithms. Despite the wide spread use of randomized algorithms, in almost all cases it is not at all clear whether randomization is really necessary. <p> It is probably fair to say that there are only two or three basic types of tools which are commonly used in the construction of small sample spaces for De-randomizations: 1. Pairwise (and k-wise) independence and Hashing. 2. Small Bias Spaces. 3. Expanders. We refer the reader, again, to <ref> [AS92a, MR95] </ref> for further information as well as for references. Dispersers and Extractors This survey explores a fourth general type of tool: a family a graphs called Dispersers and Extractors. These graphs have certain strong "random-like" properties hence they can be used in many cases where "random-like" properties are needed.
Reference: [NZ93] <author> N. Nisan and D. Zuckerman. </author> <title> More deterministic simulation in logspace. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 235-244, </pages> <year> 1993. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman <ref> [NZ93] </ref>. Organization of the Survey In section 2 we provide the basics: go over some preliminaries, define extractors and dispersers, discuss simple lower and upper bounds, and list known results. In section 3 we give an overview of some of the constructions of extractors and dispersers. <p> with H 1 (X) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, TaS96, Zuc96] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> Remark 2.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>, who require that the random choice of the edge originating at x is also almost independent from (X). The difference is minor, though, and we prefer this defini tion. <p> This calculation was done, for certain dispersers, in [Sip88]. Lower Bounds A trivial lower bound, for any n; m and * &lt; 1=2, is d m k 1. We will mostly consider the case where k m so this does not help us. In <ref> [NZ93] </ref> a lower bound of d min (m; (log n+ log * 1 )) was proved for all n; m; k n 1; * &lt; 1=2. Expanders vs. <p> In the rest of this survey we will be interested in constructions which work for smaller values of k. Known Results The first explicit construction of extractors came in <ref> [NZ93] </ref>, and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). In [SZ94] and the final version of [Zuc91], d = O (log n) was obtained for k = (n). <p> What we need is for B to have enough min-entropy, even though it is much shorter than the original string (so that enough min-entropy is left for further blocks.) In <ref> [NZ93] </ref> it is shown that choosing a subset of the bits of X in a pairwise independent way suffices for this. Other ways of sampling a subset of the bits of X behave similarly. <p> Getting a Block Given a string x 1 :::x n and an index of a set S (of size l in a pairwise independent sample space), we output x S . Lemma 3.5 <ref> [NZ93] </ref> For every distribution X and for for almost all choices of S, the distribution of X S is close to some distribution W with H 1 (W ) ~ ( l n H 1 (X)). <p> Actually, as long as l 1 + : : : + l i1 &lt;< m, we can go on and extract another block B i , which has high min entropy even conditioned on the history. Thus we get: Lemma 3.6 <ref> [NZ93] </ref> Let X be a distribution on [N ] with H 1 (X) k. <p> Let us see several ways by which these ingridients can be put together in order to get good extractors. A Construction of Good Extractors We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in subsection 3.3. This gives a (near) block-wise source. <p> Pippenger [Pip87] showed that good explicit highly expanding graphs, yield good algorithms for "sorting in rounds" and for "selecting in rounds". 4.4 Pseudo-random Generators Extractors can also be used to construct pseudo-random generators which fool certain classes of algorithms. The following theorem of <ref> [NZ93] </ref> improves on previous results of [AKS87]. Theorem: [NZ93] There exists a pseudorandom generator which converts O (S) truly random bits into poly (S) bits which look random to any algorithm which runs in space S. The generator runs in O (S) space and poly (S) time. <p> The following theorem of <ref> [NZ93] </ref> improves on previous results of [AKS87]. Theorem: [NZ93] There exists a pseudorandom generator which converts O (S) truly random bits into poly (S) bits which look random to any algorithm which runs in space S. The generator runs in O (S) space and poly (S) time.
Reference: [Pip77] <author> Pippenger. </author> <title> Superconcentrators. </title> <journal> SICOMP: SIAM Journal on Computing, </journal> <year> 1977. </year>
Reference-contexts: In these cases, the "random-like" properties of extractors and dispersers suffice as a replacement for using random graphs, and thus convert a non constructive proof to a construction. 4.3.1 Super concentrators Definition 4.3 <ref> [Pip77] </ref> Let H = (V; E) be a directed graph with a specified subset I V of nodes called input nodes, and a disjoint subset, O V , called output nodes. Assume jIj = jOj = N .
Reference: [Pip82] <author> Pippenger. </author> <title> Superconcentrators of depth 2. </title> <journal> JCSS: Journal of Computer and System Sciences, </journal> <volume> 24, </volume> <year> 1982. </year>
Reference-contexts: Gabber and Galil [GG81] constructed, using expanders, the first explicit linear size super concentrators. The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists <ref> [Pip82] </ref>. Meshu-lam [Mes84] showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). [WZ93] give a construction based on extractors.
Reference: [Pip87] <author> N. Pippenger. </author> <title> Sorting and selecting in rounds. </title> <journal> SIAM Journal on Computing, </journal> <volume> 16 </volume> <pages> 1032-1038, </pages> <year> 1987. </year>
Reference-contexts: Thus, in the expander we have just built, W and Z will share a neighbor. The number of edges is bounded from above D 2DN=M . Pippenger <ref> [Pip87] </ref> showed that good explicit highly expanding graphs, yield good algorithms for "sorting in rounds" and for "selecting in rounds". 4.4 Pseudo-random Generators Extractors can also be used to construct pseudo-random generators which fool certain classes of algorithms. The following theorem of [NZ93] improves on previous results of [AKS87].
Reference: [Sip88] <author> Sipser. Expanders, </author> <title> randomness, or time versus space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36, </volume> <year> 1988. </year>
Reference-contexts: Dispersers were first defined (with somewhat different parameters) by Sipser <ref> [Sip88] </ref>, while extractors were defined by Nisan and Zuckerman [NZ93]. Organization of the Survey In section 2 we provide the basics: go over some preliminaries, define extractors and dispersers, discuss simple lower and upper bounds, and list known results. <p> to think of a distribution X with H 1 (X) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs <ref> [Sip88, SSZ95] </ref>, while extractors as functions [NZ93, SZ94, TaS96, Zuc96]. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> This calculation was done, for certain dispersers, in <ref> [Sip88] </ref>. Lower Bounds A trivial lower bound, for any n; m and * &lt; 1=2, is d m k 1. We will mostly consider the case where k m so this does not help us. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied [KPS85, CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits [AKS87, IZ89, CW89]. Sipser <ref> [Sip88] </ref> defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> For the precise statement see [Zuc93, SZ94]. The proof itself builds on the known hardness results for approximating clique [AS92b, ALM + 92, FGL + 91], and is best understood when viewing it as deterministic amplification for P CP systems. 4.2.4 Time vs. Space Sipser <ref> [Sip88] </ref> defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]). Again, the dispersers are needed for deterministic amplification.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> to think of a distribution X with H 1 (X) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs <ref> [Sip88, SSZ95] </ref>, while extractors as functions [NZ93, SZ94, TaS96, Zuc96]. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> In [SZ94] and the final version of [Zuc91], d = O (log n) was obtained for k = (n). In [SZ94] d = polylog (n) was obtained for k p with "nearly-logarithmic" d for larger values of k. In <ref> [SSZ95] </ref> a disperser with d = O (log n) was obtained for any k = n (1) . In [TaS96], d = polylog (n) (and m = k) was obtained for all values of k. Also in [TaS96] "nearly-logarithmic" d was obtained for all k = n (1) . <p> Also in [TaS96] "nearly-logarithmic" d was obtained for all k = n (1) . In the following table we list the currently known best explicitly constructible extractors and dispersers for various parameters. k d m ref. (n fl ) O (logn); * = 1=2 n ffi <ref> [SSZ95] </ref> Disperser (n fl ) O (log (n=*) log (k) n) n ffi [TaS96] any k poly (log (n=*)) k [TaS96] There are several open problems regarding improvement in parameters: 1. Build an extractor or disperser, for small k, with d = O (log n). 2. <p> Next we shortly mention a different idea which works even for smaller values of k. An Alternative Method This alternative method for effectively getting a block-wise source was introduced in [Zuc91], was first used for the construction of dispersers in <ref> [SSZ95] </ref>, and was extended to allow construction of extractors in [TaS96]. The intuition can again be best obtained by considering the en tropy H (X) instead of the min-entropy. <p> However, it turns out that for dispersers we can simply try all the choices, and combine them together (i.e. take the union of edges that each choice implies). As long as t is relatively small, the total number of choices is manageble. Two more tricks can be used. In <ref> [SSZ95] </ref> a "universal" set of choices for i 1 :::i t with smaller (for the intersting cases polynomial) size is presented, thus achieving optimal dispersers (for certain ranges of parameters). <p> D = poly (n)) for RP as long as H 1 (X) n ffi for any ffi &gt; 0 <ref> [SSZ95] </ref>; a polynomial time simulation for BP P as long as H 1 (X) = (n) (or even slightly less) [Zuc93]; and a quasi-polynomial time simulation for BP P for any H 1 (X) [TaS96]. <p> Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of <ref> [SSZ95] </ref>). Again, the dispersers are needed for deterministic amplification.
Reference: [SV86] <author> M. Santha and U. Vazirani. </author> <title> Generating quasi-random sequences from slightly random sources. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 33 </volume> <pages> 75-87, </pages> <year> 1986. </year>
Reference-contexts: The roots of the research surveyed here lie mostly in the work on "somewhat random sources" done in the late 1980's, by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. <p> For certain types of sources this can indeed be done. E.g. [Blu86] shows how it can be done if the source is a (known) Markov chain. For more general sources it can be shown that this cannot be done <ref> [SV86] </ref>. Instead, we may use the somewhat random source to indirectly simulate a given randomized algorithm. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as weak as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani <ref> [SV86] </ref>, Vazirani [Vaz87a, Vaz86, Vaz87b] and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> with H 1 (X) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, TaS96, Zuc96] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> Remark 2.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>, who require that the random choice of the edge originating at x is also almost independent from (X). The difference is minor, though, and we prefer this defini tion. <p> Known Results The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). In <ref> [SZ94] </ref> and the final version of [Zuc91], d = O (log n) was obtained for k = (n). In [SZ94] d = polylog (n) was obtained for k p with "nearly-logarithmic" d for larger values of k. <p> This construction had d = polylog (n) for k n=polylog (n). In <ref> [SZ94] </ref> and the final version of [Zuc91], d = O (log n) was obtained for k = (n). In [SZ94] d = polylog (n) was obtained for k p with "nearly-logarithmic" d for larger values of k. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . <p> On the other hand, our purposes do not quite require 0-collision error but only small collision error. This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L = 2 l N = 2 n and * &gt; 0, There exist explicit families H of hash functions from [N ] to [L], with * collision error, and size with jHj = poly (n; <p> This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L = 2 l N = 2 n and * &gt; 0, There exist explicit families H of hash functions from [N ] to [L], with * collision error, and size with jHj = poly (n; *; L). <p> Let us see several ways by which these ingridients can be put together in order to get good extractors. A Construction of Good Extractors We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in subsection 3.3. This gives a (near) block-wise source. <p> Let us see several ways by which these ingridients can be put together in order to get good extractors. A Construction of Good Extractors We first discuss the basic strategy used in [Zuc90, Zuc91, NZ93, SZ94] and present it essentially in the form found in <ref> [SZ94] </ref>. Choose t = O (log n) blocks each of size O (k= log n) as described in subsection 3.3. This gives a (near) block-wise source. We now compose t extractors each which multiplies the number of random bits by a constant factor as presented in subsection 3.1. <p> Reducing d We now show a simple idea from <ref> [SZ94] </ref>, showing that by composing two of the extractors just designed we can reduce d even further. <p> The error parameter * of this construction depends on the smaller block and is 1=poly (n 0 ), which is larger than 1=poly (n). By carefully controlling the composition of many extractors of differing lengths, <ref> [SZ94, Zuc96] </ref> show that d = O (log n + log * 1 ) suffices for any * as long as k = (n) (and even slightly less). Increasing m As observed in [WZ93], a simple repetition can increase the value of m. <p> In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see <ref> [Zuc93, SZ94] </ref>. The proof itself builds on the known hardness results for approximating clique [AS92b, ALM + 92, FGL + 91], and is best understood when viewing it as deterministic amplification for P CP systems. 4.2.4 Time vs.
Reference: [TaS96] <author> A. TaShma. </author> <title> On extracting randomness from weak random sources. </title> <booktitle> In STOC 1996, </booktitle> <year> 1996. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> with H 1 (X) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, TaS96, Zuc96] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> In [SZ94] d = polylog (n) was obtained for k p with "nearly-logarithmic" d for larger values of k. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . In <ref> [TaS96] </ref>, d = polylog (n) (and m = k) was obtained for all values of k. Also in [TaS96] "nearly-logarithmic" d was obtained for all k = n (1) . <p> In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . In <ref> [TaS96] </ref>, d = polylog (n) (and m = k) was obtained for all values of k. Also in [TaS96] "nearly-logarithmic" d was obtained for all k = n (1) . <p> In the following table we list the currently known best explicitly constructible extractors and dispersers for various parameters. k d m ref. (n fl ) O (logn); * = 1=2 n ffi [SSZ95] Disperser (n fl ) O (log (n=*) log (k) n) n ffi <ref> [TaS96] </ref> any k poly (log (n=*)) k [TaS96] There are several open problems regarding improvement in parameters: 1. Build an extractor or disperser, for small k, with d = O (log n). 2. <p> following table we list the currently known best explicitly constructible extractors and dispersers for various parameters. k d m ref. (n fl ) O (logn); * = 1=2 n ffi [SSZ95] Disperser (n fl ) O (log (n=*) log (k) n) n ffi <ref> [TaS96] </ref> any k poly (log (n=*)) k [TaS96] There are several open problems regarding improvement in parameters: 1. Build an extractor or disperser, for small k, with d = O (log n). 2. <p> An Alternative Method This alternative method for effectively getting a block-wise source was introduced in [Zuc91], was first used for the construction of dispersers in [SSZ95], and was extended to allow construction of extractors in <ref> [TaS96] </ref>. The intuition can again be best obtained by considering the en tropy H (X) instead of the min-entropy. Let us again be given a source X = (X 1 :::X n ) from which we aim to get a block-wise (k 1 :::k t ) source. <p> Two more tricks can be used. In [SSZ95] a "universal" set of choices for i 1 :::i t with smaller (for the intersting cases polynomial) size is presented, thus achieving optimal dispersers (for certain ranges of parameters). In <ref> [TaS96] </ref> a method by which these many choices for i 1 :::i t can be "merged" is presented, thus achieving near-optimal constructions of extractors. 3.4 Constructions of Extractors In subsection 3.3 we saw how to convert any random source with high min-entropy into a block-wise source; in subsection 3.3 we saw <p> (n)) for RP as long as H 1 (X) n ffi for any ffi &gt; 0 [SSZ95]; a polynomial time simulation for BP P as long as H 1 (X) = (n) (or even slightly less) [Zuc93]; and a quasi-polynomial time simulation for BP P for any H 1 (X) <ref> [TaS96] </ref>. Proof: (for RP ) Get x from the distribution, and set z i = G (x; i) for i = 1:::D, where G is a disperser with * &lt; 1=2. Let us calculate the probability that this simulation fails, i.e. that all z i 62 W . <p> Theorem: (following [WZ93]) There is an ex plicit super concentrator of depth 2 and size N O ( k=1 D k 2 k =M k ), where D k ; M k are the parameters for dispersers with K = 2 k . Current explicit extractors <ref> [TaS96] </ref> imply depth 2 super concentrators of size N 2 polyloglog (N) . <p> Taking all 4K=M k subsets together, the intersection size is at least 2K K 0 . Wigderson and Zuckerman showed that such a construction would also imply an explicit linear size super concentrator of small depth. With <ref> [TaS96] </ref>, of depth polyloglog (N ). 4.3.2 Highly expanding graphs We now consider expanders with very strong expansion properties. Definition 4.4 A graph H on N vertices is called a K-expander if any two sets W; Z of vertices, jW j = jZj = K, have a common neighbor. <p> Theorem: (following [WZ93]) There are constructible K-expanding graph with N vertices, and maximum degree O (N D 2 =M ). Using the extractors of <ref> [TaS96] </ref> we get max imum degree of N K exp (polyloglogN ) for any value of K. Let us remark that the construction takes time polynomial in N . It does not allow computing whether an edge exists between two given vertices in time polynomial in n.
Reference: [Vaz86] <author> U. Vazirani. </author> <title> Randomness, Adversaries and Computation. </title> <type> PhD thesis, </type> <institution> University of California, Berke-ley, </institution> <year> 1986. </year>
Reference-contexts: The roots of the research surveyed here lie mostly in the work on "somewhat random sources" done in the late 1980's, by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as weak as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani <ref> [Vaz87a, Vaz86, Vaz87b] </ref> and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [Vaz87a] <author> U. Vazirani. </author> <title> Efficiency considerations in using semi-random sources. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 160-168, </pages> <year> 1987. </year>
Reference-contexts: The roots of the research surveyed here lie mostly in the work on "somewhat random sources" done in the late 1980's, by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as weak as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani <ref> [Vaz87a, Vaz86, Vaz87b] </ref> and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [Vaz87b] <author> U. Vazirani. </author> <title> Strong communication complexity or generating quasi-random sequences from two communicating semi-random sources. </title> <journal> Com-binatorica, </journal> <volume> 7(4) </volume> <pages> 375-392, </pages> <year> 1987. </year>
Reference-contexts: The roots of the research surveyed here lie mostly in the work on "somewhat random sources" done in the late 1980's, by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as weak as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani <ref> [Vaz87a, Vaz86, Vaz87b] </ref> and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [VV85] <author> U. Vazirani and V. Vazirani. </author> <title> Ran--dom polynomial time is equal to slightly-random polynomial time. </title> <booktitle> In Proceedings of the 26th Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 417-428, </pages> <year> 1985. </year>
Reference-contexts: The roots of the research surveyed here lie mostly in the work on "somewhat random sources" done in the late 1980's, by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. <p> Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani [Vaz87a, Vaz86, Vaz87b] and Vazirani and Vazirani <ref> [VV85] </ref> studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [WZ93] <author> A. Wigderson and D. Zuckerman. </author> <title> Expanders that beat the eigenvalue bound: Explicit construction and applications. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 245-251, </pages> <year> 1993. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> By carefully controlling the composition of many extractors of differing lengths, [SZ94, Zuc96] show that d = O (log n + log * 1 ) suffices for any * as long as k = (n) (and even slightly less). Increasing m As observed in <ref> [WZ93] </ref>, a simple repetition can increase the value of m. <p> The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82]. Meshu-lam [Mes84] showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). <ref> [WZ93] </ref> give a construction based on extractors. Theorem: (following [WZ93]) There is an ex plicit super concentrator of depth 2 and size N O ( k=1 D k 2 k =M k ), where D k ; M k are the parameters for dispersers with K = 2 k . <p> For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82]. Meshu-lam [Mes84] showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). <ref> [WZ93] </ref> give a construction based on extractors. Theorem: (following [WZ93]) There is an ex plicit super concentrator of depth 2 and size N O ( k=1 D k 2 k =M k ), where D k ; M k are the parameters for dispersers with K = 2 k . <p> We would like to explicitly build K-expanding graphs with degree as close as possible to N=K. The eigenvalue methods for constructing expanders give such expanders with degree O (N 2 =K 2 ) which is quadratic in the above lower bound and trivial for K N . <ref> [WZ93] </ref> show how dispersers can be used to construct expanders, for all values of K, with near-optimal degree. Theorem: (following [WZ93]) There are constructible K-expanding graph with N vertices, and maximum degree O (N D 2 =M ). <p> eigenvalue methods for constructing expanders give such expanders with degree O (N 2 =K 2 ) which is quadratic in the above lower bound and trivial for K N . <ref> [WZ93] </ref> show how dispersers can be used to construct expanders, for all values of K, with near-optimal degree. Theorem: (following [WZ93]) There are constructible K-expanding graph with N vertices, and maximum degree O (N D 2 =M ). Using the extractors of [TaS96] we get max imum degree of N K exp (polyloglogN ) for any value of K.
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's <ref> [Zuc90, Zuc91] </ref>, and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> In the rest of this survey we will be interested in constructions which work for smaller values of k. Known Results The first explicit construction of extractors came in [NZ93], and relied on techniques developed in <ref> [Zuc90, Zuc91] </ref>. This construction had d = polylog (n) for k n=polylog (n). In [SZ94] and the final version of [Zuc91], d = O (log n) was obtained for k = (n). <p> Let us see several ways by which these ingridients can be put together in order to get good extractors. A Construction of Good Extractors We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in subsection 3.3. This gives a (near) block-wise source. <p> Chor and Goldre-ich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. [CW89, CGH + 85, LLS89]). Finally, Zuckerman <ref> [Zuc90] </ref> suggested a general model generalizing 1 Thus, in order for the whole simulation to be polynomial in terms of the original algorithm it is also required that n = poly (m), and that z 1 :::z D can be computed in polynomial time which is the definition used in [CW89].
Reference: [Zuc91] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's <ref> [Zuc90, Zuc91] </ref>, and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96]. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> In the rest of this survey we will be interested in constructions which work for smaller values of k. Known Results The first explicit construction of extractors came in [NZ93], and relied on techniques developed in <ref> [Zuc90, Zuc91] </ref>. This construction had d = polylog (n) for k n=polylog (n). In [SZ94] and the final version of [Zuc91], d = O (log n) was obtained for k = (n). <p> Known Results The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). In [SZ94] and the final version of <ref> [Zuc91] </ref>, d = O (log n) was obtained for k = (n). In [SZ94] d = polylog (n) was obtained for k p with "nearly-logarithmic" d for larger values of k. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . <p> Notice that this lemma gives non-trivial parameters as long as k ( p n). Next we shortly mention a different idea which works even for smaller values of k. An Alternative Method This alternative method for effectively getting a block-wise source was introduced in <ref> [Zuc91] </ref>, was first used for the construction of dispersers in [SSZ95], and was extended to allow construction of extractors in [TaS96]. The intuition can again be best obtained by considering the en tropy H (X) instead of the min-entropy. <p> Let us see several ways by which these ingridients can be put together in order to get good extractors. A Construction of Good Extractors We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in subsection 3.3. This gives a (near) block-wise source. <p> His model was sim-ply all random sources having high min-entropy. Later <ref> [Zuc91] </ref> he showed that BP P can be simulated given such a source (with enough min-entropy). In a very basic sense, Zuckerman's model is the most general source we can think off.
Reference: [Zuc93] <author> D. Zuckerman. </author> <title> NP-complete problems have a version that's hard to approximate. </title> <booktitle> In Proceedings of the 8th Structures in Complexity Theory, IEEE, </booktitle> <pages> pages 305-312, </pages> <year> 1993. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> D = poly (n)) for RP as long as H 1 (X) n ffi for any ffi &gt; 0 [SSZ95]; a polynomial time simulation for BP P as long as H 1 (X) = (n) (or even slightly less) <ref> [Zuc93] </ref>; and a quasi-polynomial time simulation for BP P for any H 1 (X) [TaS96]. Proof: (for RP ) Get x from the distribution, and set z i = G (x; i) for i = 1:::D, where G is a disperser with * &lt; 1=2. <p> Using current constructions we can use only n = (1 + ff)(m + t) bits to get error 2 t , for any fixed ff &gt; 0 <ref> [Zuc93, Zuc96] </ref>. Proof: Use an extractor G with * &lt; 1=4. The Algorithm: Choose randomly x 2 [N ]. For any z 2 (x) run the machine accepting L with z as the random string, and decide according to the majority of the results. <p> This cannot be true since an *-close to uniform distribution on the z's gives an *-close estimate of e, in contradiction to each element in S &lt; (resp, S &gt; ) erring on e by at least *. 4.2.3 Approximating Clique Zuckerman <ref> [Zuc93] </ref> showed how deterministic amplification obtained by extractors implies that Theorem: [Zuc93] Approximating log (Clique (G)) to within an constant factor is ~ N P -hard. <p> true since an *-close to uniform distribution on the z's gives an *-close estimate of e, in contradiction to each element in S &lt; (resp, S &gt; ) erring on e by at least *. 4.2.3 Approximating Clique Zuckerman <ref> [Zuc93] </ref> showed how deterministic amplification obtained by extractors implies that Theorem: [Zuc93] Approximating log (Clique (G)) to within an constant factor is ~ N P -hard. In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see [Zuc93, SZ94]. <p> In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see <ref> [Zuc93, SZ94] </ref>. The proof itself builds on the known hardness results for approximating clique [AS92b, ALM + 92, FGL + 91], and is best understood when viewing it as deterministic amplification for P CP systems. 4.2.4 Time vs.
Reference: [Zuc96] <author> D. Zuckerman. </author> <title> Randomness-optimal sampling, extractors, and constructive leader election. </title> <booktitle> In STOC, </booktitle> <year> 1996. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, TaS96, Zuc93, Zuc96] </ref>. Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman [NZ93]. <p> with H 1 (X) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, TaS96, Zuc96] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> The error parameter * of this construction depends on the smaller block and is 1=poly (n 0 ), which is larger than 1=poly (n). By carefully controlling the composition of many extractors of differing lengths, <ref> [SZ94, Zuc96] </ref> show that d = O (log n + log * 1 ) suffices for any * as long as k = (n) (and even slightly less). Increasing m As observed in [WZ93], a simple repetition can increase the value of m. <p> Repeating the previous construction O (1) times allows extracting m = k (1 ) random bits using only d = O (log n) truly random ones for any &gt; 0 and k = (n) <ref> [Zuc96] </ref>. Decreasing k This is done using the "alternative method" sketched in section 3.3. 4 Applications In this section we list some of the applications of extractors and dispersers. We use extractors and dispersers with different parameters. <p> Using current constructions we can use only n = (1 + ff)(m + t) bits to get error 2 t , for any fixed ff &gt; 0 <ref> [Zuc93, Zuc96] </ref>. Proof: Use an extractor G with * &lt; 1=4. The Algorithm: Choose randomly x 2 [N ]. For any z 2 (x) run the machine accepting L with z as the random string, and decide according to the majority of the results. <p> For given *, ffi, and m, our wish is to have D be polynomial and n be as small as possible. Oblivious samplers were constructed in [BR94] who used them for interactive proof systems. The best known results to date, which use extractors, appears in <ref> [Zuc96] </ref>. Theorem: There exist explicitly constructible (ffi; *) oblivious samplers where ffi = 2K=N . Using the best constructions of extractors [Zuc96], we get oblivious samplers with n = (1+ ff)(m + log ffi 1 ) and D = poly (m; * 1 ; log ffi 1 ) (for any ff <p> Oblivious samplers were constructed in [BR94] who used them for interactive proof systems. The best known results to date, which use extractors, appears in <ref> [Zuc96] </ref>. Theorem: There exist explicitly constructible (ffi; *) oblivious samplers where ffi = 2K=N . Using the best constructions of extractors [Zuc96], we get oblivious samplers with n = (1+ ff)(m + log ffi 1 ) and D = poly (m; * 1 ; log ffi 1 ) (for any ff &gt; 0). Proof: Take an extractor G and use the (x) from the extractor.
References-found: 39

