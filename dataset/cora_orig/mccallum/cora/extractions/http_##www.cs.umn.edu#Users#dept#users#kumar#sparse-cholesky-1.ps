URL: http://www.cs.umn.edu/Users/dept/users/kumar/sparse-cholesky-1.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: agupta@cs.umn.edu kumar@cs.umn.edu  
Title: A Scalable Parallel Algorithm for Sparse Matrix Factorization  
Author: Anshul Gupta and Vipin Kumar 
Date: TR 94-19, April 1994)  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Note: A short version won an Outstanding Student Paper Award in Supercomputing '94.  
Abstract: In this paper, we describe a scalable parallel algorithm for sparse matrix factorization, analyze its performance and scalability, and present experimental results of its implementation on a 1024-processor nCUBE2 parallel computer. Through our analysis and experimental results, we demonstrate that our algorithm improves the state of the art in parallel direct solution of sparse linear systems by an order of magnitude|both in terms of speedups and the number of processors that can be utilized effectively for a given problem size. An implementation of our algorithm on nCUBE2 delivers a speedup of up to 364 on 1024 processors for a relatively small problem. A variation of this algorithm [21] implemented on a 1024-processor Cray T3D delivers up to 20 GFLOPS on medium-size structural engineering and linear programming problems. This algorithm incurs less communication overhead and is more scalable than any known parallel formulation of sparse matrix factorization. We show that, for a wide class of sparse matrices (including those arising from two- and three-dimensional finite element problems), our algorithm is optimally scalable on hypercube and mesh architectures and its asymptotic scalability is the same as that of dense matrix factorization. The key features of our algorithm are that it is based on the multifrontal technique, it uses subtree-to-subcube mapping scheme, and it partitions the matrix in two dimensions along both rows and columns. Although, in this paper, we discuss Cholesky factorization of symmetric positive definite matrices, the algorithm can be adapted for solving sparse linear least squares problems and for Gaussian elimination of diagonally dominant matrices that are almost symmetric in structure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cleve Ashcraft. </author> <title> The domain/segment partition for the factorization of sparse symmetric positive definite matrices. </title> <type> Technical Report ECA-TR-148, </type> <institution> Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1990. </year>
Reference-contexts: In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [46, 45, 1, 37, 50, 18] </ref>, and the total communication volume in the best of these schemes [46, 45] is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [2] <author> Cleve Ashcraft. </author> <title> The fan-both family of column-based distributed cholesky factorization algorithms. </title> <editor> In A. George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Graph Theory and Sparse Matrix Computations. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Since the overall computation is only O (N 1:5 ) [15], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [47, 46]. In <ref> [2] </ref>, Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed [46, 45, 1, 37, 50, 18], and the total communication volume in the best of these schemes [46,
Reference: [3] <author> Cleve Ashcraft, S. C. Eisenstat, and J. W.-H. Liu. </author> <title> A fan-in algorithm for distributed sparse numerical factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 593-599, </pages> <year> 1990. </year>
Reference-contexts: Our single processor run times are four times less than the single processor run times on iPSC/2 reported in <ref> [3] </ref>.
Reference: [4] <author> Cleve Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year> <booktitle> Also appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference-contexts: However, the broadcast takes log p steps of O (m) time each; hence, the total communication overhead is O (mp log p) (on a hypercube). In the context of matrix factorization, the experimental study by Ashcraft <ref> [4] </ref> serves to demonstrate the importance of studying the total communication overhead rather than volume. In [4], the fan-in algorithm, which has a lower communication volume than the distributed multifrontal algorithm, has a higher overhead (and hence, a lower efficiency) than the multifrontal algorithm for the same distribution of the matrix <p> In the context of matrix factorization, the experimental study by Ashcraft <ref> [4] </ref> serves to demonstrate the importance of studying the total communication overhead rather than volume. In [4], the fan-in algorithm, which has a lower communication volume than the distributed multifrontal algorithm, has a higher overhead (and hence, a lower efficiency) than the multifrontal algorithm for the same distribution of the matrix among the processors.
Reference: [5] <author> Stephen T. Barnard and Horst D. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <type> Technical Report RNR-92-033, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: A drawback of using a serial implementation of SND is that its run time is too high. However, variations of SND such as multilevel SND <ref> [5, 42] </ref> run much faster without compromising on the quality of ordering. From the experimental results in Tables 1 and 2, we can infer that our algorithm can deliver substantial speedups, even on moderate problem sizes.
Reference: [6] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Communications of the ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This choice of the ordering scheme was prompted by two factors. First, there is increasing evidence that spectral orderings offer a good balance between generality of application and the quality of ordering|both in terms of load balance and fill reduction <ref> [6] </ref>. Second, the SND algorithm itself can be parallelized efficiently, whereas most other ordering schemes do not appear to be as well-suited for parallelization.
Reference: [7] <author> Iain S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (release I). </title> <type> Technical Report TR/PA/92/86, </type> <institution> Research and Technology Division, Boeing Computer Services, </institution> <address> Seattle, WA, </address> <year> 1992. </year>
Reference-contexts: We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection <ref> [7] </ref>. A variation of this algorithm [21] implemented on a 1024-processor Cray T3D delivers up to 20 GFLOPS on medium-size structural engineering and linear programming problems. <p> However, despite these inefficiencies, our implementation is more scalable than a hypothetical ideal implementation (with perfect load balance) of the previously best known parallel algorithm for sparse Cholesky factorization. In Table 2 we summarize the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices <ref> [7] </ref>. The purpose of these experiments was to to demonstrate that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection (SND) [40, 41, 42] was used to order the matrices in Table 2.
Reference: [8] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: This can make the difference between the feasibility and non-feasibility of parallel sparse factorization on highly parallel (p 256) computers. The algorithm presented in this paper is based on the multifrontal principle <ref> [8, 31] </ref> and the computation is guided by an elimination tree. Independent subtrees of the elimination tree are initially assigned to individual processors. <p> The nonzeros in the original matrix are denoted by the symbol "fi" and fill-ins are denoted by the symbol "ffi". 2 The Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening [48] and Duff and Reid <ref> [8] </ref>, and later elucidated in a tutorial by Liu [31]. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 2.
Reference: [9] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: In this paper, we focus on Cholesky factorization of symmetric positive definite matrices; however, the ideas presented here can be adapted for performing Gaussian elimination on diagonally dominant matrices that are almost symmetric in structure <ref> [9] </ref> and for solving sparse linear least squares problems [34]. It is difficult to derive analytical expressions for the number of arithmetic operations in factorization and for the size (in terms of number of nonzero entries) of the factor for general sparse matrices. <p> onto an (mn=2 l )-processor submesh of the mn-processor two-dimensional mesh. 11 3.3 Applications to Gaussian elimination and QR factorization Although we are focusing on Cholesky factorization in this paper, the serial algorithm of Figure 2 can be generalized to Gaussian elimination without pivoting for nearly structurally symmetric sparse matrices <ref> [9] </ref> and for solving sparse linear least squares problems [34].
Reference: [10] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <month> March </month> <year> 1990. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [10, 39, 12, 26] </ref>. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [23, 47].
Reference: [11] <author> G. A. Geist and E. G.-Y. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference: [12] <author> G. A. Geist and C. H. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 639-649, </pages> <year> 1988. </year> <note> Also available as Technical Report ORNL/TM-10383, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1987. </year>
Reference-contexts: This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [10, 39, 12, 26] </ref>. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [23, 47].
Reference: [13] <author> A. George. </author> <title> Nested dissection of a regular finite-element mesh. </title> <journal> SIAM Journal on Numerical Ananlysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: This method of analyzing the communication complexity of sparse Cholesky factorization has been used in [17] in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection <ref> [13] </ref> of grid graphs. We consider a cross-shaped separator (described in [17]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square subgrids of size ( N 1)=2 fi ( N 1)=2. <p> The purpose of these experiments was to compare their results with the scalability analysis in Section 5. The dimensions of the grids were chosen such that the elimination trees were as balanced as possible. The standard nested dissection ordering <ref> [13] </ref> was used for these matrices. Nested dissection has been shown to have optimal fill-in in the case of regular grids [15]. The results of our implementation for some of these grids are summarized in Table 1.
Reference: [14] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: A simple fan-out algorithm <ref> [14] </ref> with column-wise partitioning of an N fi N matrix of this type on p processors results in an O (N p log N ) total communication volume [17] (box A).
Reference: [15] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: A number of column-based parallel factorization algorithms [32, 33, 4, 43, 44, 49, 14, 11, 24, 23, 47, 36] have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) <ref> [15] </ref>, the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [47, 46]. <p> This is because the properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [30, 29, 15] </ref>. We derive these expressions for both hypercube and mesh architectures, and also extend the results to sparse matrices resulting from three-dimensional graphs whose n-node subgraphs have O (n 2=3 )-node separators. <p> If the problem size needs to grow as fast as f E (p) to maintain an efficiency E, then f E (p) is defined as the isoefficiency function of the parallel algorithm-architecture combination for efficiency E. 5.2 Scalability of the parallel multifrontal algorithm It is well known <ref> [15] </ref> that the total work involved in factoring the adjacency matrix of an N -node graph with an O ( N )-node separator using nested dissection ordering of nodes is O (N 1:5 ). <p> The factorization of a sparse matrix involves factoring a dense matrix and the amount of computation required to factor this dense matrix is of the same order as the amount of computation required to factor the entire sparse matrix <ref> [15, 17] </ref>. Hence, O (p 1:5 ) is the lower bound on the isoefficiency function of sparse matrix factorization, and our algorithm achieves this lower bound. 6 Experimental Results We implemented the parallel multifrontal algorithm described in this paper on the nCUBE2 parallel computer. <p> The dimensions of the grids were chosen such that the elimination trees were as balanced as possible. The standard nested dissection ordering [13] was used for these matrices. Nested dissection has been shown to have optimal fill-in in the case of regular grids <ref> [15] </ref>. The results of our implementation for some of these grids are summarized in Table 1. Matrix GRIDixj in the table refers to the sparse matrix obtained from an i fi j 9-point finite difference grid.
Reference: [16] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication reduction in parallel sparse Cholesky factorization on a hypercube. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987, </booktitle> <pages> pages 576-586. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping <ref> [16] </ref> (box B). A number of column-based parallel factorization algorithms [32, 33, 4, 43, 44, 49, 14, 11, 24, 23, 47, 36] have a lower bound of O (N p) on the total communication volume. <p> (2) the distribution of rows and columns of the matrix among the processors must follow some sort of cyclic order to ensure proper load balance, and (3) communication must be localized among as few processors as possible at every stage of elimination by following a subtree-to-subcube type work distribution strategy <ref> [16] </ref>. Ours is the only implementation we know of that satisfies all of the above conditions. Through a preliminary implementation on nCUBE2, we have demonstrated the feasibility of using highly parallel computers for numerical factorization of sparse matrices.
Reference: [17] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10(3) </volume> <pages> 287-298, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: A simple fan-out algorithm [14] with column-wise partitioning of an N fi N matrix of this type on p processors results in an O (N p log N ) total communication volume <ref> [17] </ref> (box A). The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [16] (box B). <p> In order to simplify the analysis, we assume a somewhat different form of nested-dissection than the one used in the actual implementation. This method of analyzing the communication complexity of sparse Cholesky factorization has been used in <ref> [17] </ref> in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection [13] of grid graphs. We consider a cross-shaped separator (described in [17]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square <p> This method of analyzing the communication complexity of sparse Cholesky factorization has been used in <ref> [17] </ref> in the context of a column-based subtree-to-subcube scheme. Within very small constant factors, the analysis holds for the standard nested dissection [13] of grid graphs. We consider a cross-shaped separator (described in [17]) consisting of 2 p N 1 nodes that partitions the N -node square grid into four square subgrids of size ( N 1)=2 fi ( N 1)=2. We call this the level-0 separator that partitions the original grid (or the level-0 grid) into four level-1 grids. <p> the size of level-l subgrids is approximately p p N =2 l , (2) the number of nodes in a level-l separator is approximately 2 p and hence, the length of a supernode at level l of the supernodal elimination tree is approximately 2 p It has been proved in <ref> [17] </ref> that the number of nonzeros that an i fi i subgrid can contribute to the nodes of its bordering separators is bounded by ki 2 , where k = 341=12. Hence, a level-l subgrid can contribute at most kN=4 l nonzeros to its bordering nodes. <p> The number of nonzeros that an i fi i fi i subgrid contributes to the nodes of its bordering separators is O (i 4 ) <ref> [17] </ref>. As a result, an update or a frontal matrix at level l of the supernodal elimination tree will contain O (N 4=3 =2 4l ) entries distributed among p=8 l processors. <p> The problem size in the case of an N fi N sparse matrix resulting from a three-dimensional grid is O (N 2 ) <ref> [17] </ref>. We have shown in Section 4.3 that the overall communication overhead in this case is O (N 4=3 p p). To maintain a fixed efficiency, N 2 / N 4=3 p p, or N 2=3 / p, or N 2 = W / p 1=5 . <p> The factorization of a sparse matrix involves factoring a dense matrix and the amount of computation required to factor this dense matrix is of the same order as the amount of computation required to factor the entire sparse matrix <ref> [15, 17] </ref>. Hence, O (p 1:5 ) is the lower bound on the isoefficiency function of sparse matrix factorization, and our algorithm achieves this lower bound. 6 Experimental Results We implemented the parallel multifrontal algorithm described in this paper on the nCUBE2 parallel computer.
Reference: [18] <author> John R. Gilbert and Robert Schreiber. </author> <title> Highly parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year>
Reference-contexts: In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [46, 45, 1, 37, 50, 18] </ref>, and the total communication volume in the best of these schemes [46, 45] is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [19] <author> Gene H. Golub and Charles Van Loan. </author> <title> Matrix Computations: Second Edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1989. </year>
Reference-contexts: The parallel algorithm also works similarly, except that the frontal and update matrices are now full square matrices rather than triangular matrices as in the case of Cholesky factorization. The least square problem (LSP) min x jjAx bjj 2 is commonly solved <ref> [19] </ref> by factoring the m fi n matrix A (m n) into the product QR, where Q is an m fi n orthogonal matrix and R is an n fi n upper triangular matrix. Matstoms [34, 35] has recently developed a multifrontal algorithm for QR factorization for sparse A.
Reference: [20] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: Section 2 describes the serial multifrontal algorithm for sparse matrix factorization. Section 3 describes our parallel algorithm based on multifrontal elimination. In Section 4, we derive expressions for the communication overhead of the parallel algorithm. In Section 5, we use the isoefficiency analysis <ref> [26, 28, 20] </ref> to determine the scalability of our algorithm and compare it with the scalability of other parallel algorithms for sparse matrix factorization. Section 6 contains some preliminary experimental results on an nCUBE2 parallel computer. <p> In this section we use the isoefficiency metric <ref> [26, 28, 20] </ref> to characterize the scalability of our algorithm. <p> We describe a parallel algorithm whose implementation on a 1024-processor nCUBE2 delivers an order of magnitude higher speedups than any previously known implementation. We use the isoefficiency metric <ref> [26, 28, 20] </ref> to characterize the scalability of our algorithm. We show that the isoefficiency function of our algorithm is O (p 1:5 ) on hypercube and mesh architectures for sparse matrices arising out of both two- and three-dimensional problems.
Reference: [21] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems, 1997. Postscript file available via anonymous FTP from the site ftp://ftp.cs.umn.edu/users/kumar. </note>
Reference-contexts: We have been able to achieve speedups of up to 364 on 1024 processors and 230 on 512 processors over a highly efficient sequential implementation for moderately sized problems from the Harwell-Boeing collection [7]. A variation of this algorithm <ref> [21] </ref> implemented on a 1024-processor Cray T3D delivers up to 20 GFLOPS on medium-size structural engineering and linear programming problems. <p> The remaining 38% overhead is due to load imbalance. Although we have observed through our experiments that the upper bound on efficiency due to load imbalance does not fall below 60-70% for hundreds of processors, even this bound can be improved further. In <ref> [21] </ref>, Karypis and Kumar relax the subtree-to-subcube mapping to a subforest-to-subcube mapping, which significantly reduces load imbalances at the cost of a little increase in communication. 7 Concluding Remarks and Future Work In this paper, we analytically and experimentally demonstrate that scalable parallel implementations of direct methods for solving large sparse <p> Currently, an implementation of our parallel multifrontal algorithm is being developed for Cray T3D and IBM SP-2 parallel computers. A preliminary implementation of a variation of this scheme with improved load balancing currently yields up to 20 GFLOPS on medium-size problems on a 1024-processor Cray T3D <ref> [21] </ref>. We are also working on parallel formulations of spectral nested dissection ordering, symbolic factorization, and a triangular system solver.
Reference: [22] <author> Anshul Gupta and Vipin Kumar. </author> <title> Performance properties of large scale parallel systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 19 </volume> <pages> 234-244, </pages> <year> 1993. </year> <note> Also available as Technical Report TR 92-32, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: It is the total communication overhead that actually determines the overall efficiency and speedup, and is defined as the difference between the parallel processor-time product and the serial run time <ref> [22, 26] </ref>. The communication overhead can be asymptotically higher than the communication volume. For example, a one-to-all broadcast algorithm based on a binary tree communication pattern has a total communication volume of m (p 1) for broadcasting m words of data among p processors.
Reference: [23] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [10, 39, 12, 26]. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [23, 47] </ref>. In this paper, we show that the parallel Cholesky factorization fl This work was supported by Army Research Office under contract # DA/DAAH04-93-G-0080 and by the University of Min-nesota Army High Performance Computing Research Center under contract # DAAL03-89-C-0038.
Reference: [24] <author> Laurie Hulbert and Earl Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <month> September </month> <year> 1991. </year>
Reference: [25] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> Parallel formulation of interior point algorithms. </title> <type> Technical Report 94-20, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <month> April </month> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. </note>
Reference-contexts: Ours is the only implementation we know of that satisfies all of the above conditions. Through a preliminary implementation on nCUBE2, we have demonstrated the feasibility of using highly parallel computers for numerical factorization of sparse matrices. In <ref> [25] </ref>, we have applied our algorithm to obtain a parallel formulation of interior point algorithms and have observed significant speedups in solving linear programming problems. Currently, an implementation of our parallel multifrontal algorithm is being developed for Cray T3D and IBM SP-2 parallel computers.
Reference: [26] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [10, 39, 12, 26] </ref>. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [23, 47]. <p> It is the total communication overhead that actually determines the overall efficiency and speedup, and is defined as the difference between the parallel processor-time product and the serial run time <ref> [22, 26] </ref>. The communication overhead can be asymptotically higher than the communication volume. For example, a one-to-all broadcast algorithm based on a binary tree communication pattern has a total communication volume of m (p 1) for broadcasting m words of data among p processors. <p> Section 2 describes the serial multifrontal algorithm for sparse matrix factorization. Section 3 describes our parallel algorithm based on multifrontal elimination. In Section 4, we derive expressions for the communication overhead of the parallel algorithm. In Section 5, we use the isoefficiency analysis <ref> [26, 28, 20] </ref> to determine the scalability of our algorithm and compare it with the scalability of other parallel algorithms for sparse matrix factorization. Section 6 contains some preliminary experimental results on an nCUBE2 parallel computer. <p> The number of such successive elimination steps is equal to the number of nodes in the relaxed supernode being processed. The communication that takes place in this phase is the standard communication in pipelined grid-based dense Cholesky factorization <ref> [39, 26] </ref>. If the average size of the frontal matrices is t fi t during the processing of a relaxed supernode with m nodes on a q-processor subcube, then O (m) messages of size O (t= p q) are passed through the grid in a pipelined fashion. <p> It is shown in [27] that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing <ref> [38, 26] </ref> and cut-through or worm-hole flow control. This is a direct consequence of the fact that a circular shift is conflict free on a hypercube with e-cube routing. <p> Furthermore, on cache-based processors, the use of BLAS-3 for eliminating multiple columns simultaneously yields much higher performance than the use of BLAS-2 for eliminating one column at a time. Figure 9 (b) shows a variation of the cyclic mapping, called block-cyclic mapping <ref> [26] </ref>, that can alleviate these problems at the cost of some added load imbalance. Recall that in the mapping of Figure 7 (e), the least significant dlog p=2e bits of a row or column index of the matrix determine the processor to which that row or column belongs. <p> This matrix is distributed on a p p logical mesh of processors. As shown in Figure 8, there are two communication operations involved with each elimination step of dense Cholesky. The average size of a message is (ff p p p shown <ref> [39, 26] </ref> that in a pipelined implementation on a p p q mesh of processors, the communication time for s elimination steps with an average message size of m is O (ms). <p> In this section we use the isoefficiency metric <ref> [26, 28, 20] </ref> to characterize the scalability of our algorithm. <p> To maintain a fixed efficiency, N 2 / N 4=3 p p, or N 2=3 / p, or N 2 = W / p 1=5 . A lower bound on the isoefficiency function for dense matrix factorization is O (p 1:5 ) <ref> [26, 27] </ref>. The factorization of a sparse matrix involves factoring a dense matrix and the amount of computation required to factor this dense matrix is of the same order as the amount of computation required to factor the entire sparse matrix [15, 17]. <p> We describe a parallel algorithm whose implementation on a 1024-processor nCUBE2 delivers an order of magnitude higher speedups than any previously known implementation. We use the isoefficiency metric <ref> [26, 28, 20] </ref> to characterize the scalability of our algorithm. We show that the isoefficiency function of our algorithm is O (p 1:5 ) on hypercube and mesh architectures for sparse matrices arising out of both two- and three-dimensional problems.
Reference: [27] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Solutions Manual for Introduction to Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year> <month> 22 </month>
Reference-contexts: Figure 8 shows the communication for one step of dense Cholesky factorization of a hypothetical frontal matrix for q = 16. It is shown in <ref> [27] </ref> that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing [38, 26] and cut-through or worm-hole flow control. <p> To maintain a fixed efficiency, N 2 / N 4=3 p p, or N 2=3 / p, or N 2 = W / p 1=5 . A lower bound on the isoefficiency function for dense matrix factorization is O (p 1:5 ) <ref> [26, 27] </ref>. The factorization of a sparse matrix involves factoring a dense matrix and the amount of computation required to factor this dense matrix is of the same order as the amount of computation required to factor the entire sparse matrix [15, 17].
Reference: [28] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <year> 1994. </year> <note> Also available as Technical Report TR 91-18, </note> <institution> Department of Computer Science Department, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: Section 2 describes the serial multifrontal algorithm for sparse matrix factorization. Section 3 describes our parallel algorithm based on multifrontal elimination. In Section 4, we derive expressions for the communication overhead of the parallel algorithm. In Section 5, we use the isoefficiency analysis <ref> [26, 28, 20] </ref> to determine the scalability of our algorithm and compare it with the scalability of other parallel algorithms for sparse matrix factorization. Section 6 contains some preliminary experimental results on an nCUBE2 parallel computer. <p> In this section we use the isoefficiency metric <ref> [26, 28, 20] </ref> to characterize the scalability of our algorithm. <p> We describe a parallel algorithm whose implementation on a 1024-processor nCUBE2 delivers an order of magnitude higher speedups than any previously known implementation. We use the isoefficiency metric <ref> [26, 28, 20] </ref> to characterize the scalability of our algorithm. We show that the isoefficiency function of our algorithm is O (p 1:5 ) on hypercube and mesh architectures for sparse matrices arising out of both two- and three-dimensional problems.
Reference: [29] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: This is because the properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [30, 29, 15] </ref>. We derive these expressions for both hypercube and mesh architectures, and also extend the results to sparse matrices resulting from three-dimensional graphs whose n-node subgraphs have O (n 2=3 )-node separators.
Reference: [30] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year>
Reference-contexts: This is because the properties of separators can be generalized from grids to all such graphs within the same order of magnitude bounds <ref> [30, 29, 15] </ref>. We derive these expressions for both hypercube and mesh architectures, and also extend the results to sparse matrices resulting from three-dimensional graphs whose n-node subgraphs have O (n 2=3 )-node separators.
Reference: [31] <author> J. W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: Theory and practice. </title> <type> Technical Report CS-90-04, </type> <institution> York University, </institution> <address> Ontario, Canada, </address> <year> 1990. </year> <note> Also appears in SIAM Review, </note> <month> 34 </month> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: This can make the difference between the feasibility and non-feasibility of parallel sparse factorization on highly parallel (p 256) computers. The algorithm presented in this paper is based on the multifrontal principle <ref> [8, 31] </ref> and the computation is guided by an elimination tree. Independent subtrees of the elimination tree are initially assigned to individual processors. <p> the symbol "fi" and fill-ins are denoted by the symbol "ffi". 2 The Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening [48] and Duff and Reid [8], and later elucidated in a tutorial by Liu <ref> [31] </ref>. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 2. <p> The functionality of this algorithm is briefly described in Figure 12. If elimination of a column is regarded as a basic subtask in Cholesky factorization, then the elimination tree gives a partial ordering of these subtask for correct factorization <ref> [31] </ref>. Our tree balancing algorithm is based on the fact that a modified elimination tree that does not violate the partial order specified by the original tree still leads to correct factorization.
Reference: [32] <author> Robert F. Lucas. </author> <title> Solving planar systems of equations on distributed-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference: [33] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: However, it is worse than O (p 1:5 ), which is the asymptotic isoefficiency function derived in Section 5. There are two main reasons for this. First, the O (p 1:5 ) isoefficiency function does not take load imbalance into account. It has been shown in <ref> [33] </ref> that even 16 Table 1: Experimental results for factoring sparse symmetric positive definite matrices associated with a 9-point difference operator on rectangular grids.
Reference: [34] <author> Pontus Matstoms. </author> <title> The multifrontal solution of sparse linear least squares problems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Linkoping University, S-581 83 Linkoping, Sweden, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: In this paper, we focus on Cholesky factorization of symmetric positive definite matrices; however, the ideas presented here can be adapted for performing Gaussian elimination on diagonally dominant matrices that are almost symmetric in structure [9] and for solving sparse linear least squares problems <ref> [34] </ref>. It is difficult to derive analytical expressions for the number of arithmetic operations in factorization and for the size (in terms of number of nonzero entries) of the factor for general sparse matrices. <p> two-dimensional mesh. 11 3.3 Applications to Gaussian elimination and QR factorization Although we are focusing on Cholesky factorization in this paper, the serial algorithm of Figure 2 can be generalized to Gaussian elimination without pivoting for nearly structurally symmetric sparse matrices [9] and for solving sparse linear least squares problems <ref> [34] </ref>. Gaussian elimination without pivoting is numerically stable for diagonally dominant matrices; i.e., the matrices in which the sum of the absolute values of all the non-diagonal elements of a row or a column is less than the absolute value of the diagonal element of the same row or column. <p> The least square problem (LSP) min x jjAx bjj 2 is commonly solved [19] by factoring the m fi n matrix A (m n) into the product QR, where Q is an m fi n orthogonal matrix and R is an n fi n upper triangular matrix. Matstoms <ref> [34, 35] </ref> has recently developed a multifrontal algorithm for QR factorization for sparse A. Matstoms' approach avoids storing Q explicitly and is based on the observation that the matrix R is a Cholesky factor of the n fi n symmetric positive definite matrix A T A.
Reference: [35] <author> Pontus Matstoms. </author> <title> Sparse QR factorization in MATLAB. </title> <type> Technical Report LiTH-MAT-R-1992-05, </type> <institution> Department of Mathematics, Linkoping University, S-581 83 Linkoping, Sweden, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: The least square problem (LSP) min x jjAx bjj 2 is commonly solved [19] by factoring the m fi n matrix A (m n) into the product QR, where Q is an m fi n orthogonal matrix and R is an n fi n upper triangular matrix. Matstoms <ref> [34, 35] </ref> has recently developed a multifrontal algorithm for QR factorization for sparse A. Matstoms' approach avoids storing Q explicitly and is based on the observation that the matrix R is a Cholesky factor of the n fi n symmetric positive definite matrix A T A.
Reference: [36] <author> Mo Mu and John R. Rice. </author> <title> A grid-based subtree-subcube assignment strategy for solving partial differential equations on hypercubes. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13(3) </volume> <pages> 826-839, </pages> <month> May </month> <year> 1992. </year>
Reference: [37] <author> Vijay K. Naik and M. Patrick. </author> <title> Data traffic reduction schemes Cholesky factorization on aynchronous multiprocessor systems. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <year> 1989. </year> <note> Also available as Technical Report RC 14500, </note> <institution> IBM T. J. Watson Research Center, Yorktown Heights, NY. </institution>
Reference-contexts: In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [46, 45, 1, 37, 50, 18] </ref>, and the total communication volume in the best of these schemes [46, 45] is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [38] <author> S. F. </author> <title> Nugent. </title> <booktitle> The iPSC/2 direct-connect communications technology. In Proceedings of the Third Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 51-60, </pages> <year> 1988. </year>
Reference-contexts: It is shown in [27] that although this communication does not take place between the nearest neighbors on a subcube, the paths of all communications on any subcube are conflict free with e-cube routing <ref> [38, 26] </ref> and cut-through or worm-hole flow control. This is a direct consequence of the fact that a circular shift is conflict free on a hypercube with e-cube routing.
Reference: [39] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and scheduling in parallel matrix factorization. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 275-299, </pages> <year> 1986. </year>
Reference-contexts: This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [10, 39, 12, 26] </ref>. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [23, 47]. <p> The number of such successive elimination steps is equal to the number of nodes in the relaxed supernode being processed. The communication that takes place in this phase is the standard communication in pipelined grid-based dense Cholesky factorization <ref> [39, 26] </ref>. If the average size of the frontal matrices is t fi t during the processing of a relaxed supernode with m nodes on a q-processor subcube, then O (m) messages of size O (t= p q) are passed through the grid in a pipelined fashion. <p> This matrix is distributed on a p p logical mesh of processors. As shown in Figure 8, there are two communication operations involved with each elimination step of dense Cholesky. The average size of a message is (ff p p p shown <ref> [39, 26] </ref> that in a pipelined implementation on a p p q mesh of processors, the communication time for s elimination steps with an average message size of m is O (ms).
Reference: [40] <author> Alex Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partioning sparce matrices with eigenvectors of graphs. </title> <journal> SIAM Journal of Mathematical Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: In Table 2 we summarize the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [7]. The purpose of these experiments was to to demonstrate that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection (SND) <ref> [40, 41, 42] </ref> was used to order the matrices in Table 2. This choice of the ordering scheme was prompted by two factors.
Reference: [41] <author> Alex Pothen, H. D. Simon, and Lie Wang. </author> <title> Spectral nested dissection. </title> <type> Technical Report 92-01, </type> <institution> Computer Science Department, Pennsylvania State University, University Park, </institution> <address> PA, </address> <year> 1992. </year>
Reference-contexts: In Table 2 we summarize the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [7]. The purpose of these experiments was to to demonstrate that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection (SND) <ref> [40, 41, 42] </ref> was used to order the matrices in Table 2. This choice of the ordering scheme was prompted by two factors.
Reference: [42] <author> Alex Pothen, H. D. Simon, Lie Wang, and Stephen T. Bernard. </author> <title> Towards a fast implementation of spectral nested dissection. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 42-51, </pages> <year> 1992. </year>
Reference-contexts: In Table 2 we summarize the results of factoring some matrices from the Harwell-Boeing collection of sparse matrices [7]. The purpose of these experiments was to to demonstrate that our algorithm can deliver good speedups on hundreds of processors for practical problems. Spectral nested dissection (SND) <ref> [40, 41, 42] </ref> was used to order the matrices in Table 2. This choice of the ordering scheme was prompted by two factors. <p> A drawback of using a serial implementation of SND is that its run time is too high. However, variations of SND such as multilevel SND <ref> [5, 42] </ref> run much faster without compromising on the quality of ordering. From the experimental results in Tables 1 and 2, we can infer that our algorithm can deliver substantial speedups, even on moderate problem sizes.
Reference: [43] <author> Alex Pothen and Chunguang Sun. </author> <title> Distributed multifrontal factorization using clique trees. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 34-40, </pages> <year> 1991. </year>
Reference-contexts: These speedups are computed with respect to a very efficient serial implementation of the multifrontal algorithm. To lend credibility to our speedup figures, we compared the run times of our program on a single processor with the single processor run times given for iPSC/2 in <ref> [43] </ref> and [49]. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [43] and [49]. <p> single processor with the single processor run times given for iPSC/2 in <ref> [43] </ref> and [49]. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [43] and [49]. Our single processor run times are four times less than the single processor run times on iPSC/2 reported in [3].
Reference: [44] <author> Roland Pozo and Sharon L. Smith. </author> <title> Performance evaluation of the parallel multifrontal method in a distributed-memory environment. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 453-456, </pages> <year> 1993. </year>
Reference: [45] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon systems. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [46, 45, 1, 37, 50, 18] </ref>, and the total communication volume in the best of these schemes [46, 45] is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. <p> [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed [46, 45, 1, 37, 50, 18], and the total communication volume in the best of these schemes <ref> [46, 45] </ref> is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. It is noteworthy that, on any parallel architecture, the total communication volume is only a lower bound on the overall communication overhead.
Reference: [46] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <year> 1992. </year>
Reference-contexts: Since the overall computation is only O (N 1:5 ) [15], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [47, 46] </ref>. <p> In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [46, 45, 1, 37, 50, 18] </ref>, and the total communication volume in the best of these schemes [46, 45] is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. <p> [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed [46, 45, 1, 37, 50, 18], and the total communication volume in the best of these schemes <ref> [46, 45] </ref> is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume. It is noteworthy that, on any parallel architecture, the total communication volume is only a lower bound on the overall communication overhead. <p> In other words, the problem size must be increased as O (p 1:5 ) to maintain a constant efficiency as p is increased. In comparison, a lower bound on the isoefficiency function of Rothberg and Gupta's scheme <ref> [46] </ref> with a communication overhead of at least O (N p p log p) is O (p 1:5 (log p) 3 ). The isoefficiency function of any column-based scheme is at least O (p 3 ) because the total communication overhead has a lower bound of O (N p). <p> yield the same efficiency for p = 128, 256, and 512 if the isoefficiency function is O (p 1:5 ) and O (p 1:5 (log p) 3 ), respectively. is a lower bound on the isoefficiency function of the previously best known (in terms of total communication volume) parallel algorithm <ref> [46] </ref> for sparse matrix factorization. However, it is worse than O (p 1:5 ), which is the asymptotic isoefficiency function derived in Section 5. There are two main reasons for this. First, the O (p 1:5 ) isoefficiency function does not take load imbalance into account. <p> This improvement has been realized by reducing the order of the communication overhead. An efficient parallel sparse matrix factorization algorithm requires the following: (1) the matrix must be partitioned among the processors along both rows and columns because pure column-based partitioning schemes have some fundamental limitations <ref> [47, 46] </ref>, (2) the distribution of rows and columns of the matrix among the processors must follow some sort of cyclic order to ensure proper load balance, and (3) communication must be localized among as few processors as possible at every stage of elimination by following a subtree-to-subcube type work distribution
Reference: [47] <author> Robert Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [10, 39, 12, 26]. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [23, 47] </ref>. In this paper, we show that the parallel Cholesky factorization fl This work was supported by Army Research Office under contract # DA/DAAH04-93-G-0080 and by the University of Min-nesota Army High Performance Computing Research Center under contract # DAAL03-89-C-0038. <p> Since the overall computation is only O (N 1:5 ) [15], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [47, 46] </ref>. <p> We also show that O (p 1:5 ) is asymptotically the best possible isoefficiency function for a parallel implementation of any direct method for solving a system of linear equations, either sparse or dense. In <ref> [47] </ref>, Schreiber concludes that it is not yet clear whether sparse direct solvers can be made competitive at all for highly (p 256) and massively (p 4096) parallel computers. We hope that, through this paper, we have given an affirmative answer to at least a part of the query. <p> This improvement has been realized by reducing the order of the communication overhead. An efficient parallel sparse matrix factorization algorithm requires the following: (1) the matrix must be partitioned among the processors along both rows and columns because pure column-based partitioning schemes have some fundamental limitations <ref> [47, 46] </ref>, (2) the distribution of rows and columns of the matrix among the processors must follow some sort of cyclic order to ensure proper load balance, and (3) communication must be localized among as few processors as possible at every stage of elimination by following a subtree-to-subcube type work distribution
Reference: [48] <author> B. Speelpening. </author> <title> The generalized element method. </title> <type> Technical Report UIUCDCS-R-78-946, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <month> November </month> <year> 1978. </year>
Reference-contexts: The nonzeros in the original matrix are denoted by the symbol "fi" and fill-ins are denoted by the symbol "ffi". 2 The Multifrontal Algorithm for Sparse Matrix Factorization The multifrontal algorithm for sparse matrix factorization was proposed independently, and in somewhat different forms, by Speelpening <ref> [48] </ref> and Duff and Reid [8], and later elucidated in a tutorial by Liu [31]. In this section, we briefly describe a condensed version of multifrontal sparse Cholesky factorization. Given a sparse matrix and the associated elimination tree, the multifrontal algorithm can be recursively formulated as shown in Figure 2.
Reference: [49] <author> Chunguang Sun. </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors. </title> <type> Technical Report CTC92TR102, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: These speedups are computed with respect to a very efficient serial implementation of the multifrontal algorithm. To lend credibility to our speedup figures, we compared the run times of our program on a single processor with the single processor run times given for iPSC/2 in [43] and <ref> [49] </ref>. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [43] and [49]. <p> with the single processor run times given for iPSC/2 in [43] and <ref> [49] </ref>. The nCUBE2 processors are about 2 to 3 times faster than iPSC/2 processors and our serial implementation, with respect to which the speedups are computed, is 4 to 5 times faster than the one in [43] and [49]. Our single processor run times are four times less than the single processor run times on iPSC/2 reported in [3].
Reference: [50] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing '91 Proceedings, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference-contexts: In [2], Ashcraft proposes a fan-both family of parallel Cholesky factorization algorithms that have a total communication volume of O (N p A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [46, 45, 1, 37, 50, 18] </ref>, and the total communication volume in the best of these schemes [46, 45] is O (N p Most researchers so far have analyzed parallel sparse matrix factorization in terms of the total communication volume.
Reference: [51] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> SHAPE: A parallelization tool for sparse matrix computations. </title> <type> Technical Report DCS-TR-290, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <month> June </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: We also found that for some matrices (e.g., that from a 127 fi 127 9-point finite difference grid), our implementation on eight nCUBE2 processors (8.9 seconds) is faster than the 16-processor iPSC/860 implementation (9.7 seconds) reported in <ref> [51] </ref>, although iPSC/860 has much higher computation speeds. 6.1 Load balancing for factorization The factorization algorithm as described in this paper requires a binary relaxed supernodal elimination trees that are fairly balanced.
References-found: 51

