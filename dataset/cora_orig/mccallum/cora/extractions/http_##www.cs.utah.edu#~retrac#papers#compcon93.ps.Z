URL: http://www.cs.utah.edu/~retrac/papers/compcon93.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: Network Multicomputing Using Recoverable Distributed Shared Memory  
Author: John B. Carter Alan L. Cox, Sandhya Dwarkadas, Elmootazbellah N. Elnozahy, David B. Johnson Pete Keleher, Steven Rodrigues, Weimin Yu, and Willy Zwaenepoel John Carter and Pete Keleher 
Note: Current address:  Current address:  This work is supported in part by NSF Grants No. CCR-91163343 and CCR-9211004 and Texas ATP Grant No. 0036404013.  were supported by NASA Fellowships. Elmootazbellah N. Elnozahy was supported by an IBM Fellowship.  
Address: Houston, TX 77251-1892  Salt Lake City, UT 84112.  Pittsburgh, PA 15213-3891.  
Affiliation: Department of Computer Science Rice University  Department of Computer Science, University of Utah,  School of Computer Science, Carnegie Mellon University,  
Abstract: A network multicomputer is a multiprocessor in which the processors are connected by general-purpose networking technology, in contrast to current distributed memory multiprocessors where a dedicated special-purpose interconnect is used. The advent of high-speed general-purpose networks provides the impetus for a new look at this multiprocessor model, removing the bottleneck of current slow networks. However, major software issues remain unsolved. A convenient machine abstraction must be developed that hides from the application programmer low-level details such as message passing or machine failures. We use distributed shared memory as a programming abstraction, and rollback recovery through consistent checkpointing to provide fault tolerance. Measurements of the implementations of distributed shared memory and consistent checkpointing show that these abstractions can be implemented efficiently. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 5 we describe some of the ideas we are experimenting with in our second-generation DSM system. 4 Munin: A Prototype DSM System 4.1 Software Release Consistency Over the past few years, researchers in hardware DSM have adopted relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 13, 16, 26] </ref>. For instance, in release consistency (RC) [16], updates to shared memory need to be performed (become visible) only when a subsequent release performs.
Reference: [2] <author> W.C. Athas and C.L. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> IEEE Computer, </journal> <volume> 21(8), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction In most current distributed memory multicomputers <ref> [2] </ref> the processors are connected by a dedicated, special-purpose interconnection network, such as a hypercube network or a mesh. In contrast, we are exploring the possibility of building a network multicomputer using general-purpose networking technology to interconnect the processors [21].
Reference: [3] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The system currently recognizes a number of such annotations: read-only, migratory, write-shared, and conventional. For example, for migratory objects, a single thread performs multiple accesses to the object, including one or more writes, before another thread accesses the object <ref> [3, 30] </ref>. Such an access pattern is typical of shared objects that are accessed only inside a critical section.
Reference: [4] <author> B. Bhargava and S-R. Lian. </author> <title> Independent checkpointing and concurrent rollback recovery for distributed systems | an optimistic approach. </title> <booktitle> In Proceedings of the 7th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3-12, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Without incremental checkpointing, the worst overhead measured for any application increased from 5.8% to 17%. Synchronizing the checkpoints to form a consistent checkpoint increased the running time of the applications by very little, 3% at most, compared to independent checkpointing with no synchronization <ref> [4] </ref>.
Reference: [5] <author> K. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A more structured approach to fault tolerance is provided by mechanisms such as recovery blocks [18], transactions [17], or reliable broadcasting facilities <ref> [5] </ref>. These systems provide the application programmer with a set of basic primitives on which to build fault tolerance. While these approaches certainly have merit for other application areas, for parallel programming they present too much of a burden on the application programmer.
Reference: [6] <author> A.D. Birrell and B.J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: DSM provides a much tighter integration than message passing or remote procedure calls (RPC) <ref> [6] </ref>. These approaches relieve the programmer from having to deal with low-level networking details, but data movement still must be programmed explicitly. In contrast, DSM systems move data automatically in response to the needs of the application.
Reference: [7] <author> S. Borkar, R. Cohn, G. Cox, S. Gleason, T. Gross, H.T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P.S. Tseng, J. Sutton, J. Urbanski, and J. Webb. </author> <title> iWarp: An integrated 15 solution to high-speed parallel computing. </title> <booktitle> In Proceedings Supercomputing '88, </booktitle> <pages> pages 330-339, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: As a result, a much larger class of applications can be supported efficiently on a network multicomputer. It is by no means our position that such loosely coupled multicomputers will render obsolete more tightly coupled designs <ref> [7, 23] </ref>.
Reference: [8] <author> D. Briatico, A. Ciuffoletti, and L. Simoncini. </author> <title> A distributed domino-effect free recovery algorithm. </title> <booktitle> In Proceedings of the 4th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 207-215, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Upon receiving a marker message, a process takes a tentative checkpoint. Furthermore, every application message is tagged with the CCN of its sender <ref> [8, 22] </ref>. A process also takes a tentative checkpoint if it receives an application message whose appended CCN is greater than the local CCN. The resulting checkpoints form a consistent state.
Reference: [9] <author> J.B. Carter. Munin: </author> <title> Efficient Distributed Shared Memory Using Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The performance numbers in Table 1 are taken out of Carter's Ph.D. dissertation, to which we refer the reader for more detailed information <ref> [9] </ref>. The table shows the speedup achieved by each application running on 16 processors in each of three cases: using Munin, using a conventional DSM implementation with a single write-invalidate protocol [15, 24], and using message passing.
Reference: [10] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Equally important, this machine abstraction should allow a simple migration path for programs already developed for conventional shared memory multiprocessors. As a result of these considerations, we have chosen distributed shared memory (DSM) as our programming abstraction. We have built a DSM system, called Munin <ref> [10] </ref>, that provides good performance while requiring only minimal departures from the traditional shared memory model. Sections 2 to 5 describe our approaches to DSM and some results. <p> Therefore, in Munin's software implementation of release consistency <ref> [10] </ref>, updates are not pipelined as in the DASH implementation, but rather buffered until the release, at which time different updates going to the same destination are merged into a single message (see Figure 4). 4.2 Multiple Consistency Protocols In order to further reduce the number of messages exchanged for maintaining <p> A set of library routines linked with the application program, together with some kernel support, forms the core of the Munin system <ref> [10] </ref>. The system was evaluated by comparing the execution time on Munin of a number of shared memory programs to the execution time of the same applications implemented directly in terms of the underlying message passing primitives of the V kernel.
Reference: [11] <author> K.M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: Furthermore, the machine reboot procedure has to be modified to restart the program after a failure, with the program's recovery routine as the entry point. The problem becomes immensely more complex for a distributed application because one needs to save a consistent snapshot <ref> [11] </ref> of the application. 2 For instance, it would be inappropriate to restart a process from a state in which a particular message was received and to restart the sender from a state in which that message had not been sent yet. <p> With consistent checkpointing, the state of each process is saved separately on stable storage as a process checkpoint, and the checkpointing of individual processes is synchronized such that the collection of checkpoints represents a consistent state of the whole system <ref> [11] </ref>. A state is consistent if all messages received have been sent (see Figure 7). <p> Surviving processes may have to rollback to their latest checkpoint on stable storage in order to remain consistent with recovering processes [20]. 7.1 Implementation Many consistent checkpointing protocols have appeared in the literature (e.g., <ref> [11, 20] </ref>). In our protocol [14], each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). One distinguished process acts as a coordinator. <p> In our protocol [14], each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). One distinguished process acts as a coordinator. In the first phase of the protocol, the coordinator starts a new consistent checkpoint by incrementing CCN and sending marker messages <ref> [11] </ref> that contain CCN to all other processes. Upon receiving a marker message, a process takes a tentative checkpoint. Furthermore, every application message is tagged with the CCN of its sender [8, 22].
Reference: [12] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Compared to a conventional write-invalidate approach [24], this protocol avoids a write miss and a message to invalidate the old copy when the new thread first modifies the object. 4.3 Performance Munin was implemented on top of the V kernel <ref> [12] </ref> on an Ethernet network of Sun-3/60 workstations. A set of library routines linked with the application program, together with some kernel support, forms the core of the Munin system [10].
Reference: [13] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 16(6) </volume> <pages> 660-673, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 5 we describe some of the ideas we are experimenting with in our second-generation DSM system. 4 Munin: A Prototype DSM System 4.1 Software Release Consistency Over the past few years, researchers in hardware DSM have adopted relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 13, 16, 26] </ref>. For instance, in release consistency (RC) [16], updates to shared memory need to be performed (become visible) only when a subsequent release performs.
Reference: [14] <author> E.N. Elnozahy, D.B. Johnson, and W. Zwaenepoel. </author> <title> The performance of consistent check-pointing. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Furthermore, as we will show in Section 7, transparent methods cause only very minor performance degradation, calling into question the need for application-specific techniques. 7 Consistent Checkpointing Consistent checkpointing is an attractive approach for transparently adding fault tolerance to distributed applications <ref> [14, 20] </ref>. With consistent checkpointing, the state of each process is saved separately on stable storage as a process checkpoint, and the checkpointing of individual processes is synchronized such that the collection of checkpoints represents a consistent state of the whole system [11]. <p> Surviving processes may have to rollback to their latest checkpoint on stable storage in order to remain consistent with recovering processes [20]. 7.1 Implementation Many consistent checkpointing protocols have appeared in the literature (e.g., [11, 20]). In our protocol <ref> [14] </ref>, each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). One distinguished process acts as a coordinator. In the first phase of the protocol, the coordinator starts a new consistent checkpoint by incrementing CCN and sending marker messages [11] that contain CCN to all other processes. <p> The key to efficiency in checkpointing is to avoid interference between the execution of the process and its checkpoint. We use two techniques, incremental checkpointing <ref> [14, 19] </ref> and nonblocking copy-on-write checkpointing [25], to reduce this interference. Incremental checkpointing only writes to stable storage those pages of the address space that have been modified since the previous checkpoint. Copy-on-write checkpointing allows the application to continue executing while its checkpoint is being written to stable storage. <p> The results demonstrate that consistent checkpointing is an efficient approach for providing fault-tolerance for long-running distributed applications. Table 2, taken from Elnozahy et al. <ref> [14] </ref>, shows the increase in the running times of eight application programs relative to the running times for the same programs without checkpointing. In these experiments checkpoints are taken every 2 minutes. <p> Even with this small checkpoint interval, con 13 sistent checkpointing on average increased the running time of the applications by only 1%. The worst overhead measured was 5.8%. Detailed analysis of the measurements further demonstrates the benefits of nonblocking copy-on-write checkpointing and incremental checkpointing (see Elnozahy et al. <ref> [14] </ref> for more details). Copy-on-write checkpointing avoids a high penalty for checkpointing for processes with large checkpoints, a penalty that reached as high as 85% for one of our applications.
Reference: [15] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The table shows the speedup achieved by each application running on 16 processors in each of three cases: using Munin, using a conventional DSM implementation with a single write-invalidate protocol <ref> [15, 24] </ref>, and using message passing.
Reference: [16] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: 5 we describe some of the ideas we are experimenting with in our second-generation DSM system. 4 Munin: A Prototype DSM System 4.1 Software Release Consistency Over the past few years, researchers in hardware DSM have adopted relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 13, 16, 26] </ref>. For instance, in release consistency (RC) [16], updates to shared memory need to be performed (become visible) only when a subsequent release performs. <p> For instance, in release consistency (RC) <ref> [16] </ref>, updates to shared memory need to be performed (become visible) only when a subsequent release performs. For simplicity, a release in this context can be thought of as a lock release, although more sophisticated synchronization mechanisms can be used as well. The DASH implementation of RC [16] allows updates to <p> release consistency (RC) <ref> [16] </ref>, updates to shared memory need to be performed (become visible) only when a subsequent release performs. For simplicity, a release in this context can be thought of as a lock release, although more sophisticated synchronization mechanisms can be used as well. The DASH implementation of RC [16] allows updates to shared memory to be pipelined to combat memory latency, but delays lock acquisition requests until all updates have been performed (see 5 In software DSM systems, it is equally important to reduce the number of messages exchanged.
Reference: [17] <author> J.N. Gray. </author> <title> Notes on database operating systems. </title> <editor> In R. Bayer, R.M. Graham, and G. Seeg--muller, editors, </editor> <booktitle> Operating Systems: An Advanced Course, volume 60 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: A more structured approach to fault tolerance is provided by mechanisms such as recovery blocks [18], transactions <ref> [17] </ref>, or reliable broadcasting facilities [5]. These systems provide the application programmer with a set of basic primitives on which to build fault tolerance. While these approaches certainly have merit for other application areas, for parallel programming they present too much of a burden on the application programmer.
Reference: [18] <author> J.J. Horning, H.C. Lauer, P.M. Melliar-Smith, and B. Randell. </author> <title> Program structure for error detection and recovery. </title> <editor> In E. Gelenbe and C. Kaiser, editors, </editor> <booktitle> Operating Systems, volume 16 of Lecture Notes in Computer Science, </booktitle> <pages> pages 171-187. </pages> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: A more structured approach to fault tolerance is provided by mechanisms such as recovery blocks <ref> [18] </ref>, transactions [17], or reliable broadcasting facilities [5]. These systems provide the application programmer with a set of basic primitives on which to build fault tolerance. While these approaches certainly have merit for other application areas, for parallel programming they present too much of a burden on the application programmer.
Reference: [19] <author> D.B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: The key to efficiency in checkpointing is to avoid interference between the execution of the process and its checkpoint. We use two techniques, incremental checkpointing <ref> [14, 19] </ref> and nonblocking copy-on-write checkpointing [25], to reduce this interference. Incremental checkpointing only writes to stable storage those pages of the address space that have been modified since the previous checkpoint. Copy-on-write checkpointing allows the application to continue executing while its checkpoint is being written to stable storage.
Reference: [20] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Furthermore, as we will show in Section 7, transparent methods cause only very minor performance degradation, calling into question the need for application-specific techniques. 7 Consistent Checkpointing Consistent checkpointing is an attractive approach for transparently adding fault tolerance to distributed applications <ref> [14, 20] </ref>. With consistent checkpointing, the state of each process is saved separately on stable storage as a process checkpoint, and the checkpointing of individual processes is synchronized such that the collection of checkpoints represents a consistent state of the whole system [11]. <p> We will continue to use the word consistency because it is standard terminology in the fault tolerance literature. 11 from their latest checkpoint on stable storage. Surviving processes may have to rollback to their latest checkpoint on stable storage in order to remain consistent with recovering processes <ref> [20] </ref>. 7.1 Implementation Many consistent checkpointing protocols have appeared in the literature (e.g., [11, 20]). In our protocol [14], each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). One distinguished process acts as a coordinator. <p> Surviving processes may have to rollback to their latest checkpoint on stable storage in order to remain consistent with recovering processes [20]. 7.1 Implementation Many consistent checkpointing protocols have appeared in the literature (e.g., <ref> [11, 20] </ref>). In our protocol [14], each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). One distinguished process acts as a coordinator.
Reference: [21] <author> H.T. Kung, R. Sansom, S. Schlick, P. Steenkiste, M. Arnould, F.J. Bitz, F. Christianson, E.C. Cooper, O. Menzilcioglu, D. Ombres, and B. Zill. </author> <title> Network-based multicomputers: An emerging parallel architecture. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In most current distributed memory multicomputers [2] the processors are connected by a dedicated, special-purpose interconnection network, such as a hypercube network or a mesh. In contrast, we are exploring the possibility of building a network multicomputer using general-purpose networking technology to interconnect the processors <ref> [21] </ref>. Such a network multicomputer may be realized as a processor bank [29], a number of processors dedicated for the purpose of providing computing cycles. Alternatively, it may consist of a dynamically varying set of machines on which idle cycles are used to perform long-running computations [27].
Reference: [22] <author> T.H. Lai and T.H. Yang. </author> <title> On distributed snapshots. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 153-158, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Upon receiving a marker message, a process takes a tentative checkpoint. Furthermore, every application message is tagged with the CCN of its sender <ref> [8, 22] </ref>. A process also takes a tentative checkpoint if it receives an application message whose appended CCN is greater than the local CCN. The resulting checkpoints form a consistent state.
Reference: [23] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: As a result, a much larger class of applications can be supported efficiently on a network multicomputer. It is by no means our position that such loosely coupled multicomputers will render obsolete more tightly coupled designs <ref> [7, 23] </ref>.
Reference: [24] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: DSM allows processes to share memory even though they execute on nodes that do not physically 1 In fact, one of our interests is to incorporate more tightly-coupled designs into a network multicomputer as well. 2 share memory <ref> [24] </ref> (see Figure 1). DSM provides a much tighter integration than message passing or remote procedure calls (RPC) [6]. These approaches relieve the programmer from having to deal with low-level networking details, but data movement still must be programmed explicitly. <p> Early DSM systems have provided consistency by imitating approaches designed for implementing cache coherence in shared memory multiprocessors <ref> [24] </ref>. We believe that in order to adequately address the problems specific to DSM it is necessary to take a fresh look at the DSM consistency problem. <p> The consistency protocol for migratory objects is to migrate the single copy of the object to the new thread, provide it with read and write access (even if the first access is a read), and invalidate the original copy. Compared to a conventional write-invalidate approach <ref> [24] </ref>, this protocol avoids a write miss and a message to invalidate the old copy when the new thread first modifies the object. 4.3 Performance Munin was implemented on top of the V kernel [12] on an Ethernet network of Sun-3/60 workstations. <p> The table shows the speedup achieved by each application running on 16 processors in each of three cases: using Munin, using a conventional DSM implementation with a single write-invalidate protocol <ref> [15, 24] </ref>, and using message passing.
Reference: [25] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Real-time, concurrent checkpoint for parallel programs. </title> <booktitle> In Proceedings of the 1990 Conference on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 79-88, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The key to efficiency in checkpointing is to avoid interference between the execution of the process and its checkpoint. We use two techniques, incremental checkpointing [14, 19] and nonblocking copy-on-write checkpointing <ref> [25] </ref>, to reduce this interference. Incremental checkpointing only writes to stable storage those pages of the address space that have been modified since the previous checkpoint. Copy-on-write checkpointing allows the application to continue executing while its checkpoint is being written to stable storage.
Reference: [26] <author> R.J. Lipton and J.S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, </institution> <month> September </month> <year> 1988. </year> <month> 17 </month>
Reference-contexts: 5 we describe some of the ideas we are experimenting with in our second-generation DSM system. 4 Munin: A Prototype DSM System 4.1 Software Release Consistency Over the past few years, researchers in hardware DSM have adopted relaxed memory consistency models to reduce the latency associated with remote memory accesses <ref> [1, 13, 16, 26] </ref>. For instance, in release consistency (RC) [16], updates to shared memory need to be performed (become visible) only when a subsequent release performs.
Reference: [27] <author> M. Litzkow, M. Livny, and M. </author> <title> Mutka. Condor | a hunter of idle workstations. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Such a network multicomputer may be realized as a processor bank [29], a number of processors dedicated for the purpose of providing computing cycles. Alternatively, it may consist of a dynamically varying set of machines on which idle cycles are used to perform long-running computations <ref> [27] </ref>. In either form, such a network multicomputer should prove to be significantly cheaper than current distributed memory multiprocessors since it can be built out of general-purpose commodity technology. The idea of such a network multicomputer has been around for quite a while, but its potential has remained largely unrealized. <p> Furthermore, if the distributed computation is executed as a collection of guest processes on workstations, the return of the workstation's owner may cause processes to be evicted from that machine <ref> [27] </ref>. If no special precautions are taken, the computation will either "hang" or terminate abnormally, and will need to be restarted from the beginning.
Reference: [28] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Keleher et al. present some simulation results showing a notable reduction of message traffic as a result of using LRC compared to RC for the SPLASH benchmark suite <ref> [28] </ref>. Using a more sophisticated simulator that takes into account software communication latency and network bandwidth, we are currently studying possible speedups.
Reference: [29] <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, J. Jansen, and G. van Rossum. </author> <title> Experiences with the Amoeba distributed operating system. </title> <journal> Communications of the ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In contrast, we are exploring the possibility of building a network multicomputer using general-purpose networking technology to interconnect the processors [21]. Such a network multicomputer may be realized as a processor bank <ref> [29] </ref>, a number of processors dedicated for the purpose of providing computing cycles. Alternatively, it may consist of a dynamically varying set of machines on which idle cycles are used to perform long-running computations [27].
Reference: [30] <author> W.-D. Weber and A. Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the 3rd Symposium on Architectural Support' for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year> <month> 18 </month>
Reference-contexts: The system currently recognizes a number of such annotations: read-only, migratory, write-shared, and conventional. For example, for migratory objects, a single thread performs multiple accesses to the object, including one or more writes, before another thread accesses the object <ref> [3, 30] </ref>. Such an access pattern is typical of shared objects that are accessed only inside a critical section.
References-found: 30

