URL: http://www.cs.jhu.edu/~murthy/thesis/thesis.ps.gz
Refering-URL: http://www.cs.jhu.edu/~murthy/thesis/home.html
Root-URL: http://www.cs.jhu.edu
Title: On Growing Better Decision Trees from Data  
Author: Kolluru Venkata Sreerama Murthy Kolluru Venkata Sreerama Murthy, 
Degree: A dissertation submitted to The  in conformity with the requirement for the degree of Doctor of Philosophy.  All rights reserved.  
Note: Copyright c 1995 by  
Date: 1995  
Affiliation: Johns Hopkins University  Baltimore, Maryland  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E.H.L. Aarts, P.J.M. Van Laarhoven, J.K.Lenstra, and N.L.J.Ulder. </author> <title> A computational study of local search algorithms for job shop scheduling. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(2) </volume> <pages> 118-125, </pages> <month> Spring </month> <year> 1994. </year>
Reference-contexts: We first compute the adjacency information of the leaf nodes. After initializing the class labels at all leaf nodes to k ( number of leaves), we go back and change the label of each leaf to be the smallest number in <ref> [1; k] </ref> that is not yet assigned to any neighbor.
Reference: [2] <author> J. Aczel and J. Daroczy. </author> <title> On measures of information and their characterizations. </title> <publisher> Academic Publishers, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The TSVQ growing procedure suggested by Riskin and Gray [409] can be viewed as an inverse to Chou's pruning procedure. 7 The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. Shannon's entropy [439] possesses all of these properties <ref> [2] </ref>.
Reference: [3] <author> David W. Aha and Richard L. Bankert. </author> <title> A comparitive evaluation of sequential feature selection algorithms. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 1-7. </pages>
Reference-contexts: Some of these studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets. For examples of this work, see <ref> [251, 276, 69, 113, 336, 3] </ref>. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: [4] <editor> AI&Stats-93: </editor> <booktitle> Preliminary Papers of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 3rd-6th, </address> <month> January </month> <year> 1993. </year> <institution> Society for AI and Statistics. </institution>
Reference: [5] <editor> AI&Stats-95: </editor> <booktitle> Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, 4-7th, </address> <month> January </month> <year> 1995. </year> <institution> Society for AI and Statistics. </institution>
Reference: [6] <author> Hussein Almuallim and Thomas G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 279-305, </pages> <year> 1994. </year>
Reference-contexts: In tasks where more features than the "optimal" are available, decision tree quality is known to be affected by the redundant and irrelevant attributes <ref> [6, 424] </ref>. To avoid this problem, either a feature subset selection method (Section 2.5.1) or a method to form a small set of composite features (Section 2.5.1) can be used as a preprocessing step to tree induction.
Reference: [7] <institution> American Association for Artificial Intelligence. </institution> <address> AAAI-92: </address> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> San Jose, CA, 12-16th, July 1992. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: [8] <institution> American Association for Artificial Intelligence. </institution> <address> AAAI-93: </address> <booktitle> Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Washington, DC, 11-15th, July 1993. </address> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: [9] <institution> American Association for Artificial Intelligence. </institution> <address> AAAI-94: </address> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, volume 1, </booktitle> <address> Seattle, WA, </address> <booktitle> 31st July 4th August 1994. </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: [10] <author> Peter Argentiero, Roland Chin, and Paul Beaudet. </author> <title> An automated approach to the design of decision tree classifiers. </title> <journal> IEEE Transactionson Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-4(1):51-57, </volume> <month> January </month> <year> 1982. </year> <month> 250 </month>
Reference-contexts: The problem of learning trees from decision rules instead of examples is addressed in [224]. The problem of learning trees solely from prior probability distributions is considered in <ref> [10] </ref>. Learning decision trees from qualitative causal models acquired from domain experts is the topic of [382]. Several attempts at generalizing the decision tree representation exist.
Reference: [11] <author> Sunil Arya and David M. Mount. </author> <title> Asymptotically efficient randomized algorithm for nearest neighbor searching. </title> <type> Technical Report CS-TR-3011 or UMIACS-TR-92-135, </type> <institution> Computer Science, University of Maryland, College Park, MD, </institution> <month> December </month> <year> 1992. </year>
Reference: [12] <author> Les Atlas, Ronald Cole, Yeshwant Muthuswamy, Alan Lipman, Jerome Connor, Dong Park, Muhammed El-Sharkawi, and Robert J. Marks II. </author> <title> A performance comparison of trained multilayer perceptrons and trained classification trees. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1614-1619, </pages> <year> 1990. </year>
Reference-contexts: Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis. Multilayer perceptrons and CART (with and without linear combinations) [44] are compared in <ref> [12] </ref> to find that there is not much difference in accuracy. Similar conclusions were reached in [142] when ID3 [391] and backpropagation were compared. Talmon et al. [467] compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other.
Reference: [13] <author> Peter Auer, Robert C. Holte, and Wolfgang Maass. </author> <title> Theory and applications of agnostic PAC-learning with small decision trees. </title> <booktitle> In ML-95 [333], </booktitle> <pages> pages 21-29. </pages> <editor> Editor: </editor> <publisher> Jeffrey Schlimmer. </publisher>
Reference-contexts: Hancock [189] gave a polynomial time algorithm for PAC-learning read-k decision trees. Bshouty [54] showed that decision trees are learnable under the model of exact learning with membership queries and unrestricted 55 equivalence queries. Recently, agnostic PAC-learning <ref> [13] </ref> and pruning [205] have been studied by the learnability theory community.
Reference: [14] <author> Haldun Aytug, Siddhartha Bhattacharya, Gary J. Koehler, and Jane L. Snowdon. </author> <title> A review of machine learning in scheduling. </title> <journal> IEEE Transactions on Engineering Management, </journal> <volume> 41(2) </volume> <pages> 165-171, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For a recent review of the use of machine learning (decision trees and other techniques) in scheduling, see <ref> [14] </ref>. * Medicine: Medical research and practice have long been important areas of application for decision tree techniques.
Reference: [15] <author> A. Babic, E. Krusinska, and J.-E. Stromberg. </author> <title> Extraction of diagnostic rules using recursive partitioning systems: A comparison of two approaches. </title> <journal> Artificial Intelligence in Medicine, </journal> <volume> 4(5) </volume> <pages> 373-387, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: He even claimed that random attribute selection criteria are as good as measures like information gain [391]. This later claim was refuted in [292], where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Babic et al. <ref> [15] </ref> compared ID3 [391] and CART [44], for two clinical diagnosis problems. Miyakawa [329] compared three activity-based measures, Q, O and loss, both analytically and empirically.
Reference: [16] <author> J.L. Baer and B. Schwab. </author> <title> A comparison of tree-balancing algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 20(5) </volume> <pages> 322-330, </pages> <year> 1977. </year>
Reference-contexts: path length between the root and a leaf node, averaged over all the leaf nodes. (This measure is also used in the experiments in Chapter 5.) Although little if any work has been done on balancing decision trees, a great deal of research has considered balanced search trees (e.g., see <ref> [16, 79, 461, 93, 357] </ref>). Roughly speaking, this literature deals with techniques to restructure search trees when elements are 145 inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [17] <author> L. Bahl, P.F.Brown, P.V. de Souza, and R. L. Mercer. </author> <title> A tree-based statistical language model for natural language speech recognition. </title> <journal> IEEE Transactions on Accoustics, Speech and Signal Processing, </journal> <volume> 37(7) </volume> <pages> 1001-1008, </pages> <year> 1989. </year>
Reference-contexts: Use of linear regression to find good feature combinations is explored recently in [36]. Discovery of good combinations of Boolean features to be used as tests at tree 42 nodes is explored in the machine learning literature in [372] as well as in signal processing <ref> [17] </ref>. Ragavan and Rendell [403] describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Looka-head for construction of Boolean feature combinations is also considered in [515]. Linear threshold unit trees for Boolean functions are described in [418]. <p> Quinlan discussed methods of extracting probabilities from decision trees in [397]. Buntine [62] described Bayesian methods for building, smoothing and averaging class probability trees. 16 Smoothing in the context of tree structured vector quantizers is described in <ref> [17] </ref>. An approach, which refines the class probability estimates in a greedily induced decision tree using local kernel density estimates has been suggested recently in [453]. Assignment of probabilistic goodness to splits in a decision tree is described in [187].
Reference: [18] <author> Eard Baker and A. K. Jain. </author> <title> On feature ordering in practice and some finite sample effects. </title> <booktitle> In Proceedings of the Third International Joint Conference on Pattern Recognition, </booktitle> <pages> pages 45-49, </pages> <address> San Diego, CA, </address> <year> 1976. </year>
Reference-contexts: Baker and Jain <ref> [18] </ref> reported experiments comparing eleven feature evaluation criteria and concluded that the feature rankings induced by various rules are very similar. Several feature evaluation criteria, including Shannon's entropy and divergence measures, are compared using simulated data in [22], on a sequential, multi-class classification problem.
Reference: [19] <author> F. A. Baker, David L. Verbyla, C. S. Hodges Jr., and E. W. Ross. </author> <title> Classification and regression tree analysis for assessing hazard of pine mortality caused by hetero basidion annosum. Plant Disease, </title> <address> 77(2):136, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality to pine trees <ref> [19] </ref>. * Power systems: Power system security assessment [199] and power stability prediction [414] are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see [464, 247] ).
Reference: [20] <author> D.A. Belsley. </author> <title> Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. </title> <publisher> Wiley & Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: The category variable (median value of owner-occupied homes) is actually continuous, but we discretized it so that category = 1 if value &lt; $21000, and 2 otherwise. For other uses of this data, see <ref> [20, 399] </ref>. Diabetes diagnosis. This data catalogs the presence or absence of diabetes among Pima Indian females, 21 years or older, as a function of eight numeric-valued attributes.
Reference: [21] <author> W. A. Belson. </author> <title> Matching and prediction on the principle of biological classification. </title> <journal> Applied Statistics, </journal> <volume> 8 </volume> <pages> 65-75, </pages> <year> 1959. </year>
Reference-contexts: emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic <ref> [21, 195, 323, 515, 503] </ref> are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures.
Reference: [22] <author> Moshe Ben-Bassat. </author> <title> Myopic policies in sequential classification. </title> <journal> IEEE Transactions on Computing, </journal> <volume> 27(2) </volume> <pages> 170-174, </pages> <month> February </month> <year> 1978. </year>
Reference-contexts: Baker and Jain [18] reported experiments comparing eleven feature evaluation criteria and concluded that the feature rankings induced by various rules are very similar. Several feature evaluation criteria, including Shannon's entropy and divergence measures, are compared using simulated data in <ref> [22] </ref>, on a sequential, multi-class classification problem. The conclusions are that no feature selection rule is consistently superior to the others, and that no specific strategy for alternating different rules seems to be significantly more effective.
Reference: [23] <author> Moshe Ben-Bassat. </author> <title> Use of distance measures, information measures and error bounds on feature evaluation. </title> <booktitle> In Krishnaiah and Kanal [265], </booktitle> <pages> pages 773-791. 251 </pages>
Reference-contexts: Feature evaluation rules are heuristics whose aim is to produce as reliable probability estimates from training data as possible. A taxonomy, proposed by Ben-Bassat <ref> [23] </ref>, is helpful in understanding the large number of existing feature evaluation criteria. <p> All dependence-bassed measures can be interpreted as belonging to one of the above two categories <ref> [23] </ref>. There exist several attribute selection criteria that do not clearly belong to any category in Ben-Basset's taxonomy. Gleser and Collen [172] and Talmon [465] used a combination of mutual information and 2 measures.
Reference: [24] <author> J. A. Benediktsson and P. H. Swain. </author> <title> Consensus theoretic classification methods. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 22(4) </volume> <pages> 688-704, </pages> <year> 1992. </year>
Reference: [25] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [309, 28, 25, 307, 26] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [289]. Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes. <p> However, there seem to be three significant problems with LP-based tree building methods. 80 * LP methods are unlikely to be robust to non-uniformly distributed noise in the data. We are currently experimenting with the LP formulations of <ref> [117, 25] </ref> to verify this hypothesis, and our preliminary results support the hypothesis. * For some multimodal class distributions, impurity-based methods can "shave-off" homogeneous corners of the attribute space, successively reducing the problem size and complexity. <p> Figure 3.8 shows a simple data set for which the LP formulations in <ref> [117, 25] </ref> fail to produce any split. Our experimental section includes results showing how each of these methods compares to OC1. Our algorithm, OC1, uses deterministic hill climbing most of the time, 81 of the data sets to be separated coincide. <p> We call this variation OC1-LP. To implement this method, we replaced OC1's hyperplane-finding routine with a routine that formulates and solves a LP problem. The LP formulation we used is the one suggested in <ref> [25] </ref>. 32 We used LOQO to solve the linear programs. LOQO is a linear and quadratic programming problem solver written by Robert J. <p> Mangasarian and Bennett have compiled data on the problem of diagnosing breast cancer to test several new classification methods <ref> [309, 25, 26] </ref>. This data represents a set of patients with breast cancer, where each patient was characterized by nine numeric attributes plus the diagnosis of the tumor as benign or malignant. The data set currently has 683 entries and is available from the UC Irvine machine learning repository [346]. <p> Salzberg [421] reported 96.0% accuracy using 1-NN on the same (smaller) data set. Herman and Yeung [207] reported 99.0% accuracy using piece-wise linear classification, again using a somewhat smaller data set. Bennett and Mangasarian <ref> [25] </ref> reported 97.4% accuracy using their MSM1 algorithm, using a different experimentation method from the one we employ. Classifying Irises. This is Fisher's famous iris data, which has been extensively studied in the statistics and machine learning literature.
Reference: [26] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Multicategory discrimination via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3 </volume> <pages> 29-39, </pages> <year> 1994. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [309, 28, 25, 307, 26] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [289]. Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes. <p> Mangasarian and Bennett have compiled data on the problem of diagnosing breast cancer to test several new classification methods <ref> [309, 25, 26] </ref>. This data represents a set of patients with breast cancer, where each patient was characterized by nine numeric attributes plus the diagnosis of the tumor as benign or malignant. The data set currently has 683 entries and is available from the UC Irvine machine learning repository [346].
Reference: [27] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Serial and parallel multicategory discrimination. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4), </volume> <year> 1994. </year>
Reference: [28] <author> Kristin P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <booktitle> In Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <year> 1992. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [309, 28, 25, 307, 26] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [289]. Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: [29] <author> Kristin P. Bennett. </author> <title> Machine Learning via Mathematical Programming. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference: [30] <author> Kristin P. Bennett. </author> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <booktitle> In Proceedings of Interface 94: The 26th Symposium on the Interface, </booktitle> <institution> Research Triangle, North Carolina, </institution> <year> 1994. </year>
Reference-contexts: In the first stage, a sufficient partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming [318], fuzzy logic search [494] and multi-linear programming <ref> [30] </ref>. The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree.
Reference: [31] <author> John A. Bentrup and Sylvian R. Ray. </author> <title> An examination of inductive learning algorithms for the classification of sleep signals. </title> <type> Report. </type> <institution> UIUCDCS-R-93-1792, Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <address> 1304 Springfield Avenue, Urbana, Il 61801, </address> <month> February </month> <year> 1993. </year>
Reference: [32] <author> A. Blum and R. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 9-18, </pages> <address> Boston, MA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, one might hope that, by reducing the size of the decision tree, or the dimensionality of the data, it might be possible to make the problem tractable. This does not seem to be the case either . Blum and Rivest <ref> [32] </ref> showed that the problem of constructing an optimal 3-node neural network is NP-complete. <p> The problem of inducing the smallest axis-parallel decision tree is known to be NP-hard (Section 2.6.1). It is also easy to see that the problem of constructing an optimal (e.g., smallest) oblique decision tree is NP-hard. This conclusion follows from the work of Blum and Rivest <ref> [32] </ref>. Their result implies that in d dimensions (i.e., with d attributes) the problem of producing a 3-node oblique decision tree that is consistent with the training set is NP-complete.
Reference: [33] <author> Marko Bohanec and Ivan Bratko. </author> <title> Trading accuracy for simplicity in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 223-250, </pages> <year> 1994. </year>
Reference-contexts: Forsyth et al. [149] recently suggested a pruning method that is based on viewing the decision tree as an encoding for the training data. Use of dynamic programming to prune trees optimally and efficiently has been explored recently in <ref> [33] </ref>. A few studies have been done to study the relative effectiveness of pruning methods [324, 91, 125]. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others.
Reference: [34] <author> David Bowser-Chao and Debra L. Dzialo. </author> <title> Comparison of the use of binary decision trees and neural networks in top quark detection. Physical Review D: Particles and Fields, </title> <address> 47(5):1900, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Object recognition: Tree based classification has been used recently for recognizing three dimensional objects [456, 57] and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles <ref> [34] </ref>. * Plant diseases: CART [44] was recently used to assess the hazard of mortality to pine trees [19]. * Power systems: Power system security assessment [199] and power stability prediction [414] are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has
Reference: [35] <author> D. Boyce, A. Farhi, and R. Weishedel. </author> <title> Optimal Subset Selection. </title> <publisher> Springer-Verlag, </publisher> <year> 1974. </year>
Reference-contexts: Moreover, most real world classifiers are not truly Bayesian. 40 selection method (Section 2.5.1) can be employed to filter out the unnecessary observations. Feature subset selection There is a large body of work on choosing relevant subsets of features (for example, see the texts <ref> [116, 35, 322] </ref>). Most of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: [36] <author> Anna Bramanti-Gregor and Henry W. Davis. </author> <title> The statistical learning of accurate heuristics. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 1079-1085. </pages> <editor> Editor: Ruzena Bajcsy. </editor> <volume> 252 </volume>
Reference-contexts: Friedman's tree induction method [152] could consider with equal ease atomic and composite features. Techniques to search for multivariate splits (Section 2.3.2) can be seen as ways for constructing composite features. Use of linear regression to find good feature combinations is explored recently in <ref> [36] </ref>. Discovery of good combinations of Boolean features to be used as tests at tree 42 nodes is explored in the machine learning literature in [372] as well as in signal processing [17].
Reference: [37] <author> Y. Brandman, A. Orlitsky, and J. Hennessy. </author> <title> A spectral lower bound technique for the size of decision trees and two-level AND/OR circuits. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(2) </volume> <pages> 282-286, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Goodman and Smyth [174] establish the equivalence between decision tree induction and a form of Shannon-Fano prefix coding, and show that this comparison leads to several interesting insights. Brandman et al. <ref> [37] </ref> suggested a universal technique to lower bound the size and other characteristics of decision trees for arbitrary Boolean functions. This technique is based on the power spectrum coefficients of the n-dimensional Fourier transform of the function.
Reference: [38] <institution> Breast cancer data. </institution> <note> Available in the UCI ML Repository. </note> <institution> Obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Data provided by Matjaz Zwitter and Milan Soklic. </institution>
Reference-contexts: Brief descriptions of each of the real data sets we used are given below. A brief rationale for choosing these particular data sets, and our experimental method for real world domains are given in Section 4.2.2. BC Breast cancer recurrence data <ref> [38] </ref>. Contains 286 instances, each described by 9 attributes and one class label. The task is to predict if a breast cancer event is to recur. CL Cleveland Clinic Foundation's heart disease diagnosis data [89]. Contains 303 instances, each described by 14 attributes including the class label.
Reference: [39] <author> Y. Breibart and A. Reiter. </author> <title> A branch-and-bound algorithm to obtain an optimal evaluation tree for monotonic boolean functions. </title> <journal> Acta Informatica, </journal> <volume> 4 </volume> <pages> 311-319, </pages> <year> 1975. </year>
Reference-contexts: This can be done by using dynamic programming (e.g.: [318]) or branch and bound techniques (e.g.: <ref> [39] </ref>). But when the tree uses oblique splits, it is not clear, even for a fixed number of attributes, how to generate an optimal (e.g., smallest) decision tree in polynomial time. Goodrich [176] showed that the problem of inducing the smallest oblique decision tree is NP-hard even in three dimensions.
Reference: [40] <author> Leo Breiman. </author> <title> The method for estimating multivariate functions from noisy data. </title> <journal> Technometrics, </journal> <volume> 33(2) </volume> <pages> 125-143, </pages> <year> 1991. </year>
Reference: [41] <author> Leo Breiman. </author> <title> Stacked regressions. </title> <type> Technical Report TR-367, </type> <institution> Department of Statistics, University of California at Berkeley, </institution> <year> 1992. </year>
Reference: [42] <author> Leo Breiman. </author> <title> Hinging hyperplanes for regression, classification and function approximation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3) </volume> <pages> 999-1013, </pages> <month> May </month> <year> 1993. </year>
Reference: [43] <author> Leo Breiman. </author> <title> Bagging predictors. </title> <type> Technical report, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference: [44] <author> Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: A standard reference for the current work on decision trees from a statistical perspective is Breiman et al.'s excellent monograph on classification and regression trees <ref> [44] </ref>. 18 Pattern recognition work on decision trees was motivated by the need to interpret images from remote sensing satellites such as LANDSAT in the 1970s [464]. An overview of work on decision trees in the patter recognition literature can be found in [106]. <p> Decision trees in particular, and induction methods in general, arose in machine learning to avoid the knowledge acquisition bottleneck [137] for expert systems. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s CART work <ref> [44] </ref> and Quinlan's ID3 algorithm [391]. Quinlan's book on C4.5 [398], although specific to his tree building program, is perhaps the best available overview of tree methodology from a machine learning perspective. <p> dependence measures. * Rules derived from information theory: Examples of this variety are rules based on Shannon's entropy. 7 Tree construction by maximizing global mutual information, i.e., by expanding tree nodes that contribute to the largest gain in average mutual 6 Chou et al.[85] suggested a pruning method, based on <ref> [44] </ref>, for optimally pruning a balanced TSVQ. The TSVQ growing procedure suggested by Riskin and Gray [409] can be viewed as an inverse to Chou's pruning procedure. 7 The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. <p> The feature evaluation criteria in this class measure separability, divergence or discrimination between classes. A popular distance measure is the Gini index of diversity, named after the Italian economist Corrado Gini (1884-1965). Gini index has been used for tree construction in statistics <ref> [44] </ref>, pattern recognition [162] and sequential fault diagnosis [378]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [44, 351] as a remedy. <p> Gini index has been used for tree construction in statistics [44], pattern recognition [162] and sequential fault diagnosis [378]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule <ref> [44, 351] </ref> as a remedy. Taylor and Silverman [470] pointed out that the Gini index emphasizes equal sized offspring and purity of both children. <p> The conclusions are that no feature selection rule is consistently superior to the others, and that no specific strategy for alternating different rules seems to be significantly more effective. Breiman et al. <ref> [44] </ref> conjectured that decision tree design is rather insensitive to any one from a large class of splitting rules, and it is the stopping rule that is crucial. Mingers [325] compared several attribute selection criteria, and concluded that tree quality doesn't seem to depend on the specific criterion used. <p> This later claim was refuted in [292], where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Babic et al. [15] compared ID3 [391] and CART <ref> [44] </ref>, for two clinical diagnosis problems. Miyakawa [329] compared three activity-based measures, Q, O and loss, both analytically and empirically. He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. <p> He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. Fayyad and Irani [133] showed that their measure C-SEP, performs better than Gini index <ref> [44] </ref> and information gain [391] for specific types of problems. 27 Several researchers [195, 391] pointed out that information gain is biased towards attributes with a large number of possible values. Mingers [323] compared information gain and the 2 statistic for growing the tree as well as for stop-splitting. <p> For very large tree classifiers, the critical issue is optimizing structural properties (height, balance etc.) [493, 71]. 11 For a general discussion about the relationship between complexity and predictive accuracy of classifiers, see [380]. 34 Breiman et al. <ref> [44] </ref> pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [363, 253]. Overfitting avoidance as a specific bias is studied in [507, 428]. <p> Kim and Koehler [249] analytically investigate the conditions under which pruning is beneficial for accuracy. Their main result states pruning is more beneficial with increasing skewness in class distribution and/or increasing sample size. Breiman et al.'s pruning method <ref> [44] </ref>, cost complexity pruning (a.k.a. weakest link pruning or error complexity pruning) proceeds in two stages. In the first stage, a sequence of increasingly smaller trees are built on the training data. <p> Chou et al.[85] extended Breiman et al.'s pruning method to tree structured vector quantizers. The requirement for an independent pruning set might be problematic especially when small training samples are involved. Several solutions have been suggested to get around this problem. Breiman et al. <ref> [44] </ref> describe a cross validation procedure that avoids reserving part of training data for pruning, but has a large computational complexity. Quinlan's pessimistic pruning [393, 398] does away with the need for a separate pruning set by using a statistical correlation test. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs <ref> [44, 96, 115, 72, 478] </ref>. <p> The classification of an object with missing attribute values will be the largest represented class in the union of all the leaf nodes at which the object ends up. Breiman et al.'s CART system <ref> [44] </ref> more or less implemented Friedman's suggestions. Quinlan also considered the problem of missing attribute values [395]. 44 2.5.4 Improving on greedy induction Most tree induction systems use a greedy approach | trees are induced top-down, a node at a time. <p> This occurs because the best split at a node vacillates widely while the sample at the node is still small. An incremental version of CART <ref> [44] </ref> that uses significance thresholds to avoid the above problem is described in [99]. 2.5.9 Tree quality measures The fact that several trees can correctly represent the same data raises the question of how to decide that one tree is better than another. <p> They suggested an information based metric to evaluate a classifier, as a remedy to the above problems. Martin [311] argued that information theoretic measures of classifier complexity are not practically computable except within severely restricted families of classifiers, and suggested a generalized version of CART's <ref> [44] </ref> 1-standard error rule as a means of achieving a tradeoff between classifier complexity and accuracy. <p> Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier [402, 149] . 2.5.10 Miscellaneous Most existing tree induction systems proceed in a greedy top-down fashion <ref> [464, 44, 391] </ref>. Bottom up induction of trees is considered in [275]. <p> A consequence of this result is that top-down tree induction (using mutual information) is necessarily suboptimal 19 Thanks to Kevin Van Horn for pointing this out. 54 in terms of average tree depth. Trees of maximal size generated by the CART algorithm <ref> [44] </ref> have been shown to have an error rate bounded by twice the Bayes error rate, and to be asymptotically Bayes optimal [177]. <p> This conjecture is substantiated empirically in [353], where it is shown that the expected depth of trees greedily induced using information gain [391] and Gini index <ref> [44] </ref> is very close to that of the optimal, under a variety of experimental conditions. <p> Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis. Multilayer perceptrons and CART (with and without linear combinations) <ref> [44] </ref> are compared in [12] to find that there is not much difference in accuracy. Similar conclusions were reached in [142] when ID3 [391] and backpropagation were compared. <p> The performance of decision trees improved in [50] when multivariate splits were used, and backpropagation networks did better with feature selection. Giplin et al. [171] compared stepwise linear discriminant analysis, stepwise logistic regression and CART <ref> [44] </ref> to three senior cardiologists, for predicting the problem of predicting whether a patient would die within a year of being discharged after an acute myocardial infarction. Their results showed that there was no difference between the physicians and the computers, in terms of the prediction accuracy. <p> is described in [183]. * Biomedical Engineering: Use of decision trees for identifying features to be used in implantable devices can be found in [169]. * Control Systems: Automatic induction of decision trees was recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART <ref> [44] </ref> for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical <p> has been used recently for recognizing three dimensional objects [456, 57] and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART <ref> [44] </ref> was recently used to assess the hazard of mortality to pine trees [19]. * Power systems: Power system security assessment [199] and power stability prediction [414] are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has been a strong application area <p> For axis-parallel splits, on the other hand, there are only n d distinct possibilities, and axis-parallel methods such as C4.5 [398] and CART <ref> [44] </ref> can exhaustively search for the best split at each node. The problem of searching for the best oblique split is therefore much more difficult than that of searching for the best axis-parallel split. <p> For a more comprehensive list of pointers to existing work on oblique trees, see Section 2.3.2. CART-LC: The first oblique decision tree algorithm to be proposed was CART with linear combinations <ref> [44, chapter 5] </ref>. This algorithm, referred to henceforth as CART-LC, is an important basis for OC1. Figure 3.4 summarizes (using Breiman et al.'s notation) what the CART-LC algorithm does at each node in the decision tree. <p> The two main alternatives considered in the past have been simulated annealing, used in the SADT system [204], and deterministic heuristic search, as in CART-LC <ref> [44] </ref>. <p> If we substitute a point (an example) T j into the equation for H, we get i=1 (a i x ji ) + a d+1 = V j , where the sign of V j tells us whether the point T j is above 25 As pointed out in <ref> [44, Chapter 5] </ref>, it does not make sense to use an oblique split when the number of examples at a node n is less than or almost equal to the number of features d, because the data underfits the concept. <p> For the OC1 system we implemented an existing pruning method, but note that any tree pruning method will work fine within OC1. Based on the experimental evaluations of Mingers [324] and other work, we chose Breiman et al.'s 99 Cost Complexity (CC) pruning <ref> [44] </ref> as the default pruning method for OC1. This method, which is also called Error Complexity or Weakest Link pruning, requires a separate pruning set. The pruning set can be a randomly chosen subset of the training set, or it can be approximated using cross validation. <p> For this reason, oblique DT induction methods can benefit substantially by using a feature selection method (an algorithm that selects a subset of the original attribute set) in conjunction with the coefficient learning algorithm <ref> [44, 49] </ref>. Currently, OC1 does not have a built-in mechanism to select relevant attributes. However, it is easy to include any of several standard methods (e.g., stepwise forward selection or stepwise backward selection) or even an ad hoc method to select features before running the tree-building process. <p> This is T's goodness. 3. Execute steps 3,4 of algorithm Greedy. We experimented with two pre-defined goodness measures, namely, the Gini index of diversity <ref> [44] </ref> and information gain [391]. 37 This gave us four algorithms, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [44] and Greedy-Info to the ID3 algorithm [391]. All our experiments measured tree quality in terms of four measures. <p> Execute steps 3,4 of algorithm Greedy. We experimented with two pre-defined goodness measures, namely, the Gini index of diversity <ref> [44] </ref> and information gain [391]. 37 This gave us four algorithms, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [44] and Greedy-Info to the ID3 algorithm [391]. All our experiments measured tree quality in terms of four measures. Accuracy is the classification accuracy on either an independently generated test set (for the synthetic domains) or obtained by cross validation (for the real world domains). <p> For the experiments with real world data, we used seven domains from the UCI 155 repository of machine learning databases. The tree induction methods we used are very similar to CART <ref> [44] </ref> and ID3 [391]. The benefits of limited lookahead search in all our experiments are marginal in spite of the enormous increase in the computational complexity. Greedy induction consistently produces trees that are as accurate and small as those produced with lookahead. <p> alternative way of asking the same question is, what is the penalty that decision tree algorithms pay in return for the speed gained by the greedy heuristic? We quantify the goodness of greedy tree induction empirically in this 159 chapter, using the popular decision tree algorithms, C4.5 [398] and CART <ref> [44] </ref>. We induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees, which in turn are found using a novel map coloring idea. <p> Section 5.1 describes our experimental method and Section 5.2 presents the results. Section 5.3 provides general conclusions. 160 5.1 Experimental setup Our experimental framework is quite simple | we use C4.5 [398] and CART <ref> [44] </ref> to induce decision trees on a large number of random data sets, and in each case we compare the greedily induced tree to the optimal tree. The implementation of this framework raises some interesting issues. Optimal Decision Tree for a Training Set. <p> Tree Induction Methods Used. The tree induction methods we use are C4.5 [398] and CART <ref> [44] </ref>. One main difference between C4.5 and CART is the goodness criterion, the criterion used to choose the best split at each node. C4.5 uses the information gain 38 criterion, whereas CART uses either the Gini index of diversity or the twoing rule. <p> In the experiments in which the training data is noise-free, no pruning was used with either method. In the experiments using noisy training sets, we augment both methods with cost complexity pruning <ref> [44] </ref>, reserving 10% of the training data for pruning. 38 Quinlan suggested gain ratio as an improvement over information gain. <p> Goodman and Smyth (1988) argued, by establishing the equivalence of decision tree induction and a form of Shannon-Fano prefix coding, that the average depth of trees induced by greedy one-pass (i.e., no pruning) algorithms is nearly optimal. * Cost complexity pruning <ref> [44] </ref> dealt effectively with both attribute and class noise. <p> Now consider typical decision trees 221 induced on these data sets by existing tree induction methods. Figure 7.2 displays the decision trees generated for the CB data by C4.5 [398]. Figure 7.3 shows the trees generated by multivariate CART <ref> [44] </ref> and multivariate OC1 on the RCB data. Figure 7.4 displays the trees induced by C4.5 and multivariate CART on the RGC data. These figures show that some otherwise successful tree induction methods have trouble in these apparently simple domains. <p> The ION data [446] contains classifications of radar returns from the ionosphere. 351 observations, each with 34 continuous attributes, were classified as good or bad, depending on whether they were genuine or erroneous signals. The decision tree induction programs used in our experiments were C4.5 [398], CART <ref> [44] </ref>, and OC1 (Chapter 3). Both the univariate and multivariate versions of CART and OC1 were used, unless we knew the correct bias for a data set in advance. For example, on the CB data only univariate algorithms were considered. <p> Additional experiments (not reported here due to space limitations) indicated that no other impurity measures did significantly better for the data sets used. The univariate CART and C4.5 implementations are part of the IND2.0 package developed by Wray Buntine. We implemented our own version of multivariate CART based on <ref> [44] </ref>, with error complexity pruning using a separate test set. All the methods were run using default parameters. All the classification accuracies and tree sizes (given as number of leaves) reported are averages of ten 5-fold cross validation experiments. <p> We built decision trees on thousands of synthetic data sets using CART <ref> [44] </ref> and C4.5 [398], and compared each one to the respective optimal tree. We found that, for a wide range of data characteristics, the greedy heuristic (along with pruning) produced decision trees whose expected classification cost was very close to the optimal. We changed our focus after Chapter 5.
Reference: [45] <author> Michael R. Brent, Sreerama K. Murthy, and Andrew Lundberg. </author> <title> Discovering morphemic suffixes: A case study in minimum description length induction. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Laudersdale, FL, </address> <month> January </month> <year> 1995. </year>
Reference: [46] <author> Richard P. Brent. </author> <title> Fast training algorithms for multilayer neural nets. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(3) </volume> <pages> 346-354, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks.
Reference: [47] <author> Carla E. Brodley. </author> <title> Recursive Automatic Algorithm Selection for Inductive Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1994. </year>
Reference-contexts: See [193] for example. An alternative to multiple trees is a hybrid classifier that uses several small classifiers as parts of a larger classifier. Brodley <ref> [47] </ref> describes a system that automatically selects the most suitable among a univariate decision tree, a linear discriminant and an instance based classifier at each node of a hierarchical, recursive classifier. 48 tures for some splits.
Reference: [48] <author> Carla E. Brodley and Paul E. Utgoff. </author> <title> Multivariate versus univariate decision trees. </title> <type> Technical Report COINS-CR-92-8, </type> <institution> Dept. of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: LMDT: Another oblique decision tree algorithm, one that uses a very different approach from CART-LC, is the Linear Machine Decision Trees (LMDT) system <ref> [483, 48] </ref>, which is a successor to the Perceptron Tree method [480, 482]. Each internal node in an LMDT tree is a Linear Machine [364]. The training algorithm presents examples repeatedly at each node until the linear machine converges. <p> This allows us to quantify more precisely how the parameters of our algorithm affect its performance. A second purpose of this experiment is to compare OC1's search strategy with that of two existing oblique decision tree induction systems - LMDT <ref> [48] </ref> and SADT [204]. We show that the quality of trees induced by OC1 is as good as, if not better than, that of the trees induced by these existing systems on three artificial domains.
Reference: [49] <author> Carla E. Brodley and Paul E. Utgoff. </author> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 45-77, </pages> <year> 1995. </year>
Reference-contexts: John [229] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [364], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes <ref> [49, 115] </ref>. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [496, 450] suggested a procedure to train a linear split to minimize the error probability. <p> Because these tests are equivalent to hy-perplanes at an oblique orientation to the axes, we call this class of decision trees oblique decision trees. (Trees of this form have also been called "linear" (Section 2.3.2) and "mul-tivariate" <ref> [49] </ref> . <p> For this reason, oblique DT induction methods can benefit substantially by using a feature selection method (an algorithm that selects a subset of the original attribute set) in conjunction with the coefficient learning algorithm <ref> [44, 49] </ref>. Currently, OC1 does not have a built-in mechanism to select relevant attributes. However, it is easy to include any of several standard methods (e.g., stepwise forward selection or stepwise backward selection) or even an ad hoc method to select features before running the tree-building process.
Reference: [50] <author> Donald E. Brown, Vincent Corruble, and Clarence Louis Pittard. </author> <title> A comparison of decision tree classifiers with backpropagation neural networks for multimodal classification problems. </title> <journal> Pattern Recognition, </journal> <volume> 26(6) </volume> <pages> 953-961, </pages> <year> 1993. </year> <month> 253 </month>
Reference-contexts: Talmon et al. [467] compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other. In contrast, ID3 is adjudged to be slightly better than connectionist and Bayesian methods in [458]. Brown et al. <ref> [50] </ref> compared backpropagation neural networks with decision trees on three problems that are known to be multimodal. Their analysis indicated that there was not much difference between both methods, and that neither method performed very well in its "vanilla" state. The performance of decision trees improved in [50] when multivariate splits <p> Brown et al. <ref> [50] </ref> compared backpropagation neural networks with decision trees on three problems that are known to be multimodal. Their analysis indicated that there was not much difference between both methods, and that neither method performed very well in its "vanilla" state. The performance of decision trees improved in [50] when multivariate splits were used, and backpropagation networks did better with feature selection.
Reference: [51] <author> Donald E. Brown and Clarence Louis Pittard. </author> <title> Classification trees with optimal multivariate splits. </title> <booktitle> In Proceedings of the International Conference on Systems, Man and Cybernetics, </booktitle> <volume> volume 3, </volume> <pages> pages 475-477, </pages> <address> Le Touquet, France, 17-20th, </address> <month> October </month> <year> 1993. </year> <booktitle> IEEE, </booktitle> <address> New York. </address>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular [309, 28, 25, 307, 26]. Use of zero-one integer programming for designing vector quantizers can be found in [289]. Brown and Pittard <ref> [51] </ref> also employed linear programming for finding optimal multivariate splits at classification tree nodes. Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary.
Reference: [52] <author> Randal E. Bryant. </author> <title> Graph-based algorithms for boolean function manipulation. </title> <journal> IEEE Transactions on Computing, </journal> <volume> C-35(8):677-691, </volume> <month> August </month> <year> 1986. </year>
Reference-contexts: A lot of work exists in the speech and signal processing literature, 19 on building and analyzing TSVQs. A Binary Decision Diagram (BDD) represents a Boolean function as a rooted, directed acyclic graph [279, 441]. Ordered binary decision diagrams (OBDD) <ref> [52, 53] </ref> impose restrictions on the ordering of variables at the nodes of a BDD. OBDDs have been used for digital system design, verification and testing.
Reference: [53] <author> Randal E. Bryant. </author> <title> Symbolic boolean manipulation with ordered binary decision diagrams. </title> <type> Technical Report CMU-CS-92-160, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213., </address> <month> July </month> <year> 1992. </year> <note> Accepted to ACM Computing Surveys. </note>
Reference-contexts: A lot of work exists in the speech and signal processing literature, 19 on building and analyzing TSVQs. A Binary Decision Diagram (BDD) represents a Boolean function as a rooted, directed acyclic graph [279, 441]. Ordered binary decision diagrams (OBDD) <ref> [52, 53] </ref> impose restrictions on the ordering of variables at the nodes of a BDD. OBDDs have been used for digital system design, verification and testing. <p> Safavin and Landgrebe [417] more recently summarized decision tree construction methodology, almost entirely from a pattern recognition perspective. Bryant <ref> [53] </ref> surveyed the methodology and applications of ordered binary decision diagrams. 2.3 Finding splits To build a decision tree, it is necessary to find, at each internal node, a split for the data. <p> In the context of ordered binary decision diagrams, tree compaction has been attempted using operations that merge, delete and exchange nodes <ref> [53] </ref>. 36 2.4.1 Pruning Pruning, the method most widely used for obtaining right sized trees, was proposed by Breiman et al. ([44], Chapter 3).
Reference: [54] <author> Nader H. Bshouty. </author> <title> Exact learning via monotone theory. </title> <booktitle> In Proceedings. 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 302-311, </pages> <address> New York, NY, 1993. </address> <publisher> IEEE. </publisher>
Reference-contexts: Kushilevitz and Mansour [273] gave a polynomial time PAC-learning algorithm with membership queries for decision trees under the uniform distribution. Hancock [189] gave a polynomial time algorithm for PAC-learning read-k decision trees. Bshouty <ref> [54] </ref> showed that decision trees are learnable under the model of exact learning with membership queries and unrestricted 55 equivalence queries. Recently, agnostic PAC-learning [13] and pruning [205] have been studied by the learnability theory community.
Reference: [55] <author> R.S. Bucy and R.S. Diesposti. </author> <title> Decision tree design by simulated annealing. </title> <journal> Mathematical Modieling and Numerical Analysis, </journal> <volume> 27(5) </volume> <pages> 515-534, </pages> <year> 1993. </year> <note> A RAIRO Journal. </note>
Reference-contexts: The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. In order to escape local minima in the search space, randomized search techniques such as genetic programming 45 [264] and simulated annealing <ref> [55, 303] </ref> have been attempted. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [463]. <p> It will be interesting to explore the uses of randomization to build optimal or near-optimal axis-parallel trees. Randomized search techniques, such as genetic programming [264] and simulated annealing <ref> [55, 303] </ref> have already been used to improve axis-parallel decision trees. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits.
Reference: [56] <author> C. Bull, M. Chiogna, R. Franklin, and D. Spiegelhalter. </author> <title> Expert derived and automatically generated classification trees: an example from pediatric cardiology. </title> <booktitle> In Proceedings: Computers in Cardiology, </booktitle> <pages> pages 217-220, </pages> <address> Los Alamitos, CA, September 5th-8th 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [57] <author> M. E. Bullock, D. L. Wang, Fairchild S. R., and T. J. Patterson. </author> <title> Automated training of 3-D morphology algorithm for object recognition. </title> <booktitle> Proceedings of SPIE The International Society for Optical Engineering, </booktitle> <volume> 2234 </volume> <pages> 238-251, </pages> <year> 1994. </year> <title> Issue title: Automatic Object Recognition IV. </title>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in [442] and [423]. * Object recognition: Tree based classification has been used recently for recognizing three dimensional objects <ref> [456, 57] </ref> and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality
Reference: [58] <author> W. Buntine. </author> <title> Tree classification software. </title> <booktitle> Technology 2002: The third national technology transfer conference and exposition, </booktitle> <month> December </month> <year> 1992. </year>
Reference: [59] <author> W. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-85, </pages> <year> 1992. </year>
Reference-contexts: This data is used by Norton [365] in his experiments. The data contains 435 instances, each described by 16 nominal attributes and one class label. The task is to classify democrats from republicans on the basis of their voting records. V1 This data, used in <ref> [59, 209] </ref>, is identical to the VO data, except that the "best" attribute physician-fee freeze is removed. All the experimental results reported in this section are obtained with information gain. The experiments with Gini index did not offer any more insights, and are omitted for brevity.
Reference: [60] <author> Wray Buntine. </author> <title> Classifiers: A theoretical and empirical study. </title> <booktitle> In IJCAI-91 [220], </booktitle> <pages> pages 638-644. </pages> <editor> Editors: </editor> <publisher> John Mylopoulos and Ray Reiter. </publisher>
Reference: [61] <author> Wray Buntine. </author> <title> A theory of learning classification rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in <ref> [61, 62] </ref>. Oliver [368] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [490]. Rymon [415] suggested SE-trees, set enumeration structures each of which can embed several decision trees.
Reference: [62] <author> Wray Buntine. </author> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 63-73, </pages> <year> 1992. </year> <month> 254 </month>
Reference-contexts: For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338]. Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees <ref> [62] </ref>, for neural trees [102] and in machine learning [365, 403, 354]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. <p> On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes. Breiman et al. ([44], Chapter 4) proposed a method for building class probability trees. Quinlan discussed methods of extracting probabilities from decision trees in [397]. Buntine <ref> [62] </ref> described Bayesian methods for building, smoothing and averaging class probability trees. 16 Smoothing in the context of tree structured vector quantizers is described in [17]. <p> A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [274, 443, 444, 62, 203] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree [443, 444]. <p> Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in <ref> [61, 62] </ref>. Oliver [368] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [490]. Rymon [415] suggested SE-trees, set enumeration structures each of which can embed several decision trees. <p> Some techniques to augment decision tree methods to output class probabilities have been explored in the literature (Section 2.5.6). We experimented with one such technique <ref> [62] </ref> on the SDSS data. The most obvious way of determining class probabilities is to compute them directly from the counts of training examples at the leaf nodes. <p> The interested reader is referred to <ref> [62] </ref>.) Once the probability estimates are smoothed in each tree, we classify the test instances using the ten trees. Each tree outputs two probabilities for each test instance: the confidence of the object being a star and the confidence of it being a galaxy.
Reference: [63] <author> Wray Buntine and Rich Caruana. </author> <title> Introduction to IND and recursive partitioning. </title> <type> Technical Report FIA-91-28, </type> <institution> RIACS and NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <year> 1991. </year>
Reference-contexts: We used the implementations of these algorithms from the IND 2.1 package <ref> [63] </ref>. The default cart0 and c4.5 "styles" defined in the package were used, without altering any parameter settings. The cart0 style uses the Twoing Rule and 0-SE cost complexity pruning with 10-fold cross validation.
Reference: [64] <author> G. Burger and U. J utting. </author> <title> Specimen classification in cytometry: An intercom-parison of various means of decision making. </title> <booktitle> In Gelsema and Kanal [163], </booktitle> <pages> pages 509-519. </pages>
Reference: [65] <author> A. Buzo, A.H. Gray Jr., Robert M. Gray, and J.D. Markel. </author> <title> Speech coding based upon vector quantization. </title> <journal> IEEE Transactions on Accoustics, Speech and Signal Processing, </journal> <volume> 28 </volume> <pages> 562-574, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: The testing algorithms normally take the form of decision trees or AND/OR trees [488, 378]. Many heuristics used to construct decision trees are used for test sequencing also. Vector quantization (VQ) [167] is a data compression technique that has proved useful for image coding. Tree structured vector quantizers (TSVQ) <ref> [65] </ref> are structures very similar to decision trees. A lot of work exists in the speech and signal processing literature, 19 on building and analyzing TSVQs. A Binary Decision Diagram (BDD) represents a Boolean function as a rooted, directed acyclic graph [279, 441]. <p> Methods used for selecting 21 a good subset of features are typically quite different and are used as preprocessing steps to tree induction. (We will discuss feature subset selection methods separately in Section 2.5.1.) Tree structure vector quantizers, when they were proposed <ref> [65] </ref>, were grown one layer at a time, by splitting all nodes in the previous layer. Makhoul et al.[306] introduced an unbalanced tree algorithm that grew the tree a node at a time.
Reference: [66] <author> Janice D. Callahan and Stephen W. Sorensen. </author> <title> Rule induction for group decisions with statistical data an example. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 42(3) </volume> <pages> 227-234, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Pizzi and 60 Jackson [384] compare an expert systems developed using traditional knowledge engineering methods to Quinlan's ID3 [391] in the domain of tonsillectormy. Comparisons of CART to multiple linear regression and discriminant analysis can be found in <ref> [66] </ref> where it is argued that CART is more suitable than the other methods for very noisy domains with lots of missing values.
Reference: [67] <author> Jan M. Van Campenhout. </author> <title> Topics in measurement selection. </title> <booktitle> In Krishnaiah and Kanal [265], </booktitle> <pages> pages 793-803. </pages>
Reference-contexts: On the other hand, if the training sample has too many objects, a subsample 14 Van Campenhout <ref> [67] </ref> argues that increasing the amount of information in a measurement subset through enlarging its size or complexity never worsens the error probability of a truly Bayesian classifier.
Reference: [68] <author> C. Carter and Jason Catlett. </author> <title> Assessing credit card applications using machine learning. </title> <journal> IEEE Expert, </journal> <volume> Fall:71-79, </volume> <year> 1987. </year>
Reference: [69] <author> Rich Caruana and Dayne Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In ML-94 [331], </booktitle> <pages> pages 28-36. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: In stepwise backward elimination, we start with the full feature set and remove, at each step, the worst feature. When more than one feature is greedily added or removed, beam search is said to have been performed <ref> [445, 69] </ref>. <p> Some of these studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets. For examples of this work, see <ref> [251, 276, 69, 113, 336, 3] </ref>. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: [70] <author> Richard G. Casey and George Nagy. </author> <title> Decision tree design using a probabilistic model. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-30(1):93-99, </volume> <month> January </month> <year> 1984. </year>
Reference-contexts: entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition <ref> [197, 493, 70, 192] </ref>, in sequential fault diagnosis [488] and in machine learning [391]. Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop.
Reference: [71] <author> Jason Catlett. </author> <title> Megainduction. </title> <type> PhD thesis, </type> <institution> Basser Department of Computer Science, University of Sydney, Australia, </institution> <year> 1991. </year>
Reference-contexts: For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation 11 and gaining insight into the predictive and generalization structure of the data. For very large tree classifiers, the critical issue is optimizing structural properties (height, balance etc.) <ref> [493, 71] </ref>. 11 For a general discussion about the relationship between complexity and predictive accuracy of classifiers, see [380]. 34 Breiman et al. [44] pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [363, 253].
Reference: [72] <author> Jason Catlett. </author> <title> Tailoring rulesets to misclassification costs. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 88-94. </pages>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs <ref> [44, 96, 115, 72, 478] </ref>.
Reference: [73] <author> Gert Cauwenberghs. </author> <title> A fast stochastic error-descent algorithm for supervised learning and optimization. </title> <booktitle> In Proceedings of Neural Information Processing Systems, </booktitle> <year> 1992. </year>
Reference: [74] <author> G. Cestnik, I. Kononenko, and I. Bratko. </author> <title> Assistant 86: A knowledge acquisition tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning. </booktitle> <publisher> Sigma Press, </publisher> <year> 1987. </year>
Reference: [75] <author> C. Chan and J. Bao. </author> <title> On the design of a tree classifier and its application to speech recognition. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 5(5) </volume> <pages> 677-692, </pages> <month> December </month> <year> 1991. </year> <month> 255 </month>
Reference: [76] <author> B. Chandrasekaran. </author> <title> From numbers to symbols to knowledge structures: </title> <booktitle> Pattern Recognition and Artificial Intelligence perspectives on the classification task. In Gelsema and Kanal [163], </booktitle> <pages> pages 547-559. </pages>
Reference-contexts: An overview of work on decision trees in the patter recognition literature can be found in [106]. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in <ref> [76] </ref>. Decision trees in particular, and induction methods in general, arose in machine learning to avoid the knowledge acquisition bottleneck [137] for expert systems. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s CART work [44] and Quinlan's ID3 algorithm [391].
Reference: [77] <author> B. Chandrasekaran and A. K. Jain. </author> <title> Quantization complexity and independent measurements. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-23(1):102-106, </volume> <month> January </month> <year> 1974. </year>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: [78] <author> C.-L. Chang. </author> <title> Finding prototypes for nearest neighbor classifiers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 23(11) </volume> <pages> 1179-1184, </pages> <month> November </month> <year> 1974. </year>
Reference: [79] <author> Hsi Chang and Sitharama Iyengar. </author> <title> Efficient algorithms to globally balance a binary search tree. </title> <journal> Communications of the ACM, </journal> <volume> 27(7) </volume> <pages> 695-702, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: path length between the root and a leaf node, averaged over all the leaf nodes. (This measure is also used in the experiments in Chapter 5.) Although little if any work has been done on balancing decision trees, a great deal of research has considered balanced search trees (e.g., see <ref> [16, 79, 461, 93, 357] </ref>). Roughly speaking, this literature deals with techniques to restructure search trees when elements are 145 inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [80] <author> A. R. Chaturvedi and D. L. Nazareth. </author> <title> Investigating the effectiveness of conditional classification: an application to manufacturing scheduling. </title> <journal> IEEE Transactions on Engineering Management, </journal> <volume> 41(2) </volume> <pages> 183-193, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: For example, as part of a scheduling process, decisions need to be made regarding the release of new orders into the system as well as the assignment of work pieces to available workstations. Chaturvedi and Nazareth <ref> [80] </ref> discuss possible solutions for this problem and provide algorithms for conditional classification. Cox [95] argues that classification tree technology, as implemented in commercially 51 available systems, is often more useful for pattern recognition than it is for decision support.
Reference: [81] <author> M. R. Chmielewski and J. W. Grzymala-Busse. </author> <title> Global discretization of continuous attributes as preprocessing for machine learning. </title> <editor> In T. Y. Lin, editor, RSSC-94: </editor> <booktitle> The Third International Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pages 294-301, </pages> <address> San Jose, CA, </address> <month> November </month> <year> 1994. </year> <booktitle> American Association of Artificial Intelligence, </booktitle> <address> San Jose State University. </address>
Reference-contexts: The problem of meaningfully discretizing a continuous dimension is considered in [134, 245, 486, 343]. Methods of discretization that operate on a single continuous attribute at a time can be said to be "local" discretization methods. In contrast, "global" discretization methods simultaneously convert all continuous attributes <ref> [81] </ref>. Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature [135, 157]. Trees in which an internal node can have more than 2 children, have also been considered in the vector quantization literature [431].
Reference: [82] <author> Philip A. Chou. </author> <title> Applications of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1988. </year>
Reference-contexts: The problem of learning trees solely from prior probability distributions is considered in [10]. Learning decision trees from qualitative causal models acquired from domain experts is the topic of [382]. Several attempts at generalizing the decision tree representation exist. Chou <ref> [82] </ref> considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes (i.e., trellises are trees in which internal nodes may have multiple parents).
Reference: [83] <author> Philip A. Chou. </author> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4) </volume> <pages> 340-354, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This seems natural considering application domains such as spectral analysis and remote sensing [464]. In these fields, special techniques [436] were developed to accommodate discrete attributes into what are primarily algorithms for ordered attributes. Fast methods for splitting multiple valued categorical variables are described in <ref> [83] </ref>. In machine learning, a subfield of Artificial Intelligence, which in turn has been 33 dominated by symbolic processing, many tree induction methods (e.g. [388] were originally developed for categorical attributes. The problem of incorporating continuous attributes into these algorithms is considered subsequently.
Reference: [84] <author> Philip A. Chou and Robert M. Gray. </author> <title> On decision trees for pattern recognition. </title> <booktitle> In Proceedings of the IEEE Symposium on Information Theory, </booktitle> <pages> page 69, </pages> <address> Ann Arbor, MI, </address> <year> 1986. </year>
Reference-contexts: Such view points provide valuable tools for analyzing decision trees. Wang and Suen [493] show that entropy-reduction point of view is powerful in theoretically bounding search depth and classification error. Chou and Gray <ref> [84] </ref> view decision trees as variable-length encoder-decoder pairs, and show that rate is equivalent to tree depth while distortion is the probability of misclassification.
Reference: [85] <author> Philip A. Chou, Tom Lookabaugh, and Robert M. Gray. </author> <title> Optimal pruning with applications to tree-structured source coding and modeling. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(2) </volume> <pages> 299-315, </pages> <month> March </month> <year> 1989. </year>
Reference: [86] <author> Athene Chuk-Kwan Choy. </author> <title> Decision Rules and Decision Tree Approach in Pattern Recognition Theory. </title> <type> PhD thesis, </type> <institution> University of Missouri, Columbia, MO, </institution> <year> 1978. </year>
Reference: [87] <author> Krzysztof J. Cios and Ning Liu. </author> <title> A machine learning method for generation of a neural network architecture: A continuous ID3 algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 280-291, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks.
Reference: [88] <author> I. Cleote and H. Theron. CID3: </author> <title> An extension of ID3 for attributes with ordered domains. </title> <journal> South African Computer Journal, </journal> <volume> 4 </volume> <pages> 10-16, </pages> <month> March </month> <year> 1991. </year> <month> 256 </month>
Reference-contexts: Trees in which an internal node can have more than 2 children, have also been considered in the vector quantization literature [431]. An extension to ID3 [391] that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in <ref> [88] </ref>. 2.4 Obtaining the right sized trees One of the main difficulties of inducing a recursive partitioning structure is knowing when to stop. Obtaining the "right" sized trees may be important for several reasons, which depend on the size of the classification problem [162].
Reference: [89] <institution> Cleveland heart disease database. </institution> <note> Available in the UCI ML Repository. Collected by Roberto Detrano, </note> <institution> M.D., </institution> <type> Ph.D., </type> <institution> V.A. Medical center, Long Beach and Cleveland Clinic Foundation. </institution>
Reference-contexts: BC Breast cancer recurrence data [38]. Contains 286 instances, each described by 9 attributes and one class label. The task is to predict if a breast cancer event is to recur. CL Cleveland Clinic Foundation's heart disease diagnosis data <ref> [89] </ref>. Contains 303 instances, each described by 14 attributes including the class label. We use the "processed" data in the UCI repository, where there are only two classes, namely the presence and absence of heart disease. GL Glass identification data.
Reference: [90] <author> J.R.B. Cockett and J.A. Herrera. </author> <title> Decision tree reduction. </title> <journal> Journal of the ACM, </journal> <volume> 37(4) </volume> <pages> 815-842, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Simple heuristics to generalize and combine the rules generated from trees can act as a substitute for pruning for Quinlan's univariate trees. * Other: Cockett and Herrera <ref> [90] </ref> suggested a method to reduce an arbitrary binary decision tree to an "irreducible" form, using discrete decision theory principles. Every irreducible tree is optimal with respect to some expected testing cost criterion, and the tree reduction algorithm has the same worst-case complexity as most greedy tree induction methods.
Reference: [91] <author> W.W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 988-994. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored recently in [33]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [324, 91, 125] </ref>. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others.
Reference: [92] <author> Douglas Comer and Ravi Sethi. </author> <title> The complexity of trie index construction. </title> <journal> Journal of the ACM, </journal> <volume> 24(3) </volume> <pages> 428-440, </pages> <month> July </month> <year> 1977. </year>
Reference-contexts: The problem of constructing the smallest decision tree which best distinguishes characteristics of multiple distinct groups is shown to be NP-complete in [476]. Comer and Sethi <ref> [92] </ref> studied the asymptotic complexity of trie index construction in the document retrieval literature. Megiddo [317] investigated the problem of polyhedral separability (separating two sets of points using k hyperplanes), and proved that several variants of this problem are NP-complete.
Reference: [93] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press and McGraw-Hill Book Company, </publisher> <year> 1990. </year>
Reference-contexts: path length between the root and a leaf node, averaged over all the leaf nodes. (This measure is also used in the experiments in Chapter 5.) Although little if any work has been done on balancing decision trees, a great deal of research has considered balanced search trees (e.g., see <ref> [16, 79, 461, 93, 357] </ref>). Roughly speaking, this literature deals with techniques to restructure search trees when elements are 145 inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [94] <author> T.M. Cover and J.M. Van Campenhout. </author> <title> On the possible orderings in the measurement selection problems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-7(9), </volume> <year> 1977. </year>
Reference-contexts: Cover et al. <ref> [94, 484] </ref> showed that heuristic sequential feature selection methods can do arbitrarily worse than the optimal strategy. Mucciardi and Gose [342] compared seven feature subset selection techniques empirically and concluded that no technique was uniformly superior to the others.
Reference: [95] <author> Louis Anthony Cox. </author> <title> Using causal knowledge to learn more useful decision rules from data. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 151-160. </pages>
Reference-contexts: Chaturvedi and Nazareth [80] discuss possible solutions for this problem and provide algorithms for conditional classification. Cox <ref> [95] </ref> argues that classification tree technology, as implemented in commercially 51 available systems, is often more useful for pattern recognition than it is for decision support. He suggests several ways of modifying existing methods to be prescriptive rather than descriptive.
Reference: [96] <author> Louis Anthony Cox and Yuping Qiu. </author> <title> Minimizing the expected costs of classifying patterns by sequential costly inspections. </title> <booktitle> In AI&Statistics-93 [4]. </booktitle>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs <ref> [44, 96, 115, 72, 478] </ref>.
Reference: [97] <author> Louis Anthony Cox, Yuping Qiu, and Warren Kuehner. </author> <title> Heuristic least-cost computation of discrete classification functions with uncertain argument values. </title> <journal> Annals of Operations Research, </journal> <volume> 21(1) </volume> <pages> 1-30, </pages> <year> 1989. </year>
Reference-contexts: Optimal Decision Tree for a Training Set. The problem of computing the shallowest or smallest decision tree for a given data set is NP-complete (Section 2.6.1), meaning that it is highly unlikely that a polynomial solution will be found. Previous studies that attempted comparisons to optimal trees (e.g., <ref> [97] </ref>) used approaches like dynamic programming to generate the optimal trees. Because it is slow, this option is impractical for our study, in which we use hundreds of thousands of artificial data sets.
Reference: [98] <author> Stuart L. Crawford. </author> <title> Resampling strategies for recursive partitioning classification using the CART algorithm. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1987. </year>
Reference: [99] <author> Stuart L. Crawford. </author> <title> Extensions to the CART algorithm. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 31(2) </volume> <pages> 197-217, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Breiman et al. [44] describe a cross validation procedure that avoids reserving part of training data for pruning, but has a large computational complexity. Quinlan's pessimistic pruning [393, 398] does away with the need for a separate pruning set by using a statistical correlation test. Crawford <ref> [99] </ref> analyzed Breiman et al.'s cross validation procedure, and pointed out that it has a large variance, especially for small training samples. He suggested a .632 bootstrap method 13 as an effective alternative. <p> Crawford <ref> [99] </ref> shows that approaches like Utgoff's, which attempt to update the tree so that the "best" split according to the updated sample is taken at each node, suffer from repeated restructuring. <p> This occurs because the best split at a node vacillates widely while the sample at the node is still small. An incremental version of CART [44] that uses significance thresholds to avoid the above problem is described in <ref> [99] </ref>. 2.5.9 Tree quality measures The fact that several trees can correctly represent the same data raises the question of how to decide that one tree is better than another. Several measures have been suggested to quantify tree quality.
Reference: [100] <author> Stephen P. Curram and John Mingers. </author> <title> Neural networks, decision tree induction and discriminant analysis: An empirical comparison. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 45(4) </volume> <pages> 440-450, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Long et al. [295] compared Quinlan's C4 [398] to logistic regression on the problem of diagnosing acute cardiac ischemia, and concluded that both methods came fairly close to the expertise of the physicians. In their experiments, logistic regression outperformed C4. Curram and Mingers <ref> [100] </ref> compare decision trees, neural networks and discriminant analysis on several real world data sets.
Reference: [101] <author> K.T. Dago, R. Luthringer, R. Lengelle, G. Rinaudo, and J. P. </author> <title> Matcher. Statistical decision tree: A tool for studying pharmaco-EEG effects of CNS-active drugs. </title> <journal> Neuropsychobiology, </journal> <volume> 29(2) </volume> <pages> 91-96, </pages> <year> 1994. </year>
Reference-contexts: trees for analyzing amino acid sequences can be found in [442] and [423]. * Object recognition: Tree based classification has been used recently for recognizing three dimensional objects [456, 57] and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in <ref> [101] </ref>. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality to pine trees [19]. * Power systems: Power system security assessment [199] and power stability prediction [414] are two areas in power
Reference: [102] <author> Florence DAlch e-Buc, Didier Zwierski, and Jean-Pierre Nadal. </author> <title> Trio learning: A new strategy for building hybrid neural trees. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5(4) </volume> <pages> 259-274, </pages> <month> December </month> <year> 1994. </year> <month> 257 </month>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks. <p> Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees <ref> [102] </ref> and in machine learning [365, 403, 354]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: [103] <author> S.K. Das and S. Bhambri. </author> <title> A decision tree approach for selecting between demand based, reorder and JIT/kanban methods for material procurement. Production Planning and Control, </title> <address> 5(4):342, </address> <year> 1994. </year>
Reference-contexts: nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection <ref> [103] </ref>, to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [104] <author> Belur V. Dasarathy, </author> <title> editor. Nearest neighbor (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Quinlan suggested "win-dowing", a random training set sampling method, for his programs ID3 and C4.5 [398, 506]. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see <ref> [104, 105] </ref>, for examples). Some of these may be helpful for inducing trees efficiently on large samples, if it is possible to choose good subsamples efficiently. 2.5.2 Incorporating costs In most real-world domains, attributes can have costs of measurement, and objects can have misclassification costs.
Reference: [105] <author> Belur V. Dasarathy. </author> <title> Minimal consistent set (MCS) identification for optimal nearest neighbor systems design. </title> <journal> IEEE transactions on systems, man and cybernetics, </journal> <volume> 24(3) </volume> <pages> 511-517, </pages> <year> 1994. </year>
Reference-contexts: Quinlan suggested "win-dowing", a random training set sampling method, for his programs ID3 and C4.5 [398, 506]. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see <ref> [104, 105] </ref>, for examples). Some of these may be helpful for inducing trees efficiently on large samples, if it is possible to choose good subsamples efficiently. 2.5.2 Incorporating costs In most real-world domains, attributes can have costs of measurement, and objects can have misclassification costs.
Reference: [106] <editor> G. R. Dattatreya and Laveen N. Kanal. </editor> <title> Decision trees in pattern recognition. </title> <editor> In Kanal and Rosenfeld, editors, </editor> <booktitle> Progress in Pattern Recognition, </booktitle> <volume> volume 2, </volume> <pages> pages 189-239. </pages> <publisher> Elsevier Science, </publisher> <year> 1985. </year>
Reference-contexts: An overview of work on decision trees in the patter recognition literature can be found in <ref> [106] </ref>. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [76]. Decision trees in particular, and induction methods in general, arose in machine learning to avoid the knowledge acquisition bottleneck [137] for expert systems.
Reference: [107] <author> G. R. Dattatreya and V. V. S. Sarma. </author> <title> Bayesian and decision tree approaches to pattern recognition including feature measurement costs. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-3(3):293-298, </volume> <year> 1981. </year>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, <ref> [107, 340] </ref> in pattern recognition and [250] in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478].
Reference: [108] <author> L. Devroye. </author> <title> Automatic pattern recognition : A study of the probability of error. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(4) </volume> <pages> 530-543, </pages> <year> 1988. </year>
Reference: [109] <author> Thomas G. Dietterich and Ghulum Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <month> January </month> <year> 1995. </year>
Reference: [110] <author> Thomas G. Dietterich, Hermann Hild, and Ghulum Bakiri. </author> <title> A comparison of ID3 and backpropagation for english text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 51-80, </pages> <year> 1995. </year>
Reference-contexts: Dietterich et al. <ref> [110] </ref> argue that the inadequacy of trees for certain domains may be due to the fact that trees are unable to take into account some statistical information that is available to other methods like neural networks.
Reference: [111] <author> Thomas G. Dietterich and Eun Bae Kong. </author> <title> Machine learning bias, statistical bias and statistical variance of decision tree algorithms. </title> <note> In ML-95 [333]. to appear. </note>
Reference-contexts: Averaging improves probability estimates by considering multiple trees. 47 2.5.7 Multiple trees A known peril of decision tree construction is its variance, especially when the samples are small and the features are many <ref> [111] </ref>. Variance can be caused by random choice of training and pruning samples, by many equally good attributes only one of which can be chosen at a node, due to cross validation or because of other reasons. <p> These conclusions, however, were refuted by Elomaa [123] on several grounds. Elomaa argued that Holte's observations may have been the peculiarities of the data he used, and that the slight differences in accuracy that Holte observed were still significant. 20 It is argued empirically <ref> [111] </ref> that the variance in decision tree methods is more a reason than bias for their poor performance on some domains. 57 * Bias: Smaller consistent decision trees have higher generalization accuracy than larger consistent trees (Occam's Razor). Analysis: Murphy and Pazzani [347] empirically investigated the truth of this bias.
Reference: [112] <author> Thomas G. Dietterich and Ryszard S. Michalski. </author> <title> A comparitive view of selected methods for learning from examples. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning, an Artificial Intelligence Approach, </booktitle> <volume> volume 1, </volume> <pages> pages 41-81. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference-contexts: Several researchers have compared trees to these other methods on specific problems. An early study comparing machine learning methods for learning from examples can be found in <ref> [112] </ref>. Comparisons of symbolic and connectionist methods can be found in [501, 440]. Quinlan empirically compared decision trees to genetic classifiers [394] and to neural networks [400]. Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems.
Reference: [113] <author> Justin Doak. </author> <title> An evaluation of search algorithms for feature selection. </title> <type> Technical report, </type> <institution> Graduate Group in Computer Science, University of California at Davis; and Safeguards Systems Group, Los Alamos National Laboratory, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Some of these studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets. For examples of this work, see <ref> [251, 276, 69, 113, 336, 3] </ref>. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: [114] <author> M. Doi, J. Gunn, and D. Weinberg. </author> <title> Simulated data for the SDSS: User's guide. </title> <type> Technical report, </type> <institution> Princeton University, Princeton, NJ, </institution> <year> 1994. </year> <month> 258 </month>
Reference-contexts: A set of simulated images and object catalogs are being made available to the SDSS researchers. The procedures for generating the simulated data are quite elaborate, and some details can be found in <ref> [114] </ref>. Considerable effort is being expended in making the simulated data as close to reality as possible, in terms of the noise levels, magnitudes, colors, atmospheric effects, distributions of objects etc. <p> will be released at the end of the second year of the survey, probably in 1997 or 1998.) We were able to get very high accuracies on this data, almost down to the detection limit of the survey. 6.2.1 The task There are three basic categories of simulated data sets <ref> [114] </ref>: catalogs, images and tapes. A catalog is a list of objects, each of which has a position, magnitude, colors, shape parameters and so forth. An image represents a CCD frame, or a subsection of a frame.
Reference: [115] <author> B. A. Draper, Carla E. Brodley, and Paul E. Utgoff. </author> <title> Goal-directed classification using linear machine decision trees. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(9):888, </volume> <year> 1994. </year>
Reference-contexts: John [229] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [364], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes <ref> [49, 115] </ref>. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [496, 450] suggested a procedure to train a linear split to minimize the error probability. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs <ref> [44, 96, 115, 72, 478] </ref>.
Reference: [116] <author> N. R. Draper and H. Smith. </author> <title> Applied Regression Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1966. </year> <note> 2nd edition in 1981. </note>
Reference-contexts: Moreover, most real world classifiers are not truly Bayesian. 40 selection method (Section 2.5.1) can be employed to filter out the unnecessary observations. Feature subset selection There is a large body of work on choosing relevant subsets of features (for example, see the texts <ref> [116, 35, 322] </ref>). Most of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: [117] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Methods used in the literature for finding good linear tests include linear discriminant analysis, hill climbing search, linear programming, perceptron training and others. Linear Discriminant Trees: Several authors have considered the problem of constructing tree-structured classifiers that have linear discriminants <ref> [117] </ref> at each node. You and Fu [511] used a linear discriminant at each node in the decision tree, computing the hyperplane coefficients using the Fletcher-Powell descent method [144]. Their method requires that the best set of features at each node be prespecified by a human. <p> Decision trees with perceptrons at all internal nodes were described in [482, 438]. Mathematical Programming: Linear programming has been used for building adaptive classifiers since late 1960s [216]. Given two possibly interesecting sets of points, Duda 31 and Hart <ref> [117] </ref> proposed a linear programming formulation for finding the split whose distance from the misclassified points is minimized. More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular [309, 28, 25, 307, 26]. <p> However, there seem to be three significant problems with LP-based tree building methods. 80 * LP methods are unlikely to be robust to non-uniformly distributed noise in the data. We are currently experimenting with the LP formulations of <ref> [117, 25] </ref> to verify this hypothesis, and our preliminary results support the hypothesis. * For some multimodal class distributions, impurity-based methods can "shave-off" homogeneous corners of the attribute space, successively reducing the problem size and complexity. <p> Figure 3.8 shows a simple data set for which the LP formulations in <ref> [117, 25] </ref> fail to produce any split. Our experimental section includes results showing how each of these methods compares to OC1. Our algorithm, OC1, uses deterministic hill climbing most of the time, 81 of the data sets to be separated coincide.
Reference: [118] <author> Gunter Dueck and Tobias Scheuer. </author> <title> Threshold accepting: A general purpose optimization algorithm appearing superior to simulated annealing. </title> <journal> Journal of computational physics, </journal> <volume> 90 </volume> <pages> 161-175, </pages> <year> 1990. </year>
Reference-contexts: For instance, one may use linear programming in place of our deterministic perturbation algorithm, and perhaps impose limits on the amount of deterministic search to be performed before the random perturbations are attempted. Random search methods like simulated annealing or threshold accepting <ref> [118] </ref> may be used in place of our randomization 35 OC1's deterministic coefficient perturbation algorithm was constructed independently of CART, and this is the reason for the slight differences in the algorithm. We realized the similarity with CART subsequently. 121 steps.
Reference: [119] <author> Eades and Staples. </author> <title> On optimal trees. </title> <journal> Journal of Algorithms, </journal> <volume> 2(4) </volume> <pages> 369-384, </pages> <year> 1981. </year>
Reference-contexts: Miyakawa [328] considered the problem of coverting decision tables to optimal trees, and studied the properties of optimal variables, the class of attributes only members of which can be used at the root of an optimal tree. Eades and Staples <ref> [119] </ref> showed that the optimality in search trees, in terms of worst-case depth, is very closely related to regularity. A c-regular tree is a tree in which all nodes have c children, and if one child of an internal node is a leaf, then so are all other children.
Reference: [120] <author> Bradley Efron. </author> <title> Estimating the error rate of a prediction rule: improvements on cross-validation. </title> <journal> Journal of American Statistical Association, </journal> <volume> 78(382) </volume> <pages> 316-331, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: An error in their 13 In bootstrapping, B independent learning samples, each of size N are created by random sampling with replacement from the original learning sample L. In cross validation, L is divided randomly into B mutually exclusive, equal sized partitions. Efron <ref> [120] </ref> showed that, although cross validation closely approximates the true result, bootstrap has much less variance, especially for small samples.
Reference: [121] <author> A. Ehrenfeucht and David Haussler. </author> <title> Learning decision trees from random examples. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 231-246, </pages> <year> 1989. </year>
Reference-contexts: Computational Learning Theory is a young discipline that studies the "learn-ability" of specific concepts or concept classes. For a good introduction to the theory of learnability, see [242]. We summarize below significant learnability results for decision trees. Ehrenfeucht and Haussler <ref> [121] </ref> gave an algorithm for PAC-learning (without membership queries) decision trees of constant rank in polynomial time. They also gave a PAC-learning algorithm for general polynomial size decision trees in time O (n O (log n) ).
Reference: [122] <author> John F. Elder, </author> <title> IV. Heuristic search for model structure. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 199-210. </pages>
Reference-contexts: The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338]. Tree construction using partial or exhaustive lookahead has been considered in statistics <ref> [139, 122] </ref>, in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning [365, 403, 354]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: [123] <author> Tapio Elomaa. </author> <title> In defence of C4.5: Notes on learning one-level decision trees. </title> <booktitle> In ML-94 [331], </booktitle> <pages> pages 62-69. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: He concluded that, on most real world data sets commonly used by the machine learning community [346], decision trees do not perform significantly better than one level rules. These conclusions, however, were refuted by Elomaa <ref> [123] </ref> on several grounds.
Reference: [124] <author> A. Ercil. </author> <title> Classification trees prove useful in nondestructive testing of spotweld quality. </title> <journal> Welding Journal, </journal> <volume> 72(9):59, </volume> <month> September </month> <year> 1993. </year> <note> Issue Title: Special emphasis: Rebuilding America's roads, railways and bridges. </note>
Reference-contexts: Control Systems: Automatic induction of decision trees was recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality <ref> [124] </ref>, for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [125] <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> A further study of pruning methods in decision tree induction. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 211-218. </pages>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored recently in [33]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [324, 91, 125] </ref>. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others.
Reference: [126] <author> Bob Evans and Doug Fisher. </author> <title> Overcoming process delays with decision tree induction. </title> <journal> IEEE Expert, </journal> <pages> pages 60-66, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing <ref> [126] </ref>, for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [127] <author> Brian Everitt. </author> <title> Cluster Analysis 3rd Edition. </title> <editor> E. </editor> <publisher> Arnold Press, </publisher> <address> London., </address> <year> 1993. </year>
Reference-contexts: Work Not Covered: Work not covered includes automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis <ref> [127] </ref> and machine learning (e.g., [141, 165]). A body of work using decision trees as a representational paradigm, existing in fields such as programming languages and analysis of algorithms, is not included. <p> (EMST) clustering as a preprocessing step, on the CB, RCB and RGC domains and two real-world data sets taken from the UCI machine learning repository [346]. 7.3.1 Minimum Spanning Tree Clustering A host of unsupervised clustering methods have been developed in the fields of psychology, statistics and machine learning. (See <ref> [127, 241] </ref> for a tutorial.) The choice of which clustering technique to use for a given data set is often very difficult. Most techniques require the user to define the number of clusters in advance, and those that do not, often require tuning of various parameters.
Reference: [128] <editor> Final orbital/science verification report: </editor> <title> Wide field/planetary camera investigation definition team, </title> <month> February </month> <year> 1991. </year> <note> Principal Investigator: </note> <author> James A. Westphal. </author> <month> 259 </month>
Reference: [129] <author> Judith A. Falconer, Bruce J. Naughton, Dorothy D. Dunlop, Elliot J. Roth, and Dale C. Strasser. </author> <title> Predicting stroke inpatient rehabilitation outcome using a classification tree approach. </title> <journal> Archives of Physical Medicine and Rehabilitation, </journal> <volume> 75(6):619, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology <ref> [295, 129, 258] </ref>, psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data
Reference: [130] <author> A. Famili. </author> <title> Use of decision tree induction for process optimization and knowledge refinement of an industrial process. </title> <journal> Artificial Intelligence for Engineering Design, Analysis and Manufacturing (AI EDAM), </journal> <volume> 8(1) </volume> <pages> 63-75, </pages> <month> Winter </month> <year> 1994. </year>
Reference-contexts: asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining <ref> [130] </ref>, to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [131] <author> R. M. Fano. </author> <title> Transmission of Information. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1961. </year>
Reference-contexts: Goodrich [176] proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 2.6.2 Other analytical results Goodman and Smyth [174] showed that greedy top-down induction of decision trees is directly equivalent to a form of Shannon-Fano prefix coding <ref> [131] </ref>. A consequence of this result is that top-down tree induction (using mutual information) is necessarily suboptimal 19 Thanks to Kevin Van Horn for pointing this out. 54 in terms of average tree depth.
Reference: [132] <author> Usama M. Fayyad and Keki B. Irani. </author> <title> What should be minimized in a decision tree? In AAAI-90: </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 749-754. </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: Moret [338] summarizes work on measures such as tree size, expected testing cost and worst-case testing cost. He shows that these three measures are pairwise incompatible, which implies that an algorithm minimizing one measure is guaranteed not to minimize the others, for some tree. Fayyad and Irani <ref> [132] </ref> argue that, by concentrating on optimizing one measure, number of leaf nodes, one can achieve performance improvement along other measures. Generalization accuracy is a popular measure for quantifying the goodness learning systems.
Reference: [133] <author> Usama M. Fayyad and Keki B. Irani. </author> <title> The attribute specification problem in decision tree generation. </title> <booktitle> In AAAI-92 [7], </booktitle> <pages> pages 104-110. </pages>
Reference-contexts: Class separation-based metrics developed in the machine learning literature <ref> [133, 514] </ref> are also distance measures. <p> Miyakawa [329] compared three activity-based measures, Q, O and loss, both analytically and empirically. He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. Fayyad and Irani <ref> [133] </ref> showed that their measure C-SEP, performs better than Gini index [44] and information gain [391] for specific types of problems. 27 Several researchers [195, 391] pointed out that information gain is biased towards attributes with a large number of possible values.
Reference: [134] <author> Usama M. Fayyad and Keki B. Irani. </author> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8(2) </volume> <pages> 87-102, </pages> <year> 1992. </year>
Reference-contexts: The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in <ref> [134, 245, 486, 343] </ref>. Methods of discretization that operate on a single continuous attribute at a time can be said to be "local" discretization methods. In contrast, "global" discretization methods simultaneously convert all continuous attributes [81].
Reference: [135] <author> Usama M. Fayyad and Keki B. Irani. </author> <title> Multi-interval discretization of continuous valued attributes for classification learning. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 1022-1027. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: In contrast, "global" discretization methods simultaneously convert all continuous attributes [81]. Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature <ref> [135, 157] </ref>. Trees in which an internal node can have more than 2 children, have also been considered in the vector quantization literature [431].
Reference: [136] <author> Usama M. Fayyad, Nicholas Weir, and D. Djorgovski. SKICAT: </author> <title> A machine learning system for automated cataloging of large scale sky surveys. </title> <booktitle> In ML-93 [330], </booktitle> <pages> pages 112-119. </pages> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference: [137] <author> Edward A. Feigenbaum. </author> <title> Expert systems in the 1980s. </title> <editor> In A. Bond, editor, </editor> <booktitle> State of the Art in Machine Intelligence. </booktitle> <address> Pergamon-Infotech, Maidenhead, </address> <year> 1981. </year>
Reference-contexts: A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [76]. Decision trees in particular, and induction methods in general, arose in machine learning to avoid the knowledge acquisition bottleneck <ref> [137] </ref> for expert systems. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s CART work [44] and Quinlan's ID3 algorithm [391].
Reference: [138] <author> C. Feng, A. Sutherland, R. King, S. Muggleton, and R. Henery. </author> <title> Comparison of machine learning classifiers to statistics and neural networks. </title> <booktitle> In AI&Statistics-93 [4], </booktitle> <pages> pages 41-52. </pages>
Reference-contexts: Comparisons between decision trees and statistical methods like linear discriminant function analysis and automatic interaction detection (AID) are given in [313], where it is argued that machine learning methods sometimes outperform the statistical methods and so should not be ignored. Feng et al. <ref> [138] </ref> present a comparison of several machine learning methods (including decision trees, neural networks and statistical classifiers) as a part of the European Statlog 21 project.
Reference: [139] <author> A. Fielding. </author> <title> Binary segmentation: the automatic interaction detector and related techniques for exploring data structure. </title> <booktitle> In O'Muircheartaigh and Payne [370], </booktitle> <pages> pages 221-257. </pages>
Reference-contexts: For a general rationale for multistage classification schemes and a categorization of such schemes, see [237]. 2.2.1 Treatises The work on decision tree construction in statistics has its origins in methods for exploring survey data. For a review of earlier statistical work on hierarchical classification, see <ref> [139] </ref>. Statistical programs such as AID [454], MAID [170], THAID [339] and CHAID [240] built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables. <p> impurities, assigns an integer to each class and measures the variance between class numbers in each partition [204, 351]. 9 Quinlan's C4.5 [398] uses a naive version of the confidence intervals for doing pessimistic pruning. 25 An almost identical measure was used earlier in the Automatic Interaction Detection (AID) program <ref> [139] </ref>. Most of the above feature evaluation criteria assume no knowledge of the probability distribution of the training objects. <p> The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338]. Tree construction using partial or exhaustive lookahead has been considered in statistics <ref> [139, 122] </ref>, in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning [365, 403, 354]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. <p> He suggested an improvement to inaccuracy called weighted inaccuracy. 31 Sum Of Variances was called Sum of Impurities by Heath et al.. The earliest use of this measure we found was in the Automatic Interaction Detection (AID) program <ref> [139] </ref>. 98 how numbers are assigned to the classes.
Reference: [140] <author> P. E. File, P. I. Dugard, and A. S. Houston. </author> <title> Evaluation of the use of induction in the develeopment of a medical expert system. </title> <journal> Computers and Biomedical Research, </journal> <volume> 27(5) </volume> <pages> 383-395, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology [295, 129, 258], psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders <ref> [140] </ref>. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data exploration methods in molecular biology.
Reference: [141] <author> Douglas Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 130-172, </pages> <year> 1987. </year> <month> 260 </month>
Reference-contexts: Work Not Covered: Work not covered includes automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis [127] and machine learning (e.g., <ref> [141, 165] </ref>). A body of work using decision trees as a representational paradigm, existing in fields such as programming languages and analysis of algorithms, is not included.
Reference: [142] <author> Douglas Fisher and Kathleen McKusick. </author> <title> An empirical comparison of ID3 and back propagation. </title> <booktitle> In IJCAI-89 [219]. </booktitle> <editor> Editor: N. S. </editor> <publisher> Sridharan. </publisher>
Reference-contexts: Multilayer perceptrons and CART (with and without linear combinations) [44] are compared in [12] to find that there is not much difference in accuracy. Similar conclusions were reached in <ref> [142] </ref> when ID3 [391] and backpropagation were compared. Talmon et al. [467] compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other. In contrast, ID3 is adjudged to be slightly better than connectionist and Bayesian methods in [458].
Reference: [143] <author> P. A. Flach. </author> <title> Predicate invention in inductive data engineering. </title> <editor> In P. B. Brazdil, editor, </editor> <booktitle> ECML: European Conference on Machine Learning, </booktitle> <address> Berlin, Germany, </address> <year> 1993. </year> <note> Springer-Verlag. Conf. held in Vienna, </note> <institution> Austria, </institution> <month> 5-7 April </month> <year> 1993. </year>
Reference-contexts: Scatterd attempts exist in literature which qualify as examples of domain-independent data massaging. Nearest Neighbor classification is used to preprocess training data before using a neural network, in [362]. Flach <ref> [143] </ref> discusses inductive data engineering, an interactive process of restructuring a knowledge base by means of rule induction. Section 7.1 presents some "simple" artificial data sets for which several common goodness measures fail to produce good trees.
Reference: [144] <author> R. Fletcher and M. J. D. Powell. </author> <title> A rapidly convergent descent method for minimization. </title> <journal> Computer Journal, </journal> <volume> 6(ISS.2):163-168, </volume> <year> 1963. </year>
Reference-contexts: Linear Discriminant Trees: Several authors have considered the problem of constructing tree-structured classifiers that have linear discriminants [117] at each node. You and Fu [511] used a linear discriminant at each node in the decision tree, computing the hyperplane coefficients using the Fletcher-Powell descent method <ref> [144] </ref>. Their method requires that the best set of features at each node be prespecified by a human. Friedman [152] reported that applying Fisher's linear discriminants, instead of atomic features, at some internal nodes was useful in building better trees.
Reference: [145] <author> D. H. Foley. </author> <title> Considerations of sample and feature size. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-18:618-626, </volume> <year> 1972. </year>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: [146] <author> F. Forouraghi, L. W. Schmerr, and G. M. Prabhu. </author> <title> Induction of multivariate regression trees for design optimization. </title> <booktitle> In AAAI-94 [9], </booktitle> <pages> pages 607-612. </pages>
Reference-contexts: C4.5 [398] uses a simple form of soft splitting (chapter 8). 46 Use of fuzzy splits in pattern recognition literature can be found in [432, 494]. Jordan and Jacobs [233] describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered <ref> [146] </ref>. Induction of fuzzy decision trees has also been considered in [281, 512]. 2.5.6 Estimating probabilities Decision trees have crisp decisions at leaf nodes. On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes.
Reference: [147] <author> Iman Foroutan. </author> <title> Feature Selection for Piecewise Linear Classifiers. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Irvine, CA, </address> <year> 1985. </year>
Reference-contexts: Their method identifies the closest-opposed pairs of clusters in the data, and trains each linear discriminant locally. The final classifier produced by this method is a piecewise linear decision surface, not a tree. Foroutan <ref> [147] </ref> discovered that the resubstitution error rate of optimized piece-wise linear classifiers is 30 nearly monotonic with respect to the number of features. Based on this result, Foroutan and Sklansky [148] suggest an effective feature selection procedure for linear splits that uses zero-one integer programming.
Reference: [148] <author> Iman Foroutan and Jack Sklansky. </author> <title> Feature selection for automatic classification of non-Gaussian data. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 17(2) </volume> <pages> 187-198, </pages> <month> March/April </month> <year> 1987. </year>
Reference-contexts: The final classifier produced by this method is a piecewise linear decision surface, not a tree. Foroutan [147] discovered that the resubstitution error rate of optimized piece-wise linear classifiers is 30 nearly monotonic with respect to the number of features. Based on this result, Foroutan and Sklansky <ref> [148] </ref> suggest an effective feature selection procedure for linear splits that uses zero-one integer programming. Park and Sklansky [375, 376] describe methods to induce linear tree classifiers and piece-wise linear discriminators. The main idea in these methods is to find hyperplanes that cut a maximal number of Tomek links. <p> Feature subsets have been compared in the literature using either a feature evaluation criterion discussed in Section 2.3.1 (e.g. Bhattacharya distance was used for comparing subsets of features in [358]), or using direct error estimation <ref> [148, 230] </ref>. The second component of feature subset selection methods is a search algorithm through the space of possible feature subsets. Most existing search procedures are heuristic in nature, 15 as exhaustive search for the best feature subset is typically prohibitively expensive. A heuristic commonly used is the greedy heuristic.
Reference: [149] <author> Richard S. Forsyth, David D. Clarke, and Richard L. Wright. </author> <title> Overfit-ting revisited: an information-theoretic approach to simplifying discrimination trees. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6(3) </volume> <pages> 289-302, </pages> <month> July-September </month> <year> 1994. </year>
Reference-contexts: However, there exist arguments that cross validation is clearly preferable to bootstrap in practice [256]. 38 coding method (which did not have an effect on their main conclusions) was pointed out in [491]. Forsyth et al. <ref> [149] </ref> recently suggested a pruning method that is based on viewing the decision tree as an encoding for the training data. Use of dynamic programming to prune trees optimally and efficiently has been explored recently in [33]. <p> Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier <ref> [402, 149] </ref> . 2.5.10 Miscellaneous Most existing tree induction systems proceed in a greedy top-down fashion [464, 44, 391]. Bottom up induction of trees is considered in [275].
Reference: [150] <author> M. Frean. </author> <title> Small Nets and Short Paths: Optimising neural computation. </title> <type> PhD thesis, </type> <institution> Centre for Cognitive Science, University of Edinburgh, </institution> <year> 1990. </year>
Reference-contexts: The training algorithm presents examples repeatedly at each node until the linear machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to determine when the node has stabilized. To make the training stable even when the set of training instances is not linearly separable, a "thermal training" method <ref> [150] </ref> is used, similar to simulated annealing. 78 uses random jumps to escape local minima are marked.
Reference: [151] <author> W. L. Freedman, S. M. Hughes, B. F. Madore, J. R. Mould, M. G. Lee, P. Stetson, R. C. Kennicutt, A. Turner, L. Ferrarese, H. C. Ford, J. A. Graham, R. Hill, J. G. Hoessel, J. Huchra, and G. D. Illingworth. </author> <title> The Hubble Space Telescope extragalactic distance scale key project. I. The discovery of cepheids and a new distance to M81. </title> <journal> Astrophysical Journal, </journal> <volume> 427:628, </volume> <year> 1994. </year>
Reference-contexts: We histogramed the V magnitudes from both images. Before plotting the histogram, we needed to find the correct zeropoint magnitude for our data, in order to use it as an offset. We followed a procedure similar to that outlined in <ref> [151] </ref>. We located the 5 Cepheids listed for CCD WF2 and did core fitting with an aperture of 2.5 pixels. In order to perform the aperture correction, we used a theoretical PSF generated by TinyTim for WF2.
Reference: [152] <author> Jerome H. Friedman. </author> <title> A recursive partitioning decision rule for nonparametric classifiers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26:404-408, </volume> <month> April </month> <year> 1977. </year>
Reference-contexts: criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance <ref> [152, 413, 198] </ref> and the 2 statistic [21, 195, 323, 515, 503] are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> You and Fu [511] used a linear discriminant at each node in the decision tree, computing the hyperplane coefficients using the Fletcher-Powell descent method [144]. Their method requires that the best set of features at each node be prespecified by a human. Friedman <ref> [152] </ref> reported that applying Fisher's linear discriminants, instead of atomic features, at some internal nodes was useful in building better trees. Qing-Yun and Fu [387] also describe a method to build linear discriminant trees. <p> This strategy, which is known to be not robust, is used in some early methods <ref> [152] </ref>. * Two stage search: In this variant, tree induction is divided into two subtasks: first, a good structure for the tree is determined; then splits are found at all the nodes. 12 The optimization method in the first stage may or may not be related to that used in the <p> In the decision tree literature, Henrichon and Fu [206] were probably the first to discuss "transgenerated" features, features generated from the original attributes. Friedman's tree induction method <ref> [152] </ref> could consider with equal ease atomic and composite features. Techniques to search for multivariate splits (Section 2.3.2) can be seen as ways for constructing composite features. Use of linear regression to find good feature combinations is explored recently in [36]. <p> Several researchers have addressed the problem of dealing with missing attribute values in the training as well as testing sets. For training data, Friedman <ref> [152] </ref> suggested that all objects with missing attribute values can be ignored while forming the split at each node. <p> A crucial property of neural network training methods is that they are incremental | network weights can be continually adjusted to accommodate training examples. Incremental induction of decision trees is considered by several authors. Friedman's binary tree induction method <ref> [152] </ref> could use "adaptive" fea 17 A lot of work exists in the neural networks literature on using committees or ensembles of networks to improve classification performance. See [193] for example.
Reference: [153] <author> Jerome H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <type> Technical report, </type> <institution> Department of Statistics and Stanford Linear Accelerator Center, Stanford University, Stanford, </institution> <address> CA 94305, </address> <month> November </month> <year> 1994. </year>
Reference: [154] <author> M. Fujita, H. Fujisawa, and Y. Matsunaga. </author> <title> Variable ordering algorithms for ordered binary decision diagrams and their evaluation. </title> <journal> IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, </journal> <volume> 12(1) </volume> <pages> 6-12, </pages> <month> January </month> <year> 1993. </year> <month> 261 </month>
Reference-contexts: In the context of ordered binary decision diagrams (OBDDs), the order in which variables are chosen at tree nodes determines the complexity of the OBDD, and many heuristics have been evaluated for variable order selection (eg., <ref> [457, 154] </ref>). In machine learning, feature evaluation rules are used mainly for picking the single best feature at every node of the decision tree.
Reference: [155] <author> Keinosuke Fukanaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference: [156] <author> Keinosuke Fukanaga and R. A. Hayes. </author> <title> Effect of sample size in classifier design. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 873-885, </pages> <year> 1989. </year>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa. <p> Several authors (e.g., <ref> [156] </ref>) have argued that for a finite sized data with no a priori probabilistic information, the ratio of training sample size to the dimensionality must be as large as possible. Our results are consistent with these studies.
Reference: [157] <author> Truxton K. Fulton, Simon Kasif, and Steven Salzberg. </author> <title> An efficient algorithm for for finding multi-way splits for decision trees. </title> <note> In ML-95 [333]. to appear. </note>
Reference-contexts: In contrast, "global" discretization methods simultaneously convert all continuous attributes [81]. Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature <ref> [135, 157] </ref>. Trees in which an internal node can have more than 2 children, have also been considered in the vector quantization literature [431].
Reference: [158] <author> G. M. Furnival. </author> <title> Regression by leaps and bounds. </title> <journal> Technometrics, </journal> <volume> 16(4) </volume> <pages> 499-511, </pages> <year> 1974. </year>
Reference: [159] <author> Michael R. Garey and Ronald L. Graham. </author> <title> Performance bounds on the splitting algorithm for binary testing. </title> <journal> Acta Informatica, 3(Fasc. </journal> 4):347-355, 1974. 
Reference-contexts: Breiman et al.'s CART system [44] more or less implemented Friedman's suggestions. Quinlan also considered the problem of missing attribute values [395]. 44 2.5.4 Improving on greedy induction Most tree induction systems use a greedy approach | trees are induced top-down, a node at a time. Several authors (e.g., <ref> [159, 405] </ref>) pointed out the inadequacy of greedy induction for difficult concepts. The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338]. <p> Zimmerman [516] considered the problem of building identification keys for complete classes of splits, given arbitrary class distributions. Garey and Graham <ref> [159] </ref> analyze the properties of recursive greedy splitting on the quality of trees induced from decision tables, and showed that greedy algorithms using information theoretic splitting criteria can be made to perform arbitrarily worse than the optimal.
Reference: [160] <author> Michael R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: a Guide to the theory of NP-Completeness. </title> <publisher> Freeman and Co., </publisher> <address> San Francisco, CA, </address> <year> 1979. </year>
Reference-contexts: answer questions such as is it possible to build optimal trees?, how good are particular heuristics (feature evaluation rules or pruning methods)? Most of these investigations are theoretical, though there have been a few recent empirical ones. 2.6.1 NP-completeness Several aspects of optimal tree construction are known to be intractable <ref> [160] </ref>. Hyafil and Rivest [215] proved that the problem of building optimal decision trees from decision tables, optimal in the sense of minimizing the expected number of tests required to classify an unknown sample is NP-Complete.
Reference: [161] <author> S. B. Gelfand and C. S. Ravishankar. </author> <title> A tree-structured piecewise-linear adaptive filter. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(6) </volume> <pages> 1907-1922, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Jordan and Jacobs [233] described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [427]. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in [432]. In information theory, Gelfand and Ravishanker <ref> [161] </ref> describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. [204, 202] used simulated annealing to find the best oblique split at each tree node.
Reference: [162] <author> Saul B. Gelfand, C. S. Ravishankar, and Edward J. Delp. </author> <title> An iterative growing and pruning algorithm for classification tree design. </title> <journal> IEEE Transaction on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(2) </volume> <pages> 163-174, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The feature evaluation criteria in this class measure separability, divergence or discrimination between classes. A popular distance measure is the Gini index of diversity, named after the Italian economist Corrado Gini (1884-1965). Gini index has been used for tree construction in statistics [44], pattern recognition <ref> [162] </ref> and sequential fault diagnosis [378]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [44, 351] as a remedy. <p> Obtaining the "right" sized trees may be important for several reasons, which depend on the size of the classification problem <ref> [162] </ref>. For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation 11 and gaining insight into the predictive and generalization structure of the data. <p> Crawford [99] analyzed Breiman et al.'s cross validation procedure, and pointed out that it has a large variance, especially for small training samples. He suggested a .632 bootstrap method 13 as an effective alternative. Gelfand et al. <ref> [162] </ref> claimed that the cross validation method is both inefficient and possibly ineffective in finding the optimally pruned tree. They suggested an efficient iterative tree growing and pruning algorithm that is guaranteed to converge.
Reference: [163] <editor> Edward S. Gelsema and Laveen N. Kanal, editors. </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <volume> volume 2. </volume> <publisher> Elsevier Science, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1986. </year>
Reference: [164] <editor> Edzard S. Gelsema and Laveen S. Kanal, editors. </editor> <title> Pattern Recognition in Practice IV: Multiple paradigms, Comparative studies and hybrid systems, </title> <booktitle> volume 16 of Machine Intelligence and Pattern Recognition. </booktitle> <editor> Series editors: Kanal, L. S. and Rozenfeld, A. </editor> <publisher> Elsevier, </publisher> <year> 1994. </year>
Reference: [165] <author> G. H. Gennari, Pat Langley, and Douglas Fisher. </author> <title> Models of incremental concept formation. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):11-62, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Work Not Covered: Work not covered includes automatic construction of hierarchical structures using data in which the categories of objects are not known (unsupervised learning), present in fields such as cluster analysis [127] and machine learning (e.g., <ref> [141, 165] </ref>). A body of work using decision trees as a representational paradigm, existing in fields such as programming languages and analysis of algorithms, is not included.
Reference: [166] <author> Ian P. Gent and Toby Walsh. </author> <title> An empirical analysis of search in GSAT. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 47-59, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The experiment in Section 3.4.4 was intended to serve this purpose, but several alternate experiments can be designed. Interesting analyses of search methods, though not in the context of decision tree induction, can be found in <ref> [239, 166, 434] </ref>. 122 Chapter 4 Limited lookahead search The standard algorithm for constructing decision trees from a set of examples is greedy induction | a tree is induced top-down with locally optimal choices made at each node, without lookahead or backup.
Reference: [167] <author> Allen Gersho and Robert M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The testing algorithms normally take the form of decision trees or AND/OR trees [488, 378]. Many heuristics used to construct decision trees are used for test sequencing also. Vector quantization (VQ) <ref> [167] </ref> is a data compression technique that has proved useful for image coding. Tree structured vector quantizers (TSVQ) [65] are structures very similar to decision trees. A lot of work exists in the speech and signal processing literature, 19 on building and analyzing TSVQs.
Reference: [168] <author> Casimiro Giampaolo, Andrew T. Gray, Richard A. Olshen, and Sandor Szabo. </author> <title> Predicting chemically induced duodenal ulcer and adrenal necrosis with classification trees. </title> <booktitle> Proceedings of the National Academy of Sciences of the USA, </booktitle> <volume> 88(14) </volume> <pages> 6298-6302, </pages> <month> July </month> <year> 1991. </year> <month> 262 </month>
Reference: [169] <author> W. J. Gibb, D. M. Auslander, and J. C. Griffin. </author> <title> Selection of myocardial electrogram features for use by implantable devices. </title> <journal> IEEE Transactions on Biomedical Engineering, </journal> <volume> 40(8) </volume> <pages> 727-735, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Use of neural trees for ultraviolet stellar spectral classification is described in [183]. * Biomedical Engineering: Use of decision trees for identifying features to be used in implantable devices can be found in <ref> [169] </ref>. * Control Systems: Automatic induction of decision trees was recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding
Reference: [170] <author> M. W. Gillo. MAID: </author> <title> A Honeywell 600 program for an automatised survey analysis. </title> <booktitle> Behavioral Science, </booktitle> <volume> 17 </volume> <pages> 251-252, </pages> <year> 1972. </year>
Reference-contexts: For a review of earlier statistical work on hierarchical classification, see [139]. Statistical programs such as AID [454], MAID <ref> [170] </ref>, THAID [339] and CHAID [240] built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables.
Reference: [171] <author> Elizabeth A. Giplin, Richard A. Olshen, Kanu Chatterjee, John Kjek-shus, Arthur J. Moss, Harmut Henning, Robert Engler, A. Robert Blacky, Howard Dittrich, and John Ross Jr. </author> <title> Predicting 1-year outcome following acute myocardial infarction. </title> <journal> Computers and biomedical research, </journal> <volume> 23(1) </volume> <pages> 46-63, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Their analysis indicated that there was not much difference between both methods, and that neither method performed very well in its "vanilla" state. The performance of decision trees improved in [50] when multivariate splits were used, and backpropagation networks did better with feature selection. Giplin et al. <ref> [171] </ref> compared stepwise linear discriminant analysis, stepwise logistic regression and CART [44] to three senior cardiologists, for predicting the problem of predicting whether a patient would die within a year of being discharged after an acute myocardial infarction.
Reference: [172] <author> Malcolm A. Gleser and Morris F. Collen. </author> <title> Towards automated medical decisions. </title> <journal> Computers and Biomedical Research, </journal> <volume> 5(2) </volume> <pages> 180-189, </pages> <month> April </month> <year> 1972. </year>
Reference-contexts: Shannon's entropy [439] possesses all of these properties [2]. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition <ref> [172, 437, 465] </ref>. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [197, 493, 70, 192], in sequential fault diagnosis [488] and in machine learning [391]. <p> All dependence-bassed measures can be interpreted as belonging to one of the above two categories [23]. There exist several attribute selection criteria that do not clearly belong to any category in Ben-Basset's taxonomy. Gleser and Collen <ref> [172] </ref> and Talmon [465] used a combination of mutual information and 2 measures. <p> The former alternative is used in <ref> [172, 413, 390, 312] </ref> and the latter in [437]. A problem with the former method is that the value of most splitting criteria (Section 2.3.1) varies with the size of the training sample.
Reference: [173] <author> M. Golea and M. Marchand. </author> <title> A growth algorithm for neural network decision trees. </title> <journal> EuroPhysics Letters, </journal> <volume> 12(3) </volume> <pages> 205-210, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks.
Reference: [174] <author> Rodney M. Goodman and Padhraic J. Smyth. </author> <title> Decision tree design from a communication theory standpoint. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34(5) </volume> <pages> 979-994, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Taylor and Silverman [470] pointed out that the Gini index emphasizes equal sized offspring and purity of both children. They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth <ref> [174] </ref> report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic [21, 195, 323, 515, 503] are some other distance-based measures that have <p> Blum and Rivest [32] showed that the problem of constructing an optimal 3-node neural network is NP-complete. Goodrich [176] proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 2.6.2 Other analytical results Goodman and Smyth <ref> [174] </ref> showed that greedy top-down induction of decision trees is directly equivalent to a form of Shannon-Fano prefix coding [131]. <p> Chou and Gray [84] view decision trees as variable-length encoder-decoder pairs, and show that rate is equivalent to tree depth while distortion is the probability of misclassification. Goodman and Smyth <ref> [174] </ref> establish the equivalence between decision tree induction and a form of Shannon-Fano prefix coding, and show that this comparison leads to several interesting insights. Brandman et al. [37] suggested a universal technique to lower bound the size and other characteristics of decision trees for arbitrary Boolean functions. <p> Loveland [296] analyzed the performance of variants of Gini index in the context of sequential fault diagnosis. Goodman and Smyth <ref> [174, 175] </ref> analyzed mutual information based greedy tree induction from an information theoretic view point. They proved that mutual information-based induction is equivalent to a form of Shannon-Fano prefix coding, and through this insight argued that greedily induced trees are nearly optimal in terms of depth. <p> As the greedy approach can produce suboptimal trees <ref> [174] </ref>, it is naturally of interest to explore ways to improve the greedy strategy. Fixed-depth lookahead search is a standard technique for improving greedy algorithms [426]. Though scattered uses of lookahead exist in the literature (Section 2.5.4), there have not been any systematic evaluations (analytical or empirical). <p> It is known that all these methods are necessarily suboptimal <ref> [174] </ref>. It is desirable, particularly in view of the huge increase in the available computing power, to have techniques that can systematically bridge the gap between greedily induced trees and the optimal trees. Limited lookahead search is commonly believed to be one such technique. <p> The choice of a "best" test is what makes this algorithm greedy. The best test at a given internal node of the tree is only a locally optimal choice; and a strategy choosing locally optimal splits necessarily produces suboptimal trees <ref> [174] </ref>. Optimality of a decision tree may be measured in terms of prediction accuracy, size or depth. It should be clear that it is desirable to build optimal trees in terms of one or more of these criteria. <p> Randomization can also be beneficial for axis-parallel tree methods. Note that although greedy axis-parallel tree methods do find the optimal test (with respect to an impurity measure) for each node of a tree, they are necessarily suboptimal <ref> [174] </ref>. It will be interesting to explore the uses of randomization to build optimal or near-optimal axis-parallel trees. Randomized search techniques, such as genetic programming [264] and simulated annealing [55, 303] have already been used to improve axis-parallel decision trees.
Reference: [175] <author> Rodney M. Goodman and Padhriac J. Smyth. </author> <title> Decision tree design using information theory. </title> <journal> Knowledge Acquisition, </journal> <volume> 2 </volume> <pages> 1-19, </pages> <year> 1990. </year>
Reference-contexts: Loveland [296] analyzed the performance of variants of Gini index in the context of sequential fault diagnosis. Goodman and Smyth <ref> [174, 175] </ref> analyzed mutual information based greedy tree induction from an information theoretic view point. They proved that mutual information-based induction is equivalent to a form of Shannon-Fano prefix coding, and through this insight argued that greedily induced trees are nearly optimal in terms of depth.
Reference: [176] <author> Michael T. Goodrich, Vincent Mirelli, Mark Orletsky, and Jeffery Sa-lowe. </author> <title> Decision tree conctruction in fixed dimensions: Being global is hard but local greed is good. </title> <type> Technical Report TR-95-1, </type> <institution> Johns Hopkins University, Department of Computer Science, </institution> <address> Baltimore, MD 21218, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: This does not seem to be the case either . Blum and Rivest [32] showed that the problem of constructing an optimal 3-node neural network is NP-complete. Goodrich <ref> [176] </ref> proved that optimal (smallest) linear decision tree construction is NP-complete even in three dimensions. 2.6.2 Other analytical results Goodman and Smyth [174] showed that greedy top-down induction of decision trees is directly equivalent to a form of Shannon-Fano prefix coding [131]. <p> This can be done by using dynamic programming (e.g.: [318]) or branch and bound techniques (e.g.: [39]). But when the tree uses oblique splits, it is not clear, even for a fixed number of attributes, how to generate an optimal (e.g., smallest) decision tree in polynomial time. Goodrich <ref> [176] </ref> showed that the problem of inducing the smallest oblique decision tree is NP-hard even in three dimensions. This suggests that the complexity of constructing good oblique trees is greater than that for axis-parallel trees.
Reference: [177] <author> L. Gordon and R. A. Olshen. </author> <title> Asymptotically efficient solutions to the classification problem. </title> <journal> Annals of Statistics, </journal> <volume> 6(3) </volume> <pages> 515-533, </pages> <year> 1978. </year>
Reference-contexts: Trees of maximal size generated by the CART algorithm [44] have been shown to have an error rate bounded by twice the Bayes error rate, and to be asymptotically Bayes optimal <ref> [177] </ref>. Miyakawa [328] considered the problem of coverting decision tables to optimal trees, and studied the properties of optimal variables, the class of attributes only members of which can be used at the root of an optimal tree.
Reference: [178] <author> R. Graham and P. Hell. </author> <title> On the history of minimum spanning tree problem. </title> <journal> Annals of History of Computing, </journal> <volume> 7, </volume> <year> 1985. </year>
Reference-contexts: That is, it makes locally optimal choices in the hope that they will lead to a globally optimal solution. Greedy algorithms do not always yield optimal solutions, but for some problems they do. Examples of the latter variety include methods for constructing minimum spanning trees <ref> [178] </ref> and methods for producing optimal Huffman codes for data compression [283]. Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem [419], multiprocessor scheduling [210] and the problem under consideration here, decision tree induction. <p> The criteria we used to select this method from the many 226 alternatives were (1) MST-clustering is intuitive and easy to implement, and (2) it has been researched extensively, and shown to work well on a variety of distributions <ref> [178, 513, 232] </ref>. A minimum spanning tree of a weighted graph G is the minimum-weight connected acyclic subgraph G 0 of G containing all vertices of G. Many robust and efficient algorithms are available to compute MSTs.
Reference: [179] <author> N. A. B. Gray. </author> <title> Capturing knowledge through top-down induction of decision trees. </title> <journal> IEEE Expert, </journal> <volume> 5(3) </volume> <pages> 41-50, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Analysis: However, the data fragmentation caused by multi-stage hierarchical classifiers may compensate for the gain in accuracy. Michie [320] argues that top-down induction algorithms may provide overly complex classifiers that have no real conceptual structure in encoding relevant knowledge. As a solution to this problem, Gray <ref> [179] </ref> suggested an induction method that generates a single disjuncts of conjuncts rule, using the same time complexity as tree induction. The efficacy of multi-level decision trees is compared by Holte [209] to simple, one-level classification rules.
Reference: [180] <author> Robert M. Gray. </author> <title> Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-28, </pages> <month> April </month> <year> 1984. </year>
Reference: [181] <author> Gabriel Groner. </author> <title> Statistical Analysis of Adaptive Linear Classifiers. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1964. </year> <month> 263 </month>
Reference: [182] <author> A. Guenoche. </author> <title> Representation of classifications as trees. </title> <journal> RAIRO Recherche Opera-tionelle, </journal> <volume> 20(4) </volume> <pages> 341-353, </pages> <month> November </month> <year> 1986. </year>
Reference: [183] <author> R.K. Gulati, R. Gupta, P. Gothoskar, and S. Khobragade. </author> <title> Ultraviolet stellar spectral classification using multilevel tree neural networks. </title> <booktitle> Vistas in Astronomy, 38:293, 1993. Part 3: Neural Networks in Astronomy. </booktitle>
Reference-contexts: Decision trees have helped in star-galaxy classification [500], determining galaxy counts [499] and discovering quasars [244] in the Second Palomar Sky Survey. Use of neural trees for ultraviolet stellar spectral classification is described in <ref> [183] </ref>. * Biomedical Engineering: Use of decision trees for identifying features to be used in implantable devices can be found in [169]. * Control Systems: Automatic induction of decision trees was recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the
Reference: [184] <author> Heng Guo and Saul B. Gelfand. </author> <title> Classification trees with neural network feature extraction. </title> <journal> IEEE Transactions on Neural Networks., </journal> <volume> 3(6) </volume> <pages> 923-933, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: An extension of entropy nets, that 32 converts linear decision trees into neural nets was described in [374]. Decision trees with small multilayer networks at each node, implementing nonlinear, multivariate splits, were described in <ref> [184] </ref>. Jordan and Jacobs [233] described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [427]. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in [432].
Reference: [185] <author> Y. Guo and K.J. Dooley. </author> <title> Distinguishing between mean, variance and autocorrelation changes in statistical quality control. </title> <journal> International Journal of Production Research, </journal> <volume> 33(2) </volume> <pages> 497-510, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control <ref> [185] </ref>. For a recent review of the use of machine learning (decision trees and other techniques) in scheduling, see [14]. * Medicine: Medical research and practice have long been important areas of application for decision tree techniques.
Reference: [186] <author> R. Gupta, S.A. Smolka, and S. Bhaskar. </author> <title> On randomization in sequential and distributed algorithms. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(1) </volume> <pages> 7-86, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: It is natural to ask why randomization helps OC1 in the task of inducing decision trees. Researchers in combinatorial optimization have observed that randomized search usually succeeds when the search space holds an abundance of good solutions <ref> [186] </ref>. Furthermore, randomization can improve upon deterministic search when many of the local maxima in a search space lead to poor solutions. In OC1's search space, a local maximum is a hyperplane that cannot be improved by the deterministic search procedure, and a "solution" is a complete decision tree.
Reference: [187] <author> Ouzden Guur-Ali and William A. Wallace. </author> <title> Induction of rules subject to a quality constraint: Probabilistic inductive learning. </title> <journal> IEEE Transactions on Knowldge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 979-984, </pages> <month> December </month> <year> 1993. </year> <note> Special Issue on Learning and Discovery in Knowledge-based databases. </note>
Reference-contexts: An approach, which refines the class probability estimates in a greedily induced decision tree using local kernel density estimates has been suggested recently in [453]. Assignment of probabilistic goodness to splits in a decision tree is described in <ref> [187] </ref>.
Reference: [188] <author> S.E. Hampson and D.J. Volper. </author> <title> Linear function neurons: Structure and training. </title> <journal> Biological Cybernetics, </journal> <volume> 53(4) </volume> <pages> 203-217, </pages> <year> 1986. </year>
Reference-contexts: This algorithm uses heuristic hill climbing and backward feature elimination to find good linear combinations at each node. Murthy et al. [350, 351] described significant extensions to CART's linear combinations algorithm, using randomized techniques. (See Chapter 3) Perceptron Learning: A perceptron is a linear function neuron <ref> [326, 188] </ref> which can be trained to optimize the sum of distances of the misclassified objects to it, using a convergent procedure for adjusting its coefficients. Perceptron trees, which are decision trees with perceptrons just above the leaf nodes, were discussed in [480].
Reference: [189] <author> Thomas R. Hancock. </author> <title> Learning k decision trees on the uniform distribution. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 352-360, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: They also gave a PAC-learning algorithm for general polynomial size decision trees in time O (n O (log n) ). Kushilevitz and Mansour [273] gave a polynomial time PAC-learning algorithm with membership queries for decision trees under the uniform distribution. Hancock <ref> [189] </ref> gave a polynomial time algorithm for PAC-learning read-k decision trees. Bshouty [54] showed that decision trees are learnable under the model of exact learning with membership queries and unrestricted 55 equivalence queries. Recently, agnostic PAC-learning [13] and pruning [205] have been studied by the learnability theory community.
Reference: [190] <author> D. J. </author> <title> Hand. Discrimination and Classification. </title> <publisher> Wiley, </publisher> <address> Chichester, UK, </address> <year> 1981. </year>
Reference-contexts: There are several reasons why one might construct a decision tree from data, such as concise data description, discrimination or classification. Discrimination is the process of deriving classification rules from samples of classified objects, and classification is applying the rules to new objects of unknown class <ref> [190] </ref>.
Reference: [191] <editor> D. J. Hand, editor. </editor> <booktitle> Artificial Intelligence Frontiers in Statistics III. </booktitle> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference: [192] <author> W. Hanisch. </author> <title> Design and optimization of a hierarchical classifier. </title> <journal> Journal of new Generation Computer Systems, </journal> <volume> 3(2) </volume> <pages> 159-173, </pages> <year> 1990. </year>
Reference-contexts: entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition <ref> [197, 493, 70, 192] </ref>, in sequential fault diagnosis [488] and in machine learning [391]. Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop.
Reference: [193] <author> L. K. Hansen and P. Salomon. </author> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001, </pages> <year> 1990. </year>
Reference-contexts: Incremental induction of decision trees is considered by several authors. Friedman's binary tree induction method [152] could use "adaptive" fea 17 A lot of work exists in the neural networks literature on using committees or ensembles of networks to improve classification performance. See <ref> [193] </ref> for example. An alternative to multiple trees is a hybrid classifier that uses several small classifiers as parts of a larger classifier.
Reference: [194] <author> D. Harrison and D.L. Rubinfeld. </author> <title> Hedonic prices and the demand for clean air. </title> <journal> Journal of Environmental Economics and Management, </journal> <volume> 5 </volume> <pages> 81-102, </pages> <year> 1978. </year>
Reference-contexts: The first line for each method gives accuracies, and the second line gives average tree sizes. attributes and 1 binary attribute <ref> [194] </ref>. The category variable (median value of owner-occupied homes) is actually continuous, but we discretized it so that category = 1 if value &lt; $21000, and 2 otherwise. For other uses of this data, see [20, 399]. Diabetes diagnosis.
Reference: [195] <author> A. Hart. </author> <title> Experience in the use of an inductive system in knowledge engineering. </title> <editor> In M. Bramer, editor, </editor> <booktitle> Research and Development in Expert Systems. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year> <month> 264 </month>
Reference-contexts: emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic <ref> [21, 195, 323, 515, 503] </ref> are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> Fayyad and Irani [133] showed that their measure C-SEP, performs better than Gini index [44] and information gain [391] for specific types of problems. 27 Several researchers <ref> [195, 391] </ref> pointed out that information gain is biased towards attributes with a large number of possible values. Mingers [323] compared information gain and the 2 statistic for growing the tree as well as for stop-splitting.
Reference: [196] <author> P. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14(3), </volume> <month> May </month> <year> 1968. </year>
Reference: [197] <author> Carlos R. P. Hartmann, Pramod K. Varshney, Kishan G. Mehrotra, and Carl L. Gerberich. </author> <title> Application of information theory to the construction of efficient decision trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-28(4):565-577, </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition <ref> [197, 493, 70, 192] </ref>, in sequential fault diagnosis [488] and in machine learning [391]. Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop. <p> The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338]. Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition <ref> [197] </ref>, for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning [365, 403, 354]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: [198] <author> R. E. Haskell and A. Noui-Mehidi. </author> <title> Design of hierarchical classifiers. </title> <editor> In N. A. Sherwani, E. de Doncker, and J. A. Kapenga, editors, </editor> <booktitle> Computing in the 90's: The First Great Lakes Computer Science Conference Proceedings, </booktitle> <pages> pages 118-124, </pages> <address> Berlin, </address> <year> 1991. </year> <note> Springer-Verlag. Conference held in Kalamazoo, MI on 18th-20th, </note> <month> October </month> <year> 1989. </year>
Reference-contexts: criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance <ref> [152, 413, 198] </ref> and the 2 statistic [21, 195, 323, 515, 503] are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures.
Reference: [199] <author> N.D. Hatziargyriou, </author> <title> G.C. Contaxis, and N.C. Sideris. A decision tree method for on-line steady state security assessment. </title> <journal> IEEE Transactions on Power Systems, </journal> <volume> 9(2):1052, </volume> <year> 1994. </year>
Reference-contexts: tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality to pine trees [19]. * Power systems: Power system security assessment <ref> [199] </ref> and power stability prediction [414] are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see [464, 247] ).
Reference: [200] <author> Patrick Hayes and Kenneth Ford. </author> <title> Turing test considered harmful. </title> <booktitle> In IJCAI-95 [223], </booktitle> <pages> pages 972-977. </pages> <note> Invited talk. </note>
Reference-contexts: Nevertheless, this thesis is based on the belief that biological justification is neither necessary nor sufficient for a successful machine learning system. We believe that an attempt to evaluate aircrafts by comparing them with birds is futile <ref> [200] </ref>. Throughout this thesis, machine learning techniques are viewed as helpful tools, and not as techniques that attempt to emulate natural intelligence. 1.2 Overview of the thesis This thesis can be roughly divided into three parts.
Reference: [201] <author> Mark A. Heap and M. R. Mercer. </author> <title> Least upper bounds on OBDD sizes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 764-767, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Recently, agnostic PAC-learning [13] and pruning [205] have been studied by the learnability theory community. In the context of ordered binary decision diagrams (OBDD), the bounds on the tree size have been investigated, as a function of the tree compaction operators and the specific Boolean functions being represented (eg., <ref> [315, 457, 201] </ref>). 2.6.3 Tools Some authors pointed out the similarity or equivalence between the problem of constructing decision trees and existing, seemingly unrelated, problems. Such view points provide valuable tools for analyzing decision trees.
Reference: [202] <author> D. Heath. </author> <title> A Geometric Framework for Machine Learning. </title> <type> PhD thesis, </type> <institution> Johns Hop-kins University, Baltimore, MD, </institution> <year> 1992. </year>
Reference-contexts: Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in [432]. In information theory, Gelfand and Ravishanker [161] describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. <ref> [204, 202] </ref> used simulated annealing to find the best oblique split at each tree node. Lubinsky [299, 298] attempted bivariate trees, trees in which some functions of two variables can be used as tests at internal nodes. <p> n d-dimensional vectors in at most 2 fl P d (n1) ways if n &gt; d + 1 and 2 n ways if n d + 1, and for any given n and d, one can find a set of vectors for which this bound is achieved ([473]). 19 Heath <ref> [202] </ref> proved that the problem of finding the split that minimizes the number of misclassified points, given two sets of mutually exclusive points, is NP-complete. <p> Odewahn et al. (1992) reported accuracy of 99.8% accuracy on the bright objects, and 92.0% on the dim ones, although it should be noted that this study used a single training and test set partition. Heath <ref> [202] </ref> reported 99.0% accuracy on the bright objects using SADT, with an average tree size of 7.03 leaves. This study also used a single training and test set.
Reference: [203] <author> D. Heath, S. Kasif, and S. Salzberg. k-DT: </author> <title> A multi-tree learning method. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pages 138-149, </pages> <address> Harpers Ferry, WV, </address> <year> 1993. </year> <institution> George Mason University. </institution>
Reference-contexts: A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [274, 443, 444, 62, 203] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree [443, 444]. <p> The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness <ref> [203] </ref> or using different subsets of attributes for each tree [443, 444]. Classification results of the trees have been combined using either simplistic voting methods [203] or using statistical methods for combining evidence [443]. 2.5.8 Incremental tree induction Most tree induction algorithms use batch training | the entire tree needs to <p> a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness <ref> [203] </ref> or using different subsets of attributes for each tree [443, 444]. Classification results of the trees have been combined using either simplistic voting methods [203] or using statistical methods for combining evidence [443]. 2.5.8 Incremental tree induction Most tree induction algorithms use batch training | the entire tree needs to be recomputed to accommodate a new training example.
Reference: [204] <author> D. Heath, S. Kasif, and S. Salzberg. </author> <title> Learning oblique decision trees. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 1002-1007. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Another measure suggested by Heath et al., called the sum of impurities, assigns an integer to each class and measures the variance between class numbers in each partition <ref> [204, 351] </ref>. 9 Quinlan's C4.5 [398] uses a naive version of the confidence intervals for doing pessimistic pruning. 25 An almost identical measure was used earlier in the Automatic Interaction Detection (AID) program [139]. <p> Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in [432]. In information theory, Gelfand and Ravishanker [161] describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. <ref> [204, 202] </ref> used simulated annealing to find the best oblique split at each tree node. Lubinsky [299, 298] attempted bivariate trees, trees in which some functions of two variables can be used as tests at internal nodes. <p> Note that the impurity of OC1's perturbations is monotonically decreasing unlike that of CART-LC. 79 SADT: A third system that creates oblique trees is Simulated Annealing of Decision Trees (SADT) <ref> [204] </ref> which, like OC1, uses randomization. SADT uses simulated annealing [252] to find good values for the coefficients of the hyperplane at each node of a tree. SADT first places a hyperplane in a canonical location, and then iteratively perturbs all the coefficients by small random amounts. <p> Because there are an exponential number of distinct ways to partition the examples with a hyperplane, any procedure that simply enumerates all of them will be unreasonably costly. The two main alternatives considered in the past have been simulated annealing, used in the SADT system <ref> [204] </ref>, and deterministic heuristic search, as in CART-LC [44]. <p> The TwoingValue is actually a goodness measure rather than an impurity measure. Therefore OC1 attempts to minimize the reciprocal of this value. 4. Max Minority: The measures Max Minority, Sum Minority and Sum Of Variances were defined in the context of decision trees by Heath, Kasif, and Salzberg <ref> [204] </ref>. Max Minority has the theoretical advantage that a tree built minimizing this measure will have depth at most log n. Our experiments indicated that this is not a great advantage in practice: seldom do other impurity measures produce trees substantially deeper than those produced with Max Minority. <p> This allows us to quantify more precisely how the parameters of our algorithm affect its performance. A second purpose of this experiment is to compare OC1's search strategy with that of two existing oblique decision tree induction systems - LMDT [48] and SADT <ref> [204] </ref>. We show that the quality of trees induced by OC1 is as good as, if not better than, that of the trees induced by these existing systems on three artificial domains.
Reference: [205] <author> David P. Helmhold and Robert E. Schapire. </author> <title> Predicting nearly as well as the best pruning of a decision tree. </title> <booktitle> In Proceedings of the 8th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 61-68, </pages> <address> New York, NY, 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Hancock [189] gave a polynomial time algorithm for PAC-learning read-k decision trees. Bshouty [54] showed that decision trees are learnable under the model of exact learning with membership queries and unrestricted 55 equivalence queries. Recently, agnostic PAC-learning [13] and pruning <ref> [205] </ref> have been studied by the learnability theory community.
Reference: [206] <author> Ernest G. Henrichon Jr. and King-Sun Fu. </author> <title> A nonparametric partitioning procedure for pattern classification. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-18(7):614-624, </volume> <month> July </month> <year> 1969. </year>
Reference-contexts: Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features. In the decision tree literature, Henrichon and Fu <ref> [206] </ref> were probably the first to discuss "transgenerated" features, features generated from the original attributes. Friedman's tree induction method [152] could consider with equal ease atomic and composite features. Techniques to search for multivariate splits (Section 2.3.2) can be seen as ways for constructing composite features.
Reference: [207] <author> Gabor T. Herman and K.T. Daniel Yeung. </author> <title> On piecewise-linear classification. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(7) </volume> <pages> 782-786, </pages> <month> July </month> <year> 1992. </year> <month> 265 </month>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks. <p> Heath et al.[204] reported 94.9% accuracy on a subset of this data set (it then had only 470 instances), with an average decision tree size of 4.6 nodes, using SADT. Salzberg [421] reported 96.0% accuracy using 1-NN on the same (smaller) data set. Herman and Yeung <ref> [207] </ref> reported 99.0% accuracy using piece-wise linear classification, again using a somewhat smaller data set. Bennett and Mangasarian [25] reported 97.4% accuracy using their MSM1 algorithm, using a different experimentation method from the one we employ. Classifying Irises.
Reference: [208] <author> Klaus-U Hoeffgen, Hans-U Simon, and Kevin S. Van Horn. </author> <title> Robust trainability of single neurons. </title> <journal> Journal of Computer System Sciences, </journal> <volume> 50(1) </volume> <pages> 114-125, </pages> <year> 1995. </year>
Reference-contexts: Hoeffgen et al. <ref> [208] </ref> proved that a more general problem is NP-hard | they proved that, for any C 1, the problem of finding a hyperplane that misclassifies no more than C fl opt examples, where opt is the minimum number of misclassifications possible using a hyperplane, is also NP-hard.
Reference: [209] <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: As a solution to this problem, Gray [179] suggested an induction method that generates a single disjuncts of conjuncts rule, using the same time complexity as tree induction. The efficacy of multi-level decision trees is compared by Holte <ref> [209] </ref> to simple, one-level classification rules. He concluded that, on most real world data sets commonly used by the machine learning community [346], decision trees do not perform significantly better than one level rules. These conclusions, however, were refuted by Elomaa [123] on several grounds. <p> It is known that many of the commonly-used data sets from the UCI repository are easy to learn with very simple representations <ref> [209] </ref>; therefore those data sets may not be ideal for our purposes. Thus we created a number of artificial data sets that present different 109 problems for learning, and for which we know the "correct" concept definition. <p> It has been observed that most of the data sets in the UCI repository can be described by very simple classification rules <ref> [209] </ref>. So, we needed to be careful in our choice of real world domains. We used a survey of results on several UCI data sets provided by Holte [209] to choose six "difficult" domains domains for which the best known accuracy is at most 90%. <p> It has been observed that most of the data sets in the UCI repository can be described by very simple classification rules <ref> [209] </ref>. So, we needed to be careful in our choice of real world domains. We used a survey of results on several UCI data sets provided by Holte [209] to choose six "difficult" domains domains for which the best known accuracy is at most 90%. <p> This data is used by Norton [365] in his experiments. The data contains 435 instances, each described by 16 nominal attributes and one class label. The task is to classify democrats from republicans on the basis of their voting records. V1 This data, used in <ref> [59, 209] </ref>, is identical to the VO data, except that the "best" attribute physician-fee freeze is removed. All the experimental results reported in this section are obtained with information gain. The experiments with Gini index did not offer any more insights, and are omitted for brevity.
Reference: [210] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1984. </year>
Reference-contexts: Examples of the latter variety include methods for constructing minimum spanning trees [178] and methods for producing optimal Huffman codes for data compression [283]. Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem [419], multiprocessor scheduling <ref> [210] </ref> and the problem under consideration here, decision tree induction. For these problems, greedy polynomial algorithms obviously can not guarantee optimal solutions, assuming P 6= N P .
Reference: [211] <author> A. S. Houston, R. J. Iorns, and M. A. Macleod. </author> <title> The use of thyroid function studies. </title> <journal> Nuclear Medicine Communications, </journal> <volume> 12 </volume> <pages> 497-506, </pages> <year> 1991. </year>
Reference: [212] <author> G. E. Hughes. </author> <title> On the mean accuracy of statistical pattern recognition. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14(1):55-63, </volume> <month> January </month> <year> 1968. </year>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: [213] <author> K. J. Hunt. </author> <title> Classification by induction: Applications to modelling and control of non-linear dynamic systems. </title> <journal> Intelligent Systems Engineering, </journal> <volume> 2(4) </volume> <pages> 231-245, </pages> <month> Winter </month> <year> 1993. </year>
Reference-contexts: neural trees for ultraviolet stellar spectral classification is described in [183]. * Biomedical Engineering: Use of decision trees for identifying features to be used in implantable devices can be found in [169]. * Control Systems: Automatic induction of decision trees was recently used for con trol of nonlinear dynamical systems <ref> [213] </ref>. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure
Reference: [214] <author> D. Hunter, S. Faber, R. </author> <title> Light, </title> <editor> and E. Shaya. Stellar photometry and zero-points. In S. Faber, editor, </editor> <title> Final Orbital/Science Verification Report, </title> <year> 1991. </year>
Reference-contexts: As an external check on the completeness of this CR catalogue, we found the surface density of the CRs and compared it to the values recently determined in [505]. The Final Orbital/Science Verification Report by the WF/PC-1 Investigation Definition Team lists zeropoint offsets for the M81 field for WF2 <ref> [214] </ref>. Consequently, we used the same CCD for our CR surface density determination. We defined a cosmic ray to be a single pixel in the CR image with a threshold of 4:0 fi local in ADU flux. <p> In order to perform the aperture correction, we used a theoretical PSF generated by TinyTim for WF2. From this we found that ~ 14:3% of the light falls 242 within a 2.5 pixel radius. We ignored color corrections as was done in <ref> [214] </ref>, but we did perform aperture corrections. Comparing our F555W V magnitudes to the V magnitudes listed by the IDT, we confirmed the zeropoint magnitude of 23.0 to within 0.1 magnitudes (or 10%), and used this number as our offset.
Reference: [215] <author> Laurent Hyafil and Ronald L. Rivest. </author> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5(1) </volume> <pages> 15-17, </pages> <year> 1976. </year>
Reference-contexts: Hyafil and Rivest <ref> [215] </ref> proved that the problem of building optimal decision trees from decision tables, optimal in the sense of minimizing the expected number of tests required to classify an unknown sample is NP-Complete.
Reference: [216] <author> Toshihide Ibaraki and Saburo Muroga. </author> <title> Adaptive linear classifiers by linear programming. </title> <type> Technical Report 284, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, </institution> <year> 1968. </year>
Reference-contexts: Perceptron trees, which are decision trees with perceptrons just above the leaf nodes, were discussed in [480]. Decision trees with perceptrons at all internal nodes were described in [482, 438]. Mathematical Programming: Linear programming has been used for building adaptive classifiers since late 1960s <ref> [216] </ref>. Given two possibly interesecting sets of points, Duda 31 and Hart [117] proposed a linear programming formulation for finding the split whose distance from the misclassified points is minimized.
Reference: [217] <author> M. Ichino and Jack Sklansky. </author> <title> Optimum feature selection by zero-one integer programming. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-14:737-746, </volume> <month> September/October </month> <year> 1984. </year>
Reference-contexts: A combination of forward selection and backward elimination, a bidirectional search, was attempted in [445]. 15 An exception is the optimal feature subset selection method using zero-one integer programming, suggested by Ichino and Sklansky <ref> [217] </ref>. 41 Comparisons of heuristic feature subset selection methods resound the conclusions of studies comparing feature evaluation criteria and studies comparing pruning methods | no feature subset selection heuristic is far superior to the others.
Reference: [218] <author> Y. Iikura and Y. Yasuoka. </author> <title> Utilization of a best linear discriminant function for designing the binary decision tree. </title> <journal> International Journal of Remote Sensing, </journal> <volume> 12(1) </volume> <pages> 55-67, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Use of linear discriminants in a decision tree is considered in the remote sensing literature in <ref> [218] </ref>. A method for building linear discriminant classification trees, in which the user can decide at each node what classes need to be split, is described in [472]. John [229] recently considered linear discriminant trees in the machine learning literature.
Reference: [219] <editor> IJCAI-89: </editor> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA, </address> <year> 1989. </year> <editor> Editor: N. S. </editor> <publisher> Sridharan. </publisher>
Reference: [220] <editor> IJCAI-91: </editor> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, volume 2, </booktitle> <address> Darling Harbour, Sydney, Australia, 24-30th, August 1991. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, </address> <publisher> CA. </publisher> <editor> Editors: John Mylopoulos and Ray Reiter. </editor> <volume> 266 </volume>
Reference: [221] <editor> IJCAI-93: </editor> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, volume 2, </booktitle> <address> Chambery, France, 28th August-3rd September 1993. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address> <publisher> Editor: Ruzena Bajcsy. </publisher>
Reference: [222] <institution> Data Engineering for Inductive Learning, </institution> <address> Montreal, Canada, 16th-21st, August 1995. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address> <note> Workshop organized by Peter Turney. http://ai.iit.nrc.ca/DEIL/. </note>
Reference-contexts: The most detailed discussion about data massaging that the author is 179 aware of is from two recent workshop proceedings <ref> [222, 332] </ref>. Both these forums discussed the problem of structuring, or engineering the data into a form suitable for inductive learning. Each paper at these two workshops, like the current chapter, argues data massaging mostly relies on domain knowledge.
Reference: [223] <editor> IJCAI-95: </editor> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, 16th-21st, August 1995. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address> <publisher> Editor: Chris Mellish. </publisher>
Reference: [224] <author> I. F. Imam and Ryszard S. Michalski. </author> <title> Should decision trees be learned from examples or from decision rules? In Methodologies for Intelligent Systems: </title> <booktitle> 7th International Symposium. ISMIS '93, volume 689 of Lecture Notes in Computer Science, </booktitle> <pages> pages 395-404. </pages> <publisher> Springer-Verlag, </publisher> <address> Trondheim, Norway, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: As mentioned in Section 2.1, these methods are beyond the scope of the current chapter. 50 We concentrate in this chapter on decision trees that are constructed from labelled examples. The problem of learning trees from decision rules instead of examples is addressed in <ref> [224] </ref>. The problem of learning trees solely from prior probability distributions is considered in [10]. Learning decision trees from qualitative causal models acquired from domain experts is the topic of [382]. Several attempts at generalizing the decision tree representation exist.
Reference: [225] <author> Keki B. Irani, Cheng Jie, Usama M. Fayyad, and Qian Zhaogang. </author> <title> Applying machine learning to semiconductor manufacturing. </title> <journal> IEEE Expert, </journal> <volume> 8(1) </volume> <pages> 41-47, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: of decision trees was recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing <ref> [225] </ref>, for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [226] <author> P. Israel and C. Koutsougeras. </author> <title> A hybrid electro-optical architecture for classification trees and associative memory mechanisms. </title> <booktitle> International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), </booktitle> <volume> 2(3) </volume> <pages> 373-393, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Issues in preprocessing data to be in a form suitable to decision tree induction are discussed in some detail in [475]. Parallelization of tree induction algorithms is considered in [381]. Hardware architectures to implement decision trees are described in <ref> [226] </ref>. 2.6 Analyses Several researchers have tried to evaluate the tree induction method itself, to precisely answer questions such as is it possible to build optimal trees?, how good are particular heuristics (feature evaluation rules or pruning methods)? Most of these investigations are theoretical, though there have been a few recent
Reference: [227] <author> A. K. Jain and B. Chandrasekaran. </author> <title> Dimensionality and sample size considerations in pattern recognition. </title> <booktitle> In Krishnaiah and Kanal [265], </booktitle> <pages> pages 835-855. </pages>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: [228] <author> Mike James. </author> <title> Classification Algorithms. </title> <publisher> Wiley-Interscience Publications, </publisher> <year> 1985. </year>
Reference: [229] <author> George H. John. </author> <title> Robust linear discriminant trees. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 285-291. </pages>
Reference-contexts: Use of linear discriminants in a decision tree is considered in the remote sensing literature in [218]. A method for building linear discriminant classification trees, in which the user can decide at each node what classes need to be split, is described in [472]. John <ref> [229] </ref> recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [364], which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes [49, 115].
Reference: [230] <author> George H. John, Ron Kohavi, and Karl Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In ML-94 [331], </booktitle> <pages> pages 121-129. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: Feature subsets have been compared in the literature using either a feature evaluation criterion discussed in Section 2.3.1 (e.g. Bhattacharya distance was used for comparing subsets of features in [358]), or using direct error estimation <ref> [148, 230] </ref>. The second component of feature subset selection methods is a search algorithm through the space of possible feature subsets. Most existing search procedures are heuristic in nature, 15 as exhaustive search for the best feature subset is typically prohibitively expensive. A heuristic commonly used is the greedy heuristic.
Reference: [231] <author> David S. Johnson, Christos H. Papadimitriou, and Mihalis Yannakakis. </author> <title> How easy is local search? Journal of Computer and System Sciences, </title> <booktitle> 37 </booktitle> <pages> 79-100, </pages> <year> 1988. </year>
Reference: [232] <author> S.C. Johnson. </author> <title> Hierarchical clustering schemes. </title> <journal> Psychometrika, </journal> <volume> 32(3), </volume> <month> September </month> <year> 1967. </year>
Reference-contexts: The criteria we used to select this method from the many 226 alternatives were (1) MST-clustering is intuitive and easy to implement, and (2) it has been researched extensively, and shown to work well on a variety of distributions <ref> [178, 513, 232] </ref>. A minimum spanning tree of a weighted graph G is the minimum-weight connected acyclic subgraph G 0 of G containing all vertices of G. Many robust and efficient algorithms are available to compute MSTs.
Reference: [233] <author> Michael I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year> <month> 267 </month>
Reference-contexts: An extension of entropy nets, that 32 converts linear decision trees into neural nets was described in [374]. Decision trees with small multilayer networks at each node, implementing nonlinear, multivariate splits, were described in [184]. Jordan and Jacobs <ref> [233] </ref> described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [427]. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in [432]. <p> C4.5 [398] uses a simple form of soft splitting (chapter 8). 46 Use of fuzzy splits in pattern recognition literature can be found in [432, 494]. Jordan and Jacobs <ref> [233] </ref> describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [146]. Induction of fuzzy decision trees has also been considered in [281, 512]. 2.5.6 Estimating probabilities Decision trees have crisp decisions at leaf nodes.
Reference: [234] <author> J. Judmaier, P. Meyersbach, G. Weiss, H. Wachter, and G. Reibnegger. </author> <title> The role of Neopterin in assessing disease activity in Crohn's disease: Classification and regression trees. </title> <journal> The American Journal of Gastroenterology, </journal> <volume> 88(5):706, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology [295, 129, 258], psychiatry [314], gastroenterology <ref> [234] </ref>, for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data exploration methods in molecular biology.
Reference: [235] <author> G. Kalkanis. </author> <title> The application of confidence interval error analysis to the design of decision tree classifiers. </title> <journal> Pattern Recognition Letters, </journal> <volume> 14(5) </volume> <pages> 355-361, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The computational requirements for computing activity are the same as those for the information-based measures. Quinlan and Rivest [402] suggested the use of Risannen's minimum description length [408] for deciding which splits to prefer over others and also for pruning. Kalkanis <ref> [235] </ref> pointed out that measures like information gain and Gini index are all concave (i.e., they never report a worse goodness value after trying a split than before splitting), so there is no natural way of assessing where to stop further expansion of a node. <p> Some feature evaluation rules, whose distribution does not depend on the number of training samples (i.e., a goodness value of k would have the same significance anywhere in the tree) have been suggested in the literature <ref> [286, 515, 235] </ref>. * Trees to rules conversion: Quinlan [393, 398] gave efficient procedures for converting a decision tree into a set of production rules.
Reference: [236] <editor> Laveen N. Kanal. </editor> <title> Patterns in pattern recognition: 1968-1974. </title> <journal> IEEE Transactions in Information Theory, </journal> <volume> 20 </volume> <pages> 697-722, </pages> <year> 1974. </year>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: [237] <editor> Laveen N. Kanal. </editor> <title> Problem solving methods and search strategies for pattern recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-1:193-201, </volume> <year> 1979. </year>
Reference-contexts: A univariate tree is also axis-parallel, and an oblique tree is the same as a linear tree. 2.2 High-level pointers A decision tree performs mutistage hierarchical decision making. For a general rationale for multistage classification schemes and a categorization of such schemes, see <ref> [237] </ref>. 2.2.1 Treatises The work on decision tree construction in statistics has its origins in methods for exploring survey data. For a review of earlier statistical work on hierarchical classification, see [139].
Reference: [238] <editor> Laveen N. Kanal and B. </editor> <title> Chandrasekaran. On dimensionality and sample size in statistical pattern classification. </title> <journal> Pattern Recognition, </journal> <volume> 3 </volume> <pages> 225-234, </pages> <year> 1971. </year>
Reference: [239] <author> Younkyung Cha Kang. </author> <title> Randomized Algorithms for Query Optimization. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, </institution> <month> October </month> <year> 1991. </year> <note> Computer Sciences Report #1053. </note>
Reference-contexts: The experiment in Section 3.4.4 was intended to serve this purpose, but several alternate experiments can be designed. Interesting analyses of search methods, though not in the context of decision tree induction, can be found in <ref> [239, 166, 434] </ref>. 122 Chapter 4 Limited lookahead search The standard algorithm for constructing decision trees from a set of examples is greedy induction | a tree is induced top-down with locally optimal choices made at each node, without lookahead or backup.
Reference: [240] <author> G. V. Kass. </author> <title> An exploratory technique for investigating large quantities of categorical data. </title> <journal> Applied Statistics, </journal> <volume> 29(2) </volume> <pages> 119-127, </pages> <year> 1980. </year>
Reference-contexts: For a review of earlier statistical work on hierarchical classification, see [139]. Statistical programs such as AID [454], MAID [170], THAID [339] and CHAID <ref> [240] </ref> built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables.
Reference: [241] <author> Leonard Kaufman and Peter J. Rousseeuw. </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis. </title> <publisher> Wiley-Interscience Publication, </publisher> <year> 1990. </year>
Reference-contexts: (EMST) clustering as a preprocessing step, on the CB, RCB and RGC domains and two real-world data sets taken from the UCI machine learning repository [346]. 7.3.1 Minimum Spanning Tree Clustering A host of unsupervised clustering methods have been developed in the fields of psychology, statistics and machine learning. (See <ref> [127, 241] </ref> for a tutorial.) The choice of which clustering technique to use for a given data set is often very difficult. Most techniques require the user to define the number of clusters in advance, and those that do not, often require tuning of various parameters.
Reference: [242] <author> Michael J. Kearns and Umesh Virkumar Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Computational Learning Theory is a young discipline that studies the "learn-ability" of specific concepts or concept classes. For a good introduction to the theory of learnability, see <ref> [242] </ref>. We summarize below significant learnability results for decision trees. Ehrenfeucht and Haussler [121] gave an algorithm for PAC-learning (without membership queries) decision trees of constant rank in polynomial time. They also gave a PAC-learning algorithm for general polynomial size decision trees in time O (n O (log n) ).
Reference: [243] <author> Davis M. Kennedy. </author> <title> Decision tree bears fruit. Products Finishing, </title> <address> 57(10):66, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity <ref> [243] </ref>, for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [244] <author> J. D. Kennefick, R. R. Carvalho, S. G. Djorgovski, M. M. Wilber, E. S. Dickson, N. Weir, U. Fayyad, and J. Roden. </author> <title> The discovery of five quasars at z &gt; 4 using the second Palomar Sky Survey. </title> <journal> The Astronomical Journal, </journal> <volume> 110(1):78, </volume> <year> 1995. </year>
Reference-contexts: Decision trees have helped in star-galaxy classification [500], determining galaxy counts [499] and discovering quasars <ref> [244] </ref> in the Second Palomar Sky Survey.
Reference: [245] <author> Randy Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In AAAI-92 [7], </booktitle> <pages> pages 123-128. </pages>
Reference-contexts: The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in <ref> [134, 245, 486, 343] </ref>. Methods of discretization that operate on a single continuous attribute at a time can be said to be "local" discretization methods. In contrast, "global" discretization methods simultaneously convert all continuous attributes [81]. <p> The "adoption" of these methods into numeric domains may be less than ideal, as numeric domains have their own peculiarities <ref> [485, 245, 486, 481] </ref>. In this chapter, we present a framework for augmenting decision tree induction so it can take advantage of patterns in numeric attribute spaces that would otherwise be ignored.
Reference: [246] <author> B. Khoshnevis and S. Parisay. </author> <title> Machine learning and simulation: Application in queuing systems. </title> <journal> Simulation, </journal> <volume> 61(5) </volume> <pages> 294-302, </pages> <year> 1993. </year> <month> 268 </month>
Reference: [247] <author> Byungyong Kim and David Landgrebe. </author> <title> Hierarchical decision tree classifiers in high-dimensional and large class data. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 29(4) </volume> <pages> 518-528, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Bottom up induction of trees is considered in [275]. Bottom up tree induction is also common [378] in problems such as building identification keys and optimal test sequences. 18 A hybrid approach to tree construction, that combined top-down and bottom-up induction can be found in <ref> [247] </ref>. 18 Hierarchical unsupervised clustering can construct, using bottom-up or top-down methods, tree-structured classifiers. As mentioned in Section 2.1, these methods are beyond the scope of the current chapter. 50 We concentrate in this chapter on decision trees that are constructed from labelled examples. <p> mortality to pine trees [19]. * Power systems: Power system security assessment [199] and power stability prediction [414] are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see <ref> [464, 247] </ref> ).
Reference: [248] <author> Hyunsoo Kim. </author> <title> PAC Learning: A Decision Tree with Pruning. </title> <type> PhD thesis, </type> <institution> University of Florida, </institution> <year> 1992. </year>
Reference: [249] <author> Hyunsoo Kim and G. J. Koehler. </author> <title> An investigation on the conditions of pruning an induced decision tree. </title> <journal> European Journal of Operational Research, </journal> <volume> 77(1):82, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Kim and Koehler <ref> [249] </ref> analytically investigate the conditions under which pruning is beneficial for accuracy. Their main result states pruning is more beneficial with increasing skewness in class distribution and/or increasing sample size.
Reference: [250] <author> Sung-Ho Kim. </author> <title> A general property among nested, pruned subtrees of a decision support tree. </title> <journal> Communications in Statistics|Theory and Methods, </journal> <volume> 23(4) </volume> <pages> 1227-1238, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, [107, 340] in pattern recognition and <ref> [250] </ref> in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478].
Reference: [251] <author> Kenji Kira and Larry A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92 [7], </booktitle> <pages> pages 129-134. </pages>
Reference-contexts: Some of these studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets. For examples of this work, see <ref> [251, 276, 69, 113, 336, 3] </ref>. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: [252] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt, and M.P. Vecci. Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220(4598) </volume> <pages> 671-680, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: Note that the impurity of OC1's perturbations is monotonically decreasing unlike that of CART-LC. 79 SADT: A third system that creates oblique trees is Simulated Annealing of Decision Trees (SADT) [204] which, like OC1, uses randomization. SADT uses simulated annealing <ref> [252] </ref> to find good values for the coefficients of the hyperplane at each node of a tree. SADT first places a hyperplane in a canonical location, and then iteratively perturbs all the coefficients by small random amounts.
Reference: [253] <author> Y. Kodratoff and M. </author> <title> Manago. Generalization and noise. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 181-204, </pages> <year> 1987. </year>
Reference-contexts: Effects of noise on generalization are discussed in <ref> [363, 253] </ref>. Overfitting avoidance as a specific bias is studied in [507, 428]. Effect of noise on classification tree construction methods is studied in the pattern recognition literature in [468]. Several techniques have been suggested for obtaining the right sized trees.
Reference: [254] <editor> Y. Kodratoff and R.S. Michalski, editors. </editor> <booktitle> Machine learning, and Artificial Intelligence approach, </booktitle> <volume> volume 3. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: [255] <author> Y. Kodratoff and S. Moscatelli. </author> <title> Machine learning for object recognition and scene analysis. </title> <journal> Internationa Journal of Pattern recognition and AI, </journal> <volume> 8(1) </volume> <pages> 259-304, </pages> <year> 1994. </year>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in [442] and [423]. * Object recognition: Tree based classification has been used recently for recognizing three dimensional objects [456, 57] and for high level vision <ref> [255] </ref>. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality to pine trees [19]. * Power systems:
Reference: [256] <author> Ron Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In IJCAI-95 [223], </booktitle> <pages> pages 1137-1143. </pages> <editor> Editor: </editor> <publisher> Chris Mellish. </publisher>
Reference-contexts: In cross validation, L is divided randomly into B mutually exclusive, equal sized partitions. Efron [120] showed that, although cross validation closely approximates the true result, bootstrap has much less variance, especially for small samples. However, there exist arguments that cross validation is clearly preferable to bootstrap in practice <ref> [256] </ref>. 38 coding method (which did not have an effect on their main conclusions) was pointed out in [491]. Forsyth et al. [149] recently suggested a pruning method that is based on viewing the decision tree as an encoding for the training data.
Reference: [257] <author> Ron Kohavi and Chia-Hsin Li. </author> <title> Oblivious decision trees, graphs and top-down pruning. </title> <booktitle> In IJCAI-95 [223], </booktitle> <pages> pages 1071-1077. </pages> <editor> Editor: </editor> <publisher> Chris Mellish. </publisher>
Reference-contexts: Moret [338] provided a tutorial overview of the work on representing Boolean functions as decision trees and diagrams. He summarized results on constructing decision trees in discrete variable domains. Though Moret does mention some pattern recognition 5 Oblivious decision trees <ref> [257] </ref> from the machine learning literature are nearly identical in structure to OBDDs. 20 work on constructing decision trees from data, this was not his main emphasis. Safavin and Landgrebe [417] more recently summarized decision tree construction methodology, almost entirely from a pattern recognition perspective.
Reference: [258] <author> P. Kokol, M. Mernik, J. Zavrsnik, and K. Kancler. </author> <title> Decision trees based on automatic learning and their use in cardiology. </title> <journal> Journal of Medical Systems, </journal> <volume> 18(4):201, </volume> <year> 1994. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology <ref> [295, 129, 258] </ref>, psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data
Reference: [259] <author> Igor Kononenko. </author> <title> Inductive and bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 317-337, </pages> <month> October-December </month> <year> 1993. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis <ref> [259] </ref>, cardiology [295, 129, 258], psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and
Reference: [260] <author> Igor Kononenko. </author> <title> On biases in estimating multi-valued attributes. </title> <booktitle> In IJCAI-95 [223], </booktitle> <pages> pages 1034-1040. </pages> <editor> Editor: Chris Mellish. </editor> <volume> 269 </volume>
Reference-contexts: A hypergeometric distribution is proposed as a means to avoid the biases of information gain, gain ratio and 2 metrics in [312]. Kononenko recently pointed out that <ref> [260] </ref> Minimum Description Length based feature evaluation criteria have the least bias towards multi-valued attributes. 2.3.2 Multivariate splits Decision trees most commonly are univariate, i.e., they use splits based on a single attribute at each internal node.
Reference: [261] <author> Igor Kononenko and Ivan Bratko. </author> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 67-80, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The accuracy of the tree is computed using a testing set that is independent of 49 the training set or using estimation techniques like cross-validation or bootstrap, and more accurate trees are preferred to the less accurate ones. Kononenko and Bratko <ref> [261] </ref> pointed out that comparisons on the basis of classification accuracy are unreliable, because different classifiers produce different types of estimates (e.g., some produce yes-or-no classifications, some output class probabilities) and accuracy values can vary with prior probabilities of the classes.
Reference: [262] <author> J. A. Kors and J. H. Van Bemmel. </author> <title> Classification methods for computerized interpretation of the electrocardiogram. </title> <booktitle> Methods of Information in Medicine, </booktitle> <volume> 29(4) </volume> <pages> 330-336, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Their results showed that there was no difference between the physicians and the computers, in terms of the prediction accuracy. Kors and Van Bemmel <ref> [262] </ref> compared statistical multivariate methods with heuristic decision tree methods, in the domain of electrocardiogram (ECG) analysis. Their comparisons show that decision tree classifiers are more comprehensible and flexible to incorporate or change existing categories.
Reference: [263] <author> V. A. Kovalevsky. </author> <title> The problem of character recognition from the point of view of mathematical statistics. </title> <editor> In V. A. Kovalevsky, editor, </editor> <title> Character Readers and Pattern Recognition. </title> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: Relationship between feature evaluation by Shannon's entropy and the probability of error is investigated in <ref> [263, 406] </ref>. 2.7 Comparisons with other exploration methods There exist several alternatives to decision trees for data exploration, such as neural networks, nearest neighbor methods and regression analysis. Several researchers have compared trees to these other methods on specific problems.
Reference: [264] <author> J. R. Koza. </author> <title> Concept formation and decision tree induction using the genetic programming paradigm. </title> <editor> In H. P. Schwefel and R. Manner, editors, </editor> <booktitle> Parallel Problem Solving from Nature Proceedings of 1st Workshop, PPSN 1, volume 496 of Lecture Notes in Computer Science, </booktitle> <pages> pages 124-128, </pages> <address> Dortmund, Germany, October 1991. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany. </address>
Reference-contexts: The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. In order to escape local minima in the search space, randomized search techniques such as genetic programming 45 <ref> [264] </ref> and simulated annealing [55, 303] have been attempted. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [463]. <p> It will be interesting to explore the uses of randomization to build optimal or near-optimal axis-parallel trees. Randomized search techniques, such as genetic programming <ref> [264] </ref> and simulated annealing [55, 303] have already been used to improve axis-parallel decision trees. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits.
Reference: [265] <editor> Paruchuri Rama Krishnaiah and Laveen N. Kanal, editors. </editor> <title> Classification, Pattern Recognition and Reduction of Dimensionality, </title> <booktitle> volume 2 of Handbook of Statistics. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference: [266] <author> Srinivasan Krishnamoorthy and Douglas Fisher. </author> <title> Machine learning approaches to estimating software development effort. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(2) </volume> <pages> 126-137, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: A recent use of tree-based classi 64 fication in remote sensing can be found in [416]. * Software development: Regression trees (and backpropagation networks) were recently used to estimate the development effort of a given software module in <ref> [266] </ref>, where it is argued that machine learning methods compare favorably with traditional methods. * Text processing: A recent use of ID3 [391] for medical text classification can be found in [282]. * Miscellaneous: Decision trees have also been used recently for building personal learning assistants [327] and for classifying sleep
Reference: [267] <author> M. Kubat, G. Pfurtscheller, and D. Flotzinger. </author> <title> AI-based approach to automatic sleep classification. </title> <journal> Biological Cybernetics, </journal> <volume> 70(5) </volume> <pages> 443-448, </pages> <year> 1994. </year>
Reference-contexts: it is argued that machine learning methods compare favorably with traditional methods. * Text processing: A recent use of ID3 [391] for medical text classification can be found in [282]. * Miscellaneous: Decision trees have also been used recently for building personal learning assistants [327] and for classifying sleep signals <ref> [267] </ref>. 2.9 A word of caution The hierarchical, recursive tree construction methodology is simple and intuitively appealing. However, the simplicity of the methodology should not lead a practitioner to take a slack attitude towards using decision trees.
Reference: [268] <author> Ashok K. Kulkarni. </author> <title> On the mean accuracy of hierarchical classifiers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(8):771-776, </volume> <month> August </month> <year> 1978. </year>
Reference-contexts: construction, estimating class probabilities from trees, use of multiple trees to reduce variance and incremental induction of trees. 2.5.1 Sample size vs. dimensionality The relationship between the size of the training set and the dimensionality of the problem is studied extensively in the pattern recognition literature. (For some pointers, see <ref> [212, 39 238, 145, 77, 236, 268, 227, 156] </ref>.) Researchers considered the problem of how sample size should vary according to dimensionality and vice versa.
Reference: [269] <author> Michael J. Kurtz. </author> <title> Astronomical object classification. </title> <editor> In E. S. Gelsema and Laveen N. Kanal, editors, </editor> <booktitle> Pattern Recognition and Artificial Intelligence, </booktitle> <pages> pages 317-328. </pages> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: described in [316]. * Astronomy: Astronomy has been an active domain for using automated classification techniques. 22 Use of decision trees for filtering noise from Hubble Space Tele-22 For a general description of modern classification problems in astronomy, which prompt the use of pattern recognition and machine learning techniques, see <ref> [269] </ref>. 62 scope images was reported recently in [424]. Decision trees have helped in star-galaxy classification [500], determining galaxy counts [499] and discovering quasars [244] in the Second Palomar Sky Survey.
Reference: [270] <author> M. W. Kurzynski. </author> <title> The optimal strategy of a tree classifier. </title> <journal> Pattern Recognition, </journal> <volume> 16 </volume> <pages> 81-87, </pages> <year> 1983. </year>
Reference-contexts: Most of the above feature evaluation criteria assume no knowledge of the probability distribution of the training objects. The optimal decision rule at each tree node, a rule that minimizes the overall error probability, is considered in <ref> [270, 271, 272] </ref> assuming that complete probabilistic information about the data is known. Comparisons Given the large number of feature evaluation rules, a natural concern is to decide their relative effectiveness in constructing "good" trees. <p> Garey and Graham [159] analyze the properties of recursive greedy splitting on the quality of trees induced from decision tables, and showed that greedy algorithms using information theoretic splitting criteria can be made to perform arbitrarily worse than the optimal. Kurzynski <ref> [270] </ref> showed that, for globally optimum performance, decisions made at each node should "emphasize the decision that leads to a greater joint probability of correct classification at the next 58 level", i.e., decisions made at different nodes in the tree should not be independent.
Reference: [271] <author> M. W. Kurzynski. </author> <title> On the multi-stage Bayes classifier. </title> <journal> Pattern Recognition, </journal> <volume> 21(4) </volume> <pages> 355-365, </pages> <year> 1988. </year>
Reference-contexts: Most of the above feature evaluation criteria assume no knowledge of the probability distribution of the training objects. The optimal decision rule at each tree node, a rule that minimizes the overall error probability, is considered in <ref> [270, 271, 272] </ref> assuming that complete probabilistic information about the data is known. Comparisons Given the large number of feature evaluation rules, a natural concern is to decide their relative effectiveness in constructing "good" trees.
Reference: [272] <author> M. W. Kurzynski. </author> <title> On the identity of optimal strategies for multi-stage classifiers. </title> <journal> Pattern Recognition Letters, </journal> <volume> 10(1) </volume> <pages> 39-46, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Most of the above feature evaluation criteria assume no knowledge of the probability distribution of the training objects. The optimal decision rule at each tree node, a rule that minimizes the overall error probability, is considered in <ref> [270, 271, 272] </ref> assuming that complete probabilistic information about the data is known. Comparisons Given the large number of feature evaluation rules, a natural concern is to decide their relative effectiveness in constructing "good" trees.
Reference: [273] <author> Eyal Kushilevitz and Yishay Mansour. </author> <title> Learning decision trees using the fourier spectrum. </title> <journal> SIAM Journal of Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year> <note> Earlier version in STOC-91. 270 </note>
Reference-contexts: Ehrenfeucht and Haussler [121] gave an algorithm for PAC-learning (without membership queries) decision trees of constant rank in polynomial time. They also gave a PAC-learning algorithm for general polynomial size decision trees in time O (n O (log n) ). Kushilevitz and Mansour <ref> [273] </ref> gave a polynomial time PAC-learning algorithm with membership queries for decision trees under the uniform distribution. Hancock [189] gave a polynomial time algorithm for PAC-learning read-k decision trees.
Reference: [274] <author> S.W. Kwok and Carter. C. </author> <title> Multiple decision trees. </title> <editor> In R.D. Schachter, T.S. Levitt, L.N. Kanal, and J.F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <volume> volume 4, </volume> <pages> pages 327-335. </pages> <publisher> Elsevier Science, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [274, 443, 444, 62, 203] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree [443, 444].
Reference: [275] <author> G. Landeweerd, T. Timmers, E. Gersema, M. Bins, and M. Halic. </author> <title> Binary tree versus single level tree classification of white blood cells. </title> <journal> Pattern Recognition, </journal> <volume> 16 </volume> <pages> 571-577, </pages> <year> 1983. </year>
Reference-contexts: Bottom up induction of trees is considered in <ref> [275] </ref>.
Reference: [276] <author> P. Langley and S. Sage. </author> <title> Scaling to domains with many irrelevant features. </title> <type> Unpublished manuscript. </type> <institution> Learning Systems Department, Siemens Corporate Research, Princeton, NJ, </institution> <year> 1993. </year>
Reference-contexts: Some of these studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets. For examples of this work, see <ref> [251, 276, 69, 113, 336, 3] </ref>. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: [277] <author> M. Lebowitz. </author> <title> Categorizing numeric information for generalization. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 285-308, </pages> <year> 1985. </year>
Reference: [278] <author> Chulhee Lee and David Landgrebe. </author> <title> Decision boundary feature extraction for nonparametric classification. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 23(2) </volume> <pages> 433-444, </pages> <month> March/April </month> <year> 1993. </year>
Reference: [279] <author> C.Y. Lee. </author> <title> Representation of switching circuits by binary decision programs. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 38 </volume> <pages> 985-999, </pages> <month> July </month> <year> 1959. </year>
Reference-contexts: Tree structured vector quantizers (TSVQ) [65] are structures very similar to decision trees. A lot of work exists in the speech and signal processing literature, 19 on building and analyzing TSVQs. A Binary Decision Diagram (BDD) represents a Boolean function as a rooted, directed acyclic graph <ref> [279, 441] </ref>. Ordered binary decision diagrams (OBDD) [52, 53] impose restrictions on the ordering of variables at the nodes of a BDD. OBDDs have been used for digital system design, verification and testing.
Reference: [280] <author> Kun Chang Lee and Sung Joo Park. PRTSM: </author> <title> Pattern recognition based time series modeler. </title> <booktitle> Computer Science in Economics and Management, </booktitle> <volume> 2(3) </volume> <pages> 239-254, </pages> <year> 1989. </year>
Reference: [281] <author> Seong-Whan Lee. </author> <title> Noisy Hangul character recognition with fuzzy tree classifier. </title> <booktitle> Proceedings of SPIE, </booktitle> <volume> 1661 </volume> <pages> 127-136, </pages> <year> 1992. </year> <title> Volume title: Machine vision applications in character recognition and industrial inspection. </title> <booktitle> Conference location: </booktitle> <address> San Jose, CA. </address> <month> 10th-12th February, </month> <year> 1992. </year>
Reference-contexts: Jordan and Jacobs [233] describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [146]. Induction of fuzzy decision trees has also been considered in <ref> [281, 512] </ref>. 2.5.6 Estimating probabilities Decision trees have crisp decisions at leaf nodes. On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes. Breiman et al. ([44], Chapter 4) proposed a method for building class probability trees.
Reference: [282] <author> Wendy Lehnert, Stephen Soderland, David Aronow, Fangfang Feng, and Avinoam Shmueli. </author> <title> Inductive text classification for medical applications. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7(1) </volume> <pages> 49-80, </pages> <month> January-March </month> <year> 1995. </year>
Reference-contexts: Regression trees (and backpropagation networks) were recently used to estimate the development effort of a given software module in [266], where it is argued that machine learning methods compare favorably with traditional methods. * Text processing: A recent use of ID3 [391] for medical text classification can be found in <ref> [282] </ref>. * Miscellaneous: Decision trees have also been used recently for building personal learning assistants [327] and for classifying sleep signals [267]. 2.9 A word of caution The hierarchical, recursive tree construction methodology is simple and intuitively appealing.
Reference: [283] <author> Debra A. Lelewer and Daniel S. Hirschberg. </author> <title> Data compression. </title> <journal> ACM Computing Surveys, </journal> <volume> 19(3) </volume> <pages> 261-296, </pages> <year> 1987. </year>
Reference-contexts: Greedy algorithms do not always yield optimal solutions, but for some problems they do. Examples of the latter variety include methods for constructing minimum spanning trees [178] and methods for producing optimal Huffman codes for data compression <ref> [283] </ref>. Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem [419], multiprocessor scheduling [210] and the problem under consideration here, decision tree induction.
Reference: [284] <author> David D. Lewis and Jason Catlett. </author> <title> Heterogeneous uncertainty sampling for supervised learning. In ML-94 [331]. </title> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference: [285] <author> P.M. Lewis. </author> <title> The characteristic selection problem in recognition systems. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-18:171-178, </volume> <year> 1962. </year>
Reference-contexts: They suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis <ref> [285] </ref>. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic [21, 195, 323, 515, 503] are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures.
Reference: [286] <author> Xiaobo Li and Richard C. Dubes. </author> <title> Tree classifier design with a permutation statistic. </title> <journal> Pattern Recognition, </journal> <volume> 19(3) </volume> <pages> 229-235, </pages> <year> 1986. </year> <month> 271 </month>
Reference-contexts: The split that minimized P (I (T i )) was chosen by these methods. A permutation statistic was used for univariate tree construction for 2-class problems in <ref> [286] </ref>. The main advantage of this statistic is that, unlike most of the other measures, its distribution is independent of the number of training instances. As will be seen in Section 2.4, this property provides a natural measure of when to stop tree growth. <p> Some feature evaluation rules, whose distribution does not depend on the number of training samples (i.e., a goodness value of k would have the same significance anywhere in the tree) have been suggested in the literature <ref> [286, 515, 235] </ref>. * Trees to rules conversion: Quinlan [393, 398] gave efficient procedures for converting a decision tree into a set of production rules.
Reference: [287] <author> Jianhia Lin and L.A. Storer. </author> <title> Design and performance of tree structured vector quantizers. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 30(6) </volume> <pages> 851-862, </pages> <year> 1994. </year>
Reference: [288] <author> Jianhua Lin, J. A. Storer, and M. Cohn. </author> <title> Optimal pruning for tree-structured vector quantizers. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 28(6) </volume> <pages> 723-733, </pages> <year> 1992. </year>
Reference: [289] <author> Jyh-Han Lin and J. S. Vitter. </author> <title> Nearly optimal vector quantization via linear programming. </title> <editor> In J. A. Storer and M. Cohn, editors, </editor> <booktitle> DCC 92. Data Compression Conference, </booktitle> <pages> pages 22-31, </pages> <address> Los Alamitos, CA, March 24th-27th 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular [309, 28, 25, 307, 26]. Use of zero-one integer programming for designing vector quantizers can be found in <ref> [289] </ref>. Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes. Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary.
Reference: [290] <author> Y. K. Lin and King-Sun Fu. </author> <title> Automatic classification of cervical cells using a binary tree classifier. </title> <journal> Pattern Recognition, </journal> <volume> 16(1) </volume> <pages> 69-80, </pages> <year> 1983. </year>
Reference-contexts: suggested a splitting criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance <ref> [290] </ref>, Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic [21, 195, 323, 515, 503] are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> Lin and Fu <ref> [290] </ref> use K-means clustering for both stages, whereas Qing-Yun and Fu [387] use multi-variate stepwise regression for the first stage and linear discriminant analysis for the second stage. * Thresholds on Impurity: In this method, a threshold is imposed on the value of the splitting criterion, such that if the splitting
Reference: [291] <author> D. V. Lindley. </author> <title> On a measure of the information provided by an experiment. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 27(4) </volume> <pages> 986-1005, </pages> <month> December </month> <year> 1956. </year>
Reference: [292] <author> W. Z. Liu and A. P. White. </author> <title> The importance of attribute selection measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 25-41, </pages> <year> 1994. </year>
Reference-contexts: Mingers [325] compared several attribute selection criteria, and concluded that tree quality doesn't seem to depend on the specific criterion used. He even claimed that random attribute selection criteria are as good as measures like information gain [391]. This later claim was refuted in <ref> [292] </ref>, where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Babic et al. [15] compared ID3 [391] and CART [44], for two clinical diagnosis problems.
Reference: [293] <author> Zhen-Ping Lo and B. Bavarian. </author> <title> Development of a two-stage neural network classifier. </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> 1(3) </volume> <pages> 307-327, </pages> <year> 1994. </year>
Reference: [294] <author> Wei-Yin Loh and Nunta Vanichsetakul. </author> <title> Tree-structured classification via generalized discriminant analysis. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83(403) </volume> <pages> 715-728, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Their method uses multivariate stepwise regression to optimize the structure of the decision tree as well as to choose subsets of features to be used in the linear discriminants. More recently, use of linear discriminants at each node is considered by 29 Loh and Vanichsetakul <ref> [294] </ref>. Unlike in [511], the variables at each stage are appropriately chosen in [294] according to the data and the type of splits desired. Other features of the tree building algorithm in [294] are: (1) it yields trees with univariate, linear combination or linear combination of polar coordinate splits, and (2) <p> More recently, use of linear discriminants at each node is considered by 29 Loh and Vanichsetakul <ref> [294] </ref>. Unlike in [511], the variables at each stage are appropriately chosen in [294] according to the data and the type of splits desired. Other features of the tree building algorithm in [294] are: (1) it yields trees with univariate, linear combination or linear combination of polar coordinate splits, and (2) allows both ordered and unordered variables in the same linear split. <p> More recently, use of linear discriminants at each node is considered by 29 Loh and Vanichsetakul <ref> [294] </ref>. Unlike in [511], the variables at each stage are appropriately chosen in [294] according to the data and the type of splits desired. Other features of the tree building algorithm in [294] are: (1) it yields trees with univariate, linear combination or linear combination of polar coordinate splits, and (2) allows both ordered and unordered variables in the same linear split. Use of linear discriminants in a decision tree is considered in the remote sensing literature in [218].
Reference: [295] <author> William J. Long, John L. Griffith, Harry P. Selker, and Ralph B. D'Agostino. </author> <title> A comparison of logistic regression to decision tree induction in a medical domain. </title> <journal> Computers and Biomedical Research, </journal> <volume> 26(1) </volume> <pages> 74-97, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Their main conclusions are that (1) no method seems uniformly superior to others, (2) machine learning methods seem to be superior for multimodal distributions, and (3) statistical methods are computationally the most efficient. Long et al. <ref> [295] </ref> compared Quinlan's C4 [398] to logistic regression on the problem of diagnosing acute cardiac ischemia, and concluded that both methods came fairly close to the expertise of the physicians. In their experiments, logistic regression outperformed C4. <p> Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology <ref> [295, 129, 258] </ref>, psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data
Reference: [296] <author> D.W. Loveland. </author> <title> Performance bounds for binary testing with arbitrary weights. </title> <journal> Acta Informatica, </journal> <volume> 22 </volume> <pages> 101-114, </pages> <year> 1985. </year>
Reference-contexts: Kurzynski [270] showed that, for globally optimum performance, decisions made at each node should "emphasize the decision that leads to a greater joint probability of correct classification at the next 58 level", i.e., decisions made at different nodes in the tree should not be independent. Loveland <ref> [296] </ref> analyzed the performance of variants of Gini index in the context of sequential fault diagnosis. Goodman and Smyth [174, 175] analyzed mutual information based greedy tree induction from an information theoretic view point.
Reference: [297] <author> David Lubinsky. </author> <title> Algorithmic speedups in growing classification trees by using an additive split criterion. </title> <booktitle> In AI&Statistics-93 [4], </booktitle> <pages> pages 435-444. </pages>
Reference-contexts: Max minority has the theoretical advantage that the depth of the tree constructed using this measure is at worst logarithmic in the number of examples. Lubinsky <ref> [297, 298] </ref> also used the number of misclassified points as a splitting criterion, calling it inaccuracy. The performance of these measures does not seem to be in general as good as the information theory or distance based measures, and additional tricks are needed to make these measures robust [297, 351]. <p> Lubinsky [297, 298] also used the number of misclassified points as a splitting criterion, calling it inaccuracy. The performance of these measures does not seem to be in general as good as the information theory or distance based measures, and additional tricks are needed to make these measures robust <ref> [297, 351] </ref>.
Reference: [298] <author> David Lubinsky. </author> <title> Bivariate splits and consistent split criteria in dichotomous classification trees. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: Max minority has the theoretical advantage that the depth of the tree constructed using this measure is at worst logarithmic in the number of examples. Lubinsky <ref> [297, 298] </ref> also used the number of misclassified points as a splitting criterion, calling it inaccuracy. The performance of these measures does not seem to be in general as good as the information theory or distance based measures, and additional tricks are needed to make these measures robust [297, 351]. <p> In information theory, Gelfand and Ravishanker [161] describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. [204, 202] used simulated annealing to find the best oblique split at each tree node. Lubinsky <ref> [299, 298] </ref> attempted bivariate trees, trees in which some functions of two variables can be used as tests at internal nodes. <p> As this measure is computed using the actual class labels, it is easy to see that the impurity computed varies depending on 30 Lubinsky <ref> [298] </ref> also used this measure for tree construction, referring to it as inaccuracy. He suggested an improvement to inaccuracy called weighted inaccuracy. 31 Sum Of Variances was called Sum of Impurities by Heath et al..
Reference: [299] <author> David Lubinsky. </author> <title> Classification trees with bivariate splits. </title> <journal> Applied Intelligence: The International Journal of Artificial Intelligence, Neural Networks and Complex Problem-Solving Technologies, </journal> <volume> 4(3) </volume> <pages> 283-296, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In information theory, Gelfand and Ravishanker [161] describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. [204, 202] used simulated annealing to find the best oblique split at each tree node. Lubinsky <ref> [299, 298] </ref> attempted bivariate trees, trees in which some functions of two variables can be used as tests at internal nodes.
Reference: [300] <author> David Lubinsky. </author> <title> Tree structured interpretable regression. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 331-340. 272 </pages>
Reference-contexts: All internal nodes have two or more child nodes. 2 All non-terminal nodes contain splits, which test the value of a mathematical or logical expression of the attributes. Edges from an internal node T to its children are labelled with distinct outcomes of the test 2 Lubinsky <ref> [300] </ref> considered trees that can have internal nodes with just one child. At these nodes, the data are not split, but residuals are taken from a single variable regression. 15 at T . Each leaf node has a class label associated with it. 3 The number of classes is finite.
Reference: [301] <author> Paul Lukowitz, Ernst A. Heinz, Lutz Prechelt, and Walter F. Tichy. </author> <title> Experimental evaluation in computer science: A quantitative study. </title> <journal> Journal of Systems and Software, </journal> <note> January 1995. Anonymous FTP at ftp.ira.uka.de in the file /pub/uni-karlsruhe/papers/techreports/1994/1994-17.ps.Z. </note>
Reference: [302] <author> Ren C. Luo, Ralph S. Scherp, and Mark Lanzo. </author> <title> Object identification using automated decision tree construction approach for robotics applications. </title> <journal> Journal of Robotic Systems, </journal> <volume> 4(3) </volume> <pages> 423-433, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. A relatively simplistic method for estimating class separation, which assumes that the values of each feature follow a Gaussian distribution in each class, was used for tree construction in <ref> [302] </ref>. * Rules derived from dependence measures: These measure the statistical dependence between two random variables. All dependence-bassed measures can be interpreted as belonging to one of the above two categories [23]. There exist several attribute selection criteria that do not clearly belong to any category in Ben-Basset's taxonomy.
Reference: [303] <author> J. F. Lutsko and B. Kuijpers. </author> <title> Simulated annealing in the construction of near-optimal decision trees. </title> <booktitle> In AI&Statistics-93 [4]. </booktitle>
Reference-contexts: The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. In order to escape local minima in the search space, randomized search techniques such as genetic programming 45 [264] and simulated annealing <ref> [55, 303] </ref> have been attempted. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [463]. <p> It will be interesting to explore the uses of randomization to build optimal or near-optimal axis-parallel trees. Randomized search techniques, such as genetic programming [264] and simulated annealing <ref> [55, 303] </ref> have already been used to improve axis-parallel decision trees. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits.
Reference: [304] <author> Lymphography data. </author> <note> Available in the UCI ML Repository. </note> <institution> Obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Data provided by Matjaz Zwitter and Milan Soklic. </institution>
Reference-contexts: HE Hepatitis domain. Contains 155 instances, described using 20 attributes including the class label. The task is to classify patients that die from hepatitis from those that do not. LA Final settlements in labor negotiations in Canadian industry. 57 instances each de scribed using 16 features. LY Lymphography domain <ref> [304] </ref>. Contains 148 instances, each described using 19 at tributes, including the class attribute. VO 1984 United States congressional voting records database. This data is used by Norton [365] in his experiments. The data contains 435 instances, each described by 16 nominal attributes and one class label.
Reference: [305] <author> F. Maffioli, M.G. Speranza, and C. Vercellis. </author> <title> Randomized algorithms. </title> <editor> In M. OhEigeartaigh, J.K. Lenstra, and A.H.G. Rinnooy Kan, editors, </editor> <booktitle> Combinatorial Optimization Annotated Bibliographies, </booktitle> <pages> pages 89-105. </pages> <year> 1985. </year>
Reference: [306] <author> John Makhoul, Salim Roucos, and Herbert Gish. </author> <title> Vector quantization in speech coding. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73 </volume> <pages> 1551-1588, </pages> <month> November </month> <year> 1985. </year> <type> Invited paper. </type>
Reference: [307] <author> Olvi Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <month> Fall </month> <year> 1993. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [309, 28, 25, 307, 26] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [289]. Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes.
Reference: [308] <author> Olvi L. Mangasarian. </author> <title> Misclassification minimization, 1994. </title> <type> Unpublished manuscript. </type>
Reference-contexts: Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary. In that sense, these methods are more similar to perceptron training methods [326], than to decision tree splitting criteria. Mangasarian <ref> [308] </ref> describes a linear programming formulation to minimize the number of misclassified points instead of the geometric distance. Neural Trees: In the neural networks community, many researchers have recently considered hybrid structures between decision trees and neural nets.
Reference: [309] <author> Olvi L. Mangasarian, R. Setiono, and W. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <booktitle> In SIAM Workshop on Optimization, </booktitle> <year> 1990. </year>
Reference-contexts: More recently, Mangasarian and Bennett used linear and quadratic programming techniques to build machine learning systems in general and decision trees in particular <ref> [309, 28, 25, 307, 26] </ref>. Use of zero-one integer programming for designing vector quantizers can be found in [289]. Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes. <p> Mangasarian and Bennett have compiled data on the problem of diagnosing breast cancer to test several new classification methods <ref> [309, 25, 26] </ref>. This data represents a set of patients with breast cancer, where each patient was characterized by nine numeric attributes plus the diagnosis of the tumor as benign or malignant. The data set currently has 683 entries and is available from the UC Irvine machine learning repository [346].
Reference: [310] <author> L opez de M antaras. </author> <title> Technical note: A distance-based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 81-92, </pages> <year> 1991. </year>
Reference-contexts: He concluded that 2 corrected information gain's bias towards multivalued attributes, however to such an extent that they were never chosen, and the latter produced trees that were extremely deep and hard to interpret. Quinlan suggested gain ratio [398] as a remedy for the bias of information gain. Mantaras <ref> [310] </ref> argued that gain ratio had its own set of problems, and suggested using information theory-based distance between partitions for tree construction. He formally proved that his measure is not biased towards multiple-valued attributes.
Reference: [311] <author> J. Kent Martin. </author> <title> Evaluating and comparing classifiers: complexity measures. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 372-378. </pages>
Reference-contexts: They suggested an information based metric to evaluate a classifier, as a remedy to the above problems. Martin <ref> [311] </ref> argued that information theoretic measures of classifier complexity are not practically computable except within severely restricted families of classifiers, and suggested a generalized version of CART's [44] 1-standard error rule as a means of achieving a tradeoff between classifier complexity and accuracy.
Reference: [312] <author> J. Kent Martin. </author> <title> An exact probability metric for decision tree splitting and stopping. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 379-385. </pages>
Reference-contexts: A hypergeometric distribution is proposed as a means to avoid the biases of information gain, gain ratio and 2 metrics in <ref> [312] </ref>. Kononenko recently pointed out that [260] Minimum Description Length based feature evaluation criteria have the least bias towards multi-valued attributes. 2.3.2 Multivariate splits Decision trees most commonly are univariate, i.e., they use splits based on a single attribute at each internal node. <p> The former alternative is used in <ref> [172, 413, 390, 312] </ref> and the latter in [437]. A problem with the former method is that the value of most splitting criteria (Section 2.3.1) varies with the size of the training sample.
Reference: [313] <author> Dean P. McKenzie and Lee Hun Low. </author> <title> The construction of computerized classification systems using machine learning algorithms: An overview. </title> <booktitle> Computers in Human Behaviour, </booktitle> 8(2/3):155-167, 1992. <volume> 273 </volume>
Reference-contexts: Comparisons between decision trees and statistical methods like linear discriminant function analysis and automatic interaction detection (AID) are given in <ref> [313] </ref>, where it is argued that machine learning methods sometimes outperform the statistical methods and so should not be ignored. Feng et al. [138] present a comparison of several machine learning methods (including decision trees, neural networks and statistical classifiers) as a part of the European Statlog 21 project.
Reference: [314] <author> Dean P. McKenzie, P. D. McGorry, C. S. Wallace, Lee Hun Low, D. L. Copolov, and B. S. Singh. </author> <title> Constructing a minimal diagnostic decision tree. </title> <booktitle> Methods of Information in Medicine, </booktitle> <volume> 32(2) </volume> <pages> 161-166, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology [295, 129, 258], psychiatry <ref> [314] </ref>, gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data exploration methods in molecular
Reference: [315] <author> K. L. McMillan. </author> <title> Symbolic model checking: an approach to the state explosion problem. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Recently, agnostic PAC-learning [13] and pruning [205] have been studied by the learnability theory community. In the context of ordered binary decision diagrams (OBDD), the bounds on the tree size have been investigated, as a function of the tree compaction operators and the specific Boolean functions being represented (eg., <ref> [315, 457, 201] </ref>). 2.6.3 Tools Some authors pointed out the similarity or equivalence between the problem of constructing decision trees and existing, seemingly unrelated, problems. Such view points provide valuable tools for analyzing decision trees.
Reference: [316] <author> R.J. McQueen, S. R. Garner, C.G. Nevill-Manning, and I.H. Witten. </author> <title> Applying machine learning to agricultural data. </title> <journal> Computers and Electronics in Agriculture, </journal> <volume> 12(4) </volume> <pages> 275-293, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The application areas are listed below in alphabetical order. * Agriculture: Application of a range of machine learning methods to problems in agriculture and horticulture is described in <ref> [316] </ref>. * Astronomy: Astronomy has been an active domain for using automated classification techniques. 22 Use of decision trees for filtering noise from Hubble Space Tele-22 For a general description of modern classification problems in astronomy, which prompt the use of pattern recognition and machine learning techniques, see [269]. 62 scope
Reference: [317] <author> Nimrod Megiddo. </author> <title> On the complexity of polyhedral separability. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 3 </volume> <pages> 325-337, </pages> <year> 1988. </year>
Reference-contexts: The problem of constructing the smallest decision tree which best distinguishes characteristics of multiple distinct groups is shown to be NP-complete in [476]. Comer and Sethi [92] studied the asymptotic complexity of trie index construction in the document retrieval literature. Megiddo <ref> [317] </ref> investigated the problem of polyhedral separability (separating two sets of points using k hyperplanes), and proved that several variants of this problem are NP-complete. Results in the above three papers throw light on the complexity of decision tree induction.
Reference: [318] <author> William S. Meisel and Demetrios A. Michalopoulos. </author> <title> A partitioning algorithm with application in pattern classification and the optimization of decision trees. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(1):93-103, </volume> <month> January </month> <year> 1973. </year>
Reference-contexts: Thresholds can be imposed on local (i.e., individual node) 12 Techniques that start with a sufficient partitioning and then optimize the structure (e.g., <ref> [318] </ref>) can be thought of as being a converse to this approach. 35 goodness measures or on global (i.e., entire tree) goodness. The former alternative is used in [172, 413, 390, 312] and the latter in [437]. <p> In the first stage, a sufficient partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming <ref> [318] </ref>, fuzzy logic search [494] and multi-linear programming [30]. The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. <p> This can be done by using dynamic programming (e.g.: <ref> [318] </ref>) or branch and bound techniques (e.g.: [39]). But when the tree uses oblique splits, it is not clear, even for a fixed number of attributes, how to generate an optimal (e.g., smallest) decision tree in polynomial time.
Reference: [319] <author> Joseph J. Mezrich. </author> <title> When is a tree a hedge? Financial Analysts Journal, </title> <address> pages 75-81, </address> <month> November-December </month> <year> 1994. </year>
Reference-contexts: trees for identifying features to be used in implantable devices can be found in [169]. * Control Systems: Automatic induction of decision trees was recently used for con trol of nonlinear dynamical systems [213]. * Financial analysis: Use of CART [44] for asserting the attractiveness of buy-writes is reported in <ref> [319] </ref>. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383],
Reference: [320] <author> Donald Michie. </author> <title> The superarticulatory phenomenon in the context of software manufacture. </title> <journal> Proceedings of the Royal Society of London, </journal> <volume> 405A:185-212, </volume> <year> 1986. </year>
Reference-contexts: Some authors have attempted to evaluate the validity and relevance of the assumptions and biases in tree induction. 20 * Assumption: Multi-stage classifiers may be more accurate than single stage classifiers. Analysis: However, the data fragmentation caused by multi-stage hierarchical classifiers may compensate for the gain in accuracy. Michie <ref> [320] </ref> argues that top-down induction algorithms may provide overly complex classifiers that have no real conceptual structure in encoding relevant knowledge. As a solution to this problem, Gray [179] suggested an induction method that generates a single disjuncts of conjuncts rule, using the same time complexity as tree induction.
Reference: [321] <author> Donald Michie. </author> <title> Current developments in expert systems. </title> <editor> In J. Ross Quinlan, editor, </editor> <booktitle> Applications of Expert Systems, </booktitle> <pages> pages 137-156. </pages> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference: [322] <author> A. J. Miller. </author> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall, </publisher> <year> 1990. </year>
Reference-contexts: Moreover, most real world classifiers are not truly Bayesian. 40 selection method (Section 2.5.1) can be employed to filter out the unnecessary observations. Feature subset selection There is a large body of work on choosing relevant subsets of features (for example, see the texts <ref> [116, 35, 322] </ref>). Most of this work was not developed in the context of tree induction, but a lot of it has direct applicability. There are two components to any method that attempts to choose the best subset of features.
Reference: [323] <author> John Mingers. </author> <title> Expert systems | rule induction with statistical data. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 38(1) </volume> <pages> 39-47, </pages> <year> 1987. </year>
Reference-contexts: Mingers <ref> [323] </ref> suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop. <p> emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic <ref> [21, 195, 323, 515, 503] </ref> are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> Fayyad and Irani [133] showed that their measure C-SEP, performs better than Gini index [44] and information gain [391] for specific types of problems. 27 Several researchers [195, 391] pointed out that information gain is biased towards attributes with a large number of possible values. Mingers <ref> [323] </ref> compared information gain and the 2 statistic for growing the tree as well as for stop-splitting.
Reference: [324] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: Use of dynamic programming to prune trees optimally and efficiently has been explored recently in [33]. A few studies have been done to study the relative effectiveness of pruning methods <ref> [324, 91, 125] </ref>. Just as in the case of splitting criteria, no single pruning method has been adjudged to be superior to the others. <p> For the OC1 system we implemented an existing pruning method, but note that any tree pruning method will work fine within OC1. Based on the experimental evaluations of Mingers <ref> [324] </ref> and other work, we chose Breiman et al.'s 99 Cost Complexity (CC) pruning [44] as the default pruning method for OC1. This method, which is also called Error Complexity or Weakest Link pruning, requires a separate pruning set. <p> When the 0-SE rule (k = 0) is used, the tree with highest accuracy on the pruning set is selected. When k &gt; 0, smaller tree size is preferred over higher accuracy. For details of Cost Complexity pruning, see Breiman et al.[44] or Mingers <ref> [324] </ref>. Irrelevant attributes Irrelevant attributes pose a significant problem for most machine learning methods. Decision tree algorithms, even axis-parallel ones, can be confused by too many irrelevant attributes (see Section 2.5.1 for pointers to existing work).
Reference: [325] <author> John Mingers. </author> <title> An empirical comparison of selection measures for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: Breiman et al. [44] conjectured that decision tree design is rather insensitive to any one from a large class of splitting rules, and it is the stopping rule that is crucial. Mingers <ref> [325] </ref> compared several attribute selection criteria, and concluded that tree quality doesn't seem to depend on the specific criterion used. He even claimed that random attribute selection criteria are as good as measures like information gain [391].
Reference: [326] <author> M. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference-contexts: This algorithm uses heuristic hill climbing and backward feature elimination to find good linear combinations at each node. Murthy et al. [350, 351] described significant extensions to CART's linear combinations algorithm, using randomized techniques. (See Chapter 3) Perceptron Learning: A perceptron is a linear function neuron <ref> [326, 188] </ref> which can be trained to optimize the sum of distances of the misclassified objects to it, using a convergent procedure for adjusting its coefficients. Perceptron trees, which are decision trees with perceptrons just above the leaf nodes, were discussed in [480]. <p> Brown and Pittard [51] also employed linear programming for finding optimal multivariate splits at classification tree nodes. Almost all the above papers attempt to minimize the distance of the misclassified points from the decision boundary. In that sense, these methods are more similar to perceptron training methods <ref> [326] </ref>, than to decision tree splitting criteria. Mangasarian [308] describes a linear programming formulation to minimize the number of misclassified points instead of the geometric distance. Neural Trees: In the neural networks community, many researchers have recently considered hybrid structures between decision trees and neural nets.
Reference: [327] <author> Tom Mitchell, Rich Caruana, Dayne Freitag, John McDermott, and David Zabowski. </author> <title> Experience with a learning personal assistant. </title> <journal> Communications of the ACM, </journal> <month> July </month> <year> 1994. </year>
Reference-contexts: given software module in [266], where it is argued that machine learning methods compare favorably with traditional methods. * Text processing: A recent use of ID3 [391] for medical text classification can be found in [282]. * Miscellaneous: Decision trees have also been used recently for building personal learning assistants <ref> [327] </ref> and for classifying sleep signals [267]. 2.9 A word of caution The hierarchical, recursive tree construction methodology is simple and intuitively appealing. However, the simplicity of the methodology should not lead a practitioner to take a slack attitude towards using decision trees.
Reference: [328] <author> Masahiro Miyakawa. </author> <title> Optimum decision trees an optimal variable theorem and its related applications. </title> <journal> Acta Informatica, </journal> <volume> 22(5) </volume> <pages> 475-498, </pages> <year> 1985. </year> <month> 274 </month>
Reference-contexts: Trees of maximal size generated by the CART algorithm [44] have been shown to have an error rate bounded by twice the Bayes error rate, and to be asymptotically Bayes optimal [177]. Miyakawa <ref> [328] </ref> considered the problem of coverting decision tables to optimal trees, and studied the properties of optimal variables, the class of attributes only members of which can be used at the root of an optimal tree.
Reference: [329] <author> Masahiro Miyakawa. </author> <title> Criteria for selecting a variable in the construction of efficient decision trees. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(1) </volume> <pages> 130-141, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: As will be seen in Section 2.4, this property provides a natural measure of when to stop tree growth. Measures that use the activity of an attribute have been explored for tree construction <ref> [337, 329] </ref>. The activity of a variable is equal to the testing cost of the variable 24 times the a priori probability that it will be tested. The computational requirements for computing activity are the same as those for the information-based measures. <p> This later claim was refuted in [292], where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Babic et al. [15] compared ID3 [391] and CART [44], for two clinical diagnosis problems. Miyakawa <ref> [329] </ref> compared three activity-based measures, Q, O and loss, both analytically and empirically. He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss.
Reference: [330] <editor> Machine Learning: </editor> <booktitle> Proceedings of the Tenth International Conference, </booktitle> <institution> University of Massachusetts, </institution> <address> Amherst, MA, 27-29th, June 1993. </address> <publisher> Morgan Kaufmann Publishers Inc. </publisher> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference: [331] <editor> Machine Learning: </editor> <booktitle> Proceedings of the Eleventh International Conference, </booktitle> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, 10-13th, July 1994. </address> <publisher> Morgan Kaufmann Publishers Inc. </publisher> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference: [332] <institution> Applying Machine Learning in Practice, </institution> <address> Tahoe City, CA, 10-13th, July 1995. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address> <note> Workshop organized by David Aha. http://www.aic.nrl.navy.mil/ aha/imlc95-workshop/. </note>
Reference-contexts: The most detailed discussion about data massaging that the author is 179 aware of is from two recent workshop proceedings <ref> [222, 332] </ref>. Both these forums discussed the problem of structuring, or engineering the data into a form suitable for inductive learning. Each paper at these two workshops, like the current chapter, argues data massaging mostly relies on domain knowledge.
Reference: [333] <editor> Machine Learning: </editor> <booktitle> Proceedings of the Twelfth International Conference, </booktitle> <address> Tahoe City, CA, 10-13th, July 1995. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address> <publisher> Editor: Jeffrey Schlimmer. </publisher>
Reference: [334] <author> Dunja Mladenic. </author> <title> Combinatorial optimization in inductive concept learning. </title> <booktitle> In ML-93 [330], </booktitle> <pages> pages 205-211. </pages> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference: [335] <author> Advait Mogre, Robert McLaren, James Keller, and Raghuram Krish-napuram. </author> <title> Uncertainty management for rule-based systems with application to image analysis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 24(3) </volume> <pages> 470-481, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Assignment of probabilistic goodness to splits in a decision tree is described in [187]. A unified methodology for combining uncertainties associated with attributes into that of a given test, which can then be systematically propagated down the decision tree, is given in <ref> [335] </ref>. 16 Smoothing is the process of adjusting probabilities at a node in the tree based on the probabilities at other nodes on the same path.
Reference: [336] <author> Andrew W. Moore and Mary S. Lee. </author> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In ML-94 [331], </booktitle> <pages> pages 190-198. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: Some of these studies produced interesting insights on how to increase the efficiency and effectiveness of the heuristic search for good feature subsets. For examples of this work, see <ref> [251, 276, 69, 113, 336, 3] </ref>. Composite features Sometimes the aim is not to choose a good subset of features, but instead to find a few good "composite" features, which are arithmetic or logical combinations of the atomic features.
Reference: [337] <author> Bernard M. E. Moret, M. G. Thomason, and R. C. Gonzalez. </author> <title> The activity of a variable and its relation to decision trees. </title> <journal> ACM Transactions on Programming Language Systems, </journal> <volume> 2(4) </volume> <pages> 580-595, </pages> <month> October </month> <year> 1980. </year>
Reference-contexts: As will be seen in Section 2.4, this property provides a natural measure of when to stop tree growth. Measures that use the activity of an attribute have been explored for tree construction <ref> [337, 329] </ref>. The activity of a variable is equal to the testing cost of the variable 24 times the a priori probability that it will be tested. The computational requirements for computing activity are the same as those for the information-based measures.
Reference: [338] <author> Bernard M.E. Moret. </author> <title> Decision trees and diagrams. </title> <journal> Computing Surveys, </journal> <volume> 14(4) </volume> <pages> 593-623, </pages> <month> December </month> <year> 1982. </year>
Reference-contexts: In spite of a diverse body of literature on automatic construction of decision trees, there exist no comprehensive, multi-disciplinary surveys of up-to-date results on this topic (see Section 2.2 for discussion of existing surveys <ref> [379, 338, 417] </ref>). A characteristic of existing decision tree work seems to be a lack of directed progress. As most research on this subject is (perhaps needs to be) empirical, researchers and system developers typically try ad hoc variations of the basic methodology. <p> tree work in multiple disciplines in contrast to existing surveys that concentrated on specific disciplines (e.g., Safavin and Langrebe's survey [417] covers work mostly from the pattern recognition literature). * Our main emphasis is on automatically constructing decision trees for parsimonious descriptions of, and generalization from, data. (In contrast, Moret's <ref> [338] </ref> main em phasis was on representing Boolean functions as decision trees.) * A significant portion of this survey is devoted to comparisons of tree-based data exploration with other (e.g., statistical and neural) methods, and to recent real-world applications of decision trees. <p> Our main emphasis is to trace the directions that decision tree work has taken. * We avoid repeating many of the references from <ref> [379, 338, 417] </ref>. <p> The problem of constructing identification keys is not the same as the problem of constructing decision trees from data, but many common concerns exist (e.g: optimal key construction, choosing good tests at tree nodes etc.). Moret <ref> [338] </ref> provided a tutorial overview of the work on representing Boolean functions as decision trees and diagrams. He summarized results on constructing decision trees in discrete variable domains. <p> Several authors (e.g., [159, 405]) pointed out the inadequacy of greedy induction for difficult concepts. The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see <ref> [338] </ref>. Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning [365, 403, 354]. <p> Several measures have been suggested to quantify tree quality. Moret <ref> [338] </ref> summarizes work on measures such as tree size, expected testing cost and worst-case testing cost. He shows that these three measures are pairwise incompatible, which implies that an algorithm minimizing one measure is guaranteed not to minimize the others, for some tree.
Reference: [339] <author> J. N. Morgan and R. C. </author> <title> Messenger. THAID: a sequential search program for the analysis of nominal scale dependent variables. </title> <type> Technical report, </type> <institution> Institute for Social Research, University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1973. </year>
Reference-contexts: For a review of earlier statistical work on hierarchical classification, see [139]. Statistical programs such as AID [454], MAID [170], THAID <ref> [339] </ref> and CHAID [240] built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables.
Reference: [340] <author> D. T. Morris and D. Kalles. </author> <title> Decision trees and domain knowledge in pattern recognition. </title> <booktitle> In Gelsema and Kanal [164], </booktitle> <pages> pages 25-36. </pages>
Reference-contexts: Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, <ref> [107, 340] </ref> in pattern recognition and [250] in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478].
Reference: [341] <author> Paul Morris. </author> <title> The breakout method for escaping from local minima. </title> <booktitle> In AAAI-93 [8], </booktitle> <pages> pages 40-45. 275 </pages>
Reference: [342] <author> A. N. Mucciardi and E. E. Gose. </author> <title> A comparison of seven techniques for choosing subsets of pattern recognition properties. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-20(9):1023-1031, </volume> <month> September </month> <year> 1971. </year>
Reference-contexts: Cover et al. [94, 484] showed that heuristic sequential feature selection methods can do arbitrarily worse than the optimal strategy. Mucciardi and Gose <ref> [342] </ref> compared seven feature subset selection techniques empirically and concluded that no technique was uniformly superior to the others. There has been a recent surge of interest in feature subset selection methods in the machine learning community, resulting in several empirical evaluations.
Reference: [343] <author> W. Muller and F. Wysotzki. </author> <title> Automatic construction of decision trees for classification. </title> <journal> Annals of Operations Research, </journal> <volume> 52:231, </volume> <year> 1994. </year>
Reference-contexts: The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in <ref> [134, 245, 486, 343] </ref>. Methods of discretization that operate on a single continuous attribute at a time can be said to be "local" discretization methods. In contrast, "global" discretization methods simultaneously convert all continuous attributes [81].
Reference: [344] <author> O. J. Murphy and R. L. McCraw. </author> <title> Designing storage efficient decision trees. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(3) </volume> <pages> 315-319, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: They show that even the problem of identifying the root node in an optimal strategy is NP-hard. Building optimal trees from decision tables, in terms of the size of the tree (number of nodes), is considered by Murphy and McCraw in <ref> [344] </ref>, who proved that for most cases, construction of storage optimal trees is NP-complete. Naumov [361] proved that optimal decision tree construction from decision tables is NP-complete under a variety of measures.
Reference: [345] <author> Patrick M. Murphy. </author> <title> An empirical analysis of the benefit of decision tree size biases as a function of concept distribution. </title> <note> Submitted to the Machine Learning journal, </note> <month> July </month> <year> 1994. </year>
Reference-contexts: Analysis: Murphy and Pazzani [347] empirically investigated the truth of this bias. Their experiments indicate that this conjecture seems to be true. However, their experiments indicate that the smallest decision trees typically have lesser generalization accuracy than trees that are slightly larger. In an extension of this study, Murphy <ref> [345] </ref> evaluated the size bias as a function of concept size.
Reference: [346] <author> Patrick M. Murphy and David Aha. </author> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases, </note> <year> 1994. </year>
Reference-contexts: The efficacy of multi-level decision trees is compared by Holte [209] to simple, one-level classification rules. He concluded that, on most real world data sets commonly used by the machine learning community <ref> [346] </ref>, decision trees do not perform significantly better than one level rules. These conclusions, however, were refuted by Elomaa [123] on several grounds. <p> This data represents a set of patients with breast cancer, where each patient was characterized by nine numeric attributes plus the diagnosis of the tumor as benign or malignant. The data set currently has 683 entries and is available from the UC Irvine machine learning repository <ref> [346] </ref>. Heath et al.[204] reported 94.9% accuracy on a subset of this data set (it then had only 470 instances), with an average decision tree size of 4.6 nodes, using SADT. Salzberg [421] reported 96.0% accuracy using 1-NN on the same (smaller) data set. <p> We then created training and tests sets for all of these concepts. We also experimented with seven real-world domains from the UCI machine learning repository <ref> [346] </ref>. Our experiments only consider one level lookahead in this chapter. Although deeper lookahead might also be interesting, it is prohibitively expensive for the experimental design we employed. 124 Section 4.1 describes related work on using lookahead to improve greedy search. Section 4.2 describes our experimental method. <p> We evaluated the effects of one-level lookahead on seven data sets taken from the University of California at Irvine repository of machine 129 learning databases <ref> [346] </ref>. If a greedy method can induce a highly accurate, concise classifier for a domain (the well-known Iris data is one such example), it is unlikely that we can observe significant benefits of lookahead on that domain. <p> This section describes experiments we did with seven data sets available in the UCI Machine Learning repository <ref> [346] </ref>. Brief descriptions of each of the real data sets we used are given below. A brief rationale for choosing these particular data sets, and our experimental method for real world domains are given in Section 4.2.2. BC Breast cancer recurrence data [38]. <p> [486]. 7.3 A Concrete Example This section illustrates that five decision tree methods (three axis-parallel and two oblique) benefit by using Euclidean minimum spanning tree (EMST) clustering as a preprocessing step, on the CB, RCB and RGC domains and two real-world data sets taken from the UCI machine learning repository <ref> [346] </ref>. 7.3.1 Minimum Spanning Tree Clustering A host of unsupervised clustering methods have been developed in the fields of psychology, statistics and machine learning. (See [127, 241] for a tutorial.) The choice of which clustering technique to use for a given data set is often very difficult. <p> Obviously, the success of the clustering method will affect the quality of the decision tree induced. 7.3.2 Experiments We have experimented with the synthetic data sets CB, RCB, RGC (Fig. 7.1), as well as two real-world data sets from UC Irvine machine learning repository <ref> [346] </ref>. The BUPA liver disorders data contains patients that have specific liver disorders. It has 345 instances, each described using 6 numeric attributes.
Reference: [347] <author> Patrick M. Murphy and Michael J. Pazzani. </author> <title> Exploring the decision forest: An empirical investigation of Occam's Razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 257-275, </pages> <year> 1994. </year>
Reference-contexts: Analysis: Murphy and Pazzani <ref> [347] </ref> empirically investigated the truth of this bias. Their experiments indicate that this conjecture seems to be true. However, their experiments indicate that the smallest decision trees typically have lesser generalization accuracy than trees that are slightly larger. <p> Murphy and Pazzani <ref> [347] </ref> used this style of experimentation 37 We chose Gini index and information gain because they have been widely used for real world applications.
Reference: [348] <author> Sreerama K. Murthy. </author> <title> Data exploration using decision trees: A survey. </title> <note> In preparation, 1995. http://www.cs.jhu.edu/grad/murthy. </note>
Reference: [349] <author> Sreerama K. Murthy. </author> <title> Using structure to improve decision trees. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 403-409. </pages>
Reference: [350] <author> Sreerama K. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In AAAI-93 [8], </booktitle> <pages> pages 322-327. </pages>
Reference-contexts: Hill Climbing Methods: CART's use of linear combinations of attributes ([44], Chapter 5) is well-known. This algorithm uses heuristic hill climbing and backward feature elimination to find good linear combinations at each node. Murthy et al. <ref> [350, 351] </ref> described significant extensions to CART's linear combinations algorithm, using randomized techniques. (See Chapter 3) Perceptron Learning: A perceptron is a linear function neuron [326, 188] which can be trained to optimize the sum of distances of the misclassified objects to it, using a convergent procedure for adjusting its coefficients. <p> Perturb (H; m) R-50: Repeat a fixed number of times (50 in our experiments): m = random integer between 1 and d + 1 Perturb (H; m) Our previous experiments <ref> [350] </ref> indicated that the order of perturbation of the coefficients does not affect the classification accuracy as much as other parameters, especially the randomization parameters (see below).
Reference: [351] <author> Sreerama K. Murthy, Simon Kasif, and Steven Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-33, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Gini index has been used for tree construction in statistics [44], pattern recognition [162] and sequential fault diagnosis [378]. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule <ref> [44, 351] </ref> as a remedy. Taylor and Silverman [470] pointed out that the Gini index emphasizes equal sized offspring and purity of both children. <p> Lubinsky [297, 298] also used the number of misclassified points as a splitting criterion, calling it inaccuracy. The performance of these measures does not seem to be in general as good as the information theory or distance based measures, and additional tricks are needed to make these measures robust <ref> [297, 351] </ref>. <p> Another measure suggested by Heath et al., called the sum of impurities, assigns an integer to each class and measures the variance between class numbers in each partition <ref> [204, 351] </ref>. 9 Quinlan's C4.5 [398] uses a naive version of the confidence intervals for doing pessimistic pruning. 25 An almost identical measure was used earlier in the Automatic Interaction Detection (AID) program [139]. <p> Hill Climbing Methods: CART's use of linear combinations of attributes ([44], Chapter 5) is well-known. This algorithm uses heuristic hill climbing and backward feature elimination to find good linear combinations at each node. Murthy et al. <ref> [350, 351] </ref> described significant extensions to CART's linear combinations algorithm, using randomized techniques. (See Chapter 3) Perceptron Learning: A perceptron is a linear function neuron [326, 188] which can be trained to optimize the sum of distances of the misclassified objects to it, using a convergent procedure for adjusting its coefficients.
Reference: [352] <author> Sreerama K. Murthy and Steven Salzberg. </author> <title> Clustering astronomical objects using minimum spanning trees. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Johns Hopkins University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: In this way, the MST connects all regions of like examples before making any connections between unlike examples. When the heaviest edges in an MST are cut, we obtain homogeneous clusters. (For more details of our clustering method, see <ref> [352] </ref>.) In most of our experiments, the clustering algorithm did not do a perfect job, but our clusters were distinct enough that the tree induction methods did quite well.
Reference: [353] <author> Sreerama K. Murthy and Steven Salzberg. </author> <title> Decision tree induction: How effective is the greedy heuristic? In Proceedings of the First International Conference on Knowledge Discovery in Databases, </title> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: They proved that mutual information-based induction is equivalent to a form of Shannon-Fano prefix coding, and through this insight argued that greedily induced trees are nearly optimal in terms of depth. This conjecture is substantiated empirically in <ref> [353] </ref>, where it is shown that the expected depth of trees greedily induced using information gain [391] and Gini index [44] is very close to that of the optimal, under a variety of experimental conditions.
Reference: [354] <author> Sreerama K. Murthy and Steven Salzberg. </author> <title> Lookahead and pathology in decision tree induction. </title> <note> In IJCAI-95 [223]. to appear. 276 </note>
Reference-contexts: Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning <ref> [365, 403, 354] </ref>. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. Murthy and Salzberg [354] argued that one-level lookahead does not help build significantly better trees, and that lookahead may actually worsen the quality of trees, causing pathology [360]. <p> Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. Murthy and Salzberg <ref> [354] </ref> argued that one-level lookahead does not help build significantly better trees, and that lookahead may actually worsen the quality of trees, causing pathology [360]. Constructing optimal or near-optimal decision trees using a two-stage approach has been attempted by many authors.
Reference: [355] <author> R. Musick, Jason Catlett, and S. Russell. </author> <title> Decision theoretic subsampling for induction on large databases. </title> <booktitle> In ML-93 [330], </booktitle> <pages> pages 212-219. </pages> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference: [356] <author> D. Mutchler. </author> <title> The multi-player version of minimax displays game pathology. </title> <journal> Artificial Intelligence, </journal> <volume> 64(2) </volume> <pages> 323-336, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: This property, where lookahead search finds inferior solutions, is known as pathology in the context of game trees <ref> [356, 360] </ref>. We discuss pathology for decision trees further in Section 4.3.2, where this trend is exhibited more prominently. Pathology cannot occur for tree size or depth for class C, because one-level lookahead is equivalent to exhaustive search. <p> However, it has been observed that for some games, deeper search can actually 156 produce an inferior program, both with two players [360] and with multiple players <ref> [356] </ref>. Decision trees, one can argue, are analogous to a one-player game tree. Our discovery that deeper search can lead to inferior decision trees thus extends the earlier pathology results to a new domain.
Reference: [357] <author> Y. Nakamura, S. Abe, Y. Ohsawa, and M. Sakauchi. </author> <title> A balanced hierarchical data structure for multidimensional data with highly efficient dynamic characteristics. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(4) </volume> <pages> 682-694, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: path length between the root and a leaf node, averaged over all the leaf nodes. (This measure is also used in the experiments in Chapter 5.) Although little if any work has been done on balancing decision trees, a great deal of research has considered balanced search trees (e.g., see <ref> [16, 79, 461, 93, 357] </ref>). Roughly speaking, this literature deals with techniques to restructure search trees when elements are 145 inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [358] <author> P. M. Narendra and K. Fukanaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26(9):917-922, </volume> <year> 1977. </year>
Reference-contexts: The first is a metric using which two feature subsets can be compared to determine which is better. Feature subsets have been compared in the literature using either a feature evaluation criterion discussed in Section 2.3.1 (e.g. Bhattacharya distance was used for comparing subsets of features in <ref> [358] </ref>), or using direct error estimation [148, 230]. The second component of feature subset selection methods is a search algorithm through the space of possible feature subsets. Most existing search procedures are heuristic in nature, 15 as exhaustive search for the best feature subset is typically prohibitively expensive.
Reference: [359] <author> S. C. Narula and J. F. Wellington. </author> <title> The minimum sum of absolute errors regression: a state of the art survey. </title> <journal> International Statistical Review, </journal> <volume> 50 </volume> <pages> 317-326, </pages> <year> 1982. </year>
Reference: [360] <author> Dana S. Nau. </author> <title> Decision quality as a function of search depth on game trees. </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> 30(4) </volume> <pages> 687-708, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: The existence of pathology <ref> [360] </ref> in the context of decision trees is reported for the first time, and several real and synthetic data sets for which lookahead hurts tree quality are presented. * A comprehensive, multidisciplinary survey of work on decision trees is presented whose coverage is broader than that of the existing surveys in <p> Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. Murthy and Salzberg [354] argued that one-level lookahead does not help build significantly better trees, and that lookahead may actually worsen the quality of trees, causing pathology <ref> [360] </ref>. Constructing optimal or near-optimal decision trees using a two-stage approach has been attempted by many authors. In the first stage, a sufficient partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. <p> with approximately the same classification accuracy and size as greedy induction, with slightly shorter longest paths. * Limited lookahead search produces inferior (less accurate, larger and/or deeper) decision trees in a significant number of cases; i.e., decision tree induction exhibits the same pathology that has been observed in game trees <ref> [360] </ref>. * Tree post-processing techniques such as pruning are at least as beneficial as limited lookahead for a variety of real-world data sets. Our empirical evaluations are based on both synthetic and real-world data sets. <p> This property, where lookahead search finds inferior solutions, is known as pathology in the context of game trees <ref> [356, 360] </ref>. We discuss pathology for decision trees further in Section 4.3.2, where this trend is exhibited more prominently. Pathology cannot occur for tree size or depth for class C, because one-level lookahead is equivalent to exhaustive search. <p> Intuitively, doing more search (lookahead) should produce better decision trees, just as deeper search in game trees (e.g., for chess) produces better game-playing programs. However, it has been observed that for some games, deeper search can actually 156 produce an inferior program, both with two players <ref> [360] </ref> and with multiple players [356]. Decision trees, one can argue, are analogous to a one-player game tree. Our discovery that deeper search can lead to inferior decision trees thus extends the earlier pathology results to a new domain.
Reference: [361] <author> G. E. Naumov. </author> <title> NP-completeness of problems of construction of optimal decision trees. </title> <journal> Soviet Physics, Doklady, </journal> <volume> 36(4) </volume> <pages> 270-271, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Building optimal trees from decision tables, in terms of the size of the tree (number of nodes), is considered by Murphy and McCraw in [344], who proved that for most cases, construction of storage optimal trees is NP-complete. Naumov <ref> [361] </ref> proved that optimal decision tree construction from decision tables is NP-complete under a variety of measures. The measures considered by the earlier papers on NP-completeness appear to be a subset of Naumov's measures, though he does not reference any of the existing work.
Reference: [362] <author> V. Nedeljkovic and M. Milosavljevic. </author> <title> On the influence of training set data preprocessing on neural networks training. </title> <booktitle> In Proceedings of the 11th International Conference on Pattern Recognition, </booktitle> <volume> volume II, </volume> <pages> pages 33-36, </pages> <address> Los Alamitos, CA, </address> <month> Septem-ber </month> <year> 1992. </year> <title> International Association for Pattern Recognition, </title> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Scatterd attempts exist in literature which qualify as examples of domain-independent data massaging. Nearest Neighbor classification is used to preprocess training data before using a neural network, in <ref> [362] </ref>. Flach [143] discusses inductive data engineering, an interactive process of restructuring a knowledge base by means of rule induction. Section 7.1 presents some "simple" artificial data sets for which several common goodness measures fail to produce good trees.
Reference: [363] <author> T. Niblett. </author> <title> Constructing decision trees in noisy domains. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning. </booktitle> <publisher> Sigma Press, </publisher> <address> England, </address> <year> 1986. </year>
Reference-contexts: Effects of noise on generalization are discussed in <ref> [363, 253] </ref>. Overfitting avoidance as a specific bias is studied in [507, 428]. Effect of noise on classification tree construction methods is studied in the pattern recognition literature in [468]. Several techniques have been suggested for obtaining the right sized trees.
Reference: [364] <author> N.J. Nilsson. </author> <title> Learning Machines. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: A method for building linear discriminant classification trees, in which the user can decide at each node what classes need to be split, is described in [472]. John [229] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines <ref> [364] </ref>, which are linear structures that can discriminate between multiple classes. In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes [49, 115]. <p> LMDT: Another oblique decision tree algorithm, one that uses a very different approach from CART-LC, is the Linear Machine Decision Trees (LMDT) system [483, 48], which is a successor to the Perceptron Tree method [480, 482]. Each internal node in an LMDT tree is a Linear Machine <ref> [364] </ref>. The training algorithm presents examples repeatedly at each node until the linear machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to determine when the node has stabilized.
Reference: [365] <author> Steven W. Norton. </author> <title> Generating better decision trees. </title> <booktitle> In IJCAI-89 [219], </booktitle> <pages> pages 800-805. </pages> <editor> Editor: N. S. </editor> <publisher> Sridharan. </publisher>
Reference-contexts: If the measurement (misclassification) costs are not identical between 43 different attributes (classes), decision tree algorithms need to be designed explicitly to prefer cheaper trees. Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see <ref> [365, 366, 469, 478] </ref> in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478]. <p> Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning <ref> [365, 403, 354] </ref>. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. Murthy and Salzberg [354] argued that one-level lookahead does not help build significantly better trees, and that lookahead may actually worsen the quality of trees, causing pathology [360]. <p> However, Hartmann et al. do not demonstrate that lookahead yields any improvements over greedy search. The ideas in GOTA motivated Norton's IDX system <ref> [365] </ref>, which is a variant of ID3 [391] that performs lookahead. Norton conducted experiments on a voting records database (see Section 4.4) using ID3, IDX and GOTA, and found that lookahead reduced the average decision tree depth. <p> In addition to these six "difficult" domains, we also experimented with two variants of the congressional voting records data used by Norton <ref> [365] </ref> for his lookahead experiments. Brief descriptions of all our real world domains, and the results of the experiments are given in Section 4.4. <p> LA Final settlements in labor negotiations in Canadian industry. 57 instances each de scribed using 16 features. LY Lymphography domain [304]. Contains 148 instances, each described using 19 at tributes, including the class attribute. VO 1984 United States congressional voting records database. This data is used by Norton <ref> [365] </ref> in his experiments. The data contains 435 instances, each described by 16 nominal attributes and one class label. The task is to classify democrats from republicans on the basis of their voting records. <p> In addition, we considered only one-level lookahead. One can attempt to evaluate the benefits of lookahead as a function of search depth. We feel that such a systematic evaluation is not only going to be computationally prohibitive, but also probably not very useful. Norton <ref> [365] </ref> presents experiments comparing one and two level lookahead for decision tree induction, on one particular data set. Another interesting question for further study is whether there exist effective goodness measures that guarantee no pathology. Section 4.3.4 described a decision tree rebalancing method.
Reference: [366] <author> M. N u ~ nez. </author> <title> The use of background knowledge in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 231-250, </pages> <year> 1991. </year>
Reference-contexts: If the measurement (misclassification) costs are not identical between 43 different attributes (classes), decision tree algorithms need to be designed explicitly to prefer cheaper trees. Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see <ref> [365, 366, 469, 478] </ref> in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478].
Reference: [367] <author> S.C. Odewahn, E.B. Stockwell, </author> <title> R.L. Pennington, R.M. Humphreys, and W.A. Zumach. Automated star-galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference-contexts: One can also divide objects into brightness ranges, and build separate classifiers for different brightness ranges. (The latter approach was taken in <ref> [367] </ref>.) At first, we tried several heuristic, empirically derived rules to filter out the faint objects. The following is an example of the kind of rules we considered: Rule: Retain only the objects that have a peak intensity value 20 in at least one color band.
Reference: [368] <author> J. Oliver. </author> <title> Decision graphs|an extension of decision trees. </title> <booktitle> In AI&Statistics-93 [4]. </booktitle> <pages> 277 </pages>
Reference-contexts: Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in [61, 62]. Oliver <ref> [368] </ref> suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [490]. Rymon [415] suggested SE-trees, set enumeration structures each of which can embed several decision trees.
Reference: [369] <author> Colm A. O'Muircheartaigh. </author> <title> Statistical analysis in the context of survey research. </title> <booktitle> In O'Muircheartaigh and Payne [370], </booktitle> <pages> pages 1-40. </pages>
Reference-contexts: Discovering whether the data contains well-separated clusters of objects, such that the clusters can be interpreted meaningfully in the context of a substantive theory. * Uncovering a mapping from independent to dependent variables that is useful for predicting the value of the dependent variable in the future. 1 Adapted from <ref> [369] </ref>, where a similar taxonomy was suggested in the general framework of search ing for structure in data. 11 Work related to automatically constructing and using decision trees for data description, classification and generalization exists in a wide variety of disciplines.
Reference: [370] <author> Colm A. O'Muircheartaigh and Clive Payne, </author> <title> editors. The analysis of survey data, volume I. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, UK, </address> <year> 1977. </year>
Reference: [371] <author> Giulia M. Pagallo. </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples. </title> <type> PhD thesis, </type> <institution> University of California, Computer Research Laboratory, </institution> <address> Santa Cruz, CA, </address> <month> June </month> <year> 1990. </year>
Reference: [372] <author> Giulia M. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-99, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Use of linear regression to find good feature combinations is explored recently in [36]. Discovery of good combinations of Boolean features to be used as tests at tree 42 nodes is explored in the machine learning literature in <ref> [372] </ref> as well as in signal processing [17]. Ragavan and Rendell [403] describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Looka-head for construction of Boolean feature combinations is also considered in [515].
Reference: [373] <author> Shailendra C. Palvia and Steven R. Gordon. </author> <title> Tables, trees and formulas in decision analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(10) </volume> <pages> 104-113, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Comparisons of symbolic and connectionist methods can be found in [501, 440]. Quinlan empirically compared decision trees to genetic classifiers [394] and to neural networks [400]. Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon <ref> [373] </ref> compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis. Multilayer perceptrons and CART (with and without linear combinations) [44] are compared in [12] to find that there is not much difference in accuracy.
Reference: [374] <author> Youngtae Park. </author> <title> A comparison of neural net classifiers and linear tree classifiers: Their similarities and differences. </title> <journal> Pattern Recognition, </journal> <volume> 27(11) </volume> <pages> 1493-1503, </pages> <year> 1994. </year>
Reference-contexts: Sethi [435] described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits. An extension of entropy nets, that 32 converts linear decision trees into neural nets was described in <ref> [374] </ref>. Decision trees with small multilayer networks at each node, implementing nonlinear, multivariate splits, were described in [184]. Jordan and Jacobs [233] described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [427].
Reference: [375] <author> Youngtae Park and Jack Sklansky. </author> <title> Automated design of linear tree classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 23(12) </volume> <pages> 1393-1412, </pages> <year> 1990. </year>
Reference-contexts: Foroutan [147] discovered that the resubstitution error rate of optimized piece-wise linear classifiers is 30 nearly monotonic with respect to the number of features. Based on this result, Foroutan and Sklansky [148] suggest an effective feature selection procedure for linear splits that uses zero-one integer programming. Park and Sklansky <ref> [375, 376] </ref> describe methods to induce linear tree classifiers and piece-wise linear discriminators. The main idea in these methods is to find hyperplanes that cut a maximal number of Tomek links.
Reference: [376] <author> Yountae Park and Jack Sklansky. </author> <title> Automated design of multiple-class piecewise linear classifiers. </title> <journal> Journal of Classification, </journal> <volume> 6 </volume> <pages> 195-222, </pages> <year> 1989. </year>
Reference-contexts: Foroutan [147] discovered that the resubstitution error rate of optimized piece-wise linear classifiers is 30 nearly monotonic with respect to the number of features. Based on this result, Foroutan and Sklansky [148] suggest an effective feature selection procedure for linear splits that uses zero-one integer programming. Park and Sklansky <ref> [375, 376] </ref> describe methods to induce linear tree classifiers and piece-wise linear discriminators. The main idea in these methods is to find hyperplanes that cut a maximal number of Tomek links.
Reference: [377] <author> A. Patterson and T. Niblett. </author> <title> ACLS user manual. </title> <type> Technical report, </type> <institution> MIRU, ITL, Universty of Edinburgh, </institution> <year> 1982. </year>
Reference: [378] <author> Krishna R. Pattipati and Mark G. Alexandridis. </author> <title> Application of heuristic search and information theory to sequential fault diagnosis. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 20(4) </volume> <pages> 872-887, </pages> <month> July/August </month> <year> 1990. </year>
Reference-contexts: The aim is to build a test algorithm that unambiguously identifies the occurrence of any system state using the given tests, while minimizing the total cost. The testing algorithms normally take the form of decision trees or AND/OR trees <ref> [488, 378] </ref>. Many heuristics used to construct decision trees are used for test sequencing also. Vector quantization (VQ) [167] is a data compression technique that has proved useful for image coding. Tree structured vector quantizers (TSVQ) [65] are structures very similar to decision trees. <p> A popular distance measure is the Gini index of diversity, named after the Italian economist Corrado Gini (1884-1965). Gini index has been used for tree construction in statistics [44], pattern recognition [162] and sequential fault diagnosis <ref> [378] </ref>. Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [44, 351] as a remedy. Taylor and Silverman [470] pointed out that the Gini index emphasizes equal sized offspring and purity of both children. <p> Bottom up induction of trees is considered in [275]. Bottom up tree induction is also common <ref> [378] </ref> in problems such as building identification keys and optimal test sequences. 18 A hybrid approach to tree construction, that combined top-down and bottom-up induction can be found in [247]. 18 Hierarchical unsupervised clustering can construct, using bottom-up or top-down methods, tree-structured classifiers.
Reference: [379] <author> R. W. Payne and D. A. Preece. </author> <title> Identification keys and diagnostic tables: A review. </title> <journal> Journal of the Royal Statistical Society: series A, </journal> <volume> 143:253, </volume> <year> 1980. </year>
Reference-contexts: In spite of a diverse body of literature on automatic construction of decision trees, there exist no comprehensive, multi-disciplinary surveys of up-to-date results on this topic (see Section 2.2 for discussion of existing surveys <ref> [379, 338, 417] </ref>). A characteristic of existing decision tree work seems to be a lack of directed progress. As most research on this subject is (perhaps needs to be) empirical, researchers and system developers typically try ad hoc variations of the basic methodology. <p> Our main emphasis is to trace the directions that decision tree work has taken. * We avoid repeating many of the references from <ref> [379, 338, 417] </ref>. <p> OBDDs have been used for digital system design, verification and testing. OBDDs are similar to decision trees, 5 and there exist several issues of common concern such as finding the minimal-sized representations. 2.2.2 Surveys Payne and Preece <ref> [379] </ref> surveyed results on constructing identification keys, in a paper that attempted "a synthesis of a large and widely-dispersed literature" from fields such as biology, pattern recognition, decision table programming, machine fault location, coding theory and questionnaire design.
Reference: [380] <author> Judea Pearl. </author> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> International Journal of General Systems, </journal> <volume> 4 </volume> <pages> 255-264, </pages> <year> 1978. </year>
Reference-contexts: For very large tree classifiers, the critical issue is optimizing structural properties (height, balance etc.) [493, 71]. 11 For a general discussion about the relationship between complexity and predictive accuracy of classifiers, see <ref> [380] </ref>. 34 Breiman et al. [44] pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [363, 253]. Overfitting avoidance as a specific bias is studied in [507, 428].
Reference: [381] <author> R. A. Pearson and P. E. </author> <title> Stokes. Vector evaluation in induction algorithms. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 2(1) </volume> <pages> 25-100, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Block diagrams can point out features of the data as well as the deficiencies in the classification method. Issues in preprocessing data to be in a form suitable to decision tree induction are discussed in some detail in [475]. Parallelization of tree induction algorithms is considered in <ref> [381] </ref>.
Reference: [382] <author> F. Pipitone, K. A. De Jong, and W. M. Spears. </author> <title> An artificial intelligence approach to analog systems diagnosis. </title> <editor> In Ruey-wen Liu, editor, </editor> <title> Testing and Diagnosis of Analog Circuits and Systems. </title> <publisher> Van Nostrand-Reinhold, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The problem of learning trees from decision rules instead of examples is addressed in [224]. The problem of learning trees solely from prior probability distributions is considered in [10]. Learning decision trees from qualitative causal models acquired from domain experts is the topic of <ref> [382] </ref>. Several attempts at generalizing the decision tree representation exist. Chou [82] considered decision trellises, where trellises are directed acyclic graphs with class probability vectors at the leaves and tests at internal nodes (i.e., trellises are trees in which internal nodes may have multiple parents).
Reference: [383] <author> Selwyn Piramuthu, Narayan Raman, and Michael J. Shaw. </author> <title> Learning-based scheduling in a flexible manufacturing flow line. </title> <journal> IEEE Transactions on Engineering Management, </journal> <volume> 41(2) </volume> <pages> 172-182, </pages> <month> May </month> <year> 1994. </year> <month> 278 </month>
Reference-contexts: [319]. * Manufacturing and Production: Decision trees have been recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines <ref> [383] </ref>, to uncover flaws in a Boeing manufacturing process [407] and for quality control [185].
Reference: [384] <author> N. J. Pizzi and D. Jackson. </author> <title> Comparitive review of knowledge engineering and inductive learning using data in a medical domain. </title> <booktitle> Proceedings of the SPIE: The International Society for Optical Engineering, </booktitle> <volume> 1293(2) </volume> <pages> 671-679, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Kors and Van Bemmel [262] compared statistical multivariate methods with heuristic decision tree methods, in the domain of electrocardiogram (ECG) analysis. Their comparisons show that decision tree classifiers are more comprehensible and flexible to incorporate or change existing categories. Pizzi and 60 Jackson <ref> [384] </ref> compare an expert systems developed using traditional knowledge engineering methods to Quinlan's ID3 [391] in the domain of tonsillectormy.
Reference: [385] <author> Lutz Prechelt. </author> <title> A study of experimental evaluations of neural network algorithms: Current research practice. </title> <type> Technical Report 19/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, </institution> <address> 76128 Karlsruhe, Germany, </address> <month> August </month> <year> 1994. </year> <note> anonymous FTP: /pub/papers/techreports/1994/1994-19.ps.Z on ftp.ira.uka.de. </note>
Reference: [386] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: EMSTs in two dimensions can be constructed in an optimal O (V log V ) time and O (V ) space by constructing the Delaunay triangulation of the points <ref> [386] </ref>. Unfortunately, this technique does not extend to higher-dimensional spaces, because the size of the Delaunay triangulation grows exponentially in the number of dimensions. It is easy to see how EMSTs can be a basis for clustering.
Reference: [387] <author> Shi Qing-Yun and King-Sun Fu. </author> <title> A method for the design of binary tree classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 16 </volume> <pages> 593-603, </pages> <year> 1983. </year>
Reference-contexts: Their method requires that the best set of features at each node be prespecified by a human. Friedman [152] reported that applying Fisher's linear discriminants, instead of atomic features, at some internal nodes was useful in building better trees. Qing-Yun and Fu <ref> [387] </ref> also describe a method to build linear discriminant trees. Their method uses multivariate stepwise regression to optimize the structure of the decision tree as well as to choose subsets of features to be used in the linear discriminants. <p> Lin and Fu [290] use K-means clustering for both stages, whereas Qing-Yun and Fu <ref> [387] </ref> use multi-variate stepwise regression for the first stage and linear discriminant analysis for the second stage. * Thresholds on Impurity: In this method, a threshold is imposed on the value of the splitting criterion, such that if the splitting criterion falls below (above) the threshold, tree growth is aborted.
Reference: [388] <author> John Ross Quinlan. </author> <title> Discovering rules by induction from large collections of examples. </title> <editor> In Donald Michie, editor, </editor> <booktitle> Expert Systems in the Micro Electronic Age. </booktitle> <publisher> Edinburgh University Press, Edinburgh, </publisher> <address> UK, </address> <year> 1979. </year>
Reference-contexts: Fast methods for splitting multiple valued categorical variables are described in [83]. In machine learning, a subfield of Artificial Intelligence, which in turn has been 33 dominated by symbolic processing, many tree induction methods (e.g. <ref> [388] </ref> were originally developed for categorical attributes. The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in [134, 245, 486, 343].
Reference: [389] <author> John Ross Quinlan. </author> <title> Learning efficient classification procedures and their application to chess end games. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference: [390] <author> John Ross Quinlan. </author> <title> The effect of noise on concept learning. </title> <editor> In R. S. Michal-ski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: The former alternative is used in <ref> [172, 413, 390, 312] </ref> and the latter in [437]. A problem with the former method is that the value of most splitting criteria (Section 2.3.1) varies with the size of the training sample.
Reference: [391] <author> John Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Decision trees in particular, and induction methods in general, arose in machine learning to avoid the knowledge acquisition bottleneck [137] for expert systems. A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s CART work [44] and Quinlan's ID3 algorithm <ref> [391] </ref>. Quinlan's book on C4.5 [398], although specific to his tree building program, is perhaps the best available overview of tree methodology from a machine learning perspective. <p> 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [197, 493, 70, 192], in sequential fault diagnosis [488] and in machine learning <ref> [391] </ref>. Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop. <p> Mingers [325] compared several attribute selection criteria, and concluded that tree quality doesn't seem to depend on the specific criterion used. He even claimed that random attribute selection criteria are as good as measures like information gain <ref> [391] </ref>. This later claim was refuted in [292], where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Babic et al. [15] compared ID3 [391] and CART [44], for two clinical diagnosis problems. <p> claimed that random attribute selection criteria are as good as measures like information gain <ref> [391] </ref>. This later claim was refuted in [292], where the authors argued that random attribute selection criteria are prone to overfitting, and also fail when there are several noisy attributes. Babic et al. [15] compared ID3 [391] and CART [44], for two clinical diagnosis problems. Miyakawa [329] compared three activity-based measures, Q, O and loss, both analytically and empirically. <p> He showed that Q and O do not chose non-essential variables at tree nodes, and that they produce trees that are 1/4th the size of the trees produced by loss. Fayyad and Irani [133] showed that their measure C-SEP, performs better than Gini index [44] and information gain <ref> [391] </ref> for specific types of problems. 27 Several researchers [195, 391] pointed out that information gain is biased towards attributes with a large number of possible values. Mingers [323] compared information gain and the 2 statistic for growing the tree as well as for stop-splitting. <p> Fayyad and Irani [133] showed that their measure C-SEP, performs better than Gini index [44] and information gain [391] for specific types of problems. 27 Several researchers <ref> [195, 391] </ref> pointed out that information gain is biased towards attributes with a large number of possible values. Mingers [323] compared information gain and the 2 statistic for growing the tree as well as for stop-splitting. <p> Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature [135, 157]. Trees in which an internal node can have more than 2 children, have also been considered in the vector quantization literature [431]. An extension to ID3 <ref> [391] </ref> that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in [88]. 2.4 Obtaining the right sized trees One of the main difficulties of inducing a recursive partitioning structure is knowing when to stop. <p> Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier [402, 149] . 2.5.10 Miscellaneous Most existing tree induction systems proceed in a greedy top-down fashion <ref> [464, 44, 391] </ref>. Bottom up induction of trees is considered in [275]. <p> This technique is based on the power spectrum coefficients of the n-dimensional Fourier transform of the function. Turksen and Zhao [477] proved the equivalence between a pseudo-Boolean analysis and the ID3 algorithm <ref> [391] </ref>. 56 2.6.4 Assumptions and biases Most tree induction methods are heuristic in nature. They use several assumptions and biases, hoping that together the heuristics produce good trees. <p> This conjecture is substantiated empirically in [353], where it is shown that the expected depth of trees greedily induced using information gain <ref> [391] </ref> and Gini index [44] is very close to that of the optimal, under a variety of experimental conditions. <p> Multilayer perceptrons and CART (with and without linear combinations) [44] are compared in [12] to find that there is not much difference in accuracy. Similar conclusions were reached in [142] when ID3 <ref> [391] </ref> and backpropagation were compared. Talmon et al. [467] compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other. In contrast, ID3 is adjudged to be slightly better than connectionist and Bayesian methods in [458]. <p> Their comparisons show that decision tree classifiers are more comprehensible and flexible to incorporate or change existing categories. Pizzi and 60 Jackson [384] compare an expert systems developed using traditional knowledge engineering methods to Quinlan's ID3 <ref> [391] </ref> in the domain of tonsillectormy. Comparisons of CART to multiple linear regression and discriminant analysis can be found in [66] where it is argued that CART is more suitable than the other methods for very noisy domains with lots of missing values. <p> sensing can be found in [416]. * Software development: Regression trees (and backpropagation networks) were recently used to estimate the development effort of a given software module in [266], where it is argued that machine learning methods compare favorably with traditional methods. * Text processing: A recent use of ID3 <ref> [391] </ref> for medical text classification can be found in [282]. * Miscellaneous: Decision trees have also been used recently for building personal learning assistants [327] and for classifying sleep signals [267]. 2.9 A word of caution The hierarchical, recursive tree construction methodology is simple and intuitively appealing. <p> However, Hartmann et al. do not demonstrate that lookahead yields any improvements over greedy search. The ideas in GOTA motivated Norton's IDX system [365], which is a variant of ID3 <ref> [391] </ref> that performs lookahead. Norton conducted experiments on a voting records database (see Section 4.4) using ID3, IDX and GOTA, and found that lookahead reduced the average decision tree depth. With a few exceptions, though, the advantages of lookahead were very small in Norton's experiments. <p> This is T's goodness. 3. Execute steps 3,4 of algorithm Greedy. We experimented with two pre-defined goodness measures, namely, the Gini index of diversity [44] and information gain <ref> [391] </ref>. 37 This gave us four algorithms, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [44] and Greedy-Info to the ID3 algorithm [391]. All our experiments measured tree quality in terms of four measures. <p> We experimented with two pre-defined goodness measures, namely, the Gini index of diversity [44] and information gain <ref> [391] </ref>. 37 This gave us four algorithms, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [44] and Greedy-Info to the ID3 algorithm [391]. All our experiments measured tree quality in terms of four measures. Accuracy is the classification accuracy on either an independently generated test set (for the synthetic domains) or obtained by cross validation (for the real world domains). Tree Size is the number of leaf nodes in a tree. <p> For the experiments with real world data, we used seven domains from the UCI 155 repository of machine learning databases. The tree induction methods we used are very similar to CART [44] and ID3 <ref> [391] </ref>. The benefits of limited lookahead search in all our experiments are marginal in spite of the enormous increase in the computational complexity. Greedy induction consistently produces trees that are as accurate and small as those produced with lookahead. <p> Greedy methods grow a decision tree by optimizing measures such as class entropy or diversity at each node of the tree (Section 2.3). However, as indicated by pathology, each such optimization is not necessarily improving the tree globally. It is commonly believed that information gain helps induce shallow trees <ref> [391, 403] </ref>. Our findings that Look-Info generally produces shallower trees than Greedy-Info are in accordance with this common belief. However, pathology in terms of maximum depth indicates that a split that optimizes information gain can in fact lead to a deeper tree. Finally, a word of caution.
Reference: [392] <author> John Ross Quinlan. </author> <title> Generating production rules from decision trees. </title> <booktitle> In Proceedings of Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 304-307, </pages> <address> Milan, Italy, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [393] <author> John Ross Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: Some feature evaluation rules, whose distribution does not depend on the number of training samples (i.e., a goodness value of k would have the same significance anywhere in the tree) have been suggested in the literature [286, 515, 235]. * Trees to rules conversion: Quinlan <ref> [393, 398] </ref> gave efficient procedures for converting a decision tree into a set of production rules. <p> Pruning set is a portion of the training data that is set aside exclusively for pruning alone. Use of a separate pruning set is a fairly common practice. A method other than cost complexity pruning that needs a separate pruning set is Quinlan's reduced error pruning <ref> [393] </ref>. This method, unlike cost complexity pruning, does not build a sequence of trees and 37 hence is claimed to be faster. Chou et al.[85] extended Breiman et al.'s pruning method to tree structured vector quantizers. <p> Several solutions have been suggested to get around this problem. Breiman et al. [44] describe a cross validation procedure that avoids reserving part of training data for pruning, but has a large computational complexity. Quinlan's pessimistic pruning <ref> [393, 398] </ref> does away with the need for a separate pruning set by using a statistical correlation test. Crawford [99] analyzed Breiman et al.'s cross validation procedure, and pointed out that it has a large variance, especially for small training samples.
Reference: [394] <author> John Ross Quinlan. </author> <title> An empirical comparison of genetic and decision tree classifiers. </title> <booktitle> In Fifth International Conference on Machine Learning, </booktitle> <pages> pages 135-141, </pages> <address> Ann Arbor, Michigan, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several researchers have compared trees to these other methods on specific problems. An early study comparing machine learning methods for learning from examples can be found in [112]. Comparisons of symbolic and connectionist methods can be found in [501, 440]. Quinlan empirically compared decision trees to genetic classifiers <ref> [394] </ref> and to neural networks [400]. Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis.
Reference: [395] <author> John Ross Quinlan. </author> <title> Unknown attribute values in induction. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 164-168, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Breiman et al.'s CART system [44] more or less implemented Friedman's suggestions. Quinlan also considered the problem of missing attribute values <ref> [395] </ref>. 44 2.5.4 Improving on greedy induction Most tree induction systems use a greedy approach | trees are induced top-down, a node at a time. Several authors (e.g., [159, 405]) pointed out the inadequacy of greedy induction for difficult concepts.
Reference: [396] <author> John Ross Quinlan. </author> <title> Decision trees and decisionmaking. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 20(2) </volume> <pages> 339-346, </pages> <month> March-April </month> <year> 1990. </year> <month> 279 </month>
Reference: [397] <author> John Ross Quinlan. </author> <title> Probabilistic decision trees. </title> <editor> In R.S.Michalski and Y. Ko-dratoff, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Volume 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes. Breiman et al. ([44], Chapter 4) proposed a method for building class probability trees. Quinlan discussed methods of extracting probabilities from decision trees in <ref> [397] </ref>. Buntine [62] described Bayesian methods for building, smoothing and averaging class probability trees. 16 Smoothing in the context of tree structured vector quantizers is described in [17].
Reference: [398] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: A majority of work on decision trees in machine learning is an offshoot of Breiman et al.'s CART work [44] and Quinlan's ID3 algorithm [391]. Quinlan's book on C4.5 <ref> [398] </ref>, although specific to his tree building program, is perhaps the best available overview of tree methodology from a machine learning perspective. In sequential fault diagnosis, a set of possible tests with associated costs and a set of system states with associated prior probabilities are given. <p> Another measure suggested by Heath et al., called the sum of impurities, assigns an integer to each class and measures the variance between class numbers in each partition [204, 351]. 9 Quinlan's C4.5 <ref> [398] </ref> uses a naive version of the confidence intervals for doing pessimistic pruning. 25 An almost identical measure was used earlier in the Automatic Interaction Detection (AID) program [139]. Most of the above feature evaluation criteria assume no knowledge of the probability distribution of the training objects. <p> He concluded that 2 corrected information gain's bias towards multivalued attributes, however to such an extent that they were never chosen, and the latter produced trees that were extremely deep and hard to interpret. Quinlan suggested gain ratio <ref> [398] </ref> as a remedy for the bias of information gain. Mantaras [310] argued that gain ratio had its own set of problems, and suggested using information theory-based distance between partitions for tree construction. He formally proved that his measure is not biased towards multiple-valued attributes. <p> Some feature evaluation rules, whose distribution does not depend on the number of training samples (i.e., a goodness value of k would have the same significance anywhere in the tree) have been suggested in the literature [286, 515, 235]. * Trees to rules conversion: Quinlan <ref> [393, 398] </ref> gave efficient procedures for converting a decision tree into a set of production rules. <p> Several solutions have been suggested to get around this problem. Breiman et al. [44] describe a cross validation procedure that avoids reserving part of training data for pruning, but has a large computational complexity. Quinlan's pessimistic pruning <ref> [393, 398] </ref> does away with the need for a separate pruning set by using a statistical correlation test. Crawford [99] analyzed Breiman et al.'s cross validation procedure, and pointed out that it has a large variance, especially for small training samples. <p> Subsample selection Feature subset selection attempts to choose useful features. Similarly, subsample selection attempts to choose appropriate training samples for induction. Quinlan suggested "win-dowing", a random training set sampling method, for his programs ID3 and C4.5 <ref> [398, 506] </ref>. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see [104, 105], for examples). <p> Several researchers have considered using soft splits of data for decision trees. A hard split divides the data into mutually exclusive partitions. A soft split, on the other hand, assigns a probability that each point belongs to a partition, thus allowing points to belong to multiple partitions. C4.5 <ref> [398] </ref> uses a simple form of soft splitting (chapter 8). 46 Use of fuzzy splits in pattern recognition literature can be found in [432, 494]. Jordan and Jacobs [233] describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [146]. <p> Their main conclusions are that (1) no method seems uniformly superior to others, (2) machine learning methods seem to be superior for multimodal distributions, and (3) statistical methods are computationally the most efficient. Long et al. [295] compared Quinlan's C4 <ref> [398] </ref> to logistic regression on the problem of diagnosing acute cardiac ischemia, and concluded that both methods came fairly close to the expertise of the physicians. In their experiments, logistic regression outperformed C4. <p> For axis-parallel splits, on the other hand, there are only n d distinct possibilities, and axis-parallel methods such as C4.5 <ref> [398] </ref> and CART [44] can exhaustively search for the best split at each node. The problem of searching for the best oblique split is therefore much more difficult than that of searching for the best axis-parallel split. <p> The cart0 style uses the Twoing Rule and 0-SE cost complexity pruning with 10-fold cross validation. The pruning method, impurity measure and other defaults of the c4.5 style are the same as those described in Quinlan <ref> [398] </ref>. The last method we included in our comparisons is a modification of OC1 that uses linear programming (LP) to find the split at each node. We call this variation OC1-LP. To implement this method, we replaced OC1's hyperplane-finding routine with a routine that formulates and solves a LP problem. <p> to perform? An alternative way of asking the same question is, what is the penalty that decision tree algorithms pay in return for the speed gained by the greedy heuristic? We quantify the goodness of greedy tree induction empirically in this 159 chapter, using the popular decision tree algorithms, C4.5 <ref> [398] </ref> and CART [44]. We induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees, which in turn are found using a novel map coloring idea. <p> Section 5.1 describes our experimental method and Section 5.2 presents the results. Section 5.3 provides general conclusions. 160 5.1 Experimental setup Our experimental framework is quite simple | we use C4.5 <ref> [398] </ref> and CART [44] to induce decision trees on a large number of random data sets, and in each case we compare the greedily induced tree to the optimal tree. The implementation of this framework raises some interesting issues. Optimal Decision Tree for a Training Set. <p> Tree Induction Methods Used. The tree induction methods we use are C4.5 <ref> [398] </ref> and CART [44]. One main difference between C4.5 and CART is the goodness criterion, the criterion used to choose the best split at each node. C4.5 uses the information gain 38 criterion, whereas CART uses either the Gini index of diversity or the twoing rule. <p> Now consider typical decision trees 221 induced on these data sets by existing tree induction methods. Figure 7.2 displays the decision trees generated for the CB data by C4.5 <ref> [398] </ref>. Figure 7.3 shows the trees generated by multivariate CART [44] and multivariate OC1 on the RCB data. Figure 7.4 displays the trees induced by C4.5 and multivariate CART on the RGC data. These figures show that some otherwise successful tree induction methods have trouble in these apparently simple domains. <p> The ION data [446] contains classifications of radar returns from the ionosphere. 351 observations, each with 34 continuous attributes, were classified as good or bad, depending on whether they were genuine or erroneous signals. The decision tree induction programs used in our experiments were C4.5 <ref> [398] </ref>, CART [44], and OC1 (Chapter 3). Both the univariate and multivariate versions of CART and OC1 were used, unless we knew the correct bias for a data set in advance. For example, on the CB data only univariate algorithms were considered. <p> We built decision trees on thousands of synthetic data sets using CART [44] and C4.5 <ref> [398] </ref>, and compared each one to the respective optimal tree. We found that, for a wide range of data characteristics, the greedy heuristic (along with pruning) produced decision trees whose expected classification cost was very close to the optimal. We changed our focus after Chapter 5.
Reference: [399] <author> John Ross Quinlan. </author> <title> Combining instance-based and model-based learning. </title> <booktitle> In ML-93 [330], </booktitle> <pages> pages 236-243. </pages> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference-contexts: The category variable (median value of owner-occupied homes) is actually continuous, but we discretized it so that category = 1 if value &lt; $21000, and 2 otherwise. For other uses of this data, see <ref> [20, 399] </ref>. Diabetes diagnosis. This data catalogs the presence or absence of diabetes among Pima Indian females, 21 years or older, as a function of eight numeric-valued attributes.
Reference: [400] <author> John Ross Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In S. Hanson, G. Drastal, and R. Rivest, editors, </editor> <title> Computational Learning Theory and Natural Learning Systems: Constraints and Prospects. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: An early study comparing machine learning methods for learning from examples can be found in [112]. Comparisons of symbolic and connectionist methods can be found in [501, 440]. Quinlan empirically compared decision trees to genetic classifiers [394] and to neural networks <ref> [400] </ref>. Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis.
Reference: [401] <author> John Ross Quinlan and R. M. Cameroon-Jones. </author> <title> Oversearching and layered-search in empirical learning. </title> <booktitle> In IJCAI-95 [223], </booktitle> <pages> pages 1019-1024. </pages> <editor> Editor: </editor> <publisher> Chris Mellish. </publisher>
Reference-contexts: The question that therefore arises is, what 36 Quinlan and Jones recently experimentally analyzed "oversearching" in the context of rule induction <ref> [401] </ref>. Their conclusions are similar in spirit to ours, namely that oversearching does not help, and can hurt. 123 are the benefits (if any) that we might gain from employing this more costly approach? In the current chapter, we attempt to answer this question empirically.
Reference: [402] <author> John Ross Quinlan and Ronald L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80(3) </volume> <pages> 227-248, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The activity of a variable is equal to the testing cost of the variable 24 times the a priori probability that it will be tested. The computational requirements for computing activity are the same as those for the information-based measures. Quinlan and Rivest <ref> [402] </ref> suggested the use of Risannen's minimum description length [408] for deciding which splits to prefer over others and also for pruning. <p> This algorithm divides the training sample into two halves and iteratively grows the tree using one half and prunes using the other half, exchanging the roles of the halves in each iteration. Several other pruning methods exist. Quinlan and Rivest <ref> [402] </ref> used minimum description length [408] for tree construction as well as for pruning. An error in their 13 In bootstrapping, B independent learning samples, each of size N are created by random sampling with replacement from the original learning sample L. <p> Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier <ref> [402, 149] </ref> . 2.5.10 Miscellaneous Most existing tree induction systems proceed in a greedy top-down fashion [464, 44, 391]. Bottom up induction of trees is considered in [275].
Reference: [403] <author> Harish Ragavan and Larry Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In ML-93 [330], </booktitle> <pages> pages 252-259. </pages> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference-contexts: Use of linear regression to find good feature combinations is explored recently in [36]. Discovery of good combinations of Boolean features to be used as tests at tree 42 nodes is explored in the machine learning literature in [372] as well as in signal processing [17]. Ragavan and Rendell <ref> [403] </ref> describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Looka-head for construction of Boolean feature combinations is also considered in [515]. Linear threshold unit trees for Boolean functions are described in [418]. <p> Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers [410], for Bayesian class probability trees [62], for neural trees [102] and in machine learning <ref> [365, 403, 354] </ref>. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction. Murthy and Salzberg [354] argued that one-level lookahead does not help build significantly better trees, and that lookahead may actually worsen the quality of trees, causing pathology [360]. <p> Moreover, since this study only considered a single data set, it is not clear how well these results generalize to other domains. Interesting approaches to slightly different problems include Ragavan and Rendell's Lookahead Feature Construction (LFC) algorithm <ref> [403] </ref>. This method uses lookahead to construct composite Boolean features, and uses the constructed features to induce concise 126 decision trees. This method is more efficient than methods like IDX because it caches the features found while looking ahead. <p> Greedy methods grow a decision tree by optimizing measures such as class entropy or diversity at each node of the tree (Section 2.3). However, as indicated by pathology, each such optimization is not necessarily improving the tree globally. It is commonly believed that information gain helps induce shallow trees <ref> [391, 403] </ref>. Our findings that Look-Info generally produces shallower trees than Greedy-Info are in accordance with this common belief. However, pathology in terms of maximum depth indicates that a split that optimizes information gain can in fact lead to a deeper tree. Finally, a word of caution.
Reference: [404] <author> C. R. Rao, </author> <title> editor. </title> <booktitle> Computational Statistics, volume 9 of Handbook of Statistics, </booktitle> <address> Amsterdam, 1993. </address> <publisher> Elsevier Publications, North-Holland Publishing Company. </publisher>
Reference: [405] <author> Larry Rendell and Harish Ragavan. </author> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based concept complexity. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 952-958. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Breiman et al.'s CART system [44] more or less implemented Friedman's suggestions. Quinlan also considered the problem of missing attribute values [395]. 44 2.5.4 Improving on greedy induction Most tree induction systems use a greedy approach | trees are induced top-down, a node at a time. Several authors (e.g., <ref> [159, 405] </ref>) pointed out the inadequacy of greedy induction for difficult concepts. The problem of inducing globally optimal decision trees has been addressed time and again. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338].
Reference: [406] <author> Alfred Renyi and Laszlo Vekerdi. </author> <title> Probability Theory. </title> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1970. </year>
Reference-contexts: Relationship between feature evaluation by Shannon's entropy and the probability of error is investigated in <ref> [263, 406] </ref>. 2.7 Comparisons with other exploration methods There exist several alternatives to decision trees for data exploration, such as neural networks, nearest neighbor methods and regression analysis. Several researchers have compared trees to these other methods on specific problems.
Reference: [407] <author> P. Riddle, R. Segal, and O. Etzioni. </author> <title> Representation design and brute-force induction in a Boeing manufacturing domain. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 125-147, </pages> <month> January-March </month> <year> 1994. </year>
Reference-contexts: recently used to non-destructively test welding quality [124], for semiconductor manufacturing [225], for increasing productivity [243], for material procurement method selection [103], to accelerate rotogravure printing [126], for process optimization in electrochemical machining [130], to schedule printed circuit board assembly lines [383], to uncover flaws in a Boeing manufacturing process <ref> [407] </ref> and for quality control [185]. For a recent review of the use of machine learning (decision trees and other techniques) in scheduling, see [14]. * Medicine: Medical research and practice have long been important areas of application for decision tree techniques.
Reference: [408] <author> Jorma Risannen. </author> <title> Stochastic Complexity in Statistica Enquiry. </title> <publisher> World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: The computational requirements for computing activity are the same as those for the information-based measures. Quinlan and Rivest [402] suggested the use of Risannen's minimum description length <ref> [408] </ref> for deciding which splits to prefer over others and also for pruning. <p> This algorithm divides the training sample into two halves and iteratively grows the tree using one half and prunes using the other half, exchanging the roles of the halves in each iteration. Several other pruning methods exist. Quinlan and Rivest [402] used minimum description length <ref> [408] </ref> for tree construction as well as for pruning. An error in their 13 In bootstrapping, B independent learning samples, each of size N are created by random sampling with replacement from the original learning sample L.
Reference: [409] <author> Eve A. Riskin and Robert M. Gray. </author> <title> A greedy tree growing algorithm for the design of variable rate vector quantizers. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39(11) </volume> <pages> 2500-2507, </pages> <month> November </month> <year> 1991. </year> <month> 280 </month>
Reference-contexts: Makhoul et al.[306] introduced an unbalanced tree algorithm that grew the tree a node at a time. Riskin and Gray <ref> [409] </ref> proposed a greedy method for TSVQs, which is directly related to decision tree growing. 6 2.3.1 Feature evaluation rules When used for classification or generalization, decision trees are essentially probability estimators. <p> The TSVQ growing procedure suggested by Riskin and Gray <ref> [409] </ref> can be viewed as an inverse to Chou's pruning procedure. 7 The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. Shannon's entropy [439] possesses all of these properties [2].
Reference: [410] <author> Eve A. Riskin and Robert M. Gray. </author> <title> Lookahead in growing tree-structured vector quantizers. </title> <booktitle> In ICASSP 91: International Conference on Accoustics, Speech and Signal Processing, </booktitle> <volume> volume 4, </volume> <pages> pages 2289-2292, </pages> <address> Toronto, Ontario, </address> <month> May 14th-17th </month> <year> 1991. </year> <note> IEEE. </note>
Reference-contexts: For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see [338]. Tree construction using partial or exhaustive lookahead has been considered in statistics [139, 122], in pattern recognition [197], for tree structured vector quantizers <ref> [410] </ref>, for Bayesian class probability trees [62], for neural trees [102] and in machine learning [365, 403, 354]. Most of these studies indicate that lookahead does not cause considerable improvements over greedy induction.
Reference: [411] <author> G. Ritter, H. Woodruff, S. Lowry, and T. Isenhour. </author> <title> An algorithm for a selective nearest neighbor decision rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21(6) </volume> <pages> 665-669, </pages> <year> 1975. </year>
Reference: [412] <author> Richard H. Roth. </author> <title> An approach to solving linear discrete optimization problems. </title> <journal> Journal of the ACM, </journal> <volume> 17(2) </volume> <pages> 303-313, </pages> <month> April </month> <year> 1970. </year>
Reference-contexts: See Section 3.4.3 for some examples. The second technique for avoiding local minima is a variation on the idea of performing multiple local searches. The technique of multiple local searches is a natural extension to local search, and has been widely mentioned in the optimization literature (see Roth <ref> [412] </ref> for an early example). Because most of the steps of our perturbation algorithm are deterministic, the initial hyperplane largely determines which local minimum will be encountered first. Perturbing a single initial hyperplane is thus unlikely to lead to the best split of a given data set.
Reference: [413] <author> E. </author> <title> Rounds. A combined non-parametric approach to feature selection and binary decision tree design. </title> <journal> Pattern Recognition, </journal> <volume> 12 </volume> <pages> 313-317, </pages> <year> 1980. </year>
Reference-contexts: criterion, called mean posterior improvement (MPI), that emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance <ref> [152, 413, 198] </ref> and the 2 statistic [21, 195, 323, 515, 503] are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> The former alternative is used in <ref> [172, 413, 390, 312] </ref> and the latter in [437]. A problem with the former method is that the value of most splitting criteria (Section 2.3.1) varies with the size of the training sample.
Reference: [414] <author> Steven Rovnyak, Stein Kretsinger, James Thorp, and Donald Brown. </author> <title> Decision trees for real time transient stability prediction. </title> <journal> IEEE Transactions on Power Systems, </journal> <volume> 9(3) </volume> <pages> 1417-1426, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality to pine trees [19]. * Power systems: Power system security assessment [199] and power stability prediction <ref> [414] </ref> are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see [464, 247] ).
Reference: [415] <author> Ron Rymon. </author> <title> An SE-tree based characterization of the induction problem. </title> <booktitle> In ML-93 [330], </booktitle> <pages> pages 268-275. </pages> <editor> Editor: Paul E. </editor> <publisher> Utgoff. </publisher>
Reference-contexts: Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in [61, 62]. Oliver [368] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles [490]. Rymon <ref> [415] </ref> suggested SE-trees, set enumeration structures each of which can embed several decision trees. All standard decision tree methods are applicable when rules are to be induced about one aspect, say, the presence or absence of a disease.
Reference: [416] <author> Ron Rymon and N. M. Short, Jr. </author> <title> Automatic cataloging and characterization of earth science data using set enumeration trees. </title> <journal> Telematics and Informatics, </journal> <volume> 11(4) </volume> <pages> 309-318, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: A recent use of tree-based classi 64 fication in remote sensing can be found in <ref> [416] </ref>. * Software development: Regression trees (and backpropagation networks) were recently used to estimate the development effort of a given software module in [266], where it is argued that machine learning methods compare favorably with traditional methods. * Text processing: A recent use of ID3 [391] for medical text classification can
Reference: [417] <author> S. Rasoul Safavin and David Landgrebe. </author> <title> A survey of decision tree classifier methodology. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21(3) </volume> <pages> 660-674, </pages> <month> May/June </month> <year> 1991. </year>
Reference-contexts: In spite of a diverse body of literature on automatic construction of decision trees, there exist no comprehensive, multi-disciplinary surveys of up-to-date results on this topic (see Section 2.2 for discussion of existing surveys <ref> [379, 338, 417] </ref>). A characteristic of existing decision tree work seems to be a lack of directed progress. As most research on this subject is (perhaps needs to be) empirical, researchers and system developers typically try ad hoc variations of the basic methodology. <p> Many topics that were not discussed in the existing surveys (e.g., multivariate trees) are also covered. * This chapter considers decision tree work in multiple disciplines in contrast to existing surveys that concentrated on specific disciplines (e.g., Safavin and Langrebe's survey <ref> [417] </ref> covers work mostly from the pattern recognition literature). * Our main emphasis is on automatically constructing decision trees for parsimonious descriptions of, and generalization from, data. (In contrast, Moret's [338] main em phasis was on representing Boolean functions as decision trees.) * A significant portion of this survey is devoted <p> Our main emphasis is to trace the directions that decision tree work has taken. * We avoid repeating many of the references from <ref> [379, 338, 417] </ref>. <p> Though Moret does mention some pattern recognition 5 Oblivious decision trees [257] from the machine learning literature are nearly identical in structure to OBDDs. 20 work on constructing decision trees from data, this was not his main emphasis. Safavin and Landgrebe <ref> [417] </ref> more recently summarized decision tree construction methodology, almost entirely from a pattern recognition perspective. Bryant [53] surveyed the methodology and applications of ordered binary decision diagrams. 2.3 Finding splits To build a decision tree, it is necessary to find, at each internal node, a split for the data.
Reference: [418] <author> M. Sahami. </author> <title> Learning non-linearly separable boolean functions with linear threshold unit trees and madaline-style networks. </title> <booktitle> In AAAI-93 [8], </booktitle> <pages> pages 335-341. </pages>
Reference-contexts: Ragavan and Rendell [403] describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Looka-head for construction of Boolean feature combinations is also considered in [515]. Linear threshold unit trees for Boolean functions are described in <ref> [418] </ref>. Decision trees having first order predicate calculus representations, with Horn clauses as tests at internal nodes, are considered in [497]. Subsample selection Feature subset selection attempts to choose useful features. Similarly, subsample selection attempts to choose appropriate training samples for induction.
Reference: [419] <author> Sartaj Sahni. </author> <title> Approximate algorithms for the 0/1 knapsack problem. </title> <journal> Journal of the ACM, </journal> <volume> 22 </volume> <pages> 115-124, </pages> <year> 1975. </year>
Reference-contexts: Examples of the latter variety include methods for constructing minimum spanning trees [178] and methods for producing optimal Huffman codes for data compression [283]. Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem <ref> [419] </ref>, multiprocessor scheduling [210] and the problem under consideration here, decision tree induction. For these problems, greedy polynomial algorithms obviously can not guarantee optimal solutions, assuming P 6= N P .
Reference: [420] <author> Steven Salzberg. </author> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, </booktitle> <pages> pages 399-408, </pages> <year> 1991. </year>
Reference: [421] <author> Steven Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276, </pages> <year> 1991. </year>
Reference-contexts: The data set currently has 683 entries and is available from the UC Irvine machine learning repository [346]. Heath et al.[204] reported 94.9% accuracy on a subset of this data set (it then had only 470 instances), with an average decision tree size of 4.6 nodes, using SADT. Salzberg <ref> [421] </ref> reported 96.0% accuracy using 1-NN on the same (smaller) data set. Herman and Yeung [207] reported 99.0% accuracy using piece-wise linear classification, again using a somewhat smaller data set.
Reference: [422] <author> Steven Salzberg. </author> <title> Combining learning and search to create good classifiers. </title> <type> Technical Report JHU-92/12, </type> <institution> Department of Computer Science, Johns Hopkins University, Baltimore MD, </institution> <year> 1992. </year> <month> 281 </month>
Reference-contexts: Heath [202] reported 99.0% accuracy on the bright objects using SADT, with an average tree size of 7.03 leaves. This study also used a single training and test set. Salzberg <ref> [422] </ref> reported accuracies of 98.8% on the bright objects, and 95.1% on the dim objects, using 1-Nearest Neighbor (1-NN) coupled with a feature selection method that reduces the number of features. 105 Breast Cancer Diagnosis.
Reference: [423] <author> Steven Salzberg. </author> <title> Locating protein coding regions in human DNA using a decision tree algorithm. </title> <journal> Journal of Computational Biology, </journal> <note> 1995. To appear in Fall. </note>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in [442] and <ref> [423] </ref>. * Object recognition: Tree based classification has been used recently for recognizing three dimensional objects [456, 57] and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical
Reference: [424] <author> Steven Salzberg, Rupali Chandar, Holland Ford, Sreerama Murthy, and Rick White. </author> <title> Decision trees for automated identification of cosmic-ray hits in Hub-ble Space Telescope images. </title> <journal> Publications of the Astronomical Society of the Pacific, </journal> <volume> 107 </volume> <pages> 1-10, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: In tasks where more features than the "optimal" are available, decision tree quality is known to be affected by the redundant and irrelevant attributes <ref> [6, 424] </ref>. To avoid this problem, either a feature subset selection method (Section 2.5.1) or a method to form a small set of composite features (Section 2.5.1) can be used as a preprocessing step to tree induction. <p> an active domain for using automated classification techniques. 22 Use of decision trees for filtering noise from Hubble Space Tele-22 For a general description of modern classification problems in astronomy, which prompt the use of pattern recognition and machine learning techniques, see [269]. 62 scope images was reported recently in <ref> [424] </ref>. Decision trees have helped in star-galaxy classification [500], determining galaxy counts [499] and discovering quasars [244] in the Second Palomar Sky Survey. <p> Some results with other classification methods on these data sets can be found in <ref> [424] </ref>. 6.1 Cosmic ray hits in Hubble Space Telescope images This section describes the results of an effort to identify accurately the types of noise commonly present in Hubble Space Telescope (HST) data, especially cosmic ray hits, which 182 are very common in the Wide Field Planetary Camera (WFPC) images.
Reference: [425] <author> Anant Sankar and Richard J. Mammone. </author> <title> Growing and pruning neural tree networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3) </volume> <pages> 291-299, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks.
Reference: [426] <author> U. K. Sarkar, P. P. Chakrabarti, S. Ghose, and S. C. DeSarkar. </author> <title> Improving greedy algorithms by lookahead-search. </title> <journal> Journal of Algorithms, </journal> <volume> 16(1) </volume> <pages> 1-23, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: As the greedy approach can produce suboptimal trees [174], it is naturally of interest to explore ways to improve the greedy strategy. Fixed-depth lookahead search is a standard technique for improving greedy algorithms <ref> [426] </ref>. Though scattered uses of lookahead exist in the literature (Section 2.5.4), there have not been any systematic evaluations (analytical or empirical).
Reference: [427] <author> Lawrence Saul and Michael I. Jordan. </author> <title> Learning in Boltzmann trees. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1174-1184, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Decision trees with small multilayer networks at each node, implementing nonlinear, multivariate splits, were described in [184]. Jordan and Jacobs [233] described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in <ref> [427] </ref>. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in [432]. In information theory, Gelfand and Ravishanker [161] describe a method to build a tree structured filter that has linear processing elements at internal nodes.
Reference: [428] <author> Cullen Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: Effects of noise on generalization are discussed in [363, 253]. Overfitting avoidance as a specific bias is studied in <ref> [507, 428] </ref>. Effect of noise on classification tree construction methods is studied in the pattern recognition literature in [468]. Several techniques have been suggested for obtaining the right sized trees. The most popular of these is pruning, whose discussion we will defer to Section 2.4.1.
Reference: [429] <author> Cullen Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In ML-94 [331], </booktitle> <pages> pages 259-265. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: This is to be expected as induction per se can not rigorously justify performance on unseen instances. Any strategy that results in superior generalization accuracy on some problems is bound to have inferior performance on some other problems. 10 Of course, comparisons 10 Schaffer <ref> [429] </ref> stated and proved a conservation theorem that states, essentially, that positive performance in some learning situations must be offset by an equal degree of negative performance in others.
Reference: [430] <author> Cullen Schaffer. </author> <title> Conservation of generalization: A case study. </title> <type> Technical report, </type> <institution> Department of Computer Science, CUNY/Hunter College, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: To clarify the, sometimes non-intuitive, consequences of the conservation theorem, Schaffer <ref> [430] </ref> gave an example of a concept for which information loss gives better generalization accuracy than information gain. 26 of individual methods are still interesting because they throw light on which method can be used in what situations.
Reference: [431] <author> T. M. Schmidl, P. C. Cosman, and Robert M. Gray. </author> <title> Unbalanced non-binary tree-structured vector quantizers. </title> <editor> In A. Singh, editor, </editor> <booktitle> Conference Record of the Twenty-Seventh Asilomar Conference on Signals, Systems and Computers, </booktitle> <volume> volume 2, </volume> <pages> pages 1519-1523, </pages> <address> Los Alamitos, CA, November 1st-3rd 1993. </address> <publisher> IEEE Computer Society Press. Conf. </publisher> <address> held at Pacific Grove, CA. </address>
Reference-contexts: Fast methods for splitting a continuous dimension into more than two ranges is considered in the machine learning literature [135, 157]. Trees in which an internal node can have more than 2 children, have also been considered in the vector quantization literature <ref> [431] </ref>. An extension to ID3 [391] that distinguishes between attributes with unordered domains and attributes with linearly ordered domains is suggested in [88]. 2.4 Obtaining the right sized trees One of the main difficulties of inducing a recursive partitioning structure is knowing when to stop.
Reference: [432] <author> J. Schuermann and W. Doster. </author> <title> A decision-theoretic approach in hierarchical classifier design. </title> <journal> Pattern Recognition, </journal> <volume> 17 </volume> <pages> 359-369, </pages> <year> 1984. </year>
Reference-contexts: Jordan and Jacobs [233] described hierarchical parametric classifiers with small "experts" at internal nodes. Training methods for tree structured Boltzmann machines are described in [427]. Other Methods: Use of polynomial splits at tree nodes is explored in decision theory in <ref> [432] </ref>. In information theory, Gelfand and Ravishanker [161] describe a method to build a tree structured filter that has linear processing elements at internal nodes. Heath et al. [204, 202] used simulated annealing to find the best oblique split at each tree node. <p> A soft split, on the other hand, assigns a probability that each point belongs to a partition, thus allowing points to belong to multiple partitions. C4.5 [398] uses a simple form of soft splitting (chapter 8). 46 Use of fuzzy splits in pattern recognition literature can be found in <ref> [432, 494] </ref>. Jordan and Jacobs [233] describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [146]. Induction of fuzzy decision trees has also been considered in [281, 512]. 2.5.6 Estimating probabilities Decision trees have crisp decisions at leaf nodes.
Reference: [433] <author> Richard W. Selby and Adam A. Porter. </author> <title> Learning from examples: Generation and evaluation of decision trees for software resource analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(12) </volume> <pages> 1743-1757, </pages> <month> December </month> <year> 1988. </year>
Reference: [434] <author> Bart Selman and Henry A. Kautz. </author> <title> An empirical study of greedy local search for satisfiability testing. </title> <booktitle> In AAAI-93 [8], </booktitle> <pages> pages 46-51. </pages>
Reference-contexts: The experiment in Section 3.4.4 was intended to serve this purpose, but several alternate experiments can be designed. Interesting analyses of search methods, though not in the context of decision tree induction, can be found in <ref> [239, 166, 434] </ref>. 122 Chapter 4 Limited lookahead search The standard algorithm for constructing decision trees from a set of examples is greedy induction | a tree is induced top-down with locally optimal choices made at each node, without lookahead or backup.
Reference: [435] <author> Ishwar Krishnan Sethi. </author> <title> Entropy nets: From decision trees to neural networks. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10), </volume> <month> October </month> <year> 1990. </year> <month> 282 </month>
Reference-contexts: Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks. Sethi <ref> [435] </ref> described a method for converting a univariate decision tree into a neural net and then retraining it, resulting in tree structured entropy nets with sigmoidal splits. An extension of entropy nets, that 32 converts linear decision trees into neural nets was described in [374].
Reference: [436] <author> Ishwar Krishnan Sethi and B. Chatterjee. </author> <title> Efficient decision tree design for discrete variable pattern recognition problems. </title> <journal> Pattern Recognition, </journal> <volume> 9 </volume> <pages> 197-206, </pages> <year> 1977. </year>
Reference-contexts: This seems natural considering application domains such as spectral analysis and remote sensing [464]. In these fields, special techniques <ref> [436] </ref> were developed to accommodate discrete attributes into what are primarily algorithms for ordered attributes. Fast methods for splitting multiple valued categorical variables are described in [83].
Reference: [437] <author> Ishwar Krishnan Sethi and G.P.R. Sarvarayudu. </author> <title> Hierarchical classifier design using mutual information. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-4(4):441-445, </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: Shannon's entropy [439] possesses all of these properties [2]. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition <ref> [172, 437, 465] </ref>. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [197, 493, 70, 192], in sequential fault diagnosis [488] and in machine learning [391]. <p> The former alternative is used in [172, 413, 390, 312] and the latter in <ref> [437] </ref>. A problem with the former method is that the value of most splitting criteria (Section 2.3.1) varies with the size of the training sample. Imposing a single threshold that is meaningful at all nodes in the tree is not easy and may not even be possible.
Reference: [438] <author> Ishwar Krishnan Sethi and J. H. Yoo. </author> <title> Design of multicategory, multifeature split decision trees using perceptron learning. </title> <journal> Pattern Recognition, </journal> <volume> 27(7) </volume> <pages> 939-947, </pages> <year> 1994. </year>
Reference-contexts: Perceptron trees, which are decision trees with perceptrons just above the leaf nodes, were discussed in [480]. Decision trees with perceptrons at all internal nodes were described in <ref> [482, 438] </ref>. Mathematical Programming: Linear programming has been used for building adaptive classifiers since late 1960s [216]. Given two possibly interesecting sets of points, Duda 31 and Hart [117] proposed a linear programming formulation for finding the split whose distance from the misclassified points is minimized.
Reference: [439] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27 379-423,623-656, </volume> <year> 1948. </year>
Reference-contexts: The TSVQ growing procedure suggested by Riskin and Gray [409] can be viewed as an inverse to Chou's pruning procedure. 7 The desirable properties of a measure of entropy include symmetry, expandability, decisivity, additivity and recursivity. Shannon's entropy <ref> [439] </ref> possesses all of these properties [2].
Reference: [440] <author> Jude W. Shavlik, R. J. Mooney, and G. G. Towell. </author> <title> Symbolic and neural learning algorithms: An empirical comparison. </title> <journal> Machine Learning, </journal> <volume> 6(2) </volume> <pages> 111-144, </pages> <year> 1991. </year>
Reference-contexts: Several researchers have compared trees to these other methods on specific problems. An early study comparing machine learning methods for learning from examples can be found in [112]. Comparisons of symbolic and connectionist methods can be found in <ref> [501, 440] </ref>. Quinlan empirically compared decision trees to genetic classifiers [394] and to neural networks [400]. Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis.
Reference: [441] <author> Sheldon B. Akers. </author> <title> Binary decision diagrams. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(6):509-516, </volume> <month> June </month> <year> 1978. </year>
Reference-contexts: Tree structured vector quantizers (TSVQ) [65] are structures very similar to decision trees. A lot of work exists in the speech and signal processing literature, 19 on building and analyzing TSVQs. A Binary Decision Diagram (BDD) represents a Boolean function as a rooted, directed acyclic graph <ref> [279, 441] </ref>. Ordered binary decision diagrams (OBDD) [52, 53] impose restrictions on the ordering of variables at the nodes of a BDD. OBDDs have been used for digital system design, verification and testing.
Reference: [442] <author> S. Shimozono, A. Shinohara, T. Shinohara, S. Miyano, S. Kuhara, and S. </author> <title> Arikawa. Knowledge acquisition from amino acid sequences by machine learning system BONSAI. </title> <journal> Transactions of the Information Processing Society of Japan, </journal> <volume> 35(10) </volume> <pages> 2009-2018, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in <ref> [442] </ref> and [423]. * Object recognition: Tree based classification has been used recently for recognizing three dimensional objects [456, 57] and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection
Reference: [443] <author> Seymour Shlien. </author> <title> Multiple binary decision tree classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 23(7) </volume> <pages> 757-763, </pages> <year> 1990. </year>
Reference-contexts: A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [274, 443, 444, 62, 203] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree [443, 444]. <p> The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree <ref> [443, 444] </ref>. Classification results of the trees have been combined using either simplistic voting methods [203] or using statistical methods for combining evidence [443]. 2.5.8 Incremental tree induction Most tree induction algorithms use batch training | the entire tree needs to be recomputed to accommodate a new training example. <p> Classification results of the trees have been combined using either simplistic voting methods [203] or using statistical methods for combining evidence <ref> [443] </ref>. 2.5.8 Incremental tree induction Most tree induction algorithms use batch training | the entire tree needs to be recomputed to accommodate a new training example. A crucial property of neural network training methods is that they are incremental | network weights can be continually adjusted to accommodate training examples.
Reference: [444] <author> Seymour Shlien. </author> <title> Nonparametric classification using matched binary decision trees. </title> <journal> Pattern Recognition Letters, </journal> <volume> 13(2) </volume> <pages> 83-88, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: A few authors suggested using a collection of decision trees, instead of just one, to reduce the variance in classification performance <ref> [274, 443, 444, 62, 203] </ref>. The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree [443, 444]. <p> The idea is to build a set of (correlated or uncorrelated) trees for the same training sample, and then combine their results. 17 Multiple trees have been built using randomness [203] or using different subsets of attributes for each tree <ref> [443, 444] </ref>. Classification results of the trees have been combined using either simplistic voting methods [203] or using statistical methods for combining evidence [443]. 2.5.8 Incremental tree induction Most tree induction algorithms use batch training | the entire tree needs to be recomputed to accommodate a new training example.
Reference: [445] <author> W. Siedlecki and J. Skalansky. </author> <title> On automatic feature selection. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2(2) </volume> <pages> 197-220, </pages> <year> 1988. </year>
Reference-contexts: In stepwise backward elimination, we start with the full feature set and remove, at each step, the worst feature. When more than one feature is greedily added or removed, beam search is said to have been performed <ref> [445, 69] </ref>. <p> When more than one feature is greedily added or removed, beam search is said to have been performed [445, 69]. A combination of forward selection and backward elimination, a bidirectional search, was attempted in <ref> [445] </ref>. 15 An exception is the optimal feature subset selection method using zero-one integer programming, suggested by Ichino and Sklansky [217]. 41 Comparisons of heuristic feature subset selection methods resound the conclusions of studies comparing feature evaluation criteria and studies comparing pruning methods | no feature subset selection heuristic is far
Reference: [446] <author> V.G. Sigiletto, S.P. Wing, L.V. Hutton, and K.B. Baker. </author> <title> Classification of radar returns from the ionosphere using neural networks. </title> <booktitle> In Johns Hopkins APL Technical Digest, </booktitle> <pages> pages 262-266, </pages> <year> 1989. </year>
Reference-contexts: The BUPA liver disorders data contains patients that have specific liver disorders. It has 345 instances, each described using 6 numeric attributes. The ION data <ref> [446] </ref> contains classifications of radar returns from the ionosphere. 351 observations, each with 34 continuous attributes, were classified as good or bad, depending on whether they were genuine or erroneous signals. The decision tree induction programs used in our experiments were C4.5 [398], CART [44], and OC1 (Chapter 3).
Reference: [447] <author> Herbert Simon. </author> <note> Artificial intelligence as an experimental science. In AAAI-93 [8], page 853. Invited Talk. </note>
Reference-contexts: Almost all of this thesis has an experimental 6 flavor. The investigations are empirical in nature, where carefully designed and controlled experiments form the basis of observations and conclusions. Experimental analysis has historically been a predominant form of AI research <ref> [447] </ref>. One of the grand goals of AI has been to emulate and comprehend human intelligence. A reader with such a perspective might expect machine learning systems to imitate human learning. Adaptive processes in humans in particular, and biological organisms in general, are fascinating.
Reference: [448] <author> J.A. Sirat and J.-P. Nadal. </author> <title> Neural trees: A new tool for classification. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 1(4) </volume> <pages> 423-438, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Though these techniques were developed as neural networks whose structure could be automatically determined, their outcome can be interpreted as decision trees with nonlinear splits. Examples of this work include <ref> [173, 448, 46, 87, 207, 425, 102] </ref>. Techniques very similar to those used in tree construction, such as information theoretic splitting criteria and pruning, can be found in neural tree construction also. In addition to these methods, there exist other hybrid techniques between decision trees and neural networks.
Reference: [449] <author> Jack Sklansky and Leo Michelotti. </author> <title> Locally trained piecewise linear classifiers. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-2(2):101-111, </volume> <month> March </month> <year> 1980. </year> <month> 283 </month>
Reference-contexts: Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky [496, 450] suggested a procedure to train a linear split to minimize the error probability. Using this procedure, Sklansky and Michelotti <ref> [449] </ref> developed a system to induce a piece-wise linear classifier. Their method identifies the closest-opposed pairs of clusters in the data, and trains each linear discriminant locally. The final classifier produced by this method is a piecewise linear decision surface, not a tree.
Reference: [450] <author> Jack Sklansky and Gustav Nicholas Wassel. </author> <title> Pattern classifiers and trainable machines. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes [49, 115]. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky <ref> [496, 450] </ref> suggested a procedure to train a linear split to minimize the error probability. Using this procedure, Sklansky and Michelotti [449] developed a system to induce a piece-wise linear classifier. Their method identifies the closest-opposed pairs of clusters in the data, and trains each linear discriminant locally.
Reference: [451] <author> John Smith. </author> <title> Public key cryptography. </title> <journal> Byte, </journal> <month> January </month> <year> 1983. </year>
Reference: [452] <author> J.W. Smith, J.E. Everhart, W.E. Dickson, W.C Knowler, </author> <title> and R.S. Jo-hannes. Using the ADAP learning algorithm to forecast the onset of diabetes mel-litus. </title> <booktitle> In Proceedings of the Symposium on Computer Applications and Medical Care, </booktitle> <pages> pages 261-265. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1988. </year>
Reference-contexts: The original source of the data is the National Institute of Diabetes and Digestive and Kidney Diseases, and it is now available in the UCI repository. Smith et al. <ref> [452] </ref> reported 76% accuracy on this data using their ADAP learning algorithm, using a different experimental method from that used here. 107 Discussion The table shows that, for the six data sets considered here, OC1 consistently finds better trees than the original oblique CART method.
Reference: [453] <author> Padhraic Smyth, Alex Gray, and Usama M. Fayyad. </author> <title> Retrofitting decision tree classifiers using kernel density estimation. </title> <note> In ML-95 [333]. to appear. </note>
Reference-contexts: An approach, which refines the class probability estimates in a greedily induced decision tree using local kernel density estimates has been suggested recently in <ref> [453] </ref>. Assignment of probabilistic goodness to splits in a decision tree is described in [187].
Reference: [454] <author> J. A. Sonquist, E. L. Baker, and J. N. Morgan. </author> <title> Searching for Structure. </title> <institution> Institute for Social Research, University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1971. </year>
Reference-contexts: For a review of earlier statistical work on hierarchical classification, see [139]. Statistical programs such as AID <ref> [454] </ref>, MAID [170], THAID [339] and CHAID [240] built binary segmentation trees aimed towards unearthing the interactions between predictor and dependent variables.
Reference: [455] <author> S.P.Brooks and B.J.T.Morgan. </author> <title> Automatic starting point selection for function optimization. </title> <journal> Statistics and Computing, </journal> <volume> 4 </volume> <pages> 173-177, </pages> <year> 1994. </year>
Reference: [456] <author> Lilly Spirkovska. </author> <title> Three dimensional object recognition using similar triangles and decision trees. </title> <journal> Pattern Recognition, </journal> <volume> 26(5):727, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Recent use of decision trees for analyzing amino acid sequences can be found in [442] and [423]. * Object recognition: Tree based classification has been used recently for recognizing three dimensional objects <ref> [456, 57] </ref> and for high level vision [255]. * Pharmacology: Use of tree based classification for drug analysis can be found in [101]. * Physics: Decision trees have been used for the detection of physical particles [34]. * Plant diseases: CART [44] was recently used to assess the hazard of mortality
Reference: [457] <author> Sreejit Chakravarty. </author> <title> A characterization of binary decision diagrams. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(2) </volume> <pages> 129-137, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: In the context of ordered binary decision diagrams (OBDDs), the order in which variables are chosen at tree nodes determines the complexity of the OBDD, and many heuristics have been evaluated for variable order selection (eg., <ref> [457, 154] </ref>). In machine learning, feature evaluation rules are used mainly for picking the single best feature at every node of the decision tree. <p> Recently, agnostic PAC-learning [13] and pruning [205] have been studied by the learnability theory community. In the context of ordered binary decision diagrams (OBDD), the bounds on the tree size have been investigated, as a function of the tree compaction operators and the specific Boolean functions being represented (eg., <ref> [315, 457, 201] </ref>). 2.6.3 Tools Some authors pointed out the similarity or equivalence between the problem of constructing decision trees and existing, seemingly unrelated, problems. Such view points provide valuable tools for analyzing decision trees.
Reference: [458] <author> S.Schwartz, J. Wiles, I. Gough, and S. philips. </author> <title> Connectionist, rule-based and bayesian decision aids: An empirical comparison. </title> <booktitle> In Hand [191], </booktitle> <pages> pages 264-278. </pages>
Reference-contexts: Talmon et al. [467] compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other. In contrast, ID3 is adjudged to be slightly better than connectionist and Bayesian methods in <ref> [458] </ref>. Brown et al. [50] compared backpropagation neural networks with decision trees on three problems that are known to be multimodal. Their analysis indicated that there was not much difference between both methods, and that neither method performed very well in its "vanilla" state.
Reference: [459] <author> B.S. Stewart, Ching-Fang Liaw, and C.C. White. </author> <title> A bibliography of heuristic search research through 1992. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 24(2) </volume> <pages> 268-293, </pages> <year> 1994. </year>
Reference: [460] <author> L. Stewart. </author> <title> Hierarchical bayesian analysis using monte carlo integration: computing posterior distributions when there are many possible models. </title> <journal> The Statistician, </journal> <volume> 36 </volume> <pages> 211-219, </pages> <year> 1987. </year>
Reference: [461] <author> Quentin F. Stout and Bette L. Warren. </author> <title> Tree rebalancing in optimal time and space. </title> <journal> Communications of the ACM, </journal> <volume> 29(9) </volume> <pages> 902-908, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: path length between the root and a leaf node, averaged over all the leaf nodes. (This measure is also used in the experiments in Chapter 5.) Although little if any work has been done on balancing decision trees, a great deal of research has considered balanced search trees (e.g., see <ref> [16, 79, 461, 93, 357] </ref>). Roughly speaking, this literature deals with techniques to restructure search trees when elements are 145 inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [462] <author> C. Y. Suen and Qing Ren Wang. </author> <title> ISOETRP an interactive clustering algorithm with new objectives. </title> <journal> Pattern Recognition, </journal> <volume> 17 </volume> <pages> 211-219, </pages> <year> 1984. </year>
Reference-contexts: A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [463]. Inducing topologically minimal trees, trees in which the number of occurrences of each attribute along each path are minimized, is the topic of [489]. Suen and Wang <ref> [462] </ref> described an algorithm that attempted to minimize the entropy of the whole tree and the class overlap simultaneously. (Class overlap is measured by the number of terminal nodes that represent the same class.) 2.5.5 Use of fuzziness Two common criticisms of decision trees are the following: (1) As decisions in
Reference: [463] <author> Xiaorong Sun, Yuping Qiu, and Louis Anthony Cox. </author> <title> A hill-climbing approach to construct near-optimal decision trees. </title> <booktitle> In AI&Statistics-95 [5], </booktitle> <pages> pages 513-519. </pages>
Reference-contexts: These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis <ref> [463] </ref>. Inducing topologically minimal trees, trees in which the number of occurrences of each attribute along each path are minimized, is the topic of [489].
Reference: [464] <author> P. Swain and H. Hauska. </author> <title> The decision tree classifier design and potential. </title> <journal> IEEE Transactions on Geoscience and Electronics, </journal> <volume> GE-15:142-147, </volume> <year> 1977. </year> <month> 284 </month>
Reference-contexts: standard reference for the current work on decision trees from a statistical perspective is Breiman et al.'s excellent monograph on classification and regression trees [44]. 18 Pattern recognition work on decision trees was motivated by the need to interpret images from remote sensing satellites such as LANDSAT in the 1970s <ref> [464] </ref>. An overview of work on decision trees in the patter recognition literature can be found in [106]. A high level comparative perspective on the classification literature in pattern recognition and artificial intelligence can be found in [76]. <p> This seems natural considering application domains such as spectral analysis and remote sensing <ref> [464] </ref>. In these fields, special techniques [436] were developed to accommodate discrete attributes into what are primarily algorithms for ordered attributes. Fast methods for splitting multiple valued categorical variables are described in [83]. <p> Description length, the number of bits required to "code" the tree and the data using some compact encoding, has been suggested as a means to combine the accuracy and complexity of a classifier [402, 149] . 2.5.10 Miscellaneous Most existing tree induction systems proceed in a greedy top-down fashion <ref> [464, 44, 391] </ref>. Bottom up induction of trees is considered in [275]. <p> mortality to pine trees [19]. * Power systems: Power system security assessment [199] and power stability prediction [414] are two areas in power systems maintenance for which decision trees were used. * Remote Sensing: Remote sensing has been a strong application area for pattern recognition work on decision trees (see <ref> [464, 247] </ref> ).
Reference: [465] <author> Jan L. Talmon. </author> <title> A multiclass nonparametric partitioning algorithm. </title> <journal> Pattern Recognition Letters, </journal> <volume> 4 </volume> <pages> 31-38, </pages> <year> 1986. </year>
Reference-contexts: Shannon's entropy [439] possesses all of these properties [2]. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition <ref> [172, 437, 465] </ref>. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [197, 493, 70, 192], in sequential fault diagnosis [488] and in machine learning [391]. <p> All dependence-bassed measures can be interpreted as belonging to one of the above two categories [23]. There exist several attribute selection criteria that do not clearly belong to any category in Ben-Basset's taxonomy. Gleser and Collen [172] and Talmon <ref> [465] </ref> used a combination of mutual information and 2 measures.
Reference: [466] <author> Jan L. Talmon. </author> <title> A multiclass nonparametric partitioning algorithm. </title> <note> In Gelsema and Kanal [163]. </note>
Reference: [467] <author> Jan L. Talmon, Willem R. M. Dassen, and Vincent Karthaus. </author> <title> Neural nets and classification trees: A comparison in the domain of ECG analysis. </title> <booktitle> In Gelsema and Kanal [164], </booktitle> <pages> pages 415-423. </pages>
Reference-contexts: Multilayer perceptrons and CART (with and without linear combinations) [44] are compared in [12] to find that there is not much difference in accuracy. Similar conclusions were reached in [142] when ID3 [391] and backpropagation were compared. Talmon et al. <ref> [467] </ref> compared classification trees and neural networks for analyzing electrocardiograms (ECG) and concluded that no technique is superior to the other. In contrast, ID3 is adjudged to be slightly better than connectionist and Bayesian methods in [458].
Reference: [468] <author> Jan L. Talmon and P. McNair. </author> <title> The effect of noise and biases on the performance of machine learning algorithms. </title> <journal> International Journal of Bio-Medical Computing, </journal> <volume> 31(1) </volume> <pages> 45-57, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Effects of noise on generalization are discussed in [363, 253]. Overfitting avoidance as a specific bias is studied in [507, 428]. Effect of noise on classification tree construction methods is studied in the pattern recognition literature in <ref> [468] </ref>. Several techniques have been suggested for obtaining the right sized trees. The most popular of these is pruning, whose discussion we will defer to Section 2.4.1.
Reference: [469] <author> Ming Tan. </author> <title> Cost-sensitive learning of classification knowledge and its applications in robotics. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 7-33, </pages> <year> 1993. </year>
Reference-contexts: If the measurement (misclassification) costs are not identical between 43 different attributes (classes), decision tree algorithms need to be designed explicitly to prefer cheaper trees. Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see <ref> [365, 366, 469, 478] </ref> in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478].
Reference: [470] <author> Paul C. Taylor and Bernard W. Silverman. </author> <title> Block diagrams and splitting criteria for classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 3(4) </volume> <pages> 147-161, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Breiman et al. pointed out that the Gini index has difficulty when there are a relatively large number of classes, and suggested the twoing rule [44, 351] as a remedy. Taylor and Silverman <ref> [470] </ref> pointed out that the Gini index emphasizes equal sized offspring and purity of both children. <p> He suggests several ways of modifying existing methods to be prescriptive rather than descriptive. An interesting method for displaying decision trees on multidimensional data, using block diagrams, is proposed in <ref> [470] </ref>. Block diagrams can point out features of the data as well as the deficiencies in the classification method. Issues in preprocessing data to be in a form suitable to decision tree induction are discussed in some detail in [475]. Parallelization of tree induction algorithms is considered in [381].
Reference: [471] <author> Sebastian Thrun and et al. </author> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: An early study comparing machine learning methods for learning from examples can be found in [112]. Comparisons of symbolic and connectionist methods can be found in [501, 440]. Quinlan empirically compared decision trees to genetic classifiers [394] and to neural networks [400]. Thrun et al. <ref> [471] </ref> compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis.
Reference: [472] <author> R. Todeshini and E. Marengo. </author> <title> Linear discriminant classification tree: a user-driven multicriteria classification method. </title> <journal> Chemometrics and Intelligent Laboratory Systems, </journal> <volume> 16 </volume> <pages> 25-35, </pages> <year> 1992. </year>
Reference-contexts: Use of linear discriminants in a decision tree is considered in the remote sensing literature in [218]. A method for building linear discriminant classification trees, in which the user can decide at each node what classes need to be split, is described in <ref> [472] </ref>. John [229] recently considered linear discriminant trees in the machine learning literature. An extension of linear discriminants are linear machines [364], which are linear structures that can discriminate between multiple classes.
Reference: [473] <author> J.T. Tou and R.C. Gonzalez. </author> <title> Pattern Recognition Principles. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: a set of n d-dimensional vectors in at most 2 fl P d (n1) ways if n &gt; d + 1 and 2 n ways if n d + 1, and for any given n and d, one can find a set of vectors for which this bound is achieved <ref> [473] </ref>. For axis-parallel splits, on the other hand, there are only n d distinct possibilities, and axis-parallel methods such as C4.5 [398] and CART [44] can exhaustively search for the best split at each node.
Reference: [474] <author> Godfried T. Toussaint. </author> <title> Bibliography on estimation of misclassification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 20(4) </volume> <pages> 472-479, </pages> <month> July </month> <year> 1974. </year>
Reference: [475] <author> Charalambos Tsatsarakis and D. Sleeman. </author> <title> Supporting preprocessing and post-processing for machine learning algorithms: A workbench for ID3. </title> <journal> Knowledge Acquisition, </journal> <volume> 5(4) </volume> <pages> 367-383, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Block diagrams can point out features of the data as well as the deficiencies in the classification method. Issues in preprocessing data to be in a form suitable to decision tree induction are discussed in some detail in <ref> [475] </ref>. Parallelization of tree induction algorithms is considered in [381].
Reference: [476] <author> Pei-Lei Tu and Jen-Yao Chung. </author> <title> A new decision-tree classification algorithm for machine learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Tools with AI, </booktitle> <pages> pages 370-377, </pages> <address> Arlington, Virginia, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: The measures considered by the earlier papers on NP-completeness appear to be a subset of Naumov's measures, though he does not reference any of the existing work. The problem of constructing the smallest decision tree which best distinguishes characteristics of multiple distinct groups is shown to be NP-complete in <ref> [476] </ref>. Comer and Sethi [92] studied the asymptotic complexity of trie index construction in the document retrieval literature. Megiddo [317] investigated the problem of polyhedral separability (separating two sets of points using k hyperplanes), and proved that several variants of this problem are NP-complete.
Reference: [477] <author> I. B. Turksen and H. Zhao. </author> <title> An equivalence between inductive learning and pseudo-Boolean logic simplification: a rule generation and reduction scheme. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 23(3) </volume> <pages> 907-917, </pages> <month> May-June </month> <year> 1993. </year> <month> 285 </month>
Reference-contexts: Brandman et al. [37] suggested a universal technique to lower bound the size and other characteristics of decision trees for arbitrary Boolean functions. This technique is based on the power spectrum coefficients of the n-dimensional Fourier transform of the function. Turksen and Zhao <ref> [477] </ref> proved the equivalence between a pseudo-Boolean analysis and the ID3 algorithm [391]. 56 2.6.4 Assumptions and biases Most tree induction methods are heuristic in nature. They use several assumptions and biases, hoping that together the heuristics produce good trees.
Reference: [478] <author> Peter D. Turney. </author> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 369-409, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: If the measurement (misclassification) costs are not identical between 43 different attributes (classes), decision tree algorithms need to be designed explicitly to prefer cheaper trees. Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see <ref> [365, 366, 469, 478] </ref> in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs [44, 96, 115, 72, 478]. <p> Several attempts have been made to make tree construction cost-sensitive. These involve incorporating attribute measurement costs (see [365, 366, 469, 478] in machine learning literature, [107, 340] in pattern recognition and [250] in statistics) and incorporating misclassification costs <ref> [44, 96, 115, 72, 478] </ref>.
Reference: [479] <author> Paul E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 161-186, </pages> <year> 1989. </year>
Reference-contexts: An adaptive split depends on the training subsample it is splitting. (An overly simple example of an adaptive split is a test on the mean value of a feature.) Utgoff et al. proposed incremental tree induction methods in the context of univariate decision trees <ref> [479, 481] </ref> as well as multivariate trees [482]. Crawford [99] shows that approaches like Utgoff's, which attempt to update the tree so that the "best" split according to the updated sample is taken at each node, suffer from repeated restructuring.
Reference: [480] <author> Paul E. Utgoff. </author> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 377-391, </pages> <year> 1989. </year>
Reference-contexts: Perceptron trees, which are decision trees with perceptrons just above the leaf nodes, were discussed in <ref> [480] </ref>. Decision trees with perceptrons at all internal nodes were described in [482, 438]. Mathematical Programming: Linear programming has been used for building adaptive classifiers since late 1960s [216]. <p> LMDT: Another oblique decision tree algorithm, one that uses a very different approach from CART-LC, is the Linear Machine Decision Trees (LMDT) system [483, 48], which is a successor to the Perceptron Tree method <ref> [480, 482] </ref>. Each internal node in an LMDT tree is a Linear Machine [364]. The training algorithm presents examples repeatedly at each node until the linear machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to determine when the node has stabilized.
Reference: [481] <author> Paul E. Utgoff. </author> <title> An improved algorithm for incremental induction of decision trees. </title> <booktitle> In ML-94 [331], </booktitle> <pages> pages 318-325. </pages> <editor> Editors: William W. </editor> <booktitle> Cohen and Haym Hirsh. </booktitle>
Reference-contexts: An adaptive split depends on the training subsample it is splitting. (An overly simple example of an adaptive split is a test on the mean value of a feature.) Utgoff et al. proposed incremental tree induction methods in the context of univariate decision trees <ref> [479, 481] </ref> as well as multivariate trees [482]. Crawford [99] shows that approaches like Utgoff's, which attempt to update the tree so that the "best" split according to the updated sample is taken at each node, suffer from repeated restructuring. <p> The "adoption" of these methods into numeric domains may be less than ideal, as numeric domains have their own peculiarities <ref> [485, 245, 486, 481] </ref>. In this chapter, we present a framework for augmenting decision tree induction so it can take advantage of patterns in numeric attribute spaces that would otherwise be ignored.
Reference: [482] <author> Paul E. Utgoff and Carla E. Brodley. </author> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 58-65, </pages> <address> Los Altos, CA, 1990. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Perceptron trees, which are decision trees with perceptrons just above the leaf nodes, were discussed in [480]. Decision trees with perceptrons at all internal nodes were described in <ref> [482, 438] </ref>. Mathematical Programming: Linear programming has been used for building adaptive classifiers since late 1960s [216]. Given two possibly interesecting sets of points, Duda 31 and Hart [117] proposed a linear programming formulation for finding the split whose distance from the misclassified points is minimized. <p> adaptive split depends on the training subsample it is splitting. (An overly simple example of an adaptive split is a test on the mean value of a feature.) Utgoff et al. proposed incremental tree induction methods in the context of univariate decision trees [479, 481] as well as multivariate trees <ref> [482] </ref>. Crawford [99] shows that approaches like Utgoff's, which attempt to update the tree so that the "best" split according to the updated sample is taken at each node, suffer from repeated restructuring. <p> LMDT: Another oblique decision tree algorithm, one that uses a very different approach from CART-LC, is the Linear Machine Decision Trees (LMDT) system [483, 48], which is a successor to the Perceptron Tree method <ref> [480, 482] </ref>. Each internal node in an LMDT tree is a Linear Machine [364]. The training algorithm presents examples repeatedly at each node until the linear machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to determine when the node has stabilized.
Reference: [483] <author> Paul E. Utgoff and Carla E. Brodley. </author> <title> Linear machine decision trees. </title> <type> Technical Report 10, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA, </address> <year> 1991. </year>
Reference-contexts: LMDT: Another oblique decision tree algorithm, one that uses a very different approach from CART-LC, is the Linear Machine Decision Trees (LMDT) system <ref> [483, 48] </ref>, which is a successor to the Perceptron Tree method [480, 482]. Each internal node in an LMDT tree is a Linear Machine [364]. The training algorithm presents examples repeatedly at each node until the linear machine converges.
Reference: [484] <author> J.M. Van Campenhout. </author> <title> On the Problem of Measurement Selection. </title> <type> PhD thesis, </type> <institution> Stanford University, Dept. of Electrical Engineering, </institution> <year> 1978. </year>
Reference-contexts: Cover et al. <ref> [94, 484] </ref> showed that heuristic sequential feature selection methods can do arbitrarily worse than the optimal strategy. Mucciardi and Gose [342] compared seven feature subset selection techniques empirically and concluded that no technique was uniformly superior to the others.
Reference: [485] <author> Thierry Van de Merckt. NFDT: </author> <title> A system that learns flexible concepts based on decision trees for numerical attributes. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 322-331, </pages> <year> 1992. </year>
Reference-contexts: The "adoption" of these methods into numeric domains may be less than ideal, as numeric domains have their own peculiarities <ref> [485, 245, 486, 481] </ref>. In this chapter, we present a framework for augmenting decision tree induction so it can take advantage of patterns in numeric attribute spaces that would otherwise be ignored.
Reference: [486] <author> Thierry Van de Merckt. </author> <title> Decision trees in numerical attribute spaces. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 1016-1021. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference-contexts: Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop. De Merckt <ref> [486] </ref> suggested an attribute selection measure that combined geometric distance with information gain, and argued that such measures are more appropriate for numeric attribute spaces. * Rules derived from distance measures: "Distance" here refers to the distance between class probability distributions. <p> The problem of incorporating continuous attributes into these algorithms is considered subsequently. The problem of meaningfully discretizing a continuous dimension is considered in <ref> [134, 245, 486, 343] </ref>. Methods of discretization that operate on a single continuous attribute at a time can be said to be "local" discretization methods. In contrast, "global" discretization methods simultaneously convert all continuous attributes [81]. <p> The "adoption" of these methods into numeric domains may be less than ideal, as numeric domains have their own peculiarities <ref> [485, 245, 486, 481] </ref>. In this chapter, we present a framework for augmenting decision tree induction so it can take advantage of patterns in numeric attribute spaces that would otherwise be ignored. <p> One solution to this problem is to augment the definition of the goodness measures, to somehow take into account the "structure" of the examples in addition to the class distribution. Van de Merckt <ref> [486] </ref> used this approach to define a selection criterion that combines proximity with class entropy. Though this certainly is a step towards using structure, it leaves open some potential problems. * de Merckt considers only one kind of structure information, namely clusters. <p> Structure extraction methods of varying complexity can be used in conjunction with univariate, multivariate and/or incremental decision tree methods in this model. The complexity of the resulting system is only a sum of the complexities of the preprocessing and tree building stages, as opposed to a product as in <ref> [486] </ref>. 7.3 A Concrete Example This section illustrates that five decision tree methods (three axis-parallel and two oblique) benefit by using Euclidean minimum spanning tree (EMST) clustering as a preprocessing step, on the CB, RCB and RGC domains and two real-world data sets taken from the UCI machine learning repository [346]. <p> For example, some trees displayed in Figures 7.2 and 7.4 are quite small and accurate, but impose a counterintuitive structure on the data. Alternative ways to quantify the goodness of decision trees would be useful in numerical domains <ref> [486] </ref>. In our experiments, after clustering information was provided to the tree induction programs, the trees induced were consistently identical to the original concept descriptions for the synthetic data. This indicates that using structure information helps induce better decision trees in some domains.
Reference: [487] <author> P.J.M. Van Laarhoven and Aarts. E.H.L. </author> <title> Simulated Annealing: Theory and Applications. </title> <publisher> Reidel, </publisher> <address> Dordrecht, </address> <year> 1987. </year>
Reference: [488] <author> P.K. Varshney, C.R.P. Hartmann, and J.M. De Faria Jr. </author> <title> Applications of information theory to sequential fault diagnosis. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(2):164-170, </volume> <year> 1982. </year>
Reference-contexts: The aim is to build a test algorithm that unambiguously identifies the occurrence of any system state using the given tests, while minimizing the total cost. The testing algorithms normally take the form of decision trees or AND/OR trees <ref> [488, 378] </ref>. Many heuristics used to construct decision trees are used for test sequencing also. Vector quantization (VQ) [167] is a data compression technique that has proved useful for image coding. Tree structured vector quantizers (TSVQ) [65] are structures very similar to decision trees. <p> pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [197, 493, 70, 192], in sequential fault diagnosis <ref> [488] </ref> and in machine learning [391]. Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop.
Reference: [489] <author> Walter Van de Velde. </author> <title> Incremental induction of topologically minimal trees. </title> <editor> In Bruce W. Porter and Ray J. Mooney, editors, </editor> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 66-74, </pages> <address> Austin, Texas, </address> <year> 1990. </year>
Reference-contexts: A deterministic hill-climbing search procedure has also been suggested for searching for optimal trees, in the context of sequential fault diagnosis [463]. Inducing topologically minimal trees, trees in which the number of occurrences of each attribute along each path are minimized, is the topic of <ref> [489] </ref>.
Reference: [490] <author> C. S. Wallace and D. M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: Option trees, in which every internal node holds several optional tests along with their respective subtrees, are discussed in [61, 62]. Oliver [368] suggested a method to build decision graphs, which are similar to Chou's decision trellises, using minimum length encoding principles <ref> [490] </ref>. Rymon [415] suggested SE-trees, set enumeration structures each of which can embed several decision trees. All standard decision tree methods are applicable when rules are to be induced about one aspect, say, the presence or absence of a disease.
Reference: [491] <author> C. S. Wallace and J. D. Patrick. </author> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 7-22, </pages> <month> April </month> <year> 1993. </year> <month> 286 </month>
Reference-contexts: However, there exist arguments that cross validation is clearly preferable to bootstrap in practice [256]. 38 coding method (which did not have an effect on their main conclusions) was pointed out in <ref> [491] </ref>. Forsyth et al. [149] recently suggested a pruning method that is based on viewing the decision tree as an encoding for the training data. Use of dynamic programming to prune trees optimally and efficiently has been explored recently in [33].
Reference: [492] <author> Qing Ren Wang. </author> <title> Decision Tree Approach to Pattern Recognition Problems on a Large Character Set. </title> <type> PhD thesis, </type> <institution> Concordia University, Canada, </institution> <year> 1984. </year>
Reference: [493] <author> Qing Ren Wang and C. Y. Suen. </author> <title> Analysis and design of a decision tree based on entropy reduction and its application to large character set recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 406-417, </pages> <year> 1984. </year>
Reference-contexts: entropy reduction as a common theme underlying several pattern recognition problems, see [498]. 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition <ref> [197, 493, 70, 192] </ref>, in sequential fault diagnosis [488] and in machine learning [391]. Mingers [323] suggested the G-statistic, an information theoretic measure that is a close approximation to 2 distribution, for tree construction as well as for deciding when to stop. <p> For moderate sized problems, the critical issues are generalization accuracy, honest error rate estimation 11 and gaining insight into the predictive and generalization structure of the data. For very large tree classifiers, the critical issue is optimizing structural properties (height, balance etc.) <ref> [493, 71] </ref>. 11 For a general discussion about the relationship between complexity and predictive accuracy of classifiers, see [380]. 34 Breiman et al. [44] pointed out that tree quality depends more on good stopping rules than on splitting rules. Effects of noise on generalization are discussed in [363, 253]. <p> Such view points provide valuable tools for analyzing decision trees. Wang and Suen <ref> [493] </ref> show that entropy-reduction point of view is powerful in theoretically bounding search depth and classification error. Chou and Gray [84] view decision trees as variable-length encoder-decoder pairs, and show that rate is equivalent to tree depth while distortion is the probability of misclassification.
Reference: [494] <author> Qing Ren Wang and Ching Y. Suen. </author> <title> Large tree classifier with heuristic search and global training. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9(1):91-102, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: In the first stage, a sufficient partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming [318], fuzzy logic search <ref> [494] </ref> and multi-linear programming [30]. The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. <p> A soft split, on the other hand, assigns a probability that each point belongs to a partition, thus allowing points to belong to multiple partitions. C4.5 [398] uses a simple form of soft splitting (chapter 8). 46 Use of fuzzy splits in pattern recognition literature can be found in <ref> [432, 494] </ref>. Jordan and Jacobs [233] describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [146]. Induction of fuzzy decision trees has also been considered in [281, 512]. 2.5.6 Estimating probabilities Decision trees have crisp decisions at leaf nodes.
Reference: [495] <author> Gustav Nicholas Wassel. </author> <title> Training a Linear Classifier to Optimize the Error Probability. </title> <type> PhD thesis, </type> <institution> University of California, Irvine, </institution> <year> 1972. </year>
Reference: [496] <author> Gustav Nicholas Wassel and Jack Sklansky. </author> <title> Training a one-dimensional classifier to minimize the probability of error. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-2:533-541, </volume> <month> September </month> <year> 1972. </year>
Reference-contexts: In the machine learning literature, Utgoff et al. explored decision trees that used linear machines at internal nodes [49, 115]. Locally Opposed Clusters of Objects: Sklansky and his students developed several piecewise linear discriminants based on the principle of locally opposed clusters of objects. Wassel and Sklansky <ref> [496, 450] </ref> suggested a procedure to train a linear split to minimize the error probability. Using this procedure, Sklansky and Michelotti [449] developed a system to induce a piece-wise linear classifier. Their method identifies the closest-opposed pairs of clusters in the data, and trains each linear discriminant locally.
Reference: [497] <author> Larry Watanabe and Larry Rendell. </author> <title> Learning structural decision trees from examples. </title> <booktitle> In IJCAI-91 [220], </booktitle> <pages> pages 770-776. </pages> <editor> Editors: </editor> <publisher> John Mylopoulos and Ray Reiter. </publisher>
Reference-contexts: Looka-head for construction of Boolean feature combinations is also considered in [515]. Linear threshold unit trees for Boolean functions are described in [418]. Decision trees having first order predicate calculus representations, with Horn clauses as tests at internal nodes, are considered in <ref> [497] </ref>. Subsample selection Feature subset selection attempts to choose useful features. Similarly, subsample selection attempts to choose appropriate training samples for induction. Quinlan suggested "win-dowing", a random training set sampling method, for his programs ID3 and C4.5 [398, 506].
Reference: [498] <author> S. Watanabe. </author> <title> Pattern recognition as a quest for minimum entropy. </title> <journal> Pattern Recognition, </journal> <volume> 13 </volume> <pages> 381-387, </pages> <year> 1981. </year>
Reference-contexts: Shannon's entropy [439] possesses all of these properties [2]. For an insightful treatment of entropy reduction as a common theme underlying several pattern recognition problems, see <ref> [498] </ref>. 22 information of the whole tree, is explored in pattern recognition [172, 437, 465]. 8 Tree construction by locally optimizing information gain, the reduction in entropy due to splitting each individual node, is explored in pattern recognition [197, 493, 70, 192], in sequential fault diagnosis [488] and in machine learning
Reference: [499] <author> Nicholas Weir, S. Djorgovski, and Usama M. Fayyad. </author> <title> Initial galaxy counts from digitized POSS-II. </title> <journal> The Astronomical Journal, </journal> <volume> 110(1):1, </volume> <year> 1995. </year>
Reference-contexts: Decision trees have helped in star-galaxy classification [500], determining galaxy counts <ref> [499] </ref> and discovering quasars [244] in the Second Palomar Sky Survey.
Reference: [500] <author> Nicholas Weir, Usama M. Fayyad, and S. Djorgovski. </author> <title> Automated star/galaxy classification for digitized POSS-II. </title> <journal> The Astronomical Journal, </journal> <volume> 109(6):2401, </volume> <year> 1995. </year>
Reference-contexts: Decision trees have helped in star-galaxy classification <ref> [500] </ref>, determining galaxy counts [499] and discovering quasars [244] in the Second Palomar Sky Survey.
Reference: [501] <author> S. Weiss and I. Kapouleas. </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In IJCAI-89 [219], </booktitle> <pages> pages 781-787. </pages> <editor> Editor: N. S. </editor> <publisher> Sridharan. </publisher>
Reference-contexts: Several researchers have compared trees to these other methods on specific problems. An early study comparing machine learning methods for learning from examples can be found in [112]. Comparisons of symbolic and connectionist methods can be found in <ref> [501, 440] </ref>. Quinlan empirically compared decision trees to genetic classifiers [394] and to neural networks [400]. Thrun et al. [471] compared several learning algorithms on simulated 59 Monk's problems. Palvia and Gordon [373] compared decision tables, decision trees and decision rules, to determine which formalism is best for decision analysis. <p> This is Fisher's famous iris data, which has been extensively studied in the statistics and machine learning literature. The data consists of 150 examples, where each example is described by four numeric attributes. There are 50 examples of each of three different types of iris flower. Weiss and Kapouleas <ref> [501] </ref> obtained accuracies of 96.7% and 96.0% on this data with back propagation and 1-NN, respectively. Note that Table 3.1 does not report results of OC1-LP on the Iris data. This is because we have implemented the LP-formulation for only two-class problems. Housing Costs in Boston.
Reference: [502] <author> Shalom M. Weiss and Nitin Indurkhya. </author> <title> Rule-based regression. </title> <booktitle> In IJCAI-93 [221], </booktitle> <pages> pages 1072-1078. </pages> <editor> Editor: </editor> <publisher> Ruzena Bajcsy. </publisher>
Reference: [503] <author> Allan P. White and Wei Zhang Liu. </author> <title> Technical note: Bias in information-based measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15(3) </volume> <pages> 321-329, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic <ref> [21, 195, 323, 515, 503] </ref> are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> Mantaras [310] argued that gain ratio had its own set of problems, and suggested using information theory-based distance between partitions for tree construction. He formally proved that his measure is not biased towards multiple-valued attributes. However, White and Liu <ref> [503] </ref> present experiments to conclude that information gain, gain ratio and Mantaras' measure are worse than a 2 based statistical measure, in terms of their bias towards multiple-valued attributes.
Reference: [504] <author> P.A.D. Wilks and M.J. </author> <title> English. Accurate segmentation of respiration waveforms from infants enabling identification and classification of irregular breathing patterns. </title> <journal> Medical Engineering and Physics, </journal> <volume> 16(1) </volume> <pages> 19-23, </pages> <month> January </month> <year> 1994. </year> <month> 287 </month>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology [295, 129, 258], psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography [508], to analyze Sudden Infant Death (SID) syndrome <ref> [504] </ref> and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data exploration methods in molecular biology.
Reference: [505] <author> Rogier A. Windhorst, Barbara E. Franklin, and Lyman W. Neuschaefer. </author> <title> Removing cosmic ray hits from multi-orbit HST wide field camera images. </title> <booktitle> Proceedings of the Astronomical Society of Pacific, </booktitle> <volume> 106, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: We then used DAOFIND on each of the CR images to catalog the positions of the CRs. As an external check on the completeness of this CR catalogue, we found the surface density of the CRs and compared it to the values recently determined in <ref> [505] </ref>. The Final Orbital/Science Verification Report by the WF/PC-1 Investigation Definition Team lists zeropoint offsets for the M81 field for WF2 [214]. Consequently, we used the same CCD for our CR surface density determination.
Reference: [506] <author> J. Wirth and J. Catlett. </author> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> In Fifth International Conference on Machine Learning, </booktitle> <pages> pages 87-99, </pages> <address> Ann Arbor, Michigan, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Subsample selection Feature subset selection attempts to choose useful features. Similarly, subsample selection attempts to choose appropriate training samples for induction. Quinlan suggested "win-dowing", a random training set sampling method, for his programs ID3 and C4.5 <ref> [398, 506] </ref>. A initially randomly chosen window can be iteratively expanded to include only the "important" training samples. Several ways of choosing representative samples for Nearest Neighbor learning methods exist (see [104, 105], for examples).
Reference: [507] <author> D. Wolpert. </author> <title> On overfitting avoidance as bias. </title> <type> Technical Report SFI TR 92-03-5001, </type> <institution> The Santa Fe Institute, </institution> <year> 1992. </year>
Reference-contexts: Effects of noise on generalization are discussed in [363, 253]. Overfitting avoidance as a specific bias is studied in <ref> [507, 428] </ref>. Effect of noise on classification tree construction methods is studied in the pattern recognition literature in [468]. Several techniques have been suggested for obtaining the right sized trees. The most popular of these is pruning, whose discussion we will defer to Section 2.4.1.
Reference: [508] <author> K. S. Woods, C. C. Doss, K. W. Vowyer, J. L. Solka, C. E. Prieve, and W. P. Jr. Kegelmeyer. </author> <title> Comparative evaluation of pattern recognition techniques for detection of microcalcifications in mammography. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(6) </volume> <pages> 1417-1436, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Recent uses of automatic induction of decision 63 trees can be found in diagnosis [259], cardiology [295, 129, 258], psychiatry [314], gastroenterology [234], for detecting microcalcifications in mammography <ref> [508] </ref>, to analyze Sudden Infant Death (SID) syndrome [504] and for diagnosing thyroid disor ders [140]. * Molecular biology: Initiatives such as the Human Genome Project and the Gen-Bank database offer fascinating opportunities for machine learning and other data exploration methods in molecular biology.
Reference: [509] <author> Chialin Wu, David Landgrebe, and Philip Swain. </author> <title> The decision tree approach to classification. </title> <type> Technical Report TR-EE-75-17, </type> <institution> Laboratory for Applications of Remote Sensing, School of Engineering, Purdue University, West Lafayette, IN, </institution> <month> May </month> <year> 1975. </year>
Reference: [510] <author> Mihalis Yannakakis. </author> <title> The analysis of local search problems and their heuristics. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> volume 415, </volume> <pages> pages 298-311. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference: [511] <author> K. C. You and King-Sun Fu. </author> <title> An approach to the design of a linear binary tree classifier. </title> <booktitle> In Proceedings of the Third Symposium on Machine Processing of Remotely Sensed Data, </booktitle> <address> West Lafayette, </address> <note> IN, 1976. </note> <institution> Purdue University. </institution>
Reference-contexts: Methods used in the literature for finding good linear tests include linear discriminant analysis, hill climbing search, linear programming, perceptron training and others. Linear Discriminant Trees: Several authors have considered the problem of constructing tree-structured classifiers that have linear discriminants [117] at each node. You and Fu <ref> [511] </ref> used a linear discriminant at each node in the decision tree, computing the hyperplane coefficients using the Fletcher-Powell descent method [144]. Their method requires that the best set of features at each node be prespecified by a human. <p> Their method uses multivariate stepwise regression to optimize the structure of the decision tree as well as to choose subsets of features to be used in the linear discriminants. More recently, use of linear discriminants at each node is considered by 29 Loh and Vanichsetakul [294]. Unlike in <ref> [511] </ref>, the variables at each stage are appropriately chosen in [294] according to the data and the type of splits desired.
Reference: [512] <author> Y. Yuan and M. J. Shaw. </author> <title> Induction of fuzzy decision trees. Fuzzy Sets and Systems, </title> <address> 69(2):125, </address> <year> 1995. </year>
Reference-contexts: Jordan and Jacobs [233] describe a parametric, hierarchical classifier with soft splits. Multivariate regression trees using fuzzy, soft splitting criteria, are considered [146]. Induction of fuzzy decision trees has also been considered in <ref> [281, 512] </ref>. 2.5.6 Estimating probabilities Decision trees have crisp decisions at leaf nodes. On the contrary, class probability trees assign a probability distribution for all classes at the terminal nodes. Breiman et al. ([44], Chapter 4) proposed a method for building class probability trees.
Reference: [513] <author> C.T. Zahn. </author> <title> Graph theoretical methods for detecting and describing gestalt clusters. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-20(1), </volume> <month> January </month> <year> 1971. </year>
Reference-contexts: The criteria we used to select this method from the many 226 alternatives were (1) MST-clustering is intuitive and easy to implement, and (2) it has been researched extensively, and shown to work well on a variety of distributions <ref> [178, 513, 232] </ref>. A minimum spanning tree of a weighted graph G is the minimum-weight connected acyclic subgraph G 0 of G containing all vertices of G. Many robust and efficient algorithms are available to compute MSTs.
Reference: [514] <author> Wang Zhengou and Lin Yan. </author> <title> A new inductive learning algorithm: Separability-Based Inductive learning algorithm. </title> <journal> Acta Automatica Sinica, </journal> <volume> 5(3) </volume> <pages> 267-270, </pages> <year> 1993. </year> <journal> Translated into Chinese Journal of Automation. </journal>
Reference-contexts: Class separation-based metrics developed in the machine learning literature <ref> [133, 514] </ref> are also distance measures.
Reference: [515] <author> Xiao Jia Zhou and Tharam S. Dillon. </author> <title> A statistical-heuristic feature selection criterion for decision tree induction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-13(8):834-841, </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: emphasizes exclusivity between offspring class subsets instead. 8 Goodman and Smyth [174] report that the idea of using the mutual information between features and classes to select the best feature was originally put forward by Lewis [285]. 23 Bhattacharya distance [290], Kolmogorov-Smirnoff distance [152, 413, 198] and the 2 statistic <ref> [21, 195, 323, 515, 503] </ref> are some other distance-based measures that have been used for tree induction. Class separation-based metrics developed in the machine learning literature [133, 514] are also distance measures. <p> Some feature evaluation rules, whose distribution does not depend on the number of training samples (i.e., a goodness value of k would have the same significance anywhere in the tree) have been suggested in the literature <ref> [286, 515, 235] </ref>. * Trees to rules conversion: Quinlan [393, 398] gave efficient procedures for converting a decision tree into a set of production rules. <p> Ragavan and Rendell [403] describe a method that constructs Boolean features using lookahead, and uses the constructed feature combinations as tests at tree nodes. Looka-head for construction of Boolean feature combinations is also considered in <ref> [515] </ref>. Linear threshold unit trees for Boolean functions are described in [418]. Decision trees having first order predicate calculus representations, with Horn clauses as tests at internal nodes, are considered in [497]. Subsample selection Feature subset selection attempts to choose useful features.
Reference: [516] <author> Seth Zimmerman. </author> <title> An optimal search procedure. </title> <journal> The American Mathematical Monthly, </journal> <volume> 66(8) </volume> <pages> 690-693, </pages> <month> March </month> <year> 1959. </year> <month> 288 </month>
Reference-contexts: Analysis: Aclass of binary splits S for a data set is said to be complete if, informally, for every partition of the data, there exists a member of S that induces the partition. Zimmerman <ref> [516] </ref> considered the problem of building identification keys for complete classes of splits, given arbitrary class distributions.
References-found: 516

