URL: http://www.cse.ucsc.edu/research/ml/papers/hkst/paper.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/avrim/www/ML98/presentations.html
Root-URL: 
Title: Rigorous Learning Curve Bounds from Statistical Mechanics  
Author: David Haussler Michael Kearns H. Sebastian Seung Naftali Tishby 
Address: Santa Cruz, California  Murray Hill, New Jersey  Murray Hill, New Jersey  Jerusalem, Israel  
Affiliation: U.C. Santa Cruz  AT&T Bell Laboratories  AT&T Bell Laboratories  Hebrew University  
Abstract: In this paper we introduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics. The advantage of our theory over the well-established Vapnik-Chervonenkis theory is that our bounds can be considerably tighter in many cases, and are also more reflective of the true behavior (functional form) of learning curves. This behavior can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory. The disadvantages of our theory are that its application requires knowledge of the input distribution, and it is limited so far to finite cardinality function classes. We illustrate our results with many concrete examples of learning curve bounds derived from our theory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari, N. Fujita, and S. Shinomoto. </author> <title> Four types of learning curves. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 605-618, </pages> <year> 1992. </year>
Reference-contexts: In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [18, 24, 1, 27] </ref> and the thermodynamic limit [27]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 This theory will be distribution-specific, but will not attempt to force a power law form on learning curves. <p> As mentioned already, these are functions that are assumed to be given in the thermodynamic limit method. Let t (N ) be any mapping from the natural numbers to the natural numbers such that t (N ) ! 1 as N ! 1, and let s : <ref> [0; 1] </ref> ! &lt; + be any continuous function. <p> Let us now let m; N ! 1 (and thus t (N ) ! 1) but let m=t (N ) = ff &gt; 0 remain constant. Define * fl 2 <ref> [0; 1] </ref> to be the largest * 2 [0; 1] such that s (*) ff log (1 *). Note that both s (*) and ff log (1 *) are non-negative functions, and 0 = ff log (1 *) s (*) for * = 0. <p> Let us now let m; N ! 1 (and thus t (N ) ! 1) but let m=t (N ) = ff &gt; 0 remain constant. Define * fl 2 <ref> [0; 1] </ref> to be the largest * 2 [0; 1] such that s (*) ff log (1 *). Note that both s (*) and ff log (1 *) are non-negative functions, and 0 = ff log (1 *) s (*) for * = 0. <p> Let us define by = minfff log (1 *) s (*) : * 2 [* fl Note that is well-defined since the quantify ff log (1 *) s (*) is strictly positive for all * 2 <ref> [* fl + t; 1] </ref>. <p> We can now write r (N) X e j )+ff log (1* N (12) r (N) X e (13) t (N) r (N )e (15) where the first inequality follows from the fact that for all i N;t j r (N ) we have * N j 2 <ref> [* fl + t; 1] </ref>. <p> Similarly, the lower bound shows that better learning curves for the Ising perceptron and boolean conjunction problems that depend only on the entropy bound cannot be obtained. Theorem 5 Let s : [0; 1=2] ! <ref> [0; 1] </ref> be any continuous function bounded away from 1 and such that s (0) = s (1) = 0. <p> A high-level sketch of the main ideas follows. For any N , the class F N will be constructed so that there are exactly N=2 error levels, namely * N j = j=N for 1 j N=2. Now let s : [0; 1=2] ! <ref> [0; 1] </ref> be any continuous function bounded away from 1 and satisfying s (0) = s (1=2) = 0. <p> Let the target function f N be the perceptron in which every weight is +1, and let the function class F N consist of all Ising perceptrons which have at least flN weights (fl 2 <ref> [0; 1] </ref>) that are 1. (Note that unlike the realizable Ising perceptron case, here the choice of target function matters.) Again let the distribution D N be any spherically symmetric distribution on &lt; N . <p> (53) Note how much weaker some of the bounds are than others. 3.5 Large-ff asymptotics of scaled learning curves Our formalism can be used to give a classification of the large-ff asymptotics of scaled learning curves, 7 thus com pleting a classification program that has been suggested by several researchers <ref> [24, 25, 1] </ref>. From Equation (32) and Lemma 9, the weaker form u (*) = 2v (*) is derived as a permissible energy bound in the Appendix in Section A.2. <p> For any fixed function class F (of possibly infinite car-dinality), any distribution D, and any value fl 2 <ref> [0; 1] </ref>, a subclass F [fl] F is called a fl-cover of F with respect to D if for every f 2 F there exists an f 0 2 F [fl] such that *(f; f 0 ) fl. <p> In this way we can obtain for any sequence fl 1 &gt; fl 2 &gt; fl 3 &gt; a sequence of nested covers F [fl 1 ] F [fl 2 ] F [fl 3 ] . Let us fix fl 2 <ref> [0; 1] </ref>, and assume that F has a finite fl-cover with respect to D. This is not as severe an assumption as it might initially seem.
Reference: [2] <author> E. B. Baum and Y.-D. Lyuu. </author> <title> The transition to perfect generalization in perceptrons. </title> <journal> Neural Comput., </journal> <volume> 3 </volume> <pages> 386-401, </pages> <year> 1991. </year>
Reference-contexts: This bound on the critical value was known from the work of Gardner and Derrida [12], and extended to the case of boolean inputs by Baum, Lyuu and Rivin <ref> [2, 19] </ref>.
Reference: [3] <author> G. Benedek and A. Itai. </author> <title> Learnability with respect to fixed distributions. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 86(2) </volume> <pages> 377-389, </pages> <year> 1991. </year>
Reference-contexts: some theorists have sought to preserve the functional form of the VC bounds, but to replace the VC dimension in this functional form by an appropriate distribution-specific quantity, such as the VC entropy (which is the expectation of the logarithm of the number of dichotomies realized by the function class) <ref> [30, 17, 3] </ref>. Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments [29]. <p> The idea of using empirical minimization over a finite cover for an infinite class has also been investigated by Benedek and Itai <ref> [3] </ref> in their investigation of distribution-specific sample complexity, and also by Vapnik [30]. Things become more interesting when we take the natural step of analyzing the algorithm that first chooses an advantageous value for the realizability parameter fl and then performs empirical minimization using F [fl].
Reference: [4] <author> D. Cohn and G. Tesauro. </author> <title> How tight are the Vapnik-Chervonenkis bounds. </title> <journal> Neural Comput., </journal> <volume> 4 </volume> <pages> 249-269, </pages> <year> 1992. </year>
Reference-contexts: However, it is becoming clear that learning curves exhibit a diversity of behaviors. For instance, some researchers have attempted to fit learning curves from backpropagation experiments with a variety of functional forms, including expo-nentials <ref> [4] </ref>. Backpropagation experiments with handwritten digits and characters indicate that good generalization error is sometimes obtained for sample sizes considerably smaller than the number of weights (presumed to be roughly the same as the VC dimension) [20], though the VC bounds are vacuous for m smaller than d.
Reference: [5] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: The first inequality follows from the fact that the training error of any hypothesis h in the version space must be no greater than the training error of any other hypothesis in the class, including h fl in particular. The second follows from Sanov's theorem on large deviations <ref> [5] </ref> (see Section A.2 of the Appendix). For the realizable case we have * min = 0 and *(h; h fl ) = * gen (h), so Pr S [h 2 VS (S)] (1* gen (h)) m already follows from the second inequality.
Reference: [6] <author> L. Devroye and G. Lugosi. </author> <title> Lower bounds in pattern recognition and learning. 1994. </title> <type> Preprint. </type>
Reference-contexts: It has been shown that these bounds are essentially the best distribution-independent bounds possible, in the sense that for any function class, there exists an input distribution for which matching lower bounds on the generalization error can be given <ref> [6, 8, 26] </ref>.
Reference: [7] <author> R. M. Dudley. </author> <title> Central limit theorems for empirical measures. </title> <journal> Annals of Probability, </journal> <volume> 6(6) </volume> <pages> 899-929, </pages> <year> 1978. </year>
Reference-contexts: In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [30, 22, 7, 16] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells. <p> We are currently investigating extensions to the infinite case that are more refined than the covering approach discussed in Section 4.1, and are based on combining the shell decomposition with the VC dimension, VC entropy and random covering numbers <ref> [30, 22, 7, 16] </ref>. * Expressing our bounds as penalty functions. One of the most interesting aspects of the VC theory is Vapnik's explicit prescription in the unrealizable setting for trading off hypothesis class complexity (and therefore, ability to realize the target function) against empirical error [30].
Reference: [8] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference-contexts: It has been shown that these bounds are essentially the best distribution-independent bounds possible, in the sense that for any function class, there exists an input distribution for which matching lower bounds on the generalization error can be given <ref> [6, 8, 26] </ref>.
Reference: [9] <author> A. Engel and W. Fink. </author> <title> Statistical mechanics calculation of Vapnik Chervonenkis bounds for perceptrons. </title> <journal> J. Phys., </journal> <volume> 26 </volume> <pages> 6893-6914, </pages> <year> 1993. </year>
Reference-contexts: Engel, van den Broeck, and Fink have used the replica method to calculate the maximum deviation between empirical and generalization error in the function class, and the maximum generalization error in the version space <ref> [10, 9] </ref>. Although the replica method produces exact results when used correctly, it rests upon an interchange of limits for which no rigorous justification has been found. convergence bounds, one for each error shell, into a learning curve bound, even in the finite case. <p> Here we are actually giving a bound on the entire learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in both simulations and non-rigorous replica calculations from statistical physics <ref> [14, 28, 25, 9] </ref>. 6 In Figure 11, we graph the difference of the entropy and energy curves shown in Figure 3, that is, we plot s (*) + ff log (1 *) for the three values of ff. This plot is simply another way of visualizing the entropy-energy competition. <p> The left zero crossing also has a meaning. With high probability, there are no hypotheses in the version space with error less than this left crossing except for the target itself. So the version space minus the target is contained within an annulus <ref> [9] </ref> whose inner and outer limits are the left and right zero crossings. It is instructive to compare our bounds with the cardi-nality and VC bounds for this problem.
Reference: [10] <author> A. Engel and C. van den Broeck. </author> <title> Systems that can learn from examples: replica calculation of uniform convergence bounds for the perceptron. </title> <journal> Phys. Rev. Lett., </journal> <volume> 71 </volume> <pages> 1772-1775, </pages> <year> 1993. </year>
Reference-contexts: Engel, van den Broeck, and Fink have used the replica method to calculate the maximum deviation between empirical and generalization error in the function class, and the maximum generalization error in the version space <ref> [10, 9] </ref>. Although the replica method produces exact results when used correctly, it rests upon an interchange of limits for which no rigorous justification has been found. convergence bounds, one for each error shell, into a learning curve bound, even in the finite case.
Reference: [11] <author> E. Gardner. </author> <title> The space of interactions in neural network models. </title> <journal> J. Phys., </journal> <volume> A21:257-270, </volume> <year> 1988. </year>
Reference-contexts: These bounds hold for all empirical error minimization algorithms, including the zero temperature limit of the Gibbs algorithm. Because of our desire for rigor, we have not used the replica method <ref> [11] </ref> in this paper. Engel, van den Broeck, and Fink have used the replica method to calculate the maximum deviation between empirical and generalization error in the function class, and the maximum generalization error in the version space [10, 9].
Reference: [12] <author> E. Gardner and B. Derrida. </author> <title> Three unfinished works on the optimal storage capacity of networks. </title> <journal> J. Phys., </journal> <volume> A22:1983-1994, </volume> <year> 1989. </year>
Reference-contexts: We first consider the class of Ising perceptrons <ref> [12, 14, 28] </ref>. <p> However, this bound does show that any consistent learning algorithm must have reached zero error with probability approaching 1 in the thermodynamic limit for scaled sample size greater than 1:448. This bound on the critical value was known from the work of Gardner and Derrida <ref> [12] </ref>, and extended to the case of boolean inputs by Baum, Lyuu and Rivin [2, 19].
Reference: [13] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proceedings of the 3rd Workshop on Computational Learning Theory, </booktitle> <pages> pages 217-231. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA, </address> <year> 1990. </year>
Reference-contexts: The remaining sample points fail to eliminate any of the functions of generalization error * since they all agree with the target function f N on the remaining points. Now it is known <ref> [13] </ref> that in order to eliminate 2 s (*)N parity functions over a uniform distribution, the sample size m must obey m s (*) N ; for smaller m, there is a constant probability that at least one parity function remains in the version space.
Reference: [14] <author> G. Gyorgyi. </author> <title> First-order transition to perfect generalization in a neural network with binary synapses. </title> <journal> Phys. Rev., </journal> <volume> A41:7097-7100, </volume> <year> 1990. </year>
Reference-contexts: We first consider the class of Ising perceptrons <ref> [12, 14, 28] </ref>. <p> Here we are actually giving a bound on the entire learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in both simulations and non-rigorous replica calculations from statistical physics <ref> [14, 28, 25, 9] </ref>. 6 In Figure 11, we graph the difference of the entropy and energy curves shown in Figure 3, that is, we plot s (*) + ff log (1 *) for the three values of ff. This plot is simply another way of visualizing the entropy-energy competition.
Reference: [15] <author> G. Gyorgyi and N. Tishby. </author> <title> Statistical theory of learning a rule. </title> <editor> In K. Thuemann and R. Koeberle, editors, </editor> <title> Neural Networks and Spin Glasses. </title> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: The distribution of noise ~ is also Gaussian, with variance fl 2 1 on each component. A similar problem was examined by Gyorgyi and Tishby <ref> [15] </ref>. In this case, one can show that * gen (w) = 1 * min (fl) = * gen (w ) = 1 * gen (w; w ) = 1 where R = w w fl =N .
Reference: [16] <author> D. Haussler. </author> <title> Decision-theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [30, 22, 7, 16] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells. <p> We are currently investigating extensions to the infinite case that are more refined than the covering approach discussed in Section 4.1, and are based on combining the shell decomposition with the VC dimension, VC entropy and random covering numbers <ref> [30, 22, 7, 16] </ref>. * Expressing our bounds as penalty functions. One of the most interesting aspects of the VC theory is Vapnik's explicit prescription in the unrealizable setting for trading off hypothesis class complexity (and therefore, ability to realize the target function) against empirical error [30].
Reference: [17] <author> D. Haussler, M. Kearns, and R. E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proceedings of the 4th Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: some theorists have sought to preserve the functional form of the VC bounds, but to replace the VC dimension in this functional form by an appropriate distribution-specific quantity, such as the VC entropy (which is the expectation of the logarithm of the number of dichotomies realized by the function class) <ref> [30, 17, 3] </ref>. Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments [29]. <p> Second, although one might not implement such an algorithm in practice, any bound we can provide on its generalization error can provide bounds on the generalization error of optimal algorithms (such as the Bayes or Gibbs algorithms in a Bayesian framework <ref> [17] </ref>). In the thermodynamic limit, we may upper bound the generalization error of this algorithm by * = min * fl : (57) Let us interpret this bound. For each fixed fl, we are computing the rightmost crossing * fl fl of s fl (*) and ffu fl (*).
Reference: [18] <author> E. Levin, N. Tishby, and S. Solla. </author> <title> A statistical approach to learning and generalization in neural networks. </title> <editor> In R. Rivest, editor, </editor> <booktitle> Proc. 3rd Annu. Workshop on Com-put. Learning Theory. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [18, 24, 1, 27] </ref> and the thermodynamic limit [27]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 This theory will be distribution-specific, but will not attempt to force a power law form on learning curves.
Reference: [19] <author> Y.-D. Lyuu and I. Rivin. </author> <title> Tight bounds on transition to perfect generalization in perceptrons. </title> <journal> Neural Comput., </journal> <volume> 4 </volume> <pages> 854-862, </pages> <year> 1992. </year>
Reference-contexts: This bound on the critical value was known from the work of Gardner and Derrida [12], and extended to the case of boolean inputs by Baum, Lyuu and Rivin <ref> [2, 19] </ref>.
Reference: [20] <author> G. L. Martin and J. A. Pittman. </author> <title> Recognizing hand-printed letters and digits using backpropagation learning. </title> <journal> Neural Comput., </journal> <volume> 3 </volume> <pages> 258-267, </pages> <year> 1991. </year>
Reference-contexts: Backpropagation experiments with handwritten digits and characters indicate that good generalization error is sometimes obtained for sample sizes considerably smaller than the number of weights (presumed to be roughly the same as the VC dimension) <ref> [20] </ref>, though the VC bounds are vacuous for m smaller than d. Discrepancies between the VC bounds and actual learning curve behavior have also been pointed out and analyzed in other machine learning work [23, 21].
Reference: [21] <author> E. Oblow. </author> <title> Implementing Valiant's learnability theory using random sets. </title> <journal> Machine Learning, </journal> <volume> 8(1) </volume> <pages> 45-74, </pages> <year> 1992. </year>
Reference-contexts: Discrepancies between the VC bounds and actual learning curve behavior have also been pointed out and analyzed in other machine learning work <ref> [23, 21] </ref>. Of course, the VC bounds might simply be inapplicable to these experiments, because backpropagation is not equivalent to empirical error minimization. Vapnik has conjectured that backpropagation can access only a limited portion of the function space, so that the "effective dimension" is much smaller than the VC dimension. <p> The in put distribution D N is uniform over f0; 1g N . A similar sce nario has also been analyzed in the machine learning litera ture <ref> [23, 21] </ref>. We will examine the thermodynamic limit for two different choices of target functions f N . We begin with the target function f = f0; 1g N , in which every input is a positive example.
Reference: [22] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [30, 22, 7, 16] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells. <p> We are currently investigating extensions to the infinite case that are more refined than the covering approach discussed in Section 4.1, and are based on combining the shell decomposition with the VC dimension, VC entropy and random covering numbers <ref> [30, 22, 7, 16] </ref>. * Expressing our bounds as penalty functions. One of the most interesting aspects of the VC theory is Vapnik's explicit prescription in the unrealizable setting for trading off hypothesis class complexity (and therefore, ability to realize the target function) against empirical error [30].
Reference: [23] <author> W. Sarrett and M. Pazzani. </author> <title> Average case analysis of empirical and explanation-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 349-372, </pages> <year> 1992. </year>
Reference-contexts: Discrepancies between the VC bounds and actual learning curve behavior have also been pointed out and analyzed in other machine learning work <ref> [23, 21] </ref>. Of course, the VC bounds might simply be inapplicable to these experiments, because backpropagation is not equivalent to empirical error minimization. Vapnik has conjectured that backpropagation can access only a limited portion of the function space, so that the "effective dimension" is much smaller than the VC dimension. <p> The in put distribution D N is uniform over f0; 1g N . A similar sce nario has also been analyzed in the machine learning litera ture <ref> [23, 21] </ref>. We will examine the thermodynamic limit for two different choices of target functions f N . We begin with the target function f = f0; 1g N , in which every input is a positive example.
Reference: [24] <author> D. B. Schwartz, V. K. Samalam, J. S. Denker, and S. A. Solla. </author> <title> Exhaustive learning. </title> <journal> Neural Comput., </journal> <volume> 2 </volume> <pages> 374-385, </pages> <year> 1990. </year>
Reference-contexts: In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [18, 24, 1, 27] </ref> and the thermodynamic limit [27]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 This theory will be distribution-specific, but will not attempt to force a power law form on learning curves. <p> (53) Note how much weaker some of the bounds are than others. 3.5 Large-ff asymptotics of scaled learning curves Our formalism can be used to give a classification of the large-ff asymptotics of scaled learning curves, 7 thus com pleting a classification program that has been suggested by several researchers <ref> [24, 25, 1] </ref>. From Equation (32) and Lemma 9, the weaker form u (*) = 2v (*) is derived as a permissible energy bound in the Appendix in Section A.2.
Reference: [25] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review, </journal> <volume> A45:6056-6091, </volume> <year> 1992. </year>
Reference-contexts: Here we are actually giving a bound on the entire learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in both simulations and non-rigorous replica calculations from statistical physics <ref> [14, 28, 25, 9] </ref>. 6 In Figure 11, we graph the difference of the entropy and energy curves shown in Figure 3, that is, we plot s (*) + ff log (1 *) for the three values of ff. This plot is simply another way of visualizing the entropy-energy competition. <p> (53) Note how much weaker some of the bounds are than others. 3.5 Large-ff asymptotics of scaled learning curves Our formalism can be used to give a classification of the large-ff asymptotics of scaled learning curves, 7 thus com pleting a classification program that has been suggested by several researchers <ref> [24, 25, 1] </ref>. From Equation (32) and Lemma 9, the weaker form u (*) = 2v (*) is derived as a permissible energy bound in the Appendix in Section A.2. <p> In the presence of a logarithmic correction, s (*)v (*) ~ (*) 2 log *, the error bound decays exponentially with ff. This classification scheme is a generalization of that of Som polinsky and his colleagues to include unrealizable rules <ref> [25] </ref>. 7 Note that the large-ff asymptotics, which by definition invoke a thermodynamic limit, may be different from the large m asymptotics for a fixed function class. 12 4 The Infinite Case The final generalization of our theory that needs to be discussed is to the frequent case in which the
Reference: [26] <author> H. U. Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the 6th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 402-411. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: It has been shown that these bounds are essentially the best distribution-independent bounds possible, in the sense that for any function class, there exists an input distribution for which matching lower bounds on the generalization error can be given <ref> [6, 8, 26] </ref>.
Reference: [27] <author> H. Sompolinsky, H. S. Seung, and N. Tishby. </author> <title> Learning curves in large neural networks. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 112-127. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [18, 24, 1, 27] </ref> and the thermodynamic limit [27]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 This theory will be distribution-specific, but will not attempt to force a power law form on learning curves. <p> In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation [18, 24, 1, 27] and the thermodynamic limit <ref> [27] </ref>) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 This theory will be distribution-specific, but will not attempt to force a power law form on learning curves.
Reference: [28] <author> H. Sompolinsky, N. Tishby, and H. S. Seung. </author> <title> Learning from examples in large neural networks. </title> <journal> Phys. Rev. Lett., </journal> <volume> 65(13) </volume> <pages> 1683-1686, </pages> <year> 1990. </year>
Reference-contexts: We first consider the class of Ising perceptrons <ref> [12, 14, 28] </ref>. <p> Here we are actually giving a bound on the entire learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in both simulations and non-rigorous replica calculations from statistical physics <ref> [14, 28, 25, 9] </ref>. 6 In Figure 11, we graph the difference of the entropy and energy curves shown in Figure 3, that is, we plot s (*) + ff log (1 *) for the three values of ff. This plot is simply another way of visualizing the entropy-energy competition.
Reference: [29] <author> V. Vapnik, E. Levin, and Y. LeCun. </author> <title> Measuring the VC dimension of a learning machine. </title> <journal> Neural Comput., </journal> <note> 1994. To appear. </note>
Reference-contexts: Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments <ref> [29] </ref>. Perhaps the most striking evidence for the fact that the VC bounds can sometimes fail to model the true behavior of learning curves has come from statistical physics.
Reference: [30] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: 1 Introduction According to the Vapnik-Chervonenkis (VC) theory of learning curves <ref> [31, 30] </ref>, minimizing empirical error within a function class F on a random sample of m examples leads to generalization error bounded by ~ O (d=m) (in the case that the target function is contained in F ) or ~ O ( p d=m) plus the optimal generalization error achievable within <p> some theorists have sought to preserve the functional form of the VC bounds, but to replace the VC dimension in this functional form by an appropriate distribution-specific quantity, such as the VC entropy (which is the expectation of the logarithm of the number of dichotomies realized by the function class) <ref> [30, 17, 3] </ref>. Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments [29]. <p> In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [30, 22, 7, 16] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells. <p> This behavior has also been noted by Vapnik <ref> [30] </ref>. Returning to the general development, just as in the realizable case we can refine the union bound of Theorem 6 via a shell decomposition. Still more improvement may come 10 from finding a better energy function of the form in Equa--tion (32). <p> The idea of using empirical minimization over a finite cover for an infinite class has also been investigated by Benedek and Itai [3] in their investigation of distribution-specific sample complexity, and also by Vapnik <ref> [30] </ref>. Things become more interesting when we take the natural step of analyzing the algorithm that first chooses an advantageous value for the realizability parameter fl and then performs empirical minimization using F [fl]. <p> We are currently investigating extensions to the infinite case that are more refined than the covering approach discussed in Section 4.1, and are based on combining the shell decomposition with the VC dimension, VC entropy and random covering numbers <ref> [30, 22, 7, 16] </ref>. * Expressing our bounds as penalty functions. One of the most interesting aspects of the VC theory is Vapnik's explicit prescription in the unrealizable setting for trading off hypothesis class complexity (and therefore, ability to realize the target function) against empirical error [30]. <p> One of the most interesting aspects of the VC theory is Vapnik's explicit prescription in the unrealizable setting for trading off hypothesis class complexity (and therefore, ability to realize the target function) against empirical error <ref> [30] </ref>. This prescription is known as structural risk minimization, and the form it takes can be directly traced to the form of the VC bounds on learning curves.
Reference: [31] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction According to the Vapnik-Chervonenkis (VC) theory of learning curves <ref> [31, 30] </ref>, minimizing empirical error within a function class F on a random sample of m examples leads to generalization error bounded by ~ O (d=m) (in the case that the target function is contained in F ) or ~ O ( p d=m) plus the optimal generalization error achievable within
Reference: [32] <author> T. L. H. Watkin, A. Rau, and M. Biehl. </author> <title> The statistical mechanics of learning a rule. </title> <journal> Rev. Mod. Phys., </journal> <volume> 65 </volume> <pages> 499-556, </pages> <year> 1993. </year>
Reference-contexts: In recent years, the tools of statistical mechanics have been applied to analyze learning curves with rather curious and dramatic behavior (See the survey of Watkin, Rau and Biehl and the references therein <ref> [32] </ref>). This has included learning curves exhibiting "phase transitions" (sudden drops in the generalization error) at small sample sizes, as well as asymptotic power law behavior 2 in which the power law exponent is neither 1 nor 1/2.
References-found: 32

