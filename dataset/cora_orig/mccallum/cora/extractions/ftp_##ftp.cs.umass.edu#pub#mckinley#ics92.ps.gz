URL: ftp://ftp.cs.umass.edu/pub/mckinley/ics92.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: ken@rice.edu kats@rice.edu  
Title: Optimizing for Parallelism and Data Locality  
Author: Ken Kennedy Kathryn S. M c Kinley 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Previous research has used program transformation to introduce parallelism and to exploit data locality. Unfortunately, these two objectives have usually been considered independently. This work explores the tradeoffs between effectively utilizing parallelism and memory hierarchy on shared-memory multiprocessors. We present a simple, but surprisingly accurate, memory model to determine cache line reuse from both multiple accesses to the same memory location and from consecutive memory access. The model is used in memory optimizing and loop parallelization algorithms that effectively exploit data locality and parallelism in concert. We demonstrate the efficacy of this approach with very encouraging experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [ACK87] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Auto matic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Many algorithms have been proposed in the literature for introducing parallelism into programs. Calla-han et al. use the metric of minimizing barrier syn chronization points via loop distribution, fusion and interchange for introducing parallelism <ref> [ACK87, Cal87] </ref>. Wolf and Lam [WL90] introduce all possible parallelism via the unimodular transformations: loop interchange, skewing, and reversal. Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered.
Reference: [AK84] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic loop inter change. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The same result is obtained by previous researchers <ref> [AK84, WL91] </ref>. 5.2 Permuting to Achieve Memory Order We must now decide whether the desired memory order is legal. If it is not, we must select some legal loop permutation close to memory order. To determine Page 6 whether a loop permutation is legal is straightforward. <p> The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> The proof by contradiction of the theorem proceeds as follows. Given an original set of legal direction vectors, each step of the "for" is guaranteed to find a loop which results in a legal direction vector, otherwise the original was not legal <ref> [AK84, Ban90a] </ref>. In addition, if any loop 1 through n1 may be legally positioned prior to n it will be.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Dependences may also be characterized as either loop-independent or loop-carried. Loop-independent dependences occur on the same iteration of a loop. A dependence between iterations of a loop is called loop-carried and prevents the iterations of a loop from being executed in parallel <ref> [AK87] </ref>. A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Data dependence is used to determine the legality of a given loop permutation by checking whether any permuted true, anti, or output dependence vector becomes lexicographically negative [Ban90b, WL90].
Reference: [AS79] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Vir tual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: For reasonably large computations, references such as C (J,I) do not provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. In this paper we concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line.
Reference: [Ban90a] <author> U. Banerjee. </author> <title> A theory of loop permutations. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> The proof by contradiction of the theorem proceeds as follows. Given an original set of legal direction vectors, each step of the "for" is guaranteed to find a loop which results in a legal direction vector, otherwise the original was not legal <ref> [AK84, Ban90a] </ref>. In addition, if any loop 1 through n1 may be legally positioned prior to n it will be.
Reference: [Ban90b] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Data dependence is used to determine the legality of a given loop permutation by checking whether any permuted true, anti, or output dependence vector becomes lexicographically negative <ref> [Ban90b, WL90] </ref>. Data dependence also characterizes reuse of individual memory locations [CCK90]. 2.2 Memory and Language Model The techniques developed in this paper are intended for shared-memory multiprocessors where each processor has at a local cache and the processors are connected with a common bus. <p> The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered. Banerjee also considers introducing parallelism via unimodular transformations, but only for doubly nested loops <ref> [Ban90b] </ref>. Banerjee does however consider loop bound information. Because we accept some imprecision, our algorithms are simpler and may be applied to computations that have not been fully characterized in Wolf and Lam's unimodular framework. For instance, we can support imperfectly nested loops, multiple loop nests, and imprecise data dependences.
Reference: [BFKK92] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kre mer. </author> <title> A static performance estimator in the Fortran D programming system. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Our performance estimator predicts the cost of parallel and sequential performance using a loop model and a training set approach. The goal of our performance estimator is to assist in code generation for both shared and distributed memory multiprocessors <ref> [BFKK92, KMM91] </ref>. Modeling the target machines at an architectural level would require calculating an analytical model for each supported architecture. Instead our performance estimator uses a training set to characterize each architecture in a machine-independent fashion.
Reference: [Cal87] <author> D. Callahan. </author> <title> A Global Approach to Detection of Par allelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: Many algorithms have been proposed in the literature for introducing parallelism into programs. Calla-han et al. use the metric of minimizing barrier syn chronization points via loop distribution, fusion and interchange for introducing parallelism <ref> [ACK87, Cal87] </ref>. Wolf and Lam [WL90] introduce all possible parallelism via the unimodular transformations: loop interchange, skewing, and reversal. Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This optimization approach may be divided into three phases: 1. optimizing to improve data locality, 2. finding and positioning a parallel loop, and 3. performing low-level memory optimizations such as tiling for cache and placing references in registers <ref> [LRW91, CCK90] </ref>. This paper focuses on the first two phases. We advocate the first two phases be followed by a low-level memory optimizing phase, but do not address it here. The remainder of this paper is divided into 10 sections. <p> Data dependence is used to determine the legality of a given loop permutation by checking whether any permuted true, anti, or output dependence vector becomes lexicographically negative [Ban90b, WL90]. Data dependence also characterizes reuse of individual memory locations <ref> [CCK90] </ref>. 2.2 Memory and Language Model The techniques developed in this paper are intended for shared-memory multiprocessors where each processor has at a local cache and the processors are connected with a common bus. Because we are evaluating reuse, we require some knowledge of the memory hierarchy. <p> For reasonably large computations, references such as C (J,I) do not provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. In this paper we concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. <p> When we apply RefCost to calculate the number of cache lines accessed by a reference group, we need to select the most deeply nested member of 1 Of course, loop-invariant references should eventually be put in registers by later optimizations <ref> [CCK90] </ref>. <p> We believe that this approximation is a very reasonable one, especially in view of the fact that we intend to use a scalar cache tiling method as a final step in the code generation process <ref> [CCK90] </ref>.
Reference: [CKPK90] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Su percomputer performance evaluation and the Perfect benchmarks. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amster-dam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: These loops may be an artifact of a vectorizable programming style. They appear frequently in the Perfect benchmarks <ref> [CKPK90] </ref>, the Level 2 BLAS [DCHH88], and the Livermore loops [McM86]. Table 3 illustrates the performance benefits with the organization of dmxpy generated by our algorithm on matrices of size 200fi200 on 19 processors.
Reference: [DBMS79] <author> J. Dongarra, J. Bunch, C. Moler, and G. </author> <title> Stew art. LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: To illustrate this point, consider the subroutine dmxpy from Lin-packd written in memory order <ref> [DBMS79] </ref>. DO J = JMIN, N2 DO I = 1, N1 The J loop is not parallel. The I loop can be parallel. Both contain reuse. A simple parallelization that maximizes granularity would interchange the two loops and make the I loop parallel without strip mining.
Reference: [DCHH88] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An extended set of Fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: These loops may be an artifact of a vectorizable programming style. They appear frequently in the Perfect benchmarks [CKPK90], the Level 2 BLAS <ref> [DCHH88] </ref>, and the Livermore loops [McM86]. Table 3 illustrates the performance benefits with the organization of dmxpy generated by our algorithm on matrices of size 200fi200 on 19 processors.
Reference: [FST91] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimat ing and enhancing cache effectiveness. </title> <editor> In U. Baner-jee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Ferrante et al. present a more general formula that approximates the number of cache lines and is applicable across a wider range of loops <ref> [FST91] </ref>. However, they first compute an estimate for every array reference in a loop nest and then combine them, trying not to do dependence testing. Like Wolf and Lam, they exhaustively search for a loop permutation with the lowest estimated cost.
Reference: [GJG88] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: For reasonably large computations, references such as C (J,I) do not provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. In this paper we concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. <p> Skewing in particular is undesirable because it reduces spatial reuse. Gannon et al. also formulate the dependence testing problem to give reuse and volumetric information about array references <ref> [GJG88] </ref>. This information is then used to tile and interchange the loop nests for cache, after which parallelism is inserted at the outermost possible position. They do not consider how the parallelism affects the volumetric information nor if interchange would improve the granularity of parallelism.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Tiling combines strip mining and loop interchange to promote reuse across a loop nest <ref> [IT88, Wol89a] </ref>. For matrix multiply, the loop nest may be tiled by strip mining the K loop by TS and then interchanging it with J.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Reuse of a particular memory reference for arrays can be discovered using fl Proceedings of the 1992 ACM International Conference on Supercomputing, Washington, D.C, July, 1992. data-dependence analysis <ref> [KKP + 81] </ref>. However, reuse of consecutive accesses, often called unit stride access, is a significant source of reuse that can easily be determined when the storage order of arrays and the cache line size is known. <p> We then overview related work and conclude. 2 Background 2.1 Data Dependence Dependence analysis is the compile-time analysis of a program's memory accesses. A data dependence between two references Ref 1 and Ref 2 indicates that they read or write a common memory location <ref> [KKP + 81] </ref>. True, anti, and output dependences arise when at least one reference is write; the order between Ref 1 and Ref 2 must be preserved to maintain the semantics of the original program.
Reference: [KMC72] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: Input dependences arise if both Ref 1 and Ref 2 are reads; they do not restrict program order. Data dependences may be characterized by their access pattern between loop iterations. The number of loop iterations d separating the source and sink of the dependence is its dependence distance <ref> [KMC72, Lam74] </ref>; it may also be summarized as a dependence direction consisting of `&lt;', `=', or `&gt;' [WB87, Wol89b]. Dependence distances and directions are represented as a vector whose elements, displayed left to right, represent the dependence from the outermost to the innermost loop in the nest.
Reference: [KMM91] <author> K. Kennedy, N. McIntosh, and K. S. M c Kinley. </author> <title> Static performance estimation in a parallelizing compiler. </title> <type> Technical Report TR91-174, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Our performance estimator predicts the cost of parallel and sequential performance using a loop model and a training set approach. The goal of our performance estimator is to assist in code generation for both shared and distributed memory multiprocessors <ref> [BFKK92, KMM91] </ref>. Modeling the target machines at an architectural level would require calculating an analytical model for each supported architecture. Instead our performance estimator uses a training set to characterize each architecture in a machine-independent fashion.
Reference: [KMT92] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Improv ing data locality. </title> <type> Technical Report TR92-179, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: In this paper we concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. Empirical results show that improving spatial reuse can be significantly more effective than techniques that consider temporal reuse alone <ref> [KMT92] </ref>. In addition, consecutive memory access results in reuse at all levels of the memory hierarchy except for registers. 4.2 Simplifying Assumptions To simplify analysis we make two assumptions. First, our loop cost function assumes that reuse occurs only across iterations of the innermost loop.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: Input dependences arise if both Ref 1 and Ref 2 are reads; they do not restrict program order. Data dependences may be characterized by their access pattern between loop iterations. The number of loop iterations d separating the source and sink of the dependence is its dependence distance <ref> [KMC72, Lam74] </ref>; it may also be summarized as a dependence direction consisting of `&lt;', `=', or `&gt;' [WB87, Wol89b]. Dependence distances and directions are represented as a vector whose elements, displayed left to right, represent the dependence from the outermost to the innermost loop in the nest.
Reference: [LRW91] <author> M. Lam, E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: This optimization approach may be divided into three phases: 1. optimizing to improve data locality, 2. finding and positioning a parallel loop, and 3. performing low-level memory optimizations such as tiling for cache and placing references in registers <ref> [LRW91, CCK90] </ref>. This paper focuses on the first two phases. We advocate the first two phases be followed by a low-level memory optimizing phase, but do not address it here. The remainder of this paper is divided into 10 sections. <p> Lam et al. show that this assumption may not hold if cache lines must remain live for longer periods of time. Considerable interference may take place when loops are tiled to increase reuse across outer loops <ref> [LRW91] </ref>. 4.3 Loop Cost Function Given these assumptions, we present a loop cost function LoopCost based on our memory model. Its goal is to estimate the total number of cache lines accessed when a candidate loop l is positioned as the innermost loop.
Reference: [McK92] <author> K. S. McKinley. </author> <title> Automatic and Interactive Paral lelization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: If parallelism is not discovered and would be profitable, other optimization strategies that consider all loop permutations, loop skewing [WL90], or loop distribution <ref> [McK92] </ref> should be explored. 8 Experimental results We tested the algorithm for optimizing data locality independently and report some of those results here. The overall parallelization strategy was also tested by applying it by hand to several kernels and to the program Erlebacher, provided by Thomas Eidson from ICASE.
Reference: [McM86] <author> F. McMahon. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1986. </year>
Reference-contexts: These loops may be an artifact of a vectorizable programming style. They appear frequently in the Perfect benchmarks [CKPK90], the Level 2 BLAS [DCHH88], and the Livermore loops <ref> [McM86] </ref>. Table 3 illustrates the performance benefits with the organization of dmxpy generated by our algorithm on matrices of size 200fi200 on 19 processors. For comparison, the performance when the I strip is not returned to its best memory position and a parallel inner I loop were also measured.
Reference: [Por89] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: They do not consider how the parallelism affects the volumetric information nor if interchange would improve the granularity of parallelism. Porterfield presents a formula that approximates the number of cache lines accessed, but is restricted to a cache line size of one and loops with uniform dependences <ref> [Por89] </ref>. Ferrante et al. present a more general formula that approximates the number of cache lines and is applicable across a wider range of loops [FST91]. However, they first compute an estimate for every array reference in a loop nest and then combine them, trying not to do dependence testing.
Reference: [WB87] <author> M. J. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Data dependences may be characterized by their access pattern between loop iterations. The number of loop iterations d separating the source and sink of the dependence is its dependence distance [KMC72, Lam74]; it may also be summarized as a dependence direction consisting of `&lt;', `=', or `&gt;' <ref> [WB87, Wol89b] </ref>. Dependence distances and directions are represented as a vector whose elements, displayed left to right, represent the dependence from the outermost to the innermost loop in the nest. By definition all distance and direction vectors are lexicographically positive.
Reference: [WL90] <author> M. E. Wolf and M. Lam. </author> <title> Maximizing parallelism via loop transformations. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: A dependence is carried by the outermost loop for which the element in the direction vector is not an `='. Data dependence is used to determine the legality of a given loop permutation by checking whether any permuted true, anti, or output dependence vector becomes lexicographically negative <ref> [Ban90b, WL90] </ref>. Data dependence also characterizes reuse of individual memory locations [CCK90]. 2.2 Memory and Language Model The techniques developed in this paper are intended for shared-memory multiprocessors where each processor has at a local cache and the processors are connected with a common bus. <p> The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> If parallelism is not discovered and would be profitable, other optimization strategies that consider all loop permutations, loop skewing <ref> [WL90] </ref>, or loop distribution [McK92] should be explored. 8 Experimental results We tested the algorithm for optimizing data locality independently and report some of those results here. <p> Many algorithms have been proposed in the literature for introducing parallelism into programs. Calla-han et al. use the metric of minimizing barrier syn chronization points via loop distribution, fusion and interchange for introducing parallelism [ACK87, Cal87]. Wolf and Lam <ref> [WL90] </ref> introduce all possible parallelism via the unimodular transformations: loop interchange, skewing, and reversal. Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered.
Reference: [WL91] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimiz ing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: For reasonably large computations, references such as C (J,I) do not provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. In this paper we concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. <p> The same result is obtained by previous researchers <ref> [AK84, WL91] </ref>. 5.2 Permuting to Achieve Memory Order We must now decide whether the desired memory order is legal. If it is not, we must select some legal loop permutation close to memory order. To determine Page 6 whether a loop permutation is legal is straightforward. <p> No low-level memory optimizations were performed. The speed-up from this algorithm on 19 processors was 14.2 for the entire application. The speed-up for the parallel portions of the program was 15.0. 9 Related work Our work bears the most similarity to research by Wolf and Lam <ref> [WL91] </ref>. They develop an algorithm that estimates all temporal and spatial reuse for a given loop permutation, including reuse on outer loops. This reuse is represented as a localized vector space. Vector spaces representing reuse for individual and multiple references are combined to discover all loops L carrying some reuse.
Reference: [Wol89a] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceed ings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Tiling combines strip mining and loop interchange to promote reuse across a loop nest <ref> [IT88, Wol89a] </ref>. For matrix multiply, the loop nest may be tiled by strip mining the K loop by TS and then interchanging it with J.
Reference: [Wol89b] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Super computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <pages> Page 12 </pages>
Reference-contexts: Data dependences may be characterized by their access pattern between loop iterations. The number of loop iterations d separating the source and sink of the dependence is its dependence distance [KMC72, Lam74]; it may also be summarized as a dependence direction consisting of `&lt;', `=', or `&gt;' <ref> [WB87, Wol89b] </ref>. Dependence distances and directions are represented as a vector whose elements, displayed left to right, represent the dependence from the outermost to the innermost loop in the nest. By definition all distance and direction vectors are lexicographically positive.
References-found: 29

