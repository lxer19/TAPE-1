URL: http://www.robotics.stanford.edu/~koller/papers/uai98.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/uai98.html
Root-URL: http://www.robotics.stanford.edu
Email: xb@cs.stanford.edu  koller@cs.stanford.edu  
Title: Tractable Inference for Complex Stochastic Processes  
Author: Xavier Boyen Daphne Koller 
Affiliation: Stanford University  Stanford University  
Date: July 1998  
Address: Madison, Wisconsin,  
Note: In Proceedings of the Fourteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-98), pages 33-42,  
Abstract: The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief statea probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K.J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> J. Math. Anal. Applic., </journal> <volume> 10, </volume> <year> 1965. </year>
Reference-contexts: Since the domain is stochastic and partially observable, the true state of the process is rarely known with certainty. However, most reasoning tasks can be performed by using a belief state, which is a probability distribution over the state of a system at a given time <ref> [1] </ref>. It follows from the Markov assumption that the belief state at time t completely captures all of our information about the past. In particular, it suffices both for predicting the probabilities of future trajectories of the system, and for making optimal decisions about our actions.
Reference: [2] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: More specifically, we have proved the important (and to our knowledge) new result that stochastic processes (under certain assumptions) are a contraction for KL-divergence <ref> [2] </ref>: propagation of two distributions through a stochastic transition model results in a constant factor reduction of the KL-divergence between them. We believe that this result will have significant ap plications in many other contexts. <p> Thus, we must search for an alternative distance measure for which to try and prove our contraction result. The obvious candidate is relative entropy, or KL divergence, which quantifies the loss or inefficiency incurred by using distribution when the true distribution is ''' <ref> [2, p.18] </ref>: Definition 2 If ''' and are two distributions over the same space W, the relative entropy of ''' to is ID [''' k ] = IE ''' [ln X '''[! i ] ln [! i ] Relative entropy is, for a variety of reasons detailed in [2, ch.2], a <p> is ''' [2, p.18]: Definition 2 If ''' and are two distributions over the same space W, the relative entropy of ''' to is ID [''' k ] = IE ''' [ln X '''[! i ] ln [! i ] Relative entropy is, for a variety of reasons detailed in <ref> [2, ch.2] </ref>, a very natural measure of discrepancy to use between a distribution and an approximation to it. <p> Unfortunately, we seem to have simply shifted the problem from one place to another. While relative entropy is better behaved with respect to O, there is no known contraction result for T . Indeed, until now, the only related properties that seem to have been known <ref> [2, p.34] </ref> are that relative entropy never increases by transition through a stochastic process (i.e., that ID [T [ (t*) ] k T [ (t*) ]] ID [ (t*) k (t*) ]), and that it ultimately tends to zero for a very broad class of processes (i.e., ID [T k [ <p> First, the bound involves relative entropy between the two entire belief states. In certain applications, we may be interested in errors for individual variables or for subsets of variables. Fortunately, any bound on the entire distribution immediately carries over to any projection onto a subset of variables <ref> [2] </ref>. Furthermore, we note that bounds on relative entropy immediately imply bounds on the L 1 error, as k''' k 1 (2 ln 2ID [''' k ]) 1=2 . Second, note that the bounds are on the expected error; the error for specific sequences of evidence are much weaker.
Reference: [3] <author> P. Dagum, A. Galper, and E. Horwitz. </author> <title> Dynamic network models for forecasting. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications <ref> [6, 8, 3, 14] </ref>. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models [15], Kalman Filters [9], and Dynamic Bayesian Networks [4]. <p> Here, a belief state is a distribution over some subset of the state variables at time t. In general, not all of the variables at time t must participate in the belief state <ref> [3] </ref>; however, (at least) every variable whose value at time t directly affects its value at time t + 1 must be included. In large DBNs, the obvious representation of a belief state (as a flat distribution over its state space) is therefore typically infeasible, particularly in time-critical applications.
Reference: [4] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Comp. Int., </journal> <volume> 5(3), </volume> <year> 1989. </year>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications [6, 8, 3, 14]. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models [15], Kalman Filters [9], and Dynamic Bayesian Networks <ref> [4] </ref>. These very different models all share the same underlying Markov assumption, the fact that the future is conditionally independent of the past given the current state. Since the domain is stochastic and partially observable, the true state of the process is rarely known with certainty.
Reference: [5] <author> R. Dechter and I. Rish. </author> <title> A scheme for approximating probabilistic inference. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1997. </year>
Reference-contexts: Specifically, the mini-bucket approach of <ref> [5] </ref> is somewhat related to ours, as it also uses a (different) form of factoring decomposition during the course of inference. However, none of the potentially relevant algorithms are associated with any performance guarantees on the error of approximation over time.
Reference: [6] <author> J. Forbes, T. Huang, K. Kanazawa, and S.J. Russell. </author> <title> The BATmobile: Towards a Bayesian automated taxi. </title> <booktitle> In Proc. IJCAI, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications <ref> [6, 8, 3, 14] </ref>. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models [15], Kalman Filters [9], and Dynamic Bayesian Networks [4]. <p> Our experimental results illustrate these tradeoffs. 5.3 Experimental results We validated this algorithm in the context of two real-life DBNs: the WATER network [8], used for monitoring the biological processes of a water purification plant; and the BAT network <ref> [6] </ref>, used for monitoring freeway traffic (see did not have any; these duplicate a few of the state variables lines indicate some of the clusterings used in our approximations. with added noise.
Reference: [7] <author> Z. Ghahramani and M.I. Jordan. </author> <title> Factorial hidden Markov models. </title> <booktitle> In Proc. NIPS, </booktitle> <year> 1996. </year>
Reference-contexts: Both of these ideas can be viewed as falling into the framework described in this paper. However, neither contains any analysis nor an explicit connection to the structure of the process. The recent work of <ref> [7] </ref> and [16] utilize mean field approximation in the context of various types of structured HMMs. Of these approaches, [7] is the closest to our work (to the part of it dealing with structured processes). <p> However, neither contains any analysis nor an explicit connection to the structure of the process. The recent work of <ref> [7] </ref> and [16] utilize mean field approximation in the context of various types of structured HMMs. Of these approaches, [7] is the closest to our work (to the part of it dealing with structured processes). In their approach, the compound process is approximated as being composed of independent subprocesses, whose parameters are chosen in a way that depends on the evidence. This approach differs from ours in several ways.
Reference: [8] <author> F.V. Jensen, U. Kjrulff, K.G. Olesen, and J. Ped-ersen. </author> <title> An expert system for control of waste water treatmenta pilot project. </title> <type> Technical report, </type> <note> Judex Datasystemer A/S, Aalborg, 1989. In Danish. </note>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications <ref> [6, 8, 3, 14] </ref>. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models [15], Kalman Filters [9], and Dynamic Bayesian Networks [4]. <p> Specifically, if we have two (sets of) variables that are highly correlated, splitting them into two separate subprocesses is not a good idea. Our experimental results illustrate these tradeoffs. 5.3 Experimental results We validated this algorithm in the context of two real-life DBNs: the WATER network <ref> [8] </ref>, used for monitoring the biological processes of a water purification plant; and the BAT network [6], used for monitoring freeway traffic (see did not have any; these duplicate a few of the state variables lines indicate some of the clusterings used in our approximations. with added noise.
Reference: [9] <author> R.E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> J. of Basic Engineering, </journal> <year> 1960. </year>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications [6, 8, 3, 14]. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models [15], Kalman Filters <ref> [9] </ref>, and Dynamic Bayesian Networks [4]. These very different models all share the same underlying Markov assumption, the fact that the future is conditionally independent of the past given the current state. <p> The effectiveness of this procedure (as well as of many others) depends crucially on the representation of the belief state. Certain types of systems, e.g., Gaussian processes, admit a compact representation of the belief state and an effective update process (via Kalman filtering <ref> [9] </ref>). However, in other cases matters are not so simple. Consider, for example, a stochastic system represented as a dynamic Bayesian network (DBN).
Reference: [10] <author> K. Kanazawa, D. Koller, and S.J. Russell. </author> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1995. </year>
Reference-contexts: There has been fairly little work on approximate inference in complex temporal models. 4 The early work of [14] considers a simple approach of using domain knowledge to simply eliminate some of the variables from each time slice. The random sampling approach of <ref> [10] </ref> can also be viewed as maintaining an approximate belief state, albeit one represented very naively as a set of weighted samples. The recent work of [12] extends this idea, using the random samples at each time slice as data in order to learn an approximate belief state.
Reference: [11] <author> U. Kjrulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1992. </year>
Reference-contexts: We applied the idea of approximate inference to the monitoring task. Exact inference for this task is infeasible in complex stochastic processes with a large number of states. Even if the process is highly structured, exact inference (e.g., <ref> [11] </ref>) is forced into intractability by the full correlation of the belief state that invariably occurs. We propose an approach whereby the algorithm maintains an approximate belief state with compact representation.
Reference: [12] <author> D. Koller and R. Fratkina. </author> <title> Using learning for approximation in stochastic processes. </title> <booktitle> In Proc. ML, </booktitle> <year> 1998. </year>
Reference-contexts: Thus, rather than using an exact belief state which is very complex, we use a compactly represented approximate belief state. For example, in the context of a DBN, we might choose to represent an approximate belief state using a factored representation. (See <ref> [12] </ref> for some discussion of possible belief state representations for DBNs.) In the context of a hybrid process, we might choose to restrict the number of components in our Gaussian mixture. <p> The random sampling approach of [10] can also be viewed as maintaining an approximate belief state, albeit one represented very naively as a set of weighted samples. The recent work of <ref> [12] </ref> extends this idea, using the random samples at each time slice as data in order to learn an approximate belief state. Both of these ideas can be viewed as falling into the framework described in this paper. <p> For example, as our experiments show, it can be very beneficial to make the states of two processes in our approximate belief state conditionally independent given a third; we would like to derive the formal conditions under which this is indeed an advantage. The results of <ref> [12] </ref> also demonstrate that other representations of a belief state might be useful, specifically ones that allow some context-sensitivity of the structure of the distribution.
Reference: [13] <author> S.L. Lauritzen and D.J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Roy. Stat. Soc., </journal> <volume> B 50, </volume> <year> 1988. </year>
Reference-contexts: In the case of DBNs, we can actually accomplish this update procedure quite efficiently. We first generate a clique tree <ref> [13] </ref> in which, for every l, some clique contains XXX (t) and some clique contains XXX (t+1) l . A standard clique tree propagation algorithm can then be used to compute the posterior distribution over every clique.
Reference: [14] <author> G. Provan. </author> <title> Tradeoffs in constructing and evaluating temporal influence diagrams. </title> <booktitle> In Proc. </booktitle> <address> UAI, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications <ref> [6, 8, 3, 14] </ref>. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models [15], Kalman Filters [9], and Dynamic Bayesian Networks [4]. <p> Thus, our result is the first to show how the structure of a stochastic process can be exploited for inference. There has been fairly little work on approximate inference in complex temporal models. 4 The early work of <ref> [14] </ref> considers a simple approach of using domain knowledge to simply eliminate some of the variables from each time slice. The random sampling approach of [10] can also be viewed as maintaining an approximate belief state, albeit one represented very naively as a set of weighted samples.
Reference: [15] <author> L. Rabiner and B. Juang. </author> <title> An introduction to hidden Markov models. </title> <booktitle> IEEE Acoustics, Speech & Signal Processing, </booktitle> <year> 1986. </year>
Reference-contexts: 1 Introduction The ability to model and reason about stochastic processes is fundamental to many applications [6, 8, 3, 14]. A number of formal models have been developed for describing situations of this type, including Hidden Markov Models <ref> [15] </ref>, Kalman Filters [9], and Dynamic Bayesian Networks [4]. These very different models all share the same underlying Markov assumption, the fact that the future is conditionally independent of the past given the current state.
Reference: [16] <author> L.K. Saul and M.I. Jordan. </author> <title> Exploiting tractable substructure in intractable networks. </title> <booktitle> In Proc. NIPS, </booktitle> <year> 1995. </year>
Reference-contexts: Both of these ideas can be viewed as falling into the framework described in this paper. However, neither contains any analysis nor an explicit connection to the structure of the process. The recent work of [7] and <ref> [16] </ref> utilize mean field approximation in the context of various types of structured HMMs. Of these approaches, [7] is the closest to our work (to the part of it dealing with structured processes).
Reference: [17] <author> P. Smyth, D. Heckerman, and M.I. Jordan. </author> <title> Probabilistic independence networks for hidden Markov probability models. </title> <journal> Neural Computation, </journal> <volume> 9(2), </volume> <year> 1996. </year>
Reference-contexts: This is very fortunate, as it is feasible to enforce decomposability properties on the approximate belief state, whereas we have no control over the true belief state. Formally, it is most convenient to describe our results in the framework of factored HMMs <ref> [17] </ref>; in the next section, we discuss how they can be applied to dynamic Bayesian networks. We assume that our system is composed of several subprocesses T l . Each subprocess has a state with a Markovian evolution model.
References-found: 17

