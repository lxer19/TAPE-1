URL: http://www.cs.colostate.edu/~ftppub/TechReports/1997/tr97-115.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Email: malaiya@cs.colostate.edu  
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: What Do the Software Reliability Growth Model Parameters Represent?  
Author: Yashwant K. Malaiya and Jason Denton 
Note: This research was supported by a BMDO funded project monitored by ONR  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523  Fort Collins, CO 80523-1873  
Affiliation: Computer Science  Computer Science Dept. Colorado State University  Computer Science Department Colorado State University  
Pubnum: Technical Report  Technical Report CS-97-115  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.R. Adam, </author> <title> "Software Reliability Predictions are Practical Today", </title> <booktitle> Proc. IEEE Ann. Symp. on Software Reliability, </booktitle> <address> Colorado Springs, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: F s = 1 + 0:4a (14) where a is the fraction of the code in assembly language. Here we are assuming that assembly code has 40% more defects <ref> [1] </ref>. 9 3.1.5 Calibrating and using the defect density model The model given in equation 13 provides an initial estimate. It should be calibrated using past data from the same organization.
Reference: [2] <author> W. W. Agresti and W. M. Evanco, </author> <title> "Projecting Software Defects from Analyzing Ada Designs," </title> <journal> IEEE Trans. Software Engineering, </journal> <month> Nov. </month> <year> 1992, </year> <pages> pp. 288-297. </pages>
Reference-contexts: Such models have also been used to estimate the hardware failure rates. Several linear additive models for estimating the number of defects have also been proposed, they have the disadvantage that they can project zero or negative number of defects. The models by Agresti and Evanco <ref> [2] </ref>, Rome Lab [21] and THAAD [6] are factor multiplicative like our model. A preliminary version of our model [13] is being implemented in the ROBUST software reliability tool [10]. Our model, presented below, has the following advantages: 1. <p> There is some evidence that for the same size, modules with significantly higher complexity are likely to have a higher number of defects. However, further studies are needed to propose a model. It is known that module size influences defect density with a module <ref> [2] </ref>. However in a software system consisting of modules, the variability due to different block sizes may cancel out if we are considering the average defect density.
Reference: [3] <author> G.F. Cole and S.N. Keene, </author> <title> "Reliability and Growth of Fielded Software," Reliability Review, </title> <month> March </month> <year> 1994, </year> <pages> pp. 5-26. 15 </pages>
Reference-contexts: Here we assume level II as the default level, since a level I organization is not likely to be using software reliability engineering. Kolkhurst [9] assumes that for delivered software, change from level II to level V will reduce defect density by a factor of 500. However, Keene <ref> [3] </ref> suggests a reduction in the inherent defect density by a factor of 20 for the same change. Jones [7] suggests an improvement by a factor of 4 in potential defects and a factor of 9 in delivered defects for changing from level II to level V.
Reference: [4] <author> W. Farr, </author> <title> Software Reliability Modeling Survey, in Handbook of Software Reliability Engi--neering, </title> <editor> Ed. M. R. Lyu, </editor> <publisher> McGraw-Hill, </publisher> <year> 1996, </year> <pages> pp. 71-117. </pages>
Reference-contexts: It is given by (t) = fi E 1 t ) (1) where (t) is the mean value function and fi E 0 and fi E 1 are the two model parameters. Farr mentions that this model has had the widest distribution among the software reliability models <ref> [4] </ref>. Musa [18] states that the basic execution model generally appears to be superior in capability and applicability to other published models. Some of the other models are similar to this model. The Logarithmic model is other model considered here. It is also termed Musa-Okumoto logarithmic poisson Model. <p> It is also termed Musa-Okumoto logarithmic poisson Model. It is given by (t) = fi L 1 t) (2) where fi L 0 and fi L 1 are the two model parameters. Farr states that the logarithmic model is one of the model that has been extensively applied <ref> [4] </ref>. This is one of the selected models in the AIAA Recommended Practice Standard. Musa [18] writes that the logarithmic model is superior in predictive validity compared with the exponential model.
Reference: [5] <author> J. Gaffney and J. Pietrolewicz, </author> <title> "An Automated Model for Early Error Prediction in Software Development Process," </title> <booktitle> Proc. IEEE Software Reliability Symposium Colorado Spring, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Our model, presented below, has the following advantages: 1. It can be used when only incomplete or partial information is available. The default value of a multiplicative factor is one, which corresponds to the average case. 2. It takes into account the phase dependence as suggested by Gaffney <ref> [5] </ref> 3. It can be recalibrated by choosing a suitable constant of proportionality and be refined by using a better model for each factor, when additional data is available. <p> The constant of proportionality C represents the defect density per thousand source lines of code (KSLOC). Here are the preliminary sub-models for each factor. 7 3.1.1 Phase Factor (F ph ) The number of defects at the beginning of different test phases is different. Gaffney <ref> [5] </ref> has proposed a phase based model that uses the Rayleigh curve. Here we present a simpler model using actual data reported by Musa et al. [18] and the error profile presented by Piwowarski et al. [20]. <p> = 2:75 i:e: fi L 142 = 51:6 (28) 3.3 Direct Estimation of fi L 0 and fi L An alternative to the above method is to use the interpretation of fi L 0 and fi L 1 in terms of D min and K min as given by equation <ref> [5] </ref>. A reasonable estimate for K min is 1:5 fi 10 7 as suggested by the data given by Musa et al. [18]. As estimation of D min , the defect density at which the minimum value of K occurs is harder to estimate.
Reference: [6] <author> M. Gechman and K. Kao, </author> <title> "Tracking Software Reliability and Reliability with Metrics," </title> <booktitle> Proc. ISSRE Industry Reports, </booktitle> <year> 1994. </year>
Reference-contexts: Several linear additive models for estimating the number of defects have also been proposed, they have the disadvantage that they can project zero or negative number of defects. The models by Agresti and Evanco [2], Rome Lab [21] and THAAD <ref> [6] </ref> are factor multiplicative like our model. A preliminary version of our model [13] is being implemented in the ROBUST software reliability tool [10]. Our model, presented below, has the following advantages: 1. It can be used when only incomplete or partial information is available.
Reference: [7] <author> C. Jones, </author> <title> "Software Benchmarking" Web Document, </title> <booktitle> IEEE Computer, </booktitle> <month> Oct. </month> <year> 1995. </year> <note> http://www.computer.org/pubs/computer/software/10/software.htm. </note>
Reference-contexts: Kolkhurst [9] assumes that for delivered software, change from level II to level V will reduce defect density by a factor of 500. However, Keene [3] suggests a reduction in the inherent defect density by a factor of 20 for the same change. Jones <ref> [7] </ref> suggests an improvement by a factor of 4 in potential defects and a factor of 9 in delivered defects for changing from level II to level V. Here we use the numbers suggested by Keene to propose the model given in Table 4.
Reference: [8] <author> T. M. Khoshgoftar and J. C. Munson, </author> <title> The Line of Code Metric as a Predictor of Program Faults: a Critical Analysis, </title> <booktitle> Proc. COMPSAC'90, </booktitle> <pages> pp. 408-413. </pages>
Reference-contexts: It can be reasonably assumed that assembly language code is harder to write and thus will have a higher defect density. The influence of program complexity has been extensively debated in the literature <ref> [8] </ref>. Many complexity measures are strongly correlated to software size. Since we are constructing a model for defect density, software size has already been taken into account. There is some evidence that for the same size, modules with significantly higher complexity are likely to have a higher number of defects.
Reference: [9] <author> B.A. Kolkhurst, </author> <booktitle> "Perspectives on Software Reliability Engineering Approaches found in Industry" Proc. ISSRE Industry Reports, </booktitle> <year> 1994. </year>
Reference-contexts: This level, as measured by the SEI Capability Maturity Model, can be used to quantify it. Here we assume level II as the default level, since a level I organization is not likely to be using software reliability engineering. Kolkhurst <ref> [9] </ref> assumes that for delivered software, change from level II to level V will reduce defect density by a factor of 500. However, Keene [3] suggests a reduction in the inherent defect density by a factor of 20 for the same change.
Reference: [10] <author> N. Li and Y.K. </author> <title> Malaiya "ROBUST: A Next Generation Software Reliability Engineering Tool" Proc. </title> <booktitle> IEEE Int. Symp. on Software Reliability Engineering, </booktitle> <pages> pp. 375-380, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: The models by Agresti and Evanco [2], Rome Lab [21] and THAAD [6] are factor multiplicative like our model. A preliminary version of our model [13] is being implemented in the ROBUST software reliability tool <ref> [10] </ref>. Our model, presented below, has the following advantages: 1. It can be used when only incomplete or partial information is available. The default value of a multiplicative factor is one, which corresponds to the average case. 2.
Reference: [11] <author> N. Li and Y.K. Malaiya, </author> <title> ROBUST: A Next Generation Software Reliability Engineering Tool, </title> <booktitle> Proc. IEEE Int. Symp. on Software Reliability Engineering, </booktitle> <pages> pp. 375-380, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: The value of ^ K is some times approximated by 4:2 fi 10 7 failures per fault, the average value determined by Musa et al. [18]. Li and Malaiya <ref> [11] </ref> have suggested that ^ K varies with the initial defect density and have given this expression to estimate ^ K. ^ K = 1:2fi10 6 D 0 e 0:05D 0 where D 0 is the defect density per KSLOC.
Reference: [12] <author> N. Li and Y.K. Malaiya, </author> <title> "Fault Exposure Ratio: </title> <booktitle> Estimation and Applications" Proc. IEEE Int. Symp. Software Reliability Engineering 1996 pp. </booktitle> <pages> 372-381. </pages>
Reference-contexts: If the logarithmic model does describe the test process, that would imply that the fault exposure ratio is variable. We can assume that this variation depends on the test process phase which is given by the density of defect present at a time during testing <ref> [12] </ref>.
Reference: [13] <author> N. Li, </author> <title> "Measurement and Enhancement of Software Reliability Through Testing," </title> <type> Ph.D. dissertation, </type> <institution> Colorado State University, </institution> <year> 1997. </year>
Reference-contexts: The models by Agresti and Evanco [2], Rome Lab [21] and THAAD [6] are factor multiplicative like our model. A preliminary version of our model <ref> [13] </ref> is being implemented in the ROBUST software reliability tool [10]. Our model, presented below, has the following advantages: 1. It can be used when only incomplete or partial information is available. The default value of a multiplicative factor is one, which corresponds to the average case. 2.
Reference: [14] <author> Y. K. Malaiya, </author> <title> Early Characterization of the Defect Removal Process, </title> <booktitle> Proc. 9th Annual Software Reliability Symposium, </booktitle> <month> May </month> <year> 1991, </year> <pages> pp. </pages> <month> 6.1-6.4. </month>
Reference-contexts: If we know how a parameter arises, we can estimate it even before testing begins. Such a priori values when estimated using past experience, can be used to do preliminary planning and resource allocation before testing begins <ref> [14] </ref>. fl This research was supported by a BMDO funded project monitored by ONR 1 2.
Reference: [15] <author> Y. K. Malaiya, N. Karunanithi and P. Verma, </author> <title> Predictability of Software Reliability Models, </title> <journal> IEEE Trans. Reliability, </journal> <month> December </month> <year> 1992, </year> <pages> pp. 539-546. </pages>
Reference-contexts: Musa [18] writes that the logarithmic model is superior in predictive validity compared with the exponential model. In a study using 18 data sets from diverse projects, Malaiya et al. evaluated the prediction accuracy of five two-parameter models <ref> [15] </ref>. They found that the logarithmic model has the best overall prediction capability. Using ANOVA, they found that this superiority is statistically significant. All software reliability growth models (SRGMs) are approximations of the real testing process, thus none of the models can be regarded to be perfect.
Reference: [16] <author> Y. K. Malaiya, A. von Mayrhauser and P. K. Srimani, </author> <title> An Examination of Fault Exposure Ratio, </title> <journal> IEEE Trans. Software Engineering, </journal> <month> Nov. </month> <year> 1993, </year> <pages> pp. 1087-1094. </pages>
Reference-contexts: Thus the parameters fi 0 and fi 1 have the following interpretations: fi E 1 = T L Experimental data suggests that K actually varies during testing <ref> [16] </ref>. We will denote the constant equivalent as determined by the application of the exponential model by ^ K. 2.2 Implications of the Logarithmic model The logarithmic model has been found to have very good predictive capability in many cases. <p> However to derive it from basic considerations would require one to make some assumptions as done in references [18], [17] and <ref> [16] </ref>. If the logarithmic model does describe the test process, that would imply that the fault exposure ratio is variable. We can assume that this variation depends on the test process phase which is given by the density of defect present at a time during testing [12].
Reference: [17] <author> J. D. Musa and K. Okumoto, </author> <title> A Logarithmic Poisson Execution Time Model for Software Reliability Measurement, </title> <booktitle> Proc. 7th Int. Conf. on Software Engineering, </booktitle> <year> 1984, </year> <pages> pp. 230-238. </pages>
Reference-contexts: However to derive it from basic considerations would require one to make some assumptions as done in references [18], <ref> [17] </ref> and [16]. If the logarithmic model does describe the test process, that would imply that the fault exposure ratio is variable. We can assume that this variation depends on the test process phase which is given by the density of defect present at a time during testing [12].
Reference: [18] <author> J. D. Musa, A. Iannino, K. Okumoto, </author> <title> Software Reliability Measurement, Prediction, Applications, </title> <publisher> McGraw-Hill, </publisher> <year> 1987. </year> <title> [19] "Personal Software Process" Web Document, </title> <institution> Carnegie Mellon University, </institution> <note> http://www.sei.cmu.edu/technology/psp/Results.htm, Rev. 5 Sept. </note> <year> 1997 </year>
Reference-contexts: Section 3 discusses estimation of parameters. Some observations on parameter variations are presented next followed by the conclusions. 2 Exponential and Logarithmic SRGMs In this paper we will consider two two-parameter models. The Exponential model, in the formulation used here is also termed Musa's basic execution model <ref> [18] </ref>. It is given by (t) = fi E 1 t ) (1) where (t) is the mean value function and fi E 0 and fi E 1 are the two model parameters. Farr mentions that this model has had the widest distribution among the software reliability models [4]. Musa [18] <p> <ref> [18] </ref>. It is given by (t) = fi E 1 t ) (1) where (t) is the mean value function and fi E 0 and fi E 1 are the two model parameters. Farr mentions that this model has had the widest distribution among the software reliability models [4]. Musa [18] states that the basic execution model generally appears to be superior in capability and applicability to other published models. Some of the other models are similar to this model. The Logarithmic model is other model considered here. It is also termed Musa-Okumoto logarithmic poisson Model. <p> Farr states that the logarithmic model is one of the model that has been extensively applied [4]. This is one of the selected models in the AIAA Recommended Practice Standard. Musa <ref> [18] </ref> writes that the logarithmic model is superior in predictive validity compared with the exponential model. In a study using 18 data sets from diverse projects, Malaiya et al. evaluated the prediction accuracy of five two-parameter models [15]. They found that the logarithmic model has the best overall prediction capability. <p> Let k s be the fraction of existing faults exposed during a single execution. Then dN (t) T s = k s N (t) It would be convenient to replace T s with something which can be easily estimated. Let T L be the linear execution time <ref> [18] </ref> which is defined as the total time needed if each instruction in the program was executed once and only once. <p> Using this, equation 2.1 can be rewritten as dN (t) = T L The per-fault hazard rate as given in equation 2.1 is K=T L . Thus K, termed fault exposure ratio <ref> [18] </ref> directly controls the efficiency of the testing process. If we assume that K is time 3 invariant, then the above equation has the following solution: N (t) = N 0 e T L t Where N 0 is the initial number of defects. <p> However to derive it from basic considerations would require one to make some assumptions as done in references <ref> [18] </ref>, [17] and [16]. If the logarithmic model does describe the test process, that would imply that the fault exposure ratio is variable. We can assume that this variation depends on the test process phase which is given by the density of defect present at a time during testing [12]. <p> Gaffney [5] has proposed a phase based model that uses the Rayleigh curve. Here we present a simpler model using actual data reported by Musa et al. <ref> [18] </ref> and the error profile presented by Piwowarski et al. [20]. In Table 2 we take the default value of one to represent the beginning of the system test phase. <p> The value of ^ K is some times approximated by 4:2 fi 10 7 failures per fault, the average value determined by Musa et al. <ref> [18] </ref>. Li and Malaiya [11] have suggested that ^ K varies with the initial defect density and have given this expression to estimate ^ K. ^ K = 1:2fi10 6 D 0 e 0:05D 0 where D 0 is the defect density per KSLOC. <p> The parameter values have been computed here by fitting the values for fault exposure ratio for several projects reported by Musa et al. <ref> [18] </ref>. Example 2: Let us assume that the initial defect density for a project has been estimated to be 25 faults/KSLOC and the software size is 5400 lines. The program is tested on a CPU that runs at 4 MIPS and each source instruction compiles into 4 objects instructions. <p> A reasonable estimate for K min is 1:5 fi 10 7 as suggested by the data given by Musa et al. <ref> [18] </ref>. As estimation of D min , the defect density at which the minimum value of K occurs is harder to estimate. First the curve for K, as shown in figure 1 has a very flat minimum. <p> However if D 0 is higher, the resulting value of Dmin is also higher. in many cases, taking D min = D 0 =3 yields a suitable first estimate. Example 4: For the T2 data <ref> [18] </ref>, the initial defect density is 8.23 defects/KSLOC and the size is approximately 6.92 KSLOC (27.7K object lines). The instruction execution rate is not given 12 in [18], however we can obtain the value of T L using available information. <p> Example 4: For the T2 data <ref> [18] </ref>, the initial defect density is 8.23 defects/KSLOC and the size is approximately 6.92 KSLOC (27.7K object lines). The instruction execution rate is not given 12 in [18], however we can obtain the value of T L using available information.
Reference: [20] <author> P. Piwowarski. M. Ohba and J. Caruso, </author> <title> "Coverage measurement Experience during Function Test," </title> <booktitle> Proc. </booktitle> <address> ICSE, </address> <year> 1993, </year> <pages> pp. 287-301. 16 </pages>
Reference-contexts: Gaffney [5] has proposed a phase based model that uses the Rayleigh curve. Here we present a simpler model using actual data reported by Musa et al. [18] and the error profile presented by Piwowarski et al. <ref> [20] </ref>. In Table 2 we take the default value of one to represent the beginning of the system test phase. With respect to this, the first two columns of Table 2 represent the multipliers suggested by the numbers given by Musa et al. and Piwowarski et al..
Reference: [21] <author> Rome Lab, </author> <title> "Methodology for Software Reliability Prediction and Assessment," </title> <type> Tech Report RL-TR-95-52, </type> <note> Vol. 1 and 2, </note> <year> 1992. </year>
Reference-contexts: Such models have also been used to estimate the hardware failure rates. Several linear additive models for estimating the number of defects have also been proposed, they have the disadvantage that they can project zero or negative number of defects. The models by Agresti and Evanco [2], Rome Lab <ref> [21] </ref> and THAAD [6] are factor multiplicative like our model. A preliminary version of our model [13] is being implemented in the ROBUST software reliability tool [10]. Our model, presented below, has the following advantages: 1. It can be used when only incomplete or partial information is available.
Reference: [22] <author> M. Takahashi and Y. Kamayachi, </author> <title> An Empirical Study of a Model for Program Error Prediction, in Software Reliability Models, </title> <publisher> IEEE Computer Society, </publisher> <year> 1991. </year> <pages> pp. 71-77. </pages>
Reference-contexts: 5 4 Subsystem Testing Insufficient data 2.5 2.5 System testing 1 1 1 (default) Operation Testing 0.25 0.45 0.35 Table 2: Phase Factor (F ph ) 3.1.2 The Programming Team Factor (F pt ) The defect density varies significantly due to the coding and debugging capabilities of the individuals involved <ref> [22] </ref> [23]. The only available quantitative characterization is in terms of programmers average experience in years, given by Takahashi and Kamayachi [22]. Their model can take into account programming experience of up to 7 years, each year reducing the number of defects by about 14%. <p> Phase Factor (F ph ) 3.1.2 The Programming Team Factor (F pt ) The defect density varies significantly due to the coding and debugging capabilities of the individuals involved <ref> [22] </ref> [23]. The only available quantitative characterization is in terms of programmers average experience in years, given by Takahashi and Kamayachi [22]. Their model can take into account programming experience of up to 7 years, each year reducing the number of defects by about 14%. The data in the study reported by Takada et al [23] suggests that programmers can vary in debugging efficiency by a factor of 3.
Reference: [23] <author> Y. Tokada, K. Matsumoto and K. Torii, </author> <title> "A programmer Performance Measure based on Programmer State Transitions in Testing and Debugging Process," </title> <booktitle> Proc. International Conference of Software Engineering, </booktitle> <year> 1994, </year> <pages> pp. 123-132. 17 </pages>
Reference-contexts: 4 Subsystem Testing Insufficient data 2.5 2.5 System testing 1 1 1 (default) Operation Testing 0.25 0.45 0.35 Table 2: Phase Factor (F ph ) 3.1.2 The Programming Team Factor (F pt ) The defect density varies significantly due to the coding and debugging capabilities of the individuals involved [22] <ref> [23] </ref>. The only available quantitative characterization is in terms of programmers average experience in years, given by Takahashi and Kamayachi [22]. Their model can take into account programming experience of up to 7 years, each year reducing the number of defects by about 14%. <p> Their model can take into account programming experience of up to 7 years, each year reducing the number of defects by about 14%. The data in the study reported by Takada et al <ref> [23] </ref> suggests that programmers can vary in debugging efficiency by a factor of 3. In a study about the PSP process [19], the defect densities in a program separately written by 104 programmers were evaluated.
References-found: 22

