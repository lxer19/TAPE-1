URL: http://www.cs.ucsb.edu/~acha/publications/isca99-submitted.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~acha/publications/isca99-submitted.html
Root-URL: http://www.cs.ucsb.edu
Title: An Evaluation of Architectural Alternatives for Rapidly Growing Datasets: Active Disks, Clusters, SMPs  
Author: Mustafa Uysal Anurag Acharya Joel Saltz 
Date: 1998  
Note: October  
Address: College Park Santa Barbara College Park  
Affiliation: Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science University of Maryland University of California University of Maryland  
Abstract: Technical Report TRCS98-27 Dept of Computer Science University of California, Santa Barbara Abstract Growth and usage trends for several large datasets indicate that there is a need for architectures that scale the processing power as the dataset increases. In this paper, we evaluate three architectural alternatives for rapidly growing and frequently reprocessed datasets: active disks, clusters, and shared memory multiprocessors (SMPs). The focus of this evaluation is to identify potential bottlenecks in each of the alternative architectures and to determine the performance of these architectures for the applications of interest. We evaluate these architectural alternatives using a detailed simulator and a suite of nine applications. Our results indicate that for most of these applications Active Disk and cluster configurations were able to achieve significantly better performance than SMP configurations. Active Disk configurations were able to match (and in some cases improve upon) the performance of commodity cluster configurations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya and S. Setia. </author> <title> Availability and utility of idle memory in workstation clusters. </title> <type> Technical Report TRCS-98-26, </type> <institution> Dept of Computer Science, University of California, Santa Barbara, </institution> <month> Oct </month> <year> 1998. </year>
Reference-contexts: Figure 1 (b) illustrates cluster configurations. We selected these configurations based on the design of large Beowulf-class clusters with commodity components (e.g. Avalon [9]). We assumed that these machines ran a standard full-function operating system similar to So-laris. Acharya et al <ref> [1] </ref> report that, on the average, the kernel on a 128 MB Solaris machine has a memory footprint of 24 MB (including the paging free list but not including the file cache). Accordingly, we assumed that only 104 MB on these hosts is available to user processes.
Reference: [2] <author> A. Acharya, M. Uysal, and J. Saltz. </author> <title> Active Disks: Programming Model, Algorithms and Evaluation. </title> <booktitle> In Proceedings of ASPLOS VIII, </booktitle> <year> 1998. </year>
Reference-contexts: In this paper, we evaluate architectural alternatives for scaling the processing power with the growth in dataset size. We consider three alternatives: Active Disks <ref> [2, 19, 23, 31] </ref> (see section 2 for a brief review of Active Disks.), clusters and shared memory multiprocessors (SMPs). <p> Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort [20]) is held by NOW-sort running on a cluster [7]. Active Disks have been identified by several researchers as a cost-effective architectural alternative for applications that process rapidly growing datasets <ref> [2, 19, 23, 31] </ref>. <p> Requiring all communication to pass through the front-end host can lead to substantial loss of performance for these applications. Given the substantial impact for important applications, we revise our original proposal for Active Disk architectures <ref> [2] </ref> to include direct disk-to-disk communication. 2 2 Background: Active Disks In this section, we provide a brief introduction to Active Disks. <p> The key idea is to o*oad bulk of the processing to the disk-resident processors and to use the host processor primarily for coordination, scheduling and combination of results from individual disks. Acharya et al <ref> [2] </ref> propose a stream-based programming model for the disk-resident component (disklet) and its interaction with host-resident peer. Disklets take streams as inputs and generate streams as outputs. Files (and ranges in files) are represented as streams. <p> It uses a modified version of DiskSim that is driven by the disk operating system layer. Disklets are written in C and interact with Howsim using a stream-based API <ref> [2] </ref>. Howsim has additional parameters for the DiskOS. For this study, we assumed the system call and context switch costs on the DiskOS to be 1 s. In addition, another 1 s is charged to initiate a disk request from DiskOS and to service an interrupt from the disk mechanism. <p> This allowed these configurations to tolerate longer communication and I/O latencies. Finally, to understand the impact of allowing the disks to communicate directly with each other, we considered alternative configurations that restrict disks to communicate only with the front-end host (as proposed in <ref> [2, 31] </ref>). Clusters: For the cluster configurations, we assumed that each host contained: (1) a 300 MHz Pentium II, (2) 128 MB of 10ns SDRAM, (3) a 133 MB/s PCI bus, and (4) a 100BaseT ethernet NIC. <p> For each application, we started with a well-known efficient algorithm from the literature and adapted it for each architecture and the corresponding programming model. For Active Disks, we adapted the algorithms to use the stream-based programming model proposed by Acharya et al <ref> [2] </ref>. Note that, overlapping computation and communication is handled by the DiskOS (the disk-resident OS layer) by using multiple buffers per stream. For clusters, we adapted the algorithms to use MPI-like asynchronous message-passing operations and global synchronization primitives. <p> Requiring all communication to pass through the front-end host can lead to substantial loss of performance for these applications (a three-fold slowdown on 32-disk configurations, a five-fold slowdown on 128-disk configurations). Given the substantial impact for important applications, we revise our original proposal for Active Disk architectures <ref> [2] </ref> to include direct disk-to-disk communication. We also extend the stream-based programming model proposed in [2]to allow the host to establish connections between disklets running on different disks.
Reference: [3] <author> A. Afework, M. Beynon, F. Bustamante, A. Demarzo, R. Ferriera, R. Miller, M. Silberman, J. Saltz, A. Sussman, and H. Tsang. </author> <title> Digital dynamic telepathology the virtual microscope. </title> <booktitle> In Proceedings of the AMIA'98 Fall Symposium, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: These images are to be used for telepathology, medical research and pedagogy and require a variety of processing including three-dimensional reconstruction of tissue sections, image segmentation, virtual staining and histological image analysis <ref> [3] </ref>. In this paper, we evaluate architectural alternatives for scaling the processing power with the growth in dataset size. We consider three alternatives: Active Disks [2, 19, 23, 31] (see section 2 for a brief review of Active Disks.), clusters and shared memory multiprocessors (SMPs).
Reference: [4] <author> S. Agarwal, R. Agrawal, P. Deshpande, A. Gupta, J. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the computation of multidimensional aggregates. </title> <booktitle> In Proceedings of the 22nd International Conference on Very Large Databases, </booktitle> <pages> pages 506-21, </pages> <year> 1996. </year>
Reference-contexts: Datacube: the datacube is the most general form of aggregation for relational databases. It computes multi-dimensional aggregates that are indexed by values of multiple aggregates [21]. In effect, a datacube computes group-bys for all possible combinations of a list of attributes. We used the PipeHash algorithm proposed in <ref> [4] </ref> as the starting point for our algorithms. It schedules the group-bys as a sequence of pipelines; all the group-bys in a pipeline are computed as a part of a single scan of disk-resident data. <p> The number of distinct values for each of the group-by attributes were 5.36 million, 536,000, 53,600 and 5,360. We created this dataset by scaling one of the datasets used in the paper that described the PipeHash algorithm <ref> [4] </ref>. Sort: for sort, we used a dataset with 100-byte tuples and 10-byte uniformly distributed keys. The total number of tuples was about 170 million. We created this dataset based on the standard sort benchmark described in [20].
Reference: [5] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD Conference on Management of Data, </booktitle> <pages> pages 207-16, </pages> <year> 1993. </year>
Reference-contexts: To evaluate these architectures, we use a suite of nine applications that process datasets of interest: (1) SQL select, (2) SQL aggregate, (3) SQL group-by, (4) external sort, (5) the datacube operation for decision support [21], (6) SQL join, (7) datamining retail data for association rules <ref> [5] </ref>, (8) image convolution, and (9) generation of earth images from raw satellite data [12, 13, 34]. The first seven applications process relational databases and are used in data warehouses; the remaining two are used in image databases and satellite data repositories respectively. <p> Note that the first pass of join is communication-intensive and requires all-to-all communication. Since both sort and join repartition their entire dataset, their communication requirements are similar. Datamining: we focus on frequent itemset counting for mining association rules in retail transaction data <ref> [5] </ref>. We used the eclat algorithm [42] as the starting point for our algorithms. It is a multi-pass algorithm with the first two passes same as the Count distribution algorithm proposed by Agrawal et al [6].
Reference: [6] <author> R. Agrawal and J. Shafer. </author> <title> Parallel mining of association rules. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 962-9, </pages> <year> 1996. </year>
Reference-contexts: We used the eclat algorithm [42] as the starting point for our algorithms. It is a multi-pass algorithm with the first two passes same as the Count distribution algorithm proposed by Agrawal et al <ref> [6] </ref>. After the first two passes, it clusters the candidate itemsets into equivalence classes and uses these classes to filter, transpose and repartition the input data sets. The third pass is localized and does not require any communication.
Reference: [7] <author> A. Arpaci-Dusseau, R. Arpaci-Dusseau, D. Culler, J. Hellerstein, and D. Patterson. </author> <title> High-performance sorting on networks of workstations. </title> <booktitle> In Proceedings of SIGMOD'97, </booktitle> <year> 1997. </year>
Reference-contexts: Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort [20]) is held by NOW-sort running on a cluster <ref> [7] </ref>. Active Disks have been identified by several researchers as a cost-effective architectural alternative for applications that process rapidly growing datasets [2, 19, 23, 31]. <p> Note that for sort and join, which shu*e their entire dataset and write it back to disk, we partitioned the disks into separate read and write groups (as in NOW-sort <ref> [7] </ref>). Since all processors can address all disks, we did not a-priori partition the input datasets to processors. Instead, we maintained two shared queues (read/write) of fixed-size blocks in the order they appear on disk. When idle, each processor locks the queue and grabs the next block off the queue. <p> Note that since datacube performs multiple group-bys in a single scan, it performs more computation per byte read than group-by. Also, since it computes a multi-dimensional aggregate, it generates and communicates significantly more data. External sort: we used the two-pass parallel NOW-sort <ref> [7] </ref> as the starting point for our sort algorithms. The active-disk and cluster algorithms are fully pipelined in that they overlap reading data, sending data to peers and sorting and writing data. <p> The active-disk and cluster algorithms are fully pipelined in that they overlap reading data, sending data to peers and sorting and writing data. The SMP algorithm overlaps just the first two operations; reading and writing operations are performed synchronously (Dusseau et al <ref> [7] </ref> recommend that for less than four disks, all operations should be overlapped whereas for more than four disks, only the first two should be overlapped). The first pass of these algorithms repartitions their entire input on disks and has large communication requirements. <p> Note that join seems to achieve better than perfect scaling for all architectures. We suspect that this is due to cache capacity effects during the merge phase. A similar result was reported by Arpaci-Dusseau et al <ref> [7] </ref> for NOW-sort with more than 17 runs per node (our experiments had between 2 and 80 runs per node). 12 (a) Active Disks (b) Clusters (c) SMPs architectures.
Reference: [8] <institution> The Avalon FAQ 3 , 1998. </institution>
Reference-contexts: We estimated the price of the SMP configuration using the SGI Origin 2000. The Avalon project at Los Alamos Labs quote the list price of a 64-processor SGI Origin 2000 with 250MHz processors and 8 GB memory to be about $1.8 million <ref> [8] </ref>. We estimated the price of the cluster configuration using the Micron PC ClientPro as the individual nodes, the Dell Poweredge 4300 as the front-end host, Seagate ST39102 as the disks and 3Com SuperStack II 3900 and SuperStack II 9300 as the Fast ethernet and gigabit ethernet switches.
Reference: [9] <institution> The Avalon Home Page 4 , 1998. Avalon is a 140-processor Beowulf cluster at Los Alamos National Lab. </institution>
Reference-contexts: Figure 1 (b) illustrates cluster configurations. We selected these configurations based on the design of large Beowulf-class clusters with commodity components (e.g. Avalon <ref> [9] </ref>). We assumed that these machines ran a standard full-function operating system similar to So-laris. Acharya et al [1] report that, on the average, the kernel on a 128 MB Solaris machine has a memory footprint of 24 MB (including the paging free list but not including the file cache).
Reference: [10] <author> E. Brewer, F. Chong, L. Liu, S. Sharma, and J. Kubiatowicz. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In Proc. of the 7th SPAA, </booktitle> <pages> pages 42-53, </pages> <year> 1995. </year>
Reference-contexts: For communication, it models one-way block-transfers, shmemget/shmemput, as available on the Origin machines as well as the Cray T3D/T3E. Block transfers are suitable for the applications under consideration as they move large volumes of data in relatively large chunks. For synchronization on SMPs, Howsim provides spin-locks, remote queues <ref> [10] </ref> and global barriers. We used at-memory fetch-and-op primitive as provided by SGI Origin for spin-locks (which cost around 3s [22]). Howsim models a high-bandwidth I/O subsystem similar to the XIO subsystem available in the Origin 2000. <p> We assumed that these machines ran a standard full-function operating system like IRIX and provided the lio listio asynchronous I/O interface and user-controllable disk striping for individual files. Further, we assumed that these machines provided a remote queue abstraction (as suggested by Brewer et al <ref> [10] </ref>). To identify the bottleneck resources for individual applications, we studied alternative configurations that individually scaled the bandwidth of the serial I/O interconnect to 400 MB/s. 6 5 Applications Our suite of applications consists of nine applications from three application domains relational databases, image databases and satellite data repositories.
Reference: [11] <author> S. Donaldson, J. Hill, and D. Skillicorn. </author> <title> BSP Clusters: High Performance, Reliable and Very Low Cost. </title> <type> Technical Report PRG-TR-5-98, </type> <institution> Oxford University Computing Laboratory, </institution> <year> 1998. </year>
Reference-contexts: Accordingly, we assumed that only 104 MB on these hosts is available to user processes. We assumed these machines provided an efficient user-space messaging and synchronization library similar to BSPlib <ref> [11] </ref> that pins send/receive buffers on every host for every communicating peer. We also assumed an asynchronous I/O interface like lio listio. To determine the impact of varying the network bandwidth, we studied alternative configurations that scaled the bandwidth of the network interconnect to 1 Gbps per host.
Reference: [12] <author> J. Eidenshink and J. Fenno. </author> <title> Source code for LAS, </title> <editor> ADAPS and XID, </editor> <year> 1995. </year> <institution> Eros Data Center, Sioux Falls. </institution>
Reference-contexts: applications that process datasets of interest: (1) SQL select, (2) SQL aggregate, (3) SQL group-by, (4) external sort, (5) the datacube operation for decision support [21], (6) SQL join, (7) datamining retail data for association rules [5], (8) image convolution, and (9) generation of earth images from raw satellite data <ref> [12, 13, 34] </ref>. The first seven applications process relational databases and are used in data warehouses; the remaining two are used in image databases and satellite data repositories respectively. <p> Generating a composite image requires pre-processing and projection of the sensor values onto a two-dimensional grid followed by composition of all values that map onto a single grid point to generate the associated pixel. We base our algorithms on the technique used in several programs used by NASA <ref> [12, 13, 34] </ref>. All our algorithms process sensor values in large chunks, mapping each value to the output grid and performing the composition operation using an accumulator for every output pixel. The active-disk and cluster algorithms perform local accumulation to reduce the amount of data communicated.
Reference: [13] <author> G. Feldman. </author> <title> Source code for the SeaWIFS ocean data processing system, 1995. </title> <institution> SeaWIFS group (NASA Goddard). </institution>
Reference-contexts: applications that process datasets of interest: (1) SQL select, (2) SQL aggregate, (3) SQL group-by, (4) external sort, (5) the datacube operation for decision support [21], (6) SQL join, (7) datamining retail data for association rules [5], (8) image convolution, and (9) generation of earth images from raw satellite data <ref> [12, 13, 34] </ref>. The first seven applications process relational databases and are used in data warehouses; the remaining two are used in image databases and satellite data repositories respectively. <p> Generating a composite image requires pre-processing and projection of the sensor values onto a two-dimensional grid followed by composition of all values that map onto a single grid point to generate the associated pixel. We base our algorithms on the technique used in several programs used by NASA <ref> [12, 13, 34] </ref>. All our algorithms process sensor values in large chunks, mapping each value to the output grid and performing the composition operation using an accumulator for every output pixel. The active-disk and cluster algorithms perform local accumulation to reduce the amount of data communicated.
Reference: [14] <author> R. Ferriera, B.Moon, J. Humphries, A. Sussman, J. Saltz, R. Miller, and A. Demarzo. </author> <title> The virtual microscope. </title> <booktitle> In Proceedings of the AMIA Fall'97 Symposium, </booktitle> <pages> pages 449-53, </pages> <year> 1997. </year> <note> 3 http://cnls.lanl.gov/avalon/FAQ.html 4 http://cnls.lanl.gov/avalon/ 16 </note>
Reference-contexts: Ferreira et al <ref> [14] </ref> estimate that digitizing a single slide under a high-resolution confocal light microscope requires between 35 and 200 GB. Plans for managing the pathology record of the Johns Hopkins Medical School call for digitizing tens of thousands of such slides.
Reference: [15] <author> G. Ganger, B. Worthington, and Y. Patt. </author> <title> The DiskSim Simulation Environment Version 1.0 Reference Manual 5 . Technical Report CSE-TR-358-98, </title> <institution> Dept of Electrical Engineering and Computer Science, </institution> <month> Feb </month> <year> 1998. </year>
Reference-contexts: Howsim contains detailed models for disks, networks and the associated libraries and device drivers and relatively coarse-grain models of processors and I/O interconnects. For modeling the behavior of disk drives, controllers and device drivers, Howsim uses the Disksim simulator developed by Ganger et al <ref> [15] </ref>. Disksim has a detailed disk model that supports zoned disks, spare regions, segmented caches, defect management, prefetch algorithms, bus delays and control overheads. <p> Disksim has been validated against several disk drives using the published disk specifications and SCSI logic analyzers; it achieves high accuracy theworst case demerit figure [32] for Disksim is only 2.0% of the corresponding average response time <ref> [15] </ref>. For modeling I/O interconnects, Howsim uses a simple queue-based model that has parameters for startup latency, transfer speed and the capacity of the interconnect. For modeling the behavior of networks, message-passing libraries and global synchronization operations, Howsim uses the Netsim customizable network simulator developed by Uysal et al [38].
Reference: [16] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> Jun </month> <year> 1993. </year>
Reference-contexts: It partitions a relation into disjoint sets of tuples based on the value (s) of index attribute (s) and computes an aggregate value for each set of tuples. We used the hashing-based algorithm from <ref> [16] </ref> as the starting point. The active-disk and cluster algorithms are similar. They perform the group-by in two steps. In the first step, each disk/host performs local group-bys as long as the number of aggregates being computed fits in its memory. <p> The final results of each pipeline are stored back on disk; some of these results are used as input for following pipelines. For individual group-bys, PipeHash uses a hashing-based technique <ref> [16] </ref>. The active-disk and cluster algorithms are similar. For every pipeline, they partition the memory available at each disk/host in proportion to the estimated size of the group-bys being performed in the pipeline.
Reference: [17] <author> J. Gray. </author> <title> Some Challenges in Building Petabyte Data Stores. </title> <institution> Distinguished Lecture, University of California, Santa Barbara, </institution> <month> Oct </month> <year> 1997. </year>
Reference-contexts: Jim Gray argues that satellite data repositories will grow to petabyte size over the next few years and will require a variety of processing ranging from reprocessing the entire dataset to take advantage of new algorithms to re-projection and composition 1 to suit different display requirements <ref> [17, 18] </ref>. Ferreira et al [14] estimate that digitizing a single slide under a high-resolution confocal light microscope requires between 35 and 200 GB. Plans for managing the pathology record of the Johns Hopkins Medical School call for digitizing tens of thousands of such slides.
Reference: [18] <author> J. Gray. </author> <title> What Happens When Processors Are Infinitely Fast and Storage Is Free? Keynote Speech at the Fifth Workshop on I/O in Parallel and Distributed Systems, </title> <month> Nov </month> <year> 1997. </year>
Reference-contexts: Jim Gray argues that satellite data repositories will grow to petabyte size over the next few years and will require a variety of processing ranging from reprocessing the entire dataset to take advantage of new algorithms to re-projection and composition 1 to suit different display requirements <ref> [17, 18] </ref>. Ferreira et al [14] estimate that digitizing a single slide under a high-resolution confocal light microscope requires between 35 and 200 GB. Plans for managing the pathology record of the Johns Hopkins Medical School call for digitizing tens of thousands of such slides.
Reference: [19] <author> J. Gray. </author> <title> Put EVERYTHING in the Storage Device. Talk at NASD workshop on storage embedded computing 6 , June 1998. </title>
Reference-contexts: In this paper, we evaluate architectural alternatives for scaling the processing power with the growth in dataset size. We consider three alternatives: Active Disks <ref> [2, 19, 23, 31] </ref> (see section 2 for a brief review of Active Disks.), clusters and shared memory multiprocessors (SMPs). <p> Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort [20]) is held by NOW-sort running on a cluster [7]. Active Disks have been identified by several researchers as a cost-effective architectural alternative for applications that process rapidly growing datasets <ref> [2, 19, 23, 31] </ref>.
Reference: [20] <author> J. Gray. </author> <title> The Sort Benchmark Home Page. </title> <note> Available at http://research.microsoft.com/- research/barc/SortBenchmark/, </note> <year> 1998. </year>
Reference-contexts: Shared memory multiprocessors are widely used for relational databases (Strenstrom et al [35] estimate that in 2000, 40% of such machines will sold for handling relational databases). Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort <ref> [20] </ref>) is held by NOW-sort running on a cluster [7]. Active Disks have been identified by several researchers as a cost-effective architectural alternative for applications that process rapidly growing datasets [2, 19, 23, 31]. <p> Sort: for sort, we used a dataset with 100-byte tuples and 10-byte uniformly distributed keys. The total number of tuples was about 170 million. We created this dataset based on the standard sort benchmark described in <ref> [20] </ref>. Project-Join: for join, we used a dataset with 64-byte tuples and 4-byte uniformly distributed keys. The projection operation extracted eight 4-byte fields from each tuple. Each relation was 16 GB and contained 268 million tuples. The output for join was about 108 MB.
Reference: [21] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: A relational aggregation operator generalizing group-by, </title> <booktitle> cross-tab, and sub-totals. In Proceedings of the 12th International Conference on Data Engineering, </booktitle> <pages> pages 152-9, </pages> <address> New Orleans, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: To evaluate these architectures, we use a suite of nine applications that process datasets of interest: (1) SQL select, (2) SQL aggregate, (3) SQL group-by, (4) external sort, (5) the datacube operation for decision support <ref> [21] </ref>, (6) SQL join, (7) datamining retail data for association rules [5], (8) image convolution, and (9) generation of earth images from raw satellite data [12, 13, 34]. <p> It also performs more computation/byte as it needs to maintain a hash-table of aggregates. Datacube: the datacube is the most general form of aggregation for relational databases. It computes multi-dimensional aggregates that are indexed by values of multiple aggregates <ref> [21] </ref>. In effect, a datacube computes group-bys for all possible combinations of a list of attributes. We used the PipeHash algorithm proposed in [4] as the starting point for our algorithms.
Reference: [22] <author> D. Jiang and J. Singh. </author> <title> A methodology and an evaluation of the SGI Origin 2000. </title> <booktitle> In Proc. of the Intl. Conf. on Measurement and Modeling of Computer Systems (SIGMETRICS), </booktitle> <pages> pages 171-81, </pages> <address> Madison, WI, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: Block transfers are suitable for the applications under consideration as they move large volumes of data in relatively large chunks. For synchronization on SMPs, Howsim provides spin-locks, remote queues [10] and global barriers. We used at-memory fetch-and-op primitive as provided by SGI Origin for spin-locks (which cost around 3s <ref> [22] </ref>). Howsim models a high-bandwidth I/O subsystem similar to the XIO subsystem available in the Origin 2000.
Reference: [23] <author> K. Keeton, D. Patterson, and J. Hellerstein. </author> <title> The Case for Intelligent Disks (IDISKS). </title> <booktitle> SIGMOD Record, </booktitle> <volume> 27(3), </volume> <year> 1998. </year>
Reference-contexts: In this paper, we evaluate architectural alternatives for scaling the processing power with the growth in dataset size. We consider three alternatives: Active Disks <ref> [2, 19, 23, 31] </ref> (see section 2 for a brief review of Active Disks.), clusters and shared memory multiprocessors (SMPs). <p> Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort [20]) is held by NOW-sort running on a cluster [7]. Active Disks have been identified by several researchers as a cost-effective architectural alternative for applications that process rapidly growing datasets <ref> [2, 19, 23, 31] </ref>. <p> To determine the impact of varying the network bandwidth, we studied alternative configurations that scaled the bandwidth of the network interconnect to 1 Gbps per host. Shared memory multiprocessors (SMPs): For the SMP configurations, we followed the guidelines for configuring decision support servers (as quoted by <ref> [23] </ref>): (1) put as many processors in a box as possible to amortize the cost of enclosures and interconnects; (2) put as much memory as possible into the box to avoid going to disk as much as possible; and (3) attach as many disks as needed for capacity and stripe data
Reference: [24] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: a ccNUMA highly scalable server. </title> <booktitle> In In Proc. of Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 241-51, </pages> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: We assumed an SMP configuration similar to the SGI Origin 2000: (1) two-processor boards (with 250 MHz processors) that directly share 128 MB memory; (2) a low-latency high-bandwidth interconnect between these boards (1s latency and 780 MB/s bandwidth); (3) a high-performance block-transfer engine (521 MB/s sustained bandwidth <ref> [24] </ref>); (4) a high-bandwidth I/O subsystem (two I/O nodes with a total of 1.4 GB/s bandwidth), similar to XIO, that connects to the network interconnect; and (5) a dual-loop Fiber Channel I/O interconnect (200 MB/s) for all disks. Figure 1 (c) illustrates the SMP configurations.
Reference: [25] <author> L. McVoy and C. Staelin. lmbench: </author> <title> portable tools for performance analysis. </title> <booktitle> In In Proc. of 1996 USENIX Technical Conference, </booktitle> <month> Jan </month> <year> 1996. </year>
Reference-contexts: We obtained the first two using lmbench <ref> [25] </ref> on a 300MHz Pentium II running Linux (10s for read/write calls, 103s for context-switch). We charged a fixed cost of 16s to queue an I/O request in the device-driver.
Reference: [26] <author> J. Melton and A. Simon. </author> <title> Understanding the New SQL: A Complete Guide. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Both select and aggregate perform little computation/byte. SQL group-by: The group-by operation computes a one-dimensional vector of aggregates indexed by a list of attributes <ref> [26] </ref>. It partitions a relation into disjoint sets of tuples based on the value (s) of index attribute (s) and computes an aggregate value for each set of tuples. We used the hashing-based algorithm from [16] as the starting point. The active-disk and cluster algorithms are similar.
Reference: [27] <author> Mier Communications. </author> <title> Evaluation of 10/100 BaseT Switches. </title> <address> http://www.mier.com/reports/- cisco/cisco2916mxl.pdf, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: We further assumed that the hosts were connected to 24-port 100BaseT ethernet switches with two gigabit ethernet uplinks similar to the 3Com SuperStack II 3900 <ref> [27, 36] </ref> the 16 host configuration being connected to a single switch, larger configurations being connected to an array of switches with the uplinks connecting to a gigabit ethernet switch similar to the 3Com SuperStack II 9300 [36, 37]. Figure 1 (b) illustrates cluster configurations.
Reference: [28] <author> G. Papadopolous. </author> <title> The future of computing. Unpublished talk at NOW Workshop, </title> <month> July </month> <year> 1997. </year>
Reference-contexts: Patterson et al [29] quote an observation by Greg Papadopolous while processors are doubling performance every 18 months, customers are doubling data storage every nine-to-twelve months and would like to "mine" this data overnight to shape their business practices <ref> [28] </ref>. Jim Gray argues that satellite data repositories will grow to petabyte size over the next few years and will require a variety of processing ranging from reprocessing the entire dataset to take advantage of new algorithms to re-projection and composition 1 to suit different display requirements [17, 18].
Reference: [29] <author> D. Patterson et al. </author> <title> Intelligent RAM (IRAM): the Industrial Setting, Applications, and Architectures. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <year> 1997. </year>
Reference-contexts: Results from the 1997 and 1998 Winter Very Large Database surveys document the growth trends for decision support databases [40, 41]. For example, the Sears Roebuck and Co decision support database grew from 1.3 TB in 1997 to 4.6 TB in 1998. Patterson et al <ref> [29] </ref> quote an observation by Greg Papadopolous while processors are doubling performance every 18 months, customers are doubling data storage every nine-to-twelve months and would like to "mine" this data overnight to shape their business practices [28].
Reference: [30] <institution> IBM Quest Data Mining Project. </institution> <note> The Quest retail transaction data generator 7 , 1996. 5 Available at http://www.ece.cmu.edu/ ganger/disksim/disksim1.0.tar.gz 6 http://www.nsic.org/nasd/1998-jun/gray.pdf 7 Available at http://www.almaden.ibm.com/cs/quest/syndata.html. 17 </note>
Reference-contexts: Datamining: for dmine, we used a dataset with 300 million transactions. The total number of items was 1 million and the average length of the transactions was 4 items. We generated this dataset using the Quest datamining dataset generator which we obtained from IBM Almaden <ref> [30] </ref>. For generating the frequent itemsets, we used a minimum support parameter of 0.001 (0.1%). Image convolution: for conv, we used a dataset consisting of 65536 512x512 grayscale images with one byte per pixel. We used a 5x5 convolution kernel.
Reference: [31] <author> E. Riedel, G. Gibson, and C. Faloutsos. </author> <title> Active storage for large scale data mining and mul-timedia applications. </title> <booktitle> In Proceedings of 24th Conference on Very Large Databases, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: In this paper, we evaluate architectural alternatives for scaling the processing power with the growth in dataset size. We consider three alternatives: Active Disks <ref> [2, 19, 23, 31] </ref> (see section 2 for a brief review of Active Disks.), clusters and shared memory multiprocessors (SMPs). <p> Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort [20]) is held by NOW-sort running on a cluster [7]. Active Disks have been identified by several researchers as a cost-effective architectural alternative for applications that process rapidly growing datasets <ref> [2, 19, 23, 31] </ref>. <p> This allowed these configurations to tolerate longer communication and I/O latencies. Finally, to understand the impact of allowing the disks to communicate directly with each other, we considered alternative configurations that restrict disks to communicate only with the front-end host (as proposed in <ref> [2, 31] </ref>). Clusters: For the cluster configurations, we assumed that each host contained: (1) a 300 MHz Pentium II, (2) 128 MB of 10ns SDRAM, (3) a 133 MB/s PCI bus, and (4) a 100BaseT ethernet NIC.
Reference: [32] <author> C. Ruemmler and J. Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-29, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Disksim has a detailed disk model that supports zoned disks, spare regions, segmented caches, defect management, prefetch algorithms, bus delays and control overheads. Disksim has been validated against several disk drives using the published disk specifications and SCSI logic analyzers; it achieves high accuracy theworst case demerit figure <ref> [32] </ref> for Disksim is only 2.0% of the corresponding average response time [15]. For modeling I/O interconnects, Howsim uses a simple queue-based model that has parameters for startup latency, transfer speed and the capacity of the interconnect.
Reference: [33] <author> Seagate Technology Inc. </author> <title> The Cheetah 9LP Family: ST39102 Product Manual, </title> <month> July </month> <year> 1998. </year> <note> Publication number 83329240 Rev B. </note>
Reference-contexts: To understand the impact of scaling individual components and to identify the bottleneck resources for individual applications, we performed additional experiments by selectively scaling individual components. For all configurations, we assumed disks similar to the Seagate 39102FC from the Cheetah 9LP disk family <ref> [33] </ref>. These disks have a spindle speed of 10,025 rpm, a formatted media transfer rate of 14.5-21.3 MB/s, an average seek time of 5.4 ms/6.2 ms (read/write) and a maximum seek time of 12.2 ms/13.2 ms (read/write). They support Ultra2 SCSI and dual-loop Fiber Channel interfaces.
Reference: [34] <author> P. Smith and B-B. Ding. </author> <title> Source code for the AVHRR Pathfinder system, 1995. Main program of the AVHRR Land Pathfinder effort (NASA Goddard). </title>
Reference-contexts: applications that process datasets of interest: (1) SQL select, (2) SQL aggregate, (3) SQL group-by, (4) external sort, (5) the datacube operation for decision support [21], (6) SQL join, (7) datamining retail data for association rules [5], (8) image convolution, and (9) generation of earth images from raw satellite data <ref> [12, 13, 34] </ref>. The first seven applications process relational databases and are used in data warehouses; the remaining two are used in image databases and satellite data repositories respectively. <p> Generating a composite image requires pre-processing and projection of the sensor values onto a two-dimensional grid followed by composition of all values that map onto a single grid point to generate the associated pixel. We base our algorithms on the technique used in several programs used by NASA <ref> [12, 13, 34] </ref>. All our algorithms process sensor values in large chunks, mapping each value to the output grid and performing the composition operation using an accumulator for every output pixel. The active-disk and cluster algorithms perform local accumulation to reduce the amount of data communicated. <p> The active-disk and cluster algorithms perform local accumulation to reduce the amount of data communicated. The output image for the high-resolution datasets (about 556 MB <ref> [34] </ref>), however, does not fit into the memory available at individual disks/hosts. To deal with this, each disk/host performs accumulation for a contiguous section of the output grid that fits into its memory. <p> Image convolution: for conv, we used a dataset consisting of 65536 512x512 grayscale images with one byte per pixel. We used a 5x5 convolution kernel. Earth science: for earth, we used a dataset which corresponds to high-resolution AVHRR images from the NOAA polar-orbiting satellites <ref> [34] </ref>. The output image for this dataset was 556 MB. 7 Results architectures. The results for each application on configurations of a particular size (16/32/64/128) are normalized with respect to the performance of the same application on the Active Disk configuration of the same size.
Reference: [35] <author> P. Strenstrom, E. Hagersten, D. Lilja, M. Martonosi, and M. Venugopal. </author> <booktitle> Trends in shared memory multiprocessing. IEEE Computer, </booktitle> <year> 1997. </year>
Reference-contexts: All three architectures are either currently in use for processing datasets of interest (relational databases, image databases, satellite data repositories) or have been proposed as suitable alternatives. Shared memory multiprocessors are widely used for relational databases (Strenstrom et al <ref> [35] </ref> estimate that in 2000, 40% of such machines will sold for handling relational databases). Clusters have been shown to provide excellent I/O performance: the current world-record for disk-to-disk sort (the Indy MinuteSort [20]) is held by NOW-sort running on a cluster [7].
Reference: [36] <institution> The 3Com SuperStack II Switch Datasheets. </institution> <address> http://www.3com.com/products/dsheets/- 400260a.html, </address> <year> 1998. </year>
Reference-contexts: We further assumed that the hosts were connected to 24-port 100BaseT ethernet switches with two gigabit ethernet uplinks similar to the 3Com SuperStack II 3900 <ref> [27, 36] </ref> the 16 host configuration being connected to a single switch, larger configurations being connected to an array of switches with the uplinks connecting to a gigabit ethernet switch similar to the 3Com SuperStack II 9300 [36, 37]. Figure 1 (b) illustrates cluster configurations. <p> switches with two gigabit ethernet uplinks similar to the 3Com SuperStack II 3900 [27, 36] the 16 host configuration being connected to a single switch, larger configurations being connected to an array of switches with the uplinks connecting to a gigabit ethernet switch similar to the 3Com SuperStack II 9300 <ref> [36, 37] </ref>. Figure 1 (b) illustrates cluster configurations. We selected these configurations based on the design of large Beowulf-class clusters with commodity components (e.g. Avalon [9]). We assumed that these machines ran a standard full-function operating system similar to So-laris.
Reference: [37] <institution> The Tolly Group. 3Com Corporation Superstack II 9300 Gigabit Ethernet Performance. </institution> <note> Available off http://www.tolly.com, Jan 1998. </note>
Reference-contexts: switches with two gigabit ethernet uplinks similar to the 3Com SuperStack II 3900 [27, 36] the 16 host configuration being connected to a single switch, larger configurations being connected to an array of switches with the uplinks connecting to a gigabit ethernet switch similar to the 3Com SuperStack II 9300 <ref> [36, 37] </ref>. Figure 1 (b) illustrates cluster configurations. We selected these configurations based on the design of large Beowulf-class clusters with commodity components (e.g. Avalon [9]). We assumed that these machines ran a standard full-function operating system similar to So-laris.
Reference: [38] <author> M. Uysal, A. Acharya, R. Bennett, and J. Saltz. </author> <title> A customizable simulator for workstation clusters. </title> <booktitle> In Proc. of the 11th International Parallel Processing Symposium, </booktitle> <pages> pages 249-54, </pages> <year> 1997. </year>
Reference-contexts: For modeling I/O interconnects, Howsim uses a simple queue-based model that has parameters for startup latency, transfer speed and the capacity of the interconnect. For modeling the behavior of networks, message-passing libraries and global synchronization operations, Howsim uses the Netsim customizable network simulator developed by Uysal et al <ref> [38] </ref>. Netsim models switched networks and an efficient user-space message-passing and global synchronization library with an MPI-like interface.
Reference: [39] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 203-16, </pages> <year> 1993. </year>
Reference-contexts: Streams are accessed using a standard interface which delivers the data in buffers whose size is known apriori. A disklet can be written in any language. However, it is required to adhere to certain guidelines. A disklet cannot allocate (or free) memory. It is sandboxed <ref> [39] </ref> within the buffers corresponding to each of its input streams, which are allocated and freed by the operating system, and a scratch space that is allocated on its behalf when it is initialized. A disklet is also not allowed to initiate I/O operations on its own.
Reference: [40] <author> R. Winter and K. </author> <title> Auerbach. Giants walk the earth: </title> <booktitle> the 1997 VLDB survey. Database Programming and Design, </booktitle> <volume> 10(9), </volume> <month> Sep </month> <year> 1997. </year>
Reference-contexts: The usage trends indicate that there is a change in user expectations regarding large datasets from primarily archival storage to frequent reprocessing in their entirety. Results from the 1997 and 1998 Winter Very Large Database surveys document the growth trends for decision support databases <ref> [40, 41] </ref>. For example, the Sears Roebuck and Co decision support database grew from 1.3 TB in 1997 to 4.6 TB in 1998.
Reference: [41] <author> R. Winter and K. </author> <title> Auerbach. The big time: </title> <booktitle> the 1998 VLDB survey. Database Programming and Design, </booktitle> <volume> 11(8), </volume> <month> Aug </month> <year> 1998. </year>
Reference-contexts: The usage trends indicate that there is a change in user expectations regarding large datasets from primarily archival storage to frequent reprocessing in their entirety. Results from the 1997 and 1998 Winter Very Large Database surveys document the growth trends for decision support databases <ref> [40, 41] </ref>. For example, the Sears Roebuck and Co decision support database grew from 1.3 TB in 1997 to 4.6 TB in 1998.
Reference: [42] <author> M. Zaki, S. Parthasarathy, and W. Li. </author> <title> A localized algorithm for parallel association mining. </title> <booktitle> In Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1997. </year>
Reference-contexts: Note that the first pass of join is communication-intensive and requires all-to-all communication. Since both sort and join repartition their entire dataset, their communication requirements are similar. Datamining: we focus on frequent itemset counting for mining association rules in retail transaction data [5]. We used the eclat algorithm <ref> [42] </ref> as the starting point for our algorithms. It is a multi-pass algorithm with the first two passes same as the Count distribution algorithm proposed by Agrawal et al [6].
References-found: 42

