URL: ftp://ic.eecs.berkeley.edu/pub/Thesis/aml.ps.Z
Refering-URL: http://www-cad.eecs.berkeley.edu/Respep/Research/Thesis/thesis.html
Root-URL: http://www.cs.berkeley.edu
Title: Inductive Learning by Selection of Minimal Complexity Representations  
Author: by Arlindo Manuel Limede de Oliveira 
Degree: B.S. (Instituto Superior Tecnico) 1986 M.S. (Instituto Superior Tecnico) 1989 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Engineering in Electrical Engineering and Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor Alberto Sangiovanni-Vincentelli, Chair Professor Robert K. Brayton Professor Donald A. Glaser  
Date: 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <editor> J.A. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: This means that an object d 2 D with a certain combination of attributes will have a certain probability fl (d) of being labeled as a positive instance of the target concept. The function fl (d) : D ! <ref> [0; 1] </ref> gives the probability that a given element of D is assigned a positive label. This corresponds to the problem of classification in the presence of noise, as defined in section 1.1.2.
Reference: [2] <author> D. Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Inform. Control, </journal> <volume> 39(3) </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: Note, however, that the size of the input is in itself exponential on the number of states in the resulting DFA. Angluin has shown that even if an arbitrarily small fixed fraction (j (n) j) * , * &gt; 0 is missing, the problem remains NP-complete <ref> [2] </ref>. The problem becomes easier if the algorithm is allowed to make queries or experiment with the unknown machine. Angluin [3] proposes an algorithm based on the approach described by Gold [31] that solves the problem in polynomial time by allowing the algorithm to ask membership queries.
Reference: [3] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Inform. Com-put., </journal> <volume> 75(2) </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Angluin has shown that even if an arbitrarily small fixed fraction (j (n) j) * , * &gt; 0 is missing, the problem remains NP-complete [2]. The problem becomes easier if the algorithm is allowed to make queries or experiment with the unknown machine. Angluin <ref> [3] </ref> proposes an algorithm based on the approach described by Gold [31] that solves the problem in polynomial time by allowing the algorithm to ask membership queries.
Reference: [4] <author> William Armstrong and Jan Gecsei. </author> <title> Adaptation algorithms for binary tree networks. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 9 </volume> <pages> 276-285, </pages> <year> 1979. </year>
Reference-contexts: The results and computation times obtained using these algorithms are shown in section 4.5 and compared with the ones obtained using the greedy approach described in this chapter. The algorithms for the adaption of decision trees by Armstrong and Gecsei <ref> [4] </ref> are also very limited in the type of transformations they can perform in the structure of the network and will work only if the structure is well tuned to the problem at hand.
Reference: [5] <author> Timur Ash. </author> <title> Dynamic node creation in backpropagation networks. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 365-375, </pages> <year> 1989. </year>
Reference-contexts: Many algorithms based on this technique have been proposed [8, 41, 52, 84] and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights. Constructive algorithms like the cascade-correlation [24] and others <ref> [25, 5] </ref> derive the architecture by modifying the error minimization strategy [39] and allowing for new units to be created if the current solution is not satisfactory. Another possibility is to prune the network obtained in an effort to minimize the overall complexity of the classifier [55]. 3.1.
Reference: [6] <author> Les Atlas, Ronald Cole, Yeshwant Muthusamy, Alan Lippman, Jerome Connor, Dong Park, Mohamed El-Sharkawi, and Robert J. Marks II. </author> <title> A performance comparison of trained multi-layer perceptrons and trained classification trees. </title> <booktitle> IEEE Proceedings, </booktitle> <year> 1990. </year>
Reference-contexts: Little or no extra information is required apart from the data present in the training set and the resulting tree can be easily implemented using standard digital technology. Empirical results <ref> [6] </ref> have shown that the performance of decision trees is similar to that of trained neural networks for many problems. There are, however, some domains where the algorithms described in this dissertation perform much better than decision trees.
Reference: [7] <author> K. A. Bartlett, R. K. Brayton, G. D. Hachtel, R. M. Jacoby, R. Rudell, A. Sangiovanni-Vincentelli, and A. Wang. </author> <title> Multi-level logic minimization using implicit don't cares. </title> <journal> IEEE Transactions on CAD, </journal> <year> 1988. </year>
Reference-contexts: Techniques that allow multi-level logic synthesis systems to make a better use of the don't care set were studied and implemented in systems like SIS <ref> [7, 87] </ref> but they require too much computation time for this application and are not always effective. The results and computation times obtained using these algorithms are shown in section 4.5 and compared with the ones obtained using the greedy approach described in this chapter.
Reference: [8] <author> S. Becker and Y. Le Cun. </author> <title> Improving the convergence of back-propagation learning with second order methods. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 29-37, </pages> <address> San Ma-teo, 1989. </address> <publisher> Morgan Kaufmann. BIBLIOGRAPHY 147 </publisher>
Reference-contexts: The partial derivatives of expression (3.3) with respect to each one of the weights in the network can be easily obtained and gradient descent techniques can be used to obtain a local minimum of the error in weight space. Many algorithms based on this technique have been proposed <ref> [8, 41, 52, 84] </ref> and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights.
Reference: [9] <author> A. W. Biermann and R. Krishnaswamy. </author> <title> Constructing programs from example computations. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-2:141-153, </volume> <year> 1976. </year>
Reference-contexts: The best algorithms known for the specific problem addressed here, where the learner has not control over the training set, remain the ones proposed by Bierman et Al. <ref> [9, 10] </ref>. These algorithms are based on an explicit search algorithm and are guaranteed to obtain the exact solution, although they require, in general, exponential time. These algorithms are described in some detail in section 6.4. <p> This lower bound is used as the starting point for the implicit enumeration algorithm. 6.4 The Explicit Search Algorithm The explicit search algorithm implemented for the purpose of comparison is based on the algorithm proposed by Bierman et Al. <ref> [9, 10] </ref>. It builds a finite-state machine and a mapping function F by fitting transitions from the TFSM T into the machine M , one by one, forcing the transition (6.6) and output requirements (6.5) to be satisfied for all the 6.4.
Reference: [10] <author> A. W. B. R. I. Biermann and F. E. Petry. </author> <title> Speeding up the synthesis of programs from traces. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-24:122-136, </volume> <year> 1975. </year>
Reference-contexts: The best algorithms known for the specific problem addressed here, where the learner has not control over the training set, remain the ones proposed by Bierman et Al. <ref> [9, 10] </ref>. These algorithms are based on an explicit search algorithm and are guaranteed to obtain the exact solution, although they require, in general, exponential time. These algorithms are described in some detail in section 6.4. <p> This lower bound is used as the starting point for the implicit enumeration algorithm. 6.4 The Explicit Search Algorithm The explicit search algorithm implemented for the purpose of comparison is based on the algorithm proposed by Bierman et Al. <ref> [9, 10] </ref>. It builds a finite-state machine and a mapping function F by fitting transitions from the TFSM T into the machine M , one by one, forcing the transition (6.6) and output requirements (6.5) to be satisfied for all the 6.4.
Reference: [11] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <booktitle> In Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Lemma 1 It is NP-complete to determine if a two-level threshold gate network consistent with a given training set and with less than k gates exists. Proof: this problem is one of the versions of the loading problem proved NP-complete in [45] and <ref> [11] </ref>. Lemma 2 For a fixed ordering of the variables, it is NP-complete to determine if a reduced ordered decision graph that is consistent with a given training set and has less than k nodes exists. Proof: obtained by a reduction from graph K-colorability in [97]. <p> However, by using this restricted representation and a different approach, the solution is guaranteed to match the training set data, a problem that is, in itself, NP-complete to solve using neural network type algorithms <ref> [11, 45] </ref>. The selection of the minimal complexity solution is still an NP-complete problem, but any solution generated by the algorithm classifies correctly, by construction, all the instances in the training set.
Reference: [12] <author> B. Boser and E. Sackinger. </author> <title> An analog neural network processor with programmable network topology. </title> <booktitle> In ISSCC91 Digest of Technical Papers. IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: Applications that require high speed and/or compact hardware implementations can benefit from an approach based on Boolean networks because the speed and compactness of digital implementations is still unmatched by its analog counterparts <ref> [12, 86] </ref>. Additionally, many alternatives are available to designers that want to implement Boolean networks, from full-custom design to field programmable gate arrays. This makes the digital alternative more cost effective than solutions based on analog designs.
Reference: [13] <author> K. Brace, R. Rudell, and R. Bryant. </author> <title> Efficient implementation of a BDD package. </title> <booktitle> In Design Automation Conference, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Also used are the constructive induction algorithms proposed by Pagallo and Haussler [69] and further developed in [70] and [101]. From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions <ref> [18, 13] </ref> and the variable reordering algorithms studied in [26] and [83] are critically important. For the benefit of readers not familiar with the use of RODGs as a tool for 5.2. <p> The resulting 3TRODG is no longer reduced, but this transformation is required to warrant that the minimum solution is found. Obtaining F from the training set is a simple procedure [18] that can be performed using a standard RODG package like, for instance, the one described in <ref> [13] </ref>. 68 CHAPTER 5. <p> The sift algorithm is based on the fact that swapping the order of two variables in the RODG ordering can be done very efficiently <ref> [13, 44] </ref> because only the nodes in these two levels are affected. The algorithm selects the best position in the ordering for a given variable by moving that variable up and down (using the inexpensive swap operation) and recording the smaller size observed. <p> The algorithms described in this section were implemented in a system called smog ( Selection of Minimal Ordered Graphs) that uses the RODG package described in <ref> [13] </ref> to perform the standard RODG manipulations. After an extensive empirical evaluation of the algorithm performance with various values of the parameters, the default value for the parameter ff in equation (2.15) was set equal to 0.5. <p> of VLSI implementations by reducing the amount of space dedicated to routing. 135 Appendix A Function Manipulation Using RODGs A.1 Algorithms for RODG Manipulation This section gives a brief overview of the algorithms that were developed for RODG manipulation and follows closely in form and content the work presented in <ref> [13] </ref>. For a much more complete description of the algorithms used, the interested reader should consult this reference. This first section is concerned with RODGs defined over Boolean spaces. <p> FUNCTION MANIPULATION USING RODGS Packages that manipulate reduced ordered decision graphs are widely available and have become the most commonly used tool for discrete function manipulation in the logic synthesis community [16]. Some of these packages are restricted to Boolean functions <ref> [13] </ref> (each non-terminal node has exactly two outgoing edges) while others [46] can accept multi-valued attributes. All these packages provide at least the same basic functionality: the ability to combine functions using basic Boolean and arithmetic operations and the ability to test for containment or equivalence of two functions.
Reference: [14] <author> R. Brayton, G. Hachtel, C. McMullen, and A. Sangiovanni-Vincentelli. </author> <title> Logic Minimization Algorithms for VLSI Synthesis. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1984. </year>
Reference-contexts: The minimization of expression (2.14) using this encoding scheme is equivalent to the selection of an M-cover of minimal cardinality for the incompletely specified function f defined by the training set. 3.3.2 A Local Search Algorithm Algorithms for the minimization of two-level Boolean gate networks like, for instance, espresso <ref> [14] </ref>, perform the search for a cover of minimal cardinality by expanding the cubes in a solution and then solving a set covering problem using standard techniques. A similar approach could be used here if the objective was to select a pyramid cover (i.e., a 1-cover) for a given function. <p> 64 2 2 23.2 10-asymmetry 1024 160 2 2 408.6 12-asymmetry 4096 384 2 2 11752.9 Table 3.1: Experiments using threshold gates. 3.4.2 Comparison With Standard Two-Level Minimizers In the second set of problems, the performance of the algorithm was compared with the performance of a popular two-level minimizer, espresso <ref> [14] </ref>. This comparison was performed by constraining lsat to use only and gates in the first level and or gates in the second level. This can be easily done by not allowing the second type of expand operations to be performed.
Reference: [15] <author> R. Brayton, R. Rudell, A. Sangiovanni-Vincentelli, and A. Wang. </author> <title> MIS: A multiple-level logic optimization system. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> November </month> <year> 1987. </year>
Reference-contexts: In these conditions, it is possible to compare the performance of the algorithm with standard logic synthesis techniques, and, in particular, multi-level logic synthesis algorithms. The two algorithms selected are both based on the SIS system <ref> [15] </ref> developed at Berkeley but use two different scripts that give very different results. The rugged script uses the computation of local don't care sets for each node to achieve a simpler final network.
Reference: [16] <author> R. K. Brayton, G. D. Hachtel, and A. L. Sangiovanni-Vincentelli. </author> <title> Multilevel logic synthesis. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 264-300, </pages> <year> 1990. </year>
Reference-contexts: On the other hand, extensive work has been done on logic synthesis algorithms for multi-level Boolean networks <ref> [16] </ref>. This algorithms can be used, in principle, to select a multi-level Boolean network that is of minimal size and is consistent with the incompletely specified function defined by the training set. <p> FUNCTION MANIPULATION USING RODGS Packages that manipulate reduced ordered decision graphs are widely available and have become the most commonly used tool for discrete function manipulation in the logic synthesis community <ref> [16] </ref>. Some of these packages are restricted to Boolean functions [13] (each non-terminal node has exactly two outgoing edges) while others [46] can accept multi-valued attributes.
Reference: [17] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: This limitations make it both easier to implement this type of networks in hardware (although not as easy as the other three representations) and to design minimal size networks. A more detailed comparison between the two approaches is presented in chapter 3. 1.4.2 Decision Trees Decision trees <ref> [17, 76, 79] </ref>, on the other hand, do not suffer from these drawbacks. Little or no extra information is required apart from the data present in the training set and the resulting tree can be easily implemented using standard digital technology. <p> Decision graphs can be viewed as a generalization of decision trees, a very successful approach for the inference of classification rules <ref> [17, 76, 79] </ref>. The selection of decision graphs instead of decision trees as the hypothesis representation scheme is important because, even though decision trees can represent any concept, they are not concise representations for some concepts of interest.
Reference: [18] <author> R. E. Bryant. </author> <title> Graph-based algorithms for Boolean function manipulation. </title> <journal> IEEE Transactions on Computers, </journal> <year> 1986. </year>
Reference-contexts: INTRODUCTION always tested in that order (possibly skipping some variables). A decision graph is called reduced if no two nodes are equal (same label and same descendents) and no node has the else and then edges pointing to the same node <ref> [18] </ref>. A decision graph that is both reduced and ordered is called a reduced ordered decision graph (RODG). Reduced ordered decision graphs are known in the logic synthesis community as Boolean decision diagrams (BDDs). <p> Also used are the constructive induction algorithms proposed by Pagallo and Haussler [69] and further developed in [70] and [101]. From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions <ref> [18, 13] </ref> and the variable reordering algorithms studied in [26] and [83] are critically important. For the benefit of readers not familiar with the use of RODGs as a tool for 5.2. <p> The resulting 3TRODG is no longer reduced, but this transformation is required to warrant that the minimum solution is found. Obtaining F from the training set is a simple procedure <ref> [18] </ref> that can be performed using a standard RODG package like, for instance, the one described in [13]. 68 CHAPTER 5. <p> Since they already exist they are shared by different functions. A.2 Manipulating Boolean Functions Using RODGs For a given ordering of the variables, reduced ordered decision graphs are a canonical representation for functions in that domain <ref> [18] </ref>. This means that given a function f : f0; 1g N ! f0; 1g and an ordering of the variables, there is one and only one representation for the function f . A.2.
Reference: [19] <author> R. Carraghan and P. M. Pardalos. </author> <title> An exact algorithm for the maximum clique problem. </title> <journal> Operations Research Letters, </journal> <volume> 9 </volume> <pages> 375-382, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: A large clique in the incompatibility graph is identified using a slightly modified version of an exact algorithm proposed by Carraghan and Pardalos <ref> [19] </ref>. Pseudo-code for 98 CHAPTER 6.
Reference: [20] <author> G. J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences. </title> <journal> Journal ACM, </journal> <volume> 16 </volume> <pages> 145-159, </pages> <year> 1969. </year>
Reference-contexts: Now, if M is chosen to be a universal Turing machine the Kolmogorov complexity with respect to M is called simply the Kolmogorov complexity and is defined as K (s). Kolmogorov [51], Solomonoff [95] and Chaitin <ref> [20] </ref> proved that, for any Turing machine M K (s) K M (s) + k for all finite strings s (2.9) where k is a constant that depends only on M .
Reference: [21] <author> Olivier Coudert, Christian Berthet, and Jean Christophe Madre. </author> <title> Verification of synchronous sequential machines based on symbolic execution. </title> <editor> In J. Sifakis, editor, </editor> <booktitle> Proceedings of the Workshop on Automatic Verification Methods for Finite State Systems, 148 BIBLIOGRAPHY volume 407 of Lecture Notes in Computer Science, </booktitle> <pages> pages 365-373. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: In this case, the final RODG is the same as the one obtained using the initial decision tree although this is not always the case. 5.4.1.3 Initialization Using the Restrict Operator The third way to initialize the algorithm is to use the restrict operator <ref> [21] </ref>. This RODG operator can be used to obtain a more compact RODG representation for a function defined by its on and off sets. The restrict operator belongs to a family of heuristics [92] that generate a small RODG by merging, in a bottom up fashion, nodes in an RODG.
Reference: [22] <author> S. Das and M. Mozer. </author> <title> A unified gradient-descent/clustering algorithm architecture for finite state machine induction. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> Denver, CO, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These algorithms are described in some detail in section 6.4. Recently, connectionist approaches have been proposed that address the problem of learning from a given set of strings, but had limited success. Das and Mozer <ref> [22] </ref>, Giles et al. [29] and Polack [74] propose different approaches based on gradient descent algorithms for neural network training, but their results show that this strategy does not have any important advantages over search-based methods like the ones proposed by Bierman.
Reference: [23] <author> T. G. Dietterich and G. Bakiri. </author> <title> Error-correcting output codes: A general method for improving multiclass inductive learning programs. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pages 572-577. </pages> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: INTRODUCTION one and a class value can not be attributed to that instance, but will not result in a mis-classification. * Error correcting code: Encode the label value using an error correcting code <ref> [23, 73] </ref>. This approach preserves most of the compactness of a binary encoding while being much less sensitive to errors in one of the single class problems. Additionally, the Hamming distance between the observed outputs and the closest valid codeword gives a measure of the certainty of the classification.
Reference: [24] <author> S.E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> San Mateo, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Many algorithms based on this technique have been proposed [8, 41, 52, 84] and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights. Constructive algorithms like the cascade-correlation <ref> [24] </ref> and others [25, 5] derive the architecture by modifying the error minimization strategy [39] and allowing for new units to be created if the current solution is not satisfactory.
Reference: [25] <author> M Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference-contexts: Many algorithms based on this technique have been proposed [8, 41, 52, 84] and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights. Constructive algorithms like the cascade-correlation [24] and others <ref> [25, 5] </ref> derive the architecture by modifying the error minimization strategy [39] and allowing for new units to be created if the current solution is not satisfactory. Another possibility is to prune the network obtained in an effort to minimize the overall complexity of the classifier [55]. 3.1.
Reference: [26] <author> Steven J. Friedman and Kenneth J. Supowit. </author> <title> Finding the optimal variable ordering for binary decision diagrams. </title> <journal> IEEE Trans. Comput., </journal> <volume> 39(5) </volume> <pages> 710-713, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions [18, 13] and the variable reordering algorithms studied in <ref> [26] </ref> and [83] are critically important. For the benefit of readers not familiar with the use of RODGs as a tool for 5.2. <p> However, many heuristic algorithms have been proposed for this problem <ref> [26, 83] </ref>. In this setting, the problem is even more complex because the objective is to select an ordering that minimizes the final RODG and this ordering may not be the same as the one that minimizes the RODG obtained after the initialization step. The solution found is 5.4.
Reference: [27] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: A complete explanation of the concepts involved in the definition of this complexity class is outside the scope of this work and the reader is referred to <ref> [27] </ref> for an excellent review of the subject. The following lemmas show that the computational complexity of the problems addressed in this dissertation is high. Since these results are not critical for the exposition that follows, only brief references are made to the respective proofs.
Reference: [28] <author> M. D. Garris and R. A. Wilkinson. </author> <title> NIST Special Database 3 : Handwritten Segmented Characters. </title> <institution> National Institute of Standards and Technology, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: This problem is used as one of the case studies in chapter 7. The second domain used to motivate this work is in the field of handwritten character recognition. Figure 1.4 shows instances of patterns that correspond to different handwritten digits extracted from the widely available NIST database <ref> [28] </ref>. The objective in this case is to infer a classification rule that allows a classifier system to recognize which particular digit the writer intended to write. This problem is an example of a multi-class problem because the number of different labels is higher than 2. <p> The problem addressed is the recognition of handwritten digits introduced in section 1.1.1. The input data used was obtained from the National Institute of Standards and Technology (NIST) database of segmented handwritten characters <ref> [28] </ref> available in CDROM. The handwritten characters in this database were obtained by scanning a large number of forms filled by volunteers and were assembled under the auspices of NIST. This database contains a total of 223,125 images of handwritten digits.
Reference: [29] <author> C. L. Giles, C. B. Miller, D. Chen, H. H. Chen, G. Z. Sun, and Y. C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: These algorithms are described in some detail in section 6.4. Recently, connectionist approaches have been proposed that address the problem of learning from a given set of strings, but had limited success. Das and Mozer [22], Giles et al. <ref> [29] </ref> and Polack [74] propose different approaches based on gradient descent algorithms for neural network training, but their results show that this strategy does not have any important advantages over search-based methods like the ones proposed by Bierman.
Reference: [30] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Inform. Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: FORMAL LEARNING MODELS 19 2.3 Formal Learning Models 2.3.1 Identification in the Limit One of the first formal treatments of the problem addressed here is due to Gold <ref> [30] </ref>. He studies the problem of identifying a given language given a sample of strings in that language. The learner is presented with an infinite sequence of strings in some language L that is known to belong to a given class of languages.
Reference: [31] <author> E. M. Gold. </author> <title> System identification via state characterization. </title> <journal> Automatica, </journal> <volume> 8 </volume> <pages> 621-636, </pages> <year> 1972. </year>
Reference-contexts: The problem becomes easier if the algorithm is allowed to make queries or experiment with the unknown machine. Angluin [3] proposes an algorithm based on the approach described by Gold <ref> [31] </ref> that solves the problem in polynomial time by allowing the algorithm to ask membership queries. Schapire [89] proposes an interesting approach that does not require the availability of a reset signal to take the machine to a known state.
Reference: [32] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Inform. Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year> <note> BIBLIOGRAPHY 149 </note>
Reference-contexts: Lemma 3 It is NP-complete to determine if a finite state machine that has less than k states and is consistent with a given training set 6 exists. Proof: made by a reduction from the satisfiability problem with 3 variable clauses (3-SAT) in <ref> [32] </ref>. The problem of selecting the minimum multi-level Boolean network that is consistent with a given training set is also of high complexity. It is known to be in NP, because a solution can easily be checked for correctness in polynomial time. <p> Therefore, all the previous work done using the DFA formalism is relevant and closely related to the work discussed here. The problem of selecting the minimum DFA consistent with a set of labeled strings is known to be NP-complete. Specifically, Gold <ref> [32] </ref> proved that given a finite alphabet , two finite subsets S; T fl and an integer k, determining if there is a k-state DFA that 90 CHAPTER 6. FINITE STATE MACHINES recognizes L such that S L and T fl L is NP-complete.
Reference: [33] <author> J. A. Goldman. </author> <title> Pattern theoretic knowledge discovery. </title> <booktitle> In 6th International IEEE Conference on Tools with AI, </booktitle> <year> 1994. </year>
Reference-contexts: a wide variety of problems for testing machine learning systems. * Randomly generated functions: rnd1, rnd2, and rnd3. * Randomly generated functions with a fixed number of minority elements: rnd m1, rnd m5, rnd m10, rnd m25 and rnd m50. * Random functions with irrelevant variables: rmdvv36. * Boolean expressions <ref> [33] </ref> : kdd1, kdd2, kdd3, kdd4, kdd5, kdd6, kdd7, kdd8, kdd9, kdd10 * Multiplexer functions : mux6. * Deep functions : and or chain8 = ((((x 1 x 2 + x 3 )x 4 + x 5 )x 6 + x 7 )x 8 ) * Monkish Problems: 8 binary variable
Reference: [34] <author> Jeffrey A. Goldman. </author> <title> Machine learning: A comparative study of pattern theory and C4.5. </title> <type> Technical Report WL-TR-94-1102, </type> <institution> Wright Laboratory, USAF, </institution> <address> WL/AART, WPAFB, OH 45433-6543, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The meaning of the column labels is the same as before. The results listed for the program C4.5 were obtained with the best combination found for the several options allowed by this program <ref> [34] </ref>. Since these problems are known to be noise free smog was 7.1. <p> DESCRIPTION OF THE PROBLEMS USED B.2 Problems from the Wright Laboratory Set This set of problems has been assembled by a research group at the Pattern Theory Program of the Air Force Wright Laboratory. A more complete description of these problems can be found in <ref> [34] </ref>. All functions are of the form f (x 1 :::x 8 ) ! f0; 1g.
Reference: [35] <author> Nelson Goodman. </author> <title> Fact, Fiction and Forecast. </title> <publisher> Harvard University Press, </publisher> <year> 1983. </year>
Reference-contexts: the examples in the training set will, in general, be obtained using the probabilistic labeling defined by the function fl (d) described in section 2.2. 2.4 Equivalence of all Biases The equivalence of all biases (and consequently, of all learning algorithms) in the absence of a context is well known <ref> [35, 60] </ref>. Schaffer [88] presented a very general argument for this equivalence in the form of a conservation principle. He restricts the analysis to single concept learning problems in discrete domains. Since the domain is discrete, there will be a finite number of possible attribute combinations, or objects in D.
Reference: [36] <author> A. Grasselli and F. Luccio. </author> <title> A method for minimizing the number of internal states in incompletely specified sequential networks. </title> <journal> IRE Transactions on Electronic Computers, </journal> <volume> EC-14(3):350-359, </volume> <month> June </month> <year> 1965. </year>
Reference-contexts: j that is a compatible set of G and satisfies: s i s j ^ C e (s j ) C e (s i ) ^ C t (s j ) C t (s i ) Since there exists one minimum solution that consists entirely of prime compatible sets (see <ref> [36] </ref>) an exact algorithm for solving the minimum closed cover is the following: 1. Generate all prime compatible sets of G. 2. Formulate the closed covering problem as the satisfiability of a Boolean expression and solve it. <p> The minimal machine can be found by selecting a minimal set of compatibles that satisfies two simple requirements. This method was improved by Grasselli and Luccio <ref> [36] </ref> who showed that only a subset of the compatibles, the prime compatibles, need be considered. An efficient implementation of these algorithms was made available in the stamina program by Hachtel et al. [38].
Reference: [37] <author> James N. Gray and Michael A. Harrison. </author> <title> The theory of sequential relations. </title> <journal> Information and Control, </journal> <volume> 9, </volume> <year> 1966. </year>
Reference-contexts: FINITE STATE MACHINES recognizes L such that S L and T fl L is NP-complete. If all strings of length n or less are given (a uniform-complete sample), then the problem can be solved in a time polynomial on the input size <ref> [100, 75, 37] </ref>. Note, however, that the size of the input is in itself exponential on the number of states in the resulting DFA. Angluin has shown that even if an arbitrarily small fixed fraction (j (n) j) * , * &gt; 0 is missing, the problem remains NP-complete [2].
Reference: [38] <author> G. Hachtel, J.-K. Rho, F. Somenzi, and R. </author> <title> Jacoby. Exact and heuristic algorithms for the minimization of incompletely specified state machines. </title> <booktitle> In The Proceedings of the European Design Automation Conference, </booktitle> <year> 1991. </year>
Reference-contexts: The satisfiability problem is NP-complete and can be solved, for some problems, using one the methods presented in the literature. A detailed analysis of these algorithms is outside the scope of this work. The implementation of this algorithm uses the routines described in <ref> [38] </ref> to solve the minimum covering problem. 5.4 An Heuristic Minimization Algorithm The heuristic minimization algorithm described in this section derives an RODG that corresponds to a local minimum of (2.15) under the encoding scheme described in section 5.2.3. <p> This method was improved by Grasselli and Luccio [36] who showed that only a subset of the compatibles, the prime compatibles, need be considered. An efficient implementation of these algorithms was made available in the stamina program by Hachtel et al. <ref> [38] </ref>. This algorithm is still the state of the art in finite state machine reduction for the majority of the cases. Some problems, however, exhibit exponentially large numbers of compatibles, rendering an explicit enumeration approach such as stamina's ineffective. <p> In this section, the performance of iasmin, the algorithm described in section 6.5 is compared with the performance of two algorithms that solve the problem using the FSM reduction paradigm: stamina <ref> [38] </ref> and ism [47]. To perform this comparison, three target machines, shown in Figure 6.11 were selected. For each machine, a number of training sets was generated, each training set consisting of a single random string of length between 10 and 65. <p> To obtain a tighter bound on the number of states in the minimal consistent machine for a given training set, all states and transitions which were not visited are discarded. The resulting completely specified machine is sent through a traditional state minimizer, stamina, <ref> [38] </ref> and the number of states in this minimized machine is used as an estimate. To examine the performance of the implicit and explicit search approaches with differently-sized minimum consistent machines, 575 training sets 3 were generated from 115 finite state machines generated in this fashion.
Reference: [39] <author> S.J. Hanson and L. Pratt. </author> <title> A comparison of different biases for minimal network construction with back-propagation. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 177-185, </pages> <address> San Mateo, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These algorithms, however, do not derive the architecture of the network, but only the connection weights. Constructive algorithms like the cascade-correlation [24] and others [25, 5] derive the architecture by modifying the error minimization strategy <ref> [39] </ref> and allowing for new units to be created if the current solution is not satisfactory. Another possibility is to prune the network obtained in an effort to minimize the overall complexity of the classifier [55]. 3.1.
Reference: [40] <author> D. Haussler, M. Kearns, and R. E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This was the approach used by Pagallo and Haussler [69] to derive appropriate training set sizes for the problems they consider. Regrettably, this approach gives, in many cases, estimates of the required size that are too pessimistic because they are based on theoretical worst-case bounds <ref> [40] </ref>. Furthermore, the complexity of the representation of the target concept depends on the underlying representation used by the learning algorithms. This makes it difficult to use this approach when two very different representation schemes are used.
Reference: [41] <author> G.E. Hinton and T.J. Sejnowski. </author> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 7, </volume> <pages> pages 282-317. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Each processing element computes a weighted sum of its inputs and outputs a continuous function of this sum. Among all algorithms that can be used to select the node functions, the back-propagation <ref> [41] </ref> algorithm is the most popular one. This approach is remarkably effective in a wide variety of problems and the algorithms used are straightforward. <p> The partial derivatives of expression (3.3) with respect to each one of the weights in the network can be easily obtained and gradient descent techniques can be used to obtain a local minimum of the error in weight space. Many algorithms based on this technique have been proposed <ref> [8, 41, 52, 84] </ref> and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights.
Reference: [42] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Finite state machines are a natural representation for hypotheses in this domain. Unlike the case in the previous chapters the selected representation cannot represent all possible concepts defined in the domain because only sets that correspond to regular languages can be represented by finite state machines <ref> [42] </ref>. This set is, however, rich enough to contain many concepts of interest and many researchers have addressed inductive inference problems using this representation.
Reference: [43] <author> Alan Hutchinson. </author> <title> Algorithmic Learning. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1994. </year> <note> 150 BIBLIOGRAPHY </note>
Reference-contexts: In particular, this thesis describes algorithms for the inference of classification rules in discrete domains. The exposition made in the following sections intends to introduce the basic concepts and definitions involved in inductive inference but does not pretend to be exhaustive or even complete. The reader is referred to <ref> [43] </ref> and [64] for comprehensive reviews of both the empirical and theoretical issues involved in inductive inference. 1.1 Inductive Inference Inductive inference problems are characterized by a domain D and a learner, or learning algorithm. The learner is able to observe some objects (or instances) in the domain.
Reference: [44] <author> N. Ishiura, H. Sawada, and S. Yajima. </author> <title> Minimization of binary decision diagrams based on exchanges of variables. </title> <booktitle> In ICCAD, </booktitle> <pages> pages 472-475. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: The sift algorithm is based on the fact that swapping the order of two variables in the RODG ordering can be done very efficiently <ref> [13, 44] </ref> because only the nodes in these two levels are affected. The algorithm selects the best position in the ordering for a given variable by moving that variable up and down (using the inexpensive swap operation) and recording the smaller size observed.
Reference: [45] <author> J. S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Lemma 1 It is NP-complete to determine if a two-level threshold gate network consistent with a given training set and with less than k gates exists. Proof: this problem is one of the versions of the loading problem proved NP-complete in <ref> [45] </ref> and [11]. Lemma 2 For a fixed ordering of the variables, it is NP-complete to determine if a reduced ordered decision graph that is consistent with a given training set and has less than k nodes exists. Proof: obtained by a reduction from graph K-colorability in [97]. <p> However, by using this restricted representation and a different approach, the solution is guaranteed to match the training set data, a problem that is, in itself, NP-complete to solve using neural network type algorithms <ref> [11, 45] </ref>. The selection of the minimal complexity solution is still an NP-complete problem, but any solution generated by the algorithm classifies correctly, by construction, all the instances in the training set.
Reference: [46] <author> T. Kam and R.K. Brayton. </author> <title> Multi-valued decision diagrams. </title> <type> Tech. Report No. </type> <institution> UCB/ERL M90/125, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Some of these packages are restricted to Boolean functions [13] (each non-terminal node has exactly two outgoing edges) while others <ref> [46] </ref> can accept multi-valued attributes. All these packages provide at least the same basic functionality: the ability to combine functions using basic Boolean and arithmetic operations and the ability to test for containment or equivalence of two functions. <p> For a given variable ordering, reduced, ordered MDDs are canonical representations for functions defined over that domain, thus implying that two functions can easily be checked for equivalence. The implicit approach described in chapter 6 used the MDD package described in <ref> [46] </ref>. This MDD package provides an array of primitives for function manipulation. The reader is referred to that reference for a more detailed description of these primitives.
Reference: [47] <author> T. Kam, T. Villa, R. K. Brayton, and A. Sangiovanni-Vincentelli. </author> <title> A fully implicit algorithm for exact state minimization. </title> <booktitle> Proc. Design Automat. Conf., </booktitle> <year> 1994. </year>
Reference-contexts: In this case, a version of Grasseli and Luccio's algorithm based on the implicit enumeration of the compatibles proposed by Kam et al. <ref> [47] </ref> is more efficient. The approach described in this chapter represents joint work with Stephen Ed-wards and is also based on the use of implicit techniques to perform the search for the minimum consistent finite state machine. <p> In this section, the performance of iasmin, the algorithm described in section 6.5 is compared with the performance of two algorithms that solve the problem using the FSM reduction paradigm: stamina [38] and ism <ref> [47] </ref>. To perform this comparison, three target machines, shown in Figure 6.11 were selected. For each machine, a number of training sets was generated, each training set consisting of a single random string of length between 10 and 65.
Reference: [48] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <pages> pages 291-307, </pages> <month> February </month> <year> 1970. </year>
Reference-contexts: A partition of these clusters that maximizes locally (4.5) is then obtained using the Kerninghan-Lin partitioning algorithm <ref> [48] </ref>. This algorithm selects a good partition with respect to some cost function by swapping objects (in this case by swapping clusters) between the two sides of the partition and evaluating the net effect of such swaps in the target cost function.
Reference: [49] <author> S. Kirkpatrick, C.D. Gelatt Jr., </author> , <title> and M.P. Vecchi. Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <year> 1983. </year> <note> Reprinted in [1]. </note>
Reference-contexts: Algorithms like the metropolis algorithm [58] or simulated annealing <ref> [49] </ref> use such an approach.
Reference: [50] <author> Ron Kohavi. </author> <title> Bottom-up induction of oblivious read-once decision graphs. </title> <booktitle> In European Conference in Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: Section 5.4 describes an heuristic approach that obtains an RODG that minimizes (2.15) and is much faster than the exact one for large examples. Furthermore, it derives a good ordering of the variables together with the RODG structure. The approach differs from the one proposed by Kohavi <ref> [50] </ref> that also uses RODGs. Although his approach performs well for small problems, it requires far too much computation to be applicable to any problems of reasonable size.
Reference: [51] <author> A. N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems Information Transmission, </journal> <volume> 1 </volume> <pages> 1-7, </pages> <year> 1965. </year>
Reference-contexts: Now, if M is chosen to be a universal Turing machine the Kolmogorov complexity with respect to M is called simply the Kolmogorov complexity and is defined as K (s). Kolmogorov <ref> [51] </ref>, Solomonoff [95] and Chaitin [20] proved that, for any Turing machine M K (s) K M (s) + k for all finite strings s (2.9) where k is a constant that depends only on M .
Reference: [52] <author> A. Kramer. </author> <title> Optimization techniques for neural networks. </title> <type> Technical Report UCB-ERL-M89-1, </type> <institution> UC Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1989. </year>
Reference-contexts: The partial derivatives of expression (3.3) with respect to each one of the weights in the network can be easily obtained and gradient descent techniques can be used to obtain a local minimum of the error in weight space. Many algorithms based on this technique have been proposed <ref> [8, 41, 52, 84] </ref> and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights.
Reference: [53] <author> K. J. Lang. </author> <title> Random DFA's can be approximately learned from sparse uniform examples. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 45-52. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: It must be pointed out, however, that the main purpose of the connectionist work was not to beat discrete search algorithms, but to evaluate the applicability of such an approach to problems of this type. Lang <ref> [53] </ref> describes a promising heuristic approach that is also based on a discrete search strategy and is very efficient. He has shown that it can be applied to find approximate solutions for machines with several hundred states.
Reference: [54] <author> Y. Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. </author> <title> Handwritten digit recognition with a back-propagation network. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 396-404, </pages> <address> San Mateo, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: selected together with an independent set of 52467 digits that was used to evaluate the accuracy obtained by the algorithms. 7.2.2 Pre-processing and Encoding The images in the database were subject to a standard pre-processing procedure that is commonly used by the majority of the algorithms used for this problem <ref> [54] </ref>. First, they were de-skewed. This procedure identifies the angle of the principal component of the image and performs a transformation of the image using a linear operator that has the net effect of normalizing the characters with respect to the angle they were written initially.
Reference: [55] <author> Y. Le Cun, J.S. Denker, and S.A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo, 1990. </address> <publisher> Morgan Kaufmann. BIBLIOGRAPHY 151 </publisher>
Reference-contexts: Another possibility is to prune the network obtained in an effort to minimize the overall complexity of the classifier <ref> [55] </ref>. 3.1. RELATED WORK 33 In all these approaches, the selection of a minimal complexity network remains secondary to the minimization of (3.3). All these algorithms use standard optimization techniques to select a connection weights combination that is a local minimum of (3.3).
Reference: [56] <author> M. Li and P. M. B Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity. </title> <publisher> Addison-Wesley, </publisher> <address> MA, </address> <year> 1994. </year>
Reference-contexts: The reader is referred to one of the existing reviews of the subject <ref> [56] </ref> for a more complete treatment. The Kolmogorov complexity theory defines a universal complexity measure for finite binary strings.
Reference: [57] <author> J. J. Mahoney and R. J. Mooney. </author> <title> Initializing ID5R with a domain theory: some negative results. </title> <type> Technical Report 91-154, </type> <institution> CS Dept., University of Texas at Austin, Austin, TX, </institution> <year> 1991. </year>
Reference-contexts: Oliver [67] describes in some detail these limitations. Decision graphs have been proposed as one way to alleviate these problems, but the algorithms proposed to date for the construction of these graphs suffer from serious limitations. Mahoney and Mooney <ref> [57] </ref> proposed to identify related subtrees in a decision tree obtained using standard methods but reported limited success in the sense that they observed no improvement in the quality of the generalization performed. They used a non-canonical representation of Boolean functions (DNF expressions) to represent the functions 64 CHAPTER 5.
Reference: [58] <author> N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, and E. Teller. </author> <title> Equation of state calculations for fast computing machines. </title> <journal> Journal of Chemical Physics, </journal> <volume> 21 </volume> <pages> 1087-1092, </pages> <year> 1953. </year>
Reference-contexts: Algorithms like the metropolis algorithm <ref> [58] </ref> or simulated annealing [49] use such an approach.
Reference: [59] <author> Paul L. Meyer. </author> <title> Introductory Probability and Statistical Applications. </title> <publisher> Addison-Wesley, </publisher> <year> 1965. </year>
Reference-contexts: EXPERIMENTAL EVALUATION AND APPLICATIONS In order to obtain a test for the validity of H0, observe that the non-biased esti mators for the unknown parameters are z = n and z = n 1 Under this conditions, it is known <ref> [59] </ref> that t = p ^ z has a Student's t distribution with n 1 degrees of freedom.
Reference: [60] <author> T. M. Mitchell. </author> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1980. </year>
Reference-contexts: the examples in the training set will, in general, be obtained using the probabilistic labeling defined by the function fl (d) described in section 2.2. 2.4 Equivalence of all Biases The equivalence of all biases (and consequently, of all learning algorithms) in the absence of a context is well known <ref> [35, 60] </ref>. Schaffer [88] presented a very general argument for this equivalence in the form of a conservation principle. He restricts the analysis to single concept learning problems in discrete domains. Since the domain is discrete, there will be a finite number of possible attribute combinations, or objects in D.
Reference: [61] <author> S. Muroga. </author> <title> Threshold Logic and its Applications. </title> <publisher> Wiley-Interscience, </publisher> <year> 1971. </year>
Reference-contexts: This approach is therefore an interesting alternative in problems that do not require real valued weights and map, in a natural way, into a two-level threshold gate network representation. Previous work closely related to the approach described here is limited. Muroga <ref> [61] </ref> makes a good exposition of the properties of functions implemented by threshold gate networks and proposes algorithms for their synthesis but his approach is limited to functions 34 CHAPTER 3.
Reference: [62] <author> P. M. Murphy and D. W. Aha. </author> <title> Repository of Machine Learning Databases Machine readable data repository. </title> <institution> University of California, Irvine, </institution> <year> 1991. </year>
Reference-contexts: The problems tictactoe, vote, mushroom, breast and splice are from the UCI database <ref> [62] </ref> and are described in detail in the online documentation publicly available. The problems krkp and kkp result from the encoding of chess positions. The first one is described in [90] and is the encoding of a King+Rook vs. King+Pawn chess ending using high level attributes.
Reference: [63] <author> Alan F. Murray and Anthony V. W. Smith. </author> <title> Asynchronous vlsi neural networks using pulse-stream arithmetic. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 23:3:688-697, </volume> <year> 1988. </year>
Reference-contexts: The choice of multi-level Boolean networks instead of threshold gate networks as the hypothesis representation scheme has other advantages. Although threshold gate networks can be implemented using standard digital technologies, for many applications this approach is expensive and inefficient. Pulse stream modulation <ref> [63] </ref> is one possible alternative, but is limited to a relatively small number of neurons and becomes slow if high precision is required. Dedicated boards based on DSP processors can achieve very high performance and are very flexible but may be too expensive for some applications.
Reference: [64] <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: The exposition made in the following sections intends to introduce the basic concepts and definitions involved in inductive inference but does not pretend to be exhaustive or even complete. The reader is referred to [43] and <ref> [64] </ref> for comprehensive reviews of both the empirical and theoretical issues involved in inductive inference. 1.1 Inductive Inference Inductive inference problems are characterized by a domain D and a learner, or learning algorithm. The learner is able to observe some objects (or instances) in the domain.
Reference: [65] <author> Arlindo L. Oliveira. </author> <title> Logic synthesis using threshold gates. UC Berkeley EE290LS Class Report, </title> <year> 1990. </year>
Reference-contexts: A similar approach could be used here if the objective was to select a pyramid cover (i.e., a 1-cover) for a given function. In fact, the expand operations described above can be used to first expand maximally all pyramids in a given cover. Such an approach was actually implemented <ref> [65] </ref> but is restricted to the selection of 1-covers. When the objective is to 3.3. THE SEARCH ALGORITHM 39 select an M-cover, M &gt; 1, these techniques are no longer useful, because the solution does not, in general, consist of prime pyramids.
Reference: [66] <author> Arlindo L. Oliveira and A. Sangiovanni-Vincentelli. </author> <title> Learning complex boolean functions : Algorithms and applications. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> Denver, CO, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The algorithm used in fulfringe <ref> [66] </ref>, identifies patterns near the fringes of the decision tree and uses them to build new attributes. The idea was first proposed in [69] and further developed in [70]. Another algorithm of this family, dcfringe [101] identifies the patterns shown in the first two rows of figure 5.5.
Reference: [67] <author> J. J. Oliver. </author> <title> Decision graphs an extension of decision trees. </title> <type> Technical Report 92/173, </type> <institution> Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1993. </year> <note> 152 BIBLIOGRAPHY </note>
Reference-contexts: Oliver <ref> [67] </ref> describes in some detail these limitations. Decision graphs have been proposed as one way to alleviate these problems, but the algorithms proposed to date for the construction of these graphs suffer from serious limitations. <p> REDUCED ORDERED DECISION GRAPHS implemented by these subtrees. The non-canonicity of this representation makes is a nontrivial process to identify identical subtrees and that renders this approach impracticable for large decision trees. Oliver <ref> [67] </ref> proposed a greedy algorithm that performs either a join or a split operation, depending on which one reduces more the description length.
Reference: [68] <author> Y. Le Cun P. Simard and John Denker. </author> <title> Efficient pattern recognition using a new transformation distance. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 51-58, </pages> <address> Denver, CO, </address> <year> 1992. </year>
Reference-contexts: These solutions <ref> [68] </ref> make heavy use of the characteristics of the domain and use the full set of 223,125 digits as the training set. Depending on the test set used, raw error values between 0.3% and 3.2% have been reported.
Reference: [69] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: These functions are described in appendix B. The first 4 functions were designed to be specially simple and are defined over Boolean spaces with a small number of variables. The last 8 functions have been proposed in the machine learning literature <ref> [69] </ref> and are relatively more complex. For each of these functions, training sets of increasing sizes were randomly generated and labeled in accordance with the function being learned. <p> From machine learning, I use many of the techniques developed for the induction of decision trees described in [76] and [79]. Also used are the constructive induction algorithms proposed by Pagallo and Haussler <ref> [69] </ref> and further developed in [70] and [101]. From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions [18, 13] and the variable reordering algorithms studied in [26] and [83] are critically important. <p> The algorithm used in fulfringe [66], identifies patterns near the fringes of the decision tree and uses them to build new attributes. The idea was first proposed in <ref> [69] </ref> and further developed in [70]. Another algorithm of this family, dcfringe [101] identifies the patterns shown in the first two rows of figure 5.5. These patterns correspond to 8 Boolean functions of 2 variables. <p> For instance, the size of the training set may be selected as a function of the complexity of the target concept when expressed using a given representation scheme. This was the approach used by Pagallo and Haussler <ref> [69] </ref> to derive appropriate training set sizes for the problems they consider. Regrettably, this approach gives, in many cases, estimates of the required size that are too pessimistic because they are based on theoretical worst-case bounds [40]. <p> For example, the problem par5 32 was solved exactly for all experiments when the training set size reached 800 while Pagallo and Haussler reported that they were unable to solve this problem using the constructive induction algorithm fringe even using training sets of size 4000 <ref> [69] </ref>. The superior performance of smog in problems like mux11 is partly due to its ability to select the right ordering. For this problem, the value of the output is controlled by two sets of bits in the input: 3 control bits and 8 data bits. <p> The problems dnf1, dnf2, dnf3, dnf4, par4 16, par5 32, mux6 and mux11 were proposed in <ref> [69] </ref>.
Reference: [70] <author> Giulia Pagallo. </author> <title> Adaptive Decision Tree Algorithms for Learning From Examples. </title> <type> PhD thesis, </type> <institution> UC Santa Cruz, </institution> <year> 1990. </year>
Reference-contexts: From machine learning, I use many of the techniques developed for the induction of decision trees described in [76] and [79]. Also used are the constructive induction algorithms proposed by Pagallo and Haussler [69] and further developed in <ref> [70] </ref> and [101]. From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions [18, 13] and the variable reordering algorithms studied in [26] and [83] are critically important. <p> The algorithm used in fulfringe [66], identifies patterns near the fringes of the decision tree and uses them to build new attributes. The idea was first proposed in [69] and further developed in <ref> [70] </ref>. Another algorithm of this family, dcfringe [101] identifies the patterns shown in the first two rows of figure 5.5. These patterns correspond to 8 Boolean functions of 2 variables.
Reference: [71] <author> M. Paull and S. Unger. </author> <title> Minimizing the number of states in incompletely specified state machines. </title> <journal> IRE Transactions on Electronic Computers, </journal> <month> September </month> <year> 1959. </year>
Reference-contexts: However, since this problem is of great practical importance, many different algorithms have been proposed for its solution. Paull and Unger <ref> [71] </ref> were the first to propose a method based on the selection of compatibility classes, or compatibles. A compatible is a set of states that are equivalent in the sense that they can be merged without affecting the behavior of the machine.
Reference: [72] <author> C. F. Pfleeger. </author> <title> State reduction in incompletely specified finite state machines. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-22:1099-1102, </volume> <year> 1973. </year>
Reference-contexts: approach is to view the problem of selecting the minimum automaton consistent with a set of strings as equivalent to the problem of reducing an incompletely specified finite state machine. 1 This problem is more general than the one addressed here and was also proved to be NP-complete by Pfleeger <ref> [72] </ref>. However, since this problem is of great practical importance, many different algorithms have been proposed for its solution. Paull and Unger [71] were the first to propose a method based on the selection of compatibility classes, or compatibles.
Reference: [73] <author> Vera Pless. </author> <title> Introduction to the theory of error-correcting codes. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: INTRODUCTION one and a class value can not be attributed to that instance, but will not result in a mis-classification. * Error correcting code: Encode the label value using an error correcting code <ref> [23, 73] </ref>. This approach preserves most of the compactness of a binary encoding while being much less sensitive to errors in one of the single class problems. Additionally, the Hamming distance between the observed outputs and the closest valid codeword gives a measure of the certainty of the classification.
Reference: [74] <author> Jordan B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 123-148, </pages> <year> 1991. </year>
Reference-contexts: These algorithms are described in some detail in section 6.4. Recently, connectionist approaches have been proposed that address the problem of learning from a given set of strings, but had limited success. Das and Mozer [22], Giles et al. [29] and Polack <ref> [74] </ref> propose different approaches based on gradient descent algorithms for neural network training, but their results show that this strategy does not have any important advantages over search-based methods like the ones proposed by Bierman.
Reference: [75] <author> S. Porat and J. A. Feldman. </author> <title> Learning automata from ordered examples. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 386-396, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: FINITE STATE MACHINES recognizes L such that S L and T fl L is NP-complete. If all strings of length n or less are given (a uniform-complete sample), then the problem can be solved in a time polynomial on the input size <ref> [100, 75, 37] </ref>. Note, however, that the size of the input is in itself exponential on the number of states in the resulting DFA. Angluin has shown that even if an arbitrarily small fixed fraction (j (n) j) * , * &gt; 0 is missing, the problem remains NP-complete [2].
Reference: [76] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: This limitations make it both easier to implement this type of networks in hardware (although not as easy as the other three representations) and to design minimal size networks. A more detailed comparison between the two approaches is presented in chapter 3. 1.4.2 Decision Trees Decision trees <ref> [17, 76, 79] </ref>, on the other hand, do not suffer from these drawbacks. Little or no extra information is required apart from the data present in the training set and the resulting tree can be easily implemented using standard digital technology. <p> Decision graphs can be viewed as a generalization of decision trees, a very successful approach for the inference of classification rules <ref> [17, 76, 79] </ref>. The selection of decision graphs instead of decision trees as the hypothesis representation scheme is important because, even though decision trees can represent any concept, they are not concise representations for some concepts of interest. <p> This work draws heavily on techniques developed by other authors in the machine learning and logic synthesis fields. From machine learning, I use many of the techniques developed for the induction of decision trees described in <ref> [76] </ref> and [79]. Also used are the constructive induction algorithms proposed by Pagallo and Haussler [69] and further developed in [70] and [101]. <p> Since all the attributes are Boolean and we are not concerned with algorithms for pruning 4 the tree, a relatively straightforward algorithm can be used to generate the decision tree. The algorithm used is the same as the one proposed in <ref> [76] </ref> and uses the concepts of entropy and mutual information as defined in section 4.3.2. At each point, the decision tree algorithm has to select one variable to be tested at a given node.
Reference: [77] <author> J. R. Quinlan. </author> <title> An empirical comparison of genetic and decision-tree classifiers. </title> <booktitle> In Fifth International Conference on Machine Learning, </booktitle> <pages> pages 135-141, </pages> <year> 1988. </year>
Reference-contexts: For this problem, the value of the output is controlled by two sets of bits in the input: 3 control bits and 8 data bits. The 3 control bits select which one of the data bits defines the output value. Other authors have noted <ref> [77] </ref> that this problem 116 CHAPTER 7. EXPERIMENTAL EVALUATION AND APPLICATIONS is very hard for decision tree algorithms because the mutual information heuristic tends to select the data bits first, while the minimal decision tree (or graph) should test the control bits first.
Reference: [78] <author> J. R. Quinlan. </author> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: In the next section, the quality of the inference performed by smog and 112 CHAPTER 7. EXPERIMENTAL EVALUATION AND APPLICATIONS muesli is compared in a detailed way with the quality of the inference performed by C4.5 <ref> [78] </ref>, the most widely used algorithm for the inference of decision trees. 7.1.1 Experimental Setup The preferred approach to perform the performance comparison between different algorithms is to use multiple runs with fixed training set sizes.
Reference: [79] <author> J. R. Quinlan and R. L. Rivest. </author> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <journal> Inform. Comput., </journal> <volume> 80(3) </volume> <pages> 227-248, </pages> <month> March </month> <year> 1989. </year> <note> (An early version appeared as MIT LCS Technical report MIT/LCS/TM-339 (September 1987).). </note>
Reference-contexts: This limitations make it both easier to implement this type of networks in hardware (although not as easy as the other three representations) and to design minimal size networks. A more detailed comparison between the two approaches is presented in chapter 3. 1.4.2 Decision Trees Decision trees <ref> [17, 76, 79] </ref>, on the other hand, do not suffer from these drawbacks. Little or no extra information is required apart from the data present in the training set and the resulting tree can be easily implemented using standard digital technology. <p> However, even though different representations will use different encoding schemes, most of them share the need to describe a graph. The encoding proposed 28 CHAPTER 2. INDUCTIVE BIASES AND COMPLEXITY here for graphs is inspired on the encoding techniques proposed by Quinlan and Rivest <ref> [79] </ref> to encode decision trees. The simple version presented here assumes that each node has a known number of outgoing edges. A variation that can handle the more general case will be presented in the chapters that require it. Let G be a directed graph with one distinguished node. <p> The exceptions will be encoded using strings of 0's and 1's. The encoding of this type of strings follows closely the encoding used by Quinlan and Rivest <ref> [79] </ref> where the 1's 2.8. COMPUTATIONAL COMPLEXITY OF HYPOTHESIS SELECTION 29 will indicate the location of the exceptions. In general, these strings will have many more 0's than 1's. Assume that the strings are of length m, known and that there are k m 1's in the strings. <p> Decision graphs can be viewed as a generalization of decision trees, a very successful approach for the inference of classification rules <ref> [17, 76, 79] </ref>. The selection of decision graphs instead of decision trees as the hypothesis representation scheme is important because, even though decision trees can represent any concept, they are not concise representations for some concepts of interest. <p> This work draws heavily on techniques developed by other authors in the machine learning and logic synthesis fields. From machine learning, I use many of the techniques developed for the induction of decision trees described in [76] and <ref> [79] </ref>. Also used are the constructive induction algorithms proposed by Pagallo and Haussler [69] and further developed in [70] and [101].
Reference: [80] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year> <note> BIBLIOGRAPHY 153 </note>
Reference-contexts: The Minimum Description Length Principle can be viewed as a way to choose between alternative hypotheses using an approximation to the Solomonoff-Levin distribution. 2.7 The Minimum Description Length Principle Rissanen's Minimum Description Length (MDL) Principle <ref> [80, 81] </ref> can be viewed as a way to select hypotheses in a way that approximates the results obtained using Bayes law and the Solomonoff-Levin distribution.
Reference: [81] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> Ann. of Statist., </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: The Minimum Description Length Principle can be viewed as a way to choose between alternative hypotheses using an approximation to the Solomonoff-Levin distribution. 2.7 The Minimum Description Length Principle Rissanen's Minimum Description Length (MDL) Principle <ref> [80, 81] </ref> can be viewed as a way to select hypotheses in a way that approximates the results obtained using Bayes law and the Solomonoff-Levin distribution.
Reference: [82] <author> R. Rudell and A. Sangiovanni-Vincentelli. </author> <title> Multiple-valued minimization for PLA optimization. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> September </month> <year> 1987. </year>
Reference-contexts: The ReplacePair procedure is more expensive and requires O (s 3 m) operations. By using bit packing techniques <ref> [82] </ref>, the algorithm can be applied to reduce large RODGs in reasonable times. For large problems, the decision graph obtained from the initialization phase may be too large. In this case, the local optimization algorithm may take a large amount of time to reduce this graph.
Reference: [83] <author> Richard Rudell. </author> <title> Dynamic variable ordering for ordered binary decision diagrams. </title> <booktitle> In ICCAD, </booktitle> <pages> pages 42-47. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions [18, 13] and the variable reordering algorithms studied in [26] and <ref> [83] </ref> are critically important. For the benefit of readers not familiar with the use of RODGs as a tool for 5.2. <p> However, many heuristic algorithms have been proposed for this problem <ref> [26, 83] </ref>. In this setting, the problem is even more complex because the objective is to select an ordering that minimizes the final RODG and this ordering may not be the same as the one that minimizes the RODG obtained after the initialization step. The solution found is 5.4. <p> such that j (n k t) ^ wj is minimal Modify RODG such that incoming edges into n i and n j point to n k if Modified RODG has smaller cost function return (Modified RODG) else Undo changes return (Failure) to use the sift algorithm for dynamic RODG ordering <ref> [83] </ref> after each local modification is performed. The sift algorithm is based on the fact that swapping the order of two variables in the RODG ordering can be done very efficiently [13, 44] because only the nodes in these two levels are affected.
Reference: [84] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year> <note> Reprinted in [1]. </note>
Reference-contexts: The partial derivatives of expression (3.3) with respect to each one of the weights in the network can be easily obtained and gradient descent techniques can be used to obtain a local minimum of the error in weight space. Many algorithms based on this technique have been proposed <ref> [8, 41, 52, 84] </ref> and used with varying degrees of success in a variety of problems. These algorithms, however, do not derive the architecture of the network, but only the connection weights.
Reference: [85] <author> K. Hamaguchi S. Tani and S. Yajima. </author> <title> The complexity of the optimal variable ordering problems of shared binary decision diagrams. </title> <booktitle> In Algorithms and Computation, 4th Internation Symposium, </booktitle> <pages> pages 389-98, </pages> <year> 1993. </year>
Reference-contexts: pointing to n i Select n k such that j (n k t) ^ w j ^ v j j is minimal Modify RODG such that n then j = n k if Modified RODG has smaller description length return (Modified RODG) else Undo changes return (Failure) function is NP-complete <ref> [85] </ref> and cannot be solved exactly in many cases. However, many heuristic algorithms have been proposed for this problem [26, 83].
Reference: [86] <author> Eduard Sackinger, Bernard Boser, and Lawrence D. Jackel. </author> <title> A neurocomputer board based on the anna neural network chip. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 773-780. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Applications that require high speed and/or compact hardware implementations can benefit from an approach based on Boolean networks because the speed and compactness of digital implementations is still unmatched by its analog counterparts <ref> [12, 86] </ref>. Additionally, many alternatives are available to designers that want to implement Boolean networks, from full-custom design to field programmable gate arrays. This makes the digital alternative more cost effective than solutions based on analog designs.
Reference: [87] <author> Hamid Savoj. </author> <title> Don't Cares in Multi-Level Network Optimization. </title> <type> PhD thesis, </type> <institution> UC Berkeley, </institution> <year> 1992. </year>
Reference-contexts: Techniques that allow multi-level logic synthesis systems to make a better use of the don't care set were studied and implemented in systems like SIS <ref> [7, 87] </ref> but they require too much computation time for this application and are not always effective. The results and computation times obtained using these algorithms are shown in section 4.5 and compared with the ones obtained using the greedy approach described in this chapter.
Reference: [88] <author> Cullen Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference in Machine Learning, </booktitle> <address> San Mateo, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Schaffer <ref> [88] </ref> presented a very general argument for this equivalence in the form of a conservation principle. He restricts the analysis to single concept learning problems in discrete domains. Since the domain is discrete, there will be a finite number of possible attribute combinations, or objects in D. <p> INDUCTIVE BIASES AND COMPLEXITY The fact that this sum equals 0 means that it is impossible for a learning algorithm to improve its performance in a particular set of concepts without showing decreased performance in another set. Some interesting examples of possible and impossible learners are given in <ref> [88] </ref>. However, not all possible concepts are equally likely to appear in the real world. In fact, if there were no regularities, learning would be fundamentally impossible because, no matter how large the training sets, there would be no basis to infer any particular rule.
Reference: [89] <author> R. E. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: The problem becomes easier if the algorithm is allowed to make queries or experiment with the unknown machine. Angluin [3] proposes an algorithm based on the approach described by Gold [31] that solves the problem in polynomial time by allowing the algorithm to ask membership queries. Schapire <ref> [89] </ref> proposes an interesting approach that does not require the availability of a reset signal to take the machine to a known state. All these algorithms address simpler versions of the problem discussed here.
Reference: [90] <author> Alen D. Shapiro. </author> <title> Structured Induction in Expert Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: The problems tictactoe, vote, mushroom, breast and splice are from the UCI database [62] and are described in detail in the online documentation publicly available. The problems krkp and kkp result from the encoding of chess positions. The first one is described in <ref> [90] </ref> and is the encoding of a King+Rook vs. King+Pawn chess ending using high level attributes. The kkp problem is obtained from the encoding of the chess endings described in section 1.1.1. 144 APPENDIX B.
Reference: [91] <author> Thomas R. Shiple, Ramin Hojati, Robert K. Brayton, and Alberto L. Sangiovanni-Vincentelli. </author> <title> Exact minimization of BDDs using don't cares. Private communication, 1993. 154 BIBLIOGRAPHY </title>
Reference-contexts: Although the domain of applicability of the exact solver is limited, this approach is interesting because it is the first formulation of this optimization problem that does not require an explicit search of the solution space as previous approaches do <ref> [91] </ref>. Section 5.4 describes an heuristic approach that obtains an RODG that minimizes (2.15) and is much faster than the exact one for large examples. Furthermore, it derives a good ordering of the variables together with the RODG structure. <p> This approach is inspired in the standard algorithms used for the minimization of incompletely specified state machines and uses some of the concepts developed for that purpose. 5.3.1 The Compatibility Graph Previous algorithms <ref> [91] </ref> for this problem used directly the RODG representation of f on and f off . The exact approach described in this paper works with the 3TRODG F that corresponds to f . F is assumed to be complete. <p> These results seem to imply that the applicability of the exact minimization algorithm is limited to problems of no more than 8 variables. It is an open question whether or not this represents a clear advantage over the exact approach proposed in <ref> [91] </ref>, although the algorithms presented here seem to be less limited by the number of points in the don't care set.
Reference: [92] <author> Thomas R. Shiple, Ramin Hojati, Alberto L. Sangiovanni-Vincentelli, and Robert K. Brayton. </author> <title> Heuristic minimization of BDDs using don't cares. </title> <booktitle> In Proc. Design Automat. Conf., </booktitle> <pages> pages 225-231, </pages> <address> San Diego, CA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Other heuristic algorithms that can be used for the selection of compact RODGs compatible with the training set have been proposed before in the logic synthesis literature <ref> [92] </ref> but the quality of the results obtained makes them ineffective for the type of problems found in machine learning. They are, however, useful as a generator of the initial RODG, as described in section 5.4.1. <p> This RODG operator can be used to obtain a more compact RODG representation for a function defined by its on and off sets. The restrict operator belongs to a family of heuristics <ref> [92] </ref> that generate a small RODG by merging, in a bottom up fashion, nodes in an RODG. The merging of nodes is performed in a way that keeps the function that corresponds to the generated RODG contained in the union of f dc and f on .
Reference: [93] <author> K. Y. Siu and J. Bruck. </author> <title> Neural computation of arithmetic functions. </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> 10 </volume> <pages> 1669-1675, </pages> <year> 1990. </year>
Reference-contexts: For example, it has been shown that functions like multiple-addition, multiplication, division and sorting can be implemented by polynomial-size threshold circuits of small constant depth <ref> [93, 94] </ref>. In particular, compact two-level threshold gate networks can represent many interesting concepts that require exponentially large two-level Boolean networks. Extensive research in the field of neural networks has also created interest in algorithms for the synthesis and optimization of threshold gate networks.
Reference: [94] <author> K. Y. Siu and J. Bruck. </author> <title> On the power of threshold circuits with small weights. </title> <journal> SIAM Journal of Discrete Math, </journal> <note> page to appear, </note> <year> 1994. </year>
Reference-contexts: For example, it has been shown that functions like multiple-addition, multiplication, division and sorting can be implemented by polynomial-size threshold circuits of small constant depth <ref> [93, 94] </ref>. In particular, compact two-level threshold gate networks can represent many interesting concepts that require exponentially large two-level Boolean networks. Extensive research in the field of neural networks has also created interest in algorithms for the synthesis and optimization of threshold gate networks.
Reference: [95] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference, part 1 and part 2. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <year> 1964. </year>
Reference-contexts: Now, if M is chosen to be a universal Turing machine the Kolmogorov complexity with respect to M is called simply the Kolmogorov complexity and is defined as K (s). Kolmogorov [51], Solomonoff <ref> [95] </ref> and Chaitin [20] proved that, for any Turing machine M K (s) K M (s) + k for all finite strings s (2.9) where k is a constant that depends only on M .
Reference: [96] <author> R. Spickelmier, </author> <title> editor. Oct Tools Distribution 2.1. </title> <institution> University of California, Berkeley, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: The Boolean network obtained by muesli was optimized using standard logic synthesis optimization techniques and mapped into a standard cell structure using the octtools system <ref> [96] </ref>, a package for the logic and physical design of VLSI circuits developed at Berke-ley. The logic optimizations performed didn't change the logic functions implemented by the network because, in this step, the flexibility given by the use of don't care information was not used.
Reference: [97] <author> Yasuhiko Takenaga and Shuzo Yajima. </author> <title> NP-completeness of minimum binary decision diagram identification. </title> <type> Technical Report COMP 92-99, </type> <institution> Institute of Electronics, Information and Communication Engine ers (of Japan), </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Lemma 2 For a fixed ordering of the variables, it is NP-complete to determine if a reduced ordered decision graph that is consistent with a given training set and has less than k nodes exists. Proof: obtained by a reduction from graph K-colorability in <ref> [97] </ref>. Lemma 3 It is NP-complete to determine if a finite state machine that has less than k states and is consistent with a given training set 6 exists. Proof: made by a reduction from the satisfiability problem with 3 variable clauses (3-SAT) in [32]. <p> This chapter describes two different approaches for the problem of selecting the RODG of minimal description length. Since the problem is NP-complete <ref> [97] </ref> both an exact and an heuristic approach are described for this problem. The exact approach described in section 5.3 works only in the noise free case and selects the RODG that minimizes (2.14) under a fixed ordering of the variables.
Reference: [98] <author> S. B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. de Jong, S. Dze-roski, S. E. Fahlman, D. Fisher, R. Hamann, K. Kaufaman, S. Keller, I. Kononenko, J. Kreusiger, R. S. Michalski, T. Mitchell, P. Pachowitz, Y. Reich, H. Vafaic, W. Van de Weldel, W. WEnzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems: a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: = x 1 x 2 x 3 + x 4 x 5 x 6 + x 7 x 8 x 9 + x 10 x 11 x 12 + x 13 x 14 x 15 + x 16 x 17 x 18 Problems monk1, monk2 and monk3 were proposed in <ref> [98] </ref> and are the encoding of concepts in a hypothetic robot world. The problems tictactoe, vote, mushroom, breast and splice are from the UCI database [62] and are described in detail in the online documentation publicly available. The problems krkp and kkp result from the encoding of chess positions. <p> kdd5, kdd6, kdd7, kdd8, kdd9, kdd10 * Multiplexer functions : mux6. * Deep functions : and or chain8 = ((((x 1 x 2 + x 3 )x 4 + x 5 )x 6 + x 7 )x 8 ) * Monkish Problems: 8 binary variable "approximations" to the Monk's problems <ref> [98] </ref> * String functions. Palindrome acceptor and variants : pal, pal output and doubley. * Interval acceptors.
Reference: [99] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proc. Fourth Annual Cognitive Science Conference, </booktitle> <pages> page 105, </pages> <year> 1982. </year>
Reference-contexts: Not only they are not guaranteed to find the exact solution but they are also very limited in the size of problems they can handle. For example, they are not even able to solve some of the Tomita grammars <ref> [99] </ref>, none of which requires more than 5-state DFAs. It must be pointed out, however, that the main purpose of the connectionist work was not to beat discrete search algorithms, but to evaluate the applicability of such an approach to problems of this type. <p> Figure 6.2 shows an example of this alternative way to specify a training set. Both forms of training sets description are equivalent and 2 This particular training set for this concept was proposed by Tomita <ref> [99] </ref>. 94 CHAPTER 6.
Reference: [100] <author> B. A. Trakhtenbrot and Y. M. Barzdin. </author> <title> Finite Automata. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1973. </year>
Reference-contexts: FINITE STATE MACHINES recognizes L such that S L and T fl L is NP-complete. If all strings of length n or less are given (a uniform-complete sample), then the problem can be solved in a time polynomial on the input size <ref> [100, 75, 37] </ref>. Note, however, that the size of the input is in itself exponential on the number of states in the resulting DFA. Angluin has shown that even if an arbitrarily small fixed fraction (j (n) j) * , * &gt; 0 is missing, the problem remains NP-complete [2].
Reference: [101] <author> D. S. Yang, L. Rendell, and G. Blix. </author> <title> Fringe-like feature construction: A comparative study and a unifying scheme. </title> <booktitle> In Proceedings of the Eight International Conference in Machine Learning, </booktitle> <pages> pages 223-227, </pages> <address> San Mateo, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: From machine learning, I use many of the techniques developed for the induction of decision trees described in [76] and [79]. Also used are the constructive induction algorithms proposed by Pagallo and Haussler [69] and further developed in [70] and <ref> [101] </ref>. From the logic synthesis field, the use of the vast array of techniques developed for the manipulation of RODGs as canonical representations for Boolean functions [18, 13] and the variable reordering algorithms studied in [26] and [83] are critically important. <p> The algorithm used in fulfringe [66], identifies patterns near the fringes of the decision tree and uses them to build new attributes. The idea was first proposed in [69] and further developed in [70]. Another algorithm of this family, dcfringe <ref> [101] </ref> identifies the patterns shown in the first two rows of figure 5.5. These patterns correspond to 8 Boolean functions of 2 variables.
References-found: 101

