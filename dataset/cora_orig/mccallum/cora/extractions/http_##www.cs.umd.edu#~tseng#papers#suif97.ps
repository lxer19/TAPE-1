URL: http://www.cs.umd.edu/~tseng/papers/suif97.ps
Refering-URL: http://www.cs.umd.edu/projects/cosmic/papers.html
Root-URL: 
Title: Compiler Optimizations for High Performance Architectures  
Author: Hwansoo Han, Gabriel Rivera, Chau-Wen Tseng 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: We describe two ongoing compiler projects for high performance architectures at the University of Maryland being developed using the Stanford SUIF compiler infrastructure. First, we are investigating the impact of compilation techniques for eliminating synchronization overhead in compiler-parallelized programs running on software distributed-shared-memory (DSM) systems. Second, we are evaluating data layout transformations to improve cache performance on uniprocessors by eliminating conflict misses through inter- and intra-variable padding. Our optimizations have been implemented in SUIF and tested on a number of programs. Preliminary results are encouraging. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformation for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs [14]. Cierniak and Li examined combining array transpose and loop transformations to improve locality for parallel programs [5]. Amarasinghe et al. demonstrated the utility of data layout transformations for parallel applications <ref> [1] </ref>. They found it to be significant in eliminating adverse cache effects, though specialized optimizations were necessary to reduce computation overhead for modified array subscripts. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations [8, 9, 26].
Reference: [2] <author> W. Bolosky, R. Fitzgerald, and M. Scott. </author> <title> Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: We evaluate two compiler transformations to eliminate conflict misses: 1) inter-variable padding (modify variable base address), 2) intra-variable padding (increase array dimension size) <ref> [2, 24] </ref>. Unlike compiler transformations that restructure the computation performed by the program, these two techniques modify the program's data layout. Two motivating examples of these transformations are shown in Figures 7 and 8.
Reference: [3] <author> S. Carr, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [18]. Most researchers have concentrated on computation-reordering transformations. Loop permutation and tiling are the primary optimization techniques <ref> [3, 9, 26] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [3]. Researchers have previously examined changing data layout in parallel applications, but have generally not studied its effect on sequential applications. Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs [14]. <p> Most researchers have concentrated on computation-reordering transformations. Loop permutation and tiling are the primary optimization techniques [3, 9, 26], though loop fission (distribution) and loop fusion have also been found to be helpful <ref> [3] </ref>. Researchers have previously examined changing data layout in parallel applications, but have generally not studied its effect on sequential applications. Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs [14]. Cierniak and Li examined combining array transpose and loop transformations to improve locality for parallel programs [5].
Reference: [4] <author> S. Chandra and J.R. Larus. </author> <title> Optimizing communication in HPF programs for fine-grain distributed shared memory. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Ve-gas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Recently researchers have investigated optimizations when compiling for software DSMs. Dwarkadas et al. applied compiler analysis to explicitly parallel programs to improve their performance on a software DSM [7]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system <ref> [4] </ref>. Cox et al. conducted an experimental study to evaluate the performance of TreadMarks as a target for the Forge SPF shared-memory compiler from APR [6]. They identify opportunities to eliminate unneeded barrier synchronization; many of their suggestions are implemented in the SUIF/CVM system.
Reference: [5] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Researchers have previously examined changing data layout in parallel applications, but have generally not studied its effect on sequential applications. Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs [14]. Cierniak and Li examined combining array transpose and loop transformations to improve locality for parallel programs <ref> [5] </ref>. Amarasinghe et al. demonstrated the utility of data layout transformations for parallel applications [1]. They found it to be significant in eliminating adverse cache effects, though specialized optimizations were necessary to reduce computation overhead for modified array subscripts.
Reference: [6] <author> A. Cox, S. Dwarkadas, H. Lu, and W. Zwaenepoel. </author> <title> Evaluating the performance of software distributed shared memory as a target for parallelizing compilers. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Recent studies indicate such systems can approach the performance of current message-passing HPF compilers and explicit message-passing MPI programs <ref> [6, 17] </ref>. However, load imbalance and synchronization overhead were identified as key sources of inefficiency. Execution times for five SUIF-parallelized applications on a 16 processor IBM SP-2 are shown in Figure 1. <p> Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [4]. Cox et al. conducted an experimental study to evaluate the performance of TreadMarks as a target for the Forge SPF shared-memory compiler from APR <ref> [6] </ref>. They identify opportunities to eliminate unneeded barrier synchronization; many of their suggestions are implemented in the SUIF/CVM system. Rajamony and Cox developed a performance debugger for detecting unnecessary synchronization at run-time by instrumenting all loads and stores [21].
Reference: [7] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> Octo-ber </month> <year> 1996. </year>
Reference-contexts: Stoher and O'Boyle [23] extend this work present an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. Recently researchers have investigated optimizations when compiling for software DSMs. Dwarkadas et al. applied compiler analysis to explicitly parallel programs to improve their performance on a software DSM <ref> [7] </ref>. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [4]. Cox et al. conducted an experimental study to evaluate the performance of TreadMarks as a target for the Forge SPF shared-memory compiler from APR [6].
Reference: [8] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: They found it to be significant in eliminating adverse cache effects, though specialized optimizations were necessary to reduce computation overhead for modified array subscripts. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [8, 9, 26] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache.
Reference: [9] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: First, we find uniformly generated references described by Gannon et al. <ref> [9] </ref>. We then compute the conflict distance, the difference between the memory addresses of two array references modulo the cache size C S . If on each loop iteration the conflict distance is less than the cache line size L S , severe conflict misses can result. <p> We find that such pairs are commonly uniformly generated references <ref> [9] </ref> extended for conforming arrays, arrays that have equal dimension sizes in all but their highest dimension and possess equal-sized elements. <p> Wolf and Lam provide a concise definition and summary of important types of data locality [26]. Gannon et al. introduce the notion of uniformly generated references as a means of discovering group reuse between references to the same array <ref> [9] </ref>. McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [18]. Most researchers have concentrated on computation-reordering transformations. <p> McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [18]. Most researchers have concentrated on computation-reordering transformations. Loop permutation and tiling are the primary optimization techniques <ref> [3, 9, 26] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [3]. Researchers have previously examined changing data layout in parallel applications, but have generally not studied its effect on sequential applications. Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs [14]. <p> They found it to be significant in eliminating adverse cache effects, though specialized optimizations were necessary to reduce computation overhead for modified array subscripts. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [8, 9, 26] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache.
Reference: [10] <author> S. Ghosh, M. Martonosi, and S. Malik. </author> <title> Cache miss equations: An analytical representation of cache misses. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Observe that making the conflict distance L S or greater is a sufficient (though not necessary) condition for eliminating severe conflicts; we use this to guide our data layout optimizations. Our techniques for computing conflict distances is a special simplified case of cache miss equations <ref> [10] </ref>. 3.3.1 Inter-variable Padding Heuristic Central to our technique for eliminating severe conflict misses is considering pairs of arrays references with constant conflict distances on each loop iteration. <p> These models typically can predict only capacity misses because they assume a fully-associative cache. In comparison, Ghosh et al. can determine conflict misses by calculating cache miss equations, linear Diophantine equations that summarize each loop's memory behavior by calculating exactly the cache slot to which each reference is mapped <ref> [10] </ref>. They demonstrate how to use cache miss equations to select array paddings to eliminate conflict misses, and block sizes for tiling. In comparison, we focus only on severe conflicts, using a simplified version of cache miss equations to calculate the conflict distance between pairs of uniformly generated array references.
Reference: [11] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: are currently pursuing research in two areas: compiling for software distributed-shared-memory (DSM) systems and data layout optimizations for high performance architectures. 2 Reducing Synchronization Overhead We begin by looking at compiler optimizations for reducing synchronization overhead, These optimizations are examined in the context of a prototype system [17] extending SUIF <ref> [11] </ref> to target the CVM [15] software distributed-shared-memory (DSM). Recent studies indicate such systems can approach the performance of current message-passing HPF compilers and explicit message-passing MPI programs [6, 17]. However, load imbalance and synchronization overhead were identified as key sources of inefficiency. <p> However, virtual memory primitive costs in the current system are location-dependent, occasionally increasing these costs to a millisecond or more. In our experiments, CVM [15] applications written in Fortran 77 were automatically parallelized by the Stanford SUIF par-allelizing compiler version 1.1.2 <ref> [11] </ref>, with close to 100% of the computation in parallel regions. A simple chunk scheduling policy assigns contiguous iterations of equal or near-equal size to each processor, resulting in a consistent computation partition that encourages good locality.
Reference: [12] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Our results show not all programs were greatly improved, but the impact is significant enough that data layout optimizations should be considered an important tool for optimizing compilers. 4 Related Work 4.1 Synchronization Optimizations Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [12, 20] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. Eliminating barriers in compiler-parallelized codes is more difficult.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Our implementation of nearest-neighbor synchronization solves this problem by invoking a global barrier the first time it is invoked at each location in the program. Since anti-dependences may be ignored, the algorithm for inserting barrier synchronization becomes similar to the algorithm for message vectorization <ref> [13] </ref>. The level of the deepest true/flow cross-processor dependence becomes the point where synchronization must be inserted to prevent data races. Synchronization at lower loop levels is not needed. 2.2.3 Customized Nearest-Neighbor Synchronization At some barriers, the compiler can detect communication only takes place between neighboring processors [25]. <p> Eliminating barriers in compiler-parallelized codes is more difficult. In previous work [25] we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication <ref> [13] </ref>. O'Boyle and Bodin [19] present techniques similar to local subscript analysis to to identify data dependences that cross processor boundaries. Stoher and O'Boyle [23] extend this work present an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. Recently researchers have investigated optimizations when compiling for software DSMs.
Reference: [14] <author> T. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Researchers have previously examined changing data layout in parallel applications, but have generally not studied its effect on sequential applications. Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs <ref> [14] </ref>. Cierniak and Li examined combining array transpose and loop transformations to improve locality for parallel programs [5]. Amarasinghe et al. demonstrated the utility of data layout transformations for parallel applications [1].
Reference: [15] <author> P. Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <booktitle> In 16th International Conference on Distributed Computing Systems, </booktitle> <address> Hong Kong, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: two areas: compiling for software distributed-shared-memory (DSM) systems and data layout optimizations for high performance architectures. 2 Reducing Synchronization Overhead We begin by looking at compiler optimizations for reducing synchronization overhead, These optimizations are examined in the context of a prototype system [17] extending SUIF [11] to target the CVM <ref> [15] </ref> software distributed-shared-memory (DSM). Recent studies indicate such systems can approach the performance of current message-passing HPF compilers and explicit message-passing MPI programs [6, 17]. However, load imbalance and synchronization overhead were identified as key sources of inefficiency. <p> It is clear from these measurements that reducing sequential wait and load imbalance caused by synchronization overhead is important for achieving good performance. 2.1 Background 2.1.1 CVM Software DSM CVM is a software DSM that supports coherent shared memory for multiple protocols and consistency models <ref> [15] </ref>. It is written entirely as a user-level library and runs on most UNIX-like systems. Its primary coherence protocol implements a multiple-writer version of lazy release consistency [16], a derivation of release consistency. <p> In the best case, AIX requires 128 secs to call user-level handlers for page faults, and mprotect system calls require 12 secs. However, virtual memory primitive costs in the current system are location-dependent, occasionally increasing these costs to a millisecond or more. In our experiments, CVM <ref> [15] </ref> applications written in Fortran 77 were automatically parallelized by the Stanford SUIF par-allelizing compiler version 1.1.2 [11], with close to 100% of the computation in parallel regions.
Reference: [16] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It is written entirely as a user-level library and runs on most UNIX-like systems. Its primary coherence protocol implements a multiple-writer version of lazy release consistency <ref> [16] </ref>, a derivation of release consistency. Release consistency allows a processor to delay making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur.
Reference: [17] <author> P. Keleher and C.-W. Tseng. </author> <title> Enhancing software DSM for compiler-parallelized applications. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: We are currently pursuing research in two areas: compiling for software distributed-shared-memory (DSM) systems and data layout optimizations for high performance architectures. 2 Reducing Synchronization Overhead We begin by looking at compiler optimizations for reducing synchronization overhead, These optimizations are examined in the context of a prototype system <ref> [17] </ref> extending SUIF [11] to target the CVM [15] software distributed-shared-memory (DSM). Recent studies indicate such systems can approach the performance of current message-passing HPF compilers and explicit message-passing MPI programs [6, 17]. However, load imbalance and synchronization overhead were identified as key sources of inefficiency. <p> Recent studies indicate such systems can approach the performance of current message-passing HPF compilers and explicit message-passing MPI programs <ref> [6, 17] </ref>. However, load imbalance and synchronization overhead were identified as key sources of inefficiency. Execution times for five SUIF-parallelized applications on a 16 processor IBM SP-2 are shown in Figure 1. <p> Performance was improved by adding customized support for reductions, as well as a flush update protocol that at barriers automatically sends updates to processors possessing copies of recently modified shared data <ref> [17] </ref>. Analysis needed for the flush update protocol is simpler than communication analysis needed in HPF compilers; the compiler locates data likely to be communicated in a stable pattern, then insert calls to DSM routines f Default Model g DOALL I = 1,N ... ENDDO DOALL J = 1,N ... <p> The resulting C output code was compiled by g++ version 2.7.2 with the -O2 flag, then linked with the SUIF run-time system and the CVM libraries to produce executable code on the IBM SP-2. Customized support for reductions and the flush update protocol were used to improve overall performance <ref> [17] </ref>. 2.3.3 Effectiveness of Compiler Synchronization Optimizations First, we examine the effectiveness of compiler algorithms in eliminating synchronization. Table 2 displays the number of barriers found in each program at compile time, and the percentage eliminated by different levels of optimization.
Reference: [18] <author> K. S. McKinley and O. Temam. </author> <title> A quantitative analysis of loop nest locality. </title> <booktitle> In Proceedingsof the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Conflict misses have been found to be a significant source of poor performance in scientific programs, particularly within loop nests <ref> [18] </ref>. 3.1 Eliminating Conflict Misses Compiler transformations can be very effective in eliminating conflict misses for scientific programs with regular access patterns. We evaluate two compiler transformations to eliminate conflict misses: 1) inter-variable padding (modify variable base address), 2) intra-variable padding (increase array dimension size) [2, 24]. <p> McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses <ref> [18] </ref>. Most researchers have concentrated on computation-reordering transformations. Loop permutation and tiling are the primary optimization techniques [3, 9, 26], though loop fission (distribution) and loop fusion have also been found to be helpful [3].
Reference: [19] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Eliminating barriers in compiler-parallelized codes is more difficult. In previous work [25] we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication [13]. O'Boyle and Bodin <ref> [19] </ref> present techniques similar to local subscript analysis to to identify data dependences that cross processor boundaries. Stoher and O'Boyle [23] extend this work present an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. Recently researchers have investigated optimizations when compiling for software DSMs.
Reference: [20] <author> M. Philippsen and E. Heinz. </author> <title> Automatic synchronization elimination in synchronous FORALLs. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Our results show not all programs were greatly improved, but the impact is significant enough that data layout optimizations should be considered an important tool for optimizing compilers. 4 Related Work 4.1 Synchronization Optimizations Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [12, 20] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. Eliminating barriers in compiler-parallelized codes is more difficult.
Reference: [21] <author> R. Rajamony and A.L. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the Fourth International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems (MASCOTS), </institution> <month> Febru-ary </month> <year> 1996. </year>
Reference-contexts: They identify opportunities to eliminate unneeded barrier synchronization; many of their suggestions are implemented in the SUIF/CVM system. Rajamony and Cox developed a performance debugger for detecting unnecessary synchronization at run-time by instrumenting all loads and stores <ref> [21] </ref>. In the SPLASH application Water, it was able to detect barriers guarding only anti and output dependences that may be eliminated by applying odd-even renaming.
Reference: [22] <author> G. Rivera and C.-W. Tseng. </author> <title> Compiler optimizations for eliminated cache conflict misses. </title> <type> Technical Report CS-TR-3819, </type> <institution> Dept. of Computer Science, University of Maryland at College Park, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Assuming cache size C S and cache line size L S , we assign array base addresses to a maximum of C S locations, separated by D L cache lines. Based on empirical studies <ref> [22] </ref>, our implementation currently separates array base addresses by four cache lines (D L = 4). An alternative heuristic which separates arrays as far as possible in the cache was shown to be less effective in eliminating conflict misses [22]. 3.2.2 Intra-variable Padding Heuristic A possible intra-variable padding heuristic is to <p> Based on empirical studies <ref> [22] </ref>, our implementation currently separates array base addresses by four cache lines (D L = 4). An alternative heuristic which separates arrays as far as possible in the cache was shown to be less effective in eliminating conflict misses [22]. 3.2.2 Intra-variable Padding Heuristic A possible intra-variable padding heuristic is to pad the dimensions of each array so that the size of subarrays comprising lower dimensions are far enough from any cache size multiple. <p> In the simplest case, array columns would be padded when neighboring columns are mapped to the same location in cache. Empirical studies <ref> [22] </ref> indicate this heuristic misses many profitable opportunities for intra-variable padding. As a result, our implementation currently pads all array columns by four elements unconditionally when safe. <p> The advantage of analysis-based heuristics thus appears to be the ability to usually detecting profitable opportunities for padding, while avoiding padding decisions which degrade performance. Previous studies indicate padding transformations become more important as the ratio of application data to cache size increases <ref> [22] </ref>, so analysis-based heuristics may become more important for real-world programs.
Reference: [23] <author> E. Stohr and M. O'Boyle. </author> <title> A graph based approach to barrier synchronisation minimisation. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: In previous work [25] we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication [13]. O'Boyle and Bodin [19] present techniques similar to local subscript analysis to to identify data dependences that cross processor boundaries. Stoher and O'Boyle <ref> [23] </ref> extend this work present an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. Recently researchers have investigated optimizations when compiling for software DSMs. Dwarkadas et al. applied compiler analysis to explicitly parallel programs to improve their performance on a software DSM [7].
Reference: [24] <author> J. Torrellas, M. Lam, and J. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: We evaluate two compiler transformations to eliminate conflict misses: 1) inter-variable padding (modify variable base address), 2) intra-variable padding (increase array dimension size) <ref> [2, 24] </ref>. Unlike compiler transformations that restructure the computation performed by the program, these two techniques modify the program's data layout. Two motivating examples of these transformations are shown in Figures 7 and 8.
Reference: [25] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedingsof the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In previous work, we developed compiler algorithms for barrier elimination based on the observation that synchronization between a pair of processors is only necessary if they communicate shared data <ref> [25] </ref>. If data dependenceanalysis can show two adjacent loop nests access disjoint sets of data, then the barrier separating them may be eliminated, as in Figure 2 (B). <p> neighboring processors, the barrier may be replaced by nearest-neighbor synchronization, as shown in Figure 2 (D). 2.2 Novel Techniques 2.2.1 Communication Analysis Using Local Subscripts Previous communication analysis relied on compile-time information on the data and computation decomposition selected for a program to precisely determine whether interprocessor communication takes place <ref> [25] </ref>. We find that an alternative communication analysis technique based on local subscript analysis can also yield good results, but with less complex analysis. The Local subscript analysis algorithm is shown in Figure 3. <p> The level of the deepest true/flow cross-processor dependence becomes the point where synchronization must be inserted to prevent data races. Synchronization at lower loop levels is not needed. 2.2.3 Customized Nearest-Neighbor Synchronization At some barriers, the compiler can detect communication only takes place between neighboring processors <ref> [25] </ref>. To take advantage of this information, we implemented a customized routine for nearest-neighbor synchronization (where each processor has either zero, one, or two neighbors) directly in CVM. <p> Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. Eliminating barriers in compiler-parallelized codes is more difficult. In previous work <ref> [25] </ref> we presented techniques to eliminate or lessen synchronization based on communication analysis used by distributed-memory compilers to calculate explicit communication [13]. O'Boyle and Bodin [19] present techniques similar to local subscript analysis to to identify data dependences that cross processor boundaries.
Reference: [26] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In comparison, SUIF at compile time eliminates many barriers guarding only anti-dependences. 4.2 Data Locality Optimizations Data locality has been recognized as a significant performance issue for both scalar and parallel architectures. Wolf and Lam provide a concise definition and summary of important types of data locality <ref> [26] </ref>. Gannon et al. introduce the notion of uniformly generated references as a means of discovering group reuse between references to the same array [9]. <p> McKinley and Temam performed a study of loop-nest oriented cache behavior for scientific programs and concluded that conflict misses cause half of all cache misses and most intra-nest misses [18]. Most researchers have concentrated on computation-reordering transformations. Loop permutation and tiling are the primary optimization techniques <ref> [3, 9, 26] </ref>, though loop fission (distribution) and loop fusion have also been found to be helpful [3]. Researchers have previously examined changing data layout in parallel applications, but have generally not studied its effect on sequential applications. Jeremiassen and Eggers automatically eliminate false sharing in explicitly parallel programs [14]. <p> They found it to be significant in eliminating adverse cache effects, though specialized optimizations were necessary to reduce computation overhead for modified array subscripts. Many researchers have also examined the problem of deriving estimates of cache misses in order to help guide data locality optimizations <ref> [8, 9, 26] </ref>. These models typically can predict only capacity misses because they assume a fully-associative cache.
References-found: 26

