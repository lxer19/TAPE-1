URL: http://www.cs.rice.edu/CS/Systems/papers/envy-asplos94.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~mikewu/papers.html
Root-URL: 
Email: mikewu@rice.edu  willy@rice.edu  
Title: eNVy: A NonVolatile, Main Memory Storage System To appear in ASPLOS VI  
Author: Michael Wu Willy Zwaenepoel 
Note: This research was supported in part by the National Science Foundation under Grant CCR-9116343 and by the Texas Advanced Technology Program under Grant 003604012.  
Affiliation: Dept. of Electrical and Computer Engineering Rice University  Department of Computer Science Rice University  
Abstract: This paper describes the architecture of eNVy, a large non-volatile main memory storage system built primarily with Flash memory. eNVy presents its storage space as a linear, memory mapped array rather than as an emulated disk in order to provide an efficient and easy to use software interface. Flash memories provide persistent storage with solid-state memory access times at a lower cost than other solid-state technologies. However, they have a number of drawbacks. Flash chips are write-once, bulk-erase devices whose contents cannot be updated in-place. They also suffer from slow program times and a limit on the number of program/erase cycles. eNVy uses a copy-on-write scheme, page remapping, a small amount of battery backed SRAM, and high bandwidth parallel data transfers to provide low latency, in-place update semantics. A cleaning algorithm optimized for large Flash arrays is used to reclaim space. The algorithm is designed to evenly wear the array, thereby extending its lifetime. Software simulations of a 2 gigabyte eNVy system show that it can support I/O rates corresponding to approximately 30,000 transactions per second on the TPC-A database benchmark. Despite the added work done to overcome the deficiencies associated with Flash memories, average latencies to the storage system are as low as 180ns for reads and 200ns for writes. The estimated lifetime of this type of storage system is in the 10 year range when exposed to a workload of 10,000 transactions per second. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Baker, S. Asami, E. Deprit, J. Ousterhout, and M. Seltzer. </author> <title> Non-volatile memory for fast, reliable file system. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-22, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Methods such as caches, write buffers, and RAID arrays have been developed to alleviate disk bottlenecks. Caches are commonly used to hide the read latency of disks [15] while non-volatile write buffers have been developed to hide write latency <ref> [1, 3, 12] </ref>. RAID arrays improve available I/O rates by transferring data in parallel [7, 10, 11]. We are exploring an alternative approach, building a persistent storage system using solid-state memories, thereby avoiding the mechanical latencies associated with disks. <p> Others have studied the problem of how to minimize write costs for data reclamation in log structured file systems [13, 14]. The benefits possible from having fast access to stable storage were demonstrated by advocates of non-volatile write caches <ref> [1, 3, 12] </ref>. Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field [6, 8, 9].
Reference: [2] <author> R. Caceres, F. Douglis, K. Li, and B. Marsh. </author> <title> Operating system implications of solid-state mobile computers. </title> <type> Technical Report MITL-TR-56-93, </type> <institution> Matsushita Information Technology Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: This function has been studied in the context of log file systems [14] which use similar data movement primitives. 7 Related Work There has been other work done in areas relevant to eNVy. Researchers at MITL <ref> [2] </ref> argue that Flash has interesting benefits as a memory mapped storage medium, and suggest a similar copy-on-write scheme handled by software in the operating system rather than a hardware controller although with lower performance.
Reference: [3] <author> G. Copeland, T. Keller, R. Krishnamurthy, and M. Smith. </author> <title> The case for safe RAM. </title> <booktitle> In Proceedings of the 15th International Conference on Very Large Databases, </booktitle> <pages> pages 327-335, </pages> <year> 1989. </year>
Reference-contexts: Methods such as caches, write buffers, and RAID arrays have been developed to alleviate disk bottlenecks. Caches are commonly used to hide the read latency of disks [15] while non-volatile write buffers have been developed to hide write latency <ref> [1, 3, 12] </ref>. RAID arrays improve available I/O rates by transferring data in parallel [7, 10, 11]. We are exploring an alternative approach, building a persistent storage system using solid-state memories, thereby avoiding the mechanical latencies associated with disks. <p> Others have studied the problem of how to minimize write costs for data reclamation in log structured file systems [13, 14]. The benefits possible from having fast access to stable storage were demonstrated by advocates of non-volatile write caches <ref> [1, 3, 12] </ref>. Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field [6, 8, 9].
Reference: [4] <author> Intel Corporation. </author> <title> Flash memory, </title> <year> 1994. </year>
Reference-contexts: Simulation results are presented in Section 5. The remaining sections discuss extensions and conclusions. 2 Flash Characteristics A Flash chip is structurally and functionally very similar to an EPROM, except for the fact that it is electrically erasable <ref> [4] </ref>. This simple structure allows it to be manufactured in high densities at low cost. Each chip consists of a byte wide array of non-volatile memory cells. An arbitrary read can be performed with DRAM-like access times (under 100ns).
Reference: [5] <author> Transaction Processing Performance Council. </author> <title> TPC benchmark A standard specification rev 1.1. </title>
Reference-contexts: To account for this, 60ns is added to each access, raising the simulated latency of a Flash read or SRAM operation to 160ns. 5.2 Simulated Workload The simulator is driven by the I/O workload generated by the TPC-A database benchmark <ref> [5] </ref>. The benchmark consists of short database transactions with relatively small amounts of processing which makes it particularly susceptible to becoming I/O bound.
Reference: [6] <author> H. Garcia-Molina and K. Salem. </author> <title> Main memory database systems: An overview. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 509-516, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field <ref> [6, 8, 9] </ref>. Some particularly interesting work was done at IBM as part of their Starburst project [8] in which their database was tested with two storage components, one that uses memory mapped data structures and one that uses a standard cached disk model.
Reference: [7] <author> G. A. Gibson. </author> <title> Performance and reliability in redundant arrays of inexpensive disks. </title> <booktitle> In 1989 Computer Measurement Group Annual Conference Proceedings, </booktitle> <pages> pages 1-17, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Caches are commonly used to hide the read latency of disks [15] while non-volatile write buffers have been developed to hide write latency [1, 3, 12]. RAID arrays improve available I/O rates by transferring data in parallel <ref> [7, 10, 11] </ref>. We are exploring an alternative approach, building a persistent storage system using solid-state memories, thereby avoiding the mechanical latencies associated with disks. Solid-state memories provide a factor of 100,000 improvement in access times compared to disks, from about 100ns for memory to 10ms for disks.
Reference: [8] <author> T. Lehman, E. Shekita, and L. Cabrera. </author> <title> An evaluation of Starburst's memory resident storage component. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 555-566, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field <ref> [6, 8, 9] </ref>. Some particularly interesting work was done at IBM as part of their Starburst project [8] in which their database was tested with two storage components, one that uses memory mapped data structures and one that uses a standard cached disk model. <p> Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field [6, 8, 9]. Some particularly interesting work was done at IBM as part of their Starburst project <ref> [8] </ref> in which their database was tested with two storage components, one that uses memory mapped data structures and one that uses a standard cached disk model.
Reference: [9] <author> L. Li and J.F. Naughton. </author> <title> Multiprocessor main memory transaction processing. </title> <booktitle> In Proceedings of International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 177-187, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field <ref> [6, 8, 9] </ref>. Some particularly interesting work was done at IBM as part of their Starburst project [8] in which their database was tested with two storage components, one that uses memory mapped data structures and one that uses a standard cached disk model.
Reference: [10] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of SIGMOD '88, </booktitle> <pages> pages 109-116, </pages> <year> 1988. </year>
Reference-contexts: Caches are commonly used to hide the read latency of disks [15] while non-volatile write buffers have been developed to hide write latency [1, 3, 12]. RAID arrays improve available I/O rates by transferring data in parallel <ref> [7, 10, 11] </ref>. We are exploring an alternative approach, building a persistent storage system using solid-state memories, thereby avoiding the mechanical latencies associated with disks. Solid-state memories provide a factor of 100,000 improvement in access times compared to disks, from about 100ns for memory to 10ms for disks.
Reference: [11] <author> A. L. N. Reddy and P. Banerjee. </author> <title> An evaluation of multiple disk i/o systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1680-1690, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Caches are commonly used to hide the read latency of disks [15] while non-volatile write buffers have been developed to hide write latency [1, 3, 12]. RAID arrays improve available I/O rates by transferring data in parallel <ref> [7, 10, 11] </ref>. We are exploring an alternative approach, building a persistent storage system using solid-state memories, thereby avoiding the mechanical latencies associated with disks. Solid-state memories provide a factor of 100,000 improvement in access times compared to disks, from about 100ns for memory to 10ms for disks.
Reference: [12] <author> C. Reummler and J. Wilkes. </author> <title> UNIX disk access patterns. </title> <booktitle> In Proceedings of the 1993 Winter Usenix Conference, </booktitle> <pages> pages 405-420, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Methods such as caches, write buffers, and RAID arrays have been developed to alleviate disk bottlenecks. Caches are commonly used to hide the read latency of disks [15] while non-volatile write buffers have been developed to hide write latency <ref> [1, 3, 12] </ref>. RAID arrays improve available I/O rates by transferring data in parallel [7, 10, 11]. We are exploring an alternative approach, building a persistent storage system using solid-state memories, thereby avoiding the mechanical latencies associated with disks. <p> Others have studied the problem of how to minimize write costs for data reclamation in log structured file systems [13, 14]. The benefits possible from having fast access to stable storage were demonstrated by advocates of non-volatile write caches <ref> [1, 3, 12] </ref>. Finally, extensive work on in-memory data structures and other aspects of retrieval of data in large memory arrays have been done by researchers in the main memory database field [6, 8, 9].
Reference: [13] <author> M. Rosenblum and J.K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 26-52, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Space in the Flash array invalidated during the copy-on-write needs to be reclaimed and erased so that it can be made available for new data. A cleaning algorithm is used for this purpose. Our cleaning algorithm is similar in function to the Sprite LFS cleaning algorithm <ref> [13] </ref>, but optimized for use with a large Flash array. Simulation results of a 2 gigabyte eNVy system running under a TPC-A database benchmark workload show that the system can support I/O rates corresponding to 30,000 TPS (transactions per second). <p> If there is no free space where the controller wants to flush the page, a cleaning operation is initiated to reclaim space. The eNVy cleaning system has the same basic responsibility as the Sprite LFS segment cleaner <ref> [13] </ref>, although the reasons for cleaning differ. eNVy recovers space invalidated by the copy-on-write which cannot be overwritten with new data because of the bulk-erase nature of Flash memory. In LFS, data is invalidated as files are modified. <p> This ratio is essentially the amount of overhead (cleaning) that has to be done for every useful write to Flash. The cleaning cost differs from the write cost <ref> [13] </ref> used in Sprite LFS in two ways. First, the cleaning cost does not include the cost of reads since cleaning time is dominated by the time it takes to write to Flash memory. Second, it does not include the cost of doing the initial flush from SRAM. <p> Sprite LFS reduces cleaning costs by separating data with different access frequencies into different segments and picking which segments to clean using a cost/benefit equation <ref> [13] </ref>. The Sprite LFS cleaner reads several segments at once, sorts the active data by age, and rewrites the data to several unused segments. <p> For example, "10/90" means that 90% of all accesses go to 10% of the data, while 10% goes to the remaining 90% of data. 4.2 Greedy Method Our greedy policy is similar to the one described in the Sprite LFS <ref> [13] </ref> paper. When there is no space to flush data, the cleaner chooses to clean the segment with the most invalidated space, hoping to recover as much space as possible. <p> Others have studied the problem of how to minimize write costs for data reclamation in log structured file systems <ref> [13, 14] </ref>. The benefits possible from having fast access to stable storage were demonstrated by advocates of non-volatile write caches [1, 3, 12].
Reference: [14] <author> M. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin. </author> <title> An implementation of a log structured file system for unix. </title> <booktitle> In Proceedings of the 1993 Winter Usenix Conference, </booktitle> <pages> pages 307-326, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: In order to implement this feature, the controller has to keep track of the location of the shadow copies and protect them from being cleaned. This function has been studied in the context of log file systems <ref> [14] </ref> which use similar data movement primitives. 7 Related Work There has been other work done in areas relevant to eNVy. <p> Others have studied the problem of how to minimize write costs for data reclamation in log structured file systems <ref> [13, 14] </ref>. The benefits possible from having fast access to stable storage were demonstrated by advocates of non-volatile write caches [1, 3, 12].
Reference: [15] <author> A. J. Smith. </author> <title> Disk cache miss ratio analysis and design considerations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 161-203, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Methods such as caches, write buffers, and RAID arrays have been developed to alleviate disk bottlenecks. Caches are commonly used to hide the read latency of disks <ref> [15] </ref> while non-volatile write buffers have been developed to hide write latency [1, 3, 12]. RAID arrays improve available I/O rates by transferring data in parallel [7, 10, 11].
References-found: 15

