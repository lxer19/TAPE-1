URL: http://www.cs.purdue.edu/homes/park/performance-dios.ps.Z
Refering-URL: http://www.cs.purdue.edu/homes/park/publ_projects.html
Root-URL: http://www.cs.purdue.edu
Email: fcruz,parkg@cs.purdue.edu  
Phone: tel.: (765) 494-7821, fax.: (765) 494-0739.  
Title: Towards Performance-Driven System Support for Distributed Computing in Clustered Environments  
Author: John Cruz Kihong Park 
Note: Supported in part by NSF grant ESS-9806741. Additionally supported by a fellowship from the Purdue Research Foundation (PRF). Contact author;  Additionally supported by NSF grant NCR-9714707, and grants from PRF and Sprint.  
Date: October 27, 1998  
Address: West Lafayette, IN 47907  
Affiliation: Department of Computer Sciences Purdue University  
Pubnum: CSD-TR 98-035  
Abstract: With the proliferation of networked distributed resources and the prevalence of workstation clusters as a dominant computing platform, providing adequate system support for distributed computing, including parallel computing, has become an important problem. A principal focus of previous works has been on enabling technologies that facilitate various forms of transparency including interoperability and ease-of-programming. Less emphasis has been given to performance and efficiency considerations, the dual, but equally important side to effectively achieving distributed computing in networked environments. This paper describes a set of performance features, their properties, implementation, and evaluation in a software support environment called DUNES. The main performance features consist of push/pull-based forms of active and passive end-point caching, communication-sensitive load balancing, distributed demand-based paging, and adaptive communication control. Although each feature targets a separate aspect of performance, collectively, they affect the scheduling of distributed resources to application processes where both communication and computation requirements are taken into account. The architecture of DUNES|in addition to incorporating the aforementioned performance features|allows commodity operating systems to be easily transformed into a distributed operating system while achieving complete transparency with respect to the existing application base as well as preserving semantic correctness. We show performance measurements of a Solaris UNIX based implementation of DUNES on Sparc and x86 architectures over LAN environments. We show that significant performance gains in terms of parallel application speed-up and high system throughput is achievable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache performance of operating system and multiprogramming workloads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The benefit of prefetching and caching has a long history spanning a number of areas from database systems, to file systems, to computer architecture and operating systems, and has gained renewed interest due to the proliferation of networked systems where latency hiding has become imperative for performance <ref> [1, 38, 59] </ref>. Sprite [54], for example, adopts non-write-through file caching employing a simple cache consistency mechanism and it is shown that "client side" caching| corresponding to our passive end-point caching|can reduce both server and network load significantly. <p> Following is the generalization of Lemma 4.4 to n processes. The proof is a straightforward extension of the counting argument used in Lemma 4.4 and is omitted here for brevity. Lemma 4.7 (n-Process Utilization) Consider n mutually independent processes with single-process-alone-on-a-host ratios ff k , k 2 <ref> [1; n] </ref>. Then if the n processes are jointly scheduled on host i 2 V R implementing PS processor scheduling, i (k) = ff k 1 + h=2 ( n s=1 r=1 r (1 ff r ) 1x r (s) ; k 2 [1; n]; (4.8) with probability 1 where x <p> single-process-alone-on-a-host ratios ff k , k 2 <ref> [1; n] </ref>. Then if the n processes are jointly scheduled on host i 2 V R implementing PS processor scheduling, i (k) = ff k 1 + h=2 ( n s=1 r=1 r (1 ff r ) 1x r (s) ; k 2 [1; n]; (4.8) with probability 1 where x r (s) 2 f0; 1g and the binary vector (x 1 (s); x 2 (s); : : : ; x n (s)), 1 s n respresents the s'th element in a canonical ordering of n Clearly, when the number of processes n is
Reference: [2] <author> R. Agrawal and A. Ezzat. </author> <title> Processor sharing in NEST: A network of computer workstations. </title> <booktitle> In Proc. 1st IEEE Conference on Computer Workstations, </booktitle> <pages> pages 198-208, </pages> <year> 1985. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [3] <author> P. Arbenz, M. Billeter, P. Guentert, and P. Luginbuehl. </author> <title> Molecular dynamics simulations on Cray clusters using the SCIDDLE-PVM environment. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 1156, </volume> <year> 1996. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [4] <author> M. Ashraf Iqbal, J. H. Saltz, and S. H. Bokhari. </author> <title> A comparitive analysis of static and dynamic load balancing strategies. </title> <booktitle> In Proc. Int. Conf. on Parallel Processing, </booktitle> <pages> pages 1040-1047, </pages> <year> 1986. </year>
Reference: [5] <author> C. F. Baillie, G. Carr, L. Hart, and T. Henderson. </author> <title> Comparison of shared memory and distributed memory parallelization strategies for grid-based weather forecast models. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <address> May 23-25, 1994, Knoxville, Tennessee, </address> <pages> pages 560-567, </pages> <year> 1994. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [6] <author> H. Bal, J. Steiner, and A. Tanenbaum. </author> <title> Programming languages for distributed computer systems. </title> <journal> ACM Computer Surveys, </journal> <volume> 21(3) </volume> <pages> 262-322, </pages> <year> 1989. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [7] <author> A. Barak and A. Shilow. </author> <title> A distributed load balancing algorithm for a multicomputer. </title> <journal> Software Practice and Experience, </journal> <volume> 15(9) </volume> <pages> 901-913, </pages> <month> September </month> <year> 1985. </year>
Reference: [8] <author> Amnon Barak, Shai Guday, and Richard G. Wheeler. </author> <title> The MOSIX distributed operating system: load balancing for UNIX, </title> <booktitle> volume 672 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag Inc., </publisher> <address> New York, NY, USA, </address> <year> 1993. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX <ref> [8] </ref>, and Charlotte [31], to mention a few. The Tui system [65] is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in [13]. <p> Transparency and deployability, we believe, represent an important advantage over kernel based distributed operating systems which, in spite of their numerous manifestations <ref> [8, 20, 24, 55] </ref> have as yet achieved only partial success at wide-spread use due to the practical burden of incompatibility and inertia posed by stand-alone commodity operating systems. Figure 3.1 illustrates this design methodology.
Reference: [9] <author> B. Bershad, S. Savage, P. Pardyak, E. Sirer, M. Fiuczynski, D. Becker, C. Chambers, and S. Eggers. </author> <title> Extensibility, safety, and performance in the SPIN operating system. </title> <booktitle> In Proc. 15th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 267-284, </pages> <year> 1995. </year>
Reference-contexts: Distributed operating systems, for the most part, are written from scratch and are kernel-based: distributed OS functionalities are resident inside the kernel. Building a complete distributed operating system from scratch is a monumental task. A subarea of operating systems|extensible operating systems <ref> [9, 29, 32] </ref>|tries to make the building of new functionalities and reuse of existing functionalities easier through various means. In all cases, there is a well-defined interface to kernel services and other predefined services.
Reference: [10] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and distributed computation: numerical methods. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: Consider the problem of solving a system of linear equations Ax + b = 0 where A = (a ij ) is an m fi m matrix. Finding a numerical solution for x can be formulated as a fixed point problem <ref> [10] </ref> which, in turn, can be solved by the iterative procedure x i = a ii @ b i + j=1 m X a ij x j A : If the spectral radius of A is less than 1, the iteration can be shown to converge.
Reference: [11] <author> K. Birman and R. Cooper. </author> <title> The isis project: Real experience with a fault tolerant programming system. </title> <journal> ACM Operating Systems Review, SIGOPS, </journal> <volume> 25(2) </volume> <pages> 103-107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [12] <author> K.P. Birman and T. Clark. </author> <title> Performance of the Isis distributed computing system. </title> <type> Technical Report TR-94-1432, </type> <institution> Cornell Univ., Computer Science Dept., </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [13] <author> Matt Bishop, Mark Valence, and Leonard F. Wisniewski. </author> <title> Process migration for heterogeneous distributed systems. </title> <type> Technical Report PCS-TR95-264, </type> <institution> Dartmouth College, Computer Science, </institution> <address> Hanover, NH, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX [8], and Charlotte [31], to mention a few. The Tui system [65] is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in <ref> [13] </ref>. However, due to the significant translation cost involved, it is rarely considered a viable dynamic load balancing strategy. A survey of process 7 migration mechanisms and related issues can be found in [64].
Reference: [14] <author> R. K. Boel and J. H. van Schuppen. </author> <title> Distributed load balancing. </title> <booktitle> In Proc. 27th Conf. on Decision and Control, </booktitle> <pages> page 1486, </pages> <month> December </month> <year> 1988. </year>
Reference: [15] <author> Clemens Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks.
Reference: [16] <author> N. Carriero and D. Gelernter. </author> <title> Application experience with linda. </title> <booktitle> Proceedings of the ACM/SIGPLAN PPEALS 1988, </booktitle> <volume> 23(9) </volume> <pages> 173-187, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [17] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-459, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [18] <author> Jeremy Casas, Dan L. Clark, Ravi Konuru, Steve W. Otto, Robert M. Prouty, and Jonathan Walpole. MPVM: </author> <title> A migration transparent version of PVM. </title> <journal> Computing systems: the journal of the USENIX Association, </journal> <volume> 8(2) </volume> <pages> 171-216, </pages> <month> Spring </month> <year> 1995. </year> <month> 37 </month>
Reference-contexts: A similar observation holds for PVM (Parallel Virtual Machine) [68], an execution environment for the development and execution of large concurrent and parallel applications that consist of many interacting, but relatively independent, components. MPVM <ref> [18] </ref> and Dynam-icPVM [25] are extensions to PVM that support process migration. They follow the approach used by Condor to checkpoint and restart processes.
Reference: [19] <author> P. C. Chen. </author> <title> Climate and weather simulations and data visualization using a supercomputer, work-stations, and microcomputers [2656-26]. In Visual data exploration and analysis III: 31 January-2 February, </title> <booktitle> 1996, </booktitle> <address> San Jose, California, </address> <pages> pages 254-264, </pages> <year> 1996. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [20] <author> D. R. Cheriton. </author> <title> The V distributed system. </title> <editor> In Akkihebbal L. Ananda and Balasubramaniam Srinivasan, editors, </editor> <booktitle> Distributed Computing Systems: Concepts and Structures, </booktitle> <pages> pages 165-184. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Los Alamos, CA, </address> <year> 1992. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> The most challenging technical (i.e., mechanistic) aspect of distributed OS functionality|process migration|can be achieved at the user-level on top of commodity UNIX [51, 53, 60] and forms the starting point for facilitating full distributed OS functionality. Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V <ref> [20] </ref>, MOSIX [8], and Charlotte [31], to mention a few. The Tui system [65] is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in [13]. <p> Transparency and deployability, we believe, represent an important advantage over kernel based distributed operating systems which, in spite of their numerous manifestations <ref> [8, 20, 24, 55] </ref> have as yet achieved only partial success at wide-spread use due to the practical burden of incompatibility and inertia posed by stand-alone commodity operating systems. Figure 3.1 illustrates this design methodology.
Reference: [21] <author> Alex Cheung and Anthony Reeves. </author> <title> High performance computing on a cluster of workstations. </title> <booktitle> In Proc. First International Symp. on High-Performance Distributed Computing, </booktitle> <pages> pages 152-160, </pages> <year> 1992. </year>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks.
Reference: [22] <author> S. Chowdhury. </author> <title> The greedy load sharing algorithm. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(1) </volume> <pages> 93-99, </pages> <month> May </month> <year> 1990. </year>
Reference: [23] <author> H. Clark and B. McMillin. </author> <title> Dawgs|a distributed compute server utilizing idle workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(2) </volume> <pages> 175-186, </pages> <year> 1992. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [24] <author> Partha Dasgupta, Richard LeBlanc, Mustaque Ahamad, and Umakishore Ramachandran. </author> <title> The Clouds distributed operations system. </title> <journal> IEEE Computer, </journal> <volume> 24(11) </volume> <pages> 34-44, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> The most challenging technical (i.e., mechanistic) aspect of distributed OS functionality|process migration|can be achieved at the user-level on top of commodity UNIX [51, 53, 60] and forms the starting point for facilitating full distributed OS functionality. Some kernel-based process migration facilities include those in Amoeba [69], Clouds <ref> [24] </ref>, V [20], MOSIX [8], and Charlotte [31], to mention a few. The Tui system [65] is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in [13]. <p> Transparency and deployability, we believe, represent an important advantage over kernel based distributed operating systems which, in spite of their numerous manifestations <ref> [8, 20, 24, 55] </ref> have as yet achieved only partial success at wide-spread use due to the practical burden of incompatibility and inertia posed by stand-alone commodity operating systems. Figure 3.1 illustrates this design methodology.
Reference: [25] <author> L. Dikken, F. van der Linden, J. J. J. Vesseur, and P. M. A. Sloot. DynamicPVM: </author> <title> Dynamic load balancing on parallel systems. </title> <editor> In W. Gentzsch and U. Harms, editors, </editor> <booktitle> High Performance Computing and Networking, </booktitle> <pages> pages 273-277, </pages> <address> Munich, Germany, April 1994. </address> <publisher> Springer Verlag, LNCS 797. </publisher>
Reference-contexts: A similar observation holds for PVM (Parallel Virtual Machine) [68], an execution environment for the development and execution of large concurrent and parallel applications that consist of many interacting, but relatively independent, components. MPVM [18] and Dynam-icPVM <ref> [25] </ref> are extensions to PVM that support process migration. They follow the approach used by Condor to checkpoint and restart processes.
Reference: [26] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software Practice and Experience, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: In <ref> [26] </ref>, different methods of transferring virtual memory are discussed. They apply to systems where process migration is supported by the kernel. One of the promising approaches (described in [26]) is the method used by Accent [75] called the copy-on-reference mechanism. <p> In <ref> [26] </ref>, different methods of transferring virtual memory are discussed. They apply to systems where process migration is supported by the kernel. One of the promising approaches (described in [26]) is the method used by Accent [75] called the copy-on-reference mechanism. Here, when a process migrates, only the process state is transferred. The migrated process begins execution almost immediately. Virtual memory pages are transferred only on demand, resulting in short delays during execution.
Reference: [27] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software-Practice & Experience, </journal> <volume> 21(8) </volume> <pages> 757-785, </pages> <month> August </month> <year> 1991. </year>
Reference: [28] <author> W. Dzwinel and J. Blasiak. </author> <title> Pattern recognition via molecular dynamics on vector supercomputers and networked workstations. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> (919), </volume> <year> 1995. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [29] <author> D. Engler, M. Kaashoek, and J. O'Toole Jr. Exokernel: </author> <title> an operating system architecture for application-level resource management. </title> <booktitle> In Proc. 15th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 251-266, </pages> <year> 1995. </year>
Reference-contexts: Distributed operating systems, for the most part, are written from scratch and are kernel-based: distributed OS functionalities are resident inside the kernel. Building a complete distributed operating system from scratch is a monumental task. A subarea of operating systems|extensible operating systems <ref> [9, 29, 32] </ref>|tries to make the building of new functionalities and reuse of existing functionalities easier through various means. In all cases, there is a well-defined interface to kernel services and other predefined services. <p> In all cases, there is a well-defined interface to kernel services and other predefined services. The microkernel approach to operating system design tries to make the functionality exported by a kernel minimal with behavioral customizations carried out at the user level. The exokernel <ref> [29] </ref> is an extreme instance where management functionality is separated from protection functionality and only the latter is provided by the kernel. Thus memory management, scheduling, and other traditionally kernel-based services are now implemented at the user-level allowing for maximum user control and flexibility. <p> by Guy et al. [38] has shown that an optimistic replication policy based on the former leads to satisfactory performance and provides room for further improvement. 3 Architecture of DUNES 3.1 Overall Structure DUNES (Distributed UNix ExtenSion) is a distributed operating system designed using the approach of library operating systems <ref> [29] </ref>. Operating systems export a number of services executed by the kernel via the interface of systems calls, e.g., in the case of UNIX, the system call stubs in 8 augmented by system call encapsulation and DUNES resource management routines. the standard C library.
Reference: [30] <author> M.R. Eskicioglu. </author> <title> Design issues of process migration facilities in distributed systems. </title> <journal> IEEE Comp. Soc. Techn. Comm. on Oper. Syst. and Appl. Env. Newsletter, </journal> <volume> 4(2), </volume> <year> 1990. </year>
Reference: [31] <author> Raphael Finkel and Yeshayahu Artsy. </author> <title> The process migration mechanism of Charlotte. </title> <journal> IEEE Computer Society Technical Committee on Operating Systems Newsletter, </journal> <volume> 3(1) </volume> <pages> 11-14, </pages> <month> Winter </month> <year> 1989. </year>
Reference-contexts: Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX [8], and Charlotte <ref> [31] </ref>, to mention a few. The Tui system [65] is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in [13]. However, due to the significant translation cost involved, it is rarely considered a viable dynamic load balancing strategy.
Reference: [32] <author> B. Ford, K. Van Maren, J. Lepreau, S. Clawson, B. Robinson, and J. Turner. </author> <title> The FLUX OS toolkit: reusable components for OS implementation. </title> <booktitle> In Proc. 6th Workshop on Hot Topics in Operating Systems, </booktitle> <pages> pages 14-19, </pages> <year> 1997. </year>
Reference-contexts: Distributed operating systems, for the most part, are written from scratch and are kernel-based: distributed OS functionalities are resident inside the kernel. Building a complete distributed operating system from scratch is a monumental task. A subarea of operating systems|extensible operating systems <ref> [9, 29, 32] </ref>|tries to make the building of new functionalities and reuse of existing functionalities easier through various means. In all cases, there is a well-defined interface to kernel services and other predefined services.
Reference: [33] <author> U. Gaertel, W. Joppich, and A. Schueller. </author> <title> Medium-range weather forecast on parallel systems. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <address> May 23-25, 1994, Knoxville, Tennessee, </address> <pages> pages 388-391, </pages> <year> 1994. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [34] <author> Robert G. Gallager. </author> <title> A minimum delay routing algorithm using distributed computation. </title> <journal> IEEE Trans. Commun., </journal> <volume> COM-25(1):73-85, </volume> <year> 1977. </year>
Reference-contexts: We take the quasi-stationary approach to solving dynamic optimization problems <ref> [34] </ref> where we assume that the nonstationary system can be decomposed, in time, into stationary segments during which a solution procedure to the stationary problem|incorporating both computation and communication costs|is applied. 4.3 Iterative Load Balancer An elementary operation in the integrated scheduling algorithm will be the approximation of the progress rate
Reference: [35] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [36] <author> E. Gelenbe and I. Mitrani. </author> <title> Analysis and synthesis of computer systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: That is, their coupling is zero. Fix two hosts i; j 2 V R where ~(a) 6= i; j, for all a 2 V P . That is, they are empty. Assume that all hosts engage in head-of-line processor sharing (PS) <ref> [36, 49] </ref> when scheduling processes, the basic template on which most OS processor scheduling algorithms are based. <p> The OFF periods are blocking periods where a process is waiting on one or more events to occur, either related to communication or other interrupts. Thus this empty or idle period is consequent or causal to the last instruction executing and is not "forgetful." In traditional task modeling <ref> [36, 49] </ref>, the task arrival process is not of a single task in the blocking sense of above but rather of sequential tasks whose interarrival time obeys a fixed distribution (e.g., exponential, heavy-tailed).
Reference: [37] <author> D. Ghormley, D. Petrou, S. Rodrigues, A. Vahdat, and T. Anderson. GLUnix: </author> <title> a globale layer Unix for a network of workstations. </title> <note> To appear in Software: Practice and Experience, </note> <year> 1998. </year>
Reference-contexts: This is important from the perspective that present day applications tend to engage in some form of interaction|frequent or infrequent|with other applications and resources, and thus maintaining dependencies correctly with respect to standard UNIX semantics is an important requirement. Another related system is GLUnix <ref> [37] </ref>. However, GLUnix does not support process migration and dynamic load balancing, and it does not possess the performance enhancement features of DUNES.
Reference: [38] <author> R. Guy, G. Popek, and T. </author> <title> Page. Consistency algorithms for optimistic replication. </title> <booktitle> In Proc. IEEE International Conference on Network Protocols, </booktitle> <year> 1993. </year>
Reference-contexts: The benefit of prefetching and caching has a long history spanning a number of areas from database systems, to file systems, to computer architecture and operating systems, and has gained renewed interest due to the proliferation of networked systems where latency hiding has become imperative for performance <ref> [1, 38, 59] </ref>. Sprite [54], for example, adopts non-write-through file caching employing a simple cache consistency mechanism and it is shown that "client side" caching| corresponding to our passive end-point caching|can reduce both server and network load significantly. <p> Our passive end-point caching mechanism adopts a token-based cache consistency mechanism similar to DFS' scheme. This is motivated by the relative simplicity of implementing single writer/multiple reader semantics. A recent study by Guy et al. <ref> [38] </ref> has shown that an optimistic replication policy based on the former leads to satisfactory performance and provides room for further improvement. 3 Architecture of DUNES 3.1 Overall Structure DUNES (Distributed UNix ExtenSion) is a distributed operating system designed using the approach of library operating systems [29]. <p> Single writer/multiple reader semantics is the simplest but also most restrictive cache consistency protocol. However, previous studies on read/write access in UNIX file systems and more recent studies for collaborative workgroup environments <ref> [38] </ref> have shown that concurrent read/write access|and even more so for write/write access|to common files is a infrequent event which, by Amdahl's Law, warrants the use of more optimistic consistency protocols.
Reference: [39] <author> A. Hac and Th.J. Johnson. </author> <title> Sensitivity study of the load balancing algorithm in a distributed system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 85-89, </pages> <year> 1990. </year>
Reference: [40] <author> M. Harchol-Balter. </author> <title> Process lifetimes are not exponential, more like 1=t: implications on dynamic load balancing. </title> <type> Technical report, </type> <institution> EECS, University of California, Berkeley, </institution> <year> 1996. </year> <month> CSD-94-826. </month>
Reference-contexts: However, more and more, processes engage in some level of IPC and network communication| for some applications such as parallel computing applications the communication/computation ratio can be exceedingly high|and excluding them from dynamic load balancing may incur a significant opportunity cost <ref> [40, 41] </ref>. DUNES provides the mechanisms for maintaining dependencies transparently, and the issue of whether for certain processes it would be beneficial to migrate is left to a performance enhancement feature|the communication-sensitive load balancer|to decide. <p> Given the progress rates of two different configurations, there is a break-even point, parameterized by the cost of process migration itself, where only if the process' lifetime is longer than a certain period|which itself depends on progress rate|is process migration beneficial. Recent work <ref> [40, 41] </ref> in process lifetime distribution of UNIX processes has shown that process lifetimes are heavy-tailed (i.e., the tail decays hyperbolically, not exponentially) which allows effective prediction due to the presence of long-range correlation structure. <p> This also implies that need only be a function of process migration cost|an easily estimable quantity|and the lifetime duration of processes which has been investigated in <ref> [40, 41] </ref>. <p> In general, many types of processes are short-lived, and in such cases, the overhead due to migration can overshadow any gain obtained from a temporary increase of progress rate. For example, in <ref> [40, 41] </ref> it is shown that the lifetime distribution of UNIX processes is heavy-tailed which carries the implication that most processes have very short life spans (e.g., utility commands such as ls executed under a shell).
Reference: [41] <author> M. Harchol-Balter and A. Downey. </author> <title> Exploiting process lifetime distributions for dynamic load balancing. </title> <booktitle> In Proceedings of SIGMETRICS '96, </booktitle> <pages> pages 13-24, </pages> <year> 1996. </year>
Reference-contexts: However, more and more, processes engage in some level of IPC and network communication| for some applications such as parallel computing applications the communication/computation ratio can be exceedingly high|and excluding them from dynamic load balancing may incur a significant opportunity cost <ref> [40, 41] </ref>. DUNES provides the mechanisms for maintaining dependencies transparently, and the issue of whether for certain processes it would be beneficial to migrate is left to a performance enhancement feature|the communication-sensitive load balancer|to decide. <p> Given the progress rates of two different configurations, there is a break-even point, parameterized by the cost of process migration itself, where only if the process' lifetime is longer than a certain period|which itself depends on progress rate|is process migration beneficial. Recent work <ref> [40, 41] </ref> in process lifetime distribution of UNIX processes has shown that process lifetimes are heavy-tailed (i.e., the tail decays hyperbolically, not exponentially) which allows effective prediction due to the presence of long-range correlation structure. <p> This also implies that need only be a function of process migration cost|an easily estimable quantity|and the lifetime duration of processes which has been investigated in <ref> [40, 41] </ref>. <p> In general, many types of processes are short-lived, and in such cases, the overhead due to migration can overshadow any gain obtained from a temporary increase of progress rate. For example, in <ref> [40, 41] </ref> it is shown that the lifetime distribution of UNIX processes is heavy-tailed which carries the implication that most processes have very short life spans (e.g., utility commands such as ls executed under a shell).
Reference: [42] <author> A. Heddaya and K. Park. </author> <title> Mapping parallel iterative algorithms onto workstation networks. </title> <booktitle> In Proc. 3rd IEEE International Symposium on High-Performance Distributed Computing, </booktitle> <pages> pages 211-218, </pages> <year> 1994. </year>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks. <p> However, it has been shown that dynamic communication control in the form of application-sensitive congestion control is needed to achieve stable network behavior and high throughput which can reduce application completion time by several factors <ref> [42, 44] </ref>. <p> Another important performance feature is adaptive communication control where application-sensitive congestion control is applied to improve the effective throughput achieved by processes belonging to a concurrent application distributed across a network. In previous works <ref> [42, 44, 45] </ref> we have shown that significant performance gains|up to a factor of 4 under heavily congested network conditions|can be obtained for parallel applications if a state-of-the-art dynamic congestion control [58] is applied to regulate traffic flow on a per-application basis.
Reference: [43] <author> A. Heddaya and K. Park. </author> <title> Parallel computing on high-speed wide-area networks: a pricing policy for its communication needs. </title> <booktitle> In Proc. 3rd IEEE Workshop on the Architecture and Implementation of High Performance Communication Subsystems, </booktitle> <pages> pages 188-191, </pages> <year> 1995. </year>
Reference-contexts: Synchronous applications|in particular, those with lock-step computation-communication 1 iterations|are subject to synchronization penalties which are determined by the progress rate of the slowest processing element. The latter can be shown to be "exponentially sensitive" in the number of nodes or workstations participating in the computation <ref> [43] </ref>. 1.2 Problem Statement This paper addresses the issue of achieving parallel speed-up of concurrent applications and high system throughput in shared network environments, where scheduling of processes to resources is determined by an integrated approach to computation and communication control.
Reference: [44] <author> A. Heddaya and K. Park. </author> <title> Congestion control for asynchronous parallel computing on workstation networks. </title> <journal> Parallel Computing, </journal> <volume> 23 </volume> <pages> 1855-1875, </pages> <year> 1997. </year>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks. <p> However, it has been shown that dynamic communication control in the form of application-sensitive congestion control is needed to achieve stable network behavior and high throughput which can reduce application completion time by several factors <ref> [42, 44] </ref>. <p> Another important performance feature is adaptive communication control where application-sensitive congestion control is applied to improve the effective throughput achieved by processes belonging to a concurrent application distributed across a network. In previous works <ref> [42, 44, 45] </ref> we have shown that significant performance gains|up to a factor of 4 under heavily congested network conditions|can be obtained for parallel applications if a state-of-the-art dynamic congestion control [58] is applied to regulate traffic flow on a per-application basis.
Reference: [45] <author> A. Heddaya, K. Park, and H. Sinha. </author> <title> Using Warp to control network contention in Mermera. </title> <booktitle> In Proc. 27th Hawaii International Conference on System Sciences, Maui, Hawaii, </booktitle> <pages> pages 96-105, </pages> <year> 1994. </year>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks. <p> Another important performance feature is adaptive communication control where application-sensitive congestion control is applied to improve the effective throughput achieved by processes belonging to a concurrent application distributed across a network. In previous works <ref> [42, 44, 45] </ref> we have shown that significant performance gains|up to a factor of 4 under heavily congested network conditions|can be obtained for parallel applications if a state-of-the-art dynamic congestion control [58] is applied to regulate traffic flow on a per-application basis.
Reference: [46] <author> C. Jacqmot, E. Milgrom, W. Joossen, and Y. Berbers. </author> <title> Unix and load-balancing: A survey. </title> <booktitle> In Proc. </booktitle> <volume> EUUG '89, </volume> <pages> pages 1-15, </pages> <month> April </month> <year> 1989. </year>
Reference: [47] <author> M. Kaashoek, D. Engler, G. Ganger, H. Brice no, R. Hunt, D. Mazieres, T. Pinckney, R. Grimm, J. Jannotti, and K. Mackenzie. </author> <title> Application performance and flexibility on exokernel systems. </title> <booktitle> In Proc. 16th ACM Symp. on Operating System Principles, </booktitle> <year> 1997. </year>
Reference-contexts: Thus applications need not be recompiled. This method of adding functionalities has been used in the past, among other things, to facilitate user-level checkpointing and process migration in UNIX [51, 53], and it can be viewed as part of the general framework of library operating systems <ref> [47] </ref> to extending OS functionality. In the context of concurrent application development for parallel and distributed applications, the programming model that DUNES exports to the programmer is one of writing concurrent programs for a single processor UNIX environment. <p> It is clear that a complete distributed OS can be built at the user level on top of Xok|the exokernel for x86-based machines|and exported as a set of libraries, also called library operating systems (libOS) <ref> [47] </ref>. A key question associated with the microkernel approach is whether the resulting systems perform as efficiently as their monolithic brethren (e.g., UNIX).
Reference: [48] <author> J. O. Kephart, T. Hogg, and B. A. Huberman. </author> <title> Dynamics of computational ecosystems. </title> <journal> Physical Review A, </journal> <volume> 40(1) </volume> <pages> 404-421, </pages> <year> 1989. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments.
Reference: [49] <author> Leonard Kleinrock. </author> <title> Queueing Systems, </title> <booktitle> Volume 2: Computer Applications. </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: That is, their coupling is zero. Fix two hosts i; j 2 V R where ~(a) 6= i; j, for all a 2 V P . That is, they are empty. Assume that all hosts engage in head-of-line processor sharing (PS) <ref> [36, 49] </ref> when scheduling processes, the basic template on which most OS processor scheduling algorithms are based. <p> The OFF periods are blocking periods where a process is waiting on one or more events to occur, either related to communication or other interrupts. Thus this empty or idle period is consequent or causal to the last instruction executing and is not "forgetful." In traditional task modeling <ref> [36, 49] </ref>, the task arrival process is not of a single task in the blocking sense of above but rather of sequential tasks whose interarrival time obeys a fixed distribution (e.g., exponential, heavy-tailed).
Reference: [50] <author> W. Leland and T. Ott. </author> <title> Load-balancing heuristics and process behavior. In ACM Performance Evaluation Review: </title> <booktitle> Proc. Performance '86 and ACM SIGMETRICS 1986, </booktitle> <volume> Vol. 14, </volume> <pages> pages 54-69, </pages> <month> May </month> <year> 1986. </year>
Reference: [51] <author> M. Litzkow, M. Livny, and M. </author> <title> Mutka. Condor ahunter of idle workstations. </title> <booktitle> In Proc. 8'th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <year> 1988. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming. <p> The control algorithm|making use of state information encompassing both computation and communication behavior|dynamically schedules application processes so as to enhance performance. Performance Features As part of the distributed OS functionality, our system implements dynamic process migration following the user-level mechanism employed in Condor <ref> [51] </ref>. However, unlike in Condor, our dynamic process migration mechanism handles dependencies arising from interprocess communication and file access maintaining transparent bindings consistent with UNIX semantics. <p> Thus applications need not be recompiled. This method of adding functionalities has been used in the past, among other things, to facilitate user-level checkpointing and process migration in UNIX <ref> [51, 53] </ref>, and it can be viewed as part of the general framework of library operating systems [47] to extending OS functionality. <p> We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> We seek the best of both worlds|transparency from network computing and efficiency from distributed operating systems. The most challenging technical (i.e., mechanistic) aspect of distributed OS functionality|process migration|can be achieved at the user-level on top of commodity UNIX <ref> [51, 53, 60] </ref> and forms the starting point for facilitating full distributed OS functionality. Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX [8], and Charlotte [31], to mention a few. <p> Process migration for heterogeneous environments is also studied in [13]. However, due to the significant translation cost involved, it is rarely considered a viable dynamic load balancing strategy. A survey of process 7 migration mechanisms and related issues can be found in [64]. Our system is similar to Condor <ref> [51] </ref> in that it follows the latter's user-level process migration scheme. However, unlike Condor, our dynamic process migration mechanism handles dependencies arising from interprocess communication, network communication, process creation, and file access while maintaining transparent bindings consistent with UNIX semantics. <p> Techniques for transparent process migration using user-level libraries for checkpointing and restart are well-known <ref> [51, 53, 60] </ref> and we follow an analogous strategy in our own implementation. First, application binaries, when being relinked with the modified system call library, are also linked with the DUNES start-up routine Main () which, after initialization, transfers control to the application binary proper by calling main (). <p> Furthermore, a process, after migration, may fork off another process which can complicate the picture significantly. Transparently maintaining dependencies in the presence of mobility using user-level techniques is a nontrivial challenge. Condor <ref> [51] </ref>, for example, supports transparent file access but does not allow process migration in the presence of IPC, network communication, or fork. One practical justification for this is that, other things being equal, the processes that benefit most from migration are isolated processes.
Reference: [52] <author> Jaroslav Mackerle. </author> <title> Implementing finite element methods on supercomputers, workstations and PCs: A bibliography (1985-1995). </title> <journal> Engineering Computations, </journal> <volume> 13(1), </volume> <year> 1996. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [53] <author> K. I. Mandelberg and V. S. Sunderam. </author> <title> Process migration in UNIX networks. </title> <booktitle> In USENIX Technical Conference Proceedings, </booktitle> <pages> pages 357-363, </pages> <address> Dallas, TX, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Thus applications need not be recompiled. This method of adding functionalities has been used in the past, among other things, to facilitate user-level checkpointing and process migration in UNIX <ref> [51, 53] </ref>, and it can be viewed as part of the general framework of library operating systems [47] to extending OS functionality. <p> We seek the best of both worlds|transparency from network computing and efficiency from distributed operating systems. The most challenging technical (i.e., mechanistic) aspect of distributed OS functionality|process migration|can be achieved at the user-level on top of commodity UNIX <ref> [51, 53, 60] </ref> and forms the starting point for facilitating full distributed OS functionality. Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX [8], and Charlotte [31], to mention a few. <p> Techniques for transparent process migration using user-level libraries for checkpointing and restart are well-known <ref> [51, 53, 60] </ref> and we follow an analogous strategy in our own implementation. First, application binaries, when being relinked with the modified system call library, are also linked with the DUNES start-up routine Main () which, after initialization, transfers control to the application binary proper by calling main ().
Reference: [54] <author> M. N. Nelson, B. Welch, and J. Ousterhout. </author> <title> Caching in the sprite network file system. </title> <booktitle> Preprints for the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 34-47, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Sprite <ref> [54] </ref>, for example, adopts non-write-through file caching employing a simple cache consistency mechanism and it is shown that "client side" caching| corresponding to our passive end-point caching|can reduce both server and network load significantly.
Reference: [55] <author> John K. Ousterhout, A. R. Cherenson, Fred Douglis, Michael N. Nelson, and Brent B. Welch. </author> <title> The Sprite network operating system. </title> <journal> Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> Transparency and deployability, we believe, represent an important advantage over kernel based distributed operating systems which, in spite of their numerous manifestations <ref> [8, 20, 24, 55] </ref> have as yet achieved only partial success at wide-spread use due to the practical burden of incompatibility and inertia posed by stand-alone commodity operating systems. Figure 3.1 illustrates this design methodology.
Reference: [56] <author> M. Parashar, S. Hariri, A. Mohamed, and G. Fox. </author> <title> A requirement analysis for high performance distributed computing over LAN's. </title> <booktitle> In Proc. First International Symp. on High-Performance Distributed Computing, </booktitle> <pages> pages 142-151, </pages> <year> 1992. </year> <month> 39 </month>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks.
Reference: [57] <author> K. Park, G. Kim, and M. Crovella. </author> <title> On the relationship between file sizes, transport protocols, and self--similar network traffic. </title> <booktitle> In Proc. IEEE International Conference on Network Protocols, </booktitle> <pages> pages 171-180, </pages> <year> 1996. </year>
Reference-contexts: However, by the same token, heavy-tailedness also implies the existence of a few very long-lived processes who are then potential candidates for dynamic load balancing. In fact, heavy-tailedness implies the existence of nontrivial long-range correlation structure <ref> [57] </ref> which can then be exploited for prediction purposes. In essence, if we observe a process running for some time interval T that is "not too small," then the conditional probability that the process will continue to run for a "very long" while is high. migration. without, migration.
Reference: [58] <author> Kihong Park. </author> <title> Warp control: a dynamically stable congestion protocol and its analysis. </title> <journal> Journal of High Speed Networks, </journal> <volume> 2(4) </volume> <pages> 373-404, </pages> <year> 1993. </year>
Reference-contexts: In previous works [42, 44, 45] we have shown that significant performance gains|up to a factor of 4 under heavily congested network conditions|can be obtained for parallel applications if a state-of-the-art dynamic congestion control <ref> [58] </ref> is applied to regulate traffic flow on a per-application basis. We omit adaptive communication related results due to redundancy and brevity reasons.
Reference: [59] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proc. 15th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 79-95, </pages> <year> 1995. </year>
Reference-contexts: The benefit of prefetching and caching has a long history spanning a number of areas from database systems, to file systems, to computer architecture and operating systems, and has gained renewed interest due to the proliferation of networked systems where latency hiding has become imperative for performance <ref> [1, 38, 59] </ref>. Sprite [54], for example, adopts non-write-through file caching employing a simple cache consistency mechanism and it is shown that "client side" caching| corresponding to our passive end-point caching|can reduce both server and network load significantly.
Reference: [60] <author> Stefan Petri and Horst Langendorfer. </author> <title> Load balancing and fault tolerance in workstation clusters - migrating groups of communicating processes. </title> <journal> Operating Systems Review, </journal> <volume> 29(4) </volume> <pages> 25-36, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: We seek the best of both worlds|transparency from network computing and efficiency from distributed operating systems. The most challenging technical (i.e., mechanistic) aspect of distributed OS functionality|process migration|can be achieved at the user-level on top of commodity UNIX <ref> [51, 53, 60] </ref> and forms the starting point for facilitating full distributed OS functionality. Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX [8], and Charlotte [31], to mention a few. <p> Techniques for transparent process migration using user-level libraries for checkpointing and restart are well-known <ref> [51, 53, 60] </ref> and we follow an analogous strategy in our own implementation. First, application binaries, when being relinked with the modified system call library, are also linked with the DUNES start-up routine Main () which, after initialization, transfers control to the application binary proper by calling main ().
Reference: [61] <author> K.W. Ross and D.D. Yao. </author> <title> Optimal load balancing and scheduling in a distributed computer system. </title> <journal> Journal of the ACM, </journal> <volume> 38(3) </volume> <pages> 676-690, </pages> <month> July </month> <year> 1991. </year>
Reference: [62] <author> Oliver Sharp. </author> <title> The grand challenges. </title> <journal> BYTE Magazine, </journal> <volume> 20(2), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [63] <author> J.M. Smith. </author> <title> A survey of process migration mechanisms. </title> <journal> Operating Systems Review, </journal> <volume> 22(3) </volume> <pages> 28-40, </pages> <month> July </month> <year> 1988. </year>
Reference: [64] <author> Jonathan M. Smith. </author> <title> A survey of process migration mechanisms. </title> <journal> ACM Operating Systems Review, </journal> <volume> 22(3) </volume> <pages> 28-40, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Process migration for heterogeneous environments is also studied in [13]. However, due to the significant translation cost involved, it is rarely considered a viable dynamic load balancing strategy. A survey of process 7 migration mechanisms and related issues can be found in <ref> [64] </ref>. Our system is similar to Condor [51] in that it follows the latter's user-level process migration scheme. However, unlike Condor, our dynamic process migration mechanism handles dependencies arising from interprocess communication, network communication, process creation, and file access while maintaining transparent bindings consistent with UNIX semantics.
Reference: [65] <author> Peter Smith and Norman C. Hutchinson. </author> <title> Heterogeneous process migration: The tui system. </title> <type> Technical Report TR-96-04, </type> <institution> University of British Columbia. Computer Science, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Some kernel-based process migration facilities include those in Amoeba [69], Clouds [24], V [20], MOSIX [8], and Charlotte [31], to mention a few. The Tui system <ref> [65] </ref> is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in [13]. However, due to the significant translation cost involved, it is rarely considered a viable dynamic load balancing strategy.
Reference: [66] <author> Volker Strumpen. </author> <title> Parallel molecular sequence analysis on workstations in the Internet. </title> <type> Technical Report 93.28, </type> <institution> Department of Computer Science, University of Zurich, </institution> <year> 1993. </year>
Reference-contexts: More recently, performance studies of LAN- and WAN-based systems have shown the importance of controlling network communication for improving parallel or distributed application performance <ref> [15, 21, 42, 44, 45, 56, 66] </ref>. The sensitivity of application performance to congestion effects is directly dependent upon the communication/computation ratio and degree of synchrony. An application with a high communication/computation ratio is prone to generate periods of concentrated congestion which leads to debilitating communication bottlenecks.
Reference: [67] <author> Michael Stumm. </author> <title> The design and implementation of a decentralized scheduling facility for a workstation cluster. </title> <booktitle> In Proceedings of the 2nd IEEE Conference on Computer Workstations, </booktitle> <pages> pages 12-22, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: This, in turn, aggravates synchronization penalty stalling application progress. A myriad of software support environments have been advanced in the past with a view toward facilitating concurrent applications in workstation environments <ref> [2, 6, 11, 12, 16, 17, 23, 35, 51, 67] </ref>. A principal focus of previous works has been on enabling technologies that achieve various forms of transparency including interoperability and ease-of-programming.
Reference: [68] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency, practice and experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence [8, 20, 24, 48, 51, 55, 68, 69]. The former are spearheaded by recent developments in network computing <ref> [68, 70, 71] </ref> where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. Distributed operating systems, for the most part, are written from scratch and are kernel-based: distributed OS functionalities are resident inside the kernel. <p> Another related system is GLUnix [37]. However, GLUnix does not support process migration and dynamic load balancing, and it does not possess the performance enhancement features of DUNES. A similar observation holds for PVM (Parallel Virtual Machine) <ref> [68] </ref>, an execution environment for the development and execution of large concurrent and parallel applications that consist of many interacting, but relatively independent, components. MPVM [18] and Dynam-icPVM [25] are extensions to PVM that support process migration. They follow the approach used by Condor to checkpoint and restart processes.
Reference: [69] <author> Andrew S. Tanenbaum, Robert van Renesse, Hans van Staveren, Gregory J. Sharp, Sape J. Mullender, Jack Jansen, and Guido van Rossum. </author> <title> Experience with the Amoeba distributed operating system. </title> <journal> CACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence <ref> [8, 20, 24, 48, 51, 55, 68, 69] </ref>. The former are spearheaded by recent developments in network computing [68, 70, 71] where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. <p> The most challenging technical (i.e., mechanistic) aspect of distributed OS functionality|process migration|can be achieved at the user-level on top of commodity UNIX [51, 53, 60] and forms the starting point for facilitating full distributed OS functionality. Some kernel-based process migration facilities include those in Amoeba <ref> [69] </ref>, Clouds [24], V [20], MOSIX [8], and Charlotte [31], to mention a few. The Tui system [65] is interesting from the perspective that it supports process migration across heterogeneous machines. Process migration for heterogeneous environments is also studied in [13].
Reference: [70] <author> L.G. Tesler. </author> <title> Networked computing in the 1990s. </title> <journal> Scientific American, </journal> <volume> 265(3) </volume> <pages> 54-61, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence [8, 20, 24, 48, 51, 55, 68, 69]. The former are spearheaded by recent developments in network computing <ref> [68, 70, 71] </ref> where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. Distributed operating systems, for the most part, are written from scratch and are kernel-based: distributed OS functionalities are resident inside the kernel.
Reference: [71] <author> Amjad Umar. </author> <title> Client/Server Internet Environments. </title> <publisher> Prentice-Hall, </publisher> <year> 1997. </year>
Reference-contexts: We conclude with a discussion of our results and future work. 6 2 Related Work There are many distributed computing platforms and distributed operating systems currently in existence [8, 20, 24, 48, 51, 55, 68, 69]. The former are spearheaded by recent developments in network computing <ref> [68, 70, 71] </ref> where the primary focus has been on enabling technologies that achieve platform independence and allow heterogeneous distributed resources to be harnessed across networked environments. Distributed operating systems, for the most part, are written from scratch and are kernel-based: distributed OS functionalities are resident inside the kernel.
Reference: [72] <author> R. Van Engelen and L. Wolters. </author> <title> A comparison of parallel programming paradigms and data distributions for a limited area numerical weather forecast routine. </title> <booktitle> In Conference proceedings of the 1995 International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July 3-7, </month> <year> 1995, </year> <pages> pages 357-364, </pages> <year> 1995. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [73] <author> L. Wolters, G. Cats, and N. Gustafsson. </author> <title> Limited area numerical weather forecasting on a massively parallel computer. </title> <booktitle> In Supercomputing '94: International conference | July 1994, Manchester, </booktitle> <pages> pages 289-296, </pages> <year> 1994. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [74] <author> G. Yagawa, A. Yoshioka, S. Yoshimura, and N. Soneda. </author> <title> A parallel finite element method with a supercomputer network. </title> <journal> Computers and Structures, </journal> <volume> 47(3), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Concurrent applications can span from everyday applications to numerical and supercomputing applications which include a diverse spectrum of computational problems ranging from partial differential equations to global weather simulation to molecular sequence analysis <ref> [3, 5, 19, 28, 52, 62, 72, 73, 33, 74] </ref>.
Reference: [75] <author> Edward R. Zayas. </author> <title> Attacking the process migration bottleneck. </title> <booktitle> In Proceedings of the eleventh ACM Symposium on Operating System Principles, </booktitle> <pages> pages 13-24, </pages> <address> Austin, Texas, </address> <month> November </month> <year> 1987. </year> <note> ACM. </note>
Reference-contexts: In [26], different methods of transferring virtual memory are discussed. They apply to systems where process migration is supported by the kernel. One of the promising approaches (described in [26]) is the method used by Accent <ref> [75] </ref> called the copy-on-reference mechanism. Here, when a process migrates, only the process state is transferred. The migrated process begins execution almost immediately. Virtual memory pages are transferred only on demand, resulting in short delays during execution. In DUNES, a mechanism similar to Accent's copy-on-reference mechanism is incorporated.
Reference: [76] <author> S. Zhou and D. Ferrari. </author> <title> An experimental study of load balancing performance. </title> <booktitle> In Proc. IEEE Int. Conf. on Distr. Processing, </booktitle> <volume> volume 7, </volume> <pages> pages 490-497, </pages> <month> September </month> <year> 1987. </year> <month> 40 </month>
References-found: 76

