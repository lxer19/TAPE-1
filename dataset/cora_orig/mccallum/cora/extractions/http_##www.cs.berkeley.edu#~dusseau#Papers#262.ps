URL: http://www.cs.berkeley.edu/~dusseau/Papers/262.ps
Refering-URL: http://www.cs.berkeley.edu/~dusseau/Papers/papers.html
Root-URL: 
Title: The Effects of a Non-Dedicated Environment on Parallel Applications  
Author: Remzi H. Arpaci, Andrea C. Dusseau, Lok T. Liu 
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: Modern networks of workstations exhibit many of the traits of massively pararallel processors, but with better cost/performance ratios. A multitude of idle cycles exist on these workstations that may be used for running parallel applications. In this paper we use direct simulationto analyze the performance of parallel applications running in this type of non-dedicated environment. With direct simulation we study the performance effects of interrupting real parallel applications running on Thinking Machine's CM-5 with simulated serial and parallel jobs and the effects of skewing the parallel time quanta. Our results indicate that one non-idle workstation reduces the performance of parallel applications to the degree that migrating the affected process to an available idle workstation may be beneficial. The benefits of coscheduling parallel applications in this environment have been verified. If parallel processes are not coscheduled, then smaller time quanta result in better performance, ignoring cache behavior. Finally, the effects of skewing the time quanta across workstations is shown to be tolerable.
Abstract-found: 1
Intro-found: 1
Reference: [BLM91] <author> G. Blelloch, C. Leiserson, and B. Maggs. </author> <title> A comparison of sorting algorithms for the connection machine cm-2. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: In the second half step, each H nodes is updated similarly using the neighboring E nodes. Notice that each processor can proceed independently to its computation phases as soon as its ghost nodes are updated by its neighbor. 4.4 Sample Sort The idea behind sample sort <ref> [BLM91] </ref> is as follows. By sorting a sample of keys, we can get an idea about the distribution of the keys.
Reference: [CDD + 91] <author> Mark Crovella, Prakash Das, Czarek Dubnicki, Thomas LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <type> Technical Report 385, </type> <institution> University of Rochester, Computer Science Department, </institution> <month> February </month> <year> 1991. </year> <note> Revised May. </note>
Reference-contexts: In this scheme, processes of a parallel program are simultaneously scheduled across a set of processors. By doing so, communication and synchronization will be more efficient. Other studies have confirmed that coscheduling is an effective scheduling technique [GTU91], [LV90], [FR92], <ref> [CDD + 91] </ref>, and have also found other methods to be successful. Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. <p> By doing so, communication and synchronization will be more efficient. Other studies have confirmed that coscheduling is an effective scheduling technique [GTU91], [LV90], [FR92], <ref> [CDD + 91] </ref>, and have also found other methods to be successful. Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. One example of this is Tucker and Gupta's process control method [TG89], which attempts to match the number of processes your parallel program needs to run to the portion of the machine which it is allocated. <p> Due to the simplicity of the matrix algorithm, it was later implemented on the Cm*. Since that time, there have been a number of efforts which compared coscheduling to other parallel scheduling techniques. Crovella et al. <ref> [CDD + 91] </ref> implemented three different types of schedulers on a BBN Butterfly. The first had each node time-slice independently, the second a variant on Ousterhout's matrix algorithm, while the third was a space-sharing method.
Reference: [CDG + 93] <author> D. Culler, A. Dusseau, S. Goldstein, A. Krishnamurthy, S. Lumetta, T. von eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <year> 1993. </year> <month> 24 </month>
Reference-contexts: TCP/IP) precludes support for finer granularities. We envision an environment where parallel programs of any nature could be effective. 2.3 Programming Model The parallel benchmarks which we use in our simulations are written in Split-C <ref> [CDG + 93] </ref>, a parallel extension of the C programming language. On the CM-5, Split-C uses Active Messages as its message passing layer [vECGS92].
Reference: [CDMS93] <author> D. Culler, A. Dusseau, R. Martin, and K. Schauser. </author> <title> Fast parallel sorting under LogP: From theory to practice. </title> <booktitle> In Workshop on Parallel Portable Programming, </booktitle> <year> 1993. </year>
Reference-contexts: In our experiments, each processor has 512K keys. We will describe the communication pattern of Column Sort. A detailed explanation of Column Sort can be found in [Lei85]. The implementation we use is also described in <ref> [CDMS93] </ref>. In Column Sort, there are four communication phases: transpose, untranspose, shift, and unshift. A local sort is performed on every column before each communication phase. Transpose is exactly the blocked-to-cyclic remap used in bitonic sort.
Reference: [DMS93] <author> Craig C. Douglas, Timothy G. Mattson, and Martin H. Schultz. </author> <title> Parallel programming systems for workstation clusters. </title> <type> Technical Report 975, </type> <institution> Yale University, Computer Science Department, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: While this allows programs with many separate processes to be run, it ignores the possibility of running jobs that need to communicate and/or synchronize frequently. This is a result of high message latencies incurred. In a recent study, Douglas et al. <ref> [DMS93] </ref> measured the round-trip message transit time of PVM on two SPARCstations IIs. The result was 4.9 milliseconds for a 100-byte message. They also found that other packages, such as 22 C-Linda, P4, POSYBL, and TCGMSG, suffered from high end-to-end latencies.
Reference: [DO91] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent process migration: Design alternatives and the sprite implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-85, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: User-level network interfaces are becoming more common, allowing low-latency end-to-end communication. The realization is that there is a definite coalescence of these formerly distinct technologies. Studies have indicated that workstations in such an environment are often idle [ML91], [LV90], <ref> [DO91] </ref>, [Nic87], with the typical user busy roughly one-third of the day. Even then, only a small portion of their workstation's cycles are utilized, making a large quantity of potential computing available. <p> Theimer and Lantz state that roughly one third of 70 Sun workstations were completely idle, even during the busiest times of the day [TL89]. The implementors of the Sprite system estimate that 66-78% of their workstations are free at any particular moment <ref> [DO91] </ref>. Lastly, Mutka and Livny used the Condor system to perform an extended study on a group of VAXstations, and found that 75% were normally available [ML91]. To gain further insight on the interaction between users and their workstations, we have instrumented a number of machines locally.
Reference: [FR92] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Gang scheduling performance benefits for fine-grained synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 306-18, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Early on, the idea of coscheduling 1 was proposed [Ous82]. In this scheme, processes of a parallel program are simultaneously scheduled across a set of processors. By doing so, communication and synchronization will be more efficient. Other studies have confirmed that coscheduling is an effective scheduling technique [GTU91], [LV90], <ref> [FR92] </ref>, [CDD + 91], and have also found other methods to be successful. Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. <p> The process control method is another variant of space-sharing, with each program dynamically adjusting the number of processes it has to attempt to match the number of processors allocated to it. Feitelson and Rudolph <ref> [FR92] </ref> compare coscheduling to two methods where each node time-slices independently, one which blocks to synchronize, and the other which busy-waits. They implemented these three scheduling techniques on a bus-based multiprocessor, and found that gang scheduling is necessary for programs that require fine-grain synchronization.
Reference: [GTU91] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 120-32, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Early on, the idea of coscheduling 1 was proposed [Ous82]. In this scheme, processes of a parallel program are simultaneously scheduled across a set of processors. By doing so, communication and synchronization will be more efficient. Other studies have confirmed that coscheduling is an effective scheduling technique <ref> [GTU91] </ref>, [LV90], [FR92], [CDD + 91], and have also found other methods to be successful. Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. <p> By doing so, communication and synchronization will be more efficient. Other studies have confirmed that coscheduling is an effective scheduling technique <ref> [GTU91] </ref>, [LV90], [FR92], [CDD + 91], and have also found other methods to be successful. Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. One example of this is Tucker and Gupta's process control method [TG89], which attempts to match the number of processes your parallel program needs to run to the portion of the machine which it is allocated. <p> Space sharing is a technique that divides pool of processors among the programs that wish to run. By running a three synthetic benchmarks on the different configurations, they found that coscheduling was definitely better than independent time-slicing, and that space-sharing seemed to outperform them both. Anoop et al. <ref> [GTU91] </ref> performed simulations to compare a large variety of different scheduling techniques, their metric being the processor utilization achieved by each method. What differentiates their study from other simulational studies is that caching behavior is modeled.
Reference: [L + 92] <author> C. E. Leiserson et al. </author> <title> The network architecture of the connection machine cm-5. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: On the CM-5, Split-C uses Active Messages as its message passing layer [vECGS92]. In this section, we will briefly describe Active Messages and Split-C. 2.3.1 CM-5 The CM-5 is a distributed memory multiprocessor that can have up to 1K nodes <ref> [L + 92] </ref>. The nodes are interconnected by two disjoint data networks that have an incomplete fat tree topology and a control network that supports broadcast, scan and reduce.
Reference: [Lei85] <author> T. Leighton. </author> <title> Tight bounds on the comlexity of parallel sorting. </title> <journal> IEEE Transactions on Computers, </journal> <month> April </month> <year> 1985. </year>
Reference-contexts: In our experiments, each processor has 512K keys. We will describe the communication pattern of Column Sort. A detailed explanation of Column Sort can be found in <ref> [Lei85] </ref>. The implementation we use is also described in [CDMS93]. In Column Sort, there are four communication phases: transpose, untranspose, shift, and unshift. A local sort is performed on every column before each communication phase. Transpose is exactly the blocked-to-cyclic remap used in bitonic sort.
Reference: [LS93] <author> Scott T. Leutenegger and Xian-He Sun. </author> <title> Distributed computing feasibility in a non-dedicated homogenous distributed system. </title> <booktitle> In Supercomputing 93, </booktitle> <year> 1993. </year>
Reference-contexts: The result was 4.9 milliseconds for a 100-byte message. They also found that other packages, such as 22 C-Linda, P4, POSYBL, and TCGMSG, suffered from high end-to-end latencies. More recently, a study on how parallel program throughput is affected by users in a non-dedicated environment has been performed <ref> [LS93] </ref>. The model is quite simple: when a serial job needs to run, it preempts the currently running parallel job. The authors conclude that what they call the task ratio is an important metric, and must be high for parallel applications to achieve good performance in such an environment.
Reference: [LV90] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference, </booktitle> <pages> pages 226-36, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: User-level network interfaces are becoming more common, allowing low-latency end-to-end communication. The realization is that there is a definite coalescence of these formerly distinct technologies. Studies have indicated that workstations in such an environment are often idle [ML91], <ref> [LV90] </ref>, [DO91], [Nic87], with the typical user busy roughly one-third of the day. Even then, only a small portion of their workstation's cycles are utilized, making a large quantity of potential computing available. <p> Early on, the idea of coscheduling 1 was proposed [Ous82]. In this scheme, processes of a parallel program are simultaneously scheduled across a set of processors. By doing so, communication and synchronization will be more efficient. Other studies have confirmed that coscheduling is an effective scheduling technique [GTU91], <ref> [LV90] </ref>, [FR92], [CDD + 91], and have also found other methods to be successful. Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. <p> They implemented these three scheduling techniques on a bus-based multiprocessor, and found that gang scheduling is necessary for programs that require fine-grain synchronization. In their coscheduling implementation, they used a modified form of Ousterhout's matrix algorithm. Leutenegger and Vernon <ref> [LV90] </ref> simulate the effects of a multitude of different scheduling policies, including coscheduling, Tucker and Gupta's process control, and their own round-robin job (RRJob) Policy.
Reference: [ML91] <author> Matt M. Mutka and Miron Livny. </author> <title> The available capacity of a privately owned workstation environment. Performance Evaluation, </title> <booktitle> 12(4) </booktitle> <pages> 269-84, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Moreover, the processors are connected via a high-speed network, such as FDDI. User-level network interfaces are becoming more common, allowing low-latency end-to-end communication. The realization is that there is a definite coalescence of these formerly distinct technologies. Studies have indicated that workstations in such an environment are often idle <ref> [ML91] </ref>, [LV90], [DO91], [Nic87], with the typical user busy roughly one-third of the day. Even then, only a small portion of their workstation's cycles are utilized, making a large quantity of potential computing available. <p> The implementors of the Sprite system estimate that 66-78% of their workstations are free at any particular moment [DO91]. Lastly, Mutka and Livny used the Condor system to perform an extended study on a group of VAXstations, and found that 75% were normally available <ref> [ML91] </ref>. To gain further insight on the interaction between users and their workstations, we have instrumented a number of machines locally. Machines are assumed to be idle when there has been no keyboard or mouse activity for 5 minutes, and the load average is sufficiently low (below 0.3).
Reference: [Nic87] <author> David Nichols. </author> <title> Using idle workstations in a shared computing environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 5-12, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: User-level network interfaces are becoming more common, allowing low-latency end-to-end communication. The realization is that there is a definite coalescence of these formerly distinct technologies. Studies have indicated that workstations in such an environment are often idle [ML91], [LV90], [DO91], <ref> [Nic87] </ref>, with the typical user busy roughly one-third of the day. Even then, only a small portion of their workstation's cycles are utilized, making a large quantity of potential computing available. <p> These observations lead to the conclusion that a large amount of cycles are free; that is, they are available for other tasks, such as running parallel programs. Studies on the availability of idle workstations have supported this conclusion. Nichols <ref> [Nic87] </ref> finds that roughly 50-70 workstations out of an aggregate 350 machines were unused at any given time. Theimer and Lantz state that roughly one third of 70 Sun workstations were completely idle, even during the busiest times of the day [TL89].
Reference: [Ous82] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Third International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: A machine may have powerful microprocessors and a high-speed network, but poor choice of a scheduling algorithm will lead to sub-optimal system performance. 1 Many efforts have focused on this very problem. Early on, the idea of coscheduling 1 was proposed <ref> [Ous82] </ref>. In this scheme, processes of a parallel program are simultaneously scheduled across a set of processors. By doing so, communication and synchronization will be more efficient. <p> Alternatively, our study is on a message-passing architecture (the CM-5) and with a larger number of processors. The idea of coscheduling was first introduced in the early 80's <ref> [Ous82] </ref>. The basic premise is simple: try to schedule as many processes of the same job at the same time across processors. If all processes of a program ran during a particular time-slice, it was said to be coscheduled. Ousterhout assumed coscheduling was desirable, and simulated three different coscheduling algorithms.
Reference: [Sun90] <author> V.S. Sunderam. </author> <title> Pvm: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: The same is true for synchronization. Earlier efforts such as PVM <ref> [Sun90] </ref> could only successfully handle coarse-grained parallel programs. This was an artifact of the design of PVM; placing it on top of a high-latency communications protocol (i.e. TCP/IP) precludes support for finer granularities. <p> While forays into the realm of merging parallelism and the local area network environment have been made, no effort has been directed at bringing fine-grained parallel applications onto the desktop. The classic example of this is the Parallel Virtual Machine <ref> [Sun90] </ref>, better known as PVM. This is a tool which allows users to run parallel applications on a heterogeneous network of machines. While this allows programs with many separate processes to be run, it ignores the possibility of running jobs that need to communicate and/or synchronize frequently.
Reference: [TG89] <author> Andy Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <journal> Operating Systems Review, </journal> <volume> 23(5) </volume> <pages> 159-66, </pages> <year> 1989. </year>
Reference-contexts: Among these, space-partitioning schemes seem to have won strongest support [GTU91], [CDD + 91]. One example of this is Tucker and Gupta's process control method <ref> [TG89] </ref>, which attempts to match the number of processes your parallel program needs to run to the portion of the machine which it is allocated.
Reference: [TL89] <author> Marvin M. Theimer and Keith A. Lantz. </author> <title> Finding idle machines in a workstation-based distributed system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1444-57, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Nichols [Nic87] finds that roughly 50-70 workstations out of an aggregate 350 machines were unused at any given time. Theimer and Lantz state that roughly one third of 70 Sun workstations were completely idle, even during the busiest times of the day <ref> [TL89] </ref>. The implementors of the Sprite system estimate that 66-78% of their workstations are free at any particular moment [DO91]. Lastly, Mutka and Livny used the Condor system to perform an extended study on a group of VAXstations, and found that 75% were normally available [ML91].
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> (Also available as Technical Report UCB/CSD 92/675, </note> <institution> CS Div., University of California at Berkeley). </institution> <month> 25 </month>
Reference-contexts: On the CM-5, Split-C uses Active Messages as its message passing layer <ref> [vECGS92] </ref>. In this section, we will briefly describe Active Messages and Split-C. 2.3.1 CM-5 The CM-5 is a distributed memory multiprocessor that can have up to 1K nodes [L + 92].
References-found: 19

