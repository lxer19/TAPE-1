URL: ftp://theory.lcs.mit.edu/pub/people/danar/methods-ml.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~danar/papers.html
Root-URL: 
Title: An Experimental and Theoretical Comparison of Model Selection Methods on simple model selection problems, the
Author: Michael Kearns Yishay Mansour Andrew Y. Ng Dana Ron Y. Mansour, A. Ng and D. Ron 
Note: Even  This research was done while  were visiting AT&T Bell Laboratories.  
Address: Murray Hill, New Jersey  Tel Aviv, Israel  Pittsburgh, Pennsylvania  Cambridge, MA  
Affiliation: AT&T Bell Laboratories  Tel Aviv University  Carnegie Mellon University  MIT  
Abstract: We investigate the problem of model selection in the setting of supervised learning of boolean functions from independent random examples. More precisely, we compare methods for finding a balance between the complexity of the hypothesis chosen and its observed error on a random training sample of limited size, when the goal is that of minimizing the resulting generalization error. We undertake a detailed comparison of three well-known model selection methods | a variation of Vapnik's Guaranteed Risk Minimization (GRM), an instance of Rissanen's Minimum Description Length Principle (MDL), and cross validation (CV). We introduce a general class of model selection methods (called penalty-based methods) that includes both GRM and MDL, and provide general methods for analyzing such rules. We provide both controlled experimental evidence and formal theorems to support the following conclusions: * The class of penalty-based methods is fundamentally handicapped in the sense that there exist two types of model selection problems for which every penalty-based method must incur large generalization error on at least one, while CV enjoys small generalization error Despite the inescapable incomparability of model selection methods under certain circumstances, we conclude with a discussion of our belief that the balance of the evidence provides specific reasons to prefer CV to other methods, unless one is in possession of detailed problem-specific information. on both.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. R. Barron and T. M. </author> <title> Cover. Minimum complexity density estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37 </volume> <pages> 1034-1054, </pages> <year> 1991. </year>
Reference-contexts: Various types of analysis have been used to judge the performance of particular algorithms, including asymptotic consistency in the statistical sense [17, 16], asymptotic optimality under coding-theoretic measures [12], and more seldom, rates of convergence for the generalization error <ref> [1] </ref>. Perhaps surprisingly, despite the many proposed solutions for model selection and the diverse methods of analysis, direct comparisons between the different proposals (either experimental or theoretical) are rare. <p> In this model selection problem (which we shall refer to as the intervals model selection problem), the input domain is simply the real line segment <ref> [0; 1] </ref>, and the hypothesis class F d is simply the class of all boolean functions over [0; 1] in which we allow at most d alternations of label; thus F d is the class of all binary step functions with at most d=2 steps. <p> In this model selection problem (which we shall refer to as the intervals model selection problem), the input domain is simply the real line segment <ref> [0; 1] </ref>, and the hypothesis class F d is simply the class of all boolean functions over [0; 1] in which we allow at most d alternations of label; thus F d is the class of all binary step functions with at most d=2 steps. For the experiments, the underlying learning algorithm L that we have implemented performs training error minimization. <p> "underfit" the sample S, meaning that F d is not sufficiently expressive to capture the underlying regularities of f exposed by S, and for large d, the functions in VS (d) "overfit" the sample S. 4 The sample S was generated using the target function in F 100 that divides <ref> [0; 1] </ref> into 100 segments of equal width 1=100 and alternating label. (Details of the algorithm and the experimental results of the paper are provided in the Appendix.) In Figure 1 we plot *(d) (which we can calculate exactly, since we have chosen the target function) when S consists of m <p> The actual rule given in Equation (9) is slightly more complex than this, and reflects a refined bound on j^*(d) *(d)j that varies from d=m for ^*(d) close to 0 to p d=m otherwise. The next algorithm we consider, the Minimum Description Length Principle (MDL) <ref> [10, 11, 12, 1, 9] </ref> has rather different origins than GRM. MDL is actually a broad class of algorithms with a common information-theoretic motivation, each algorithm determined by the choice of a specific coding scheme for both functions and their training errors. <p> It is not difficult to see that under the assumption that the inputs are uniformly distributed in <ref> [0; 1] </ref>, this can be replaced by discretizing [0; 1] using a grid of resolution 1=p (m), for some polynomial p (), and using the grid points to describe the switches of h. 6 H (^*(h)). <p> It is not difficult to see that under the assumption that the inputs are uniformly distributed in <ref> [0; 1] </ref>, this can be replaced by discretizing [0; 1] using a grid of resolution 1=p (m), for some polynomial p (), and using the grid points to describe the switches of h. 6 H (^*(h)). <p> Here fl 2 <ref> [0; 1] </ref> is a parameter of the CV algorithm whose tuning we discuss briefly later. <p> The experimental behavior we observe foreshadows a number of important themes that we shall revisit in our formal results. We begin with Figure 3. To obtain this figure, a training sample was generated from the uniform input distribution and labeled according to an intervals function over <ref> [0; 1] </ref> consisting of 100 intervals of alternating label and equal width 8 ; the sample was corrupted with noise rate = 0:2. <p> Third, the form of the bound is suggestive of some of the behavior seen in the experimental results. Our search for a bound of this type was inspired by work of Barron and Cover <ref> [1] </ref>. Barron and Cover give bounds of a similar form (which they call the index of resolution) on the generalization error of MDL in the context of density estimation. For a given penalty-based algorithm, let G be the function that determines the algorithm as defined in Equation (12). <p> Theorem 1 Let (fF d g; f; D; L) be an instance of the model selection problem in which L performs training error minimization, and where d is the VC dimension of F d . Let G : <ref> [0; 1] </ref> fi &lt; ! &lt; be a function that is continuous and increasing in both its arguments, and let * G (m) denote the expected generalization error of the penalty-based model selection algorithm ~ d = argmin d fG (^*(d); d=m)g on 11 a training sample of size m. <p> For Theorem 2 we need the following definition. We say that a learning algorithm L is adequate if it has the following property. There exists a function L : N fi N fi <ref> [0; 1] </ref> ! [0; 1], such that for every given ffi, with probability at least 1 ffi, j^* L (d) ^* opt (d)j L (d; m; ffi) for all d, where ^* opt = min h2F d f^*(h)g. <p> For Theorem 2 we need the following definition. We say that a learning algorithm L is adequate if it has the following property. There exists a function L : N fi N fi <ref> [0; 1] </ref> ! [0; 1], such that for every given ffi, with probability at least 1 ffi, j^* L (d) ^* opt (d)j L (d; m; ffi) for all d, where ^* opt = min h2F d f^*(h)g. <p> Theorem 2 Let (fF d g; f; D; L) be an instance of the model selection problem in which L is an adequate learning algorithm, and where d is the VC dimension of F d . Let G : <ref> [0; 1] </ref> fi &lt; ! &lt; be a function that is continuous and increasing in both its arguments, and let * G (m) denote the generalization error of the penalty-based model selection algorithm ~ d = argmin d fG (^*(d); d=m)g on a training sample of size m. <p> Theorem 3 Let (fF d g; f; D; L) be an instance of the model selection problem in which L performs training error minimization, and where d is the VC dimension of F d . Let G : <ref> [0; 1] </ref> fi &lt; ! &lt; be a function that is continuous and increasing in both its arguments, and let * G (m) denote the expected 15 generalization error of the penalty-based model selection algorithm ~ d = argmin d fG (^*(d); d=m)g on a training sample of size m, and <p> Now we describe the two problems used in the proof, and briefly argue why they have the desired properties; We are in fact already familiar with the first problem: the class F d 1 is simply the class of all d-alternation functions over <ref> [0; 1] </ref>, the target function is the 0-alternation function that classifies all examples as negative, the input distribution D 1 is uniform over [0; 1], and we may choose any constant noise rate 1 . <p> the desired properties; We are in fact already familiar with the first problem: the class F d 1 is simply the class of all d-alternation functions over <ref> [0; 1] </ref>, the target function is the 0-alternation function that classifies all examples as negative, the input distribution D 1 is uniform over [0; 1], and we may choose any constant noise rate 1 . <p> A prime example for further investigation would be distribution learning with respect to the Kullback-Liebler divergence (log loss), where * opt -based upper bounds for MDL-like rules are already known <ref> [1] </ref>, yet there also exist phase transitions for natural problems [4]. Acknowledgements We give warm thanks to Yoav Freund and Ronitt Rubinfeld for their collaboration on various portions of the work presented here, and for their insightful comments. Thanks to Sebastian Seung and Vladimir Vapnik for interesting and helpful conversations. 21
Reference: [2] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <editor> In David S. Touret-zky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference: [3] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: This takes log m bits; dividing by m to normalize, we obtain (1=m) log m H (d=m) <ref> [3] </ref>, where H () is the binary entropy function (i.e. H (p) = (p log p + (1 p) log (1 p)) ).
Reference: [4] <author> D. Haussler, M. Kearns, H.S. Seung, and N. Tishby. </author> <title> Rigourous learning curve bounds from statistical mechanics. </title> <booktitle> In Proceedings of the Seventh Annual ACM Confernce on Computational Learning Theory, </booktitle> <pages> pages 76-87, </pages> <year> 1994. </year>
Reference-contexts: First, and perhaps most importantly, * m ((1 fl)m) may be considerably larger than * m (m). This could either be due to properties of the underlying learning algorithm L, or due to inherent phase transitions (sudden decreases) in the optimal information-theoretic learning curve <ref> [14, 4] </ref> | thus, in an extreme case, it could be that the generalization error that can be achieved within some class F d by training on m examples is close to 0, but that the optimal generalization error that can be achieved in F d by training on a slightly <p> We believe that giving similarly general bounds for any penalty-based algorithm would be extremely difficult, if not impossible. The reason for this belief arises from the diversity of learning curve behavior documented by the statistical mechanics approach <ref> [14, 4] </ref>, among other sources. <p> A prime example for further investigation would be distribution learning with respect to the Kullback-Liebler divergence (log loss), where * opt -based upper bounds for MDL-like rules are already known [1], yet there also exist phase transitions for natural problems <ref> [4] </ref>. Acknowledgements We give warm thanks to Yoav Freund and Ronitt Rubinfeld for their collaboration on various portions of the work presented here, and for their insightful comments. Thanks to Sebastian Seung and Vladimir Vapnik for interesting and helpful conversations. 21
Reference: [5] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: By definition of CV, ~ d is chosen according to ~ d = argmin d f^* S 00 ( ~ h 0 d )g. For a given ~ h 0 d , we know by Hoeffding's Inequality <ref> [5] </ref> that for any ff &gt; 0, P r fi d ) ^* S 00 ( ~ h 0 fi fi &gt; ff &lt; 2 exp (2ff 2 flm): (44) The probability that some ~ h 0 d deviates by more than ff from its expected value is therefore bounded by
Reference: [6] <author> M. Kearns. </author> <title> A bound on the error of cross validation, with consequences for the training-test split. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> The MIT Press, </publisher> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: For a detailed study of this line of inquiry, see Kearns <ref> [6] </ref>. Here we simply note that Equation (48) tells us that in cases for which the power law decay of generalization error within each F d holds approximately, the performance of CV will be competitive with GRM or any other algorithm.
Reference: [7] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proceedings of the 5th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 341-352. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference: [8] <author> L. Pitt and L. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference: [9] <author> J. R. Quinlan and R. L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80(3) </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: The actual rule given in Equation (9) is slightly more complex than this, and reflects a refined bound on j^*(d) *(d)j that varies from d=m for ^*(d) close to 0 to p d=m otherwise. The next algorithm we consider, the Minimum Description Length Principle (MDL) <ref> [10, 11, 12, 1, 9] </ref> has rather different origins than GRM. MDL is actually a broad class of algorithms with a common information-theoretic motivation, each algorithm determined by the choice of a specific coding scheme for both functions and their training errors.
Reference: [10] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: The actual rule given in Equation (9) is slightly more complex than this, and reflects a refined bound on j^*(d) *(d)j that varies from d=m for ^*(d) close to 0 to p d=m otherwise. The next algorithm we consider, the Minimum Description Length Principle (MDL) <ref> [10, 11, 12, 1, 9] </ref> has rather different origins than GRM. MDL is actually a broad class of algorithms with a common information-theoretic motivation, each algorithm determined by the choice of a specific coding scheme for both functions and their training errors.
Reference: [11] <author> J. Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: The actual rule given in Equation (9) is slightly more complex than this, and reflects a refined bound on j^*(d) *(d)j that varies from d=m for ^*(d) close to 0 to p d=m otherwise. The next algorithm we consider, the Minimum Description Length Principle (MDL) <ref> [10, 11, 12, 1, 9] </ref> has rather different origins than GRM. MDL is actually a broad class of algorithms with a common information-theoretic motivation, each algorithm determined by the choice of a specific coding scheme for both functions and their training errors.
Reference: [12] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry, </title> <booktitle> volume 15 of Series in Computer Science. World Scientific, </booktitle> <year> 1989. </year>
Reference-contexts: Many model selection algorithms have been proposed in the literature of several different research communities, too many to productively survey here. Various types of analysis have been used to judge the performance of particular algorithms, including asymptotic consistency in the statistical sense [17, 16], asymptotic optimality under coding-theoretic measures <ref> [12] </ref>, and more seldom, rates of convergence for the generalization error [1]. Perhaps surprisingly, despite the many proposed solutions for model selection and the diverse methods of analysis, direct comparisons between the different proposals (either experimental or theoretical) are rare. <p> In Section 3, we introduce the three model selection algorithms we examine in the experiments: Vapnik's Guaranteed Risk Minimization (GRM) [17], an instantiation of Rissanen's Minimum Description Length Principle (MDL) <ref> [12] </ref>, and Cross Validation (CV). Section 4 describes our controlled experimental comparison of the three algorithms. <p> The actual rule given in Equation (9) is slightly more complex than this, and reflects a refined bound on j^*(d) *(d)j that varies from d=m for ^*(d) close to 0 to p d=m otherwise. The next algorithm we consider, the Minimum Description Length Principle (MDL) <ref> [10, 11, 12, 1, 9] </ref> has rather different origins than GRM. MDL is actually a broad class of algorithms with a common information-theoretic motivation, each algorithm determined by the choice of a specific coding scheme for both functions and their training errors.
Reference: [13] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 259-265, </pages> <year> 1994. </year>
Reference-contexts: The reverse phenomenon (reluctance to code) is experienced for MDL with an increased complexity penalty multiplier, as demonstrated by Figures 24 and 25. This observation seems to echo recent results <ref> [13, 19] </ref> which essentially prove that no learning algorithm can perform well on all problems. <p> Furthermore, for the problems we choose, CV can in fact succeed on both. Thus we are doing more than simply demonstrating that no model selection algorithm can succeed universally for all target functions, a statement that is intuitively obvious and has been made more formal in recent papers <ref> [13, 19] </ref>. We are in fact identifying a weakness that is special to penalty-based algorithms. However, as we have discussed previously, the use of CV is not without pitfalls of its own.
Reference: [14] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review, </journal> <volume> A45:6056-6091, </volume> <year> 1992. </year>
Reference-contexts: First, and perhaps most importantly, * m ((1 fl)m) may be considerably larger than * m (m). This could either be due to properties of the underlying learning algorithm L, or due to inherent phase transitions (sudden decreases) in the optimal information-theoretic learning curve <ref> [14, 4] </ref> | thus, in an extreme case, it could be that the generalization error that can be achieved within some class F d by training on m examples is close to 0, but that the optimal generalization error that can be achieved in F d by training on a slightly <p> We believe that giving similarly general bounds for any penalty-based algorithm would be extremely difficult, if not impossible. The reason for this belief arises from the diversity of learning curve behavior documented by the statistical mechanics approach <ref> [14, 4] </ref>, among other sources.
Reference: [15] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: Notice that an ideal penalty-based algorithm would obey G (^*(d); d=m) *(d) (or at least G (^*(d); d=m) and *(d) would be minimized by the same value of d). The third model selection algorithm that we examine has a different spirit than the penalty-based algorithms. In cross validation (CV) <ref> [15, 16] </ref>, rather than attempt to reconstruct *(d) from ^*(d) and d, we instead settle for a "worse" *(d) (in a sense made precise shortly) that we can directly estimate.
Reference: [16] <author> M. Stone. </author> <title> Asymptotics for and against cross-validation. </title> <journal> Biometrika, </journal> <volume> 64(1) </volume> <pages> 29-35, </pages> <year> 1977. </year>
Reference-contexts: Many model selection algorithms have been proposed in the literature of several different research communities, too many to productively survey here. Various types of analysis have been used to judge the performance of particular algorithms, including asymptotic consistency in the statistical sense <ref> [17, 16] </ref>, asymptotic optimality under coding-theoretic measures [12], and more seldom, rates of convergence for the generalization error [1]. Perhaps surprisingly, despite the many proposed solutions for model selection and the diverse methods of analysis, direct comparisons between the different proposals (either experimental or theoretical) are rare. <p> Notice that an ideal penalty-based algorithm would obey G (^*(d); d=m) *(d) (or at least G (^*(d); d=m) and *(d) would be minimized by the same value of d). The third model selection algorithm that we examine has a different spirit than the penalty-based algorithms. In cross validation (CV) <ref> [15, 16] </ref>, rather than attempt to reconstruct *(d) from ^*(d) and d, we instead settle for a "worse" *(d) (in a sense made precise shortly) that we can directly estimate.
Reference: [17] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year> <month> 22 </month>
Reference-contexts: Many model selection algorithms have been proposed in the literature of several different research communities, too many to productively survey here. Various types of analysis have been used to judge the performance of particular algorithms, including asymptotic consistency in the statistical sense <ref> [17, 16] </ref>, asymptotic optimality under coding-theoretic measures [12], and more seldom, rates of convergence for the generalization error [1]. Perhaps surprisingly, despite the many proposed solutions for model selection and the diverse methods of analysis, direct comparisons between the different proposals (either experimental or theoretical) are rare. <p> We also introduce the specific model selection problem that will be the basis for our experimental results, and describe an initial experiment demonstrating that the problem is nontrivial. In Section 3, we introduce the three model selection algorithms we examine in the experiments: Vapnik's Guaranteed Risk Minimization (GRM) <ref> [17] </ref>, an instantiation of Rissanen's Minimum Description Length Principle (MDL) [12], and Cross Validation (CV). Section 4 describes our controlled experimental comparison of the three algorithms. <p> model selection problem consists of a tuple (fF d g; f; D; L), where fF d g is the hypothesis function class sequence, f is the target function, D is the input distribution, and L is the underlying learning 2 Such a nested sequence is called a structure by Vapnik <ref> [17] </ref> and is sometimes, but not always, the setting in which model selection methods are examined. 3 algorithm. <p> is their attempt to construct an approximation to *(d) solely on the basis of the training error ^*(d) and the complexity d, often by trying to "correct" ^*(d) by the amount that it underestimates *(d) through the addition of a "complexity penalty" term. 5 In Vapnik's Guaranteed Risk Minimization (GRM) <ref> [17] </ref>, ~ d is chosen according to the rule ~ d = argmin d q where we have assumed that d is the Vapnik-Chervonenkis dimension [17, 18] of the class F d ; this assumption holds in the intervals model selection problem. <p> to "correct" ^*(d) by the amount that it underestimates *(d) through the addition of a "complexity penalty" term. 5 In Vapnik's Guaranteed Risk Minimization (GRM) [17], ~ d is chosen according to the rule ~ d = argmin d q where we have assumed that d is the Vapnik-Chervonenkis dimension <ref> [17, 18] </ref> of the class F d ; this assumption holds in the intervals model selection problem. <p> The origin of this rule can be summarized roughly as follows (where for sake of simplicity we use the term "with high probability" to mean with probability 1 ffi over the draw of S, at a cost of a factor of log (1=ffi) in the bounds): it has been shown <ref> [17] </ref> that with high probability for every d and for every h 2 F d , p d=m is an upper bound on j^*(h) *(h)j and hence j^*(d) *(d)j p d=m. <p> We return to this issue after the proof of Theorem 1. In order to prove Theorem 1, we shall need to following uniform convergence bound which is due to Vapnik <ref> [17] </ref>. Uniform Convergence Bound Let F d be a hypothesis class with VC dimension d &lt; m. Then, for any given ffi &gt; 0, with probability at least 1 ffi, j*(h) ^*(h)j &lt; 2 u t d ln 2m ffi (19) for every h 2 F d . <p> Thus, with probability at least 1ffi, the above holds for all d &lt; m. For d m we can use the trivial bound that for every h, j*(h) ^*(h)j 1, and together we have that with probability 10 In fact, Vapnik (in <ref> [17] </ref>, page 160) gives a more general statement concerning the uniform estimation of proba bilities from their frequencies in a class of events of limited VC dimension. 12 at least 1 ffi, for every d, and for all h 2 F d , j*(h) ^*(h)j &lt; fi (d; m; ffi), where
Reference: [18] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: to "correct" ^*(d) by the amount that it underestimates *(d) through the addition of a "complexity penalty" term. 5 In Vapnik's Guaranteed Risk Minimization (GRM) [17], ~ d is chosen according to the rule ~ d = argmin d q where we have assumed that d is the Vapnik-Chervonenkis dimension <ref> [17, 18] </ref> of the class F d ; this assumption holds in the intervals model selection problem.

References-found: 18

