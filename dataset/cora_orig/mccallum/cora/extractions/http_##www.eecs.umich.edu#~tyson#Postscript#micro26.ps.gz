URL: http://www.eecs.umich.edu/~tyson/Postscript/micro26.ps.gz
Refering-URL: http://www.eecs.umich.edu/~tyson/publications.html
Root-URL: http://www.cs.umich.edu
Email: email: tyson@cs.ucdavis.edu, farrens@cs.ucdavis.edu  
Phone: tel: (916) 752-9678, fax: (916) 752-4767  
Title: d d Techniques for Extracting Instruction Level Parallelism on MIMD Architectures  
Author: Gary Tyson and Matthew Farrens 
Address: Davis, CA 95616  
Affiliation: Computer Science Department University of California, Davis  
Abstract: Extensive research has been done on extracting parallelism from single instruction stream processors. This paper presents some results of our investigation into ways to modify MIMD architectures to allow them to extract the instruction level parallelism achieved by current superscalar and VLIW machines. A new architecture is proposed which utilizes the advantages of a multiple instruction stream design while addressing some of the limitations that have prevented MIMD architectures from performing ILP operation. A new code scheduling mechanism is described to support this new architecture by partitioning instructions across multiple processing elements in order to exploit this level of parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [AhS] <author> A. V. Aho, R. Sethi and J. D. Ullman, </author> <booktitle> ``Compilers Principles, Techniques and Tools'', </booktitle> <publisher> Addison-Wesley Publishing, </publisher> <pages> pp. 644. </pages>
Reference-contexts: The detection of induction variables is a well understood problem. The algorithm used in this compiler is derived from <ref> [AhS] </ref> (Algorithm 10.9). Once the control state of the machine has been modified to support loop operations, it is a simple modification to handle the calculation of induction variables used in the loop.
Reference: [AKPW83] <author> J. R. Allen, K. Kennedy, C. Porterfield and J. Warren, </author> <title> ``Conversion of control dependencies to data dependencies'', </title> <booktitle> Proceeding of the 10th ACM Symposium on Principles of Programming Languages(January 1983), </booktitle> <pages> pp. 177-189. </pages>
Reference-contexts: At the instruction issue stage, if a queue is specified as a source input and that queue is currently empty, that instruction is delayed until all required input operands are available. There are three types of MISC instructions: predicated operations <ref> [AKPW83] </ref>, vector operations, and sentinel operations (which use a sentinel value to terminate iterations of the instruction). Predicated ALU/FPU operations perform all scalar operations as well as allow conditional operations to be specified concisely.
Reference: [AuSo92] <author> T. Austin and G. Sohi, </author> <title> ``Dynamic Dependency Analysis of Ordinary Programs'', </title> <booktitle> Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <volume> vol. 20, no. </volume> <month> 2 (May 19-21, </month> <year> 1992), </year> <pages> pp. 342-351. </pages>
Reference-contexts: Several studies [JoWa89, TjFl70] indicate that compilers using simple scheduling techniques are capable of identifying 2-3 independent instructions per cycle. Other studies <ref> [AuSo92, BuYP91] </ref> suggest that even more parallelism can be found if the compiler's scheduler is capable of performing extensive code motion. In this paper, we will present a brief overview of single and multiple instruction stream approaches to multiple issue processor design.
Reference: [BeDa91] <author> M. E. Benitez and J. W. Davidson, </author> <title> ``Code Generation for Streaming: an Access/Execute Mechanism'', </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA (April 8-11, </address> <year> 1991), </year> <pages> pp. 132-141. </pages>
Reference-contexts: For those optimization that are unique to MISC, or where existing techniques require modification (e.g. register allocation incorporating queues), care has been taken to maintain the same level of complexity found in current optimizers. The very portable C compiler (vpcc) <ref> [BeDa91] </ref> under development at the University of Virginia is used as the base compiler for MISC. The code generator translates a dependency graph representation of a program (in Register Transfer List (RTL) form) to produce parallel machine code.
Reference: [BuYP91] <author> M. Butler, T. Yeh and Y. Patt, </author> <title> ``Single Instruction Stream Parallelism is Greater than Two'', </title> <booktitle> Proceedings of the Eighteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Toronto, Canada (May 27-30, </address> <year> 1991), </year> <pages> pp. 276-286. </pages>
Reference-contexts: Several studies [JoWa89, TjFl70] indicate that compilers using simple scheduling techniques are capable of identifying 2-3 independent instructions per cycle. Other studies <ref> [AuSo92, BuYP91] </ref> suggest that even more parallelism can be found if the compiler's scheduler is capable of performing extensive code motion. In this paper, we will present a brief overview of single and multiple instruction stream approaches to multiple issue processor design.
Reference: [CGKP87] <author> G. L. Craig, J. R. Goodman, R. H. Katz, A. R. Pleszkun, K. Ramachandran, J. Sayah and J. E. Smith, </author> <title> ``PIPE: A High Performance VLSI Processor Implementation'', </title> <journal> Journal of VLSI and Computer Systems, </journal> <volume> vol. </volume> <month> 2 </month> <year> (1987). </year>
Reference-contexts: The ZS-1 [SDVK87] and WM [Wulf92] systems operate in a decoupled manner while receiving instructions from a single instruction stream. Their architectural component descriptions are similar to those of Split Register superscalar designs [Site93, SCHP92]. The PIPE machine, in contrast, [GHLP85] consists of two PIPE processors <ref> [CGKP87] </ref> which run asynchronously, each with their own instruction stream, and cooperate on the execution of a single task. 3. Exploiting ILP on a MIMD architecture Parallelism in a single instruction stream architecture resides primarily at the instruction level, and is a well-studied problem [JoWa89, Wall91].
Reference: [FaPl91] <author> M. Farrens and A. Pleszkun, </author> <title> ``Overview of the PIPE Processor Implementation'', </title> <booktitle> Proceedings of the 24th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kapaa, Kauai (January 9-11, </address> <year> 1991), </year> <pages> pp. 433-443. </pages>
Reference-contexts: This allows the issue logic to proceed without interrupt through short segments of conditionally executed code by conditionally completing instead of branching around code. In the case of control flow operations, the dest field is used as a constant to determine the number of delayed branch slots <ref> [FaPl91] </ref> to be filled. The address of the branch is calculated as the sum of the src1 and src2 operands, and the src3 operand specifies the register to be tested. Vector instructions use the third source operand (src3) to specify a vector count.
Reference: [GHLP85] <author> J. R. Goodman, J. T. Hsieh, K. Liou, A. R. Pleszkun, P. B. Schechter and H. C. Young, </author> <title> ``PIPE: a VLSI Decoupled Architecture'', </title> <booktitle> Proceedings of the Twelveth Annual International Symposium on Computer Architecture(June 1985), </booktitle> <pages> pp. 20-27. </pages>
Reference-contexts: The ZS-1 [SDVK87] and WM [Wulf92] systems operate in a decoupled manner while receiving instructions from a single instruction stream. Their architectural component descriptions are similar to those of Split Register superscalar designs [Site93, SCHP92]. The PIPE machine, in contrast, <ref> [GHLP85] </ref> consists of two PIPE processors [CGKP87] which run asynchronously, each with their own instruction stream, and cooperate on the execution of a single task. 3.
Reference: [JoWa89] <author> N. P. Jouppi and D. W. Wall, </author> <title> ``Available Instruction-Level Parallelism for Superscalar and Superpipelined Machines'', </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, Mass (April 3-6, </address> <year> 1989), </year> <pages> pp. 272-282. </pages>
Reference-contexts: The task of the scheduler in a multi-issue system is further complicated by the fact that while the latency of operational units and memory remain fixed, the number of instructions that must be scheduled in a period is increased by the width of the issue stage. Several studies <ref> [JoWa89, TjFl70] </ref> indicate that compilers using simple scheduling techniques are capable of identifying 2-3 independent instructions per cycle. Other studies [AuSo92, BuYP91] suggest that even more parallelism can be found if the compiler's scheduler is capable of performing extensive code motion. <p> Exploiting ILP on a MIMD architecture Parallelism in a single instruction stream architecture resides primarily at the instruction level, and is a well-studied problem <ref> [JoWa89, Wall91] </ref>. Extracting parallelism on a MIMD architecture, on the other hand, has traditionally been accomplished by partitioning the program into data independent portions and assigning them to separate processing elements, ignoring any other parallelism that might exist.
Reference: [Lam88] <author> M. S. Lam, </author> <title> ``Software Pipelining: An Effective Scheduling Technique for VLIW Machines'', </title> <booktitle> Proceedings of the ACM SIGPLAN Notices 1988 Conference on Programming Languages and Implementations(June 1988), </booktitle> <pages> pp. 318-328. </pages>
Reference-contexts: Superscalar designs can remove some of the restrictions imposed by single stream scheduling by regenerating some the dataflow information at the issue stage of the pipeline, but not without considerable hardware issue logic. Furthermore, software pipelining <ref> [Lam88] </ref> and loop unrolling schemes d d [WeSm87] have difficulty in efficiently scheduling instructions with variable latency dependencies. MISC avoids these scheduling problems by allowing operations with indeterminate latencies to transfer data between PEs.
Reference: [MLCH92] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank and R. A. Bringmann, </author> <title> ``Effective Compiler Support for Predicated Execution Using the Hyperblock'', </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <address> Portland, Oregon (December 1-4, </address> <year> 1992), </year> <pages> pp. 45-54. </pages>
Reference-contexts: However, in a multiple instruction stream machine such as MISC, a single PE can evaluate the test condition and broadcast the boolean result to all PEs, increasing the number of simple boolean tests evaluated during the execution of these loop semantics. Hyperblock transformation <ref> [MLCH92] </ref> also reduces the effects of branching by eliminating branching operations in favor of predicated execution.
Reference: [RaS92] <author> B. R. Rau, M. S. Schlansker and P. P. Tirumalai, </author> <title> ``Code Generation Schema for Modulo Scheduled Loops'', </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <address> Portland, Oregon (December 1-4, </address> <year> 1992), </year> <pages> pp. 158-169. </pages>
Reference-contexts: To provide a comparison with a similarly configured single instruction stream/multiple issue architecture, the loops were also hand compiled for a four-issue VLIW architecture based upon the version found in <ref> [RaS92] </ref>. This VLIW machine allows four instructions to be issued per clock cycle, and places no limitations on the type of instructions that can be issued. Furthermore, it assumes sufficient resources (e.g. register transfer bandwidth) to sustain a four instructions per cycle execution rate.
Reference: [Site93] <author> R. L. </author> <title> Sites, ``Alpha AXP Architecture'', </title> <journal> Communications of the ACM, </journal> <volume> vol. 36, no. </volume> <month> 2 (February, </month> <year> 1993), </year> <pages> pp. 33-44. </pages>
Reference-contexts: The ZS-1 [SDVK87] and WM [Wulf92] systems operate in a decoupled manner while receiving instructions from a single instruction stream. Their architectural component descriptions are similar to those of Split Register superscalar designs <ref> [Site93, SCHP92] </ref>. The PIPE machine, in contrast, [GHLP85] consists of two PIPE processors [CGKP87] which run asynchronously, each with their own instruction stream, and cooperate on the execution of a single task. 3.
Reference: [Smit82] <author> J. E. Smith, </author> <title> ``Decoupled Access/Execute Computer Architectures'', </title> <booktitle> Proceedings of the Ninth Annual International Symposium on Computer Architecture, </booktitle> <address> Austin, Texas (April 26-29, </address> <year> 1982), </year> <pages> pp. 112-119. </pages>
Reference-contexts: Furthermore, in order to transmit data among operational units by writing and then reading the contents of a register, the clocks on VLIW and superscalar processors must be synchronized. This requirement is relaxed with an explicit message passing approach. <ref> [Smit82] </ref> The greater flexibility found in a decoupled design allows both single and multiple instruction stream descriptions of a task. The ZS-1 [SDVK87] and WM [Wulf92] systems operate in a decoupled manner while receiving instructions from a single instruction stream.
Reference: [SDVK87] <author> J. E. Smith, G. E. Dermer, B. D. Vanderwarn, S. D. Klinger, C. M. Rozewski, D. L. Fowler, K. R. Scidmore and J. P. Laudon, </author> <title> ``The ZS-1 Central Processor'', </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Palo Alto, California (October 1987), </address> <pages> pp. 199-204. </pages>
Reference-contexts: This requirement is relaxed with an explicit message passing approach. [Smit82] The greater flexibility found in a decoupled design allows both single and multiple instruction stream descriptions of a task. The ZS-1 <ref> [SDVK87] </ref> and WM [Wulf92] systems operate in a decoupled manner while receiving instructions from a single instruction stream. Their architectural component descriptions are similar to those of Split Register superscalar designs [Site93, SCHP92].
Reference: [SCHP92] <author> C. Stephens, B. Cogswell, J. Heinlein, G. Palmer and J. P. Shen, </author> <title> ``Instruction Level Profiling and Evaluation of the IBM RS/6000'', </title> <booktitle> Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <volume> vol. 20, no. </volume> <month> 2 (May 19-21, </month> <year> 1992), </year> <pages> pp. 180-189. </pages>
Reference-contexts: The ZS-1 [SDVK87] and WM [Wulf92] systems operate in a decoupled manner while receiving instructions from a single instruction stream. Their architectural component descriptions are similar to those of Split Register superscalar designs <ref> [Site93, SCHP92] </ref>. The PIPE machine, in contrast, [GHLP85] consists of two PIPE processors [CGKP87] which run asynchronously, each with their own instruction stream, and cooperate on the execution of a single task. 3.
Reference: [TjFl70] <author> G. S. Tjaden and M. J. Flynn, </author> <title> ``Detection and Parallel Execution of Parallel Instructions'', </title> <journal> IEEE Transactions on Computer(May 1970), </journal> <pages> pp. 889-895. </pages>
Reference-contexts: The task of the scheduler in a multi-issue system is further complicated by the fact that while the latency of operational units and memory remain fixed, the number of instructions that must be scheduled in a period is increased by the width of the issue stage. Several studies <ref> [JoWa89, TjFl70] </ref> indicate that compilers using simple scheduling techniques are capable of identifying 2-3 independent instructions per cycle. Other studies [AuSo92, BuYP91] suggest that even more parallelism can be found if the compiler's scheduler is capable of performing extensive code motion.
Reference: [TyFP92] <author> G. Tyson, M. Farrens and A. Pleszkun, ``MISC: </author> <title> A Multiple Instruction Stream Computer'', </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <address> Portland, Oregon (December 1-4, </address> <year> 1992), </year> <pages> pp. 193-196. </pages>
Reference-contexts: MISC avoids these scheduling problems by allowing operations with indeterminate latencies to transfer data between PEs. The inherent asynchronous relationship among the PEs can compensate for the variability of the latency without affecting the execution rate of nondependent instructions. The MISC processor has been described in detail in <ref> [TyFP92] </ref>. A brief overview of MISC will be presented here, focusing on aspects of the architecture that will be featured in the code scheduling discussion later in the paper.
Reference: [TySF93] <author> G. S. Tyson, R. Shaw and M. Farrens, </author> <title> ``An Interactive Compiler Development System'', </title> <address> Tcl/Tk Workshop(June 10-11, </address> <year> 1993). </year>
Reference-contexts: The first 12 loops were compiled for both the MIPS and MISC architectures. The MIPS code was compiled using the cc compiler with optimization -O2, and the MISC code was generated using the IAGO <ref> [TySF93] </ref> compilation environment using the techniques discussed in section 5 of this paper.
Reference: [Wall91] <author> D. Wall, </author> <title> ``Limits of Instruction-Level Parallelism'', </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA (April 8-11, </address> <year> 1991), </year> <pages> pp. 176-189. </pages>
Reference-contexts: Exploiting ILP on a MIMD architecture Parallelism in a single instruction stream architecture resides primarily at the instruction level, and is a well-studied problem <ref> [JoWa89, Wall91] </ref>. Extracting parallelism on a MIMD architecture, on the other hand, has traditionally been accomplished by partitioning the program into data independent portions and assigning them to separate processing elements, ignoring any other parallelism that might exist.
Reference: [WeSm87] <author> S. Weiss and J. E. Smith, </author> <title> ``A Study of Scalar Compaction Techniques for Pipelined Supercomputers'', </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Palo Alto, California (October 1987), </address> <pages> pp. 105-109. </pages>
Reference-contexts: Superscalar designs can remove some of the restrictions imposed by single stream scheduling by regenerating some the dataflow information at the issue stage of the pipeline, but not without considerable hardware issue logic. Furthermore, software pipelining [Lam88] and loop unrolling schemes d d <ref> [WeSm87] </ref> have difficulty in efficiently scheduling instructions with variable latency dependencies. MISC avoids these scheduling problems by allowing operations with indeterminate latencies to transfer data between PEs. The inherent asynchronous relationship among the PEs can compensate for the variability of the latency without affecting the execution rate of nondependent instructions.
Reference: [Wulf92] <author> W. Wulf, </author> <title> ``Evaluation of the WM Architecture'', </title> <booktitle> Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <volume> vol. 20, no. </volume> <month> 2 (May 19-21, </month> <year> 1992), </year> <pages> pp. 382-390. </pages>
Reference-contexts: This requirement is relaxed with an explicit message passing approach. [Smit82] The greater flexibility found in a decoupled design allows both single and multiple instruction stream descriptions of a task. The ZS-1 [SDVK87] and WM <ref> [Wulf92] </ref> systems operate in a decoupled manner while receiving instructions from a single instruction stream. Their architectural component descriptions are similar to those of Split Register superscalar designs [Site93, SCHP92].
References-found: 22

