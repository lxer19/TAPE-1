URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-98-09.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+98-09
Root-URL: 
Email: E-mail: bernhard@ai.univie.ac.at  
Phone: Phone: +43 1 533 6112 17  
Title: Inducing Small and Accurate Decision Trees  Keywords: Decision-tree learning, Pruning, Noise Multiple  
Author: Bernhard Pfahringer 
Note: submission statement: not applicable  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence,  
Abstract: Recently, the quality improvement of decision trees and classifiers in general achievable by extended search efforts has received quite some attention in the literature. Contrary to the construction of ensembles of classifiers, which aims at improving overall predictive accuracy, our approach aims at improving the intelligibility of a single classifier. Our goal is the induction of a single, small, yet accurate decision tree. We describe a simple prepruning method (PreC4) that uses cross-validation to determine an appropriate stopping point for tree construction in a reliable manner. In addition to comparison with C4.5, PreC4 is also evaluated against both Robust-C4 and the combination of the two methods (Robust-PreC4). Evaluation domains comprise two artificial problems as well as a selection of small- and medium-sized UCI databases. Experimental results confirm that trees generated by both PreC4 and Robust-C4 are reasonably accurate but at the same time consistently smaller than trees generated by C4.5. PreC4 usually achieves a much larger tree-size reduction than Robust-C4 does. Interestingly, the combined procedure Robust-PreC4 does not perform as well. Trees generated by Robust-PreC4 are the smallest ones overall, but unfortunately they are also less accurate in some domains where they seemingly underfit the respective target concepts. In summary, PreC4 induces much smaller trees of comparable predictive accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [Auer et al. 95] <author> Auer P., Maass W., Holte R.: </author> <title> Theory and Applications of Agnostic PAC-Learning with Small Decision Trees, </title> <editor> in Prieditis A. and Russell S.(eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1995. </year>
Reference-contexts: However, some recent studies shed doubt onto this hope. Standard post-pruning algorithms might not be effective or radical enough in pruning overly complex trees, as significantly simpler trees often exhibit comparable predictive accuracy. In <ref> [Holte 93, Auer et al. 95] </ref> very simple decision-trees (being restricted to one or two levels respectively) are shown to perform well in quite a few domains. <p> Also the choice of one 10-fold cross-validation run for estimating the target error is rather ad-hoc. Such decisions should be investigated in more detail. * As has already been proven by both the 1R [Holte 93] and T2 <ref> [Auer et al. 95] </ref> systems, small decision trees can exhibit good predictive accuracy. But we do believe that PreC4's more dynamic bias which in a sense guesses the right decision tree size from the given data will be more appropriate in a general learner.
Reference: [Blockeel & DeRaedt 97] <author> Blockeel H., DeRaedt L.: </author> <title> Top-down induction of logical decision trees, </title> <type> Technical Report Report CW 247, </type> <institution> Katholieke Uni-versiteit Leuven, Belgium, </institution> <year> 1997. </year>
Reference-contexts: Furthermore, we plan to adapt the key ideas of PreC4 to learning in a first-order framework (like <ref> [Blockeel & DeRaedt 97] </ref> or [Kramer 96]), where due to vastly larger concept spaces effective pruning is even more essential than it already is in a propositional setting.
Reference: [Breiman et al. 84] <author> Breiman L., Friedman J.H., Olshen R.A., Stone C.J.: </author> <title> Classification and Regression Trees, </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <publisher> The Wadsworth Statistics/Probability Series, </publisher> <year> 1984. </year>
Reference-contexts: chi-square testing), or some form of error estimate (e.g. pessimistic error estimates [Quinlan 93], or reduced error pruning), or it might also take into account the complexity of the induced tree (e.g. minimum description length calculations as in [Quinlan & Rivest 89] or cost-complexity pruning used by the CART system <ref> [Breiman et al. 84] </ref>). The rationale for using post-pruning instead of pre-pruning is the hope to counter the detrimental effects of limited lookahead in the recursive construction of a tree. <p> For instance, C4.5 1 [Quinlan 93] uses a pessimistic error-estimate based on local error-rates, whereas so-called cost-complexity pruning as used in the Cart system <ref> [Breiman et al. 84] </ref> accounts for both error-rates and the tree complexities by means of a weighted sum where the coefficients are estimated from the given data. Contrary to these approaches, we try to further limit decision tree sizes during the tree growing phase already. <p> Not surprisingly, this sum-squared-error measure proved to be a good estimator, as it is equivalent to a geometric interpretation of the Gini-Index measure used in the CART system 4 <ref> [Breiman et al. 84] </ref> for choosing the best attribute to split on.
Reference: [Cherkauer & Shavlik 96] <author> Cherkauer K.J., Shavlik J.W.: </author> <title> Growing Simpler Decision Trees to Facilitate Knowledge Discovery, </title> <editor> in Simoudis E. and Han J.(eds.), </editor> <booktitle> KDD-96: Proceedings Second International Conference on Knowledge Discovery & Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, pp.315-318, </address> <year> 1996. </year>
Reference-contexts: Instead of discarding examples one can also discards a subset of the attributes used to describe the examples. This so-called feature subset selection <ref> [Cherkauer & Shavlik 96, Pfahringer 95] </ref> usually also leads to sig 1 nificantly simpler trees of comparable or better predictive accuracy.
Reference: [Holder 95] <author> Holder L.B.: </author> <title> Intermediate Decision Trees, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1056-1062, </address> <year> 1995. </year>
Reference-contexts: This hypothesis should of course be evaluated in a separate study. 12 * Also a closer comparison to pruning based on so-called "intermedi-ate decision trees" (IDT) as presented in <ref> [Holder 95] </ref> should be valuable. They try to directly estimate the right size for a decision tree also by means of cross-validation and a breadth-first tree-construction schema.
Reference: [Holte 93] <author> Holte R.C.: </author> <title> Very Simple Classification Rules Perform Well on Most Commonly Used Datasets, </title> <journal> Machine Learning, </journal> <volume> 11(1), </volume> <year> 1993. </year>
Reference-contexts: However, some recent studies shed doubt onto this hope. Standard post-pruning algorithms might not be effective or radical enough in pruning overly complex trees, as significantly simpler trees often exhibit comparable predictive accuracy. In <ref> [Holte 93, Auer et al. 95] </ref> very simple decision-trees (being restricted to one or two levels respectively) are shown to perform well in quite a few domains. <p> A reasonable alternative might be some form of information-gain or an MDL-based coding length measure. Also the choice of one 10-fold cross-validation run for estimating the target error is rather ad-hoc. Such decisions should be investigated in more detail. * As has already been proven by both the 1R <ref> [Holte 93] </ref> and T2 [Auer et al. 95] systems, small decision trees can exhibit good predictive accuracy. But we do believe that PreC4's more dynamic bias which in a sense guesses the right decision tree size from the given data will be more appropriate in a general learner.
Reference: [Jensen 97] <author> Jensen D.: </author> <title> Adjusting for Multiple Testing in Decision Tree Pruning, </title> <booktitle> Poster presentation at the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <year> 1997. </year>
Reference-contexts: Examples C4.5 R-C4 PreC4 R-PreC4 100 29.60 28.70 22.20 23.10 1000 23.34 21.16 19.78 19.78 3000 21.34 20.17 19.66 19.66 5000 20.28 19.85 19.75 19.75 Table 1: Averages error rates for Random data. 7 3.1.2 Jensen This domain was introduced in <ref> [Jensen 97] </ref> for evaluating the influence of the training set size on the pruning of decision trees. Once again, examples comprise 30 random boolean attributes and a boolean class variable. The class itself is computed by a decision tree of 5 tests on the first five boolean attributes. <p> But we also note that for the smallest training set size of only 100 examples both PreC4 and Robust-PreC4 return trees which are significantly worse predictors (as judged by a paired t-test). Additionally, it is interesting to note that the figures in <ref> [Jensen 97] </ref> seem to indicate monotonic growth of decision-tree sizes for their implementation of post-pruning based on pessimistic error estimates (that curve should be equivalent to C4.5 in Table 2). But this finding is probably misleading, because they have only used training sets of up to 250 examples.
Reference: [John 95] <author> John G.H.: </author> <title> Robust Decision Trees: Removing Outliers from Databases, </title> <editor> in Fayyad U.M. and Uthurusamy R.(eds.), </editor> <booktitle> KDD-95: Proceedings First International Conference on Knowledge Discovery & Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, pp.174-179, </address> <year> 1995. </year>
Reference-contexts: In [Holte 93, Auer et al. 95] very simple decision-trees (being restricted to one or two levels respectively) are shown to perform well in quite a few domains. In [Mehta et al. 95] a minimum description length based formula allows to post-prune trees considerably while retaining their predictive accuracy. <ref> [John 95] </ref> introduces Robust-C4 which iteratively discard misclassified examples, also resulting in smaller trees of similar predictive accuracy. Instead of discarding examples one can also discards a subset of the attributes used to describe the examples. <p> All the results given are averaged over ten runs of stratified ten-fold cross-validation. We always compare both predictive error-rates and final tree sizes for all four methods C4.5, Robust-C4 <ref> [John 95] </ref>, PreC4, and Robust-PreC4. The size of a decision tree is defined to be 5 the number of tests in a tree.
Reference: [Kramer 96] <author> Kramer S.: </author> <title> Structural Regression Trees, </title> <booktitle> in Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Cambridge, MA, pp.812-819, </address> <year> 1996. </year> <month> 14 </month>
Reference-contexts: Furthermore, we plan to adapt the key ideas of PreC4 to learning in a first-order framework (like [Blockeel & DeRaedt 97] or <ref> [Kramer 96] </ref>), where due to vastly larger concept spaces effective pruning is even more essential than it already is in a propositional setting. Acknowledgements The first idea for PreC4 was conceived while the author was visiting the Computer Science Department of the University of Waikato, Hamilton, New Zealand.
Reference: [Mehta et al. 95] <author> Mehta M., Rissanen J., Agrawal R.: </author> <title> MDL-Based Decision Tree Pruning, </title> <editor> in Fayyad U.M. and Uthurusamy R.(eds.), </editor> <booktitle> KDD-95: Proceedings First International Conference on Knowledge Discovery & Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, pp.216-221, </address> <year> 1995. </year>
Reference-contexts: In [Holte 93, Auer et al. 95] very simple decision-trees (being restricted to one or two levels respectively) are shown to perform well in quite a few domains. In <ref> [Mehta et al. 95] </ref> a minimum description length based formula allows to post-prune trees considerably while retaining their predictive accuracy. [John 95] introduces Robust-C4 which iteratively discard misclassified examples, also resulting in smaller trees of similar predictive accuracy.
Reference: [Merz & Murphy 96] <author> Merz C.J., </author> <title> Murphy P.M.: UCI Repository of machine learning databases Irvine, </title> <institution> CA: University of Califor-nia, Department of Information and Computer Science, </institution> <year> 1996. </year> <note> [http://www.ics.uci.edu/ mlearn/MLRepository.html] </note>
Reference-contexts: T ree := grow tree (T raining; T arget); 3. return postprune (T ree) Output: Decision-tree 3 Empirical Evaluation In this section we report on empirical results for two artificial domains and for a few standard benchmark domains as found in the UC Irvine repository <ref> [Merz & Murphy 96] </ref>. All the results given are averaged over ten runs of stratified ten-fold cross-validation. We always compare both predictive error-rates and final tree sizes for all four methods C4.5, Robust-C4 [John 95], PreC4, and Robust-PreC4. <p> Given enough examples, all methods including C4.5 are able to induce the correct decision tree in this domain. 3.2 Natural Domains To further test the influence of pre-pruning on tree sizes and accuracies, we also conducted experiments on a few database mostly available from the UCI repository <ref> [Merz & Murphy 96] </ref>. The specific databases were chosen to comprise a good mix of the various properties distinguishing flat-file databases: number of cases, number of classes, the number of categorical attributes, and the number of numerical attributes. Table 3 summarizes these coordinates for all databases used.
Reference: [Murthy & Salzberg 95] <author> Murthy S.K., Salzberg S.: </author> <title> Lookahead and Pathology in Decision Tree Induction, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1025-1031, </address> <year> 1995. </year>
Reference-contexts: Possibly, a different cross-validation regime could remedy this fault, e.g. using more than 10 folds or even leave-one-out cross-validation for such small training sets. In general, these findings are quite reassuring given the recent discovery of the sometimes detrimental effects of so-called oversearching <ref> [Quinlan & Cameron-Jones 95, Murthy & Salzberg 95] </ref>. Another interesting conclusion is the fact that at least with respect to predictive accuracy C4.5's default settings seem to do very well for natural domains. Even the sometimes obvious overfitting apparently causes no or only small losses in predictive performance.
Reference: [Oates & Jensen 97] <author> Oates T., Jensen D.: </author> <title> The Effects of Training Set Size on Decision Tree Complexity, </title> <booktitle> in Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <year> 1997. </year>
Reference-contexts: This so-called feature subset selection [Cherkauer & Shavlik 96, Pfahringer 95] usually also leads to sig 1 nificantly simpler trees of comparable or better predictive accuracy. Finally, in investigating the effect of training set sizes on decision tree complexity, <ref> [Oates & Jensen 97] </ref> finds a more or less linear relationship between these two factors for various post-pruning methods. Pre-pruning is usually dismissed for being non-competitive. For instance, [Quinlan 93] reports that early experiments with a stopping criterion based on the chi-square test did not perform satisfactorily.
Reference: [Pfahringer 95] <author> Pfahringer B.: </author> <title> Compression-Based Feature Subset Selection, </title> <booktitle> in Turney P.(ed.), IJCAI-95 Workshop on Data Engineering for Inductive Learning, IJCAI-95 Workshop Program Working Notes, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: Instead of discarding examples one can also discards a subset of the attributes used to describe the examples. This so-called feature subset selection <ref> [Cherkauer & Shavlik 96, Pfahringer 95] </ref> usually also leads to sig 1 nificantly simpler trees of comparable or better predictive accuracy.
Reference: [Pfahringer 97] <author> Pfahringer B.: </author> <title> On the Induction of Intelligible Ensembles, </title> <journal> Oesterreichisches Forschungsinstitut fuer Artificial Intelligence, Wien, </journal> <volume> TR-97-30, </volume> <year> 1997. </year>
Reference-contexts: Unfortunately, none of these methods is able to simultaneously optimize both criteria. Boosting PreC4 with only a small number of iterations and transforming the resulting ensemble into a single decision tree (maybe similar to work described in <ref> [Pfahringer 97] </ref>). might be an interesting starting point for such a synthesis.
Reference: [Quinlan & Rivest 89] <author> Quinlan J.R, Rivest R.L.: </author> <title> Inferring Decision Trees using the Minimum Description Length Principle, </title> <booktitle> in Information and Computation, </booktitle> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: post-pruning process is usually guided by either statistical measures of significance (e.g. chi-square testing), or some form of error estimate (e.g. pessimistic error estimates [Quinlan 93], or reduced error pruning), or it might also take into account the complexity of the induced tree (e.g. minimum description length calculations as in <ref> [Quinlan & Rivest 89] </ref> or cost-complexity pruning used by the CART system [Breiman et al. 84]). The rationale for using post-pruning instead of pre-pruning is the hope to counter the detrimental effects of limited lookahead in the recursive construction of a tree.
Reference: [Quinlan 93] <author> Quinlan J.R.: C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This post-pruning process is usually guided by either statistical measures of significance (e.g. chi-square testing), or some form of error estimate (e.g. pessimistic error estimates <ref> [Quinlan 93] </ref>, or reduced error pruning), or it might also take into account the complexity of the induced tree (e.g. minimum description length calculations as in [Quinlan & Rivest 89] or cost-complexity pruning used by the CART system [Breiman et al. 84]). <p> Finally, in investigating the effect of training set sizes on decision tree complexity, [Oates & Jensen 97] finds a more or less linear relationship between these two factors for various post-pruning methods. Pre-pruning is usually dismissed for being non-competitive. For instance, <ref> [Quinlan 93] </ref> reports that early experiments with a stopping criterion based on the chi-square test did not perform satisfactorily. Such static prepruning faces two problems: firstly, prespecified thresholds are more or less arbitrary, and secondly, hill-climbing search is inherently myopic. <p> Section 3 reports on experiments conducted in both artificial and natural domains. Section 4 concludes with a discussion of results and further research directions. 2 Pre-Pruning Standard decision tree induction algorithms usually rely solely on some form of post-pruning to avoid overfitting the training set. For instance, C4.5 1 <ref> [Quinlan 93] </ref> uses a pessimistic error-estimate based on local error-rates, whereas so-called cost-complexity pruning as used in the Cart system [Breiman et al. 84] accounts for both error-rates and the tree complexities by means of a weighted sum where the coefficients are estimated from the given data.
Reference: [Quinlan & Cameron-Jones 95] <author> Quinlan J., Cameron-Jones R.: </author> <title> Oversearch-ing and Layered Search in Empirical Learning, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1019-1024, </address> <year> 1995. </year> <month> 15 </month>
Reference-contexts: Possibly, a different cross-validation regime could remedy this fault, e.g. using more than 10 folds or even leave-one-out cross-validation for such small training sets. In general, these findings are quite reassuring given the recent discovery of the sometimes detrimental effects of so-called oversearching <ref> [Quinlan & Cameron-Jones 95, Murthy & Salzberg 95] </ref>. Another interesting conclusion is the fact that at least with respect to predictive accuracy C4.5's default settings seem to do very well for natural domains. Even the sometimes obvious overfitting apparently causes no or only small losses in predictive performance.
References-found: 18

