URL: http://www.cs.columbia.edu/robotics/publications/stamos-allen-cvpr98.ps.gz
Refering-URL: http://www.cs.columbia.edu/~istamos/publications.html
Root-URL: http://www.cs.columbia.edu
Email: fistamos, alleng@cs.columbia.edu  
Title: Interactive Sensor Planning  Pattern Recognition, 1998  
Author: Ioannis Stamos and Peter K. Allen 
Address: New York, NY 10027  
Affiliation: Department of Computer Science, Columbia University,  
Note: To appear at the IEEE Computer Society Conference on Computer Vision and  
Abstract: This paper describes an interactive sensor planning system that can be used to select viewpoints subject to camera visibility, field of view and task constraints. Application areas for this method include surveillance planning, safety monitoring, architectural site design planning, and automated site modeling. Given a description of the sensor's characteristics, the objects in the 3-D scene, and the targets to be viewed, our algorithms compute the set of admissible view points that satisfy the constraints. The system first builds topologically correct solid models of the scene from a variety of data sources. Viewing targets are then selected, and visibility volumes and field of view cones are computed and intersected to create viewing volumes where cameras can be placed. The user can interactively manipulate the scene and select multiple target features to be viewed by a camera. The user can also select candidate viewpoints within this volume to synthesize views and verify the correctness of the planning system. We present experimental results for the planning system on an actual complex city model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abrams, P. K. Allen, and K. Tarabanis. </author> <title> Computing camera viewpoints in a robot work-cell. </title> <booktitle> In Proc. IEEE Intl. Conference on Robotics and Automation, </booktitle> <pages> pages 1972-1979, </pages> <month> Apr. 22-25 </month> <year> 1996. </year>
Reference-contexts: To properly plan a correct view, all of these components must be considered. The focus of this paper is to extend our earlier sensor planning results <ref> [1, 11, 12] </ref> to urban scene planning. Urban environments are characterized by cluttered and complex object models which places a heavy emphasis on 3-D occlusion planning. In addition, these models themselves may be partial or incomplete, and lack topological relations that are central to performing the planning task.
Reference: [2] <author> M. Bern, D. Dobkin, D. Eppstein, and R. Grossman. </author> <title> Visibility with a moving point of view. </title> <journal> Algorithmica, </journal> <volume> 11 </volume> <pages> 360-378, </pages> <year> 1994. </year>
Reference-contexts: Related previous work on the geometric planning component includes computational geometry algorithms for determining visibility with much of the work focusing on 2-D visibility algorithms [8]. Other work includes Gigus and Canny [5] with aspect graphs, and work by Coorg [3] on efficient overestimation of visible polygons. Also, Bern <ref> [2] </ref> discusses visibility with a moving point of view. Work most closely related to ours in integrating sensor and visibility constraints in 3-D includes the work of [10, 7, 4]. These systems have focused on highly constrained and well-understood environments for which accurate and complete object models exist.
Reference: [3] <author> S. Coorg and S. Teller. </author> <title> Temporally coherent conservative visibility. </title> <booktitle> In Twelfth Annual ACM Symposium in Computational Geometry, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Related previous work on the geometric planning component includes computational geometry algorithms for determining visibility with much of the work focusing on 2-D visibility algorithms [8]. Other work includes Gigus and Canny [5] with aspect graphs, and work by Coorg <ref> [3] </ref> on efficient overestimation of visible polygons. Also, Bern [2] discusses visibility with a moving point of view. Work most closely related to ours in integrating sensor and visibility constraints in 3-D includes the work of [10, 7, 4].
Reference: [4] <author> C. K. Cowan and P. D. Kovesi. </author> <title> Automatic sensor placement from vision task requirements. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell., </journal> <volume> 10(3) </volume> <pages> 407-416, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Also, Bern [2] discusses visibility with a moving point of view. Work most closely related to ours in integrating sensor and visibility constraints in 3-D includes the work of <ref> [10, 7, 4] </ref>. These systems have focused on highly constrained and well-understood environments for which accurate and complete object models exist.
Reference: [5] <author> Z. Gigus, J. Canny, and R. Seidel. </author> <title> Efficiently computing and representing aspect graphs of polyhedral objects. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(6) </volume> <pages> 542-551, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Related previous work on the geometric planning component includes computational geometry algorithms for determining visibility with much of the work focusing on 2-D visibility algorithms [8]. Other work includes Gigus and Canny <ref> [5] </ref> with aspect graphs, and work by Coorg [3] on efficient overestimation of visible polygons. Also, Bern [2] discusses visibility with a moving point of view. Work most closely related to ours in integrating sensor and visibility constraints in 3-D includes the work of [10, 7, 4].
Reference: [6] <author> L. Guibas and S. J. </author> <title> Primitives for the manipulation of general subdivisions and the computation of voronoi diagrams. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 4 </volume> <pages> 74-123, </pages> <year> 1985. </year>
Reference-contexts: Those models are not guaranteed to be complete or to correspond to a proper polyhedron [9], since they lack topological information. Our planner uses solid models having a Polyhedral Boundary Representation (i.e. Quad Edge Data Structure <ref> [6] </ref>). So we need to be able to transform these existing graphics-based models to a solid, watertight boundary representation (i.e. no dangling faces). A common problem is that models are not watertight. Often, parts of the boundary are missing and the object is not bounded or closed.
Reference: [7] <author> R. Niepold, S. Sakane, and Y. Shirai. </author> <title> Vision sensor set-up planning for a hand-eye system using environmental models. </title> <institution> In Proceeding Soc. Instrum. Control Eng. Japan, Hiroshima, </institution> <address> Japan, </address> <month> July </month> <year> 1987. </year> <title> Solid CAD model computed from the graphics model. </title>
Reference-contexts: Also, Bern [2] discusses visibility with a moving point of view. Work most closely related to ours in integrating sensor and visibility constraints in 3-D includes the work of <ref> [10, 7, 4] </ref>. These systems have focused on highly constrained and well-understood environments for which accurate and complete object models exist.
Reference: [8] <author> J. O'Rourke. </author> <title> Art Gallery Theorems and Algorithms. </title> <booktitle> The International Series on Monographs on Computer Science. </booktitle> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Application areas for these methods include surveillance planning, safety monitoring, architectural site design planning, and choosing viewpoints for automatic site modeling. Related previous work on the geometric planning component includes computational geometry algorithms for determining visibility with much of the work focusing on 2-D visibility algorithms <ref> [8] </ref>. Other work includes Gigus and Canny [5] with aspect graphs, and work by Coorg [3] on efficient overestimation of visible polygons. Also, Bern [2] discusses visibility with a moving point of view.
Reference: [9] <author> J. O'Rourke. </author> <title> Computational Geometry in C. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1994. </year>
Reference-contexts: Those models are not guaranteed to be complete or to correspond to a proper polyhedron <ref> [9] </ref>, since they lack topological information. Our planner uses solid models having a Polyhedral Boundary Representation (i.e. Quad Edge Data Structure [6]). So we need to be able to transform these existing graphics-based models to a solid, watertight boundary representation (i.e. no dangling faces).
Reference: [10] <author> S. Sakane, M. Ishii, and M. Kakiura. </author> <title> Occlusion avoidance of visual sensors based on a hand eye action simulator system: HEAVEN. </title> <booktitle> Advanced Robotics, </booktitle> <volume> 2(2) </volume> <pages> 149-165, </pages> <year> 1987. </year>
Reference-contexts: Also, Bern [2] discusses visibility with a moving point of view. Work most closely related to ours in integrating sensor and visibility constraints in 3-D includes the work of <ref> [10, 7, 4] </ref>. These systems have focused on highly constrained and well-understood environments for which accurate and complete object models exist.
Reference: [11] <author> K. Tarabanis, P. K. Allen, and R. Tsai. </author> <title> The MVP sensor planning system for robotic vision tasks. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 11(1) </volume> <pages> 72-85, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: To properly plan a correct view, all of these components must be considered. The focus of this paper is to extend our earlier sensor planning results <ref> [1, 11, 12] </ref> to urban scene planning. Urban environments are characterized by cluttered and complex object models which places a heavy emphasis on 3-D occlusion planning. In addition, these models themselves may be partial or incomplete, and lack topological relations that are central to performing the planning task.
Reference: [12] <author> K. Tarabanis, R. Tsai, and P. K. Allen. </author> <title> Analytical characterization of the feature detectability constraints of resolution, focus and field-of-view for vision sensor planning. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 59(3) </volume> <pages> 340-358, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: To properly plan a correct view, all of these components must be considered. The focus of this paper is to extend our earlier sensor planning results <ref> [1, 11, 12] </ref> to urban scene planning. Urban environments are characterized by cluttered and complex object models which places a heavy emphasis on 3-D occlusion planning. In addition, these models themselves may be partial or incomplete, and lack topological relations that are central to performing the planning task.

References-found: 12

