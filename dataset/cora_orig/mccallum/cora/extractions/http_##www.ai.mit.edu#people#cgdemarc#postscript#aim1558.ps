URL: http://www.ai.mit.edu/people/cgdemarc/postscript/aim1558.ps
Refering-URL: http://www.ai.mit.edu/people/cgdemarc/cgdemarc.html
Root-URL: 
Email: cgdemarc@ai.mit.edu  
Title: The Unsupervised Acquisition of a Lexicon from Continuous Speech  
Author: Carl de Marcken 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1995  
Date: 1558 November, 1995  129  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: We present an unsupervised learning algorithm that acquires a natural-language lexicon from raw speech. The algorithm is based on the optimal encoding of symbol sequences in an MDL framework, and uses a hierarchical representation of language that overcomes many of the problems that have stymied previous grammar-induction procedures. The forward mapping from symbol sequences to the speech stream is modeled using features based on articulatory gestures. We present results on the acquisition of lexicons and language models from raw speech, text, and phonetic transcripts, and demonstrate that our algorithm compares very favorably to other reported results with respect to segmentation performance and statistical efficiency. This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. This research is supported by NSF grant 9217041-ASC and ARPA under the HPCC and AASERT programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James K. Baker. </author> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the 97th Meeting of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <year> 1979. </year>
Reference-contexts: Expressive power has not been the principle downfall of CFG induction schemes, however: the search space of stochastic CFGs under most learning strategies is riddled with local optima. This means that convergence to a global optimum using a hill-climbing approach like the inside-outside algorithm <ref> [1] </ref> is only possible given a good starting point, and there are arguments [6, 13] that algorithms will not usually start from such points. Fortunately, the form of our grammar permits the use of a significantly better behaved search algorithm. There are several reasons for this.
Reference: [2] <author> Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. </author> <title> A maximization technique occur-ing in the statistical analysis of probabalistic functions in markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: Finally, because the representation of a word serves as a prior that discriminates against unnatural words, search tends not to get bogged down in linguistically implausible grammars. The search algorithm we use is divided into four stages. In stage 1, the Baum-Welch <ref> [2] </ref> procedure is applied to the input and word representations to estimate the probabilities of the words in the current dictionary. In stage 2 new words are added to the dictionary if this is predicted to reduce the combined description length of the dictionary and input.
Reference: [3] <author> Steven Bird and T. Mark Ellison. </author> <title> One-level phonolgy: Autosegmental representations and rules as finite automata. </title> <journal> Computational Linguistics, </journal> <volume> 20(1) </volume> <pages> 55-90, </pages> <year> 1994. </year> <month> 22 </month>
Reference: [4] <author> Michael Brent. </author> <title> Minimal generative explanations: A middle ground between neurons and triggers. </title> <booktitle> In Proc. of the 15th Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 28-36, </pages> <year> 1993. </year>
Reference-contexts: Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42]. More recent work on lexical acquisition that explicitly acknowledges the cost of parameters includes Ellison [17] and Brent <ref> [4, 5, 7] </ref>. Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words.
Reference: [5] <author> Michael R. Brent, Andrew Lundberg, and Sreerama Murthy. </author> <title> Discovering morphemic suffixes: A case study in minimum description length induction. </title> <year> 1993. </year>
Reference-contexts: Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42]. More recent work on lexical acquisition that explicitly acknowledges the cost of parameters includes Ellison [17] and Brent <ref> [4, 5, 7] </ref>. Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words.
Reference: [6] <author> Glenn Carroll and Eugene Charniak. </author> <title> Learning probabalistic dependency grammars from labelled text. </title> <booktitle> In Working Notes, Fall Symposium Series, AAAI, </booktitle> <pages> pages 25-31, </pages> <year> 1992. </year>
Reference-contexts: Context-free grammars, stochastic or not, are notoriously difficult to learn using unsupervised algorithms. As a general rule, CFGs acquired this way have neither achieved the entropy rate of theoretically inferior Markov and hidden Markov processes [8], nor settled on grammars that accord with linguistic intuitions <ref> [6, 28] </ref> (for a detailed explanation of why this is so, see de Marcken [13]). However disappointing previous results have been, there is reason to be optimistic. <p> This means that convergence to a global optimum using a hill-climbing approach like the inside-outside algorithm [1] is only possible given a good starting point, and there are arguments <ref> [6, 13] </ref> that algorithms will not usually start from such points. Fortunately, the form of our grammar permits the use of a significantly better behaved search algorithm. There are several reasons for this.
Reference: [7] <author> Timothy Andrew Cartwright and Michael R. Brent. </author> <title> Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. </title> <booktitle> In Proc. of the 16th Annual Meeting of the Cognitive Science Society, </booktitle> <address> Hillsdale, New Jersey, </address> <year> 1994. </year>
Reference-contexts: Using maximum-likelihood estimation, the probability p (w) of a word is computed by normalizing these counts. Assuming a clever coding, the length of a word w's index is log p (w). The 7 Just as in the work of Cartwright and Brent <ref> [7] </ref>, Della Pietra et al [15], Olivier [27], and Wolff [40, 41]. 8 surf (w) rep (w) c (w) p (w) log p (w) jrep (w)j the t+h+e 2 2/17 3.09 10.27 t 2 2/17 3.09 cat c+at 1 1/17 4.09 7.18 thecat the+cat 1 1/17 4.09 7.18 thehat the+hat 1 <p> Again, the phoneme-to-phone portion of our work was not exercised; the output of the text-to-phoneme converter is free of noise and makes this problem little different from that of segmenting text with the spaces removed. The goal, as in Cartright and Brent <ref> [7] </ref>, is to segment the speech into words. After ten iterations of training on the phoneme sequences, the algorithm produces a dictionary of 6,630 words, and a segmentation of the input. <p> They indicate that given simple input, the program very reliably extracts the fundamental linguistic units. Comparing to the only other similar results we know of, Cartwright and Brent's <ref> [7] </ref>, is difficult: at first glance our recall figure seems dramatically better, but this is partially because of our multi-level representation, which also renders accuracy rates meaningless. <p> Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42]. More recent work on lexical acquisition that explicitly acknowledges the cost of parameters includes Ellison [17] and Brent <ref> [4, 5, 7] </ref>. Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words.
Reference: [8] <author> Stanley F. Chen. </author> <title> Bayesian grammar induction for language modeling. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 228-235, </pages> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Context-free grammars, stochastic or not, are notoriously difficult to learn using unsupervised algorithms. As a general rule, CFGs acquired this way have neither achieved the entropy rate of theoretically inferior Markov and hidden Markov processes <ref> [8] </ref>, nor settled on grammars that accord with linguistic intuitions [6, 28] (for a detailed explanation of why this is so, see de Marcken [13]). However disappointing previous results have been, there is reason to be optimistic. <p> Our search algorithm is also more efficient and has a wider range of transformations available to it than other schemes we know of, though such work as Chen <ref> [8] </ref> and Stolcke [39] use conceptually similar search strategies for grammar induction. Recursive dictionaries themselves are not new, however: a variety of universal compression schemes (such as LZ78) are based on this idea.
Reference: [9] <author> Noam A. Chomsky. </author> <title> The Logical Structure of Linguistic Theory. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1955. </year>
Reference-contexts: Nevertheless, it is important to put this work in context. The use of compression and prediction frameworks for language induction is quite common; Chomsky <ref> [9] </ref> discussed them long ago and notable early advocates include Solomonoff [37]. Olivier [27] and Wolff [40, 41] were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction.
Reference: [10] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference: [11] <author> Anne Cutler. </author> <title> Segmentation problems, rhythmic solutions. </title> <booktitle> Lingua, </booktitle> <pages> 92(1-4), </pages> <year> 1994. </year>
Reference: [12] <author> Carl de Marcken. </author> <title> The acquisition of a lexicon from paired phoneme sequences and semantic representations. </title> <booktitle> In International Colloquium on Grammatical Inference, </booktitle> <pages> pages 66-77, </pages> <address> Alicante, Spain, </address> <year> 1994. </year>
Reference-contexts: These sentences were run through a simple public-domain text-to-phoneme converter, and inter-word pauses were removed. This is the same input described in de Marcken <ref> [12] </ref>. Again, the phoneme-to-phone portion of our work was not exercised; the output of the text-to-phoneme converter is free of noise and makes this problem little different from that of segmenting text with the spaces removed. <p> The constraint that the meaning of a sentence places on the words in it makes learning sound and meaning together much easier than learning sound alone (Siskind [33, 34, 35], de Marcken <ref> [12] </ref>). Let us make the naive assumption that meanings are merely sets (the meaning of // might be ft; h; eg, or the meaning of temps perdu might be fpast; timesg) 14 .
Reference: [13] <author> Carl de Marcken. </author> <title> Lexical heads, phrase structure and the induction of grammar. </title> <booktitle> In Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: As a general rule, CFGs acquired this way have neither achieved the entropy rate of theoretically inferior Markov and hidden Markov processes [8], nor settled on grammars that accord with linguistic intuitions [6, 28] (for a detailed explanation of why this is so, see de Marcken <ref> [13] </ref>). However disappointing previous results have been, there is reason to be optimistic. First of all, as described so far the class of grammars we are considering is weaker than context-free: there is recursion in the language that grammars are described with, but not in the languages these grammars generate. <p> This means that convergence to a global optimum using a hill-climbing approach like the inside-outside algorithm [1] is only possible given a good starting point, and there are arguments <ref> [6, 13] </ref> that algorithms will not usually start from such points. Fortunately, the form of our grammar permits the use of a significantly better behaved search algorithm. There are several reasons for this.
Reference: [14] <author> Sabine Deligne and Frederic Bimbot. </author> <title> Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. </title> <booktitle> In Proceedings of the International Conference on Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 169-172, </pages> <year> 1995. </year>
Reference-contexts: Given that our only unit of representation is the word, compression of the input or a nonterminal reduces to writing out a sequence of word indices. For simplicity, these words are drawn independently from a probability distribution over a single dictionary; this language model has been called a multigram <ref> [14] </ref>. Figure 3 presents a complete description of thecatinthehat, in which the input and six words used in the description are decomposed using a multigram language model. This is a contrived example, and does not represent how our algorithm would analyze this input.
Reference: [15] <author> Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. </author> <title> Inducing features of random fields. </title> <type> Technical Report CMU-CS-95-144, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Using maximum-likelihood estimation, the probability p (w) of a word is computed by normalizing these counts. Assuming a clever coding, the length of a word w's index is log p (w). The 7 Just as in the work of Cartwright and Brent [7], Della Pietra et al <ref> [15] </ref>, Olivier [27], and Wolff [40, 41]. 8 surf (w) rep (w) c (w) p (w) log p (w) jrep (w)j the t+h+e 2 2/17 3.09 10.27 t 2 2/17 3.09 cat c+at 1 1/17 4.09 7.18 thecat the+cat 1 1/17 4.09 7.18 thehat the+hat 1 1/17 4.09 7.18 e 1 <p> These schemes use simple on-line strategies to build such representations, and do not perform the optimization necessary to arrive at linguistically meaningful dictionaries. An attractive alternative to the concatenative language models used by all the researchers mentioned here is described by Della Pietra et al <ref> [15] </ref>. The unsupervised acquisition of words from continuous speech has received relatively little study. In the child psychology literature there is extensive analysis of what sounds are available to the infant (see Jusczyk [22, 23]), but no emphasis on testable theories of the actual acquisition 21 process.
Reference: [16] <author> A. P. Dempster, N. M. Liard, and D. B. Rubin. </author> <title> Maximum liklihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B(39):1-38, </volume> <year> 1977. </year>
Reference-contexts: Stages 2 and 4 are a means of increasing the likelihood of the input by modifying the contents of the dictionary rather than the probability distribution over it, and are thus part of the maximization step of a generalized expectation-maximization (EM) procedure <ref> [16] </ref>.
Reference: [17] <author> T. Mark Ellison. </author> <title> The Machine Learning of Phonological Structure. </title> <type> PhD thesis, </type> <institution> University of Western Australia, </institution> <year> 1992. </year>
Reference-contexts: Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42]. More recent work on lexical acquisition that explicitly acknowledges the cost of parameters includes Ellison <ref> [17] </ref> and Brent [4, 5, 7]. Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words.
Reference: [18] <author> W. N. Francis and H. Kucera. </author> <title> Frequency analysis of English usage: lexicon and grammar. </title> <address> Houghton-Mi*in, Boston, </address> <year> 1982. </year>
Reference-contexts: Vertical bars indicate word boundaries. 7.1 Text Compression and Language Modeling The algorithm was run on the Brown corpus <ref> [18] </ref>, a collection of approximately one million words of text drawn from diverse sources, and a standard test of language models. We performed a test identical to Ristad and Thomas [32], training on 90% of the corpus 11 and testing on the remainder.
Reference: [19] <author> E. Mark Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference: [20] <author> Morris Halle. </author> <title> On distinctive features and their articulatory implementation. </title> <booktitle> Natural Language and Linguistic Theory, </booktitle> <volume> 1 </volume> <pages> 91-105, </pages> <year> 1983. </year>
Reference: [21] <author> Morris Halle and Alec Marantz. </author> <title> Distributed morphology and the pieces of inflection. </title> <editor> In Kenneth Hale and Samuel Jay Keyser, editors, </editor> <title> The View from Building 20: </title> <booktitle> Essays in Linguistics in Honor of Sylvain Bromberger. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [22] <author> Peter W. Jusczyk. </author> <title> Discovering sound patterns in the native language. </title> <booktitle> In Proc. of the 15th Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 49-60, </pages> <year> 1993. </year> <month> 23 </month>
Reference-contexts: The unsupervised acquisition of words from continuous speech has received relatively little study. In the child psychology literature there is extensive analysis of what sounds are available to the infant (see Jusczyk <ref> [22, 23] </ref>), but no emphasis on testable theories of the actual acquisition 21 process. The speech recognition community has generally assumed that segmentations of the input are available for the early stages of training.
Reference: [23] <author> Peter W. Jusczyk. </author> <title> Infants speech perception and the development of the mental lexicon. </title> <editor> In Judith C. Goodman and Howard C. Nusbaum, editors, </editor> <booktitle> The Development of Speech Perception. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The unsupervised acquisition of words from continuous speech has received relatively little study. In the child psychology literature there is extensive analysis of what sounds are available to the infant (see Jusczyk <ref> [22, 23] </ref>), but no emphasis on testable theories of the actual acquisition 21 process. The speech recognition community has generally assumed that segmentations of the input are available for the early stages of training.
Reference: [24] <author> Ronald M. Kaplan and Martin Kay. </author> <title> Regular models of phonological rule systems. </title> <journal> Computational Linguistics, </journal> <volume> 20(3) </volume> <pages> 331-378, </pages> <year> 1994. </year>
Reference-contexts: This sequence is mapped to the phone sequence by means of a stochastic finite-state transducer, though of a much simpler sort than Kaplan and Kay use to model morphology and phonology in their classic work <ref> [24] </ref>: it has only three states. Possible actions are to copy (write a phone related to the underlying phoneme and advance), delete (advance), map (write a phone related to the underlying phoneme without advancing), and insert (write an arbitrary phone without advancing).
Reference: [25] <author> Michael Kenstowicz. </author> <title> Phonology in Generative Grammar. </title> <publisher> Blackwell Publishers, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference: [26] <author> B. MacWhinney and C. Snow. </author> <title> The child language data exchange system. </title> <journal> Journal of Child Language, </journal> <volume> 12 </volume> <pages> 271-296, </pages> <year> 1985. </year>
Reference-contexts: Many of the rarer words are uninteresting coincidences, useful for compression only because of the peculiarities of the source. 12 7.2 Segmentation The algorithm was run on a collection of 34,438 transcribed sentences of mothers' speech to children, taken from the Nina portion of the CHILDES database <ref> [26] </ref>; a sample is shown in figure 6. These sentences were run through a simple public-domain text-to-phoneme converter, and inter-word pauses were removed. This is the same input described in de Marcken [12].
Reference: [27] <author> Donald Cort Olivier. </author> <title> Stochastic Grammars and Language Acquisition Mechanisms. </title> <type> PhD thesis, </type> <institution> Harvard University, Cambridge, Massachusetts, </institution> <year> 1968. </year>
Reference-contexts: Using maximum-likelihood estimation, the probability p (w) of a word is computed by normalizing these counts. Assuming a clever coding, the length of a word w's index is log p (w). The 7 Just as in the work of Cartwright and Brent [7], Della Pietra et al [15], Olivier <ref> [27] </ref>, and Wolff [40, 41]. 8 surf (w) rep (w) c (w) p (w) log p (w) jrep (w)j the t+h+e 2 2/17 3.09 10.27 t 2 2/17 3.09 cat c+at 1 1/17 4.09 7.18 thecat the+cat 1 1/17 4.09 7.18 thehat the+hat 1 1/17 4.09 7.18 e 1 1/17 4.09 <p> Nevertheless, it is important to put this work in context. The use of compression and prediction frameworks for language induction is quite common; Chomsky [9] discussed them long ago and notable early advocates include Solomonoff [37]. Olivier <ref> [27] </ref> and Wolff [40, 41] were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction. Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42].
Reference: [28] <author> Fernando Pereira and Yves Schabes. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proc. 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 128-135, </pages> <address> Berkeley, California, </address> <year> 1992. </year>
Reference-contexts: Context-free grammars, stochastic or not, are notoriously difficult to learn using unsupervised algorithms. As a general rule, CFGs acquired this way have neither achieved the entropy rate of theoretically inferior Markov and hidden Markov processes [8], nor settled on grammars that accord with linguistic intuitions <ref> [6, 28] </ref> (for a detailed explanation of why this is so, see de Marcken [13]). However disappointing previous results have been, there is reason to be optimistic.
Reference: [29] <author> Lawrence Rabiner and Biing-Hwang Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference: [30] <author> Jorma Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference: [31] <author> Jorma Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference: [32] <author> Eric Sven Ristad and Robert G. Thomas. </author> <title> New techniques for context modeling. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Vertical bars indicate word boundaries. 7.1 Text Compression and Language Modeling The algorithm was run on the Brown corpus [18], a collection of approximately one million words of text drawn from diverse sources, and a standard test of language models. We performed a test identical to Ristad and Thomas <ref> [32] </ref>, training on 90% of the corpus 11 and testing on the remainder. Obviously, the speech extensions discussed in section 4 were not exercised.
Reference: [33] <author> Jeffrey M. Siskind. </author> <title> Naive physics, event perception, lexical semantics, and language acquisition. </title> <type> PhD thesis TR-1456, </type> <institution> MIT Artificial Intelligence Lab., </institution> <year> 1992. </year>
Reference-contexts: The constraint that the meaning of a sentence places on the words in it makes learning sound and meaning together much easier than learning sound alone (Siskind <ref> [33, 34, 35] </ref>, de Marcken [12]). Let us make the naive assumption that meanings are merely sets (the meaning of // might be ft; h; eg, or the meaning of temps perdu might be fpast; timesg) 14 .
Reference: [34] <author> Jeffrey Mark Siskind. </author> <title> Lexical acquisition as constraint satisfaction. </title> <type> Technical Report IRCS-93-41, </type> <institution> University of Pennsylvania Institute for Research in Cognitive Science, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year>
Reference-contexts: The constraint that the meaning of a sentence places on the words in it makes learning sound and meaning together much easier than learning sound alone (Siskind <ref> [33, 34, 35] </ref>, de Marcken [12]). Let us make the naive assumption that meanings are merely sets (the meaning of // might be ft; h; eg, or the meaning of temps perdu might be fpast; timesg) 14 .
Reference: [35] <author> Jeffrey Mark Siskind. </author> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proc. of the American Association for Artificial Intelligence, </booktitle> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference-contexts: The constraint that the meaning of a sentence places on the words in it makes learning sound and meaning together much easier than learning sound alone (Siskind <ref> [33, 34, 35] </ref>, de Marcken [12]). Let us make the naive assumption that meanings are merely sets (the meaning of // might be ft; h; eg, or the meaning of temps perdu might be fpast; timesg) 14 . <p> The algorithm also occasionally produces words that cross real-word boundaries, like ed by the (see figure 5). This is an 14 Siskind <ref> [35] </ref> argues that it is easy to extend a program that learns sets to learn structures over them. 19 example of a regularity that arises because of several interacting linguistic processes that the algorithm can not capture because it has no notion of abstract categories.
Reference: [36] <author> Jeffrey L. Sokolov and Catherine E. Snow. </author> <title> The changing role of negative evidence in theories of language development. </title> <editor> In Clare Gallaway and Brian J. Richards, editors, </editor> <booktitle> Input and interaction in language acquisition, </booktitle> <pages> pages 38-55. </pages> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference: [37] <author> R. J. </author> <title> Solomonoff. </title> <booktitle> The mechanization of linguistic learning. In Proceedings of the 2nd International Conference on Cybernetics, </booktitle> <pages> pages 180-193, </pages> <year> 1960. </year>
Reference-contexts: Nevertheless, it is important to put this work in context. The use of compression and prediction frameworks for language induction is quite common; Chomsky [9] discussed them long ago and notable early advocates include Solomonoff <ref> [37] </ref>. Olivier [27] and Wolff [40, 41] were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction. Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42].
Reference: [38] <author> Andrew Spencer. </author> <title> Morphological Theory. </title> <publisher> Blackwell Publishers, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference: [39] <author> Andreas Stolcke. </author> <title> Bayesian Learning of Probabalistic Language Models. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Our search algorithm is also more efficient and has a wider range of transformations available to it than other schemes we know of, though such work as Chen [8] and Stolcke <ref> [39] </ref> use conceptually similar search strategies for grammar induction. Recursive dictionaries themselves are not new, however: a variety of universal compression schemes (such as LZ78) are based on this idea.
Reference: [40] <author> J. Gerald Wolff. </author> <title> Language acquisition and the discovery of phrase structure. </title> <journal> Language and Speech, </journal> <volume> 23(3) </volume> <pages> 255-269, </pages> <year> 1980. </year>
Reference-contexts: Assuming a clever coding, the length of a word w's index is log p (w). The 7 Just as in the work of Cartwright and Brent [7], Della Pietra et al [15], Olivier [27], and Wolff <ref> [40, 41] </ref>. 8 surf (w) rep (w) c (w) p (w) log p (w) jrep (w)j the t+h+e 2 2/17 3.09 10.27 t 2 2/17 3.09 cat c+at 1 1/17 4.09 7.18 thecat the+cat 1 1/17 4.09 7.18 thehat the+hat 1 1/17 4.09 7.18 e 1 1/17 4.09 c 1 1/17 <p> Nevertheless, it is important to put this work in context. The use of compression and prediction frameworks for language induction is quite common; Chomsky [9] discussed them long ago and notable early advocates include Solomonoff [37]. Olivier [27] and Wolff <ref> [40, 41] </ref> were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction. Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42].
Reference: [41] <author> J. Gerald Wolff. </author> <title> Language acquisition, data compression and generalization. </title> <journal> Language and Communication, </journal> <volume> 2(1) </volume> <pages> 57-89, </pages> <year> 1982. </year>
Reference-contexts: Assuming a clever coding, the length of a word w's index is log p (w). The 7 Just as in the work of Cartwright and Brent [7], Della Pietra et al [15], Olivier [27], and Wolff <ref> [40, 41] </ref>. 8 surf (w) rep (w) c (w) p (w) log p (w) jrep (w)j the t+h+e 2 2/17 3.09 10.27 t 2 2/17 3.09 cat c+at 1 1/17 4.09 7.18 thecat the+cat 1 1/17 4.09 7.18 thehat the+hat 1 1/17 4.09 7.18 e 1 1/17 4.09 c 1 1/17 <p> Nevertheless, it is important to put this work in context. The use of compression and prediction frameworks for language induction is quite common; Chomsky [9] discussed them long ago and notable early advocates include Solomonoff [37]. Olivier [27] and Wolff <ref> [40, 41] </ref> were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction. Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 [42].
Reference: [42] <author> J. Ziv and A. Lempel. </author> <title> Compression of individual sequences by variable rate coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24 </volume> <pages> 530-536, </pages> <year> 1978. </year> <month> 24 </month>
Reference-contexts: That is because in this longer input, the sequences thecat and thehat appear more often than independent chance would predict, and are more succinctly represented by a single index than by writing down their letters piecemeal. This representation is a generalization of that used by the LZ78 <ref> [42] </ref> coding scheme. It is therefore capable of universal compression, given the right estimation scheme, and compresses a sequence of identical characters of length n to size O (log n). It has a variety of other pleasing properties. <p> Olivier [27] and Wolff [40, 41] were among the first who implemented algorithms that attempt to learn words from text using techniques based on prediction. Olivier's work is particularly impressive and very similar to practical dictionary-based compression schemes like LZ78 <ref> [42] </ref>. More recent work on lexical acquisition that explicitly acknowledges the cost of parameters includes Ellison [17] and Brent [4, 5, 7]. Ellison has used three-level compression schemes to acquire intermediate representations in a manner similar to how we acquire words.
References-found: 42

