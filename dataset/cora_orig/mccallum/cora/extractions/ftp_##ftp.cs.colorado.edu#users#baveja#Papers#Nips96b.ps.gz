URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Nips96b.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: baveja@cs.colorado.edu  bertsekas@lids.mit.edu  
Title: Analytical Mean Squared Error Curves in Temporal Difference Learning  
Author: Satinder Singh Peter Dayan 
Address: Boulder, CO 80309-0430  E25-210, MIT Cambridge, MA 02139  
Affiliation: Department of Computer Science University of Colorado  Brain and Cognitive Sciences  
Abstract: We have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with o*ine updates over trials in absorbing Markov chains using lookup table representations. We illustrate classes of learning curve behavior in various chains, and show the manner in which TD is sensitive to the choice of its step size and eligibility trace parameters.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G. & Duff, M. </author> <year> (1994). </year> <title> Monte Carlo matrix inversion and reinforcement learning. </title> <booktitle> NIPS 6, </booktitle> <pages> pp 687-694. </pages>
Reference-contexts: The algorithms differ in the ffis produced from a trial. Monte Carlo algorithms use the final payoff that results from a trial to define the ffi i (t) <ref> (e.g., Barto & Duff, 1994) </ref>. Therefore in MC algorithms the estimated value of a state is unaffected by the estimated value of any other state.
Reference: <author> Singh, S.P. & Dayan, P. </author> <year> (1996). </year> <title> Analytical mean squared error curves in temporal difference learning. </title> <journal> Machine Learning, </journal> <note> submitted. </note>
Reference-contexts: In this paper we present results just for the standard TD () algorithm (Sutton, 1988), but we have analysed <ref> (Singh & Dayan, 1996) </ref> various other TD-like algorithms (e.g., Singh & Sutton, 1996) and comment on their behavior in the conclusions. <p> In this paper we present results just for the standard TD () algorithm (Sutton, 1988), but we have analysed (Singh & Dayan, 1996) various other TD-like algorithms <ref> (e.g., Singh & Sutton, 1996) </ref> and comment on their behavior in the conclusions. Our analytical results are based on two non-trivial assumptions: first that lookup tables are used, and second that the algorithm parameters ff and are functions of the trial number alone rather than also depending on the state.
Reference: <author> Singh, S.P. & Sutton, R.S. </author> <year> (1996). </year> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: In this paper we present results just for the standard TD () algorithm (Sutton, 1988), but we have analysed <ref> (Singh & Dayan, 1996) </ref> various other TD-like algorithms (e.g., Singh & Sutton, 1996) and comment on their behavior in the conclusions. <p> In this paper we present results just for the standard TD () algorithm (Sutton, 1988), but we have analysed (Singh & Dayan, 1996) various other TD-like algorithms <ref> (e.g., Singh & Sutton, 1996) </ref> and comment on their behavior in the conclusions. Our analytical results are based on two non-trivial assumptions: first that lookup tables are used, and second that the algorithm parameters ff and are functions of the trial number alone rather than also depending on the state.
Reference: <author> Sutton, R.S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> pp 9-44. </pages>
Reference-contexts: Our main contribution is in deriving the analytical update equations for the two components of MSE, bias and variance, for popular Monte Carlo (MC) and TD () <ref> (Sutton, 1988) </ref> algorithms. These derivations are presented in a larger paper. <p> Monte Carlo algorithms use the final payoff that results from a trial to define the ffi i (t) (e.g., Barto & Duff, 1994). Therefore in MC algorithms the estimated value of a state is unaffected by the estimated value of any other state. The main contribution of TD algorithms <ref> (Sutton, 1988) </ref> over MC algorithms is that they update the value of a state based not only on the terminal payoff but also on the the estimated values of the intervening states. <p> In this paper we present results just for the standard TD () algorithm <ref> (Sutton, 1988) </ref>, but we have analysed (Singh & Dayan, 1996) various other TD-like algorithms (e.g., Singh & Sutton, 1996) and comment on their behavior in the conclusions.
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD Thesis. </type> <institution> University of Cambridge, </institution> <address> England. </address>
References-found: 5

