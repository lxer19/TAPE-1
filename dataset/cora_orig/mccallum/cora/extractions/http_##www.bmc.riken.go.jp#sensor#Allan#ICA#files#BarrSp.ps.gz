URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/files/BarrSp.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: allan@ohnishi.nuie.nagoya-u.ac.jp  
Phone: 2  
Title: I&ANN98 Adaptive Blind Elimination of Artifacts in ECG Signals  
Author: Allan Kardec Barros Ali Mansour and Noboru Ohnishi ; 
Keyword: Independent component analysis, blind separation, adaptive filtering, cardiac artifacts, ECG analysis.  
Address: Furo-cho, Chikusa-ku, Nagoya 464-01, Japan.  Nagoya, Japan.  
Affiliation: 1 Graduate School of Engineering Department of Information Engineering, Nagoya University,  RIKEN BMC Research Center,  
Abstract: In this work, we deal with the elimination of artifacts (electrodes, muscle, respiration, etc.) from the electrocardiographic (ECG) signal. We use a new tool called independent component analysis (ICA) that blindly separate mixed statisticaly independent signals. ICA can separate the interference, even if they overlap in frequency. In order to estimate the mixing parameters in real-time, we propose a self-adaptive step-size, derived from the study of the averaged behavior of those parameters, and a two-layers neural network. Simulations were carried out to show the performance of the algorithm using a standard ECG database. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amari, A.Cichocki, H. H. Yang, </author> <title> "A new learning algorithm for blind signal separation", </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8, </volume> <publisher> MIT press, </publisher> <year> 1996. </year>
Reference-contexts: This is called the relative [9], natural, or Riemannian gradient <ref> [1, 2] </ref>. This algorithm works better in general because the parameter space of neural networks is Rie-mannian [3]. <p> Using (7) and (18), we can write T k+1 = E [(1 + k )I k y k z T k ] = k T k + 2 k T k P k : (19) If we assume that the variation of z k is bounded to the interval <ref> [1; 1] </ref>, we can then say that in this limit y k z k and P k T k .
Reference: [2] <author> S. Amari, A.Cichocki, H. H. Yang, </author> <title> "Gradient learning in structured parameter spaces: adaptive blind separation of signal sources", </title> <address> WCNN, </address> <year> 1996. </year>
Reference-contexts: This is called the relative [9], natural, or Riemannian gradient <ref> [1, 2] </ref>. This algorithm works better in general because the parameter space of neural networks is Rie-mannian [3].
Reference: [3] <author> S. Amari, </author> <title> "Information geometry of the EM and em algorithms for neural networks", </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 8, No. 9, </volume> <pages> pp. 1379 - 1408, </pages> <year> 1995. </year>
Reference-contexts: This is called the relative [9], natural, or Riemannian gradient [1, 2]. This algorithm works better in general because the parameter space of neural networks is Rie-mannian <ref> [3] </ref>. To obtain a better estimation, Pearlmutter and Parra [20] derived an algorithm that extracts many parameters related to the signal, and therefore their parameter space S = f ^ g was built with many variables.
Reference: [4] <author> S. Amari, </author> <title> ""Stability Analysis of Adaptive Blind Source Separation" Neural Networks, </title> <note> to appear. </note>
Reference-contexts: In the case of sub-Gaussian signals, Ci--chocki and his colleagues [12] suggested [12] the following equation B k+1 = B k + k (I z k y T An interesting discussion about this topic was carried out by Amari <ref> [4] </ref>. 3.1 Indefinition of the Solution Because the system works in a blind manner, B does not necessarily converge to the inverse of A. We can only affirm that C = DP, where D is a diagonal, and P is a permutation matrix [13].
Reference: [5] <author> A. K. Barros and N. </author> <title> Ohnishi "MSE Behavior of Biomedical Event-Related Filters" IEEE Trans. </title> <journal> Biomed. Eng., </journal> <note> (to appear) 1997. </note>
Reference-contexts: 1 Introduction Many attempts were carried out to eliminate corrupting artifacts from the actual cardiac one when measuring the electrocardiographic (ECG) signal. Cardiac signals show the well known repeating and almost periodic pattern. This caracteristic of physiological signals was already explored in some works (e.g, <ref> [5, 17, 21] </ref>) by synchronizing the parameters of the filter with the period of the signal. However, those filters fail to remove the interference when it has the same frequency of the cardiac signal. <p> This large number of works may be explained because the ICA algorithms are in general elegant, simple and may deal with signals that second order statistics (SOS) methods 1 in general do not work. This is because SOS algorithms usually search for 1 such as the one proposed in <ref> [5, 17, 21] </ref>. a solution that decorrelates the input signals while ICA looks for an independent solution. ICA is based on the following principle.
Reference: [6] <author> A.J. Bell and T. J. Sejnowski, </author> <title> "An information-maximization approach to blind separation and blind deconvolution" Neural Computation, </title> <booktitle> 7, </booktitle> <pages> pp. 1129 - 1159, </pages> <year> 1995. </year>
Reference-contexts: Cichocki and his colleagues [11] also proposed a self-adaptive step-size. However, our attempt here is to find a step-size which is directly based on the evolution of the algorithm. Moreover, we propose a neural network consisting of two layers of ICA algorithms. Some works <ref> [6, 16, 9] </ref> suggested to carry out whitening before the ICA algorithm in order to orthogonalize the inputs, which yields a faster convergence. The basis of our two-layer network is the same. <p> Hence, we should minimize lfp (s; ); ^p (z; ^ )g, and this can be done by using a gradient method. However, instead of the con ventional Euclidean gradient method <ref> [6] </ref>, that reads ^ k+1 = ^ k k @ ^ k we rather use the following gradient ^ k+1 = ^ k k = @ ^ k where = is a positive definite matrix. This is called the relative [9], natural, or Riemannian gradient [1, 2]. <p> In this work, we use the following function, as suggested by Bell and Sejnowski <ref> [6] </ref> B k+1 = B k + k (I y k z T with z = Bx and y = tanh (z). And this is the secret of ICA: this non-linearity tries to shape the sources distribution. <p> This was realized by substituting the non linear function in (8) by y = erf (z) <ref> [6] </ref> * One ICA algorithm as in (11). A2 and A3. The labels are as follows: solid: ICA+ICA; dotted: whitening+ICA; dashed: only ICA. "100" indicates optimum separation. In the top line, no filtering was used while in the bottom one, filtering was used for the case of ICA+ICA and whitening+ICA.
Reference: [7] <author> A.J. Bell and T. J. Sejnowski, </author> <title> "Fast blind separation based on information theory" Proc. </title> <booktitle> Intern. Symp. on Nonlinear Theory and Applications (NOLTA), </booktitle> <volume> vol. 1, </volume> <pages> 43-47, </pages> <address> Las Vegas, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The labels are as follows: solid: ICA with adptive learning rate; dotted: ICA with a constant learning rate of 2 fi 10 4 ; dashed: ICA with a constant learning rate of 2 fi 10 5 . Some words are necessary about the two-layers network. Some works, e.g., <ref> [7] </ref>, [16] have proposed to carry out whitening before the ICA processing in order to orthogo-nalize the input components. In the same way, we have used the first layer to force the algorithm to search for independent components.
Reference: [8] <author> Belouchrani A., </author> <title> Cichocki and Abed Meraim K."A Blind Identification and Separation Technique via Multi-layer Neural Networks" Proc. </title> <publisher> ICONIP'96, Springer-Verlag Singapore Ltd, Vol. </publisher> <pages> 2, 1195-1200, </pages> <year> 1996. </year>
Reference: [9] <author> J-F. </author> <title> Cardoso."On the performance of source separation algorithms" In Proc. </title> <booktitle> EUSIPCO, </booktitle> <pages> pp. 776-779, </pages> <address> Edinburgh, </address> <month> Septem-ber </month> <year> 1994 </year>
Reference-contexts: Cichocki and his colleagues [11] also proposed a self-adaptive step-size. However, our attempt here is to find a step-size which is directly based on the evolution of the algorithm. Moreover, we propose a neural network consisting of two layers of ICA algorithms. Some works <ref> [6, 16, 9] </ref> suggested to carry out whitening before the ICA algorithm in order to orthogonalize the inputs, which yields a faster convergence. The basis of our two-layer network is the same. <p> However, instead of the con ventional Euclidean gradient method [6], that reads ^ k+1 = ^ k k @ ^ k we rather use the following gradient ^ k+1 = ^ k k = @ ^ k where = is a positive definite matrix. This is called the relative <ref> [9] </ref>, natural, or Riemannian gradient [1, 2]. This algorithm works better in general because the parameter space of neural networks is Rie-mannian [3]. <p> However, in most of the works, the gradient method shown above was derived using only the weight matrix B as the parameter to be estimated. For this case, we have = = B T B and the weights of B are updated by <ref> [9] </ref> B k+1 = B k k [I N (z k )]B k ; (7) where N () is a non-linear function. <p> An interesting solution is to preserve the energy of the input signal, normalizing the weights by [14] W = jdet (W)j 1 3.2 Equivariance Property An equivariant estimator fl () for an invertible nfin matrix M is defined as <ref> [9] </ref> fl (M z k ) = M fl (z k ): (13) This property can be applied to relative gradient algo rithms.
Reference: [10] <author> J-F. Cardoso and B. H. </author> <title> Laheld."Equivariant adaptive source separation" IEEE Trans. Signal Process., </title> <address> SP-44 , pp.3017 - 3030, </address> <year> 1996. </year>
Reference: [11] <author> A.Cichocki , S. Amari, M. Adachi and W. Kasprzak. </author> <title> "Self-adaptive neural networks for blind separation of sources", </title> <booktitle> Proc. Intern. Symposium on Circuits and Systems, </booktitle> <volume> Vol. 2, No. 4, </volume> <pages> pp. 157-160, </pages> <year> 1996. </year>
Reference-contexts: For this approach, we can solve the problem of bounds to the step-size and derive the optimum one for one step convergence. In this field, there is the work of Douglas and Cichocki [15], with focus on decorrelation networks. Cichocki and his colleagues <ref> [11] </ref> also proposed a self-adaptive step-size. However, our attempt here is to find a step-size which is directly based on the evolution of the algorithm. Moreover, we propose a neural network consisting of two layers of ICA algorithms. <p> The index as in (27) was calculated for each simulation, ant they are shown in Fig.3. The signals recovered by the proposed network are shown in Fig.4. The "recovered signals" were obtained after normalizing the weights as in (12). A = <ref> [11; 0:91] </ref> 8 Discussion By looking at Fig.3 we can take the following conclusions: * The filtering is important in order to have less variance after convergence. This is because the lower frequency signal (trend) was removed.
Reference: [12] <author> A.Cichocki, S. Amari, M. Adachi and W. Kasprzak. </author> <title> "Local adaptive learning algorithms for blind separation of natural images", </title> <booktitle> Neural Network World, </booktitle> <volume> Vol. 6, No. 4, </volume> <pages> pp. 515-523, </pages> <year> 1996. </year>
Reference-contexts: In practice, it is not very necessary that this equation is true. For signals with a super-Gaussian distribution (kurtosis &gt; 0), it did not pose as a problem to separate them using (8). In the case of sub-Gaussian signals, Ci--chocki and his colleagues <ref> [12] </ref> suggested [12] the following equation B k+1 = B k + k (I z k y T An interesting discussion about this topic was carried out by Amari [4]. 3.1 Indefinition of the Solution Because the system works in a blind manner, B does not necessarily converge to the inverse <p> In practice, it is not very necessary that this equation is true. For signals with a super-Gaussian distribution (kurtosis &gt; 0), it did not pose as a problem to separate them using (8). In the case of sub-Gaussian signals, Ci--chocki and his colleagues <ref> [12] </ref> suggested [12] the following equation B k+1 = B k + k (I z k y T An interesting discussion about this topic was carried out by Amari [4]. 3.1 Indefinition of the Solution Because the system works in a blind manner, B does not necessarily converge to the inverse of A.
Reference: [13] <author> P. Comon, </author> <title> "Independent component analysis, a new concept?" Signal Processing, </title> <booktitle> 24, </booktitle> <pages> pp. 287 - 314, </pages> <year> 1994. </year>
Reference-contexts: We can only affirm that C = DP, where D is a diagonal, and P is a permutation matrix <ref> [13] </ref>. Without any a priori information, which is the case of blind source separation, nothing can be done concerning to the permutation, but we can still normalize the weight matrix to avoid the problem of random scaling.
Reference: [14] <author> G. Deco and W. </author> <title> Brauer, "Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures" Neural Networks, </title> <journal> Vol. </journal> <volume> 8, </volume> <pages> pp. 525 - 535, </pages> <year> 1995. </year>
Reference-contexts: An interesting solution is to preserve the energy of the input signal, normalizing the weights by <ref> [14] </ref> W = jdet (W)j 1 3.2 Equivariance Property An equivariant estimator fl () for an invertible nfin matrix M is defined as [9] fl (M z k ) = M fl (z k ): (13) This property can be applied to relative gradient algo rithms.
Reference: [15] <author> S. Douglas and A. </author> <title> Cichocki , "Neural networks for blind decor-relation of signals" IEEE Trans. Signal Processing, </title> <note> (to appear). </note>
Reference-contexts: For this approach, we can solve the problem of bounds to the step-size and derive the optimum one for one step convergence. In this field, there is the work of Douglas and Cichocki <ref> [15] </ref>, with focus on decorrelation networks. Cichocki and his colleagues [11] also proposed a self-adaptive step-size. However, our attempt here is to find a step-size which is directly based on the evolution of the algorithm. Moreover, we propose a neural network consisting of two layers of ICA algorithms. <p> Thus, we can rewrite (20) as k + 2 k : (21) Notice that when deriving (21) from (20) the orthogonal property of Q was used 3 . 2 The following steps are similar to the one carried out by Douglas and Cichocki <ref> [15] </ref>. 3 For example Q T TV k Q = Q T TQQ T V k Q.
Reference: [16] <author> Karhunen, </author> <title> J.:"Neural Approaches to Independent Component Analysis and Source Separation.", </title> <booktitle> Proc. 4th European Symposium on Artificial Neural Networks (ESANN'96), 1996, </booktitle> <address> Bruges, Belgium. </address>
Reference-contexts: Cichocki and his colleagues [11] also proposed a self-adaptive step-size. However, our attempt here is to find a step-size which is directly based on the evolution of the algorithm. Moreover, we propose a neural network consisting of two layers of ICA algorithms. Some works <ref> [6, 16, 9] </ref> suggested to carry out whitening before the ICA algorithm in order to orthogonalize the inputs, which yields a faster convergence. The basis of our two-layer network is the same. <p> The labels are as follows: solid: ICA with adptive learning rate; dotted: ICA with a constant learning rate of 2 fi 10 4 ; dashed: ICA with a constant learning rate of 2 fi 10 5 . Some words are necessary about the two-layers network. Some works, e.g., [7], <ref> [16] </ref> have proposed to carry out whitening before the ICA processing in order to orthogo-nalize the input components. In the same way, we have used the first layer to force the algorithm to search for independent components.
Reference: [17] <author> P. Laguna, R. Jane, O. Meste, P. Poon, P. Caminal, H. Rix and N. V. Thakor., </author> <title> "Adaptive filter for event-related bioelectric signals using an impulse correlated reference input: Comparison with signal averaging techniques" IEEE Trans. </title> <journal> Biomed. Eng., BME-39 , pp.1032-1043, </journal> <year> 1992. </year>
Reference-contexts: 1 Introduction Many attempts were carried out to eliminate corrupting artifacts from the actual cardiac one when measuring the electrocardiographic (ECG) signal. Cardiac signals show the well known repeating and almost periodic pattern. This caracteristic of physiological signals was already explored in some works (e.g, <ref> [5, 17, 21] </ref>) by synchronizing the parameters of the filter with the period of the signal. However, those filters fail to remove the interference when it has the same frequency of the cardiac signal. <p> This large number of works may be explained because the ICA algorithms are in general elegant, simple and may deal with signals that second order statistics (SOS) methods 1 in general do not work. This is because SOS algorithms usually search for 1 such as the one proposed in <ref> [5, 17, 21] </ref>. a solution that decorrelates the input signals while ICA looks for an independent solution. ICA is based on the following principle.
Reference: [18] <author> S. Makeig, A. J. Bell, T-P Jung, T. </author> <title> Sejnowski "Independent component analysis of electroencephalographic data" Advances in Neural Information Processing Systems, 8, </title> <publisher> MIT press, </publisher> <year> 1996. </year>
Reference: [19] <author> A. Papoulis. </author> <title> Probability, random variables, and stochastic processes. </title> <publisher> McGraw-Hill, </publisher> <year> 1991. </year>
Reference: [20] <author> B. A. Pearlmutter and L. C. </author> <title> Parra, "A Context-Sensitive Generalization of ICA", </title> <booktitle> International Conference on Neural Information Processing, </booktitle> <address> Hong Kong, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: This is called the relative [9], natural, or Riemannian gradient [1, 2]. This algorithm works better in general because the parameter space of neural networks is Rie-mannian [3]. To obtain a better estimation, Pearlmutter and Parra <ref> [20] </ref> derived an algorithm that extracts many parameters related to the signal, and therefore their parameter space S = f ^ g was built with many variables.
Reference: [21] <author> C. Vaz, X. Kong and N.V. Thakor, </author> <title> "An adaptive estimation of periodic signals using a Fourier linear combiner"IEEE Trans. Sig. </title> <publisher> Process.,ASSP-42, pp.1-10,1994. </publisher>
Reference-contexts: 1 Introduction Many attempts were carried out to eliminate corrupting artifacts from the actual cardiac one when measuring the electrocardiographic (ECG) signal. Cardiac signals show the well known repeating and almost periodic pattern. This caracteristic of physiological signals was already explored in some works (e.g, <ref> [5, 17, 21] </ref>) by synchronizing the parameters of the filter with the period of the signal. However, those filters fail to remove the interference when it has the same frequency of the cardiac signal. <p> This large number of works may be explained because the ICA algorithms are in general elegant, simple and may deal with signals that second order statistics (SOS) methods 1 in general do not work. This is because SOS algorithms usually search for 1 such as the one proposed in <ref> [5, 17, 21] </ref>. a solution that decorrelates the input signals while ICA looks for an independent solution. ICA is based on the following principle.
References-found: 21

