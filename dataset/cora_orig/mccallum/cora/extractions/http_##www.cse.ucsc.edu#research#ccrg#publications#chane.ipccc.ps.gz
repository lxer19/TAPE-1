URL: http://www.cse.ucsc.edu/research/ccrg/publications/chane.ipccc.ps.gz
Refering-URL: http://www.cse.ucsc.edu/research/ccrg/publications.html
Root-URL: http://www.cse.ucsc.edu
Title: Adding Adaptive Flow Control to Swift/RAID  
Author: Chane L. Fullmer, Darrell D. E. Long Luis-Felipe Cabrera 
Date: January 12, 1995  
Affiliation: Computer and Information Sciences University of California, Santa Cruz  Computer Science Department IBM Almaden Research Center  
Abstract: We discuss an adaptive flow control mechanism for the Swift/RAID distributed file system. Our goal is to achieve near-optimal performance on heterogeneous networks where available load capacity varies due to other network traffic. The original Swift/RAID prototype used synchronous communication, achieving throughput considerably less than available network capacity. We designed and implemented an adaptive flow control mechanism that provides greatly improved performance. Our design uses a simple automatic repeat request (ARQ) go back N protocol coupled with the congestion avoidance and control mechanism developed for the Transmission Control Protocol (TCP). The Swift/RAID implementation contains a transfer plan executor to isolate all of the communications code from the rest of Swift. The adaptive flow control design was implemented entirely in this module. Results from experimental data show the adaptive design achieving an increase in throughput for reads from 671 KB/s for the original synchronous implementation to 927 KB/s (a 38% increase) for the adaptive prototype, and an increase from 375 KB/s to 559 KB/s (a 49% increase) in write throughput. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bertsekas and R. Gallager, </author> <title> Data Networks, </title> <booktitle> 2nd Edition. </booktitle> <address> Prentice-Hall,Inc., </address> <year> 1992. </year>
Reference-contexts: Packets were transmitted one at a time and the sender waited until an acknowledgment was received from the destination before new packets were sent. This type of operation amounts to an automatic repeat request (ARQ) Stop-and-Wait protocol <ref> [1] </ref>. The protocol is simple and error free, but does not achieve as high a throughput as is possible with other protocols such as ARQ Go back N [1]. The Swift/RAID prototype was developed to add redundancy to the original Swift system [9]. <p> This type of operation amounts to an automatic repeat request (ARQ) Stop-and-Wait protocol <ref> [1] </ref>. The protocol is simple and error free, but does not achieve as high a throughput as is possible with other protocols such as ARQ Go back N [1]. The Swift/RAID prototype was developed to add redundancy to the original Swift system [9]. The prototype uses a transfer plan executor to execute a transfer plan set (one transfer plan set is generated per user request). <p> We chose an ARQ Go Back N protocol <ref> [1] </ref> as our main mechanism. To handle the avoidance of time-outs and variable network capacity, we have added congestion avoidance similar to that used for TCP [7]. The Swift/RAID architecture implementation is modular, with all of the communications code placed in one highly cohesive module, the transaction driver module. <p> The adaptive RAID-4 and RAID-5 prototypes are both able to achieve read throughputs in excess of 900 KB/s. Bertsekas and Gallager <ref> [1] </ref> have discussed that one of the limitations of end-to-end window flow control is the tradeoff of choosing a window size small window sizes keep packets in the subnet low and congestion to a minimum, but large windows allow higher rates of transmission and maximum throughput during light traffic conditions.
Reference: [2] <author> D. R. Boggs, J. C. Mogul, and C. A. Kent, </author> <title> Measured capacity of an ethernet: Myths and reality, </title> <booktitle> in Proceedings of SIGCOMM 88, </booktitle> <pages> pp. 222-234, </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: The improvement is less dramatic for larger configurations. However, it should be noted that in all cases the read operation is brought up to about 900 KB/s, which is very close to the usable bandwidth of the network <ref> [2] </ref>, making further improvements difficult. RAID-4 read operations show an improvement of 35% over the Stop and Wait protocol (from 671 KB/s to 927 KB/s) and RAID-5 improved up to 23% (from 729 KB/s to 896 KB/s).
Reference: [3] <author> L.-F. Cabrera and D. D. E. Long, Swift: </author> <title> a storage architecture for large objects, </title> <booktitle> in Digest of papers, 11th IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pp. 123-8, </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: half of the write throughput for RAID level 4 (about 900 KB/s versus 482 KB/s for the new design) and 60% of the throughput for RAID level 5 write operations (about 900 KB/s versus 559 KB/s). 2 Swift Distributed File System Architecture Our goal is to study how the Swift <ref> [4, 3] </ref> architecture can make the most effective use of available network capacity. Swift is designed to support high data rates in a general purpose distributed system. It is built on the notion of striping data over multiple storage agents and driving them in parallel. <p> In particular, the distribution agents, storage mediators, and storage agents are involved in planning and actual data transfer operations between the client and an array of disks, which are the principal storage media. We refer the reader to <ref> [4, 3] </ref> for details of the functionality of these components of Swift. The communications protocols used in the original Swift system operated in a synchronous manner. Packets were transmitted one at a time and the sender waited until an acknowledgment was received from the destination before new packets were sent.
Reference: [4] <author> L.-F. Cabrera and D. D. E. Long, Swift: </author> <title> Using distributed disk striping to provide high I/O data rates, </title> <journal> Computing Systems, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 405-36, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Multimedia and scientific visualization require huge files containing images or digitized sounds. Current systems can only offer a fraction of the data rates required by these applications. The Swift distributed file system architecture <ref> [4] </ref> was introduced to address this problem. Here we consider the problem of using the system in an efficient manner to maximize network throughput. Due to network flow problems, synchronous operation of Swift/RAID was thought to be necessary. However, this resulted in diminished throughput due to the large waiting times. <p> Our design has also attained an increase of 28% over the reported throughput of the original non-redundant Swift prototype <ref> [4, 9] </ref> (about 700 KB/s versus 927 KB/s for the new design) for read operations (both RAID levels 4 and 5), and has achieved one half of the write throughput for RAID level 4 (about 900 KB/s versus 482 KB/s for the new design) and 60% of the throughput for RAID <p> half of the write throughput for RAID level 4 (about 900 KB/s versus 482 KB/s for the new design) and 60% of the throughput for RAID level 5 write operations (about 900 KB/s versus 559 KB/s). 2 Swift Distributed File System Architecture Our goal is to study how the Swift <ref> [4, 3] </ref> architecture can make the most effective use of available network capacity. Swift is designed to support high data rates in a general purpose distributed system. It is built on the notion of striping data over multiple storage agents and driving them in parallel. <p> In particular, the distribution agents, storage mediators, and storage agents are involved in planning and actual data transfer operations between the client and an array of disks, which are the principal storage media. We refer the reader to <ref> [4, 3] </ref> for details of the functionality of these components of Swift. The communications protocols used in the original Swift system operated in a synchronous manner. Packets were transmitted one at a time and the sender waited until an acknowledgment was received from the destination before new packets were sent.
Reference: [5] <author> C. A. Eldridge, </author> <title> Rate controls in standard transport layer protocols, </title> <journal> ACM Computer Communication Review, </journal> <volume> vol. 22, no. 3, </volume> <year> 1992. </year>
Reference-contexts: Other window sizes of 1, 3, 4, 5 showed little, or no improvement for writes and in the case for a window size of five the results tended to be below all others because of the swamping of the client Ethernet queue. This is also supported by Eldridge <ref> [5] </ref>. Read operations did better with higher window sizes, but not significantly better (approximately 5% in most cases). Most of our improvement in throughput was gained by the simple self-clocking [7] of the data packets with the acknowledgments sent by the destination as packets were received.
Reference: [6] <author> M. Gerla and L. Kleinrock, </author> <title> Flow control: A comparative survey, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. COM-28, no. 4, </volume> <pages> pp. 553-574, </pages> <year> 1980. </year>
Reference-contexts: Because of the buffering needs of the Swift/RAID system, it lends itself to a class of flow control mechanisms known as window flow control. The communications primitives available to us at the client/server session level, the specific class of end-to-end windowing (also know as entry-to-exit flow control <ref> [6] </ref>) make it well suited for our prototype. In this scheme, the sender has knowledge of the destination's buffer capacity and only sends packets out in batches of sizes less up to the available buffer capacity. The destination sends permits (acknowledgments) of the packets received.
Reference: [7] <author> V. Jacobson, </author> <title> Congestion avoidance and control, </title> <booktitle> in Proceedings of SIGCOMM 88, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: To achieve a higher throughput an adaptive flow control scheme has been designed and implemented. The design is based on an ARQ go back N protocol and includes the adaptive congestion avoidance and control techniques used for the Transmission Control Protocol (TCP) <ref> [7] </ref>. <p> One solution to the dynamic window adjustment was suggested by the congestion avoidance and control mechanisms in 4.3 BSD Reno TCP <ref> [7] </ref>. 4 Implementation of the Adaptive Prototype Our design addresses network buffer saturation, avoiding time-outs and adjusting to variable network capacity. We chose an ARQ Go Back N protocol [1] as our main mechanism. <p> We chose an ARQ Go Back N protocol [1] as our main mechanism. To handle the avoidance of time-outs and variable network capacity, we have added congestion avoidance similar to that used for TCP <ref> [7] </ref>. The Swift/RAID architecture implementation is modular, with all of the communications code placed in one highly cohesive module, the transaction driver module. All modifications to accomplish the flow-control/congestion avoidance were applied to this module: all versions of the prototype remained operational. <p> This is also supported by Eldridge [5]. Read operations did better with higher window sizes, but not significantly better (approximately 5% in most cases). Most of our improvement in throughput was gained by the simple self-clocking <ref> [7] </ref> of the data packets with the acknowledgments sent by the destination as packets were received. This is because once the packet traffic has stabilized, the acknowledgment packets are being returned at the rate at which the receiver is pulling packets off of the network.
Reference: [8] <author> D. D. Kouvatsos and A. T. Othman, </author> <title> Optimal flow control of end-to-end packet-switched network with random routing, </title> <journal> IEEE Proceedings, </journal> <volume> vol. 136 Pt E, no. 2, </volume> <pages> pp. 90-100, </pages> <year> 1989. </year>
Reference-contexts: The destination sends permits (acknowledgments) of the packets received. Upon receiving the permit from the destination the sender continues by sending another batch of packets. This method is efficient, and can approach optimal performance of a given system <ref> [8] </ref>. A disadvantage of this scheme is that of choosing a window size. The choice of a window size is a trade-off: small window sizes limit the congestion and tend to avoid large delays, and large window sizes allow full-speed transmission and maximum throughput under lightly loaded conditions.
Reference: [9] <author> D. D. E. Long, B. R. Montague, and L.-F. Cabrera, Swift/RAID: </author> <title> A distributed RAID system, </title> <journal> Computing Systems, </journal> <volume> vol. 7, no. 3, </volume> <pages> pp. 333-59, </pages> <year> 1994. </year>
Reference-contexts: Our design has also attained an increase of 28% over the reported throughput of the original non-redundant Swift prototype <ref> [4, 9] </ref> (about 700 KB/s versus 927 KB/s for the new design) for read operations (both RAID levels 4 and 5), and has achieved one half of the write throughput for RAID level 4 (about 900 KB/s versus 482 KB/s for the new design) and 60% of the throughput for RAID <p> The protocol is simple and error free, but does not achieve as high a throughput as is possible with other protocols such as ARQ Go back N [1]. The Swift/RAID prototype was developed to add redundancy to the original Swift system <ref> [9] </ref>. The prototype uses a transfer plan executor to execute a transfer plan set (one transfer plan set is generated per user request). Execution begins when the client transfer plan executor has sent the transfer plan to each server. <p> Read and write throughput was measured for file transfers up to one megabyte and compared with the original nonredundant prototype as well as the Swift/RAID prototype. The throughput measurements performed to evaluate the prototype were essentially the same as those reported in <ref> [9] </ref>. In fact, the identical test programs and RAID-4 and RAID-5 modules were used. 5.1 Methodology The block size was changed from 8192 bytes (in the Swift/RAID prototype) to 7340 bytes to avoid fragmentation of the Ethernet packets. <p> Parity calculations have been shown to be a factor in limiting the ability of the workstations used to achieve better performance. Experiments have found the cost of parity to be 200KB/s in the Swift/RAID Stop and Wait prototype using a SparcStation 2 <ref> [9] </ref>. Additionally, the bottleneck created by the use of a single parity node becomes as issue for large writes, and tends to keep RAID-4 performance below others, such as RAID-5. Also, as node and window sizes increase, we run into the same buffer limitations as we did with RAID-0.
Reference: [10] <author> S. W. Ng and R. L. Mattson, </author> <title> Maintaining good performance in disk arrays during failure via uniform parity group distribution, </title> <booktitle> in Proceedings of the 5th International Symposium on High-Performance Distributed Computing, </booktitle> <pages> pp. 260-69, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: Our results agree with other research that has shown the throughput for writes in systems of fewer than four nodes actually increase during degraded mode for both RAID-4 and RAID-5 systems <ref> [10] </ref>. These results show a decrease in total load due to writes for RAID-5 systems where the number of disks is less than eight, and average load decrease for fewer than four disks.
References-found: 10

