URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1261/CS-TR-95-1261.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-95-1261/
Root-URL: http://www.cs.wisc.edu
Email: galileo@cs.wisc.edu  
Title: The Declining Effectiveness of Dynamic Caching for General-Purpose Microprocessors  
Author: Douglas C. Burger, James R. Goodman, Alain Kgi 
Address: 1210 West Dayton Street, Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: The computational power of commodity general-purpose microprocessors is racing to truly amazing levels. As peak levels of performance rise, the building of memory systems that can keep pace becomes increasingly problematic. We claim that in addition to the latency associated with waiting for operands, the bandwidth of the memory system, especially that across the chip boundary, will become a progressively greater limit to high performance. After describing the current state of microsolutions aimed at alleviating the memory bottleneck, this paper postulates that dynamic caches themselves use memory inefficiently and will impede attempts to solve the memory problem. We present an analysis of several important algorithms, which shows that increasing levels of integration will not result in computational requirements outstripping off-chip bandwidth needs, thereby preserving the memory bottleneck. We then present results from two sets of simulations, which measured both the efficiency with which current caching techniques use memory (generally less than 20%), and how well (or poorly) caches reduce traffic to main memory (cache sizes up to 2000 times worse than optimal). We then discuss how two classes of techniques, (i) decoupling memory operations from computation, and (ii) explicit compiler management of the memory hierarchy, provide better long-term solutions to lowering a programs memory latencies and bandwidth requirements. Finally, we describe Galileo, a new project that will attempt to provide a long-term solution to the pernicious memory bottleneck. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Santosh G. Abraham, Rabin A. Sugumar, B. R. Rau, and Rajiv Gupta. </author> <title> Predictability of Load/Store Instruction Latencies. </title> <booktitle> In Proceedings of the 26th International Symposium on Microarchitecture, </booktitle> <pages> pages 139152, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The rate at which data can be moved across these pins is also increasing more slowly, as are the bandwidths of inter-chip connections. l Access times for DRAMs are diminishing more slowly than are cycle times for CPUs, at the annual rate of 5-10% <ref> [1] </ref>. <p> Bus clock speeds and data widths are increasing. Page-mode DRAMs reduce the access latency to multiple adjacent memory references [33]. Decreasing feature sizes are driving down DRAM cycle times, by 5-10% <ref> [1] </ref> per year. Techniques for tolerating latency have been widely studied, and there is still considerable ongoing research in this area. We partition these techniques into two classes: scheduling and prefetching. Scheduling attempts to overlap a memory access with other useful instructions, exploiting available instruction-level parallelism. <p> However, DRAM access times are dropping much more slowly than those of CPU clockson the order of 5-10% per year <ref> [39, 14, 1] </ref>. The rate of increase of processor pins is much slower than that of transistor density. Although there are significant breakthroughs in packaging technology on the horizon, the issues of reliability, power, and cost will prevent pins from sustaining growth commensurate with the rate of transistor increase. <p> In many cases, the hardware is forced to re-create these analyses for both correctness and better performance [45]. Substantial work has been done that tries to enable the compiler to circumvent the cache, performing analyses to work against the hardware <ref> [1, 53, 52] </ref>. Several levels of compiler control are possible for the local memory hierarchy. <p> These techniques would improve the efficiency of the cache, by preventing the replacement of other data that might still be live with data that will soon be dead <ref> [1] </ref>. By mapping different portions of the address space into different cache-like memory units, the compiler could place classes of data into units whose hardware policies were most suitable for the access pattern of a particular class.
Reference: [2] <author> Todd M. Austin, T. N. Vijaykumar, and Gurindar S. Sohi. </author> <title> Knapsack: A Zero-Cycle Memory Hierarchy Component. </title> <type> Technical Report 1189, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Different hardware units could have different capacities, access times, associativities, replacement policies, or even block sizes. Some of these units could reside closer to the datapath than the level one cache <ref> [2] </ref>. A longer-term option is to provide local memory units in addition to the caches; units that are completely managed by the compiler. Such units could reduce both latency (better hit rates) and memory traffic considerably.
Reference: [3] <author> Jean-Loup Baer and Tien-Fu Chen. </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <pages> pages 176186, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: While their exploration was performed in the context of shared-memory multiprocessors, some of their conclusions are more broadly applicable. Prefetching techniques consist of both hardware and software prefetches. Hardware prefetching typically uses dynamic stride detection to perform run-time calculation of prefetch addresses to be issued <ref> [3, 15, 16] </ref>. The overheads of hardware prefetching are the cost for the additional hardware, and the limited ability of the dynamic units to perform any prefetching other than through arrays with linear strides. A different form of hardware prefetch-ing consists of stream buffers [27, 36].
Reference: [4] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: We varied cache sizes, simulating cache sizes of all powers of two between 4KB and 2MB, inclusively. We simulated caches with set associativities of 1, 2, and 4. The experiments were run on the following benchmarks: buk, compress, eqntott, g++, su2cor, and swm256. Buk is a NAS <ref> [4] </ref> kernel that implements bucket sort. G++ is release 2.6.0 of the Gnu C++ compiler. It generated the assembly code of the preprocessed CPU module of a multiprocessor simulator, and was run with full optimization enabled. Compress, eqntott, swm256, and su2cor are all from the Spec92 [50] suite.
Reference: [5] <author> Forest Baskett. </author> <title> Keynote address. </title> <booktitle> International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The growing inability of current memory systems to keep up with processor requests has significant ramifications for the design of microprocessors in the next decade. Baskett recently estimated <ref> [5] </ref> that the annual rate of performance increase for single-chip microprocessors is 80% (roughly 5% per month). Extrapolating this rate ten years into the future suggests single-processor performance of roughly 30,000 Spec-marks by the year 2005.
Reference: [6] <author> L. A. Belady. </author> <title> A Study of Replacement Algorithms for a Virtual-Storage Computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2):78 101, </volume> <year> 1966. </year>
Reference-contexts: The latter memory unit, to which we will refer as n-opt, is simulated as a fully-associative (level-one) cache that has a block size of one word. The replacement policy uses MIN <ref> [6] </ref>, in which dead cache blocks are always victimized. If every block in the cache is live, the cache block that will be referenced farthest in the future is chosen as a victim.
Reference: [7] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the Fourth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 4052, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Software prefetching is much more exible than hardware prefetching, having the advantage of compile-time knowledge, but pays the price of software overhead, both in instructions issued and code size <ref> [7, 28, 35] </ref>. Chen et al. [10] examined the trade-offs between prefetching data directly into the cache and prefetching into a prefetch buffer. The lack of run-time knowledge can also be an impediment to purely software techniques.
Reference: [8] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing Memory Latency via Non-blocking and Prefetching Caches. </title> <booktitle> In Proceedings of the Fifth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 5161, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The overheads of hardware prefetching are the cost for the additional hardware, and the limited ability of the dynamic units to perform any prefetching other than through arrays with linear strides. A different form of hardware prefetch-ing consists of stream buffers [27, 36]. Chen and Baer <ref> [8] </ref> evaluated the effectiveness of lockup-free caches and hardware prefetching, and proposed a hybrid scheme based on a combination of these approaches.
Reference: [9] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 223232, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The lack of run-time knowledge can also be an impediment to purely software techniques. Some promising approaches use hybrid hardware/software techniques, issuing limited instructions that provide hints to the prefetch hardware [11]. Chen and Baer <ref> [9] </ref> studied software and hardware prefetching schemes in the context of a multiprocessor and proposed a hybrid approach combining software and hardware schemes. 2.3 The bandwidth bottleneck We believe that these substantial efforts to reduce latencies of contention-free memory references will ultimately be successful, even for less regular non-scientific codes.
Reference: [10] <author> William Y. Chen, Scott A. Mahlke, Pohua P. Chang, and Wem mei W. Hwu. </author> <title> Data Access Microarchitectures for Superscalar Processors with Compiler-Assisted Data Prefetching. </title> <booktitle> In Proceedings of the 24th International Symposium on Microarchitecture, </booktitle> <pages> pages 6973, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Software prefetching is much more exible than hardware prefetching, having the advantage of compile-time knowledge, but pays the price of software overhead, both in instructions issued and code size [7, 28, 35]. Chen et al. <ref> [10] </ref> examined the trade-offs between prefetching data directly into the cache and prefetching into a prefetch buffer. The lack of run-time knowledge can also be an impediment to purely software techniques. Some promising approaches use hybrid hardware/software techniques, issuing limited instructions that provide hints to the prefetch hardware [11].
Reference: [11] <author> Tzi-cker Chiueh. </author> <title> Sunder: A Programmable Hardware Prefetch Architecture for Numerical Loops. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 488497, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The lack of run-time knowledge can also be an impediment to purely software techniques. Some promising approaches use hybrid hardware/software techniques, issuing limited instructions that provide hints to the prefetch hardware <ref> [11] </ref>. <p> When massive prefetching is performed, the current and future working sets may conict. Proposals in the literature have suggested providing separate units, one to contain the current working set and the other to fill simultaneously with prefetched data <ref> [11, 14] </ref>. Physically partitioning the memory optimizes for a particular workload size, and will generally be inferior to a logical, programmable partitioning of a single unit. As more and more par 1.
Reference: [12] <author> Bob Cmelik and David Keppel. Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pages 128137, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: A frame in the cache is defined to be live if its contents will be read again before they are written; i.e, a blocks lifetime extends from its first write until the last time it is read. We have modified DineroIII [21] and used Shade <ref> [12] </ref>, a tracing tool from Sun, to produce measurements of a caches efficiency. Live time is considered to be the time between a store and a read hit, or two consecutive read hits. <p> Since MIN assumes a perfect oracle, however, and is unlikely to be realized in the near future, n-opt is a sufficient bound for our study. To obtain these results, we used Shade <ref> [12] </ref> to generate an address trace in an appropriate format. The traces were then fed to Cheetah [51], with which we computed miss ratios for both the n-opt model and a normal (L1) cache, using a least-recently-used replacement policy with set associativities of 1 and 4 (lru-1 and lru-4).
Reference: [13] <author> Peter Dahl and Matthew OKeefe. </author> <title> Reducing Memory Traffic with CRegs. </title> <booktitle> In Proceedings of the 27th International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: It is unlikely that this disambiguation problem will ever be solved, unless programming models evolve to support a solution. One direction that holds promise for increasing the classes of variables that can be managed explicitly is run-time disambiguation <ref> [13, 17, 25] </ref>.
Reference: [14] <author> Stefanos Damianakis, Kai Li, and Anne Rogers. </author> <title> An Analysis of a Combined Hardware-Software Mechanism for Speculative Loads. </title> <type> Technical Report TR-455-94, </type> <institution> Prince-ton University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: However, DRAM access times are dropping much more slowly than those of CPU clockson the order of 5-10% per year <ref> [39, 14, 1] </ref>. The rate of increase of processor pins is much slower than that of transistor density. Although there are significant breakthroughs in packaging technology on the horizon, the issues of reliability, power, and cost will prevent pins from sustaining growth commensurate with the rate of transistor increase. <p> When massive prefetching is performed, the current and future working sets may conict. Proposals in the literature have suggested providing separate units, one to contain the current working set and the other to fill simultaneously with prefetched data <ref> [11, 14] </ref>. Physically partitioning the memory optimizes for a particular workload size, and will generally be inferior to a logical, programmable partitioning of a single unit. As more and more par 1.
Reference: [15] <author> John W. C. Fu and Janak H. Patel. </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 5463, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: While their exploration was performed in the context of shared-memory multiprocessors, some of their conclusions are more broadly applicable. Prefetching techniques consist of both hardware and software prefetches. Hardware prefetching typically uses dynamic stride detection to perform run-time calculation of prefetch addresses to be issued <ref> [3, 15, 16] </ref>. The overheads of hardware prefetching are the cost for the additional hardware, and the limited ability of the dynamic units to perform any prefetching other than through arrays with linear strides. A different form of hardware prefetch-ing consists of stream buffers [27, 36].
Reference: [16] <author> John W. C. Fu, Janak H. Patel, and Bob L. Janssens. </author> <title> Stride Directed Prefetching in Scalar Processor. </title> <booktitle> In Proceedings of the 25th International Symposium on Microarchitecture, </booktitle> <pages> pages 102110, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: While their exploration was performed in the context of shared-memory multiprocessors, some of their conclusions are more broadly applicable. Prefetching techniques consist of both hardware and software prefetches. Hardware prefetching typically uses dynamic stride detection to perform run-time calculation of prefetch addresses to be issued <ref> [3, 15, 16] </ref>. The overheads of hardware prefetching are the cost for the additional hardware, and the limited ability of the dynamic units to perform any prefetching other than through arrays with linear strides. A different form of hardware prefetch-ing consists of stream buffers [27, 36].
Reference: [17] <author> David M. Gallagher, William Y. Chen, Scott A. Mahlke, John C. Gyllenhaal, and Wen mei W. Hwu. </author> <title> Dynamic Memory Disambiguation Using the Memory Conict Buffer. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183193, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It is unlikely that this disambiguation problem will ever be solved, unless programming models evolve to support a solution. One direction that holds promise for increasing the classes of variables that can be managed explicitly is run-time disambiguation <ref> [13, 17, 25] </ref>.
Reference: [18] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Hiding Memory Latency using Dynamic Scheduling in Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2233, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The issuing non-blocking loads can be thought of as a first step toward decoupling memory accesses from computation. Gharachorloo, Gupta and Hennessy <ref> [18] </ref> have explored the use of dynamically scheduled processors to hide memory latencies. While their exploration was performed in the context of shared-memory multiprocessors, some of their conclusions are more broadly applicable. Prefetching techniques consist of both hardware and software prefetches.
Reference: [19] <author> James R. Goodman. </author> <title> Using Cache Memory To Reduce Processor-Memory Traffic. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Alternatively, a phase is bandwidth-bound if providing more memory bandwidth would eliminate processor stalling. 3 caches are commonly thought of as latency-reducing units, local memories in general, and caches in particular, have also been identified <ref> [19] </ref> as a way of reducing the bandwidth requirements. Traffic to the next lower level of the memory hierarchy is reduced by the cache servicing some fraction of the requests at the higher level.
Reference: [20] <author> James R. Goodman, Jian-tu Hsieh, Koujuch Liou, Andrew R. Pleszkun, P. B. Schechter, and Honesty C. Young. </author> <title> PIPE: A VLSI Decoupled Architecture. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, pages 2027, </booktitle> <month> June </month> <year> 1985. </year>
Reference-contexts: In this section, we discuss two techniques that we feel hold promise, and then propose an architectural philosophy, based on these two techniques, that will be our projects first point of evaluation. 4.1 Decoupling Although explicitly decoupled architectures <ref> [43, 44, 20, 47] </ref> have not achieved success in the mainstream commercial market, the philosophy of decoupling memory operations from computation is becoming increasingly visible in modern commercial processors, such as the PowerPC [48] and the MIPS TFP [24]. Decoupled architectures have several fundamental advantages.
Reference: [21] <author> Mark D. Hill, James R. Larus, Alvin R. Lebeck, Madhusudhan Talluri, and David A. Wood. </author> <title> Wisconsin Architectural Research Tool Set. Computer Architecture News, </title> <address> 21(4):810, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: A frame in the cache is defined to be live if its contents will be read again before they are written; i.e, a blocks lifetime extends from its first write until the last time it is read. We have modified DineroIII <ref> [21] </ref> and used Shade [12], a tracing tool from Sun, to produce measurements of a caches efficiency. Live time is considered to be the time between a store and a read hit, or two consecutive read hits.
Reference: [22] <author> Jia-Wei Hong and H. T. Kung. </author> <title> I/O Complexity: the Red 13 Blue Pebble Game. </title> <booktitle> In Proceedings of the 13th Symposium on Theory of Computing, </booktitle> <pages> pages 326333, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: For example, the conventional algorithm of matrix multiply (multiplying two matrices) has total memory requirements that grow as , while computation grows as . This simplistic argument is misleading. Consider the conventional matrix multiplication, using a tiled algorithm where tiles are of size . It is easily shown <ref> [22, 32] </ref> that the traffic between the on and off-chip memory is proportional to . Assume that the processor is sufficiently fast for the algorithm implemented to take full advantage of the on-chip memory. Holding constant keeps the amount of computation constant.
Reference: [23] <author> L. P. Horwitz, R. M. Karp, R. E. Miller, and A. Winograd. </author> <title> Index Register Allocation. </title> <journal> Journal of the ACM, </journal> <volume> 13(1):43 61, </volume> <month> January </month> <year> 1966. </year>
Reference-contexts: This policy is called n-opt (near-optimal) because MIN is not optimal; in some cases it is preferable to evict a clean block that will be referenced sooner, rather than a dirty block that will be referenced later <ref> [23] </ref>. Since MIN assumes a perfect oracle, however, and is unlikely to be realized in the near future, n-opt is a sufficient bound for our study. To obtain these results, we used Shade [12] to generate an address trace in an appropriate format.
Reference: [24] <author> Peter Yan-Tek Hsu. </author> <title> Designing the TFP Microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(2):2333, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: our projects first point of evaluation. 4.1 Decoupling Although explicitly decoupled architectures [43, 44, 20, 47] have not achieved success in the mainstream commercial market, the philosophy of decoupling memory operations from computation is becoming increasingly visible in modern commercial processors, such as the PowerPC [48] and the MIPS TFP <ref> [24] </ref>. Decoupled architectures have several fundamental advantages. They implicitly provide aggressive latency tolerance, and permit much higher performance of the execute unit, through the use of queues and renaming. The access unit has the potential for optimizing memory accesses; this will be further discussed in Section 4.3.
Reference: [25] <author> Andrew S. Huang, Gert Slavenburg, and John Paul Shen. </author> <title> Speculative Disambiguation: A Compilation Technique for Dynamic Memory Disambiguation. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 200210, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It is unlikely that this disambiguation problem will ever be solved, unless programming models evolve to support a solution. One direction that holds promise for increasing the classes of variables that can be managed explicitly is run-time disambiguation <ref> [13, 17, 25] </ref>.
Reference: [26] <author> Fred Jones. </author> <title> A New Era of Fast Dynamic RAMs. </title> <journal> IEEE Spectrum, </journal> <volume> 29(10):4349, </volume> <month> October </month> <year> 1992. </year>
Reference: [27] <author> Norman P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The overheads of hardware prefetching are the cost for the additional hardware, and the limited ability of the dynamic units to perform any prefetching other than through arrays with linear strides. A different form of hardware prefetch-ing consists of stream buffers <ref> [27, 36] </ref>. Chen and Baer [8] evaluated the effectiveness of lockup-free caches and hardware prefetching, and proposed a hybrid scheme based on a combination of these approaches.
Reference: [28] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> An Architecture for Software-Controlled Data Prefetching. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 4353, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Software prefetching is much more exible than hardware prefetching, having the advantage of compile-time knowledge, but pays the price of software overhead, both in instructions issued and code size <ref> [7, 28, 35] </ref>. Chen et al. [10] examined the trade-offs between prefetching data directly into the cache and prefetching into a prefetch buffer. The lack of run-time knowledge can also be an impediment to purely software techniques.
Reference: [29] <author> David Kroft. </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8187, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Prefetching techniques, conversely, provide non-binding hints to the memory system that attempt to bring imminently referenced data closer to the processor. In order for memory accesses to be scheduled so that useful work may be performed in parallel, memory operations must be non-blocking. Lockup-free caches <ref> [29] </ref> allow multiple memory requests to be outstanding, and compilers attempt to place as many instructions as needed between a non-blocking load and the first use of its result. The issuing non-blocking loads can be thought of as a first step toward decoupling memory accesses from computation.
Reference: [30] <author> H. T. Kung. </author> <title> Memory Requirements for Balanced Computer Architectures. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 4954, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: It is possible that such increases will reduce memory traffic requirements enough 4 Algorithm Memory Computation Memory traffic Processing/ traffic ratio TMM Table 1: Application growth rates <ref> [30] </ref> Stencil FFT Sort O N 2 ( ) O N 3 ( ) O N 2 S ( ) f O N ( ) O N N 2 log ( ) O N N 2 S 2 loglog ( ) f 2 log O N ( ) O N N
Reference: [31] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This will permit it to optimize both on-chip memory scheduling and remote accesses. When making remote operations, the access units of this system would therefore greatly resemble hardware protocol processors from some current multiprocessor projects (Tempest and FLASH) <ref> [38, 31] </ref>, although they would be much more integrated, and would function at a considerably lower level. We also expect that the programming model and compiler technology will evolve to support a data-pushing model, rather than a request/response model (which may remain the worst-case default).
Reference: [32] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6374, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, the conventional algorithm of matrix multiply (multiplying two matrices) has total memory requirements that grow as , while computation grows as . This simplistic argument is misleading. Consider the conventional matrix multiplication, using a tiled algorithm where tiles are of size . It is easily shown <ref> [22, 32] </ref> that the traffic between the on and off-chip memory is proportional to . Assume that the processor is sufficiently fast for the algorithm implemented to take full advantage of the on-chip memory. Holding constant keeps the amount of computation constant.
Reference: [33] <author> Sally A. McKee and William A. Wulf. </author> <title> Access Ordering and Memory-Conscious Cache Utilization. </title> <booktitle> In Proceedings of the First International Symposium on High-Performance Computer Architecture, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Techniques for reducing the actual main memory latency include faster interconnects, more levels of off-chip caches, and reduced access times for DRAMs [26,49, 37]. Bus clock speeds and data widths are increasing. Page-mode DRAMs reduce the access latency to multiple adjacent memory references <ref> [33] </ref>. Decreasing feature sizes are driving down DRAM cycle times, by 5-10% [1] per year. Techniques for tolerating latency have been widely studied, and there is still considerable ongoing research in this area. We partition these techniques into two classes: scheduling and prefetching.
Reference: [34] <author> Geoffrey D. McNiven and Edward S. Davidson. </author> <title> Analysis for Memory Referencing Behavior For Design of Local Memories. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 5663, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The effective use of local (on-chip) memory will be essential for alleviating the memory bottleneck. We develop the notion of memory efficiency <ref> [34, 46] </ref>, to measure how successfully current microprocessors make use of their local memories (which are primarily cache memory). <p> In this section we characterize how well a given local memory (cache) is used. We define the efficiency of a given memory to be the average fraction of the memory that holds live data <ref> [34, 46] </ref> at any point in the execution of the program. A frame in the cache is defined to be live if its contents will be read again before they are written; i.e, a blocks lifetime extends from its first write until the last time it is read.
Reference: [35] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the Fifth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Software prefetching is much more exible than hardware prefetching, having the advantage of compile-time knowledge, but pays the price of software overhead, both in instructions issued and code size <ref> [7, 28, 35] </ref>. Chen et al. [10] examined the trade-offs between prefetching data directly into the cache and prefetching into a prefetch buffer. The lack of run-time knowledge can also be an impediment to purely software techniques. <p> Multiple outstanding requests can frequently return out of order. Caches have no concept of ordering, and may allow returning requests to evict one another. Some efforts to perform heavy prefetching into a cache have met with success <ref> [35] </ref>, but these efforts relied on extremely regular codes with detailed and specific analyses that would be difficult to generalize. These regular codes would be quite easy to manage in software without a cache, given the appropriate hardware support.
Reference: [36] <author> Subbarao Palacharla and R. E. Kessler. </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2433, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The overheads of hardware prefetching are the cost for the additional hardware, and the limited ability of the dynamic units to perform any prefetching other than through arrays with linear strides. A different form of hardware prefetch-ing consists of stream buffers <ref> [27, 36] </ref>. Chen and Baer [8] evaluated the effectiveness of lockup-free caches and hardware prefetching, and proposed a hybrid scheme based on a combination of these approaches.
Reference: [37] <author> Rambus Inc. </author> <title> Architectural Overview, </title> <address> Mountain View, Cal-ifornia, </address> <year> 1992. </year>
Reference-contexts: Traffic to the next lower level of the memory hierarchy is reduced by the cache servicing some fraction of the requests at the higher level. Techniques for reducing the actual main memory latency include faster interconnects, more levels of off-chip caches, and reduced access times for DRAMs <ref> [26,49, 37] </ref>. Bus clock speeds and data widths are increasing. Page-mode DRAMs reduce the access latency to multiple adjacent memory references [33]. Decreasing feature sizes are driving down DRAM cycle times, by 5-10% [1] per year.
Reference: [38] <author> Steven K. Reinhardt, James L. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2433, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This will permit it to optimize both on-chip memory scheduling and remote accesses. When making remote operations, the access units of this system would therefore greatly resemble hardware protocol processors from some current multiprocessor projects (Tempest and FLASH) <ref> [38, 31] </ref>, although they would be much more integrated, and would function at a considerably lower level. We also expect that the programming model and compiler technology will evolve to support a data-pushing model, rather than a request/response model (which may remain the worst-case default).
Reference: [39] <author> Anne Rogers and Kai Li. </author> <title> Software Support for Speculative Loads. </title> <booktitle> In Proceedings of the Fifth Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 3850, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: However, DRAM access times are dropping much more slowly than those of CPU clockson the order of 5-10% per year <ref> [39, 14, 1] </ref>. The rate of increase of processor pins is much slower than that of transistor density. Although there are significant breakthroughs in packaging technology on the horizon, the issues of reliability, power, and cost will prevent pins from sustaining growth commensurate with the rate of transistor increase. <p> A more severe bandwidth increase occurs with speculative prefetches and speculative loads. When the compiler has an insufficient number of instructions within a basic block to hide the memory latency, the prefetch or load may be lifted <ref> [39] </ref> across a branch, becoming speculative. Speculative memory operations that turn out to be unnecessary will thereby increase the total bandwidth requirement of a program. Latency tolerating techniques will generally only increase the bandwidth load on the memory system, not reduce it.
Reference: [40] <author> Richard M. Russel. </author> <title> The CRAY-1 Computer System. </title> <journal> Communications of the ACM, </journal> <volume> 21(1):6372, </volume> <month> January </month> <year> 1978. </year>
Reference-contexts: Every major general-purpose, commodity processor recently announced has an on-chip cache. The community of scientific machine designers has traditionally been reluctant to use caches, and many of the canonical scientific supercomputers used no caches at all <ref> [40] </ref>.
Reference: [41] <author> Alan Jay Smith. </author> <title> Cache Memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3):473530, </volume> <month> September </month> <year> 1982. </year>
Reference-contexts: They can thereby eliminate most references to main memory. They have also been extensively studied, and are consequently very well understood. Voluminous papers examining block sizes, associativities, replacement policies, virtual versus physical addressing, and unification versus separation of instruction and data caches have appeared in the literature <ref> [41, 42] </ref>. 3.2 Evaluating local memory efficiency Caching as a technique has also been so successful because it is effective at reducing latency penalties and memory system bandwidth.
Reference: [42] <author> Alan Jay Smith. </author> <title> Bibliography and Readings on CPU Cache Memories and Related Topics. Computer Architecture News, </title> <address> 14(1):2242, </address> <month> January </month> <year> 1986. </year>
Reference-contexts: They can thereby eliminate most references to main memory. They have also been extensively studied, and are consequently very well understood. Voluminous papers examining block sizes, associativities, replacement policies, virtual versus physical addressing, and unification versus separation of instruction and data caches have appeared in the literature <ref> [41, 42] </ref>. 3.2 Evaluating local memory efficiency Caching as a technique has also been so successful because it is effective at reducing latency penalties and memory system bandwidth.
Reference: [43] <author> James E. Smith. </author> <title> Decoupled Access/Execute Computer Architectures. </title> <booktitle> In Proceedings of the 9th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 112 119, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: In this section, we discuss two techniques that we feel hold promise, and then propose an architectural philosophy, based on these two techniques, that will be our projects first point of evaluation. 4.1 Decoupling Although explicitly decoupled architectures <ref> [43, 44, 20, 47] </ref> have not achieved success in the mainstream commercial market, the philosophy of decoupling memory operations from computation is becoming increasingly visible in modern commercial processors, such as the PowerPC [48] and the MIPS TFP [24]. Decoupled architectures have several fundamental advantages. <p> These architectures can be viewed as a generalization of vector processing; they lend themselves well to such computations. We believe that they hold the potential for similarly high scalar code performance. One of the well-known penalties to which decoupled architectures are susceptible are loss-of-decoupling events 10 <ref> [43] </ref> (LODs). These events are due to data and/or control dependencies that would also cause traditional processors to incur large penalties, particularly when the memory access latency is very large. They also provide an easy point for optimization: memory latencies are seen only at LODs.
Reference: [44] <author> James E. Smith. </author> <title> Decoupled Access/Execute Computer Architectures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4):289308, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: In this section, we discuss two techniques that we feel hold promise, and then propose an architectural philosophy, based on these two techniques, that will be our projects first point of evaluation. 4.1 Decoupling Although explicitly decoupled architectures <ref> [43, 44, 20, 47] </ref> have not achieved success in the mainstream commercial market, the philosophy of decoupling memory operations from computation is becoming increasingly visible in modern commercial processors, such as the PowerPC [48] and the MIPS TFP [24]. Decoupled architectures have several fundamental advantages.
Reference: [45] <author> James E. Smith. </author> <title> Invited talk. </title> <booktitle> 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: State-of-the-art compilers perform sophisticated program analyses, which is usually thrown away once the code is generated, making it unavailable to the run-time system. In many cases, the hardware is forced to re-create these analyses for both correctness and better performance <ref> [45] </ref>. Substantial work has been done that tries to enable the compiler to circumvent the cache, performing analyses to work against the hardware [1, 53, 52]. Several levels of compiler control are possible for the local memory hierarchy.
Reference: [46] <author> James E. Smith. </author> <title> Private Communication. </title> <month> September </month> <year> 1994. </year>
Reference-contexts: The effective use of local (on-chip) memory will be essential for alleviating the memory bottleneck. We develop the notion of memory efficiency <ref> [34, 46] </ref>, to measure how successfully current microprocessors make use of their local memories (which are primarily cache memory). <p> In this section we characterize how well a given local memory (cache) is used. We define the efficiency of a given memory to be the average fraction of the memory that holds live data <ref> [34, 46] </ref> at any point in the execution of the program. A frame in the cache is defined to be live if its contents will be read again before they are written; i.e, a blocks lifetime extends from its first write until the last time it is read.
Reference: [47] <author> James E. Smith et al. </author> <title> The ZS-1 Central Processor. </title> <booktitle> In Proceedings of the Second Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 199204, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: In this section, we discuss two techniques that we feel hold promise, and then propose an architectural philosophy, based on these two techniques, that will be our projects first point of evaluation. 4.1 Decoupling Although explicitly decoupled architectures <ref> [43, 44, 20, 47] </ref> have not achieved success in the mainstream commercial market, the philosophy of decoupling memory operations from computation is becoming increasingly visible in modern commercial processors, such as the PowerPC [48] and the MIPS TFP [24]. Decoupled architectures have several fundamental advantages.
Reference: [48] <author> James E. Smith and Shlomo Weiss. </author> <title> PowerPC 601 and Alpha 21064: A Tale of Two RISCs. </title> <journal> IEEE Computer, </journal> <volume> 27(6):4658, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: two techniques, that will be our projects first point of evaluation. 4.1 Decoupling Although explicitly decoupled architectures [43, 44, 20, 47] have not achieved success in the mainstream commercial market, the philosophy of decoupling memory operations from computation is becoming increasingly visible in modern commercial processors, such as the PowerPC <ref> [48] </ref> and the MIPS TFP [24]. Decoupled architectures have several fundamental advantages. They implicitly provide aggressive latency tolerance, and permit much higher performance of the execute unit, through the use of queues and renaming.
Reference: [49] <editor> IEEE Computer Society. </editor> <title> IEEE Standard for High-Bandwidth Memory Interface Based on SCI Signaling Technology (RamLink). </title> <journal> Draft 1.00 IEEE P1596.4-199X, </journal> <month> December </month> <year> 1993. </year>
Reference: [50] <institution> Standard Performance Evaluation Corporation. SPEC Newsletter, Fairfax, Virginia, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: G++ is release 2.6.0 of the Gnu C++ compiler. It generated the assembly code of the preprocessed CPU module of a multiprocessor simulator, and was run with full optimization enabled. Compress, eqntott, swm256, and su2cor are all from the Spec92 <ref> [50] </ref> suite. Compress and eqntott were run with the default inputs. Su2cor was run with a short input, and swm256 was run with the default input for 20 iterations. All benchmarks were traced on Sun Sparcs 1.
Reference: [51] <author> Rabin A. Sugumar and Santosh G. Abraham. </author> <title> Efficient Simulation of Caches under Optimal Replacement with Applications to Miss Characterization. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pages 2435, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Since MIN assumes a perfect oracle, however, and is unlikely to be realized in the near future, n-opt is a sufficient bound for our study. To obtain these results, we used Shade [12] to generate an address trace in an appropriate format. The traces were then fed to Cheetah <ref> [51] </ref>, with which we computed miss ratios for both the n-opt model and a normal (L1) cache, using a least-recently-used replacement policy with set associativities of 1 and 4 (lru-1 and lru-4). The lru block sizes were 16 bytes.
Reference: [52] <author> Olivier Temam, Elana D. Granston, and William Jalby. </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Copying Should be Used to Eliminate Cache Conicts. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 410419, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In many cases, the hardware is forced to re-create these analyses for both correctness and better performance [45]. Substantial work has been done that tries to enable the compiler to circumvent the cache, performing analyses to work against the hardware <ref> [1, 53, 52] </ref>. Several levels of compiler control are possible for the local memory hierarchy.
Reference: [53] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the 1991 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3044, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In many cases, the hardware is forced to re-create these analyses for both correctness and better performance [45]. Substantial work has been done that tries to enable the compiler to circumvent the cache, performing analyses to work against the hardware <ref> [1, 53, 52] </ref>. Several levels of compiler control are possible for the local memory hierarchy.
References-found: 53

