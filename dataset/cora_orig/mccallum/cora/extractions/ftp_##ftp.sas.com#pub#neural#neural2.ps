URL: ftp://ftp.sas.com/pub/neural/neural2.ps
Refering-URL: ftp://ftp.sas.com/pub/neural/FAQ2.html
Root-URL: 
Title: Neural Network Implementation in SAS R Software  
Author: Warren S. Sarle, SAS 
Address: Cary, NC, USA  
Affiliation: Institute Inc.,  
Date: Revised April 21, 1994  
Note: Proceedings of the Nineteenth Annual SAS Users Group International Conference  
Abstract: The estimation or training methods in the neural network literature are usually some simple form of gradient descent algorithm suitable for implementation in hardware using massively parallel computations. For ordinary computers that are not massively parallel, optimization algorithms such as those in several SAS procedures are usually far more efficient. This talk shows how to fit neural networks using SAS/OR R fl , SAS/ETS R fl , and SAS/STAT R fl software. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Fahlman, </author> <title> S.E. (1988) An empirical study of learning speed in back-propagation networks, </title> <institution> CMU-CS-88-162, School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Considering how tedious and unreliable standard backprop is, it is difficult to understand why it is used so widely. Much of the NN research literature is devoted to attempts to speed up backprop. Some of these methods such as Quickprop <ref> (Fahlman 1988) </ref> and RPROP (Riedmiller and Braun 1993) are quite effective. There are also numerous references in the NN literature to conjugate gradient algorithms (Masters 1993), although Newton methods are almost always dismissed as requiring too much memory. Nevertheless, standard backprop retains its undeserved popularity.
Reference: <author> Hecht-Nielsen, R. </author> <year> (1990), </year> <title> Neurocomputing, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Kolen, J.F and Pollack, J.B. </author> <year> (1991), </year> <title> Back Propagation is Sensitive to Initial Conditions, </title> <editor> in Lippmann, R.P., Moody, J.E., and Touretzky, D.S., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> 860-867, </pages> <address> San Mateo, CA: </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: The generalized delta rule is suprisingly resistant to local optima in the XOR problem, possibly because the XOR problem was used extensively in developing the generalized delta rule. Nevertheless, local optima are easily demonstrable in the XOR data <ref> (Kolen and Pollack 1991) </ref>, the sine data (see Figure 15), and many other other common NN benchmarks. Inputs need to be normalized to lie between 0 and 1 This myth is trivial but amazingly widespread.
Reference: <author> Kosko, B. </author> <year> (1992), </year> <title> Neural Networks and Fuzzy Systems, </title> <address> Engle-wood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: If c is small enough and if enough iterations are performed, the estimate m will eventually wander around in the vicinity of the mean but will never actually converge to the mean <ref> (Kosko 1992, 148-149) </ref>. To obtain approximate convergence in practice, the parameter c can be gradually reduced over the series of iterations as in stochastic approximation (Robbins and Monro 1951).
Reference: <author> Masters, T. </author> <year> (1993), </year> <title> Practical Neural Network Recipes in C++, </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Much of the NN research literature is devoted to attempts to speed up backprop. Some of these methods such as Quickprop (Fahlman 1988) and RPROP (Riedmiller and Braun 1993) are quite effective. There are also numerous references in the NN literature to conjugate gradient algorithms <ref> (Masters 1993) </ref>, although Newton methods are almost always dismissed as requiring too much memory. Nevertheless, standard backprop retains its undeserved popularity.
Reference: <author> Medsker, L., Turban, E., and Trippi, R.R. </author> <year> (1993), </year> <title> Neural Network Fundamentals for Financial Analysts, </title> <editor> in Trippi, R.R. and Turban, E., eds., </editor> <booktitle> Neural Networks in Finance and Investing, </booktitle> <pages> 3-25, </pages> <address> Chicago: Probus. </address>
Reference: <author> Myers, R.H. </author> <year> (1986), </year> <title> Classical and Modern Regression with Applications, </title> <address> Boston: </address> <publisher> Duxbury Press Nelson, M.C. and Illingworth, </publisher> <address> W.T. </address> <year> (1991), </year> <title> A Practical Guide to Neural Nets, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Plate, T. </author> <year> (1993), </year> <title> Re: Kangaroos (Was Re: BackProp without Calculus?), </title> <note> Usenet article &lt;93Sep8.162519edt.997@neuron.ai.toronto.edu&gt; in comp.ai.neural-nets. </note>
Reference: <author> Riedmiller, M. and Braun, H. </author> <year> (1993), </year> <title> A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm, </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks 1993, </booktitle> <address> San Francisco: </address> <publisher> IEEE. </publisher>
Reference-contexts: Considering how tedious and unreliable standard backprop is, it is difficult to understand why it is used so widely. Much of the NN research literature is devoted to attempts to speed up backprop. Some of these methods such as Quickprop (Fahlman 1988) and RPROP <ref> (Riedmiller and Braun 1993) </ref> are quite effective. There are also numerous references in the NN literature to conjugate gradient algorithms (Masters 1993), although Newton methods are almost always dismissed as requiring too much memory. Nevertheless, standard backprop retains its undeserved popularity.
Reference: <author> Ripley, B.D. </author> <year> (1993), </year> <title> Statistical Aspects of Neural Networks, </title> <editor> in Barndorff-Nielsen, O.E., Jensen, J.L. and Kendall, W.S., eds., </editor> <title> Networks and Chaos: Statistical and Probabilistic Aspects, </title> <publisher> London: Chapman & Hall. </publisher>
Reference-contexts: The momentum is customarily set to 0.9 regardless of whether on-line or off-line training is used. However, it is often better to use larger momentum values, perhaps .99 or .999, for online training and smaller values, perhaps .5, for off-line training <ref> (Ripley 1993) </ref>. The best momentum can be determined only by trial and error. Considering how tedious and unreliable standard backprop is, it is difficult to understand why it is used so widely. Much of the NN research literature is devoted to attempts to speed up backprop.
Reference: <author> Robbins, H. and Monro, S. </author> <year> (1951), </year> <title> A Stochastic Approximation Method, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22, </volume> <pages> 400-407. </pages>
Reference-contexts: To obtain approximate convergence in practice, the parameter c can be gradually reduced over the series of iterations as in stochastic approximation <ref> (Robbins and Monro 1951) </ref>. Most NN training algorithms follow a similar scheme with: * random initial estimates * simple case-by-case updating formulas * slow convergence or nonconvergence even when direct analytical solutions are available.
Reference: <author> Sarle, W.S. </author> <year> (1994), </year> <title> Neural Networks and Statistical Models, </title> <booktitle> Proceedings of the Nineteenth Annual SAS Users Group International Conference, </booktitle> <institution> Cary, NC: SAS Institute. </institution>
Reference: <author> Schmidt, G., Mattern, R., and Sch u ler, F. </author> <year> (1981), </year> <title> Biomechanical Investigation to Determine Physical and Traumatologi-cal Differentiation Criteria for the Maximum Load Capacity of Head and Vertebral Column With and Without Protective Helmet Under Effects of Impact, EEC Research Program on Biome-chanics of Impacts, Final Report Phase III, </title> <type> Project 65, </type> <institution> Institut f ur Rechtsmedizin, Universit a t Heidelberg, </institution> <address> Heidelberg, Germany. </address>
Reference: <author> Weisberg, S. </author> <year> (1985), </year> <title> Applied Linear Regression, </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher> <pages> 14 </pages>
References-found: 14

