URL: http://ebbets.poly.edu/hstein/pubs/relev.ps
Refering-URL: http://ebbets.poly.edu/hstein/journal.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Attribute-efficient learning in query and mistake-bound models  
Author: Nader Bshouty Lisa Hellerstein 
Note: Partially supported by NSERC of Canada. Partially supported by NSF Grants CCR-9501660 and CCR-9210957.  
Address: Calgary, Alberta, CANADA  Brooklyn, NY, USA  
Affiliation: Department of Computer Science University of Calgary  Department of Computer and Information Science Polytechnic University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: Analogously, the algorithm makes P (n; r; s) mistakes if for any f 2 C r n and any sequence of assignments, it makes at most P (n; r; s) mistakes. In the membership query model (cf. <ref> [1] </ref>), the learning algorithm is given as input n, where V n is the set of variables on which the target f is defined. The algorithm has access to a membership oracle for f , which takes as input an assignment a, and returns f (a). <p> The mistake-bound model is closely related to the equivalence query model defined in <ref> [1] </ref>. In this model, the algorithm asks queries of an equivalence oracle, which takes as input (the representation of) a hypothesis h from the class C.
Reference: [2] <author> N. Alon, J. Bruck, J. Naor, M. Naor, and R. Roth, </author> <title> Construction of asymptotically good low-rate error-correcting codes through pseudorandom graphs, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 509-516, </pages> <year> 1992. </year>
Reference-contexts: Like the randomized algorithm, it is logn attribute-efficient. However, it does have a higher query complexity in r and s than the randomized algorithm. Our deterministic construction is based on a class of error correcting codes developed by Alon et al. <ref> [2] </ref> that have asymptotically good low-rate. The construction is similar to a previous construction of Naor et al. [16] for (n; k; r) splitters of small size. Deterministic Construction Given an alphabet , a code of length m over the alphabet is a set C m . <p> Lemma 4 For any constant 0 &lt; &lt; 1 there is an explicit construction of an (n; r; r 2 ; O (r 6 log r log n); )- coloring set. Proof: Let k = r= p 1 . We use the code in <ref> [2] </ref>. This code is n code-words v 1 : : : ; v n of length L = O (k 6 log k log n) over the alphabet [k 2 ], with a minimal relative distance of at least 1 2=k 2 .
Reference: [3] <author> D. Angluin, L. Hellerstein, and M. Karpinski. </author> <title> Learning read-once formulas with queries. </title> <journal> Journal of the ACM, </journal> <volume> 40(1) </volume> <pages> 185-210, </pages> <year> 1993. </year>
Reference-contexts: In the query models, a learning algorithm 10 is polynomial time if its total running time is polynomial in n and s. We assume that each query is answered in constant time. (A more precise definition of the computational model can be found in <ref> [3] </ref>). Algorithms in any of the above models need time at least linear in n just to read or specify an assignment. Thus we do not hope to have algorithms whose time complexity has a sublinear dependence on n.
Reference: [4] <author> A. Blum, L. Hellerstein, and N. Littlestone. </author> <title> Learning in the presence of finitely or infinitely many irrelevant attributes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1) </volume> <pages> 32-40, </pages> <year> 1995. </year>
Reference-contexts: Thus attribute-efficient algorithms make a number of mistakes or queries which has only a sublinear dependence on the number of irrelevant attributes in the target function. Littlestone developed a polynomial time, log n attribute-efficient algorithm for learning threshold functions in the mistake-bound model [15]. Subsequently, Blum, Hellerstein, and Littlestone <ref> [4] </ref> considered the following general question: If a class of functions can be learned in polynomial time in a query or mistake-bound model, can it be learned by a polynomial-time I (n) attribute-efficient algorithm in that model? (In particular, they considered the cases I (n) = log n and I (n) <p> Let C be a projection-closed class of Boolean functions. If computational time is not a concern, C can be learned by a log n attribute-efficient algorithm that is a simple variation of the halving algorithm <ref> [4] </ref>. (We assume here and throughout the paper that the number of functions in C of size s on n variables is at most 2 poly (n;s) |an assumption that holds for virtually every class of functions considered in the computational learning theory literature.) However, the hypotheses generated by the halving
Reference: [5] <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 373-386, </pages> <year> 1992. </year>
Reference-contexts: Thus we do not hope to have algorithms whose time complexity has a sublinear dependence on n. We note that in the infinite attribute model of Blum, it is possible to have algorithms whose time complexity is sublinear in n <ref> [5] </ref>. However, we do not consider that model here.
Reference: [6] <author> R. Boppana. </author> <title> Amplification of probabilistic boolean formulas. </title> <booktitle> Advances in Computing Research, </booktitle> <volume> 5(4) </volume> <pages> 27-45, </pages> <year> 1989. </year>
Reference-contexts: Proof: The formula h is constructed out of two different amplifiers. The first is the ( 1 n ; 1 n ) ! (2 2n ; 1 2 2n ) amplifier A (y 1 ; : : : ; y t ) described in [7] (see also [13] and <ref> [6] </ref>), where t = 4n 2 log n log log n . This amplifier is simply a CNF formula with 2n= log n disjoint clauses, each of which contains 2n= log log n variables.
Reference: [7] <author> N. H. Bshouty, R. Cleve, S. Kannan, and C. Tamon. </author> <title> Oracles and queries that are sufficient for exact learning. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 130-139. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year> <month> 30 </month>
Reference-contexts: We also show that polynomial size Boolean formulas of depth 2 are not sufficient for attribute-efficient learning of this class. Littlestone's WINNOW algorithm learns this class log n attribute-efficiently with hypotheses that are threshold functions [15]. Our techniques extend the results of Bshouty, Cleve, Kannan, and Tamon <ref> [7] </ref> on equivalence query learning with simple hypotheses. 2 Definitions We use log to denote the logarithm base 2, and ln to denote the natural logarithm. Let V n = fx 1 ; x 2 ; : : : ; x n g. <p> This version of the halving algorithm is log n attribute-efficient, but may use complex hypotheses. We ask whether there exist log (n) attribute-efficient equivalence query algorithms that use simple hypotheses. Kannan [13] and Bshouty et al. <ref> [7] </ref> previously considered the problem of learning within a polynomial number of equivalence queries (without consideration of attribute-efficiency) using simple hypotheses. The results of Bshouty et al. rely on a lemma relating amplifiers and ffi-good hypotheses. Definition: Let 0 p 0 &lt; p &lt; q &lt; q 0 1. <p> Lemma 5 <ref> [7] </ref> Let h (z 1 ; : : : ; z m ) be a (ffi; 1 ffi) ! (2 2n ; 1 2 2n ) amplifier. <p> Proof: The formula h is constructed out of two different amplifiers. The first is the ( 1 n ; 1 n ) ! (2 2n ; 1 2 2n ) amplifier A (y 1 ; : : : ; y t ) described in <ref> [7] </ref> (see also [13] and [6]), where t = 4n 2 log n log log n . This amplifier is simply a CNF formula with 2n= log n disjoint clauses, each of which contains 2n= log log n variables. <p> We note that by results of Bshouty et al. <ref> [7] </ref>, a randomized version of the algorithm in the above theorem can be implemented to run in polynomial time using an NP oracle. A deterministic version runs in exponential time and polynomial space.
Reference: [8] <author> N. H. Bshouty and L. Hellerstein. </author> <title> Attribute efficient learning with queries (Extended Ab--stract). </title> <booktitle> In Proc. 9th Annu. ACM Conference on Comput. Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: Our result in Section 3 consists of a method for transforming a polynomial-time membership query algorithm into a polynomial-time log (n) attribute-efficient membership query algorithm. The method is valid for all p.e.c. classes. Subsequent to the publication of a preliminary version of this paper <ref> [8] </ref>, Uehara, Tsuchida, and Wegener obtained bounds on attribute-efficient learning of particular classes of p.e.c. functions [18]. The proof of our result in Section 3 is based on the construction of coloring sets (which are closely related to splitters [16]). We give both randomized and deterministic constructions of such sets. <p> A deterministic construction in an earlier version of this paper relies on a number-theoretic coloring technique <ref> [8] </ref>. This technique is a variation of one used in constructing small, constant depth, circuits for symmetric functions [12].
Reference: [9] <author> J. Friedman. </author> <title> Constructing O(n log n) size monotone formulae for the k-th elementary symmetric polynomial of n boolean variables. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 506-515, </pages> <year> 1984. </year>
Reference-contexts: This technique is a variation of one used in constructing small, constant depth, circuits for symmetric functions [12]. A similar technique was used earlier in constructing short monotone formulas for threshold-k functions [14]. (Constructions producing shorter formulas appeared in e.g. <ref> [9] </ref>, but seem to have no relation to our learning problems.) The number of queries in the algorithm obtained from this construction is upper bounded by ~ O (r 6 Q (r 2 ; s) log 1+* n) for any * &gt; 0 (here we use ~ O to denote the
Reference: [10] <author> M. L. Fredman, J. Komlos and E. Szemeredi, </author> <title> Storing a sparse table with O(1) worst case access time, </title> <journal> J. of the ACM, </journal> <volume> 31 </volume> <pages> 538-544, </pages> <year> 1984. </year>
Reference: [11] <author> M. Furst, J. Saxe, and M. Sipser. </author> <title> Parity, circuits, and the polynomial-time hierarchy. </title> <journal> Math Systems Theory, </journal> <volume> 17 </volume> <pages> 13-27, </pages> <year> 1984. </year>
Reference-contexts: This follows easily from results of Bshouty et al. (which relied on the existence of a ( 1 4 ; 3 4 ) ! (2 2n ; 1 2 2n ) amplifier computing the majority function). However, the majority function cannot be computed by polynomial size constant depth circuits <ref> [11] </ref>. The 1=4-good hypotheses implied by Lemmas 5 and 6 can also be used in versions of the halving algorithm in which r and s are not doubled. For example, we have the following corollary. Corollary 1 Let C be a class of polynomial-sized DNF formulas.
Reference: [12] <author> J. Hastad, I. Wegener, N. Wurm, and S.-Z. Yi, </author> <title> Optimal depth, very small size circuits for symmetric functions in AC 0 . Information and Computation, </title> <booktitle> 108 </booktitle> <pages> 200-211, </pages> <year> 1994. </year>
Reference-contexts: A deterministic construction in an earlier version of this paper relies on a number-theoretic coloring technique [8]. This technique is a variation of one used in constructing small, constant depth, circuits for symmetric functions <ref> [12] </ref>.
Reference: [13] <author> S. Kannan. </author> <title> On the query complexity of learning. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 58-66. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: This version of the halving algorithm is log n attribute-efficient, but may use complex hypotheses. We ask whether there exist log (n) attribute-efficient equivalence query algorithms that use simple hypotheses. Kannan <ref> [13] </ref> and Bshouty et al. [7] previously considered the problem of learning within a polynomial number of equivalence queries (without consideration of attribute-efficiency) using simple hypotheses. The results of Bshouty et al. rely on a lemma relating amplifiers and ffi-good hypotheses. <p> Proof: The formula h is constructed out of two different amplifiers. The first is the ( 1 n ; 1 n ) ! (2 2n ; 1 2 2n ) amplifier A (y 1 ; : : : ; y t ) described in [7] (see also <ref> [13] </ref> and [6]), where t = 4n 2 log n log log n . This amplifier is simply a CNF formula with 2n= log n disjoint clauses, each of which contains 2n= log log n variables.
Reference: [14] <author> M. Kleiman and N. Pippenger. </author> <title> An explicit construction of short monotone formulae for the monotone symmetric functions. </title> <journal> Theoretical Computer Science, </journal> <volume> 7 </volume> <pages> 325-332, </pages> <year> 1978. </year>
Reference-contexts: A deterministic construction in an earlier version of this paper relies on a number-theoretic coloring technique [8]. This technique is a variation of one used in constructing small, constant depth, circuits for symmetric functions [12]. A similar technique was used earlier in constructing short monotone formulas for threshold-k functions <ref> [14] </ref>. (Constructions producing shorter formulas appeared in e.g. [9], but seem to have no relation to our learning problems.) The number of queries in the algorithm obtained from this construction is upper bounded by ~ O (r 6 Q (r 2 ; s) log 1+* n) for any * &gt; 0
Reference: [15] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Thus attribute-efficient algorithms make a number of mistakes or queries which has only a sublinear dependence on the number of irrelevant attributes in the target function. Littlestone developed a polynomial time, log n attribute-efficient algorithm for learning threshold functions in the mistake-bound model <ref> [15] </ref>. <p> We also show that polynomial size Boolean formulas of depth 2 are not sufficient for attribute-efficient learning of this class. Littlestone's WINNOW algorithm learns this class log n attribute-efficiently with hypotheses that are threshold functions <ref> [15] </ref>. Our techniques extend the results of Bshouty, Cleve, Kannan, and Tamon [7] on equivalence query learning with simple hypotheses. 2 Definitions We use log to denote the logarithm base 2, and ln to denote the natural logarithm. <p> In both the mistake-bound and membership query models of learning, the goal is to "learn" an unknown target function f taken from some fixed class of functions C. In the mistake-bound model <ref> [15] </ref>, learning proceeds in a sequence of trials. In each trial the learning algorithm is given an assignment a 2 f0; 1g V n , where V n are the variables over which the target f is defined. <p> Conversely, if there exists a (proper or improper) polynomial-time equivalence query algorithm for learning C that makes at most Q (n; r; s) queries, then there exists a mistake-bound algorithm for learning C that makes at most Q (n; r; s) mistakes <ref> [15] </ref>. Mistake-bound algorithms implicitly use a hypothesis in predicting the value f (a). A randomized membership query algorithm for learning C is one that for any f 2 C, with probability at least 2=3 outputs (a representation of) f . <p> The standard halving algorithm learns C with equivalence queries <ref> [15] </ref>. It is not computationally efficient, and we do not focus on issues of computational efficiency in this section. Throughout this section, the term "equivalence query" is meant to include improper equivalence queries. <p> It follows that the depth of g in Theorem 4 cannot be reduced. Littlestone's WINNOW algorithm learns S in polynomial-time, log n attribute-efficiently, using hypotheses that are weighted threshold formulas <ref> [15] </ref>. Theorem 5 implies that the log n attribute-efficient performance of WINNOW could not be achieved using hypotheses that are polynomial-size DNF formulas.
Reference: [16] <author> M. Naor, L. Schulman, and A. Srinivasan. </author> <title> Splitters and near-optimal decomposition. </title> <booktitle> In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 182-193, </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1995. </year> <month> 31 </month>
Reference-contexts: The proof of our result in Section 3 is based on the construction of coloring sets (which are closely related to splitters <ref> [16] </ref>). We give both randomized and deterministic constructions of such sets. Our deterministic construction is similar to a construction of Naor et al. [16] of (n; k; r) splitters of small size. In Section 4 we consider randomized attribute-efficient algorithms in the membership query model. <p> The proof of our result in Section 3 is based on the construction of coloring sets (which are closely related to splitters <ref> [16] </ref>). We give both randomized and deterministic constructions of such sets. Our deterministic construction is similar to a construction of Naor et al. [16] of (n; k; r) splitters of small size. In Section 4 we consider randomized attribute-efficient algorithms in the membership query model. <p> Our deterministic construction is based on a class of error correcting codes developed by Alon et al. [2] that have asymptotically good low-rate. The construction is similar to a previous construction of Naor et al. <ref> [16] </ref> for (n; k; r) splitters of small size. Deterministic Construction Given an alphabet , a code of length m over the alphabet is a set C m . The elements of C are called code-words.
Reference: [17] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: Since c = 1 * r 2 , this probability is greater than 1 *=2. By a standard Chernoff bound (see e.g. <ref> [17] </ref>, Theorem 4.2, p. 70), the probability that, for a given set R of r entries, the elements of R are colored differently in fewer than (1 *)t of the t colorings is at most e * 2 t 8 (1*=2) .

References-found: 17

