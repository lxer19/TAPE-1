URL: http://www.cs.cmu.edu/~tcm/tcm_papers/dsm_latency_hpca4.ps.gz
Refering-URL: http://sam.cs.umd.edu/tdsm.html
Root-URL: 
Email: tcm@cs.cmu.edu  fcharlesc,loadg@eecg.toronto.edu  
Title: Comparative Evaluation of Latency Tolerance Techniques for Software Distributed Shared Memory  
Author: Todd C. Mowry Charles Q. C. Chan and Adley K. W. Lo 
Address: Pittsburgh, PA, USA 15213  Toronto, Ontario, Canada M5S 3G4  
Affiliation: Computer Science Department Carnegie Mellon University  Department of Computer Science University of Toronto  
Abstract: A key challenge in achieving high performance on software DSMs is overcoming their relatively large communication latencies. In this paper, we consider two techniques which address this problem: prefetching and multithreading. While previous studies have examined each of these techniques in isolation, this paper is the first to evaluate both techniques using a consistent hardware platform and set of applications, thereby allowing direct comparisons. In addition, this is the first study to consider combining prefetching and multithreading in a software DSM. We performed our experiments on real hardware using a full implementation of both techniques. Our experimental results demonstrate that both prefetching and multithreading result in significant performance improvements when applied individually. In addition, we observe that prefetching and multithread-ing can potentially complement each other by using prefetching to hide memory latency and multithreading to hide synchronization latency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A Unified Formalization of Four Shared-Memory Models. </title> <type> Technical Report CS1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> Septem--ber </month> <year> 1991. </year>
Reference-contexts: This is accomplished by later merging together runlength encoded records ("diffs") which are created by comparing the modified versions of the pages with clean, unmodified copies ("twins"). These diffs are applied to the shared pages according to the happen-before-1 <ref> [1] </ref> partial order among the intervals (i.e. in increasing timestamp order) to ensure program correctness. 2.2. Hardware Platform We performed our experiments on a collection of eight IBM RS/6000 workstations running AIX 4.1.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To tolerate the latency of reading remote memory, we must separate the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for accomplishing this are prefetching [4, 23] and multithreading <ref> [2, 9, 14, 15] </ref>; the distinction between the two is that prefetching finds the parallelism within a single thread of execution, while multithread-ing exploits parallelism across multiple threads. <p> On the other hand, if synchronization stall times are small or are dominated by barrier stalls, it is less clear that the additional overhead of supporting multithreading will be worthwhile. 6. Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors <ref> [2, 8, 9, 14, 15, 20] </ref>. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications.
Reference: [3] <author> R. Bianchini, L. I. Kontothanassis, R. Pinto, M. D. Maria, M. Abud, and C. L. Amorim. </author> <title> Hiding Communication Latency and Coherence Overhead in Software DSMs. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Objectives and Overview The goal of this paper is to characterize the benefits and costs of prefetching and multithreading in a software DSM environment, using a consistent hardware platform and set of applications. While studies do exist which consider each technique in isolation <ref> [3, 7, 24] </ref>, the results cannot be directly compared since the architectural assumptions, DSM software, etc. are different. In addition to considering each technique in isolation, we also present the first results which combine prefetching and multithreading for the sake of hiding software DSM latency. <p> Since purely hardware-controlled prefetching probably does not make sense in a software DSM, the more realistic alternative to our approach is to have the DSM run-time layer issue prefetches automatically, perhaps based on access pattern histories <ref> [3] </ref>. While the advantage of this latter approach is that it does not require source code modifications, our experience has shown that by inserting prefetches explicitly, we can prefetch more intelligently and more aggressively [5]. In a multiprocessor environment, prefetches can be classified as being either binding or non-binding. <p> Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors [2, 8, 9, 14, 15, 20]. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. <ref> [3] </ref>. The former study focused on compilation techniques to automatically insert prefetches into numeric applications. The latter study examined binding prefetches which were launched at synchronization points based on access pattern histories.
Reference: [4] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: To tolerate the latency of reading remote memory, we must separate the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for accomplishing this are prefetching <ref> [4, 23] </ref> and multithreading [2, 9, 14, 15]; the distinction between the two is that prefetching finds the parallelism within a single thread of execution, while multithread-ing exploits parallelism across multiple threads.
Reference: [5] <author> C. Q. C. Chan. </author> <title> Tolerating Latency in Software Distributed Shared Memory Systems Through NonBinding Prefetching. </title> <type> Masters Thesis, </type> <institution> Department of Computer Science, University of Toronto, forthcoming. </institution>
Reference-contexts: While the advantage of this latter approach is that it does not require source code modifications, our experience has shown that by inserting prefetches explicitly, we can prefetch more intelligently and more aggressively <ref> [5] </ref>. In a multiprocessor environment, prefetches can be classified as being either binding or non-binding. With binding prefetching, the value seen by a subsequent read access is bound at the time when the prefetch operation completes. <p> Therefore the prefetches are truly non-binding, and never violate program correctness. (Further details on our prefetching implementa tion can be found in another publication <ref> [5] </ref>.) Since our prefetch operation only makes use of the existing write notices to fetch modifications, its effectiveness is limited by the lazy propagation of write notices in LRC. <p> We experimented with this approach, but found that despite all of our efforts to improve its performance (including the optimizations described later in Section 5.1), it never achieved better performance than either prefetching or multithreading alone <ref> [5, 18] </ref>. The problem with this approach is that switching between threads tends to result in bursty miss patterns, which in turn slow down requests in the network, including prefetches.
Reference: [6] <author> A. L. Cox, S. Dwarkadas, and P. Keleher. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For certain classes of applications, these software distributed shared memory (DSM) systems can deliver performance which is comparable to hardware cache-coherent machines of a similar scale <ref> [6, 11] </ref>. However, for applications with larger communication demands, the performance can be disappointing. 0 To appear in HPCA-4, February 1-4, 1998. A key stumbling block to achieving higher performance on software DSMs is the relatively large communication latency.
Reference: [7] <author> S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. </author> <title> An Integrated Compile-Time/Run-Time Software Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Objectives and Overview The goal of this paper is to characterize the benefits and costs of prefetching and multithreading in a software DSM environment, using a consistent hardware platform and set of applications. While studies do exist which consider each technique in isolation <ref> [3, 7, 24] </ref>, the results cannot be directly compared since the architectural assumptions, DSM software, etc. are different. In addition to considering each technique in isolation, we also present the first results which combine prefetching and multithreading for the sake of hiding software DSM latency. <p> Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors [2, 8, 9, 14, 15, 20]. Prefetching for software DSMs has been studied by Dwarkadas et al. <ref> [7] </ref> and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications. The latter study examined binding prefetches which were launched at synchronization points based on access pattern histories.
Reference: [8] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W. D. Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: On the other hand, if synchronization stall times are small or are dominated by barrier stalls, it is less clear that the additional overhead of supporting multithreading will be worthwhile. 6. Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors <ref> [2, 8, 9, 14, 15, 20] </ref>. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications. <p> In addition, this is the first study to consider combining prefetching and mul-tithreading in a software DSM. An interesting comparison is between our results and the results of the earlier study by Gupta et al. <ref> [8] </ref> on combining prefetching and multithreading in a tightly-coupled multiprocessor. Similar to their study, we also conclude that combining prefetching and multi-threading produces mixed results.
Reference: [9] <author> R. H. Halstead, Jr. and T. Fujita. MASA: </author> <title> A Multi-threaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To tolerate the latency of reading remote memory, we must separate the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for accomplishing this are prefetching [4, 23] and multithreading <ref> [2, 9, 14, 15] </ref>; the distinction between the two is that prefetching finds the parallelism within a single thread of execution, while multithread-ing exploits parallelism across multiple threads. <p> On the other hand, if synchronization stall times are small or are dominated by barrier stalls, it is less clear that the additional overhead of supporting multithreading will be worthwhile. 6. Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors <ref> [2, 8, 9, 14, 15, 20] </ref>. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications.
Reference: [10] <author> IEEE. </author> <title> Threads Extension for Portable Operating Systems (Draft 7), </title> <month> February </month> <year> 1992. </year>
Reference-contexts: Multithreading Implementation We implemented multithreading on top of Tread-Marks using the Pthreads <ref> [10] </ref> user-level thread library. The benefit of user-level threads is that since the threads share the same memory image within a processor, there is less state to save on a context switch, and there is less overhead managing the local portion of shared memory.
Reference: [11] <author> L. Iftode, J. P. Singh, and K. Li. </author> <title> Understanding Application Performance on Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: For certain classes of applications, these software distributed shared memory (DSM) systems can deliver performance which is comparable to hardware cache-coherent machines of a similar scale <ref> [6, 11] </ref>. However, for applications with larger communication demands, the performance can be disappointing. 0 To appear in HPCA-4, February 1-4, 1998. A key stumbling block to achieving higher performance on software DSMs is the relatively large communication latency. <p> WATER-SP performs the same simulation as WATER-NSQ except with 4096 water molecules and an O (n) algorithm. Further details on these applications can be found in studies by Woo et al. [26] and Liviu et al. <ref> [11] </ref>. 3. Prefetching We begin our study by focusing on prefetching alone. The idea behind prefetching is to use knowledge of future access patterns to bring remote data into the local memory before it is actually needed.
Reference: [12] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter Usenix Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Software DSM Performance To illustrate the impact of communication latency on software DSM performance, we ran a collection of applications taken primarily from the SPLASH-2 suite [26] 1 using TreadMarks <ref> [12] </ref> (a state-of-the-art software DSM implementation) on eight 133 MHz IBM RS/6000 workstations connected by a 155 Mbps FORE Systems ATM LAN. (Further details of the hardware platform are given later in Section 2.2). <p> Finally, we discuss related work and present conclusions in Sections 6 and 7. 2. Experimental Framework This section briefly describes the hardware and software used throughout our experiments. 2.1. DSM Software Layer: TreadMarks All of our experiments are built on top of Tread-Marks <ref> [12] </ref>, which is a state-of-the-art software DSM implementation. As mentioned earlier, TreadMarks uses lazy release consistency (LRC) [13] and a multiple-writer protocol to minimize communication traffic. <p> In this subsection, we briefly discuss some implementation details on TreadMarks which are relevant to later sections when we discuss how prefetching and multi-threading are added (further details on TreadMarks can be found in Keleher et al. <ref> [12] </ref>). TreadMarks uses a distributed timestamp and interval-based algorithm for maintaining LRC. Synchronization operations are explicitly labeled as either acquires or releases, and they define the boundaries of intervals, which processors designate by incrementing local timestamps.
Reference: [13] <author> P. Keleher, A. L. Cox, and W. Zwaepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Software DSMs accomplish this by using local memory as a cache for remote locations. The second step is to buffer and pipeline remote accesses through a relaxed memory consistency model, which TreadMarks does using lazy release consistency <ref> [13] </ref>. TreadMarks also uses a multiple-writer protocol to avoid the effects of false sharing. The net effect of all of these optimizations is to reduce the amount of communication so that it more closely approximates the inherent communication due to true data sharing and synchronization. <p> Experimental Framework This section briefly describes the hardware and software used throughout our experiments. 2.1. DSM Software Layer: TreadMarks All of our experiments are built on top of Tread-Marks [12], which is a state-of-the-art software DSM implementation. As mentioned earlier, TreadMarks uses lazy release consistency (LRC) <ref> [13] </ref> and a multiple-writer protocol to minimize communication traffic. In this subsection, we briefly discuss some implementation details on TreadMarks which are relevant to later sections when we discuss how prefetching and multi-threading are added (further details on TreadMarks can be found in Keleher et al. [12]).
Reference: [14] <editor> J. S. Kowalik, editor. </editor> <title> Parallel MIMD Computation : The HEP Supercomputer and Its Applications. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: To tolerate the latency of reading remote memory, we must separate the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for accomplishing this are prefetching [4, 23] and multithreading <ref> [2, 9, 14, 15] </ref>; the distinction between the two is that prefetching finds the parallelism within a single thread of execution, while multithread-ing exploits parallelism across multiple threads. <p> On the other hand, if synchronization stall times are small or are dominated by barrier stalls, it is less clear that the additional overhead of supporting multithreading will be worthwhile. 6. Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors <ref> [2, 8, 9, 14, 15, 20] </ref>. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications.
Reference: [15] <author> J. Laudon, A. Gupta, and M. Horowitz. </author> <title> Interleaving: A Multithreading Technique Targeting Multiprocessors and Workstations. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 308-318, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: To tolerate the latency of reading remote memory, we must separate the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for accomplishing this are prefetching [4, 23] and multithreading <ref> [2, 9, 14, 15] </ref>; the distinction between the two is that prefetching finds the parallelism within a single thread of execution, while multithread-ing exploits parallelism across multiple threads. <p> On the other hand, if synchronization stall times are small or are dominated by barrier stalls, it is less clear that the additional overhead of supporting multithreading will be worthwhile. 6. Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors <ref> [2, 8, 9, 14, 15, 20] </ref>. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications.
Reference: [16] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A cc-NUMA Highly Scalable Server. </title> <booktitle> In Proceedings of the 24th International Symposium on Computer Architecture, </booktitle> <pages> pages 241-251, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: A key stumbling block to achieving higher performance on software DSMs is the relatively large communication latency. In contrast with tightly-coupled multiprocessors, where remote miss latencies are on the order of half a microsecond <ref> [16] </ref>, the remote miss latencies for software DSM on moderately aggressive hardware are closer to half a millisecond [7]|i.e. roughly three orders of magnitude slower. This large communication latency affects not only remote memory accesses, but also synchronization operations. <p> Prefetching Implementation Adding non-binding prefetching support to Tread-Marks turns out to be non-trivial, due to the lazy and distributed nature of the consistency protocol. Unlike traditional directory-based protocols, where one can always get an up-to-date copy of memory by sending a message to the home node <ref> [16] </ref>, determining the current set of modifications to a page at a given instant is difficult in TreadMarks since write notices are distributed across processors and are only known precisely at acquire time.
Reference: [17] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transaction on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: To help simplify the task of writing parallel applications, software can provide a shared memory abstraction across the machines with the help of the virtual memory system <ref> [17] </ref>. For certain classes of applications, these software distributed shared memory (DSM) systems can deliver performance which is comparable to hardware cache-coherent machines of a similar scale [6, 11]. However, for applications with larger communication demands, the performance can be disappointing. 0 To appear in HPCA-4, February 1-4, 1998.
Reference: [18] <author> A. K. W. Lo. </author> <title> Tolerating Latency in Software Distributed Shared Memory Systems Through Multi-threading. </title> <type> Masters Thesis, </type> <institution> Department of Computer Science, University of Toronto, forthcoming. </institution>
Reference-contexts: Combining requests for barriers is somewhat different|we gather the local arrivals first such that only the last local thread to arrive generates a remote arrival message. (Further details on our multithreading implementation can be found in another publication <ref> [18] </ref>.) 4.2. Modifications to the Applications Most applications do not require any modifications to run correctly in a multithreading fashion, other than replicating "private" data on the heap whenever appropriate such that each thread has its own copy. <p> We experimented with this approach, but found that despite all of our efforts to improve its performance (including the optimizations described later in Section 5.1), it never achieved better performance than either prefetching or multithreading alone <ref> [5, 18] </ref>. The problem with this approach is that switching between threads tends to result in bursty miss patterns, which in turn slow down requests in the network, including prefetches.
Reference: [19] <author> C. K. Luk and T. C. Mowry. </author> <title> Compiler-Based Prefetch-ing for Recursive Data Structures. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 222-233, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: With our scheme, we will retry only once (upon the actual access) if the prefetch fails. giving us more time to hide the latency. We inserted prefetches into WATER-SP (a pointer-based program) by hand using a variation of the history prefetching scheme proposed by Luk and Mowry <ref> [19] </ref>. Since the recursive data structures do not change once they are created, we create a new local array and use it to record pointers to the elements in the traversal order.
Reference: [20] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Hence the non-binding property gives the programmer or the compiler the flexibility to insert prefetches more aggressively without worrying about violating program correctness <ref> [20, 22] </ref>. Therefore we focus on software-controlled non-binding prefetching throughout this study. 3.1. Prefetching Implementation Adding non-binding prefetching support to Tread-Marks turns out to be non-trivial, due to the lazy and distributed nature of the consistency protocol. <p> Inserting Prefetches We inserted explicit prefetch procedure calls into the source code of the applications as follows. With the exception of WATER-SP, all of the other applications use arrays as their primary data structures (WATER-SP uses linked lists). Therefore we apply Mowry's prefetching algorithm <ref> [20] </ref> to these applications to isolate dynamic miss instances through loop-splitting techniques (e.g., strip mining) and to schedule prefetches far enough ahead using software pipelin-ing. Prefetching for software DSM is quite similar to prefetching page faults to hide the latency of out-of-core I/O [21]. <p> On the other hand, if synchronization stall times are small or are dominated by barrier stalls, it is less clear that the additional overhead of supporting multithreading will be worthwhile. 6. Related Work Both prefetching and multithreading have been studied previously in the context of tightly-coupled multiprocessors <ref> [2, 8, 9, 14, 15, 20] </ref>. Prefetching for software DSMs has been studied by Dwarkadas et al. [7] and by Bianchini et al. [3]. The former study focused on compilation techniques to automatically insert prefetches into numeric applications.
Reference: [21] <author> T. C. Mowry, A. K. Demke, and O. Krieger. </author> <title> Automatic Compiler-Inserted I/O Prefetching for Out-of-Core Applications. </title> <booktitle> In Proceedings of the 2nd Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1996. </year>
Reference-contexts: Therefore we apply Mowry's prefetching algorithm [20] to these applications to isolate dynamic miss instances through loop-splitting techniques (e.g., strip mining) and to schedule prefetches far enough ahead using software pipelin-ing. Prefetching for software DSM is quite similar to prefetching page faults to hide the latency of out-of-core I/O <ref> [21] </ref>. For two of the seven array-based applications (FFT and LU-NCONT), our implementation of prefetching in the SUIF compiler [25] achieved performance comparable to the best that we could do by hand; in the other five cases (LU-CONT, OCEAN, RADIX, SOR and WATER-NSQ), we achieved better performance through hand-tuning.
Reference: [22] <author> T. C. Mowry and A. Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1991. </year>
Reference-contexts: Hence the non-binding property gives the programmer or the compiler the flexibility to insert prefetches more aggressively without worrying about violating program correctness <ref> [20, 22] </ref>. Therefore we focus on software-controlled non-binding prefetching throughout this study. 3.1. Prefetching Implementation Adding non-binding prefetching support to Tread-Marks turns out to be non-trivial, due to the lazy and distributed nature of the consistency protocol.
Reference: [23] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: To tolerate the latency of reading remote memory, we must separate the request for data from the use of that data, while finding enough useful parallelism to keep the processor busy in between. The two main techniques for accomplishing this are prefetching <ref> [4, 23] </ref> and multithreading [2, 9, 14, 15]; the distinction between the two is that prefetching finds the parallelism within a single thread of execution, while multithread-ing exploits parallelism across multiple threads.
Reference: [24] <author> K. Thitikamol and P. Keleher. </author> <title> Multi-threading and Remote Latency in Software DSMs. </title> <booktitle> In Proceedings of the 17th International Conference on Distributed Computing Systems, </booktitle> <year> 1997. </year>
Reference-contexts: Objectives and Overview The goal of this paper is to characterize the benefits and costs of prefetching and multithreading in a software DSM environment, using a consistent hardware platform and set of applications. While studies do exist which consider each technique in isolation <ref> [3, 7, 24] </ref>, the results cannot be directly compared since the architectural assumptions, DSM software, etc. are different. In addition to considering each technique in isolation, we also present the first results which combine prefetching and multithreading for the sake of hiding software DSM latency. <p> In contrast, our non-binding prefetches can be moved back ahead of locks, and therefore do not suffer from these same problems. Thitikamol and Keleher <ref> [24] </ref> studied the impact of multithreading on software DSMs. They found that multithreading could improve application performance with a small number of threads.
Reference: [25] <author> S. Tjiang, M. Wolf, M. Lam, K. Pieper, and J. Hen-nessy. </author> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 137-151. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1992. </year>
Reference-contexts: Prefetching for software DSM is quite similar to prefetching page faults to hide the latency of out-of-core I/O [21]. For two of the seven array-based applications (FFT and LU-NCONT), our implementation of prefetching in the SUIF compiler <ref> [25] </ref> achieved performance comparable to the best that we could do by hand; in the other five cases (LU-CONT, OCEAN, RADIX, SOR and WATER-NSQ), we achieved better performance through hand-tuning.
Reference: [26] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Software DSM Performance To illustrate the impact of communication latency on software DSM performance, we ran a collection of applications taken primarily from the SPLASH-2 suite <ref> [26] </ref> 1 using TreadMarks [12] (a state-of-the-art software DSM implementation) on eight 133 MHz IBM RS/6000 workstations connected by a 155 Mbps FORE Systems ATM LAN. (Further details of the hardware platform are given later in Section 2.2). <p> All timing measurements were done using the high-resolution timers provided by AIX 4.1. 2.3. Applications We performed our experiments on the following set of applications: FFT, LU-NCONT, LU-CONT, OCEAN, RADIX, WATER-NSQ, and WATER-SP from the SPLASH-2 suite <ref> [26] </ref>, and SOR from the TreadMarks distribution. FFT performs a 1D complex Fast Fourier Transform on 256K data points. LU-NCONT solves a blocked LU factorization of a 1024 fi 1024 matrix with a block size of 128 where each block is allocated non-contiguously. <p> WATER-SP performs the same simulation as WATER-NSQ except with 4096 water molecules and an O (n) algorithm. Further details on these applications can be found in studies by Woo et al. <ref> [26] </ref> and Liviu et al. [11]. 3. Prefetching We begin our study by focusing on prefetching alone. The idea behind prefetching is to use knowledge of future access patterns to bring remote data into the local memory before it is actually needed.
References-found: 26

