URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/theo-4/text-learning/www/pww/papers/PWW/pwwTR.ps.Z
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/theo-4/text-learning/www/pww/index.html
Root-URL: http://www.cs.cmu.edu
Email: E-mail: Dunja.Mladenic@ijs.si  
Phone: Phone: (+38)(61) 1773 272, Fax: (+38)(61) 1258-158  
Title: Personal WebWatcher: design and implementation  
Author: Dunja Mladenic 
Web: http://www-ai.ijs.si/DunjaMladenic/home.html  
Address: Jamova 39, 11000 Ljubljana, Slovenia  Pittsburgh, PA, USA  
Affiliation: Dpt. for Intelligent Systems, J.Stefan Institute,  School of Computer Science, Carnegie Mellon University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Apte, C., Damerau, F., Weiss, </author> <title> S.M., Toward Language Independent Automated Learning of Text Categorization Models, </title> <booktitle> Proc. of the 7th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Dubline, </address> <year> 1994. </year> <title> 13 (left) and hyperlinks (right) for the HomeNet project user with id: 150211. Notice that classification accuracy scale starts at 80% accuracy. Error bars show standard deviation since accuracy is calculated as average of 10 results. </title>
Reference-contexts: There is 3 Paper reference Doc. Representation Feature Selection Learning Apte et al. <ref> [1] </ref> bag-of-words (frequency) stop list+ Decision Rules frequency weight Armstrong et al. [2] bag-of-words (Boolean) mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], [35]) or frequency of word in a given document (eg. <ref> [1] </ref>, [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. <ref> [1] </ref>, [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). <p> Apte et al. <ref> [1] </ref> used Decision Rules and observed that in case of different topics being categories, it is better to select features for each given topic (using stop-list and frequency weighting) than for all topics at once, even if the set of features is additionally reduced for each topic using entropy-based measure to
Reference: [2] <author> Armstrong, R., Freitag, D., Joachims, T., Mitchell, T., WebWatcher: </author> <title> A Learning Apprentice for the World Wide Web, </title> <booktitle> AAAI 1995 Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <address> Stanford, </address> <month> March </month> <year> 1995. </year> <note> URL: http://www.cs.cmu.edu/afs/cs/project/theo-6/web-agent/www/webagents-plus.ps.Z </note>
Reference-contexts: Recent work that arises at the intersection of Information Retrieval and Machine Learning offers some novel solutions to this problem, as well as work in Intelligent Agents. For example, Armstrong et al. <ref> [2] </ref> developed WebWatcher, a system that assists user in locating information on the World Wide Web taking keywords from the user, suggesting hyperlinks and receiving evaluation. Balabanovic et al. [3] developed "a system which learns to browse the Internet on behalf of a user". <p> The structure of the system and its implementations are described in Section 4 and Perl code is given in the Appendix. Results of the first experiments are given in Section 5. 2 "Personalizing" WebWatcher Personal WebWatcher is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" <ref> [2] </ref>, [16] and some other work related to learning apprentice and learning from text [17], [19], [21], [25]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. <p> There is 3 Paper reference Doc. Representation Feature Selection Learning Apte et al. [1] bag-of-words (frequency) stop list+ Decision Rules frequency weight Armstrong et al. <ref> [2] </ref> bag-of-words (Boolean) mutual info. <p> Per sonal WebWatcher uses an approach similar to that of WebWatcher. In WebWatcher the bag-of-words representation is used, where considered text consists of underlined words, words in the sentence containing the hyperlink, words in all the headings above the hyperlink and words given as keywords by the user <ref> [2] </ref>. Some later versions of the WebWatcher system change slightly the way of constructing text for learning, eg. adding words in the document retrieved behind hyperlink. Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], <p> <ref> [2] </ref>. Some later versions of the WebWatcher system change slightly the way of constructing text for learning, eg. adding words in the document retrieved behind hyperlink. Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], [35]) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], <ref> [2] </ref>, [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). <p> In the current implementation of PWW we weight words using mutual information between word occurance and class value [34], the same way as used in <ref> [2] </ref> or [17]. The research topic is the number of best words to consider and we observe its influence on classification accuracy and precision of the best suggested hyperlink (see Section 5). Mutual information assigns higher weight to the words that make better distinction between interesting and uninteresting documents. <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. <ref> [2] </ref>, [3], [5], [17], [26]). There are also some other techniques for model generation that have been used in text-learning. Armstrong et al. [2] used a statistical approach they called Word-Stat that assumes mutual independence of words and defines probability of class c as P (c) = 1 w (1 P <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. <ref> [2] </ref>, [3], [5], [17], [26]). There are also some other techniques for model generation that have been used in text-learning. Armstrong et al. [2] used a statistical approach they called Word-Stat that assumes mutual independence of words and defines probability of class c as P (c) = 1 w (1 P (c=w)).
Reference: [3] <author> Balabanovic, M., Shoham, Y., </author> <title> Learning Information Retrieval Agents: Experiments with Automated Web Browsing, </title> <booktitle> AAAI 1995 Spring Symposium on Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <address> Stanford, </address> <month> March </month> <year> 1995. </year> <note> URL: http://robotics.stanford.edu/people/marko/papers/lira.ps </note>
Reference-contexts: For example, Armstrong et al. [2] developed WebWatcher, a system that assists user in locating information on the World Wide Web taking keywords from the user, suggesting hyperlinks and receiving evaluation. Balabanovic et al. <ref> [3] </ref> developed "a system which learns to browse the Internet on behalf of a user". It searches the World Wide Web taking bounded amount of time, selects the best pages and receives an evaluation from the user. The evaluation is used to update the search and selection heuristics. <p> There is 3 Paper reference Doc. Representation Feature Selection Learning Apte et al. [1] bag-of-words (frequency) stop list+ Decision Rules frequency weight Armstrong et al. [2] bag-of-words (Boolean) mutual info. TFIDF, Winnow, WordStat Balabanovic et al. <ref> [3] </ref> bag-of-words (frequency) stop list+stemming+ TFIDF keep 10 best words Bartell et al. [4] bag-of-words (frequency) latent semantic | indexing using SVD Berry et al. [5] bag-of-words (frequency) latent semantic TFIDF Foltz and Dumais [12] indexing using SVD Cohen [8] bag-of-words (Boolean) infrequent words Decision Rules pruned ILP Joachims [17] bag-of-words <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], [35]) or frequency of word in a given document (eg. [1], <ref> [3] </ref> [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], <ref> [3] </ref>, [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [2], <ref> [3] </ref>, [5], [17], [26]). There are also some other techniques for model generation that have been used in text-learning.
Reference: [4] <editor> Bartell, B.T., Cottrell, G.W., Belew, </editor> <title> R.K., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, </title> <booktitle> Proceedings of the ACM SIG Information Retrieval, </booktitle> <address> Copenhagen, </address> <year> 1992. </year>
Reference-contexts: There is 3 Paper reference Doc. Representation Feature Selection Learning Apte et al. [1] bag-of-words (frequency) stop list+ Decision Rules frequency weight Armstrong et al. [2] bag-of-words (Boolean) mutual info. TFIDF, Winnow, WordStat Balabanovic et al. [3] bag-of-words (frequency) stop list+stemming+ TFIDF keep 10 best words Bartell et al. <ref> [4] </ref> bag-of-words (frequency) latent semantic | indexing using SVD Berry et al. [5] bag-of-words (frequency) latent semantic TFIDF Foltz and Dumais [12] indexing using SVD Cohen [8] bag-of-words (Boolean) infrequent words Decision Rules pruned ILP Joachims [17] bag-of-words (frequency) in/frequent words+ TFIDF, PrTFIDF, mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], [35]) or frequency of word in a given document (eg. [1], [3] <ref> [4] </ref>, [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. <ref> [4] </ref>, [5], [12]). Some of the Machine Learning techniques for feature selection could also be used (eg. [7], [32], [18]) but most of them take too long in situations with several tens of thousands of features.
Reference: [5] <author> Berry, M.W., Dumais, S.T., OBrein, G.W., </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> Vol. 37, No. 4., </volume> <pages> pp. 573-595, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: TFIDF, Winnow, WordStat Balabanovic et al. [3] bag-of-words (frequency) stop list+stemming+ TFIDF keep 10 best words Bartell et al. [4] bag-of-words (frequency) latent semantic | indexing using SVD Berry et al. <ref> [5] </ref> bag-of-words (frequency) latent semantic TFIDF Foltz and Dumais [12] indexing using SVD Cohen [8] bag-of-words (Boolean) infrequent words Decision Rules pruned ILP Joachims [17] bag-of-words (frequency) in/frequent words+ TFIDF, PrTFIDF, mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], [35]) or frequency of word in a given document (eg. [1], [3] [4], <ref> [5] </ref>, [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], <ref> [5] </ref>, [12]). Some of the Machine Learning techniques for feature selection could also be used (eg. [7], [32], [18]) but most of them take too long in situations with several tens of thousands of features. <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [2], [3], <ref> [5] </ref>, [17], [26]). There are also some other techniques for model generation that have been used in text-learning. Armstrong et al. [2] used a statistical approach they called Word-Stat that assumes mutual independence of words and defines probability of class c as P (c) = 1 w (1 P (c=w)).
Reference: [6] <author> Burke, R., Hammond, K., Kozlovsky, J., </author> <title> Knowledge-based Information Retrieval for Semi-Structured Text, </title> <booktitle> Working Notes from AAAI Fall Symposium on AI Applications in Knowledge Navigation and Retrieval, </booktitle> <pages> pp. 19-24, </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Their agent accepts high-level user goals and dynamically synthesizes the appropriate sequence of Internet commands to satisfy those goals. Hammond et al. [?] and Burke et al. <ref> [6] </ref> developed a system that uses a "natural language question-based interface to access distributed text information sources" and helps the user to find answers to her/his question in a databases such as FAQ files.
Reference: [7] <author> Caruana, R., Freitag, D., </author> <title> Greedy Attribute Selection, </title> <booktitle> Proc. of the 11th International Conference on Machine Learning ICML94, </booktitle> <pages> pp. 28|26, </pages> <year> 1994. </year>
Reference-contexts: Some of the Machine Learning techniques for feature selection could also be used (eg. <ref> [7] </ref>, [32], [18]) but most of them take too long in situations with several tens of thousands of features. In the current implementation of PWW we weight words using mutual information between word occurance and class value [34], the same way as used in [2] or [17].
Reference: [8] <author> Cohen, </author> <title> W.W., Learning to Classify English Text with ILP Methods, </title> <booktitle> Workshop on Inductive Logic Programming, </booktitle> <address> Leuven, </address> <month> September </month> <year> 1995. </year> <title> 14 (left) and hyperlinks (right) for the HomeNet project user with id: 150502. Notice that classification accuracy scale starts at 80% accuracy. Error bars show standard deviation since accuracy is calculated as average of 10 results. hyperlinks (right) for the HomeNet project user with id: </title> <type> 150101. </type>
Reference-contexts: TFIDF, Winnow, WordStat Balabanovic et al. [3] bag-of-words (frequency) stop list+stemming+ TFIDF keep 10 best words Bartell et al. [4] bag-of-words (frequency) latent semantic | indexing using SVD Berry et al. [5] bag-of-words (frequency) latent semantic TFIDF Foltz and Dumais [12] indexing using SVD Cohen <ref> [8] </ref> bag-of-words (Boolean) infrequent words Decision Rules pruned ILP Joachims [17] bag-of-words (frequency) in/frequent words+ TFIDF, PrTFIDF, mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], <ref> [8] </ref>, [23], [26], [35]) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. <p> use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], <ref> [8] </ref>, [23], [26], [35]) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> One of the approaches to reduce number of different words is to use "stop-list" containing common English words (eg. a, the, with) or pruning the infrequent and/or very frequent words ( <ref> [8] </ref>, [17]). There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). <p> Cohen <ref> [8] </ref> used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER.
Reference: [9] <author> Drummond,C., Ionescu, D., Holte, R., </author> <title> A Learning Agent that Assists the Browsing of Software Libraries, </title> <type> Technical Report TR-95-12, </type> <institution> Computer Science Dept., University of Ottawa, </institution> <year> 1995. </year>
Reference-contexts: Some of these agents use the context of documents and adopt Information Retrieval approaches (eg. news filtering). Others rely on correlation between different users | performing "social 1 learning" (eg. entertainment recommendation). Holte and Drummond [15], Drummond et al. <ref> [9] </ref> designed a system that assists browsing of software libraries, taking keywords from the user and using a rule-based system with forward chaining inference, assuming that the library consists of one type of item and the user goal is a single item.
Reference: [10] <author> Mc Elligott, M., Sorensen, H., </author> <title> An emergent approach to information filtering, </title> <journal> Abakus. U.C.C. Computer Science Journal, </journal> <volume> Vol 1, No. 4, </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: Nearest Neighbor, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [33], <ref> [10] </ref> (only bigrams) edges with Genetic Algorithms Yang [35] bag-of-words (Boolean, stop list adapted frequency, TFIDF) k-Nearest Neighbor Table 1: Document representation, feature selection and learning algorithms used in some related work. some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth efforts. <p> Lewis and Gale [23] used a combination of a Naive Bayesian classifier and logistic regression defining probability of class c for given document doc that contains words w as P (c=doc) = P P (w=c) ) P P (w=c) ) Maes [24] used Memory-Based reasoning, McElligot and Sorensen <ref> [10] </ref>, [33] used a connectionist approach combined with Genetic Algorithms. We decided to test different learning algorithms on PWW data (see Section 5), since it is not clear which algorithm is the most appropriate.
Reference: [11] <author> Etizioni, O., Weld, D., </author> <title> A Softbot-Based Interface to the Internet, </title> <journal> Communications of the ACM Vol. </journal> <volume> 37, No. 7, pp.72-79, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Etizioni and Weld <ref> [11] </ref> offer an integrated interface to the Internet combining UNIX shell and the World Wide Web to interact with Internet resources. Their agent accepts high-level user goals and dynamically synthesizes the appropriate sequence of Internet commands to satisfy those goals.
Reference: [12] <author> Foltz, P. W. and Dumais, S. T., </author> <title> Personalized information delivery: An analysis of information filtering methods. </title> <journal> Communications of the ACM, </journal> <volume> 35(12), pp.51|60, </volume> <year> 1992. </year> <title> 0 0.4 0.8 1 100 200 300 400 500 600 700 800 900 1000 Precision Vector size HomeNet data, user 150202 NBayesTFHL kNNeighbourHL hyperlinks (right) for the HomeNet project user with id: 150202. hyperlinks (right) for the HomeNet project user with id: </title> <type> 150211. </type>
Reference-contexts: TFIDF, Winnow, WordStat Balabanovic et al. [3] bag-of-words (frequency) stop list+stemming+ TFIDF keep 10 best words Bartell et al. [4] bag-of-words (frequency) latent semantic | indexing using SVD Berry et al. [5] bag-of-words (frequency) latent semantic TFIDF Foltz and Dumais <ref> [12] </ref> indexing using SVD Cohen [8] bag-of-words (Boolean) infrequent words Decision Rules pruned ILP Joachims [17] bag-of-words (frequency) in/frequent words+ TFIDF, PrTFIDF, mutual info. <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], <ref> [12] </ref>). Some of the Machine Learning techniques for feature selection could also be used (eg. [7], [32], [18]) but most of them take too long in situations with several tens of thousands of features.
Reference: [13] <author> Hammond, K., Burke, R., Schmitt, K., </author> <title> A Case-Based Approach to Knowledge Navigation, </title> <booktitle> AAAI Workshop on Indexing and Reuse in Multimedia Systems, </booktitle> <pages> pp. 46-57, </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference: [14] <institution> HomeNet: a research project at Carnegie Mellon University, </institution> <address> Pittsburgh, PA, USA, </address> <year> 1996, </year> <month> URL:http://homenet.andrew.cmu.edu/progress/ </month>
Reference-contexts: Documents are currently represented using the bag-of-words approach (see Section 3.1) and feature selection is performed using mutual information approach (see Section 3.2). Our experiments are performed on data collected for four users participating in the HomeNet project <ref> [14] </ref> with the data characteristics given in Table 2.
Reference: [15] <author> Holte, R.C., Drummond, C., </author> <title> A Learning Apprentice For Browsing, </title> <booktitle> AAAI Spring Symposium on Software Agents, </booktitle> <year> 1994. </year>
Reference-contexts: Some of these agents use the context of documents and adopt Information Retrieval approaches (eg. news filtering). Others rely on correlation between different users | performing "social 1 learning" (eg. entertainment recommendation). Holte and Drummond <ref> [15] </ref>, Drummond et al. [9] designed a system that assists browsing of software libraries, taking keywords from the user and using a rule-based system with forward chaining inference, assuming that the library consists of one type of item and the user goal is a single item.
Reference: [16] <author> Joachims, T., Mitchell, T., Freitag, D., Armstrong, R., WebWatcher: </author> <title> Machine Learning and Hypertext, </title> <address> Fachgruppentreffen Maschinelles Lernen, Dortmund, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Results of the first experiments are given in Section 5. 2 "Personalizing" WebWatcher Personal WebWatcher is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [2], <ref> [16] </ref> and some other work related to learning apprentice and learning from text [17], [19], [21], [25]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. <p> It can also suggest pages related to the current page using information stored in the structure of hypertext without considering the text itself <ref> [16] </ref>, or send an e-mail message to the user whenever specified pages change. The same WebWatcher version is designed to serve all users, collecting information and sharing it between users.
Reference: [17] <author> Joachims, T., </author> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization, </title> <type> Technical report, </type> <institution> CMU-CS-96-118, School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1996. </year> <title> 16 hyperlinks (right) for the HomeNet project user with id: 150502. hyperlinks (right) for the HomeNet project user with id: 150101. Notice that log-scale on x-aix. </title>
Reference-contexts: Results of the first experiments are given in Section 5. 2 "Personalizing" WebWatcher Personal WebWatcher is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [2], [16] and some other work related to learning apprentice and learning from text <ref> [17] </ref>, [19], [21], [25]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. WebWatcher can be described as an agent that assists users in locating information on the WWW. <p> al. [3] bag-of-words (frequency) stop list+stemming+ TFIDF keep 10 best words Bartell et al. [4] bag-of-words (frequency) latent semantic | indexing using SVD Berry et al. [5] bag-of-words (frequency) latent semantic TFIDF Foltz and Dumais [12] indexing using SVD Cohen [8] bag-of-words (Boolean) infrequent words Decision Rules pruned ILP Joachims <ref> [17] </ref> bag-of-words (frequency) in/frequent words+ TFIDF, PrTFIDF, mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], [35]) or frequency of word in a given document (eg. [1], [3] [4], [5], <ref> [17] </ref>, [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> One of the approaches to reduce number of different words is to use "stop-list" containing common English words (eg. a, the, with) or pruning the infrequent and/or very frequent words ( [8], <ref> [17] </ref>). There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). <p> words is to use "stop-list" containing common English words (eg. a, the, with) or pruning the infrequent and/or very frequent words ( [8], <ref> [17] </ref>). There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). <p> In the current implementation of PWW we weight words using mutual information between word occurance and class value [34], the same way as used in [2] or <ref> [17] </ref>. The research topic is the number of best words to consider and we observe its influence on classification accuracy and precision of the best suggested hyperlink (see Section 5). Mutual information assigns higher weight to the words that make better distinction between interesting and uninteresting documents. <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [2], [3], [5], <ref> [17] </ref>, [26]). There are also some other techniques for model generation that have been used in text-learning. Armstrong et al. [2] used a statistical approach they called Word-Stat that assumes mutual independence of words and defines probability of class c as P (c) = 1 w (1 P (c=w)). <p> Joachims <ref> [17] </ref> introduced Probabilistic T F IDF that takes into account document representation fi and de 6 fines probability of class c for given document doc that contains words w as P (c=doc) = P P (c)P (w=c) P i P (c i )P (w=c i ) P (w=doc; fi) where P <p> In first experiments we observe how vector length (number of features selected) influences model quality for two learning algorithms: Naive Bayesian classifier on frequency vector as used by Joachims <ref> [17] </ref> (see Section 3.3) and k-Nearest Neighbor approach on 11 frequency vectors using Euclidean distance between examples and summing class prob-abilities predicted by k-neighbors. We tested both algorithms on data for documents behind hyperlinks U ser DOC and data for hyperlinks U ser HL (see Section 4.2).
Reference: [18] <author> John, G.H., Kohavi, R., Pfleger, K., </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <booktitle> Proc. of the 11th International Conference on Machine Learning ICML94, </booktitle> <pages> pp. 121|129, </pages> <year> 1994. </year> <title> [19] de Kroon, </title> <editor> H.C.M, Mitchell, T., Kerckhoffs, E.J.H., </editor> <title> Improving Learning Accuracy in Information Filtering, </title> <booktitle> ICML-96 Workshop Machine learning meets human computer interaction, </booktitle> <year> 1996. </year> <note> URL: http://www.ics.forth.gr/ mous-taki/ICML96 HCI ML/kroon.ps </note>
Reference-contexts: Some of the Machine Learning techniques for feature selection could also be used (eg. [7], [32], <ref> [18] </ref>) but most of them take too long in situations with several tens of thousands of features. In the current implementation of PWW we weight words using mutual information between word occurance and class value [34], the same way as used in [2] or [17].
Reference: [20] <author> Krulwich, B., Burkey, C., </author> <title> The ContactFinder agent: Answering buletin board questions with referrals, </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence AAAI 96, </booktitle> <address> pp.10|15, </address> <year> 1996. </year>
Reference-contexts: It uses these rules to provide advice to the user for new, unscheduled meetings. Lang [21] developed a system for electronic news filtering that uses text-learning to generate models of user interests. Krulwich and Burkey <ref> [20] </ref> proposed "The ContactFinder agent" that reads and responds to bulletin board messages; assists users by referring them to other people who can help them and; categorizes messages and extracts their topic areas. Maes [24] described "interface agents" that learn from the user as well as from other agents.
Reference: [21] <author> Lang, K., </author> <title> News Weeder: Learning to Filter Netnews, </title> <booktitle> Proc. of the 12th International Conference on Machine Learning ICML95, </booktitle> <year> 1995. </year>
Reference-contexts: Mitchell et al. [25] proposed a system connected to the user's electronic calendar, that generates sets of rules capturing the user's scheduling preferences and some other information about individual attendees of meetings. It uses these rules to provide advice to the user for new, unscheduled meetings. Lang <ref> [21] </ref> developed a system for electronic news filtering that uses text-learning to generate models of user interests. <p> Results of the first experiments are given in Section 5. 2 "Personalizing" WebWatcher Personal WebWatcher is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [2], [16] and some other work related to learning apprentice and learning from text [17], [19], <ref> [21] </ref>, [25]. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. WebWatcher can be described as an agent that assists users in locating information on the WWW.
Reference: [22] <author> Lashkari, Y., </author> <title> The WebHund Personalized Document Filtering System, </title> <booktitle> 1995. </booktitle> <pages> 17 </pages>
Reference-contexts: Since each user has her/his own copy of the system - her/his own agent, these agents can communicate and exchange information on a base of similarity between their users, often referred to as collaborative or social learning <ref> [22] </ref>, [31].
Reference: [23] <author> Lewis, D.,D., Gale, W., A., </author> <title> A Sequential Algorithm for Training Text Classifiers, </title> <booktitle> Proc. of the 7th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Dubline, </address> <year> 1994. </year>
Reference-contexts: Naive Bayes Lewis et al. <ref> [23] </ref> bag-of-words (Boolean) log likelihood logistic regression ratio combined with Naive Bayes Maes [24] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Pazzani et al. [26] bag-of-words (Boolean) stop list+ TFIDF, Naive Bayes, mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], <ref> [23] </ref>, [26], [35]) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], <ref> [23] </ref>, [26]) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). Some of the Machine Learning techniques for feature selection could also be used (eg. [7], [32], [18]) but most of them take too long in situations with several tens of thousands of features. <p> Cohen [8] used Decision Rules and the Inductive Logic Programming systems FOIL and FLIPPER. Lewis and Gale <ref> [23] </ref> used a combination of a Naive Bayesian classifier and logistic regression defining probability of class c for given document doc that contains words w as P (c=doc) = P P (w=c) ) P P (w=c) ) Maes [24] used Memory-Based reasoning, McElligot and Sorensen [10], [33] used a connectionist approach
Reference: [24] <author> Maes, P., </author> <title> Agents that Reduce Work and Information Overload, </title> <journal> Communications of the ACM Vol. </journal> <volume> 37, No. 7, pp.30-40, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Krulwich and Burkey [20] proposed "The ContactFinder agent" that reads and responds to bulletin board messages; assists users by referring them to other people who can help them and; categorizes messages and extracts their topic areas. Maes <ref> [24] </ref> described "interface agents" that learn from the user as well as from other agents. As examples of such agents they developed agents for: electronic mail handling, meeting scheduling, electronic news filtering and entertainment recommendation. <p> Naive Bayes Lewis et al. [23] bag-of-words (Boolean) log likelihood logistic regression ratio combined with Naive Bayes Maes <ref> [24] </ref> bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Pazzani et al. [26] bag-of-words (Boolean) stop list+ TFIDF, Naive Bayes, mutual info. <p> Lewis and Gale [23] used a combination of a Naive Bayesian classifier and logistic regression defining probability of class c for given document doc that contains words w as P (c=doc) = P P (w=c) ) P P (w=c) ) Maes <ref> [24] </ref> used Memory-Based reasoning, McElligot and Sorensen [10], [33] used a connectionist approach combined with Genetic Algorithms. We decided to test different learning algorithms on PWW data (see Section 5), since it is not clear which algorithm is the most appropriate.
Reference: [25] <author> Mitchell, T., Caruana, R., Freitag, D., McDermott, J., Zabowski, D., </author> <title> Experience with a Learning Personal Assistant, </title> <journal> Communications of the ACM Vol. </journal> <volume> 37, No. 7, pp.81-91, </volume> <month> July </month> <year> 1994. </year> <note> URL: http://www.cs.cmu.edu/afs/cs/user/mitchell/ftp/cacm.ps.Z </note>
Reference-contexts: Pazzani et al. [26] collect ratings of the explored Web pages from the user and learn a user profile from them. Pages are separated according to their topic and a separate profile is learned for each topic. Mitchell et al. <ref> [25] </ref> proposed a system connected to the user's electronic calendar, that generates sets of rules capturing the user's scheduling preferences and some other information about individual attendees of meetings. It uses these rules to provide advice to the user for new, unscheduled meetings. <p> Results of the first experiments are given in Section 5. 2 "Personalizing" WebWatcher Personal WebWatcher is mainly inspired by WebWatcher: "a Learning Apprentice for the World Wide Web" [2], [16] and some other work related to learning apprentice and learning from text [17], [19], [21], <ref> [25] </ref>. The idea of a learning apprentice is to automatically customize to individual users, using each user interaction as a training example. WebWatcher can be described as an agent that assists users in locating information on the WWW. <p> There are also some other types of "Personal agents" the user could use, for example, an agent for Calendar Apprentice <ref> [25] </ref>, and these agents can exchange information they have about the same user in different fields/activities. we plan to investigate these in the future work on Personal WebWatcher. 3 PWW "Behind the stage" There are many research question behind Personal WebWatcher, that wait to be answered.
Reference: [26] <author> Pazzani, M., Muramatsu, J., Billsus, D., Syskill & Webert: </author> <title> Identifying interesting web sites, </title> <booktitle> AAAI Spring Symposium on Machine Learning in Information Access, Stanford, March 1996 and Proceedings of the Thirteenth National Conference on Artificial Intelligence AAAI 96, </booktitle> <address> pp.54|61, </address> <year> 1996. </year> <editor> [27] van Rijsbergen, C.J,. Harper, D.J., Porter, M.F., </editor> <title> The selection of good search terms, </title> <booktitle> Information Processing & Management, 17, </booktitle> <address> pp.77|91, </address> <year> 1981. </year>
Reference-contexts: It searches the World Wide Web taking bounded amount of time, selects the best pages and receives an evaluation from the user. The evaluation is used to update the search and selection heuristics. Pazzani et al. <ref> [26] </ref> collect ratings of the explored Web pages from the user and learn a user profile from them. Pages are separated according to their topic and a separate profile is learned for each topic. <p> Naive Bayes Lewis et al. [23] bag-of-words (Boolean) log likelihood logistic regression ratio combined with Naive Bayes Maes [24] bag-of-words+ mail/news header info.+ Memory-Based header info. selecting keywords reasoning Pazzani et al. <ref> [26] </ref> bag-of-words (Boolean) stop list+ TFIDF, Naive Bayes, mutual info. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], <ref> [26] </ref>, [35]) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. <p> There is also the possibility of word stemming. Many approaches introduce some sort of word weighting and select only the best words (eg. [1], [2], [3], [17], [23], <ref> [26] </ref>) or reduce dimensionality using latent semantic indexing with singular value decomposition (eg. [4], [5], [12]). Some of the Machine Learning techniques for feature selection could also be used (eg. [7], [32], [18]) but most of them take too long in situations with several tens of thousands of features. <p> This technique has already been used in Machine Learning experiments on World Wide Web data (eg. [2], [3], [5], [17], <ref> [26] </ref>). There are also some other techniques for model generation that have been used in text-learning. Armstrong et al. [2] used a statistical approach they called Word-Stat that assumes mutual independence of words and defines probability of class c as P (c) = 1 w (1 P (c=w)). <p> Armstrong et al. [2] used a statistical approach they called Word-Stat that assumes mutual independence of words and defines probability of class c as P (c) = 1 w (1 P (c=w)). Pazzani et al. <ref> [26] </ref> used a Naive (Simple) Bayesian classifier on Boolean vectors, that assumes independence of words and defines probability of class c for given document doc that contains words w as proportional to P (c) w P (c=w). They also used Nearest Neighbor and symbolic learning using Decision Trees.
Reference: [28] <author> Shaw Jr, W.M., </author> <title> Term-relevance computations and perfect retrieval performance, </title> <booktitle> Information Processing & Management, 31(4), </booktitle> <address> pp.491|498, </address> <year> 1995. </year>
Reference-contexts: Shaw <ref> [28] </ref> proposed special handling of singularities in the above formulas for p (w) and p (w), namely, p (w) = 1 #docs 2 when T F (w; c) = 0 and p (w) = 1 1 #docs 2 when T F (w; c) = #docs (c) 3.3 Learning algorithm One of
Reference: [29] <author> Rocchio, J., </author> <title> Relevance Feedback in Information Retrieval, in The SMART Retrieval System: Experiments in Automatic Document Processing, Chapter 14, </title> <publisher> pp.313|323, Prentice-Hall Inc., </publisher> <year> 1971. </year>
Reference-contexts: algorithm One of the well-established techniques for text in Information Retrieval is to represent each document as a T F IDF -vector in the space of words that appeared in training documents [30], sum all interesting document vectors and use the resulting vector as a model for classification (based on <ref> [29] </ref> relevance feedback method).
Reference: [30] <author> Salton, G., Buckley, C., </author> <title> Term Weighting Approaches in Automatic Text Retrieval, </title> <type> Technical report, </type> <institution> COR-87-881, Department of Computer Science, Cornell University, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: and p (w) = 1 1 #docs 2 when T F (w; c) = #docs (c) 3.3 Learning algorithm One of the well-established techniques for text in Information Retrieval is to represent each document as a T F IDF -vector in the space of words that appeared in training documents <ref> [30] </ref>, sum all interesting document vectors and use the resulting vector as a model for classification (based on [29] relevance feedback method). <p> The exact formulas used in different approaches may slightly vary (some factors are added, normalization performed <ref> [30] </ref>) but the idea remains the same. A new document is then represented as a vector in the same vector space as the generated model and the distance between them is measured (usually defined as a cosine of angle between vectors) in order to classify the document.
Reference: [31] <author> Shardanand, U., Maes, P., </author> <title> Social Information Filtering: Algorithms for Automating "Word of Mouth", </title> <booktitle> CHI'95 Mosaic of Creativity, </booktitle> <pages> pp. 210|217, </pages> <year> 1995. </year>
Reference-contexts: Since each user has her/his own copy of the system - her/his own agent, these agents can communicate and exchange information on a base of similarity between their users, often referred to as collaborative or social learning [22], <ref> [31] </ref>.
Reference: [32] <author> Skalak, </author> <title> D.B., Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms, </title> <booktitle> Proc. of the 11th International Conference on Machine Learning ICML94, </booktitle> <pages> pp. 293|301, </pages> <year> 1994. </year>
Reference-contexts: Some of the Machine Learning techniques for feature selection could also be used (eg. [7], <ref> [32] </ref>, [18]) but most of them take too long in situations with several tens of thousands of features. In the current implementation of PWW we weight words using mutual information between word occurance and class value [34], the same way as used in [2] or [17].
Reference: [33] <author> Sorensen, H., McElligott, M., PSUN: </author> <title> A Profiling System for Usenet News, </title> <booktitle> CIKM'95 Intelligent Information Agents Workshop, </booktitle> <address> Baltimore, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Nearest Neighbor, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott <ref> [33] </ref>, [10] (only bigrams) edges with Genetic Algorithms Yang [35] bag-of-words (Boolean, stop list adapted frequency, TFIDF) k-Nearest Neighbor Table 1: Document representation, feature selection and learning algorithms used in some related work. some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth <p> There is also some work that uses additional information such as word position [8] or word tuples called n-grams <ref> [33] </ref>. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). In case of poor system performance, some additional information from HTML-structure could be added, for example, frequency of word in headlines of a given document. <p> Lewis and Gale [23] used a combination of a Naive Bayesian classifier and logistic regression defining probability of class c for given document doc that contains words w as P (c=doc) = P P (w=c) ) P P (w=c) ) Maes [24] used Memory-Based reasoning, McElligot and Sorensen [10], <ref> [33] </ref> used a connectionist approach combined with Genetic Algorithms. We decided to test different learning algorithms on PWW data (see Section 5), since it is not clear which algorithm is the most appropriate.
Reference: [34] <author> Quinlan, J.R., </author> <title> Constructing Decision Tree in C4.5: Programs for Machine Learning, </title> <publisher> pp.17-26, Morgan Kaufman Publishers, </publisher> <year> 1993. </year>
Reference-contexts: In the current implementation of PWW we weight words using mutual information between word occurance and class value <ref> [34] </ref>, the same way as used in [2] or [17]. The research topic is the number of best words to consider and we observe its influence on classification accuracy and precision of the best suggested hyperlink (see Section 5).
Reference: [35] <author> Yang, Y., </author> <title> Expert Network: Effective and Efficient Learning form Human Decisions in Text Categorization and Retrieval, </title> <booktitle> Proc. of the 7th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <address> Dubline, </address> <year> 1994. </year> <month> 18 </month>
Reference-contexts: Nearest Neighbor, Decision Trees Sorensen and n-gram graph weighting graph connectionist combined Mc Elligott [33], [10] (only bigrams) edges with Genetic Algorithms Yang <ref> [35] </ref> bag-of-words (Boolean, stop list adapted frequency, TFIDF) k-Nearest Neighbor Table 1: Document representation, feature selection and learning algorithms used in some related work. some evidence in Information Retrieval research, that for long documents, considering information additional to bag-of-words is not worth efforts. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], <ref> [35] </ref>) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. <p> Many current systems that learn on text use the bag-of-words representation using either Boolean features indi cating if specific word occurred in document (eg. [2], [8], [23], [26], <ref> [35] </ref>) or frequency of word in a given document (eg. [1], [3] [4], [5], [17], [35]). There is also some work that uses additional information such as word position [8] or word tuples called n-grams [33]. We decided to use the bag-of-words representation using frequency of word and observe success of given advice (whether user selected the advised hyperlink). <p> We plan to make additional experiments using the proposed combined weighting as 5 well as using some other weighting methods. For example, combining a stop list with weighting words by their frequency and keeping the most frequent words <ref> [35] </ref> or using word weighting used in the odds ratio method [27]. Odds ratio is the method of document ranking according to their relevance for a given problem (eg. being interesting for user). <p> They also used Nearest Neighbor and symbolic learning using Decision Trees. A variant of k-Nearest Neighbor was also used by Yang <ref> [35] </ref>, where relevance of class c given document doc is defined as rel (c=doc) = P k i=1 similarity (doc; D i )P (c=D i ) and similarity is measured by cosine between vectors and P (c=D i ) = #D i Inc #D i Intrainingdata (same document may occur several
References-found: 33

