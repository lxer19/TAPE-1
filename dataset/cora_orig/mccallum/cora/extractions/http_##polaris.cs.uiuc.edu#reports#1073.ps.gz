URL: http://polaris.cs.uiuc.edu/reports/1073.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: An Integrated Hardware/Software Solution for Effective Management of Local Storage in High-Performance Systems (Extended Version)  
Author: Elana D. Granston Alexander V. Veidenbaum 
Address: Urbana, Illinois 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Date: August, 1991.  1073  
Note: Abridged version in International Conference on Parallel Processing,  
Pubnum: Technical Report No.  
Abstract: The potential of high-performance systems, especially vector and parallel machines, is generally limited by the bandwidth between processors and memory. To achieve the performance of which these machines should be capable, greater emphasis must be placed on optimizing array accesses. We propose a practical, integrated hardware/software strategy for increasing the effectiveness of local storage management. Our scheme provides many of the advantages of both compile-time and run-time memory management techniques. In this paper, we describe our local storage facility: the priority data cache. We also describe compile-time techniques for easily and effectively utilizing this level of local storage.
Abstract-found: 1
Intro-found: 1
Reference: [AK86] <author> Randy Allen and Ken Kennedy. </author> <title> Vector Register Allocation. </title> <institution> Technical Report Rice COMP TR86-45, Rice University, </institution> <year> 1986. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89] </ref> or local memory [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> A simple compile-time strategy for accomplishing this has been detailed. To maximize the number of promising compile-time reuse opportunities, locality enhancing program transformations, such as those proposed in <ref> [AK86, Wol87, Por89, Wol89, FST91, WL91] </ref> should be applied beforehand. Qualitative arguments have been presented regarding the benefits of our integrated hardware/software local storage management strategy over existing techniques.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Condition (i) is met by ensuring that the sink of a dependence post-dominates the source in a program's control flow graph <ref> [ASU86] </ref>. Condition (ii) is met by ensuring that the processor that accesses a given datum at the source will be the same one that accesses it at the sink.
Reference: [Bel66] <author> L. A. Belady. </author> <title> A Study of Replacement Algorithms for a Virtual-Storage Computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: When the compiler does not resolve conflicts for cache space, the hardware does. In determining which data to cache, the cache hit ratio is maximized by giving priority to data that will be reused the soonest <ref> [Bel66] </ref>. While such data are not immune to replacement, they can only be displaced as a result of more recent accesses to other data with equal or higher priority.
Reference: [BJEW91] <author> Francois Bodin, William Jalby, Christine Eisenbeis, and Daniel Windheiser. </author> <title> Window-Based Register Allocation. </title> <type> Technical report, </type> <institution> INRIA, </institution> <year> 1991. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89] </ref> or local memory [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> Dependences meeting these requirements should be selected in order of increasing reaccess time. It is interesting to note that the idea of selecting dependences in this order has been independently adopted by Bodin et al. <ref> [BJEW91] </ref> who study the problem of allocating array elements to scalar registers. Because of the criteria by which we select dependences, we can easily detect the first and last dynamic reference to data within the period indicated by the dependence.
Reference: [Bre87] <author> Glen Alan Brent. </author> <title> Using Program Structure to Achieve Prefetching for Cache Memories. </title> <type> PhD thesis, </type> <institution> Center for Supercomputing Research and Development, </institution> <type> Technical Report 647, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: The hardware controller can also look ahead a few instructions, and prefetch any data the processor will need [Lee87]. In a simple look-ahead scheme, the prefetch mechanism is blocked when a branch instruction is encountered <ref> [Bre87] </ref>. A more sophisticated mechanism may either guess which branch may be taken or prefetch along two or more branches [SDV + 87, GGH91].
Reference: [CCK90] <author> David Callahan, Steve Carr, and Ken Kennedy. </author> <title> Improving Register Allocation for Subscripted Variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 25(6), </volume> <pages> pages 53-65, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89] </ref> or local memory [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems.
Reference: [Chi89] <author> Chi-Hung Chi. </author> <title> Compiler-Driven Cache Management Using A State Level Transition Model. </title> <type> PhD thesis, </type> <institution> School of Electrical Engineering, Purdue University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers <ref> [AK86, BJEW91, CCK90, Chi89] </ref> or local memory [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> Consequently, for this class of applications, traditional hardware resolution schemes are less effective. Several integrated hardware/software strategies have previously been applied to problems of optimizing data accesses in memory hierarchies. <ref> [Chi89, HS90] </ref> propose hardware-assisted solutions to the problem of register allocation in the presence of aliased variables. The addresses associated with register values are stored in associative caches so that data accesses can be checked against them and redirected to the appropriate registers, as necessary. <p> While this approach has potential for improving the effectiveness of registers, the cost per storage unit is relatively high. Therefore, this technique cannot be cost-effectively applied to local memory. Chi <ref> [Chi89] </ref> discusses the design of an on-chip hardware cache that can be selectively bypassed if the compiler can detect that accessed data will be stored in registers or that data will not be reused before being displaced. <p> The success of this technique in practice depends on the compiler's ability to detect with certainty (or near certainty) that data will not be reaccessed before being displaced. 3 For the range of cache sizes explored in <ref> [Chi89] </ref> (32-1024 words), a single scalar or vector access can cause a significant portion of cached data to be displaced. <p> Although the SP /P R field can take on four values, only three of these combinations are used to effect the compile-time mangement decisions described in Sections 4 and 5. The remaining unused combination could be used to signify other actions, such as selective cache bypassing <ref> [Chi89] </ref>. 7 Note that the number of priority levels is restricted to two. Supporting many software-assignable priority levels can be expensive both in space, and in the complexity of hardware.
Reference: [CV91] <author> Yun-Chin Chen and Alexander V. Veidenbaum. </author> <title> Comparison and Analysis of Software and Directory Coherence Schemes. </title> <type> Technical Report 1055, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: Since memory reference patterns are more variable in vector and parallel programs [FP91, LMY88], it is difficult to block programs [LRW91, FST91] or tune caches <ref> [CV91] </ref> so that cache memory can be used effectively over a wide range of applications. Moreover, the applications that are run on high-performance systems typically consist of large data sets that may exceed cache size and cause thrashing.
Reference: [EJWB90] <author> Christine Eisenbeis, William Jalby, Daniel Windheiser, and Francois Bodin. </author> <title> A Strategy for Array Management in Local Memory. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> Hence, in general, this bound should provide a sufficient estimate to the actual window size. When necessary, however, a much tighter upper bound on k W (ffi) k can be obtained by applying more precise methods, such as those covered in <ref> [EJWB90, FST91, GJG88b, KK91, TEW] </ref>.
Reference: [FP91] <author> John W. C. Fu and Janak H. Patel. </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Due to the compile-time difficulties in detecting profitable opportunities to reuse data, many vector and parallel machines rely primarily on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Since memory reference patterns are more variable in vector and parallel programs <ref> [FP91, LMY88] </ref>, it is difficult to block programs [LRW91, FST91] or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. <p> Hence, software prefetching can not only be incorporated into our strategy, but our compile-time analysis can be used to determine the potential benefits of prefetching at a particular program point. Hardware prefetching can be achieved using a technique proposed by Fu and Patel <ref> [FP91] </ref> to fetch multiple cache blocks when one is requested. If a cache block is requested as a result of scalar access, the next few consecutive blocks are also fetched.
Reference: [FST91] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness (Extended Abstract). </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Since memory reference patterns are more variable in vector and parallel programs [FP91, LMY88], it is difficult to block programs <ref> [LRW91, FST91] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, the applications that are run on high-performance systems typically consist of large data sets that may exceed cache size and cause thrashing. <p> Hence, in general, this bound should provide a sufficient estimate to the actual window size. When necessary, however, a much tighter upper bound on k W (ffi) k can be obtained by applying more precise methods, such as those covered in <ref> [EJWB90, FST91, GJG88b, KK91, TEW] </ref>. <p> input: C candidate dependences identified in Step 1 output: F final selection of high priority dependences F while C 6= choose ffi 2 C with shortest reaccess time C C fffig if k W ( F [ fffig) k &lt; threshold T then F F [ fffig end while in <ref> [FST91, KK91, TEW] </ref>. The computation of reference windows becomes more complex when symbolic loop bounds are involved. This case can be handled by estimating loop bounds, generating multiple-version loops, or simply ignoring these cases. <p> A simple compile-time strategy for accomplishing this has been detailed. To maximize the number of promising compile-time reuse opportunities, locality enhancing program transformations, such as those proposed in <ref> [AK86, Wol87, Por89, Wol89, FST91, WL91] </ref> should be applied beforehand. Qualitative arguments have been presented regarding the benefits of our integrated hardware/software local storage management strategy over existing techniques.
Reference: [GGH91] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessey. </author> <title> Two Techniques to Enhance the Performance of Memory Consistency Models. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 355-364, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In a simple look-ahead scheme, the prefetch mechanism is blocked when a branch instruction is encountered [Bre87]. A more sophisticated mechanism may either guess which branch may be taken or prefetch along two or more branches <ref> [SDV + 87, GGH91] </ref>. To prevent the displacement of data that will be reused by prefetched data that may not be reused, only conservatively prefetched data should be assigned high priority at the point of prefetching.
Reference: [GJG88a] <author> Kyle Gallivan, William Jalby, and Dennis Gannon. </author> <title> On the Problem of Optimizing Data Transfers for Complex Memory Systems. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 238-253, </pages> <month> July </month> <year> 1988. </year> <month> 30 </month>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems.
Reference: [GJG88b] <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for Cache and Local Memory Management. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> The general class of dependences with one or more distinct constant distant vectors is roughly analogous to the set of uniformly-generated dependences, originally introduced by Gannon et al. <ref> [GJG88b] </ref>. Since it is clearly desirable to be able to consider dependences from this class with more than one constant distance, we "split" these dependences, so that each is associated with exactly one constant distance, ignoring non-constant distance vectors. <p> This set of data is known as the reference window <ref> [GJG88b] </ref> for ffi at time t, denoted W t (ffi). In general, the size of a dependence's reference window varies in time. One of the desirable properties of a constant dependence is that, in the steady state, the size of its reference window is constant. <p> Hence, in general, this bound should provide a sufficient estimate to the actual window size. When necessary, however, a much tighter upper bound on k W (ffi) k can be obtained by applying more precise methods, such as those covered in <ref> [EJWB90, FST91, GJG88b, KK91, TEW] </ref>.
Reference: [Goo83] <author> J. R. Goodman. </author> <title> Using Cache Memory to Reduce Processor-Memory Traffic. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: This solution works for uniprocessors and multiprocessors. Note that the number of opportunities to reuse local data copies depends on the length of the time slice. This should not be a problem in practice since there are other reasons as well for adopting long time slices in high-performance systems <ref> [Goo83] </ref>. 5.6 Summary To effectively utilize the priority data cache, data priorities should be assigned dynamically. Opportunities for data reuse are indicated by data dependences. The cache size limits the amount of data that can be designated high priority, and hence, constrains the selection of dependences.
Reference: [Gor89] <author> Edward H. Gornish. </author> <title> Compile Time Analysis For Data Prefetching. </title> <type> Master's thesis, </type> <institution> Center for Supercomputing Research and Development, </institution> <type> Technical Report 949, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Software prefetching within loops can be achieved by pulling a global memory fetch back one or more iterations, so that the fetch can be initiated before the associated data is needed <ref> [Gor89, Por89] </ref>. Our priority-assigning algorithm can be extended to support such data prefetching by treating the point of prefetching as the source of a candidate dependence, and the point of use as its sink.
Reference: [GV91] <author> Elana D. Granston and Alexander V. Veidenbaum. </author> <title> Detecting Redundant Accesses to Array Data. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 854-865, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> This implies a candidate dependence must not be carried by a surrounding doall loop (if any), or by any loop that encloses this doall loop. Data flow analysis such as that described in <ref> [GV91] </ref> can be used to determine whether dependences involving array references meet these first two conditions. Conditions (iii) and (iv) can be met by restricting ourselves to constant dependences.
Reference: [HS78] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, Inc., </publisher> <year> 1978. </year>
Reference-contexts: The amount of high priority data is equivalent to the joint reference window of ffi and the set of dependences selected thus far. Assuming a bound on both the PDC size and the components of the distance vectors, the optimal solution to this problem is N P -complete <ref> [HS78] </ref>. A simple heuristic algorithm for selecting dependences is presented in Figure 10. 5.3 Step 3: Mark Memory Access Instructions After the final selection of high priority dependences has been made, memory access instructions must be augmented to affect these decisions. Consider the case of dependences involving scalar references first.
Reference: [HS90] <author> Ben Heggy and Mary Lou Soffa. </author> <title> Architectural Support for Register Allocation in the Presence of Aliasing. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 730-739, </pages> <year> 1990. </year>
Reference-contexts: Consequently, for this class of applications, traditional hardware resolution schemes are less effective. Several integrated hardware/software strategies have previously been applied to problems of optimizing data accesses in memory hierarchies. <ref> [Chi89, HS90] </ref> propose hardware-assisted solutions to the problem of register allocation in the presence of aliased variables. The addresses associated with register values are stored in associative caches so that data accesses can be checked against them and redirected to the appropriate registers, as necessary.
Reference: [KK91] <author> Apostolos D. Kallis and David Klappholz. </author> <title> Reaching Definitions Analysis on Code Containing Array References. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Hence, in general, this bound should provide a sufficient estimate to the actual window size. When necessary, however, a much tighter upper bound on k W (ffi) k can be obtained by applying more precise methods, such as those covered in <ref> [EJWB90, FST91, GJG88b, KK91, TEW] </ref>. <p> windows can be found 21 input: C candidate dependences identified in Step 1 output: F final selection of high priority dependences F while C 6= choose ffi 2 C with shortest reaccess time C C fffig if k W ( F [ fffig) k &lt; threshold T then F F <ref> [ fffig end while in [FST91, KK91, TEW] </ref>. The computation of reference windows becomes more complex when symbolic loop bounds are involved. This case can be handled by estimating loop bounds, generating multiple-version loops, or simply ignoring these cases. <p> input: C candidate dependences identified in Step 1 output: F final selection of high priority dependences F while C 6= choose ffi 2 C with shortest reaccess time C C fffig if k W ( F [ fffig) k &lt; threshold T then F F [ fffig end while in <ref> [FST91, KK91, TEW] </ref>. The computation of reference windows becomes more complex when symbolic loop bounds are involved. This case can be handled by estimating loop bounds, generating multiple-version loops, or simply ignoring these cases.
Reference: [Lee87] <author> Roland Lun Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Center for Supercomputing Research and Development, </institution> <type> Technical Report 670, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: If a block is requested as a result of a vector access, the stride of the vector request is used to determine which additional blocks to fetch. The hardware controller can also look ahead a few instructions, and prefetch any data the processor will need <ref> [Lee87] </ref>. In a simple look-ahead scheme, the prefetch mechanism is blocked when a branch instruction is encountered [Bre87]. A more sophisticated mechanism may either guess which branch may be taken or prefetch along two or more branches [SDV + 87, GGH91].
Reference: [LMY88] <author> David J. Lilja, David Marcovitz, and Pen-Chung Yew. </author> <title> Memory Referencing Behavior and a Cache Performance Metric in a Shared Memory Multiprocessor. </title> <type> Technical Report 836, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: Due to the compile-time difficulties in detecting profitable opportunities to reuse data, many vector and parallel machines rely primarily on hardware-controlled cache memory to automatically exploit opportunities for reusing array data. Since memory reference patterns are more variable in vector and parallel programs <ref> [FP91, LMY88] </ref>, it is difficult to block programs [LRW91, FST91] or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. <p> Moreover, the applications that are run on high-performance systems typically consist of large data sets that may exceed cache size and cause thrashing. Multiprocessor systems are also plagued by coherence problems that can further limit cache effectiveness, especially when large block sizes are employed <ref> [LMY88, SD88] </ref>. Enlisting conservative cache coherence algorithms and utilizing small block sizes can limit opportunities to capitalize on existing spatial and temporal locality. Consequently, for this class of applications, traditional hardware resolution schemes are less effective.
Reference: [LRW91] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance of Blocked Algorithms. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Since memory reference patterns are more variable in vector and parallel programs [FP91, LMY88], it is difficult to block programs <ref> [LRW91, FST91] </ref> or tune caches [CV91] so that cache memory can be used effectively over a wide range of applications. Moreover, the applications that are run on high-performance systems typically consist of large data sets that may exceed cache size and cause thrashing.
Reference: [MJ91] <author> J. D. Martens and D. N. Jayasimha. </author> <title> Compiling Loops for Hierarchical Memory Multiprocessors. </title> <type> Technical Report OSU-CISRC-7/91-TR16, </type> <institution> Computer and Information Science Research Center, Ohio State University, </institution> <year> 1991. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems.
Reference: [Por89] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, Technical Report Rice COMP TR89-93, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Software prefetching within loops can be achieved by pulling a global memory fetch back one or more iterations, so that the fetch can be initiated before the associated data is needed <ref> [Gor89, Por89] </ref>. Our priority-assigning algorithm can be extended to support such data prefetching by treating the point of prefetching as the source of a candidate dependence, and the point of use as its sink. <p> A simple compile-time strategy for accomplishing this has been detailed. To maximize the number of promising compile-time reuse opportunities, locality enhancing program transformations, such as those proposed in <ref> [AK86, Wol87, Por89, Wol89, FST91, WL91] </ref> should be applied beforehand. Qualitative arguments have been presented regarding the benefits of our integrated hardware/software local storage management strategy over existing techniques.
Reference: [SD88] <author> C. Scheurich and M. Dubois. </author> <title> The Design of Loackup-Free Cache for High-Performance Multiprocessors. </title> <booktitle> In Supercomputing '88, </booktitle> <pages> pages 352-359, </pages> <month> November </month> <year> 1988. </year> <month> 31 </month>
Reference-contexts: Moreover, the applications that are run on high-performance systems typically consist of large data sets that may exceed cache size and cause thrashing. Multiprocessor systems are also plagued by coherence problems that can further limit cache effectiveness, especially when large block sizes are employed <ref> [LMY88, SD88] </ref>. Enlisting conservative cache coherence algorithms and utilizing small block sizes can limit opportunities to capitalize on existing spatial and temporal locality. Consequently, for this class of applications, traditional hardware resolution schemes are less effective.
Reference: [SDV + 87] <author> J. E. Smith, G. E. Dermer, B. D. Vanderwarn, S. D. Klinger, C. M. Rozewski, D. L. Fowler, K. R. Scidmore, and J. P. Laudon. </author> <title> The ZS-1 Central Processor. </title> <booktitle> In Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 199-204, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: In a simple look-ahead scheme, the prefetch mechanism is blocked when a branch instruction is encountered [Bre87]. A more sophisticated mechanism may either guess which branch may be taken or prefetch along two or more branches <ref> [SDV + 87, GGH91] </ref>. To prevent the displacement of data that will be reused by prefetched data that may not be reused, only conservatively prefetched data should be assigned high priority at the point of prefetching.
Reference: [TEW] <author> O. Temam, C. Eisenbeis, and H. Wijshoff. </author> <title> Characterizing the Solutions of Linear Diophantine Equations. </title> <note> To be published. </note>
Reference-contexts: Hence, in general, this bound should provide a sufficient estimate to the actual window size. When necessary, however, a much tighter upper bound on k W (ffi) k can be obtained by applying more precise methods, such as those covered in <ref> [EJWB90, FST91, GJG88b, KK91, TEW] </ref>. <p> windows can be found 21 input: C candidate dependences identified in Step 1 output: F final selection of high priority dependences F while C 6= choose ffi 2 C with shortest reaccess time C C fffig if k W ( F [ fffig) k &lt; threshold T then F F <ref> [ fffig end while in [FST91, KK91, TEW] </ref>. The computation of reference windows becomes more complex when symbolic loop bounds are involved. This case can be handled by estimating loop bounds, generating multiple-version loops, or simply ignoring these cases. <p> input: C candidate dependences identified in Step 1 output: F final selection of high priority dependences F while C 6= choose ffi 2 C with shortest reaccess time C C fffig if k W ( F [ fffig) k &lt; threshold T then F F [ fffig end while in <ref> [FST91, KK91, TEW] </ref>. The computation of reference windows becomes more complex when symbolic loop bounds are involved. This case can be handled by estimating loop bounds, generating multiple-version loops, or simply ignoring these cases.
Reference: [WL91] <author> Michael Wolf and Monica Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26(6), </volume> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> A simple compile-time strategy for accomplishing this has been detailed. To maximize the number of promising compile-time reuse opportunities, locality enhancing program transformations, such as those proposed in <ref> [AK86, Wol87, Por89, Wol89, FST91, WL91] </ref> should be applied beforehand. Qualitative arguments have been presented regarding the benefits of our integrated hardware/software local storage management strategy over existing techniques.
Reference: [Wol87] <author> Michael Joseph Wolfe. </author> <title> Iteration Space Tiling for Memory Hierarchies. </title> <booktitle> In Proceedings of the 3rd SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 357-361, </pages> <year> 1987. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> A simple compile-time strategy for accomplishing this has been detailed. To maximize the number of promising compile-time reuse opportunities, locality enhancing program transformations, such as those proposed in <ref> [AK86, Wol87, Por89, Wol89, FST91, WL91] </ref> should be applied beforehand. Qualitative arguments have been presented regarding the benefits of our integrated hardware/software local storage management strategy over existing techniques.
Reference: [Wol89] <author> Michael Joseph Wolfe. </author> <title> More Iteration Space Tiling. </title> <booktitle> In Supercomputing '89, </booktitle> <month> November </month> <year> 1989. </year>
Reference-contexts: In numerical applications, a significant portion of a program's memory accesses arises from references to array data. Allocating array data to registers [AK86, BJEW91, CCK90, Chi89] or local memory <ref> [EJWB90, GJG88a, GJG88b, GV91, MJ91, Wol87, Wol89, WL91] </ref> is significantly more com 2 plex than allocating scalar data, especially in the presence of non-linear and subscripted subscripts, symbolic terms, and aliasing problems. <p> A simple compile-time strategy for accomplishing this has been detailed. To maximize the number of promising compile-time reuse opportunities, locality enhancing program transformations, such as those proposed in <ref> [AK86, Wol87, Por89, Wol89, FST91, WL91] </ref> should be applied beforehand. Qualitative arguments have been presented regarding the benefits of our integrated hardware/software local storage management strategy over existing techniques.
References-found: 31

