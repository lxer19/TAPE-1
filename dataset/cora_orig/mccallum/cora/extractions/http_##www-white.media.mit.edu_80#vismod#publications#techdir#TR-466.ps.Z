URL: http://www-white.media.mit.edu:80/vismod/publications/techdir/TR-466.ps.Z
Refering-URL: http://www-white.media.mit.edu/~testarne/asl/index.html
Root-URL: http://www.media.mit.edu
Email: thad,joshw,sandy@media.mit.edu  
Title: Sign Language Recognition Using Desk and Wearable Computer Based Video  
Author: Thad Starner, Joshua Weaver, and Alex Pentland 
Keyword: Categories: Gesture Recognition, Hidden Markov Models, Wearable Computers, Sign Language, Motion and Pattern Analysis.  
Address: 20 Ames Street, Cambridge MA 02139  
Affiliation: Room E15-383, The Media Laboratory Massachusetts Institute of Technology  
Note: Real-Time American  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 466 To appear IEEE PAMI '98; Submitted 4/26/96 Abstract We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American Sign Language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92% word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98% accuracy (97% with an unrestricted grammar). Both experiments use a 40 word lexicon. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: Note that finger position plays an important role in several of the signs (pack vs. car, food vs. pill, red vs. mouse, etc.) 2 Hidden Markov Modeling Due to space limitations, the reader is encouraged to refer to the existing literature on HMM evaluation, estimation, and decoding <ref> [1, 6, 11, 23] </ref>. A tutorial relating HMM's to sign language recognition is provided in the first author's Master's thesis [15]. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically.
Reference: [2] <author> L. Campbell, D. Becker, A. Azarbayejani, A. Bo-bick, and A. Pentland. </author> <title> Invariant features for 3-d gesture recognition. </title> <booktitle> In Second Intl. Conf. on Face and Gesture Recogn., </booktitle> <pages> pages 157-162, </pages> <year> 1996. </year>
Reference-contexts: Results were not reported with the standard accuracy measures accepted in the speech and handwriting recognition communities, and training and testing databases were often identical or dependent in some manner. Since this time, HMM-based gesture recognizers for other tasks have appeared in the literature <ref> [21, 2] </ref>, and, last year, several HMM-based continuous sign lan 1 guage systems were demonstrated. In a submission to UIST'97, Liang and Ouhyoung's work in Taiwanese Sign Language [8] shows very encouraging results with a glove-based recognizer. This HMM-based system recognizes 51 postures, 8 orientations, and 8 motion primitives.
Reference: [3] <author> B. Dorner. </author> <title> Hand shape identification and tracking for sign language interpretation. </title> <booktitle> In IJCAI Workshop on Looking at People, </booktitle> <year> 1993. </year>
Reference-contexts: several different topologies, a four state HMM with one skip transition was determined to be appropriate for this task (Figure 1). 3 Feature extraction and hand ambiguity Previous systems have shown that, given strong constraints on viewing, relatively detailed models of the 2 hands can be recovered from video images <ref> [3, 12] </ref>. How--ever, many of these constraints conflict with recognizing ASL in a natural context, since they either require simple, unchanging backgrounds (unlike clothing); do not allow occlusion; require carefully labelled gloves; or are difficult to run in real time.
Reference: [4] <author> I. Essa, T. Darrell, and A. Pentland. </author> <title> Tracking facial motion. </title> <booktitle> In Proc. of the Workshop on Motion of Non-Rigid and Articulated Objects, </booktitle> <address> Austin, Texas, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive. While we have also built systems that track facial features <ref> [4, 9] </ref>, this source of information will not be used to aid recognition in the task addressed here. 1.1 Related Work Following a similar path to early speech recognition, many previous attempts at machine sign language recognition concentrate on isolated signs or fingerspelling.
Reference: [5] <author> B. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: A sixteen element feature vector is constructed from each hand's x and y position, change in x and y between frames, area (in pixels), angle of axis of least inertia (found by the first eigenvector of the blob) <ref> [5] </ref>, length of this eigenvector, and eccentricity of bounding ellipse. When tracking skin tones, the above analysis helps to model situations of hand ambiguity implicitly.
Reference: [6] <author> X.D. Huang, Y. Ariki, and M. A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Explicit segmentation on the word level is not necessary for either training or recognition. Language and context models can be applied on several different levels, and much related development of this technology has been done by the speech recognition community <ref> [6] </ref>. When the authors first reported this project in 1995 [15, 18], very few uses of HMM's were found in the computer vision literature [22, 13]. <p> Note that finger position plays an important role in several of the signs (pack vs. car, food vs. pill, red vs. mouse, etc.) 2 Hidden Markov Modeling Due to space limitations, the reader is encouraged to refer to the existing literature on HMM evaluation, estimation, and decoding <ref> [1, 6, 11, 23] </ref>. A tutorial relating HMM's to sign language recognition is provided in the first author's Master's thesis [15]. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically.
Reference: [7] <author> T. Humphries, C. Padden, and T. O'Rourke. </author> <title> A Basic Course in American Sign Language. </title> <editor> T. J. </editor> <publisher> Publ., Inc., </publisher> <address> Silver Spring, MD, </address> <year> 1990. </year>
Reference-contexts: Six personal pronouns, nine verbs, twenty nouns, and five adjectives are included for a total lexicon of forty words. The words were chosen by paging through Humphries et al. <ref> [7] </ref> and selecting those words which would generate coherent sentences given the grammar constraint.
Reference: [8] <author> R. Liang and M. Ouhyoung. </author> <title> A real-time continuous gesture interface for Taiwanese Sign Language. </title> <note> In Submitted to UIST, </note> <year> 1997. </year>
Reference-contexts: Since this time, HMM-based gesture recognizers for other tasks have appeared in the literature [21, 2], and, last year, several HMM-based continuous sign lan 1 guage systems were demonstrated. In a submission to UIST'97, Liang and Ouhyoung's work in Taiwanese Sign Language <ref> [8] </ref> shows very encouraging results with a glove-based recognizer. This HMM-based system recognizes 51 postures, 8 orientations, and 8 motion primitives. When combined, these constituents can form a lexicon of 250 words which can be continuously recognized in real-time with 90.5% accuracy.
Reference: [9] <author> R. </author> <title> Picard. Toward agents that recognize emotion. </title> <booktitle> In Imagina98, </booktitle> <year> 1998. </year>
Reference-contexts: For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive. While we have also built systems that track facial features <ref> [4, 9] </ref>, this source of information will not be used to aid recognition in the task addressed here. 1.1 Related Work Following a similar path to early speech recognition, many previous attempts at machine sign language recognition concentrate on isolated signs or fingerspelling. <p> Another option for the wearable system is to add inertial sensors to compensate for head motion. In addition, EMG's may be placed in the cap's head band along the forehead to analyze eyebrow motion as has been discussed by Picard <ref> [9] </ref>. In this way facial gesture information may be recovered. As the system grows in lexicon size, finger and palm tracking information may be added.
Reference: [10] <author> H. Poizner, U. Bellugi, and V. Lutes-Driscoll. </author> <title> Perception of American Sign Language in dynamic point-light displays. </title> <editor> J. Exp. Pyschol.: </editor> <booktitle> Human Perform., </booktitle> <volume> 7 </volume> <pages> 430-440, </pages> <year> 1981. </year>
Reference-contexts: The tracking stage of the system does not attempt a fine description of hand shape, instead concentrating on the evolution of the gesture through time. Studies of human sign readers suggest that surprisingly little hand detail is necessary for humans to interpret sign language <ref> [10, 14] </ref>.
Reference: [11] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Note that finger position plays an important role in several of the signs (pack vs. car, food vs. pill, red vs. mouse, etc.) 2 Hidden Markov Modeling Due to space limitations, the reader is encouraged to refer to the existing literature on HMM evaluation, estimation, and decoding <ref> [1, 6, 11, 23] </ref>. A tutorial relating HMM's to sign language recognition is provided in the first author's Master's thesis [15]. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically.
Reference: [12] <author> J. M. Rehg and T. Kanade. DigitEyes: </author> <title> vision-based human hand tracking. </title> <institution> School of Computer Science Technical Report CMU-CS-93-220, Carnegie Mel-lon University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: several different topologies, a four state HMM with one skip transition was determined to be appropriate for this task (Figure 1). 3 Feature extraction and hand ambiguity Previous systems have shown that, given strong constraints on viewing, relatively detailed models of the 2 hands can be recovered from video images <ref> [3, 12] </ref>. How--ever, many of these constraints conflict with recognizing ASL in a natural context, since they either require simple, unchanging backgrounds (unlike clothing); do not allow occlusion; require carefully labelled gloves; or are difficult to run in real time.
Reference: [13] <author> J. Schlenzig, E. Hunter, and R. Jain. </author> <title> Recursive identification of gesture inputs using hidden Markov models. </title> <booktitle> Proc. Second Annual Conference on Applications of Computer Vision, </booktitle> <pages> pages 187-194, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: When the authors first reported this project in 1995 [15, 18], very few uses of HMM's were found in the computer vision literature <ref> [22, 13] </ref>. At the time, continuous density HMM's were beginning to appear in the speech community; continuous gesture recognition was scarce; gesture lexicons were very small; and automatic training through Baum-Welch re-estimation was uncommon.
Reference: [14] <author> G. Sperling, M. Landy, Y. Cohen, and M. Pavel. </author> <title> Intelligible encoding of ASL image sequences at extremely low information rates. Comp. Vis., Graph., </title> <journal> and Img. Proc., </journal> <volume> 31 </volume> <pages> 335-391, </pages> <year> 1985. </year>
Reference-contexts: Another variant, Signed Exact English (SEE), has more in common with spoken English but is not as widespread in America. Conversants in ASL may describe a person, place, or thing and then point to a place in space to store that object temporarily for later reference <ref> [14] </ref>. For the purposes of this experiment, this aspect of ASL will be ignored. Furthermore, in ASL the eyebrows are raised for a question, relaxed for a statement, and furrowed for a directive. <p> The tracking stage of the system does not attempt a fine description of hand shape, instead concentrating on the evolution of the gesture through time. Studies of human sign readers suggest that surprisingly little hand detail is necessary for humans to interpret sign language <ref> [10, 14] </ref>. <p> Studies of human sign readers suggest that surprisingly little hand detail is necessary for humans to interpret sign language [10, 14]. In fact, in movies shot from the waist up of isolated signs, Sper-ling et al. <ref> [14] </ref> show that the movies retain 85% of their full resolution intelligibility when subsampled to 24 by 16 pixels! For this experiment, the tracking process produces only a coarse description of hand shape, orientation, and trajectory. The resulting information is input to a HMM for recognition of the signed words. <p> For both systems, color NTSC composite video is captured and analyzed at 320 by 243 pixel resolution. This lower resolution avoids video interlace effects. A Silicon Graphics 200MHz R4400 Indy workstation maintains hand tracking at 10 frames per second, a frame rate which Sperling et al. <ref> [14] </ref> found sufficient for human recognition. To segment each hand initially, the algorithm scans the image until it finds a pixel of the appropriate color, determined by an a priori model of skin color.
Reference: [15] <author> T. Starner. </author> <title> Visual recognition of American Sign Language using hidden Markov models. </title> <type> Master's thesis, </type> <institution> MIT, Media Laboratory, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: Language and context models can be applied on several different levels, and much related development of this technology has been done by the speech recognition community [6]. When the authors first reported this project in 1995 <ref> [15, 18] </ref>, very few uses of HMM's were found in the computer vision literature [22, 13]. At the time, continuous density HMM's were beginning to appear in the speech community; continuous gesture recognition was scarce; gesture lexicons were very small; and automatic training through Baum-Welch re-estimation was uncommon. <p> A tutorial relating HMM's to sign language recognition is provided in the first author's Master's thesis <ref> [15] </ref>. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically. In this case, an initial topology of 5 states was considered sufficient for the most complex sign.
Reference: [16] <author> T. Starner, J. Makhoul, R. Schwartz, and G. Chou. </author> <title> On-line cursive handwriting recognition using speech recognition methods. </title> <booktitle> In ICASSP, </booktitle> <pages> pages 125-128, </pages> <year> 1994. </year>
Reference-contexts: In addition, tri-sign context models and statistical grammars may be added which may reduce error up to a factor of eight if speech and handwriting trends hold true for sign <ref> [16] </ref>. These improvements do not address user independence. Just as in speech, making a system which can understand different subjects with their own variations of language involves collecting data from many subjects.
Reference: [17] <author> T. Starner, S. Mann, B. Rhodes, J. Levine, J. Healey, D. Kirsch, R. Picard, and A. Pentland. </author> <title> Augmented reality through wearable computing. </title> <journal> Presence, </journal> <volume> 6(4) </volume> <pages> 386-398, </pages> <month> Winter </month> <year> 1997. </year>
Reference-contexts: The brim can be made into a relatively good quality speaker by lining it with a PVDF transducer (used in thin consumer-grade stereo speakers). Finally a PC/104-based CPU, digitizer, and batteries can be placed at the back of the head. See Starner et al. <ref> [17] </ref> and the MIT Wearable Computing Site (http://wearables.www.media.mit.edu /projects/wearables/) for more detailed information about wearable computing and related technologies. towards the hands, and the corresponding view. A wearable computer system provides the greatest utility for an ASL to spoken English translator.
Reference: [18] <author> T. Starner and A. Pentland. </author> <title> Real-time American Sign Language recognition from video using hidden Markov models. </title> <type> Technical Report 375, </type> <institution> MIT Media Lab, Perceptual Computing Group, </institution> <year> 1995. </year> <note> Earlier version appeared ISCV'95. </note>
Reference-contexts: Language and context models can be applied on several different levels, and much related development of this technology has been done by the speech recognition community [6]. When the authors first reported this project in 1995 <ref> [15, 18] </ref>, very few uses of HMM's were found in the computer vision literature [22, 13]. At the time, continuous density HMM's were beginning to appear in the speech community; continuous gesture recognition was scarce; gesture lexicons were very small; and automatic training through Baum-Welch re-estimation was uncommon.
Reference: [19] <author> T. Starner, J. Weaver, and A. Pentland. </author> <title> Real-time american sign language recognition using desktop and wearable computer based video. </title> <type> Technical Report 466, </type> <institution> Perceptual Computing, MIT Media Laboratory, </institution> <month> July </month> <year> 1998. </year>
Reference-contexts: Space does not permit a thorough review <ref> [19] </ref>, but, in general, most attempts either relied on instrumented gloves or a desktop-based camera system and used a form of template matching or neural nets for recognition. However, current extensible systems are beginning to employ hidden Markov models (HMM's).
Reference: [20] <author> C. Vogler and D. Metaxas. </author> <title> ASL recognition based on a coupling between HMMs and 3D motion analysis. </title> <booktitle> In ICCV, Bombay, </booktitle> <year> 1998. </year>
Reference-contexts: When combined, these constituents can form a lexicon of 250 words which can be continuously recognized in real-time with 90.5% accuracy. At ICCV'98, Vogler and Metaxas described a desk-based 3D camera system that achieves 89.9% word accuracy on a 53 word lexicon <ref> [20] </ref>.
Reference: [21] <author> A. D. Wilson and A. F. Bobick. </author> <title> Learning visual behavior for gesture analysis. </title> <booktitle> In Proc. IEEE Int'l. Symp. on Comp. Vis., Coral Gables, </booktitle> <address> Florida, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Results were not reported with the standard accuracy measures accepted in the speech and handwriting recognition communities, and training and testing databases were often identical or dependent in some manner. Since this time, HMM-based gesture recognizers for other tasks have appeared in the literature <ref> [21, 2] </ref>, and, last year, several HMM-based continuous sign lan 1 guage systems were demonstrated. In a submission to UIST'97, Liang and Ouhyoung's work in Taiwanese Sign Language [8] shows very encouraging results with a glove-based recognizer. This HMM-based system recognizes 51 postures, 8 orientations, and 8 motion primitives.
Reference: [22] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> Recognizing human action in time-sequential images using hidden Markov models. </title> <booktitle> Proc. Comp. Vis. and Pattern Rec., </booktitle> <pages> pages 379-385, </pages> <year> 1992. </year>
Reference-contexts: When the authors first reported this project in 1995 [15, 18], very few uses of HMM's were found in the computer vision literature <ref> [22, 13] </ref>. At the time, continuous density HMM's were beginning to appear in the speech community; continuous gesture recognition was scarce; gesture lexicons were very small; and automatic training through Baum-Welch re-estimation was uncommon.
Reference: [23] <author> S. Young. </author> <title> HTK: Hidden Markov Model Toolkit V1.5. </title> <institution> Cambridge Univ. Eng. Dept. Speech Group and Entropic Research Lab. Inc., </institution> <address> Washington DC, </address> <year> 1993. </year>
Reference-contexts: Note that finger position plays an important role in several of the signs (pack vs. car, food vs. pill, red vs. mouse, etc.) 2 Hidden Markov Modeling Due to space limitations, the reader is encouraged to refer to the existing literature on HMM evaluation, estimation, and decoding <ref> [1, 6, 11, 23] </ref>. A tutorial relating HMM's to sign language recognition is provided in the first author's Master's thesis [15]. The initial topology for an HMM can be determined by estimating how many different states are involved in specifying a sign. Fine tuning this topology can be performed empirically.
References-found: 23

