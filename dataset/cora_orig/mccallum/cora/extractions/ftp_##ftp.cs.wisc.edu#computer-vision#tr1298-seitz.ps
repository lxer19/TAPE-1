URL: ftp://ftp.cs.wisc.edu/computer-vision/tr1298-seitz.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Email: seitz@cs.wisc.edu dyer@cs.wisc.edu  
Title: Toward Image-Based Scene Representation Using View Morphing  
Author: Steven M. Seitz Charles R. Dyer 
Note: To appear in Proc. Intl. Conf. on Pattern Recognition (ICPR 96), Vienna 1996. The support of the National Science Foundation under Grant Nos. IRI-9220782 and CDA-9222948 is gratefully acknowl edged.  
Date: May 1996  
Address: Madison, WI 53706  
Affiliation: Department of Computer Sciences University of Wisconsin  
Pubnum: Technical Report #1298  
Abstract: The question of which views may be inferred from a set of basis images is addressed. Under certain conditions, a discrete set of images implicitly describes scene appearance for a continuous range of viewpoints. In particular, it is demonstrated that two basis views of a static scene determine the set of all views on the line between their optical centers. Additional basis views further extend the range of predictable views to a two- or three-dimensional region of viewspace. These results are shown to apply under perspective projection subject to a generic visibility constraint called monotonicity. In addition, a simple scanline algorithm is presented for actually generating these views from a set of basis images. The technique, called view morphing may be applied to both calibrated and uncalibrated images. At a minimum, two basis views and their fundamental matrix are needed. Experimental results are presented on real images. This work provides a theoretical foundation for image-based representations of 3D scenes by demonstrating that perspective view synthesis is a theoretically well-posed problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. H. Baker and T. O. Binford. </author> <title> Depth from edge and intensity based stereo. </title> <booktitle> In Proc. 7th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 631-636, </pages> <year> 1981. </year>
Reference-contexts: The monotonicity constraint dictates that all visible scene points appear in the same order along conjugate epipolar lines of I 0 and I 1 . This constraint is used commonly in stereo matching <ref> [1, 19] </ref> because the fixed relative ordering of points along epipolar lines simplifies the correspondence problem. Despite its usual definition with respect to epipolar lines and images, monotonicity constrains only the location of the optical centers with respect to points in the scene|the image planes may be chosen arbitrarily. <p> Notice, however, that the interpolation argument does not seem to depend on the condition that s 2 <ref> [0; 1] </ref>, suggesting that additional views could be synthesized by choosing other values of s. Certainly, choosing any value of s 2 &lt; will result in a new image. This image will represent a valid view of the scene only if monotonicity is preserved for C s . <p> Certainly, choosing any value of s 2 &lt; will result in a new image. This image will represent a valid view of the scene only if monotonicity is preserved for C s . In short, for values of s 2 <ref> [0; 1] </ref> the resulting image is guaranteed to be valid.
Reference: [2] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. 1431, </volume> <publisher> M.I.T., </publisher> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed.
Reference: [3] <author> S. E. Chen. </author> <title> Quicktime VR | An image-based approach to virtual environment navigation. </title> <booktitle> In Proc. </booktitle> <volume> SIG-GRAPH 95, </volume> <pages> pages 29-38, </pages> <year> 1995. </year> <month> 15 </month>
Reference-contexts: 1 Introduction Image-based representations of 3D scenes are currently being developed by many researchers in the computer vision and computer graphics communities (see, for example, <ref> [12, 17, 26, 3, 11, 28] </ref>). These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks [27, 18].
Reference: [4] <author> S. E. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Proc. SIGGRAPH 93, </booktitle> <pages> pages 279-288, </pages> <year> 1993. </year>
Reference-contexts: It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed. <p> Alternatively, we could produce the same two images by moving the camera instead of the object. Chen and Williams <ref> [4] </ref> previously considered this special case, arguing that linear image interpolation should produce new perspective views when the camera moves parallel to the image plane.
Reference: [5] <author> P. E. Debevec, C. J. Taylor, and J. Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In this example, image correspondences were automatically determined using a dynamic programming technique [19] that exploits monotonicity. Even with the monotonicity constraint, obtaining reliable correspondences with large baselines is a formidable challenge. However, incorporating limited user interaction [24] or domain knowledge <ref> [5] </ref> can significantly improve the results and is a promising line of future research. As in the previous example, some artifacts occur where monotonicity is violated, such as near the left foot and the left thigh. Also, the synthesized view is noticeably more blurry than the basis views.
Reference: [6] <author> O. Faugeras. </author> <title> Three-Dimensional Computer Vision, A Geometric Viewpoint. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: In between perspective views on the line C 0 C 1 may be synthesized by first applying homographies H 1 1 to convert I 0 and I 1 to a parallel configuration. This procedure is identical to rectification techniques used in stereo vision <ref> [6] </ref>. Given a projection matrix s = [H s j H s C s ], with C s fixed by Eq. (4), the following sequence of operations, depicted in Fig. 4, produces an image I s corresponding to a view with projection matrix s : 1. <p> This is precisely the rectification process needed to interpolate orthographic images in [22]. As a result, the three-step algorithm reduces to the version described in [22] in the orthographic case. Other methods of rectification for uncalibrated images are explored in <ref> [21, 6] </ref>. 11 triangle. If monotonicity is satisfied pairwise, any view along an edge may be synthesized. If strong monotonicity applies, views in the interior may be synthesized as well. For example, a view I with optical center C is synthesized by interpolating the second and third basis views.
Reference: [7] <author> R. I. </author> <title> Hartley. In defence of the 8-point algorithm. </title> <booktitle> In Proc. Fifth Intl. Conference on Computer Vision, </booktitle> <pages> pages 1064-1070, </pages> <year> 1995. </year>
Reference-contexts: F is defined up to a scale factor and can be computed from the images themselves when at least 8 point correspondences are known (see <ref> [14, 7] </ref> for methods of computing F from point correspondences). 4.1 Rectifying Uncalibrated Images In order to use the three-step algorithm presented in Section 3, we must find a way to rectify the images without knowing the projection matrices.
Reference: [8] <author> A. Katayama, K. Tanaka, T. Oshino, and H. Tamura. </author> <title> A viewpoint dependent stereoscopic display using interpolation of multi-viewpoint images. </title> <booktitle> In Proc. SPIE Vol. 2409A, </booktitle> <pages> pages 21-30, </pages> <year> 1995. </year>
Reference-contexts: It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed.
Reference: [9] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The singularities of the visual mapping. </title> <journal> Biological Cybernetics, </journal> <volume> 24 </volume> <pages> 51-59, </pages> <year> 1976. </year>
Reference-contexts: This result demonstrates that view synthesis under monotonicity is an inherently well-posed problem|and is therefore much easier than 3D reconstruction and related motion analysis tasks requiring smoothness conditions and regularization techniques [20]. The monotonicity constraint is closely related to aspect graphs and visual events <ref> [9, 10] </ref>. The constraint dictates that no changes in visibility may occur within a monotonic range of viewspace. In other words, 4 conjugate epipolar lines of three images.
Reference: [10] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: This result demonstrates that view synthesis under monotonicity is an inherently well-posed problem|and is therefore much easier than 3D reconstruction and related motion analysis tasks requiring smoothness conditions and regularization techniques [20]. The monotonicity constraint is closely related to aspect graphs and visual events <ref> [9, 10] </ref>. The constraint dictates that no changes in visibility may occur within a monotonic range of viewspace. In other words, 4 conjugate epipolar lines of three images.
Reference: [11] <author> R. Kumar, P. Anandan, M. Irani, J. Bergen, and K. Hanna. </author> <title> Representation of scenes from collections of images. </title> <booktitle> In Proc. IEEE Workshop on Representations of Visual Scenes, </booktitle> <pages> pages 10-17, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Image-based representations of 3D scenes are currently being developed by many researchers in the computer vision and computer graphics communities (see, for example, <ref> [12, 17, 26, 3, 11, 28] </ref>). These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks [27, 18].
Reference: [12] <author> S. Laveau and O. Faugeras. </author> <title> 3-D scene representation as a collection of images. </title> <booktitle> In Proc. International Conference on Pattern Recognition, </booktitle> <pages> pages 689-691, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Image-based representations of 3D scenes are currently being developed by many researchers in the computer vision and computer graphics communities (see, for example, <ref> [12, 17, 26, 3, 11, 28] </ref>). These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks [27, 18]. <p> It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed. <p> An alternate solution is to have the user provide the homography directly or indirectly by specification of a small number of image points <ref> [12, 24] </ref>. All three methods for choosing the postwarp transforms generally result in the synthesis of projective views. A projective view is a perspective view warped by a 2D affine transformation.
Reference: [13] <author> H. C. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference-contexts: In this section we consider the case where only the basis images and the fundamental matrix are provided. The fundamental matrix of two views is the 3 fi 3, rank-two matrix F satisfying the following relation <ref> [13, 14] </ref>: p T for any pair of points p 0 and p 1 in the two images corresponding to the same scene point.
Reference: [14] <author> Q.-T. Luong and O. Faugeras. </author> <title> The fundamental matrix: Theory, algorithms, and stability analysis. </title> <journal> Intl. Journal of Computer Vision, </journal> <volume> 17(1) </volume> <pages> 43-75, </pages> <year> 1996. </year>
Reference-contexts: In this section we consider the case where only the basis images and the fundamental matrix are provided. The fundamental matrix of two views is the 3 fi 3, rank-two matrix F satisfying the following relation <ref> [13, 14] </ref>: p T for any pair of points p 0 and p 1 in the two images corresponding to the same scene point. <p> F is defined up to a scale factor and can be computed from the images themselves when at least 8 point correspondences are known (see <ref> [14, 7] </ref> for methods of computing F from point correspondences). 4.1 Rectifying Uncalibrated Images In order to use the three-step algorithm presented in Section 3, we must find a way to rectify the images without knowing the projection matrices. <p> The epipoles are the projections of C 1 into I 0 and C 0 into I 1 : e 0 = C 1 (8) Given a vector p = [x y z] T , we introduce the notation [p] fi = 6 0 z y y x 0 7 Following <ref> [14] </ref>, the fundamental matrix may be expressed as F = [e 1 ] fi H 1 If the two views are in canonical configuration, we may assume without loss of generality that H 1 and C 1 are given by Eqs. (6) and (7) respectively, and therefore e 1 = [e
Reference: [15] <author> S. Mann and R. </author> <title> Picard. Virtual bellows: Constructing high-quality images from video. </title> <booktitle> In Proc. First International Conference on Image Processing, </booktitle> <year> 1994. </year>
Reference-contexts: It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed.
Reference: [16] <author> L. McMillan and G. Bishop. </author> <title> Head-tracked stereoscopic display using image warping. </title> <booktitle> In Proc. SPIE Vol. 2409A, </booktitle> <pages> pages 21-30, </pages> <year> 1995. </year>
Reference-contexts: It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed.
Reference: [17] <author> L. McMillan and G. Bishop. </author> <booktitle> Plenoptic modeling. In Proc. SIGGRAPH 95, </booktitle> <pages> pages 39-46, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Image-based representations of 3D scenes are currently being developed by many researchers in the computer vision and computer graphics communities (see, for example, <ref> [12, 17, 26, 3, 11, 28] </ref>). These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks [27, 18].
Reference: [18] <author> H. Murase and S. K. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearance. </title> <journal> Intl. Journal of Computer Vision, </journal> <volume> 14(1) </volume> <pages> 171-183, </pages> <year> 1990. </year>
Reference-contexts: These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks <ref> [27, 18] </ref>. In addition, these representations are readily acquired from a set of basis views, avoiding the need for automatic or manual techniques for acquiring 3D object models.
Reference: [19] <author> Y. Ohta and T. Kanade. </author> <title> Stereo by intra- and inter-scanline search using dynamic programming. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(2) </volume> <pages> 139-154, </pages> <year> 1985. </year>
Reference-contexts: The monotonicity constraint dictates that all visible scene points appear in the same order along conjugate epipolar lines of I 0 and I 1 . This constraint is used commonly in stereo matching <ref> [1, 19] </ref> because the fixed relative ordering of points along epipolar lines simplifies the correspondence problem. Despite its usual definition with respect to epipolar lines and images, monotonicity constrains only the location of the optical centers with respect to points in the scene|the image planes may be chosen arbitrarily. <p> The mannequin is an example of an object for which it is difficult to reconstruct but relatively easy to synthesize views due to lack of texture. In this example, image correspondences were automatically determined using a dynamic programming technique <ref> [19] </ref> that exploits monotonicity. Even with the monotonicity constraint, obtaining reliable correspondences with large baselines is a formidable challenge. However, incorporating limited user interaction [24] or domain knowledge [5] can significantly improve the results and is a promising line of future research.
Reference: [20] <author> T. Poggio, V. Torre, and C. Koch. </author> <title> Computational vision and regularization theory. </title> <journal> Nature, </journal> <volume> 317 </volume> <pages> 314-319, </pages> <year> 1985. </year>
Reference-contexts: This conclusion is especially unfortunate in light of the fact that 3D reconstruction from sparse images is generally ambiguous|a number of different scenes may be consistent with a given set of images; it is an ill-posed problem <ref> [20] </ref>. This suggests that view synthesis is also ill-posed. In this section we present an alternate paradigm for view synthesis that avoids 3D reconstruction and dense correspondence as intermediate steps, instead relying only on measurable quantities, computable 2 from a set of basis images. <p> Hence, all views along C 0 C 1 are determined from I 0 and I 1 . This result demonstrates that view synthesis under monotonicity is an inherently well-posed problem|and is therefore much easier than 3D reconstruction and related motion analysis tasks requiring smoothness conditions and regularization techniques <ref> [20] </ref>. The monotonicity constraint is closely related to aspect graphs and visual events [9, 10]. The constraint dictates that no changes in visibility may occur within a monotonic range of viewspace. In other words, 4 conjugate epipolar lines of three images.
Reference: [21] <author> L. Robert, C. Zeller, O. Faugeras, and M. Hebert. </author> <title> Applications of non-metric vision to some visually guided robotics tasks. </title> <type> Technical Report 2584, </type> <institution> INRIA, Sophia-Antipolis, France, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Although the particular plane can be chosen arbitrarily, certain planes may be more suitable due to image sampling considerations. 8 Methods of choosing the rectification parameters that minimize image distortion with uniform sampling are discussed in <ref> [21] </ref>. By varying s from 0 to 1, the three-step method provides a way of generating any new view on C 0 C 1 |the range of views theoretically determined by the monotonicity assumption. <p> The epipoles e 0 and e 1 span the null spaces of ^ F and ^ F T , respectively <ref> [21] </ref>. In particular, e 0 = [e x 0 0] T for some unknown constant e x . From Eq. (8) it follows that C 1 = [e x 0 0] T . For rectification, it therefore sufficies to transform I 1 to H 1 1 I 1 . <p> This is precisely the rectification process needed to interpolate orthographic images in [22]. As a result, the three-step algorithm reduces to the version described in [22] in the orthographic case. Other methods of rectification for uncalibrated images are explored in <ref> [21, 6] </ref>. 11 triangle. If monotonicity is satisfied pairwise, any view along an edge may be synthesized. If strong monotonicity applies, views in the interior may be synthesized as well. For example, a view I with optical center C is synthesized by interpolating the second and third basis views.
Reference: [22] <author> S. M. Seitz and C. R. Dyer. </author> <title> Physically-valid view synthesis by image interpolation. </title> <booktitle> In Proc. IEEE Workshop on Representations of Visual Scenes, </booktitle> <pages> pages 18-25, </pages> <year> 1995. </year>
Reference-contexts: Furthermore, all processing occurs at the scanline level, effectively reducing the original 3D synthesis problem to a set of simple 1D transformations that may be implemented efficiently on existing graphics workstations. The work presented here extends to perspective projection previous results on the orthographic case <ref> [22] </ref>. In addition, this paper discusses extensions to three or more basis views, an important generalization not considered in [22]. We begin by introducing the monotonicity constraint and describing its implications for view synthesis in Section 2. <p> The work presented here extends to perspective projection previous results on the orthographic case <ref> [22] </ref>. In addition, this paper discusses extensions to three or more basis views, an important generalization not considered in [22]. We begin by introducing the monotonicity constraint and describing its implications for view synthesis in Section 2. <p> An interesting special case is the class of orthographic projections, i.e., projections 0 and 1 whose last row is [0 0 0 1]. Linear interpolation of any two orthographic views of a scene therefore produces a new orthographic view of the same scene <ref> [22] </ref>. 3.2 Non-Parallel Views Using stereo rectification techniques, the problem of computing in-between views from two arbitrary perspective views can be reduced to the case treated in Section 3.1. <p> What follows is a simple technique that rectifies both images with two rotations. A nice property of this method is that the rectification process reduces to that in <ref> [22] </ref> when the images are orthographic. First the rectification plane is chosen indirectly by selecting its intersection with I 0 . In homogeneous coordinates, points in the image at infinity are represented by vectors of the form [x y 0] T . <p> Therefore, 0 and 1 are both zero and the rectification procedure reduces to a 2D rotation of both images and a shear of the second. This is precisely the rectification process needed to interpolate orthographic images in <ref> [22] </ref>. As a result, the three-step algorithm reduces to the version described in [22] in the orthographic case. Other methods of rectification for uncalibrated images are explored in [21, 6]. 11 triangle. If monotonicity is satisfied pairwise, any view along an edge may be synthesized. <p> This is precisely the rectification process needed to interpolate orthographic images in <ref> [22] </ref>. As a result, the three-step algorithm reduces to the version described in [22] in the orthographic case. Other methods of rectification for uncalibrated images are explored in [21, 6]. 11 triangle. If monotonicity is satisfied pairwise, any view along an edge may be synthesized. If strong monotonicity applies, views in the interior may be synthesized as well.
Reference: [23] <author> S. M. Seitz and C. R. Dyer. </author> <title> Scene appearance representation by perspective view synthesis. </title> <type> Technical Report CS-TR-1298, </type> <institution> University of Wisconsin, Madison, WI, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: However, the prediction of any range of view-space depends on the assumption that all possible pairs of views within that space satisfy monotonicity. As noted in Section 2, a monotonic range may span no more than a single aspect of an aspect graph <ref> [23] </ref>, thus limiting the range of views that may be predicted. Nevertheless, it is clear that a discrete set of views implicitly describes scene appearance from a continuous range of viewpoints.
Reference: [24] <author> S. M. Seitz and C. R. Dyer. </author> <title> View morphing. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: An alternate solution is to have the user provide the homography directly or indirectly by specification of a small number of image points <ref> [12, 24] </ref>. All three methods for choosing the postwarp transforms generally result in the synthesis of projective views. A projective view is a perspective view warped by a 2D affine transformation. <p> The basis views appear at left and right and morphed (synthesized) images appear in the center. The morphed images use 2D image transforms to synthesize a 3D scene rotation. 14 feature correspondences was used to determine the correspondence map, using an image morphing tech-nique <ref> [24] </ref>. The synthesized image represents a view from a camera viewpoint halfway between the two basis views. The image gives the convincing impression that the subject has turned his head, despite the fact that only 2D image operations have been performed. <p> In this example, image correspondences were automatically determined using a dynamic programming technique [19] that exploits monotonicity. Even with the monotonicity constraint, obtaining reliable correspondences with large baselines is a formidable challenge. However, incorporating limited user interaction <ref> [24] </ref> or domain knowledge [5] can significantly improve the results and is a promising line of future research. As in the previous example, some artifacts occur where monotonicity is violated, such as near the left foot and the left thigh. <p> The problem may be ameliorated by super-sampling the intermediate images or by concatenating the multiple image transforms into two aggregate warps and resampling only once <ref> [24] </ref>. 7 Conclusion In this paper we considered the question of which views of a static scene may be predicted from a set of two or more basis views, under perspective projection.
Reference: [25] <author> L. Shapiro, A. Zisserman, and M. Brady. </author> <title> Motion from point matches using affine epipolar geometry. </title> <booktitle> In Proc. Third European Conference on Computer Vision, </booktitle> <pages> pages 73-84, </pages> <year> 1994. </year>
Reference-contexts: This simplifies the morph step to a scanline interpolation and also avoids bottleneck problems that arise as a result of image plane rotations [30] Note that when I 0 and I 1 are orthographic or weak perspective projections, the epipoles are already at infinity <ref> [25] </ref>. Therefore, 0 and 1 are both zero and the rectification procedure reduces to a 2D rotation of both images and a shear of the second. This is precisely the rectification process needed to interpolate orthographic images in [22].
Reference: [26] <author> R. Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(2) </volume> <pages> 22-30, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Image-based representations of 3D scenes are currently being developed by many researchers in the computer vision and computer graphics communities (see, for example, <ref> [12, 17, 26, 3, 11, 28] </ref>). These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks [27, 18].
Reference: [27] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks <ref> [27, 18] </ref>. In addition, these representations are readily acquired from a set of basis views, avoiding the need for automatic or manual techniques for acquiring 3D object models.
Reference: [28] <author> J. Y. A. Wang and E. H. Adelson. </author> <title> Layered representation for motion analysis. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 361-366, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Image-based representations of 3D scenes are currently being developed by many researchers in the computer vision and computer graphics communities (see, for example, <ref> [12, 17, 26, 3, 11, 28] </ref>). These representations encode scene appearance with a set of images that may be adaptively combined to produce new views of a scene. Image-based techniques are especially attractive because they provide photometric information which has proven very valuable for recognition tasks [27, 18].
Reference: [29] <author> T. Werner, R. D. Hersch, and V. Hlavac. </author> <title> Rendering real-world objects using view interpolation. </title> <booktitle> In Proc. Fifth Intl. Conference on Computer Vision, </booktitle> <pages> pages 957-962, </pages> <year> 1995. </year>
Reference-contexts: It is not clear, however, that a more complete coverage of viewspace is theoretically possible. A number of "view synthesis" techniques have been developed recently <ref> [12, 4, 2, 16, 29, 8, 15] </ref> to extend the range of predictable views. However, those methods require solving ill-posed correspondence tasks, suggesting that the view synthesis problem is inherently ill-posed.
Reference: [30] <author> G. Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990. </year> <month> 16 </month>
Reference-contexts: The prewarping and postwarping operations, combined with the intermediate morph, require multiple image resampling operations that may contribute to a noticeable blurring in the in-between images. Resampling effects can be reduced by supersampling the input images <ref> [30] </ref> or by composing the prewarp, morph, and postwarp transformations into one aggregate warp for each image. A disadvantage of the latter approach is that the scanline property no longer applies. Rectification is possible providing that the epipoles are outside of the respective image borders. <p> Although this is technically sufficient for prewarping, it is useful to add the additional warps to align the scanlines. This simplifies the morph step to a scanline interpolation and also avoids bottleneck problems that arise as a result of image plane rotations <ref> [30] </ref> Note that when I 0 and I 1 are orthographic or weak perspective projections, the epipoles are already at infinity [25]. Therefore, 0 and 1 are both zero and the rectification procedure reduces to a 2D rotation of both images and a shear of the second. <p> Given the positions of these four points in ^ I s and I s , the required homography is found by solving a system of linear equations <ref> [30] </ref>. An alternate solution is to have the user provide the homography directly or indirectly by specification of a small number of image points [12, 24]. All three methods for choosing the postwarp transforms generally result in the synthesis of projective views.
References-found: 30

