URL: http://www.stat.wisc.edu/p/stat/ftp/pub/loh/treeprogs/quest1.7/techrep.ps
Refering-URL: http://www.stat.wisc.edu/p/stat/ftp/pub/loh/treeprogs/quest1.7/
Root-URL: 
Email: limt@stat.wisc.edu, loh@stat.wisc.edu and yshih@math.ccu.edu.tw  
Title: An Empirical Comparison of Decision Trees and Other Classification Methods  
Author: Tjen-Sien Lim, Wei-Yin Loh and Yu-Shan Shih 
Date: June 30, 1997 (revised January 27, 1998)  
Address: Wisconsin, Madison  
Affiliation: Department of Statistics University of  University of Wisconsin, Madison and National Chung Cheng University, Taiwan  
Pubnum: Technical Report 979  
Abstract: Twenty two decision tree, nine statistical, and two neural network classifiers are compared on thirty-two datasets in terms of classification error rate, computational time, and (in the case of trees) number of terminal nodes. It is found that the average error rates for a majority of the classifiers are not statistically significant but the computational times of the classifiers differ over a wide range. The statistical POLYCLASS classifier based on a logistic regression spline algorithm has the lowest average error rate. However, it is also one of the most computationally intensive. The classifier based on standard polytomous logistic regression and a decision tree classifier using the QUEST algorithm with linear splits have the second lowest average error rates and are about 50 times faster than POLYCLASS. Among decision tree classifiers with univariate splits, the classifiers based on the C4.5, IND-CART, and QUEST algorithms have the best combination of error rate and speed, although the C4.5 trees tend to have about twice as many leaves as those from the other two algorithms. The C4.5 classifier based on rules also has good accuracy, but it does not scale as well as the other methods.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agresti, A. </author> <year> (1990). </year> <title> Categorical Data Analysis, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY. </address>
Reference: <author> Aronis, J. M. and Provost, F. J. </author> <year> (1997). </year> <title> Increasing the efficiency of data mining algorithms with breadth-first marker propagation, </title> <editor> in D. Heckerman, H. Mannila, D. Pregibon and R. Uthurusamy (eds), </editor> <booktitle> Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <pages> pp. 119-122. </pages>
Reference: <author> Auer, P., Holte, R. C. and Maass, W. </author> <year> (1995). </year> <title> Theory and applications of agnostic PAC-learning with small decision trees, </title> <editor> in A. Prieditis and S. Russell (eds), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 21-29. </pages>
Reference: <author> Becker, R. A., Chambers, J. M. and Wilks, A. R. </author> <year> (1988). </year> <title> The New S Language, </title> <publisher> Wadsworth. </publisher>
Reference-contexts: The trees based on the 0-SE and 1-SE pruning rules are denoted by IC0 and IC1 respectively. S-Plus tree: This is a variant of the CART algorithm written in the S language <ref> (Becker, Chambers and Wilks, 1988) </ref>. It is described in Clark and Pregibon (1993). It treats the tree as a probability model and employs deviance as the splitting criterion. The best tree is chosen by 10-fold cross validation.
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition, </title> <publisher> Oxford University Press, </publisher> <address> New York, NY. </address>
Reference: <author> Box, G. E. P., Hunter, W. G. and Hunter, J. S. </author> <year> (1978). </year> <title> Statistics for Experimenters, </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: The statistical procedure called two-way analysis of variance can be used to test the simultaneous statistical significance of differences between average error rates of the classifiers (called "treatments") while controlling for differences between datasets (called "blocks") <ref> (Box, Hunter and Hunter, 1978, p. 209) </ref>. Simultaneous confidence intervals for differences between average error rates can then be obtained using the Tukey method (Miller, 1981, p. 71).
Reference: <author> Breiman, L., Friedman, J., Olshen, R. and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees, </title> <publisher> Chapman and Hall, </publisher> <address> New York, NY. </address>
Reference: <author> Breslow, L. A. and Aha, D. W. </author> <year> (1997). </year> <title> Simplifying decision trees: A survey, </title> <journal> Knowledge Engineering Review 12: </journal> <pages> 1-40. </pages>
Reference: <author> Brodley, C. E. and Utgoff, P. E. </author> <year> (1992). </year> <title> Multivariate versus univariate decision trees, </title> <type> Technical Report 92-8, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Brodley, C. E. and Utgoff, P. E. </author> <year> (1995). </year> <title> Multivariate decision trees, </title> <booktitle> Machine Learning 19: </booktitle> <pages> 45-77. </pages>
Reference-contexts: We evaluate some decision tree classifiers that were not included in the StatLog project, such as LMDT <ref> (Brodley and Utgoff, 1995) </ref>, OC1 (Murthy, Kasif and Salzberg, 1994), T1 (Holte, 1993; Auer, Holte and Maass, 1995), and QUEST (Loh and Shih, 1997). QUEST is unique among decision trees in that it has negligible selection bias in its splits. 3.
Reference: <author> Brown, D. E., Corruble, V. and Pittard, C. L. </author> <year> (1993). </year> <title> A comparison of decision tree classifiers with backpropagation neural networks for multimodal classification problems, </title> <booktitle> Pattern Recognition 26: </booktitle> <pages> 953-961. </pages>
Reference: <author> Bull, S. </author> <year> (1994). </year> <title> Analysis of attitudes toward workplace smoking restrictions, </title> <editor> in N. Lange, L. Ryan, L. Billard, D. Brillinger, L. Conquest and J. Greenhouse (eds), </editor> <booktitle> Case Studies in Biometry, </booktitle> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <pages> pp. </pages> <month> 249-271. </month> <type> 16 Lim, Loh & Shih Technical report 979 Buntine, </type> <institution> W. </institution> <year> (1992). </year> <title> Learning classification trees, </title> <journal> Statistics and Computing 2: </journal> <pages> 63-73. </pages>
Reference-contexts: Attitude towards smoking restrictions (smo). This dataset came from the Attitudes Toward Smoking Legislation Survey-Metropolitan Toronto 1988, which was funded by NHRDP (Health and Welfare Canada). It was collected by L. Pederson and S. Bull at the Institute for Social Research at York University <ref> (Bull, 1994) </ref>. It was obtained from http://lib.stat.cmu.edu/datasets/csb/. The problem is to predict attitude toward restrictions on smoking in the workplace (prohibited, restricted, or unrestricted) based on bylaw-related, smoking-related, and sociodemographic covariates. There are 3 classes, 3 numerical attributes, and 5 categorical attributes.
Reference: <author> Buntine, W. and Caruana, R. </author> <year> (1992). </year> <title> Introduction to IND Version 2.1 and Recursive Partitioning, </title> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA. </address>
Reference-contexts: Conclusions and recommendations are given in Section 7. 2 Descriptions of algorithms Only a short description of each algorithm is given. Details may be found in the cited references. 2.1 Trees and rules CART: We use the version of CART implemented in the cart style of the IND package <ref> (Buntine and Caruana, 1992) </ref> with the Gini index of diversity as the splitting criterion. The trees based on the 0-SE and 1-SE pruning rules are denoted by IC0 and IC1 respectively.
Reference: <author> Clark, L. A. and Pregibon, D. </author> <year> (1993). </year> <title> Tree-based models, </title> <editor> in J. M. Chambers and T. J. Hastie (eds), </editor> <title> Statistical Models in S, </title> <publisher> Chapman & Hall, </publisher> <address> New York, NY, </address> <pages> pp. 377-419. </pages>
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction, </title> <editor> in A. Prieditis and S. Russell (eds), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> pp. 115-123. </pages>
Reference: <author> Curram, S. P. and Mingers, J. </author> <year> (1994). </year> <title> Neural networks, decision tree induction and discriminant analysis: an empirical comparison, </title> <journal> Journal of the Operational Research Society 45: </journal> <pages> 440-450. </pages>
Reference: <author> Friedman, J. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines (with discussion), </title> <journal> Annals of Statistics 19: </journal> <pages> 1-141. </pages>
Reference-contexts: They obtain a nonparametric version of discriminant analysis by replacing linear regression with a nonpara-metric regression method. Therefore any multi-response regression technique can be postprocessed to improve classification performance. Only one adaptive nonparametric regression procedure is compared in this study: MARS <ref> (Friedman, 1991) </ref>. We use the S-Plus (http://www.mathsoft.com/splus.html) function fda from the mda library of the StatLib S Archive. Two models are used: the additive model (degree=1, denoted by FM1) and the model containing first-order interactions (degree=2 with penalty=3, denoted by FM2).
Reference: <author> Hand, D. J. </author> <year> (1997). </year> <title> Construction and Assessment of Classification Rules, </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, England. </address>
Reference-contexts: A third criterion that has been largely ignored is the relative computational speed of the classifiers. The StatLog project indicates that no classifier is uniformly most accurate over the datasets studied. Instead, many classifiers possess comparable accuracy. For such classifiers, computational speed may be an important criterion <ref> (Hand, 1997) </ref>. The purpose of our paper is to extend the results of the StatLog project in the following ways: 1. In addition to classification accuracy and size of trees, we compare the relative computational speed of the algorithms studied.
Reference: <author> Harrison, D. and Rubinfeld, D. L. </author> <year> (1978). </year> <title> Hedonic prices and the demand for clean air, </title> <journal> Journal of Environmental Economics and Management 5: </journal> <pages> 81-102. </pages>
Reference-contexts: The analyses conducted in the StatLog project employ misclassification costs for different possible misclassifications. We did not incorporate the cost matrix in our analyses. The error rates are estimated using 10-fold cross validation. Boston housing (bos). This dataset gives housing values in Boston suburbs <ref> (Harrison and Rubinfeld, 1978) </ref>. There are 3 classes, 12 numerical attributes, 1 binary attribute, and 506 observations. The dataset can also be obtained from the UCI repository.
Reference: <author> Hastie, T. and Tibshirani, R. </author> <year> (1996). </year> <title> Discriminant analysis by Gaussian mixtures, </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 58: </volume> <pages> 155-176. </pages>
Reference-contexts: The classification problem is cast into a penalized regression framework via optimal scoring. PDA is implemented in S-Plus using the function fda with method=gen.ridge. MDA: This stands for mixture discriminant analysis <ref> (Hastie and Tibshirani, 1996) </ref>. It fits Gaussian mixture density functions to each class to effect classification. MDA is implemented in S-Plus using the library mda. POL: This is the POLYCLASS algorithm due to Kooperberg, Bose and Stone (1997).
Reference: <author> Hastie, T., Buja, A. and Tibshirani, R. </author> <year> (1995). </year> <title> Penalized discriminant analysis, </title> <journal> Annals of Statistics 23: </journal> <pages> 73-102. </pages>
Reference-contexts: We use the S-Plus (http://www.mathsoft.com/splus.html) function fda from the mda library of the StatLib S Archive. Two models are used: the additive model (degree=1, denoted by FM1) and the model containing first-order interactions (degree=2 with penalty=3, denoted by FM2). PDA: This is a form of penalized LDA <ref> (Hastie, Buja and Tibshirani, 1995) </ref>. It is designed for situations in which there are many highly correlated attributes, such as those obtained by discretizing a function, or the grey-scale values of the pixels in a series of images.
Reference: <author> Hastie, T., Tibshirani, R. and Buja, A. </author> <year> (1994). </year> <title> Flexible discriminant analysis by optimal scoring, </title> <journal> Journal of the American Statistical Association 89: </journal> <pages> 1255-1270. </pages>
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets, </title> <booktitle> Machine Learning 11: </booktitle> <pages> 63-90. </pages>
Reference: <author> Johnson, R. A. and Wichern, D. W. </author> <year> (1992). </year> <title> Applied Multivariate Statistical Analysis, 3 edn, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Kohonen, T. </author> <year> (1995). </year> <title> Self-Organizing Maps, </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg. </address>
Reference: <author> Kooperberg, C., Bose, S. and Stone, C. J. </author> <year> (1997). </year> <title> Polychotomous regression, </title> <journal> Journal of the American Statistical Association 92: </journal> <pages> 117-127. </pages>
Reference: <author> Lerman, C., Molyneaux, J. W., Pangemanan, S. </author> <title> and Iswarati (1991). The determinants of contraceptive method and service point choice, Secondary Analysis of the 1987 National Indonesia Contraceptive Prevalence Survey, Vol. 1: Fertility and Family Planning, East-West Population Institute, </title> <address> Honolulu, HI. </address>
Reference: <author> Loh, W.-Y. and Shih, Y.-S. </author> <year> (1997). </year> <title> Split selection methods for classification trees, </title> <booktitle> Statistica Sinica 7: </booktitle> <pages> 815-840. </pages>
Reference-contexts: We evaluate some decision tree classifiers that were not included in the StatLog project, such as LMDT (Brodley and Utgoff, 1995), OC1 (Murthy, Kasif and Salzberg, 1994), T1 (Holte, 1993; Auer, Holte and Maass, 1995), and QUEST <ref> (Loh and Shih, 1997) </ref>. QUEST is unique among decision trees in that it has negligible selection bias in its splits. 3. We include some of the newest and highly accurate spline-based statistical classifiers. <p> IC0 and QU0 are very similar in terms of speed, error rate and size of trees. However, QU0 is much less likely to produce trees with spurious splits <ref> (Loh and Shih, 1997) </ref>. Acknowledgments We are grateful to many individuals for helping us with this project. J. R. Quinlan answered our questions on C4.5. W. Buntine and S. K. Murthy gave advice on the installation of the IND and OC1 programs, respectively. B. Schulmeister and C.
Reference: <author> Loh, W.-Y. and Vanichsetakul, N. </author> <year> (1988). </year> <title> Tree-structured classification via generalized discriminant analysis (with discussion), </title> <journal> Journal of the American Statistical Association 83: </journal> <pages> 715-728. </pages>
Reference: <author> Mangasarian, O. L. and Wolberg, W. H. </author> <year> (1990). </year> <title> Cancer diagnosis via linear programming, </title> <journal> Siam News 23: </journal> <pages> 1-18. </pages>
Reference-contexts: A decision tree analysis of an early subset of these data using the FACT classifier is reported in Wolberg, Tanner, Loh and Vanichsetakul (1987), Wolberg, Tanner and Loh (1988), and Wolberg, Tanner and Loh (1989). The data has also been analyzed with linear programming methods <ref> (Mangasarian and Wolberg, 1990) </ref>. Contraceptive method choice (cmc). The dataset is a subset of the 1987 National Indonesia Contraceptive Prevalence Survey. The samples are married women who were either not pregnant or do not know if they were pregnant at the time of the interview.
Reference: <author> Merz, C. J. and Murphy, P. M. </author> <year> (1996). </year> <title> UCI Repository of Machine Learning Databases, </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA. </address> <note> (http://www.ics.uci.edu/~mlearn/MLRepository.html). 17 Lim, Loh & Shih Technical report 979 Michie, </note> <editor> D., Spiegelhalter, D. J. and Taylor, C. C. (eds) (1994). </editor> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Ellis Horwood, London. </publisher>
Reference-contexts: Our experiment compared twenty-two decision tree classifiers, nine classical and modern statistical classifiers, and two neural network classifiers. Many of the datasets were taken from the University of California, Irvine, Repository of Machine Learning Databases <ref> (Merz and Murphy, 1996) </ref>. Fourteen of the datasets were from real-life domains and two were artificially constructed. Five of the datasets were also used in the StatLog project. We doubled the number of datasets by adding noise variables to each set, making a total of 32 datasets.
Reference: <author> Miller, Jr., R. G. </author> <year> (1981). </year> <title> Simultaneous Statistical Inference, second edn, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Simultaneous confidence intervals for differences between average error rates can then be obtained using the Tukey method <ref> (Miller, 1981, p. 71) </ref>. According to this procedure, a difference between the average error rates of two classifiers is statistically significant at the 10% level if they differ by more than 0.0584.
Reference: <author> Muller, W. and Wysotzki, F. </author> <year> (1994). </year> <title> Automatic construction of decision trees for classification, </title> <journal> Annals of Operations Research 52: </journal> <pages> 231-247. </pages>
Reference: <author> Muller, W. and Wysotzki, F. </author> <year> (1997). </year> <title> The decision-tree algorithm CAL5 based on a statistical approach to its splitting algorithm, </title> <editor> in G. Nakhaeizadeh and C. C. Taylor (eds), </editor> <booktitle> Machine Learning and Statistics: The Interface, </booktitle> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <pages> pp. 45-65. </pages>
Reference: <author> Murthy, S. K., Kasif, S. and Salzberg, S. </author> <year> (1994). </year> <title> A system for induction of oblique decision trees, </title> <journal> Journal of Artificial Intelligence Research 2: </journal> <pages> 1-33. </pages>
Reference-contexts: We evaluate some decision tree classifiers that were not included in the StatLog project, such as LMDT (Brodley and Utgoff, 1995), OC1 <ref> (Murthy, Kasif and Salzberg, 1994) </ref>, T1 (Holte, 1993; Auer, Holte and Maass, 1995), and QUEST (Loh and Shih, 1997). QUEST is unique among decision trees in that it has negligible selection bias in its splits. 3. We include some of the newest and highly accurate spline-based statistical classifiers.
Reference: <author> Oates, T. and Jensen, D. </author> <year> (1997). </year> <title> The effects of training set size on decision tree complexity, </title> <editor> in J. D. H. Fisher (ed.), </editor> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> pp. 254-262. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Improved use of continuous attributes in C4.5, </title> <journal> Journal of Artificial Intelligence Research 4: </journal> <pages> 77-90. </pages>
Reference-contexts: Pruning is performed with the p.tree () function in the treefix library (Venables and Ripley, 1997) 2 Lim, Loh & Shih Technical report 979 from the StatLib S Archive at http://lib.stat.cmu.edu/S/. The 0-SE and 1-SE trees are denoted by ST0 and ST1 respectively. C4.5: We use Release 8 <ref> (Quinlan, 1996) </ref> (http://www.cs.su.oz.au/~quinlan/) with the default settings, which include pruning. The algorithm is described in Quinlan (1993). After a tree is constructed, the C4.5 rules induction program is used to produce a set of rules. The trees are denoted by C4T and the rules by C4R.
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Sarle, W. S. </author> <year> (1994). </year> <title> Neural networks and statistical models, </title> <booktitle> Proceedings of the Nineteenth Annual SAS Users Groups International Conference, SAS Institute, </booktitle> <publisher> Inc., Cary, </publisher> <address> NC, </address> <pages> pp. 1538-1550. </pages> <institution> (ftp://ftp.sas.com/pub/neural/neural1.ps). SAS Institute, Inc. </institution> <year> (1990). </year> <note> SAS/STAT User's Guide, Version 6, Vol. 1 & 2, </note> <institution> SAS Institute, Inc., Cary, NC. </institution>
Reference: <author> Shavlik, J. W., Mooney, R. J. and Towell, G. G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: an empirical comparison, </title> <booktitle> Machine Learning 6: </booktitle> <pages> 111-144. </pages>
Reference: <author> Venables, W. N. and Ripley, B. D. </author> <year> (1997). </year> <title> Modern Applied Statistics with S-Plus, 2 edn, </title> <publisher> Springer, </publisher> <address> New York, NY. </address>
Reference-contexts: It is described in Clark and Pregibon (1993). It treats the tree as a probability model and employs deviance as the splitting criterion. The best tree is chosen by 10-fold cross validation. Pruning is performed with the p.tree () function in the treefix library <ref> (Venables and Ripley, 1997) </ref> 2 Lim, Loh & Shih Technical report 979 from the StatLib S Archive at http://lib.stat.cmu.edu/S/. The 0-SE and 1-SE trees are denoted by ST0 and ST1 respectively. C4.5: We use Release 8 (Quinlan, 1996) (http://www.cs.su.oz.au/~quinlan/) with the default settings, which include pruning. <p> Model selection is done with 10-fold cross validation. 4 Lim, Loh & Shih Technical report 979 2.3 Neural networks LVQ: We use the learning vector quantization algorithms in the S-Plus class library <ref> (Venables and Ripley, 1997) </ref> of the StatLib S Archive. Details of the LVQ algorithms may be found in Kohonen (1995). Codebook initialization is done using the function lvqinit. The size of the codebook is set to be 10% of the size of the training set.
Reference: <author> Wolberg, W. H., Tanner, M. A. and Loh, W.-Y. </author> <year> (1988). </year> <title> Diagnostic schemes for fine needle aspirates of breast masses, </title> <booktitle> Analytical and Quantitative Cytology and Histology 10: </booktitle> <pages> 225-228. </pages>
Reference: <author> Wolberg, W. H., Tanner, M. A. and Loh, W.-Y. </author> <year> (1989). </year> <title> Fine needle aspiration for breast mass diagnosis, </title> <booktitle> Archives of Surgery 124: </booktitle> <pages> 814-818. </pages>
Reference: <author> Wolberg, W. H., Tanner, M. A., Loh, W.-Y. and Vanichsetakul, N. </author> <year> (1987). </year> <title> Statistical approach to fine needle aspiration diagnosis of breast masses, </title> <journal> Acta Cytologica 31: </journal> <pages> 737-741. </pages>
References-found: 45

