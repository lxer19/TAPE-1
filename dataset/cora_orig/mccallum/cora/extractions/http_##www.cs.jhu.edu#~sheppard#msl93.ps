URL: http://www.cs.jhu.edu/~sheppard/msl93.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: 
Email: sheppard@cs.jhu.edu  
Title: Inducing Classification Rules for Public Health Data  
Author: John W. Sheppard 
Keyword: Key words: Classification, unsupervised learning, clustering, decision trees  
Address: Baltimore, Maryland 21218  
Affiliation: Department of Computer Science The Johns Hopkins University  
Abstract: Analyzing data with the intent of inducing classification rules typically proceeds from a set of training data in which classifications are known. In the event classifications are unknown, algorithms exist for performing unsupervised learning to determine concept classes inherent in the data. In this paper, we describe experiments applying multiple learning strategies for classifying unlabeled data. Specifically, three unsupervised learning algorithms were applied to a large set of public health data in order to determine likely concept classes for the data based on the inherent features in the data. After inducing the concept classes, the data were processed by a decision tree algorithm in order to determine more efficient classification rules under the assumption that the concepts induced during unsupervised learning were correct. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., D. Kibler, and M. K. Albert, </author> <title> ``Instance Based Learning Algorithms,'' </title> <journal> Machine Learning, </journal> <volume> Vol. 6, </volume> <pages> pp. 37-66, </pages> <year> 1991. </year>
Reference-contexts: Nearest neighbor algorithms store training examples paired with a classification <ref> (Aha et al., 1991) </ref>. When a new point is presented, the stored point that is closest in some sense (such as Euclidean distance or Hamming distance) is selected and the corresponding classification reported. At times, labels providing classification information are not available with the training set.
Reference: <author> Cheeseman, Peter, James Kelley, Matthew Self, John Stutz, Will Taylor, and Don Freeman, </author> <title> ``AutoClass: A Bayesian Classification System,'' </title> <booktitle> Proceedings of the Fifth International Workshop on Machine Learning, </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: <author> Eaton, W. W., and C. Ritter, </author> <title> ``Distinguishing Anxiety and Depression with Field Survey Data,'' </title> <journal> Psychological Medicine, </journal> <volume> Vol. 18, </volume> <pages> pp. 155-166, </pages> <year> 1988. </year>
Reference: <author> Eaton, W. W., A. McCutcheon, A. Dryman, and A. Sorenson, </author> <title> ``Latent Class Analysis of Anxiety and Depression,'' </title> <journal> Sociological Methods and Research, </journal> <volume> Vol. 18, No. 1, </volume> <pages> pp. 104-125, </pages> <note> 1989 Everitt, </note> <author> B., </author> <title> Cluster Analysis, </title> <publisher> Wiley and Sons, </publisher> <address> New York, </address> <note> 1974 Fisher, </note> <author> D. H., </author> <title> ``Knowledge Acquisition Via Incremental Conceptual Clustering,'' </title> <journal> Machine Learning, </journal> <volume> Vol. 2, </volume> <pages> pp. 139-172, </pages> <year> 1987. </year>
Reference: <author> Furukawa, T., and Y. Sumita, </author> <title> ``A Cluster-Analytically Derived Subtyping of Chronic Affective Disorders,'' </title> <journal> Acta Psychiatr Scand, </journal> <volume> Vol. 85, </volume> <pages> pp. 177-182, </pages> <year> 1992. </year>
Reference-contexts: Specifically, this study assumed two classes. The studies reported in (Eaton and Ritter, 1988; Eaton et al., 1989) also applied latent class analysis and found four classes. In a more recent study <ref> (Furukawa and Sumita, 1992) </ref>, a hierarchical clustering algorithm was applied to a similar data set and three clusters identified. Unfortunately, the data set used was extremely small (40 subjects) thus making it difficult to compare with our results. For our experiments, we were able to observe the following.
Reference: <author> Grossberg, S., </author> <title> ``Adaptive Pattern Classification and Universal Recoding: Part I. Parallel Development and Coding of Neural Feature Detectors,'' </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 23, </volume> <pages> pp. 121-134, </pages> <year> 1976. </year>
Reference: <author> Grossberg, S., </author> <title> ``Competitive Learning: From Interactive Activation to Adaptive Resonance,'' </title> <journal> Cognitive Science, The Cognitive Science Society, </journal> <volume> Vol. 11, </volume> <pages> pp. 23-63, </pages> <year> 1987. </year>
Reference: <author> Grove, W. M., N. C. Andreasen, M. Yound, </author> <note> J. </note>
Reference: <author> Endicott, M. B. Keller, R. M. A. Hirschfeld, and T. Reich, </author> <title> ``Isolation and Characterization of a Nuclear Depressive Syndrome,'' </title> <journal> Psychological Medicine, </journal> <volume> Vol. 17, </volume> <pages> pp. 471-484, </pages> <year> 1987. </year>
Reference: <author> MacQueen, J. B., </author> <title> ``Some Methods for Classification and Analysis of Multivariate Observations,'' </title> <booktitle> Proceedings of the Symposium on Mathematical Statistics and Probability, </booktitle> <publisher> University of California Press, </publisher> <address> Berkeley, California, </address> <pages> pp. 281-297, </pages> <year> 1967. </year>
Reference-contexts: The three clustering algorithms examined include a non-hierarchical approach, a hierarchical approach (thus resulting in a decision tree), and a connectionist approach. The nonhierarchical approach is based on a variation of MacQueen's k-means method <ref> (MacQueen, 1967) </ref>. The standard k-means method assumes k clusters and fits the data in the clusters with the nearest centroids. The variation of this method used permits k to vary so that an estimate of the number of classes in the data may be determined. <p> The class corresponding to the nearest centroid is the one identified for that data point. One of the most common approaches to nonhierarchical clustering is MacQueen's k-means algorithm <ref> (MacQueen, 1967) </ref>. The k-means algorithm attempts to determine the k best clusters for a set of data such that classification is made by finding the cluster with the nearest Euclidean distance.
Reference: <author> Michalski, R. S., and R. E. Stepp, </author> <title> ``Learning from Observation: Conceptual Clustering,'' </title> <booktitle> in Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 1, </volume> <editor> eds. R. Michalski, </editor> <publisher> J. </publisher>
Reference: <author> Carbonnel, and T. </author> <title> Mitchel, </title> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, California, </address> <pages> pp. 331-363, </pages> <year> 1983. </year>
Reference: <author> Quinlan, J. R., </author> <title> ``Induction of Decision Trees,'' </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: For example, decision trees (such as those generated by Quinlan's ID3 and C4 algorithms) select attributes as internal test nodes of a tree to determine the class to which a data point belongs, given at the leaves of the tree <ref> (Quinlan, 1986) </ref>. Nearest neighbor algorithms store training examples paired with a classification (Aha et al., 1991). When a new point is presented, the stored point that is closest in some sense (such as Euclidean distance or Hamming distance) is selected and the corresponding classification reported.
Reference: <author> Quinlan, J. R., </author> <title> ``Simplifying Decision Trees,'' </title> <journal> Journal of Man-Machine Studies, </journal> <volume> Vol. 27, </volume> <pages> pp. 221-234, </pages> <year> 1987. </year>
Reference-contexts: Then the information gain is simply I (T) E (attrib j ). The attribute with the maximum gain is selected for the root of the current subtree. C4 adds several techniques for pruning the trees, thus making the final trees more efficient than the initial ones <ref> (Quinlan, 1987) </ref>. Also, C4 applies a gain ratio criterion for its splitting criterion, but when all attributes are binary, the result is identical to applying information gain. 4. The Public Health Data For this study, psychiatric data on anxiety and depression were analyzed.
Reference: <author> Rumelhart, D. E. and D. Zipser, </author> <title> ``Feature Discovery by Competitive Learning,'' in Parallel Distribute Processing, David E. </title>
Reference: <editor> Rumelhart and James L. McClelland (eds.), </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 151-193, </pages> <year> 1987. </year>
Reference: <author> Shannon, C. E., </author> <title> ``A Mathematical Theory of Communications,'' </title> <journal> Bell System Technical Journal, </journal> <volume> Vol. 27, </volume> <pages> pp. 379-423, </pages> <year> 1948. </year>
Reference-contexts: The concept of a best split has been defined in several ways. For example, decision tree algorithms frequently employ concepts from Shannon's information theory to select the attribute that provides the most information independent of the actual values of the attributes <ref> (Shannon, 1948) </ref>. Association analysis selects attributes that maximize the chi-square coefficients of the data. Recall that chi-squared is computed as follows: where s 2 is that sample variance, s 2 is the population variance, and n is the sample size.
Reference: <author> Stepp, R. E., and R. S. Michalski, </author> <title> ``Conceptual Clustering: Inventing Goal-Oriented Classifications of Structured Objects,'' </title> <booktitle> in Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 2, </volume> <editor> eds. </editor> <publisher> R. </publisher>
Reference: <author> Michalski, J. Carbonnel, and T. </author> <title> Mitchel, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Palo Alto, California, </address> <pages> pp. 471-498, </pages> <year> 1986. </year> <title> von der Malsburg, C., ``Self-organizing of Orientation Sensitive Cells in the Striate Cortex,'' </title> <journal> Kybernetik, </journal> <volume> Vol. 14, </volume> <pages> pp. 85-100, </pages> <year> 1973. </year>
Reference-contexts: In our problem, little to no background knowledge was available, so this traditional approach could not be applied easily. COBWEB's advantage over CLUSTER/2 (Michalski and Stepp, 1983) or CLUSTER/S <ref> (Stepp and Michalski, 1986) </ref> is that the evaluation function is domain independent. However, we would expect the availability of domain knowledge to improve classification strategies. 7. Summary In this paper we presented the results of three approaches to analyzing and clustering a large set of psychiatric data.
References-found: 19

