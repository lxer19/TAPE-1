URL: http://www.ece.cmu.edu/~ganger/papers/CSE-TR-243-95.ps
Refering-URL: http://www.ece.cmu.edu/~ganger/disksim/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Author: Gregory Robert Ganger Professor Peter M. Banks Professor Edward S. Davidson 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee: Professor Yale N. Patt, Chair  Professor Trevor N. Mudge Professor  
Keyword: System-Oriented Evaluation of I/O Subsystem Performance  
Date: 1995  
Affiliation: University of Michigan  Joseph Pasquale, University of California, San Diego  
Note: by  
Abstract: Published as Report CSE-TR-243-95, Department of EECS, University of Michigan, Ann Arbor, June 1995. 
Abstract-found: 1
Intro-found: 1
Reference: [Abbott90] <author> R. Abbott, H. Garcia-Molina, </author> <title> "Scheduling I/O Request with Deadlines: a Performance Evaluation", </title> <booktitle> IEEE Real-Time Systems Symposium, </booktitle> <year> 1990, </year> <pages> pp. 113-124. </pages>
Reference-contexts: For example, [Carey89] evaluates a priority-based SCAN algorithm where the priorities are assigned based on the process that generates the request. Priority-based algorithms have also been studied in the context of real-time systems, with each request using the deadline of the task that generates it as its own. <ref> [Abbott90] </ref> describes the FD-SCAN algorithm, wherein the SCAN direction is chosen based on the relative position of the pending request with the earliest feasible deadline. [Chen91a] describes two deadline-weighted Shortest-Seek-Time-First algorithms and shows that they provide lower transaction loss ratios than non-priority algorithms and the other priority-based algorithms described above.
Reference: [Amdahl67] <author> G. </author> <title> Amdahl, "Validity of the single processor approach to achieving large scale computing capabilities", </title> <booktitle> AFIPS Spring Joint Computing Conference, </booktitle> <volume> Vol. 30, </volume> <month> April </month> <year> 1967, </year> <pages> pp. 483-485. </pages>
Reference-contexts: For example, microprocessor performance grows at a rate of 35-50 percent per year [Myers86], while disk drive performance grows at only 5-20 percent per year [Lee93]. As this trend continues, applications that utilize any quantity of I/O will become more and more limited by the I/O subsystem <ref> [Amdahl67] </ref>. Second, advances in technology enable new applications and expansions of existing applications, many of which rely on increased I/O capability. In response to the growing importance of I/O subsystem performance, researchers and developers are focusing more attention on the identification of high-performance I/O subsystem architectures and implementations.
Reference: [Baker91] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, J. Ousterhout, </author> <title> "Measurements of a Distributed File System", </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: I/O requests that prefetch unnecessary data are completely useless and are therefore classified as time-noncritical. The ufs file system prefetches file blocks sequentially until non-sequential access is detected. Measurements of UNIX file systems indicate that most user-level file reads are sequential <ref> [Ousterhout85, Baker91] </ref>. This is true of the system-level workloads used in this dissertation. The ufs file system distinguishes between three types of file block writes: synchronous, asynchronous and delayed [Ritchie86].
Reference: [Bennett94] <author> S. Bennett, D. Melski, </author> <title> "A Class-Based Disk Scheduling Algorithms: Implementation and Performance Study", </title> <type> Unpublished Report, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1994. </year>
Reference-contexts: Measurements of real systems suggest that this should be a rare occurrence [Ruemmler93a]. The exception would be when the main memory capacity is too small to maintain a balanced system (e.g., <ref> [Bennett94] </ref>). 2 This use of synchronous and asynchronous writes for metadata update sequencing represents a significant file system performance problem. <p> If the write working set exceeds the file cache, then the system will be limited by the throughput of the storage subsystem. In this case, scheduling based on request criticality can be detrimental because it tends to reduce storage subsystem performance. <ref> [Bennett94] </ref>, a re-evaluation of the work in [Ganger93], found that criticality-based disk scheduling consistently hurts performance on a memory-starved machine. More balanced systems will suffer from this problem only when the short-term write working set exceeds the file cache capacity. Measurements of real systems indicate that this occurs infrequently [Ruemmler93a].
Reference: [Biswas93] <author> P. Biswas, K.K. Ramakrishnan, D. Towsley, </author> <title> "Trace Driven Analysis of Write Caching Policies for Disks", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 13-23. </pages>
Reference-contexts: In such environments, the cache should be designed to minimize read response times while ensuring that the cache does not fill with dirty blocks <ref> [Reddy92, Biswas93, Treiber94] </ref>. With non-volatile cache memory becoming more and more common, it becomes easy for storage subsystem designers to translate write latency problems into write throughput problems, which are much easier to handle. This leads directly to the conclusion that read latencies are the most significant performance problem. <p> For example, [Busch85, Smith85, Miyachi86] use trace-driven simulation to quantify the value of write-thru disk block caches and determine how they should be designed. Storage subsystem models have also been used to investigate design issues for write-back disk block caches located in the on-board disk drive control logic <ref> [Ruemmler93, Biswas93] </ref> or above the disk drive (e.g., in main memory or an intermediate controller) [Solworth90, Carson92, Reddy92]. Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]).
Reference: [Bitton88] <author> D. Bitton, J. Gray, </author> <title> "Disk Shadowing", </title> <booktitle> International Conference on Very Large Data Bases, </booktitle> <month> September </month> <year> 1988, </year> <pages> pp. 331-338. </pages>
Reference-contexts: The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). Both have been known for many years (e.g., [Ouchi78]). Storage subsystem models have been used to evaluate design issues for both replication-based redundancy (e.g., <ref> [Bitton88, Bitton89, Copeland89, Hsiao90] </ref>) and parity-based redundancy (e.g., [Muntz90, Lee91, Menon91, Holland92, Menon92, Ng92, Cao93, Hou93, Hou93a, Stodolsky93, Treiber94, Chen95]).
Reference: [Bitton89] <author> D. Bitton, </author> <title> "Arm Scheduling in Shadowed Disks", </title> <booktitle> IEEE COMPCON, Spring 1989, </booktitle> <pages> pp. 132-136. </pages>
Reference-contexts: The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). Both have been known for many years (e.g., [Ouchi78]). Storage subsystem models have been used to evaluate design issues for both replication-based redundancy (e.g., <ref> [Bitton88, Bitton89, Copeland89, Hsiao90] </ref>) and parity-based redundancy (e.g., [Muntz90, Lee91, Menon91, Holland92, Menon92, Ng92, Cao93, Hou93, Hou93a, Stodolsky93, Treiber94, Chen95]).
Reference: [Brandwajn94] <author> A. Brandwajn, D. Levy, </author> <title> "A Study of Cached RAID-5 I/O", </title> <booktitle> Computer Measurement Group (CMG) Conference, </booktitle> <year> 1994, </year> <pages> pp. 393-403. </pages>
Reference-contexts: Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., <ref> [Menon91, Brandwajn94, Treiber94] </ref>). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94].
Reference: [Busch85] <author> J. Busch, A. Kondoff, </author> <title> "Disc Caching in the System Processing Units of the HP 3000 Family of Computers", </title> <journal> HP Journal, </journal> <volume> Vol. 36, No. 2, </volume> <month> February </month> <year> 1985, </year> <pages> pp. 21-39. </pages>
Reference-contexts: There have been a few instances of very simple system-level models being used to examine the value of caching disk blocks in main memory. For example, [Miller91] uses a simple system-level model to study the effects of read-ahead and write buffering on supercomputer applications. <ref> [Busch85] </ref> examines the transaction processing performance impact of changes to hit ratios and flush policies for disk block caches located in main memory. [Dan94] studies transaction throughput as a function of the database buffer pool size for skewed access patterns. <p> Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance. Most design-stage studies of disk cache designs use open subsystem models, relying on traces of disk requests collected from user environments to reproduce realistic access patterns. For example, <ref> [Busch85, Smith85, Miyachi86] </ref> use trace-driven simulation to quantify the value of write-thru disk block caches and determine how they should be designed.
Reference: [Cao93] <author> P. Cao, S. Lim, S. Venkataraman, J. Wilkes, </author> <title> "The TickerTAIP Parallel RAID Architecture", </title> <booktitle> IEEE International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 52-63. </pages>
Reference: [Cao95] <author> P. Cao, E. Felton, A. Karlin, K. Li, </author> <title> "A Study of Integrated Prefetching and Caching Strategies", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. 188-197. 125 126 </pages>
Reference-contexts: This leads directly to the conclusion that read latencies are the most significant performance problem. Researchers are currently exploring approaches to predicting and using information about future access patterns to guide aggressive prefetching activity (e.g., <ref> [Patterson93, Griffioen94, Cao95] </ref>), hoping to utilize high-throughput storage systems to reduce read latencies. Priority-based disk scheduling has been examined and shown to improve system performance. For example, [Carey89] evaluates a priority-based SCAN algorithm where the priorities are assigned based on the process that generates the request.
Reference: [Carey89] <author> M. Carey, R. Jauhari, M. Livny, </author> <title> "Priority in DBMS Resource Scheduling", </title> <booktitle> International Conference on Very Large Data Bases, </booktitle> <year> 1989, </year> <pages> pp. 397-410. </pages>
Reference-contexts: Researchers are currently exploring approaches to predicting and using information about future access patterns to guide aggressive prefetching activity (e.g., [Patterson93, Griffioen94, Cao95]), hoping to utilize high-throughput storage systems to reduce read latencies. Priority-based disk scheduling has been examined and shown to improve system performance. For example, <ref> [Carey89] </ref> evaluates a priority-based SCAN algorithm where the priorities are assigned based on the process that generates the request.
Reference: [Carson92] <author> S. Carson, S. Setia, </author> <title> "Analysis of the Periodic Update Write Policy for Disk Cache", </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 18, No. 1, </volume> <month> January </month> <year> 1992, </year> <pages> pp. 44-54. </pages>
Reference-contexts: Researchers have noted that bursts of delayed (i.e., time-noncritical) writes caused by periodic update policies can seriously degrade performance by interfering with read requests (which tend to be more critical) <ref> [Carson92, Mogul94] </ref>. Carson and Setia argued that disk cache performance should be measured in terms of its effect on read requests. While not describing or distinguishing between classes of I/O requests, they did make a solid distinction between read and write requests based on process interaction. <p> Storage subsystem models have also been used to investigate design issues for write-back disk block caches located in the on-board disk drive control logic [Ruemmler93, Biswas93] or above the disk drive (e.g., in main memory or an intermediate controller) <ref> [Solworth90, Carson92, Reddy92] </ref>. Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94]. <p> This algorithm represents a significant reduction in the write burstiness associated with the conventional approach (as studied in <ref> [Carson92, Mogul94] </ref>) but does not completely alleviate the phenomenon.
Reference: [Chen90] <author> P. Chen, D. Patterson, </author> <title> "Maximizing throughput in a striped disk array", </title> <booktitle> IEEE International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 322-331. </pages>
Reference-contexts: The stripe unit size (i.e., the quantity of data mapped onto one physical disk before switching to the next) is an important design parameter that has also been studied with both open subsystem models [Livny87, Reddy89] and closed subsystem models <ref> [Chen90] </ref>. Storage subsystem models have also been used to examine other design issues in striped disk subsystems, including spindle synchronization [Kim86, Kim91] and disk/host connectivity [Ng88]. Redundant Disk Arrays As reliability requirements and the number of disks in storage subsystems increase, it becomes important to utilize on-line redundancy.
Reference: [Chen90a] <author> P. Chen, G. Gibson, R. Katz, D. Patterson, </author> <title> "An Evaluation of Redundant Arrays of Disks using an Amdahl 5890", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: In fact, many prototypes and real storage subsystems have been evaluated with the same evaluation techniques (e.g., <ref> [Geist87a, Chen90a, Chervenak91, Chen93a, Geist94] </ref>). 3.3.2 Storage Performance Metrics The three most commonly used storage subsystem performance metrics are request response times (averages, variances and/or distributions), maximum request throughputs and peak data bandwidth. <p> Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., [Patterson88, Chen91, Hou93, Hou93b, Mourad93]) and measurements of prototypes under similar workloads (e.g., <ref> [Chen90a, Chervenak91] </ref>). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data. This concept can be applied in two ways to improve performance for reads and for writes, respectively.
Reference: [Chen91] <author> S. Chen, D. Towsley, </author> <title> "A Queueing Analysis of RAID Architectures", </title> <institution> University of Massachusetts, Amherst, </institution> <type> COINS Technical Report 91-71, </type> <month> September </month> <year> 1991. </year>
Reference-contexts: Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., <ref> [Patterson88, Chen91, Hou93, Hou93b, Mourad93] </ref>) and measurements of prototypes under similar workloads (e.g., [Chen90a, Chervenak91]). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data.
Reference: [Chen91a] <author> S. Chen, J. Kurose, J. Stankovic, D. Towsley, </author> <title> "Performance Evaluation of Two New Disk Request Scheduling Algorithms for Real-Time Systems", </title> <journal> Journal of Real-Time Systems, </journal> <volume> Vol. 3, </volume> <year> 1991, </year> <pages> pp. 307-336. </pages>
Reference-contexts: have also been studied in the context of real-time systems, with each request using the deadline of the task that generates it as its own. [Abbott90] describes the FD-SCAN algorithm, wherein the SCAN direction is chosen based on the relative position of the pending request with the earliest feasible deadline. <ref> [Chen91a] </ref> describes two deadline-weighted Shortest-Seek-Time-First algorithms and shows that they provide lower transaction loss ratios than non-priority algorithms and the other priority-based algorithms described above. In all of these cases, the priorities assigned to each request reflects the priority or the deadline of the associated processes rather than criticality.
Reference: [Chen93a] <author> P. Chen, E. Lee, A. Drapeau, K. Lutz, E. Miller, S. Seshan, K. Sherriff, D. Pat-terson, R. Katz, </author> <title> "Performance and Design Evaluation of the RAID-II Storage Server", </title> <booktitle> International Parallel Processing Symposium, Workshop on I/O, </booktitle> <year> 1993. </year>
Reference-contexts: In fact, many prototypes and real storage subsystems have been evaluated with the same evaluation techniques (e.g., <ref> [Geist87a, Chen90a, Chervenak91, Chen93a, Geist94] </ref>). 3.3.2 Storage Performance Metrics The three most commonly used storage subsystem performance metrics are request response times (averages, variances and/or distributions), maximum request throughputs and peak data bandwidth.
Reference: [Chen93b] <author> P. Chen, E. Lee, G. Gibson, R. Katz, D. Patterson, </author> <title> "RAID: High-Performance, Reliable Secondary Storage", </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 26, No. 2, </volume> <month> June </month> <year> 1994, </year> <pages> pp. 145-188. </pages>
Reference-contexts: Such combining requires gather/scatter hardware support, which is not always available. That is, outgoing data must be gathered from non-contiguous cache locations and incoming data must be scattered among them. Disk Array Data Organizations Because disk array data organizations <ref> [Chen93b, Ganger94a] </ref> can be utilized in multiple components (e.g., device drivers and intelligent storage controllers), logical address mapping is implemented as a separate module (the logorg module) that is incorporated into various components as appropriate. 4 The logorg module supports most logical data organizations, which are made up of a data <p> One parameter specifies the reconstruct-write fraction. That is, the fraction of disks in a parity stripe that must be written for reconstruct-write to be used instead of read-modify-write (see <ref> [Chen93b, Ganger94a] </ref>). Another parameter specifies the parity stripe unit size, which may differ from the data stripe unit size (although one must be an even multiple of the other). A parameter specifies the parity rotation type for the rotated parity redundancy mechanism (see [Lee91]).
Reference: [Chen95] <author> P. Chen, E. Lee, </author> <title> "Striping in a RAID Level 5 Disk Array", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. 136-145. </pages>
Reference: [Chervenak91] <author> A.L. Chervenak, R. Katz, </author> <title> "Performance of a RAID Prototype", </title> <booktitle> ACM SIG-METRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1991, </year> <pages> pp. 188-197. </pages>
Reference-contexts: In fact, many prototypes and real storage subsystems have been evaluated with the same evaluation techniques (e.g., <ref> [Geist87a, Chen90a, Chervenak91, Chen93a, Geist94] </ref>). 3.3.2 Storage Performance Metrics The three most commonly used storage subsystem performance metrics are request response times (averages, variances and/or distributions), maximum request throughputs and peak data bandwidth. <p> Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., [Patterson88, Chen91, Hou93, Hou93b, Mourad93]) and measurements of prototypes under similar workloads (e.g., <ref> [Chen90a, Chervenak91] </ref>). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data. This concept can be applied in two ways to improve performance for reads and for writes, respectively.
Reference: [Chiu78] <author> W. Chiu, W. Chow, </author> <title> "A Performance Model of MVS", </title> <journal> IBM System Journal, </journal> <volume> Vol. 17, No. 4, </volume> <year> 1978, </year> <pages> pp. 444-463. </pages>
Reference-contexts: This section describes previous work relating to system-level models and their use in storage subsystem performance evaluation. [Seaman69] and <ref> [Chiu78] </ref> describe system-level modeling efforts used mainly for examining alternative system configurations (as opposed to I/O subsystem designs). [Haigh90] describes a system performance measurement technique that consists of tracing major system events.
Reference: [Coff72] <author> E. G. Coffman, L. A. Klimko, B. Ryan, </author> <title> "Analysis of Scanning Policies for Reducing Disk Seek Times", </title> <journal> SIAM Journal of Computing, </journal> <volume> Vol. 1, No. 3, </volume> <month> September </month> <year> 1972, </year> <pages> pp. 269-279. </pages>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, <ref> [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] </ref> all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation
Reference: [Coffman82] <author> E. Coffman, M. Hofri, </author> <title> "On the Expected Performance of Scanning Disks", </title> <journal> SIAM Journal of Computing, </journal> <volume> Vol. 11, No. 1, </volume> <month> February </month> <year> 1982, </year> <pages> pp. 60-70. </pages>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, <ref> [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] </ref> all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation
Reference: [Copeland89] <author> G. Copeland, T. Keller, </author> <title> "A Comparison of High-Availability Media Recovery Techniques", </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <year> 1989, </year> <pages> pp. 98-109. 127 </pages>
Reference-contexts: The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). Both have been known for many years (e.g., [Ouchi78]). Storage subsystem models have been used to evaluate design issues for both replication-based redundancy (e.g., <ref> [Bitton88, Bitton89, Copeland89, Hsiao90] </ref>) and parity-based redundancy (e.g., [Muntz90, Lee91, Menon91, Holland92, Menon92, Ng92, Cao93, Hou93, Hou93a, Stodolsky93, Treiber94, Chen95]).
Reference: [Dan94] <author> A. Dan, D. Dias, P. Yu, </author> <title> "Buffer Analysis for a Data Sharing Environment with Skewed Data Access", </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> Vol. 6, No. 2, </volume> <month> April </month> <year> 1994, </year> <pages> pp. 331-337. </pages>
Reference-contexts: For example, [Miller91] uses a simple system-level model to study the effects of read-ahead and write buffering on supercomputer applications. [Busch85] examines the transaction processing performance impact of changes to hit ratios and flush policies for disk block caches located in main memory. <ref> [Dan94] </ref> studies transaction throughput as a function of the database buffer pool size for skewed access patterns.
Reference: [Daniel83] <author> S. Daniel, R. Geist, "V-SCAN: </author> <title> An adaptive disk scheduling algorithm", </title> <booktitle> IEEE International Workshop on Computer Systems Organization, </booktitle> <month> March </month> <year> 1983, </year> <pages> pp. 96-103. </pages>
Reference-contexts: For example, [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. <ref> [Daniel83] </ref> introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation (as well as a real implementation tested in a user environment) to show that VSCAN (0.2) outperforms previous algorithms. [Seltzer90] and [Jacobson91] introduce algorithms that attempt to minimize total positioning times (seek plus rotation) and use closed
Reference: [Denning67] <author> P. J. Denning, </author> <title> "Effects of scheduling on file memory operations", </title> <booktitle> AFIPS Spring Joint Computer Conference, </booktitle> <month> April </month> <year> 1967, </year> <pages> pp. 9-21. </pages>
Reference-contexts: Disk Request Schedulers Disk (or drum) request schedulers have been an important system software component since the introduction of mechanical secondary storage into computer systems over 25 years ago <ref> [Denning67, Seaman66] </ref>. Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays.
Reference: [Ebling94] <author> M. Ebling, M. Satyanarayanan, "SynRGen: </author> <title> An Extensible File Reference Generator", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 108-117. </pages>
Reference-contexts: The source directory tree is mostly resident in the main memory file block cache when the task begins, since the experiment removes the new copy resulting from an immediately prior copytree execution. SynRGen Workloads SynRGen is a synthetic file reference generator that operates at the system call level <ref> [Ebling94] </ref>. For trace collection, SynRGen was configured to imitate programmers performing edit/debug activity on large software systems. [Ebling94] showed that this configuration closely matches measurements of such user activity. Each programmer is emulated by a single process that executes tasks interspersed with user think times. <p> SynRGen Workloads SynRGen is a synthetic file reference generator that operates at the system call level <ref> [Ebling94] </ref>. For trace collection, SynRGen was configured to imitate programmers performing edit/debug activity on large software systems. [Ebling94] showed that this configuration closely matches measurements of such user activity. Each programmer is emulated by a single process that executes tasks interspersed with user think times. Each task consists of a series of file accesses and possibly some computation time (e.g., to emulate compilation or program execution).
Reference: [English91] <author> R. English, A. Stepanov, "Loge: </author> <title> A Self-Organizing Disk Controller", </title> <institution> Hewlett-Packard Laboratories Report, HPL-91-179, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Open subsystem models have been used to evaluate this approach for non-redundant storage systems (e.g., <ref> [English91] </ref>) and mirrored disk systems (e.g., [Solworth91, Orji93]). Simple equations for the disk service time improvements provided by dynamically mapping parity locations and/or data locations in a RAID 5 disk array are derived in [Menon92]. Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance.
Reference: [Ferr84] <author> D. Ferrari, </author> <title> "On the Foundation of Artificial Workload Design", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1984, </year> <pages> pp. 8-14. </pages>
Reference-contexts: The "goodness" of an input workload is how well it represents workloads generated by real systems operating in the user environments of interest. A quantified goodness metric should come from performance-oriented comparisons of systems under the test workload and the real workload <ref> [Ferr84] </ref>. The goal of this dissertation is not to examine the space of possible workloads or the range of reasonable goodness metrics, but to explore fundamental problems with the manner in which common workload generators for storage subsystem models trivialize important performance/workload feedback effects.
Reference: [Ganger93] <author> G. Ganger, Y. Patt, </author> <title> "The Process-Flow Model: Examining I/O Performance from the System's Point of View", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 86-97. </pages>
Reference-contexts: First, excessively long response times may violate system guarantees about data hardness (i.e., when dirty data blocks are written stable storage). To support such guarantees, the disk scheduler must prevent individual request response times from exceeding predetermined limits. <ref> [Ganger93] </ref> evaluates a modification to the algorithms described above, wherein time-noncritical requests are moved into the high-priority queue after waiting for a certain period of time (e.g., 15 seconds). This approach was found to effectively limit the maximum response times. <p> If the write working set exceeds the file cache, then the system will be limited by the throughput of the storage subsystem. In this case, scheduling based on request criticality can be detrimental because it tends to reduce storage subsystem performance. [Bennett94], a re-evaluation of the work in <ref> [Ganger93] </ref>, found that criticality-based disk scheduling consistently hurts performance on a memory-starved machine. More balanced systems will suffer from this problem only when the short-term write working set exceeds the file cache capacity. Measurements of real systems indicate that this occurs infrequently [Ruemmler93a].
Reference: [Ganger93a] <author> G. Ganger, B. Worthington, R. Hou, Y. Patt, </author> <title> "Disk Subsystem Load Balancing: Disk Striping vs. Conventional Data Placement", </title> <booktitle> Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 40-49. </pages>
Reference-contexts: The performance impact of disk striping has been studied with both open subsystem models [Kim86, Kim91] and closed subsystem models [Salem86]. The load balancing benefits of disk striping have been demonstrated with open subsystem models <ref> [Livny87, Ganger93a] </ref>. The stripe unit size (i.e., the quantity of data mapped onto one physical disk before switching to the next) is an important design parameter that has also been studied with both open subsystem models [Livny87, Reddy89] and closed subsystem models [Chen90]. <p> In 1 The results are usually compared to those produced by the system-level simulator driven by the full system-level traces. 2 For this reason, I have used open subsystem simulation driven by traces of observed disk activity in my previous work <ref> [Ganger93a, Worthington94] </ref>. The results match reality in at least one instance. There is no corresponding trace-based workload generator for closed subsystem models, which never match the reality of most workloads. 43 this subsection, I compare two of them. The First-Come-First-Served (FCFS) algorithm services requests in arrival order. <p> The latter two options are not realistic data distribution schemes. They are used only for comparison purposes (as in <ref> [Ganger93a] </ref>). One parameter specifies the data distribution scheme (one of the four supported options) and a second parameter specifies the stripe unit size. 4. the redundancy mechanism. <p> This would not have been possible without the support of NCR Corporation (now AT&T/GIS). While these traces are not used for the experiments reported in this dissertation, they are a crucial component of this simulation infrastructure and have been central to some of our previous storage subsystem research <ref> [Ganger93a, Worthington94] </ref>. Table A.1 lists basic statistics for these traces. Three of the traces come from Hewlett-Packard systems running HP-UX, a version of the UNIX operating system. [Ruemmler93] describes these traces in detail. Cello comes from a server at HP Labs used primarily for program development, simulation, mail and news. <p> Two sets of traces come from NCR systems executing database benchmarks with Oracle T M database software. These traces were captured using the instrumentation described above and are described more thoroughly in <ref> [Ganger93a] </ref>. Several traces, named TPCB#, were cap 107 tured on a workstation executing the TPC-B benchmark [TPCB90]. Other traces, named MultiWisc#, were captured on an 8-processor database server executing a multiuser database benchmark based on the Wisconsin benchmark [Gray91].
Reference: [Ganger94] <author> G. Ganger, Y. Patt, </author> <title> "Metadata Update Performance in File Systems", </title> <booktitle> USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <month> November </month> <year> 1994, </year> <pages> pp. 49-60. </pages>
Reference-contexts: Because these metadata blocks exhibit high degrees of update locality <ref> [Ganger94] </ref>, most of the asynchronous metadata writes result in time-limited I/O requests. <p> The exception would be when the main memory capacity is too small to maintain a balanced system (e.g., [Bennett94]). 2 This use of synchronous and asynchronous writes for metadata update sequencing represents a significant file system performance problem. Aggressive implementation techniques can eliminate them <ref> [Hagmann87, Seltzer93, Ganger94] </ref>. 58 HP-UX Read Requests Write Requests Trace Critical Limited/Noncritical Critical Limited/Noncritical cello 36.4 % 7.7 % 37.2 % 18.7 % snake 37.9 % 5.9 % 13.6 % 42.6 % hplajw 38.4 % 4.0 % 28.8 % 28.8 % Table 7.1: Request Criticality Breakdown for HP-UX Traces. <p> For example, databases add entries to a write-ahead log to achieve an atomic update to multiple records. Also, many file systems require ordered updates to maintain metadata integrity <ref> [Ganger94] </ref>. 120 Figure B.5: Disk Drive Module. Command Queueing Most modern disk drives support command queueing, whereby some number (e.g., 2-64) of requests can be outstanding at a time. Command queueing has several benefits.
Reference: [Ganger94a] <author> G. Ganger, B. Worthington, R. Hou, Y. Patt, </author> <title> "Disk Arrays: High Performance, High Reliability Storage Subsystems", </title> <journal> IEEE Computer, </journal> <volume> Vol. 27, No. 3, </volume> <month> March </month> <year> 1994, </year> <pages> pp. 30-36. </pages>
Reference-contexts: Such combining requires gather/scatter hardware support, which is not always available. That is, outgoing data must be gathered from non-contiguous cache locations and incoming data must be scattered among them. Disk Array Data Organizations Because disk array data organizations <ref> [Chen93b, Ganger94a] </ref> can be utilized in multiple components (e.g., device drivers and intelligent storage controllers), logical address mapping is implemented as a separate module (the logorg module) that is incorporated into various components as appropriate. 4 The logorg module supports most logical data organizations, which are made up of a data <p> components (e.g., device drivers and intelligent storage controllers), logical address mapping is implemented as a separate module (the logorg module) that is incorporated into various components as appropriate. 4 The logorg module supports most logical data organizations, which are made up of a data distribution scheme and a redundancy mechanism <ref> [Ganger94a] </ref>. Requests are referred to the logorg module when they arrive at a logorg-containing component. The logorg module translates a newly arrived logical request into the appropriate set of physical accesses (from the components point of view) and returns a subset of these. <p> One parameter specifies the reconstruct-write fraction. That is, the fraction of disks in a parity stripe that must be written for reconstruct-write to be used instead of read-modify-write (see <ref> [Chen93b, Ganger94a] </ref>). Another parameter specifies the parity stripe unit size, which may differ from the data stripe unit size (although one must be an even multiple of the other). A parameter specifies the parity rotation type for the rotated parity redundancy mechanism (see [Lee91]).
Reference: [Geist87] <author> R. Geist, S. Daniel, </author> <title> "A Continuum of Disk Scheduling Algorithms", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 5, No. 1, </volume> <month> February </month> <year> 1987, </year> <pages> pp. 77-92. </pages>
Reference-contexts: As another example, [Worthington94] determines (using an extremely detailed disk simulator) that the relative performance of seek-reducing algorithms (e.g., Shortest-Seek-Time-First, V-SCAN (R) and C-LOOK) is often opposite the order indicated by recent studies <ref> [Geist87, Seltzer90, Jacobson91] </ref>. 1 It is worth reiterating that the problems addressed in this thesis are independent of how well a storage subsystem model imitates the corresponding real storage subsystem.
Reference: [Geist87a] <author> R. Geist, R. Reynolds, E. Pittard, </author> <title> "Disk Scheduling in System V", </title> <booktitle> ACM SIG-METRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1987, </year> <pages> pp. 59-68. </pages>
Reference-contexts: In fact, many prototypes and real storage subsystems have been evaluated with the same evaluation techniques (e.g., <ref> [Geist87a, Chen90a, Chervenak91, Chen93a, Geist94] </ref>). 3.3.2 Storage Performance Metrics The three most commonly used storage subsystem performance metrics are request response times (averages, variances and/or distributions), maximum request throughputs and peak data bandwidth. <p> All of this research in disk request scheduling algorithms relied upon storage subsystem models for design-stage performance comparisons. Some previous researchers have recognized that open subsystem models can mispredict disk scheduler performance. For example, <ref> [Geist87a] </ref> compares simulation results using an 18 open, Poisson request arrival process to measured results from a real implementation, finding that the simulator mispredicted performance by over 500 percent. Geist, et al., concluded from this that no open subsystem model provides useful information about disk scheduling algorithm performance.
Reference: [Geist94] <author> R. Geist, J. Westall, </author> <title> "Disk Scheduling in Linux", </title> <booktitle> Computer Measurement Group (CMG) Conference, </booktitle> <month> December </month> <year> 1994, </year> <pages> pp. 739-746. </pages>
Reference-contexts: In fact, many prototypes and real storage subsystems have been evaluated with the same evaluation techniques (e.g., <ref> [Geist87a, Chen90a, Chervenak91, Chen93a, Geist94] </ref>). 3.3.2 Storage Performance Metrics The three most commonly used storage subsystem performance metrics are request response times (averages, variances and/or distributions), maximum request throughputs and peak data bandwidth. <p> Also, they used unrealistic synthetic arrival times for the open subsystem model rather than traced arrival times. They went on to construct a simulation workload that better matches their artificial system workload. In a subsequent paper <ref> [Geist94] </ref>, Geist and Westall exploited a pathological aspect of their artificial workload to design a scheduling algorithm that achieves anomalous improvements in disk performance. <p> As a result, evaluating criticality-based scheduling with subsystem metrics would lead one to dismiss it as poor. In fact, a recent publication observed reductions in subsystem performance for a similar algorithm (giving priority to reads over writes) and concluded that it is a bad design point <ref> [Geist94] </ref>. One interesting effect of criticality-based scheduling algorithms is that positioning-related scheduling decisions, which have long been viewed as critical to performance, become much less important to overall system performance in sub-saturation workloads.
Reference: [Gingell87] <author> R. Gingell, J. Moran, W. Shannon, </author> <title> "Virtual Memory Architecture in SunOS", </title> <booktitle> Summer USENIX Conference, </booktitle> <month> June </month> <year> 1987, </year> <pages> pp. 81-94. 128 </pages>
Reference-contexts: The operating system is UNIX SVR4 MP, AT&T/GIS's production operating system for symmetric multiprocessing. The default file system, ufs, is based on the Berkeley fast file system [McKusick84]. The virtual memory system is similar to that of SunOS <ref> [Gingell87, Moran87] </ref> and file system caching is well integrated with the virtual memory system. <p> More balanced systems will suffer from this problem only when the short-term write working set exceeds the file cache capacity. Measurements of real systems indicate that this occurs infrequently [Ruemmler93a]. Further, most modern operating systems integrate the file block cache with the virtual memory system <ref> [Gingell87, Moran87] </ref>, allowing it to utilize much of the available memory capacity in such situations. 7.5 Summary This chapter demonstrates that storage subsystem performance metrics do not, in general, correlate with overall system performance metrics.
Reference: [Golding95] <author> R. Golding, P. Bosch, C. Staelin, T. Sullivan, J. Wilkes, </author> <title> "Idleness is not sloth", </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1995, </year> <pages> pp. 201-22. </pages>
Reference-contexts: The idle detector initiates cache cleaning activity when 0.5 seconds pass with no outstanding requests <ref> [Golding95] </ref>. Cleaning activity halts whenever a new request arrives. If a dirty block must be reclaimed, it is flushed immediately, as with the base policy. The benefit of detecting and using idle time in this way depends primarily on the amount of idle time available.
Reference: [Gotl73] <author> C. C. Gotlieb, G. H. MacEwen, </author> <title> "Performance of Movable-Head Disk Storage Devices", </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> Vol. 20, No. 4, </volume> <month> October </month> <year> 1973, </year> <pages> pp. 604-623. </pages> <editor> [Gray91] ed. J. Gray, </editor> <title> The Benchmark Handbook for Database and Transaction Processing Systems, </title> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, <ref> [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] </ref> all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation
Reference: [Griffioen94] <author> J. Griffioen, R. Appleton, </author> <title> "Reducing File System Latency using a Predictive Approach", </title> <booktitle> Summer USENIX Conference, </booktitle> <month> June </month> <year> 1994, </year> <pages> pp. 197-207. </pages>
Reference-contexts: This leads directly to the conclusion that read latencies are the most significant performance problem. Researchers are currently exploring approaches to predicting and using information about future access patterns to guide aggressive prefetching activity (e.g., <ref> [Patterson93, Griffioen94, Cao95] </ref>), hoping to utilize high-throughput storage systems to reduce read latencies. Priority-based disk scheduling has been examined and shown to improve system performance. For example, [Carey89] evaluates a priority-based SCAN algorithm where the priorities are assigned based on the process that generates the request.
Reference: [Hagmann87] <author> R. Hagmann, </author> <title> "Reimplementing the Cedar File System Using Logging and Group Commit", </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <month> November </month> <year> 1987, </year> <pages> pp. 155-162. </pages>
Reference-contexts: The exception would be when the main memory capacity is too small to maintain a balanced system (e.g., [Bennett94]). 2 This use of synchronous and asynchronous writes for metadata update sequencing represents a significant file system performance problem. Aggressive implementation techniques can eliminate them <ref> [Hagmann87, Seltzer93, Ganger94] </ref>. 58 HP-UX Read Requests Write Requests Trace Critical Limited/Noncritical Critical Limited/Noncritical cello 36.4 % 7.7 % 37.2 % 18.7 % snake 37.9 % 5.9 % 13.6 % 42.6 % hplajw 38.4 % 4.0 % 28.8 % 28.8 % Table 7.1: Request Criticality Breakdown for HP-UX Traces.
Reference: [Haigh90] <author> P. Haigh, </author> <title> "An Event Tracing Method for UNIX Performance Measurement", </title> <booktitle> Computer Measurement Group (CMG) Conference, </booktitle> <year> 1990, </year> <pages> pp. 603-609. </pages>
Reference-contexts: This section describes previous work relating to system-level models and their use in storage subsystem performance evaluation. [Seaman69] and [Chiu78] describe system-level modeling efforts used mainly for examining alternative system configurations (as opposed to I/O subsystem designs). <ref> [Haigh90] </ref> describes a system performance measurement technique that consists of tracing major system events. The end purpose for this technique is to measure system performance under various workloads rather than as input to a simulator to study I/O subsystem design options.
Reference: [Holland92] <author> M. Holland, G. Gibson, </author> <title> "Parity Declustering for Continuous Operation in Redundant Disk Arrays", </title> <booktitle> ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992, </year> <pages> pp. 23-35. </pages>
Reference-contexts: More detailed models are generally more accurate, but require more implementation effort and more computational power. Simple models can be constructed easily and used to produce "quick-and-dirty" answers. More precise performance studies, however, must use detailed, validated simulation models (or real implementations) to avoid erroneous conclusions. For example, <ref> [Holland92] </ref> uses a more detailed storage subsystem simulator to refute the results of [Muntz90] regarding the value of piggybacking rebuild requests on user requests to a failed disk in a RAID 5 array. <p> one of the physical disks contains parity information computed from the other disks' contents, rather than data, (4) rotated parity, wherein one disk's worth of capacity is used for parity information that is interleaved among the physical disks, (5) table-based parity, wherein a complex parity-based organization, such as parity declustering <ref> [Holland92] </ref> is utilized. 5. values that configure replication-based redundancy mechanisms. One parameter specifies how many copies of each disk are maintained. This value must divide evenly into the number of physical disks. A second parameter specifies how the logorg module decides which copy is accessed for each read request.
Reference: [Hofri80] <author> M. Hofri, </author> <title> "Disk Scheduling: FCFS vs. SSTF Revisited", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 23, No. 11, </volume> <month> November </month> <year> 1980, </year> <pages> pp. 645-653. </pages>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). <ref> [Teorey72, Hofri80] </ref> use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation (as well as a real implementation tested in a user environment) to show that VSCAN (0.2) outperforms previous algorithms. [Seltzer90] and [Jacobson91] introduce algorithms that attempt
Reference: [Hospodor94] <author> A. Hospodor, </author> <title> "The Effect of Prefetch in Caching Disk Buffers", </title> <type> Ph.D. Dissertation, </type> <institution> Santa Clara University, </institution> <year> 1994. </year>
Reference-contexts: Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models <ref> [Ng92a, Hospodor94] </ref>. Finally, storage subsystem models have been used to evaluate the performance benefits of on-board buffers for speed-matching media and bus transfers [Mitsuishi85, Houtekamer85]. 3.4 Summary Previous work relating both to request criticality and to system-level modeling is scarce, leaving considerable room for improvement.
Reference: [Hou93] <author> R. Hou, Y. Patt, </author> <title> "Comparing Rebuild Algorithms for Mirrored and RAID5 Disk Arrays", </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 317-326. </pages>
Reference-contexts: Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., <ref> [Patterson88, Chen91, Hou93, Hou93b, Mourad93] </ref>) and measurements of prototypes under similar workloads (e.g., [Chen90a, Chervenak91]). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data.
Reference: [Hou93a] <author> R. Hou, J. Menon, Y. Patt, </author> <title> "Balancing I/O Response Time and Disk Rebuild Time in a RAID5 Disk Array", </title> <booktitle> Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 70-79. </pages>
Reference: [Hou93b] <author> R. Hou, Y. Patt, </author> <title> "Trading Disk Capacity for Performance", </title> <booktitle> International Symposium on High-Performance Distributed Computing, </booktitle> <month> July </month> <year> 1993, </year> <pages> pp. 263-270. </pages>
Reference-contexts: Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., <ref> [Patterson88, Chen91, Hou93, Hou93b, Mourad93] </ref>) and measurements of prototypes under similar workloads (e.g., [Chen90a, Chervenak91]). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data.
Reference: [Houtekamer85] <author> G. Houtekamer, </author> <title> "The Local Disk Controller", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1985, </year> <pages> pp. 173-182. </pages>
Reference-contexts: Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94]. Finally, storage subsystem models have been used to evaluate the performance benefits of on-board buffers for speed-matching media and bus transfers <ref> [Mitsuishi85, Houtekamer85] </ref>. 3.4 Summary Previous work relating both to request criticality and to system-level modeling is scarce, leaving considerable room for improvement. This chapter describes this previous work and its short-comings.
Reference: [HP92] <author> Hewlett-Packard Company, </author> <title> "HP C2244/45/46/47 3.5-inch SCSI-2 Disk Drive Technical Reference Manual", Part Number 5960-8346, </title> <address> Edition 3, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: System The base system for my experiments (both simulation and implementation) is an NCR 3433, a 33 MHz Intel 80486 machine equipped with 48 MB of main memory. 2 Most of the experiments use an HP C2247 disk drive, which is a high performance, 3.5-inch, 1 GB SCSI storage device <ref> [HP92] </ref>. Table 5.3 lists some basic performance characteristics of this disk drive and [Worthington94a] provides a thorough breakdown of simulator configuration parameter values. The operating system is UNIX SVR4 MP, AT&T/GIS's production operating system for symmetric multiprocessing. <p> The demerit figure for this validation run is 0.07 ms, or 0.5% of the corresponding mean response time. Characteristics of the HP C2247A can be found in table 5.3 and in <ref> [HP92, Worthington94] </ref>. The validation workload parameters are 50% reads, 30% sequential, 30% local [normal with 10000 sector variance], 8KB mean request size [exponential], interarrival time [uniform 0-22 ms]. 35 Drive. The demerit figure for this validation run is 0.19 ms, or 1.2% of the corresponding mean response time.
Reference: [HP93] <author> Hewlett-Packard Company, </author> <title> "HP C2490A 3.5-inch SCSI-2 Disk Drives, Technical Reference Manual", Part Number 5961-4359, </title> <address> Edition 3, </address> <month> September </month> <year> 1993. </year> <month> 129 </month>
Reference-contexts: The demerit figure for this validation run is 0.26 ms, or 2.0% of the corresponding mean response time. Characteristics of the HP C2490A can be found in <ref> [HP93] </ref>. The validation workload parameters are 50% reads, 30% sequential, 30% local [normal with 10000 sector variance], 8KB mean request size [exponential], interarrival time [uniform 0-22 ms]. 38 Disk Drive. The demerit figure for this validation run is 0.31 ms, or 1.9% of the corresponding mean response time.
Reference: [HP94] <author> Hewlett-Packard Company, </author> <title> "HP C3323A 3.5-inch SCSI-2 Disk Drives, Technical Reference Manual", Part Number 5962-6452, </title> <address> Edition 2, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The demerit figure for this validation run is 0.31 ms, or 1.9% of the corresponding mean response time. Characteristics of the HP C3323A can be found in <ref> [HP94] </ref>.
Reference: [Hsiao90] <author> H. Hsiao, D. DeWitt, </author> <title> "Chained Declustering: A New Availability Strategy for Multiprocessor Database Machines", </title> <booktitle> IEEE International Conference on Data Engineering, </booktitle> <year> 1990, </year> <pages> pp. 456-465. </pages>
Reference-contexts: The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). Both have been known for many years (e.g., [Ouchi78]). Storage subsystem models have been used to evaluate design issues for both replication-based redundancy (e.g., <ref> [Bitton88, Bitton89, Copeland89, Hsiao90] </ref>) and parity-based redundancy (e.g., [Muntz90, Lee91, Menon91, Holland92, Menon92, Ng92, Cao93, Hou93, Hou93a, Stodolsky93, Treiber94, Chen95]).
Reference: [Jacobson91] <author> D. Jacobson, J. Wilkes, </author> <title> "Disk Scheduling Algorithms Based on Rotational Position", </title> <type> Hewlett-Packard Technical Report, </type> <institution> HPL-CSP-91-7, </institution> <month> February 26, </month> <year> 1991. </year>
Reference-contexts: As another example, [Worthington94] determines (using an extremely detailed disk simulator) that the relative performance of seek-reducing algorithms (e.g., Shortest-Seek-Time-First, V-SCAN (R) and C-LOOK) is often opposite the order indicated by recent studies <ref> [Geist87, Seltzer90, Jacobson91] </ref>. 1 It is worth reiterating that the problems addressed in this thesis are independent of how well a storage subsystem model imitates the corresponding real storage subsystem. <p> First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation (as well as a real implementation tested in a user environment) to show that VSCAN (0.2) outperforms previous algorithms. [Seltzer90] and <ref> [Jacobson91] </ref> introduce algorithms that attempt to minimize total positioning times (seek plus rotation) and use closed and open subsystem simulation models, respectively, to show that they are superior to seek-reducing algorithms. [Worthington94] uses an open subsystem model to re-evaluate previous algorithms and show that they should be further modified to recognize <p> The Shortest-Positioning-Time-First (SPTF) algorithm uses full knowledge of processing overheads, logical-to-physical data block mappings, mechanical positioning delays, and the current read/write head location to select for service the pending request that will require the shortest positioning time <ref> [Seltzer90, Jacobson91] </ref>. The SPTF algorithm can be modified to track the contents of the disk's on-board cache and estimate a positioning time of zero for any request that can be serviced from the cache, resulting in the Shortest-Positioning-(w/Cache)-Time-First (SPCTF) algorithm [Worthington94]. <p> More sophisticated algorithms can be devised by considering several factors: 1 (1) Scheduling requests based on both seek times and rotational latencies is generally superior to scheduling based only on seek times <ref> [Seltzer90, Jacobson91] </ref>. (2) The algorithms explored in this dissertation do not exploit the difference between time-critical and time-limited requests. <p> Together with the current offset and rotation speed, these values are used to calculate the required estimate. 4. the age factor for the aged and weighted versions of the SPTF algorithm. These algorithms are described in <ref> [Seltzer90, Jacobson91, Worthington94] </ref>. 5. values that configure algorithm enhancements for exploiting request sequentiality. The enhancements fall into two categories, concatenation and sequencing, that can be applied selectively to reads and/or writes. A parameter value provides the four bits in this cross-product.
Reference: [Karedla94] <author> R. Karedla, J. S. Love, B. Wherry, </author> <title> "Caching Strategies to Improve Disk System Performance", </title> <journal> IEEE Computer, </journal> <volume> Vol. 27, No. 3, </volume> <month> March </month> <year> 1994, </year> <pages> pp. 38-46. </pages>
Reference-contexts: During the initialization phase, the device driver requests this value. During the simulation proper, the device driver breaks excessively large requests into multiple smaller requests. 6. the replacement policy for cache lines. Several policies are currently supported, including First-In-First-Out, Last-In-First-Out, pseudo-random, Least-Recently-Used (LRU), and Segmented-LRU <ref> [Karedla94] </ref>. A second parameter specifies the number of segments for the Segmented-LRU policy. 7. a flag specifying whether a space-allocating process waits for a reclaim-time dirty line flush (i.e., when the line to be replaced is dirty) or attempts to find a clean line to reclaim. 8. the write scheme.
Reference: [Kim86] <author> M. Kim, </author> <title> "Synchronized Disk Interleaving", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-35, No. 11, </volume> <month> November </month> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: The straight-forward approach, using multiple independently-addressed drives, tends to suffer from substantial load balancing problems and does not allow multiple drives to cooperate in servicing requests for large amounts of data. Disk striping (or interleaving) spreads logically contiguous data across multiple disks by hashing on the logical address <ref> [Kim86, Salem86] </ref>. The performance impact of disk striping has been studied with both open subsystem models [Kim86, Kim91] and closed subsystem models [Salem86]. The load balancing benefits of disk striping have been demonstrated with open subsystem models [Livny87, Ganger93a]. <p> Disk striping (or interleaving) spreads logically contiguous data across multiple disks by hashing on the logical address [Kim86, Salem86]. The performance impact of disk striping has been studied with both open subsystem models <ref> [Kim86, Kim91] </ref> and closed subsystem models [Salem86]. The load balancing benefits of disk striping have been demonstrated with open subsystem models [Livny87, Ganger93a]. <p> Storage subsystem models have also been used to examine other design issues in striped disk subsystems, including spindle synchronization <ref> [Kim86, Kim91] </ref> and disk/host connectivity [Ng88]. Redundant Disk Arrays As reliability requirements and the number of disks in storage subsystems increase, it becomes important to utilize on-line redundancy. The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). <p> However, the initial rotational offset of each drive in a simulation should be determined separately (e.g., by a uniform distribution), unless the the drives are intended to be rotationally synchronized (e.g., synchronous disk interleaving <ref> [Kim86] </ref>). B.1.2 On-Board Control Logic Figure B.4 shows the main on-board logic components used in modern disk drives. The CPU, which is often an embedded version of a previous generation commodity microprocessor (e.g., Intel 80386 or Motorola 68030), executes firmware to manage the disk's resources and handle requests.
Reference: [Kim91] <author> M. Kim, </author> <title> "Asynchronous Disk Interleaving: Approximating Access Delays", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 40, No. 7, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 801-810. </pages>
Reference-contexts: Disk striping (or interleaving) spreads logically contiguous data across multiple disks by hashing on the logical address [Kim86, Salem86]. The performance impact of disk striping has been studied with both open subsystem models <ref> [Kim86, Kim91] </ref> and closed subsystem models [Salem86]. The load balancing benefits of disk striping have been demonstrated with open subsystem models [Livny87, Ganger93a]. <p> Storage subsystem models have also been used to examine other design issues in striped disk subsystems, including spindle synchronization <ref> [Kim86, Kim91] </ref> and disk/host connectivity [Ng88]. Redundant Disk Arrays As reliability requirements and the number of disks in storage subsystems increase, it becomes important to utilize on-line redundancy. The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5).
Reference: [Kotz94] <author> D. Kotz, S. Toh, S. Radhakrishnan, </author> <title> "A Detailed Simulation Model of the HP 97560 Disk Drive", </title> <type> Report No. </type> <institution> PCS-TR94-220, Dartmouth College, </institution> <month> July 18, </month> <year> 1994. </year>
Reference-contexts: Some storage subsystem simulators maintain an image of the data and modify/provide it as indicated by each request. For example, such support is 12 13 useful when the storage subsystem simulator is attached to a storage management simulator (e.g., a file system or database model) <ref> [Kotz94] </ref>. Device Driver The device driver deals with device specific interactions (e.g., setting/reading controller registers to initiate actions or clear interrupts), isolating these details from the remainder of the system software. Device drivers will often re-order (i.e., schedule) and/or combine requests to improve performance.
Reference: [Lary93] <author> R. Lary, </author> <title> Storage Architect, Digital Equipment Corporation, </title> <type> Personal Communication, </type> <month> February </month> <year> 1993. </year>
Reference-contexts: In all of these cases, the priorities assigned to each request reflects the priority or the deadline of the associated processes rather than criticality. Finally, the Head-Of-Queue [SCSI93] or express <ref> [Lary93] </ref> request types present in many I/O architectures show recognition of the possible value of giving priority to certain requests. While present in many systems, such support is generally not exploited by system software.
Reference: [Lee91] <author> E. Lee, R. Katz, </author> <title> "Peformance Consequences of Parity Placement in Disk Arrays", </title> <booktitle> ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: Another parameter specifies the parity stripe unit size, which may differ from the data stripe unit size (although one must be an even multiple of the other). A parameter specifies the parity rotation type for the rotated parity redundancy mechanism (see <ref> [Lee91] </ref>). A parameter specifies the file that contains the block layout table for table-based parity. A final parameter specifies whether the distinct writes for read-modify-write parity updates can be issued as each read completes or must wait until all of the reads complete.
Reference: [Lee93] <author> E. Lee, </author> <title> "Performance Modeling and Analysis of Disk Arrays", </title> <type> Ph.D. Dissertation, </type> <institution> University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: First, the components that comprise the I/O subsystem have improved at a much slower rate than other system components. For example, microprocessor performance grows at a rate of 35-50 percent per year [Myers86], while disk drive performance grows at only 5-20 percent per year <ref> [Lee93] </ref>. As this trend continues, applications that utilize any quantity of I/O will become more and more limited by the I/O subsystem [Amdahl67]. Second, advances in technology enable new applications and expansions of existing applications, many of which rely on increased I/O capability.
Reference: [Livny87] <author> M. Livny, S. Khoshafian, H. Boral, </author> <title> "Multi-Disk Management Algorithms", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1987, </year> <pages> pp. 69-77. </pages>
Reference-contexts: The performance impact of disk striping has been studied with both open subsystem models [Kim86, Kim91] and closed subsystem models [Salem86]. The load balancing benefits of disk striping have been demonstrated with open subsystem models <ref> [Livny87, Ganger93a] </ref>. The stripe unit size (i.e., the quantity of data mapped onto one physical disk before switching to the next) is an important design parameter that has also been studied with both open subsystem models [Livny87, Reddy89] and closed subsystem models [Chen90]. <p> The stripe unit size (i.e., the quantity of data mapped onto one physical disk before switching to the next) is an important design parameter that has also been studied with both open subsystem models <ref> [Livny87, Reddy89] </ref> and closed subsystem models [Chen90]. Storage subsystem models have also been used to examine other design issues in striped disk subsystems, including spindle synchronization [Kim86, Kim91] and disk/host connectivity [Ng88].
Reference: [McKusick84] <author> M. McKusick, W. Joy, S. Le*er, R. Fabry, </author> <title> "A Fast File System for UNIX", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2, No. 3, </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: Table 5.3 lists some basic performance characteristics of this disk drive and [Worthington94a] provides a thorough breakdown of simulator configuration parameter values. The operating system is UNIX SVR4 MP, AT&T/GIS's production operating system for symmetric multiprocessing. The default file system, ufs, is based on the Berkeley fast file system <ref> [McKusick84] </ref>. The virtual memory system is similar to that of SunOS [Gingell87, Moran87] and file system caching is well integrated with the virtual memory system.
Reference: [McVoy91] <author> L. McVoy, S. Kleiman, </author> <title> "Extent-like Performance from a UNIX File System", </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1991, </year> <pages> pp. 1-11. </pages>
Reference-contexts: Many have recognized that synchronous (i.e., time-critical) file system writes generally cause more performance problems than nonsynchronous (i.e., time-limited and time-noncritical) <ref> [Ousterhout90, McVoy91, Ruemmler93] </ref>. In their extensive traces of disk activity, Ruemmler and Wilkes captured information (as flagged by the file system) indicating whether or not each request was synchronous. They found that 50-75% of disk requests are synchronous, largely due to the write-through meta-data cache on the systems traced.
Reference: [Menon91] <author> J. Menon, D. Mattson, </author> <title> "Performance of Disk Arrays in Transaction Processing Environments", </title> <type> IBM Research Report RJ 8230, </type> <month> July 15, </month> <year> 1991. </year>
Reference-contexts: Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., <ref> [Menon91, Brandwajn94, Treiber94] </ref>). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94].
Reference: [Menon92] <author> J. Menon, J. Kasson, </author> <title> "Methods for Improved Update Performance of Disk Arrays", </title> <booktitle> Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1992, </year> <pages> pp. 74-83. 130 </pages>
Reference-contexts: Open subsystem models have been used to evaluate this approach for non-redundant storage systems (e.g., [English91]) and mirrored disk systems (e.g., [Solworth91, Orji93]). Simple equations for the disk service time improvements provided by dynamically mapping parity locations and/or data locations in a RAID 5 disk array are derived in <ref> [Menon92] </ref>. Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance. Most design-stage studies of disk cache designs use open subsystem models, relying on traces of disk requests collected from user environments to reproduce realistic access patterns.
Reference: [Menon93] <author> J. Menon, J. Cortney, </author> <title> "The Architecture of a Fault-Tolerant Cached RAID Controller", </title> <booktitle> IEEE International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 76-86. </pages>
Reference: [Miller91] <author> E. Miller, R. Katz, </author> <booktitle> "Input/Output Behavior of Supercomputing Applications", Supercomputing, </booktitle> <year> 1991, </year> <pages> pp. 567-576. </pages>
Reference-contexts: There have been a few instances of very simple system-level models being used to examine the value of caching disk blocks in main memory. For example, <ref> [Miller91] </ref> uses a simple system-level model to study the effects of read-ahead and write buffering on supercomputer applications. [Busch85] examines the transaction processing performance impact of changes to hit ratios and flush policies for disk block caches located in main memory. [Dan94] studies transaction throughput as a function of the database
Reference: [Mitsuishi85] <author> A. Mitsuishi, T. Mizoguchi, T. Miyachi, </author> <title> "Performance Evaluation for Buffer-Contained Disk Units", </title> <journal> Systems and Computers in Japan, </journal> <volume> Vol. 16, No. 5, </volume> <year> 1985, </year> <pages> pp. 32-40. </pages>
Reference-contexts: Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94]. Finally, storage subsystem models have been used to evaluate the performance benefits of on-board buffers for speed-matching media and bus transfers <ref> [Mitsuishi85, Houtekamer85] </ref>. 3.4 Summary Previous work relating both to request criticality and to system-level modeling is scarce, leaving considerable room for improvement. This chapter describes this previous work and its short-comings.
Reference: [Miyachi86] <author> T. Miyachi, A. Mitsuishi, T. Mizoguchi, </author> <title> "Performance Evaluation for Memory Subsystem of Hierarchical Disk-Cache", </title> <journal> Systems and Computers in Japan, </journal> <volume> Vol. 17, No. 7, </volume> <year> 1986, </year> <pages> pp. 86-94. </pages>
Reference-contexts: Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance. Most design-stage studies of disk cache designs use open subsystem models, relying on traces of disk requests collected from user environments to reproduce realistic access patterns. For example, <ref> [Busch85, Smith85, Miyachi86] </ref> use trace-driven simulation to quantify the value of write-thru disk block caches and determine how they should be designed.
Reference: [Mogul94] <author> J. Mogul, </author> <title> "A Better Update Policy", </title> <booktitle> Summer USENIX Conference, </booktitle> <year> 1994, </year> <pages> pp. 99-111. </pages>
Reference-contexts: Researchers have noted that bursts of delayed (i.e., time-noncritical) writes caused by periodic update policies can seriously degrade performance by interfering with read requests (which tend to be more critical) <ref> [Carson92, Mogul94] </ref>. Carson and Setia argued that disk cache performance should be measured in terms of its effect on read requests. While not describing or distinguishing between classes of I/O requests, they did make a solid distinction between read and write requests based on process interaction. <p> This algorithm represents a significant reduction in the write burstiness associated with the conventional approach (as studied in <ref> [Carson92, Mogul94] </ref>) but does not completely alleviate the phenomenon.
Reference: [Moran87] <author> J. Moran, </author> <title> "SunOS Virtual Memory Implementation", </title> <booktitle> EUUG Conference, Spring 1988, </booktitle> <pages> pp. 285-300. </pages>
Reference-contexts: The operating system is UNIX SVR4 MP, AT&T/GIS's production operating system for symmetric multiprocessing. The default file system, ufs, is based on the Berkeley fast file system [McKusick84]. The virtual memory system is similar to that of SunOS <ref> [Gingell87, Moran87] </ref> and file system caching is well integrated with the virtual memory system. <p> More balanced systems will suffer from this problem only when the short-term write working set exceeds the file cache capacity. Measurements of real systems indicate that this occurs infrequently [Ruemmler93a]. Further, most modern operating systems integrate the file block cache with the virtual memory system <ref> [Gingell87, Moran87] </ref>, allowing it to utilize much of the available memory capacity in such situations. 7.5 Summary This chapter demonstrates that storage subsystem performance metrics do not, in general, correlate with overall system performance metrics.
Reference: [Mourad93] <author> A. Mourad, W.K. Fuchs, D. Saab, </author> <title> "Performance of Redundant Disk Array Organizations in Transaction Processing Environments", </title> <booktitle> International Conference on Parallel Processing, </booktitle> <volume> Vol. I, </volume> <year> 1993, </year> <pages> pp. 138-145. </pages>
Reference-contexts: Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., <ref> [Patterson88, Chen91, Hou93, Hou93b, Mourad93] </ref>) and measurements of prototypes under similar workloads (e.g., [Chen90a, Chervenak91]). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data.
Reference: [Muchmore89] <author> S. Muchmore, </author> <title> "A comparison of the EISA and MCA architectures", </title> <journal> Electronic Engineering, </journal> <month> March </month> <year> 1989, </year> <pages> pp. 91-97. </pages>
Reference-contexts: The decision as to which component uses the bus at any point in time, or arbitration, may occur before each bus cycle (as is common for system buses and general I/O buses, such as MicroChannel <ref> [Muchmore89] </ref>) or less frequently (as is common for storage subsystem buses, such as SCSI [SCSI93]). In most storage subsystem models, buses are modeled as shared resources with ownership characteristics that depend on the arbitration technique employed.
Reference: [Mummert95] <author> L. Mummert, M. Ebling, M. Satyanarayanan, </author> <title> "Exploiting Weak Connectivity for Mobile File Access", </title> <type> Unpublished Report, </type> <institution> Carnegie Mellon University, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: An interesting technique for replaying file system request traces in a realistic manner has recently been introduced and used to evaluate reintegration policies for disconnected and weakly connected distributed file systems <ref> [Mummert95] </ref>. Each event in their traces contains a file system request and an associated user identification. A trace is re-organized to consist of per-user sequences of file system requests. The replay process issues requests for each sequence in a closed-loop fashion.
Reference: [Muntz90] <author> R. Muntz, J. Lui, </author> <title> "Performance Analysis of Disk Arrays Under Failure", </title> <booktitle> International Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 162-173. </pages>
Reference-contexts: Simple models can be constructed easily and used to produce "quick-and-dirty" answers. More precise performance studies, however, must use detailed, validated simulation models (or real implementations) to avoid erroneous conclusions. For example, [Holland92] uses a more detailed storage subsystem simulator to refute the results of <ref> [Muntz90] </ref> regarding the value of piggybacking rebuild requests on user requests to a failed disk in a RAID 5 array.
Reference: [Myers86] <author> G. Myers, A. Yu, D. House, </author> <title> "Microprocessor Technology Trends", </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 74, </volume> <month> December </month> <year> 1986, </year> <pages> pp. 1605-1622. </pages>
Reference-contexts: First, the components that comprise the I/O subsystem have improved at a much slower rate than other system components. For example, microprocessor performance grows at a rate of 35-50 percent per year <ref> [Myers86] </ref>, while disk drive performance grows at only 5-20 percent per year [Lee93]. As this trend continues, applications that utilize any quantity of I/O will become more and more limited by the I/O subsystem [Amdahl67].
Reference: [NCR90] <author> NCR Corporation, </author> <title> "Using the 53C700 SCSI I/O Processor", </title> <journal> SCSI Engineering Notes, </journal> <volume> No. 822, Rev. 2.5, Part No. </volume> <pages> 609-3400634, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Data transfers also pass thru the bus adapter, which acts as a simple speed-matching buffer. The simple controller submodule, which emulates the behavior of a controller based on the NCR 53C700 <ref> [NCR90] </ref>, immediately forwards incoming storage I/O requests down the I/O path. The controller can deal with simple sequences of control activity on its own but requires assistance with major state changes, such as SCSI device disconnects and reconnects [SCSI93].
Reference: [NCR91] <institution> NCR Corporation, </institution> <note> "Class 3433 and 3434 Technical Reference", Document No. D2-0344-A, </note> <month> May </month> <year> 1991. </year>
Reference-contexts: This integer is set to zero during system initialization and is incremented once for each clock interrupt (i.e., during the clock interrupt service routine). Clock interrupts are generated by logic attached to the local peripheral bus (LPB) <ref> [NCR91] </ref>. The relevant logic contains two 16-bit counters (a working counter and a restart value), a high-frequency (1.1925 MHz) oscillating clock and some control bits. The control bits are set so that the working counter decrements once per clock cycle.
Reference: [Ng88] <author> S. Ng, D. Lang, R. Selinger, </author> <title> "Trade-offs Between Devices and Paths In Achieving Disk Interleaving", </title> <booktitle> IEEE International Symposium on Computer Architecture, </booktitle> <year> 1988, </year> <pages> pp. 196-201. </pages>
Reference-contexts: Storage subsystem models have also been used to examine other design issues in striped disk subsystems, including spindle synchronization [Kim86, Kim91] and disk/host connectivity <ref> [Ng88] </ref>. Redundant Disk Arrays As reliability requirements and the number of disks in storage subsystems increase, it becomes important to utilize on-line redundancy. The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). Both have been known for many years (e.g., [Ouchi78]).
Reference: [Ng92] <author> S. Ng, R. Mattson, </author> <title> "Maintaining Good Performance in Disk Arrays During Failure Via Uniform Parity Group Distribution", </title> <booktitle> International Symposium on High-Performance Distributed Computing, </booktitle> <month> September </month> <year> 1992, </year> <pages> pp. 260-269. 131 </pages>
Reference: [Ng92a] <author> S. Ng, </author> <title> "Prefetch Policies For Striped Disk Arrays", </title> <type> IBM Research Report RJ 9055, </type> <month> October 23, </month> <year> 1992. </year>
Reference-contexts: Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models <ref> [Ng92a, Hospodor94] </ref>. Finally, storage subsystem models have been used to evaluate the performance benefits of on-board buffers for speed-matching media and bus transfers [Mitsuishi85, Houtekamer85]. 3.4 Summary Previous work relating both to request criticality and to system-level modeling is scarce, leaving considerable room for improvement.
Reference: [Oney75] <author> W. Oney, </author> <title> "Queueing Analysis of the Scan Policy for Moving-Head Disks", </title> <journal> Journal of the ACM, </journal> <volume> Vol. 22, No. 3, </volume> <month> July </month> <year> 1975, </year> <pages> pp. 397-412. </pages>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, <ref> [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] </ref> all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation
Reference: [Orji93] <author> C. Orji, J. Solworth, </author> <title> "Doubly Distorted Mirrors", </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 307-316. </pages>
Reference-contexts: Open subsystem models have been used to evaluate this approach for non-redundant storage systems (e.g., [English91]) and mirrored disk systems (e.g., <ref> [Solworth91, Orji93] </ref>). Simple equations for the disk service time improvements provided by dynamically mapping parity locations and/or data locations in a RAID 5 disk array are derived in [Menon92]. Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance.
Reference: [Ouchi78] <author> N. Ouchi, </author> <title> "System for Recovering Data Stored in Failed Memory Unit", </title> <type> U.S. Patent #4,092,732, </type> <month> May 30, </month> <year> 1978. </year>
Reference-contexts: Redundant Disk Arrays As reliability requirements and the number of disks in storage subsystems increase, it becomes important to utilize on-line redundancy. The two most popular storage redundancy mechanisms are replication (e.g., mirroring or shadowing) and parity (e.g., RAID 5). Both have been known for many years (e.g., <ref> [Ouchi78] </ref>). Storage subsystem models have been used to evaluate design issues for both replication-based redundancy (e.g., [Bitton88, Bitton89, Copeland89, Hsiao90]) and parity-based redundancy (e.g., [Muntz90, Lee91, Menon91, Holland92, Menon92, Ng92, Cao93, Hou93, Hou93a, Stodolsky93, Treiber94, Chen95]).
Reference: [Ousterhout85] <author> J. Ousterhout, H. Da Costa, D. Harrison, J. Kunze, M. Kupfer, J. Thomp-son, </author> <title> "A Trace-Driven Analysis of the UNIX 4.2 BSD File System", </title> <booktitle> ACM Symposium on Operating System Principles, </booktitle> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference-contexts: I/O requests that prefetch unnecessary data are completely useless and are therefore classified as time-noncritical. The ufs file system prefetches file blocks sequentially until non-sequential access is detected. Measurements of UNIX file systems indicate that most user-level file reads are sequential <ref> [Ousterhout85, Baker91] </ref>. This is true of the system-level workloads used in this dissertation. The ufs file system distinguishes between three types of file block writes: synchronous, asynchronous and delayed [Ritchie86].
Reference: [Ousterhout90] <author> J. Ousterhout, </author> <title> "Why Aren't Operating Systems Getting Faster As Fast as Hardware?", </title> <booktitle> Summer USENIX Conference, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 247-256. </pages>
Reference-contexts: Many have recognized that synchronous (i.e., time-critical) file system writes generally cause more performance problems than nonsynchronous (i.e., time-limited and time-noncritical) <ref> [Ousterhout90, McVoy91, Ruemmler93] </ref>. In their extensive traces of disk activity, Ruemmler and Wilkes captured information (as flagged by the file system) indicating whether or not each request was synchronous. They found that 50-75% of disk requests are synchronous, largely due to the write-through meta-data cache on the systems traced.
Reference: [Patterson88] <author> D. Patterson, G. Gibson, R. Katz, </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)", </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: Comparisons of replication-based and parity-based redundancy have also relied largely upon storage subsystem models (e.g., <ref> [Patterson88, Chen91, Hou93, Hou93b, Mourad93] </ref>) and measurements of prototypes under similar workloads (e.g., [Chen90a, Chervenak91]). 19 Dynamic Logical-to-Physical Mapping Disk system performance can be improved in many environments by dynamically modifying the logical-to-physical mapping of data. <p> Some of these physical accesses may have to wait for others to complete, as with the write request of a read-modify-write parity update operation for parity-based redundancy <ref> [Patterson88] </ref>. When a physical access completes, a logorg-containing component calls into the logorg module, which returns a list of requests that are now complete and a list of additional physical accesses to be performed.
Reference: [Patterson93] <author> R.H. Patterson, G. Gibson, M. Satyanarayanan, </author> <title> "A Status Report on Research in Transparent Informed Prefetching", </title> <journal> ACM Operating Systems Review, </journal> <volume> Vol. 27, No. 2, </volume> <month> April </month> <year> 1993, </year> <pages> pp. 21-34. </pages>
Reference-contexts: This leads directly to the conclusion that read latencies are the most significant performance problem. Researchers are currently exploring approaches to predicting and using information about future access patterns to guide aggressive prefetching activity (e.g., <ref> [Patterson93, Griffioen94, Cao95] </ref>), hoping to utilize high-throughput storage systems to reduce read latencies. Priority-based disk scheduling has been examined and shown to improve system performance. For example, [Carey89] evaluates a priority-based SCAN algorithm where the priorities are assigned based on the process that generates the request.
Reference: [Rama92] <author> K. Ramakrishnan, P. Biswas, R. Karelda, </author> <title> "Analysis of File I/O Traces in Commercial Computing Environments", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference-contexts: In the case of system-level simulation modeling, this consists largely of system-level traces. Because of the popularity of the trace-driven storage subsystem simulation, many companies include storage request trace acquisition instrumentation in their operating system software <ref> [Rama92, Ruemmler93, Treiber94] </ref>. Instrumentation for collecting system-level traces, as described in section A.2.2, should also be included. With such instrumentation, large libraries of system-level traces can be collected from real user environments. <p> supported formats include: * The format of the disk request traces captured by the UNIX SVR4 MP device driver instrumentation described below. * The format of the extensive disk request traces from HP-UX systems described in [Ruemmler93]. * The format of the I/O traces from commercial VMS systems described in <ref> [Rama92] </ref>. * A simple ASCII format, used mainly for debug purposes. Internal Storage I/O Request Format During execution, the simulator reads request information from the trace file and translates it to the internal request format. <p> While each of these traces is actually two months in length, a single, week-long snapshot (5/30/92 to 6/6/92) is frequently used. Four of the traces come from commercial VAX T M systems running the VMS T M operating system. <ref> [Rama92] </ref> describes these traces in detail. Air-Rsv is from a transaction processing environment in which approximately 500 travel agents made airline and hotel reservations. Sci-TS is from a scientific time-sharing environment in which analytic modeling software and graphical and statistical packages were used.
Reference: [Reddy89] <author> A.L.N. Reddy, P. Banerjee, </author> <title> "An Evaluation of Multiple-Disk I/O Systems", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, No. 12, </volume> <month> December </month> <year> 1989, </year> <pages> pp. 1680-1690. </pages>
Reference-contexts: The stripe unit size (i.e., the quantity of data mapped onto one physical disk before switching to the next) is an important design parameter that has also been studied with both open subsystem models <ref> [Livny87, Reddy89] </ref> and closed subsystem models [Chen90]. Storage subsystem models have also been used to examine other design issues in striped disk subsystems, including spindle synchronization [Kim86, Kim91] and disk/host connectivity [Ng88].
Reference: [Reddy92] <author> A.L.N. Reddy, </author> <title> "A Study of I/O System Organizations", </title> <booktitle> IEEE International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 308-317. </pages>
Reference-contexts: In such environments, the cache should be designed to minimize read response times while ensuring that the cache does not fill with dirty blocks <ref> [Reddy92, Biswas93, Treiber94] </ref>. With non-volatile cache memory becoming more and more common, it becomes easy for storage subsystem designers to translate write latency problems into write throughput problems, which are much easier to handle. This leads directly to the conclusion that read latencies are the most significant performance problem. <p> Storage subsystem models have also been used to investigate design issues for write-back disk block caches located in the on-board disk drive control logic [Ruemmler93, Biswas93] or above the disk drive (e.g., in main memory or an intermediate controller) <ref> [Solworth90, Carson92, Reddy92] </ref>. Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94].
Reference: [Richardson92] <author> K. Richardson, M. Flynn, </author> <title> "TIME: Tools for Input/Output and Memory Evaluation", </title> <booktitle> Hawaii International Conference on Systems Sciences, </booktitle> <month> January </month> <year> 1992, </year> <pages> pp. 58-66. </pages>
Reference-contexts: The end purpose for this technique is to measure system performance under various workloads rather than as input to a simulator to study I/O subsystem design options. However, Haigh's tracing mechanism is very similar to my trace acquisition tool. <ref> [Richardson92] </ref> describes a set of tools under development that are intended to allow for studying I/O performance as part of the entire system. These tools are based on instruction-level traces.
Reference: [Ritchie86] <author> D. Ritchie, </author> <title> "The UNIX I/O System", UNIX User's Supplementory Document, </title> <institution> University of California, Berkeley, </institution> <month> April </month> <year> 1986. </year>
Reference-contexts: Measurements of UNIX file systems indicate that most user-level file reads are sequential [Ousterhout85, Baker91]. This is true of the system-level workloads used in this dissertation. The ufs file system distinguishes between three types of file block writes: synchronous, asynchronous and delayed <ref> [Ritchie86] </ref>. A synchronous file write immediately generates the corresponding I/O request and causes the process to block and wait until the request completes. So, synchronous file writes result in time-critical I/O requests.
Reference: [Ruemmler91] <author> C. Ruemmler, J. Wilkes, </author> <title> "Disk Shu*ing", </title> <type> Technical Report HPL-CSP-91-30, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> October 3, </month> <year> 1991. </year>
Reference-contexts: This concept can be applied in two ways to improve performance for reads and for writes, respectively. To improve read performance, one can occasionally re-organize the data blocks to place popular blocks near the center of the disk and cluster blocks that tend to be accessed together. <ref> [Ruemmler91] </ref> uses an open subsystem model to evaluate the benefits of such an approach. [Wolf89, Vongsathorn90], on the other hand, measure this approach by implementing it in real systems.
Reference: [Ruemmler93] <author> C. Ruemmler, J. Wilkes, </author> <title> "UNIX Disk Access Patterns", </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 405-420. 132 </pages>
Reference-contexts: Many have recognized that synchronous (i.e., time-critical) file system writes generally cause more performance problems than nonsynchronous (i.e., time-limited and time-noncritical) <ref> [Ousterhout90, McVoy91, Ruemmler93] </ref>. In their extensive traces of disk activity, Ruemmler and Wilkes captured information (as flagged by the file system) indicating whether or not each request was synchronous. They found that 50-75% of disk requests are synchronous, largely due to the write-through meta-data cache on the systems traced. <p> For example, [Busch85, Smith85, Miyachi86] use trace-driven simulation to quantify the value of write-thru disk block caches and determine how they should be designed. Storage subsystem models have also been used to investigate design issues for write-back disk block caches located in the on-board disk drive control logic <ref> [Ruemmler93, Biswas93] </ref> or above the disk drive (e.g., in main memory or an intermediate controller) [Solworth90, Carson92, Reddy92]. Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]). <p> This data was taken from <ref> [Ruemmler93] </ref>. The traces are described briefly in section A.2.2.1 and thoroughly in [Ruemmler93]. Information distinguishing between time-limited and time-noncritical requests is not available in the traces. <p> This data was taken from <ref> [Ruemmler93] </ref>. The traces are described briefly in section A.2.2.1 and thoroughly in [Ruemmler93]. Information distinguishing between time-limited and time-noncritical requests is not available in the traces. <p> Table 7.1 shows data regarding the criticality mixes observed during extensive measurements of three different HP-UX systems <ref> [Ruemmler93] </ref>. Table 7.2 shows more precise data extracted from the system-level traces described in section 5.2. The data in these tables demonstrate both that criticality mixes are common and that the exact mixture varies widely between workloads. <p> In the case of system-level simulation modeling, this consists largely of system-level traces. Because of the popularity of the trace-driven storage subsystem simulation, many companies include storage request trace acquisition instrumentation in their operating system software <ref> [Rama92, Ruemmler93, Treiber94] </ref>. Instrumentation for collecting system-level traces, as described in section A.2.2, should also be included. With such instrumentation, large libraries of system-level traces can be collected from real user environments. <p> The trace input module organization simplifies the addition of new formats. The currently supported formats include: * The format of the disk request traces captured by the UNIX SVR4 MP device driver instrumentation described below. * The format of the extensive disk request traces from HP-UX systems described in <ref> [Ruemmler93] </ref>. * The format of the I/O traces from commercial VMS systems described in [Rama92]. * A simple ASCII format, used mainly for debug purposes. Internal Storage I/O Request Format During execution, the simulator reads request information from the trace file and translates it to the internal request format. <p> Table A.1 lists basic statistics for these traces. Three of the traces come from Hewlett-Packard systems running HP-UX, a version of the UNIX operating system. <ref> [Ruemmler93] </ref> describes these traces in detail. Cello comes from a server at HP Labs used primarily for program development, simulation, mail and news. Snake is from a file server used primarily for compilation and editing at the University of California, Berkeley. <p> Modern disks can employ three options to reduce the probability of data corruption due to improper update ordering. 10 One option is to disable fast write entirely. A second option, used in HP-UX <ref> [Ruemmler93] </ref>, is to selectively disable fast write for each request that may require ordering with respect to a later request. This requires that the software executing on host systems recognizes that a disk uses fast write and tags each request appropriately.
Reference: [Ruemmler93a] <author> C. Ruemmler, J. Wilkes, </author> <title> "A Trace-Driven Analysis of Disk Working Set Sizes", </title> <type> Technical Report HPL-OSR-93-23, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Measurements of real systems suggest that this should be a rare occurrence <ref> [Ruemmler93a] </ref>. The exception would be when the main memory capacity is too small to maintain a balanced system (e.g., [Bennett94]). 2 This use of synchronous and asynchronous writes for metadata update sequencing represents a significant file system performance problem. <p> More balanced systems will suffer from this problem only when the short-term write working set exceeds the file cache capacity. Measurements of real systems indicate that this occurs infrequently <ref> [Ruemmler93a] </ref>.
Reference: [Ruemmler94] <author> C. Ruemmler, J. Wilkes, </author> <title> "An Introduction to Disk Drive Modeling", </title> <journal> IEEE Computer, </journal> <volume> Vol. 27, No. 3, </volume> <month> March </month> <year> 1994, </year> <pages> pp. 17-28. </pages>
Reference-contexts: Disk drives have grown in complexity over the years, using more powerful CPUs and increased on-board memory capacity to augment improvements in mechanical components. Descriptions of disk drive characteristics can be found in appendix B and in <ref> [Ruemmler94, Worthington94] </ref>. <p> be simulated as resources with a wide variety of complexities, ranging from servers with delays drawn from a single probability distribution (e.g., constant or exponential) to self-contained storage systems with bus control and speed-matching, request queueing and scheduling, on-board disk block caching, CPU processing delays and accurate mechanical positioning delays. <ref> [Ruemmler94] </ref> examines several points in this range of options. Appendix B describes the options supported in the disk module of my simulation environment. 3.3.1.1 Detail and Accuracy The level of detail in a storage subsystem model should depend largely upon the desired accuracy of the results. <p> Measured Response Time Distributions Greater insight into the validity of a storage subsystem model can be gained by comparing measured and simulated response time distributions <ref> [Ruemmler94] </ref>. The storage subsystem simulator has therefore been validated by exercising various disk drives and capturing traces of the resulting I/O activity. Using the observed inter-request delays, each traced request stream was also run through the simulator, which was configured to emulate the corresponding real subsystem. <p> The measured and simulated response time averages match to within 0.8% for all validation runs. Figures 5.1-5.5 show distributions of measured and simulated response times for a sample validation workload of 10,000 requests. As with most of our validation results, one can barely see that two curves are present. <ref> [Ruemmler94] </ref> defines the root mean square horizontal distance between the two distribution curves as a demerit figure for disk model calibration. The demerit figure for each of the curves shown is given in the corresponding caption. <p> This avoids measuring the warm-down period during which the activity still in the simulator is completed. The simulator executes as one monolithic process with a single thread of control. This differs from simulation environments that consist of communicating threads of control (e.g., <ref> [Ruemmler94] </ref>). When using a separate thread of control for each distinct sequence of activity, one can rely on the stack to maintain context information while a thread waits for simulation time to advance.
Reference: [Salem86] <author> K. Salem, G. Garcia-Molina, </author> <title> "Disk Striping", </title> <booktitle> IEEE International Conference on Data Engineering, </booktitle> <year> 1986, </year> <pages> pp. 336-342. </pages>
Reference-contexts: For example, non-zero think times might be used to represent the processing of one block before accessing the next. While rare, non-zero think times have been used in published storage subsystem research. For example, <ref> [Salem86] </ref> uses a closed workload of &lt;read block, process block, write block (optional)&gt; sequences to emulate a generic file processing application. 17 spersed with long periods of idle time (i.e., no incoming requests). With a constant number of requests in the system, there is no burstiness. <p> The straight-forward approach, using multiple independently-addressed drives, tends to suffer from substantial load balancing problems and does not allow multiple drives to cooperate in servicing requests for large amounts of data. Disk striping (or interleaving) spreads logically contiguous data across multiple disks by hashing on the logical address <ref> [Kim86, Salem86] </ref>. The performance impact of disk striping has been studied with both open subsystem models [Kim86, Kim91] and closed subsystem models [Salem86]. The load balancing benefits of disk striping have been demonstrated with open subsystem models [Livny87, Ganger93a]. <p> Disk striping (or interleaving) spreads logically contiguous data across multiple disks by hashing on the logical address [Kim86, Salem86]. The performance impact of disk striping has been studied with both open subsystem models [Kim86, Kim91] and closed subsystem models <ref> [Salem86] </ref>. The load balancing benefits of disk striping have been demonstrated with open subsystem models [Livny87, Ganger93a].
Reference: [Sandberg85] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, B. Lyon, </author> <title> "Design and Implementation of the Sun Network Filesystem", </title> <booktitle> Summer USENIX Conference, </booktitle> <month> June </month> <year> 1985, </year> <pages> pp. 119-130. </pages>
Reference-contexts: However, clock-based interrupts often arrive at regular intervals. For example, an NFS client periodically checks with the server to see if cached file data have been modified <ref> [Sandberg85] </ref>. As another example, an external sensor might interrupt the system periodically with sampled data.
Reference: [Satya86] <author> M. Satyanarayanan, </author> <title> Modeling Storage Systems, </title> <publisher> UMI Research Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1986. </year> <title> [SCSI93] "Small Computer System Interface-2", ANSI X3T9.2, Draft Revision 10k, </title> <address> March 17, </address> <year> 1993. </year>
Reference-contexts: be at the "top" of a subsystem and storage devices (e.g., disk drives) must be at the "bottom." The separation of component definitions and their interconnections greatly reduces the effort required to develop and integrate new components, as well as the effort required to understand and modify the existing components <ref> [Satya86] </ref>. The simulator also contains the host system component modules necessary to function as a system-level model at a level of detail appropriate for evaluating storage subsystem designs. Very briefly, the system-level model operates as follows. <p> Commonly modified parameters can be overridden from the command line, obviating the need to have separate parameter files for each distinct simulation run. The separation of component definitions and their interconnections (both physical and logical) is an important aspect of a large simulator <ref> [Satya86] </ref>. It greatly reduces the effort required to develop and integrate new components, as well as the effort required to understand and modify the existing components. Phase 2: Initialize simulation state After all parameters have been read and checked, the internal state is prepared for the ensuing simulation run.
Reference: [Seagate92] <author> Seagate Technology, Inc., </author> <title> "SCSI Interface Specification, Small Computer System Interface (SCSI), Elite Product Family", Document Number 64721702, Revision D, </title> <month> March </month> <year> 1992. </year>
Reference-contexts: The demerit figure for this validation run is 0.075 ms, or 0.5% of the corresponding mean response time. Characteristics of the Seagate ST41601N can be found in <ref> [Seagate92, Seagate92a] </ref>. The validation workload parameters are 50% reads, 30% sequential, 30% local [normal with 10000 sector variance], 8KB mean request size [exponential], interarrival time [uniform 0-22 ms]. 37 Disk Drive. The demerit figure for this validation run is 0.26 ms, or 2.0% of the corresponding mean response time.
Reference: [Seagate92a] <author> Seagate Technology, Inc., </author> <title> "Seagate Product Specification, ST41600N and ST41601N Elite Disc Drive, SCSI Interface", Document Number 64403103, Revision G, </title> <month> October </month> <year> 1992. </year>
Reference-contexts: The demerit figure for this validation run is 0.075 ms, or 0.5% of the corresponding mean response time. Characteristics of the Seagate ST41601N can be found in <ref> [Seagate92, Seagate92a] </ref>. The validation workload parameters are 50% reads, 30% sequential, 30% local [normal with 10000 sector variance], 8KB mean request size [exponential], interarrival time [uniform 0-22 ms]. 37 Disk Drive. The demerit figure for this validation run is 0.26 ms, or 2.0% of the corresponding mean response time.
Reference: [Seaman66] <author> P. H. Seaman, R. A. Lind, T. L. </author> <title> Wilson "An analysis of auxiliary-storage activity", </title> <journal> IBM System Journal, </journal> <volume> Vol. 5, No. 3, </volume> <year> 1966, </year> <pages> pp. 158-170. </pages>
Reference-contexts: Disk Request Schedulers Disk (or drum) request schedulers have been an important system software component since the introduction of mechanical secondary storage into computer systems over 25 years ago <ref> [Denning67, Seaman66] </ref>. Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays.
Reference: [Seaman69] <author> P. Seaman, R. Soucy, </author> <title> "Simulating Operating Systems", </title> <journal> IBM System Journal, </journal> <volume> Vol. 8, No. 4, </volume> <year> 1969, </year> <pages> pp. 264-279. </pages>
Reference-contexts: This section describes previous work relating to system-level models and their use in storage subsystem performance evaluation. <ref> [Seaman69] </ref> and [Chiu78] describe system-level modeling efforts used mainly for examining alternative system configurations (as opposed to I/O subsystem designs). [Haigh90] describes a system performance measurement technique that consists of tracing major system events.
Reference: [Seltzer90] <author> M. Seltzer, P. Chen, J. Ousterhout, </author> <title> "Disk Scheduling Revisited", </title> <booktitle> Winter USENIX Conference, </booktitle> <year> 1990, </year> <pages> pp. 313-324. </pages>
Reference-contexts: As another example, [Worthington94] determines (using an extremely detailed disk simulator) that the relative performance of seek-reducing algorithms (e.g., Shortest-Seek-Time-First, V-SCAN (R) and C-LOOK) is often opposite the order indicated by recent studies <ref> [Geist87, Seltzer90, Jacobson91] </ref>. 1 It is worth reiterating that the problems addressed in this thesis are independent of how well a storage subsystem model imitates the corresponding real storage subsystem. <p> algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation (as well as a real implementation tested in a user environment) to show that VSCAN (0.2) outperforms previous algorithms. <ref> [Seltzer90] </ref> and [Jacobson91] introduce algorithms that attempt to minimize total positioning times (seek plus rotation) and use closed and open subsystem simulation models, respectively, to show that they are superior to seek-reducing algorithms. [Worthington94] uses an open subsystem model to re-evaluate previous algorithms and show that they should be further modified <p> The Shortest-Positioning-Time-First (SPTF) algorithm uses full knowledge of processing overheads, logical-to-physical data block mappings, mechanical positioning delays, and the current read/write head location to select for service the pending request that will require the shortest positioning time <ref> [Seltzer90, Jacobson91] </ref>. The SPTF algorithm can be modified to track the contents of the disk's on-board cache and estimate a positioning time of zero for any request that can be serviced from the cache, resulting in the Shortest-Positioning-(w/Cache)-Time-First (SPCTF) algorithm [Worthington94]. <p> More sophisticated algorithms can be devised by considering several factors: 1 (1) Scheduling requests based on both seek times and rotational latencies is generally superior to scheduling based only on seek times <ref> [Seltzer90, Jacobson91] </ref>. (2) The algorithms explored in this dissertation do not exploit the difference between time-critical and time-limited requests. <p> Together with the current offset and rotation speed, these values are used to calculate the required estimate. 4. the age factor for the aged and weighted versions of the SPTF algorithm. These algorithms are described in <ref> [Seltzer90, Jacobson91, Worthington94] </ref>. 5. values that configure algorithm enhancements for exploiting request sequentiality. The enhancements fall into two categories, concatenation and sequencing, that can be applied selectively to reads and/or writes. A parameter value provides the four bits in this cross-product.
Reference: [Seltzer93] <author> M. Seltzer, K. Bostic, M. McKusick, C. Staelin, </author> <title> "An Implementation of a Log-Structured File System for UNIX", </title> <booktitle> Winter USENIX Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 201-220. </pages>
Reference-contexts: The exception would be when the main memory capacity is too small to maintain a balanced system (e.g., [Bennett94]). 2 This use of synchronous and asynchronous writes for metadata update sequencing represents a significant file system performance problem. Aggressive implementation techniques can eliminate them <ref> [Hagmann87, Seltzer93, Ganger94] </ref>. 58 HP-UX Read Requests Write Requests Trace Critical Limited/Noncritical Critical Limited/Noncritical cello 36.4 % 7.7 % 37.2 % 18.7 % snake 37.9 % 5.9 % 13.6 % 42.6 % hplajw 38.4 % 4.0 % 28.8 % 28.8 % Table 7.1: Request Criticality Breakdown for HP-UX Traces.
Reference: [Smith85] <author> A. Smith, </author> <title> "Disk Cache Miss Ratio Analysis and Design Considerations", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 3, </volume> <month> August </month> <year> 1985, </year> <pages> pp. 161-203. </pages>
Reference-contexts: Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance. Most design-stage studies of disk cache designs use open subsystem models, relying on traces of disk requests collected from user environments to reproduce realistic access patterns. For example, <ref> [Busch85, Smith85, Miyachi86] </ref> use trace-driven simulation to quantify the value of write-thru disk block caches and determine how they should be designed.
Reference: [Solworth90] <author> J. Solworth, C. Orji, </author> <title> "Write-Only Disk Caches", </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 123-132. </pages>
Reference-contexts: Storage subsystem models have also been used to investigate design issues for write-back disk block caches located in the on-board disk drive control logic [Ruemmler93, Biswas93] or above the disk drive (e.g., in main memory or an intermediate controller) <ref> [Solworth90, Carson92, Reddy92] </ref>. Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., [Menon91, Brandwajn94, Treiber94]). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94].
Reference: [Solworth91] <author> J. Solworth, C. Orji, </author> <title> "Distorted mirrors", </title> <booktitle> International Conference on Parallel and Distributed Information Systems, </booktitle> <month> December </month> <year> 1991, </year> <pages> pp. 10-17. 133 </pages>
Reference-contexts: Open subsystem models have been used to evaluate this approach for non-redundant storage systems (e.g., [English91]) and mirrored disk systems (e.g., <ref> [Solworth91, Orji93] </ref>). Simple equations for the disk service time improvements provided by dynamically mapping parity locations and/or data locations in a RAID 5 disk array are derived in [Menon92]. Disk Block Caches Disk block caches are powerful tools for improving storage subsystem performance.
Reference: [Stodolsky93] <author> D. Stodolsky, G. Gibson, M. Holland, </author> <title> "Parity Logging Overcoming the Small Write Problem in Redundant Disk Arrays", </title> <booktitle> IEEE International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 64-75. </pages>
Reference: [Teorey72] <author> T. Teorey, T. Pinkerton, </author> <title> "A Comparative Analysis of Disk Scheduling Policies", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 15, No. 3, </volume> <month> March </month> <year> 1972, </year> <pages> pp. 177-184. </pages>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). <ref> [Teorey72, Hofri80] </ref> use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation (as well as a real implementation tested in a user environment) to show that VSCAN (0.2) outperforms previous algorithms. [Seltzer90] and [Jacobson91] introduce algorithms that attempt <p> The former list is almost always very short because time-critical and time-limited requests tend to block processes, preventing them from generating additional requests. It is well-established that disk scheduling algorithms all perform equally for consistently small lists of pending requests <ref> [Teorey72, Worthington94] </ref>. On the other hand, the second list, which contains the time-noncritical requests, tends to grow large and presents solid opportunities for improvement. Indeed, the results show that C-LOOK results in lower average response times for these requests.
Reference: [Thekkath94] <author> C. Thekkath, J. Wilkes, E. Lazowska, </author> <title> "Techniques for File System Simulation", </title> <journal> Software Practice and Experience, </journal> <volume> Vol. 24, No. 11, </volume> <month> November </month> <year> 1994, </year> <pages> pp. 981-999. </pages>
Reference-contexts: Sensitivity analyses are still needed to show that this approach works and to identify appropriate values for . Mummert, et al., selected equal to 1 second and 10 seconds. <ref> [Thekkath94] </ref> promotes a similar file system trace replay approach with set to zero. <p> However, this technique does offer a healthy supply of input workloads for system-level models, which would of course need a module that simulates file system functionality <ref> [Thekkath94] </ref>. 3.3 Conventional Methodology The conventional approach to evaluating the performance of a storage subsystem design is to construct a model (analytic or simulation) of the components of interest, exercise the model with a sequence of storage I/O requests, and measure performance in terms of response times and/or throughput. <p> Process control and memory management policy code can be taken almost directly from the operating system being modeled, after altering the interfaces and structures to fit into the simulation environment <ref> [Thekkath94] </ref>. Interrupt Controller An interrupt controller tracks the pending interrupts in the system and routes them to the CPUs for service. The state of the interrupt controller is updated when new interrupts are generated and when a CPU begins handling an interrupt.
Reference: [TPCB90] <author> Transaction Processing Performance Council, </author> <title> "TPC Benchmark B, Standard Specification", </title> <type> Draft 4.1, </type> <month> August 23, </month> <year> 1990. </year>
Reference-contexts: Two sets of traces come from NCR systems executing database benchmarks with Oracle T M database software. These traces were captured using the instrumentation described above and are described more thoroughly in [Ganger93a]. Several traces, named TPCB#, were cap 107 tured on a workstation executing the TPC-B benchmark <ref> [TPCB90] </ref>. Other traces, named MultiWisc#, were captured on an 8-processor database server executing a multiuser database benchmark based on the Wisconsin benchmark [Gray91]. Several traces from each workload/configuration are kept because each trace is very short (6-50 minutes).
Reference: [Treiber94] <author> K. Treiber, J. Menon, </author> <title> "Simulation Study of Cached RAID 5 Designs", </title> <type> IBM Research Report RJ 9823, </type> <month> May 23, </month> <year> 1994. </year>
Reference-contexts: In such environments, the cache should be designed to minimize read response times while ensuring that the cache does not fill with dirty blocks <ref> [Reddy92, Biswas93, Treiber94] </ref>. With non-volatile cache memory becoming more and more common, it becomes easy for storage subsystem designers to translate write latency problems into write throughput problems, which are much easier to handle. This leads directly to the conclusion that read latencies are the most significant performance problem. <p> For example, the workload can be doubled by halving each inter-arrival time. This approach to scaling tends to increase the unrealistic concurrency described above. To avoid this increase, <ref> [Treiber94] </ref> uses trace folding, wherein the trace is sliced into periods of time that are then interleaved. The length of each section should be long enough to prevent undesired overlapping, yet short enough to prevent loss of time-varying arrival rates. <p> Disk block cache design issues specific to parity-based redundant disk arrays have also been examined with open subsystem models (e.g., <ref> [Menon91, Brandwajn94, Treiber94] </ref>). Data prefetching is an extremely important aspect of disk block caching that has also been studied with open subsystem models [Ng92a, Hospodor94]. <p> In the case of system-level simulation modeling, this consists largely of system-level traces. Because of the popularity of the trace-driven storage subsystem simulation, many companies include storage request trace acquisition instrumentation in their operating system software <ref> [Rama92, Ruemmler93, Treiber94] </ref>. Instrumentation for collecting system-level traces, as described in section A.2.2, should also be included. With such instrumentation, large libraries of system-level traces can be collected from real user environments.
Reference: [Vongsathorn90] <author> P. Vongsathorn, S. Carson, </author> <title> "A System for Adaptive Disk Rearrangement", </title> <journal> Software Practice and Experience, </journal> <volume> Vol. 20, No. 3, </volume> <month> March </month> <year> 1990, </year> <pages> pp. 225-242. </pages>
Reference-contexts: To improve read performance, one can occasionally re-organize the data blocks to place popular blocks near the center of the disk and cluster blocks that tend to be accessed together. [Ruemmler91] uses an open subsystem model to evaluate the benefits of such an approach. <ref> [Wolf89, Vongsathorn90] </ref>, on the other hand, measure this approach by implementing it in real systems.
Reference: [Wilhelm76] <author> N. Wilhelm, </author> <title> "An Anomoly in Disk Scheduling: A Comparison of FCFS and SSTF Seek Scheduling Using an Empirical Model for Disk Accesses", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 19, No. 1, </volume> <month> January </month> <year> 1976, </year> <pages> pp. 13-17. </pages>
Reference-contexts: Over the years, many researchers have introduced, modified and evaluated disk request scheduling algorithms to reduce mechanical delays. For example, <ref> [Coff72, Gotl73, Oney75, Wilhelm76, Coffman82] </ref> all use analytic open subsystem models to compare the performance of previously introduced seek-reducing scheduling algorithms (e.g., First-Come-First-Served, Shortest-Seek-Time-First and SCAN). [Teorey72, Hofri80] use open subsystem simulation models for the same purpose. [Daniel83] introduces a continuum of seek-reducing algorithms, V-SCAN (R), and uses open subsystem simulation
Reference: [Wolf89] <author> J. Wolf, </author> <title> "The Placement Optimization Program: A Practical Solution to the Disk File Assignment Problem", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1989, </year> <pages> pp. 1-10. </pages>
Reference-contexts: To improve read performance, one can occasionally re-organize the data blocks to place popular blocks near the center of the disk and cluster blocks that tend to be accessed together. [Ruemmler91] uses an open subsystem model to evaluate the benefits of such an approach. <ref> [Wolf89, Vongsathorn90] </ref>, on the other hand, measure this approach by implementing it in real systems.
Reference: [Worthington94] <author> B. Worthington, G. Ganger, Y. Patt, </author> <title> "Scheduling Algorithms for Modern Disk Drives", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 241-251. </pages>
Reference-contexts: Disk drives have grown in complexity over the years, using more powerful CPUs and increased on-board memory capacity to augment improvements in mechanical components. Descriptions of disk drive characteristics can be found in appendix B and in <ref> [Ruemmler94, Worthington94] </ref>. <p> For example, [Holland92] uses a more detailed storage subsystem simulator to refute the results of [Muntz90] regarding the value of piggybacking rebuild requests on user requests to a failed disk in a RAID 5 array. As another example, <ref> [Worthington94] </ref> determines (using an extremely detailed disk simulator) that the relative performance of seek-reducing algorithms (e.g., Shortest-Seek-Time-First, V-SCAN (R) and C-LOOK) is often opposite the order indicated by recent studies [Geist87, Seltzer90, Jacobson91]. 1 It is worth reiterating that the problems addressed in this thesis are independent of how well a <p> as a real implementation tested in a user environment) to show that VSCAN (0.2) outperforms previous algorithms. [Seltzer90] and [Jacobson91] introduce algorithms that attempt to minimize total positioning times (seek plus rotation) and use closed and open subsystem simulation models, respectively, to show that they are superior to seek-reducing algorithms. <ref> [Worthington94] </ref> uses an open subsystem model to re-evaluate previous algorithms and show that they should be further modified to recognize and exploit on-board disk caches. All of this research in disk request scheduling algorithms relied upon storage subsystem models for design-stage performance comparisons. <p> The demerit figure for this validation run is 0.07 ms, or 0.5% of the corresponding mean response time. Characteristics of the HP C2247A can be found in table 5.3 and in <ref> [HP92, Worthington94] </ref>. The validation workload parameters are 50% reads, 30% sequential, 30% local [normal with 10000 sector variance], 8KB mean request size [exponential], interarrival time [uniform 0-22 ms]. 35 Drive. The demerit figure for this validation run is 0.19 ms, or 1.2% of the corresponding mean response time. <p> In 1 The results are usually compared to those produced by the system-level simulator driven by the full system-level traces. 2 For this reason, I have used open subsystem simulation driven by traces of observed disk activity in my previous work <ref> [Ganger93a, Worthington94] </ref>. The results match reality in at least one instance. There is no corresponding trace-based workload generator for closed subsystem models, which never match the reality of most workloads. 43 this subsection, I compare two of them. The First-Come-First-Served (FCFS) algorithm services requests in arrival order. <p> The former algorithm performs no storage subsystem performance optimizations. The latter is used in many existing systems and has been shown to outperform other seek-reducing algorithms for many real workloads <ref> [Worthington94] </ref>. Figures 6.1-6.4 show the measured and predicted performance effects of replacing one algorithm with the other for the individual task workloads. Each figure contains two graphs, representing the change from FCFS to C-LOOK and the reverse, respectively. <p> The SPTF algorithm can be modified to track the contents of the disk's on-board cache and estimate a positioning time of zero for any request that can be serviced from the cache, resulting in the Shortest-Positioning-(w/Cache)-Time-First (SPCTF) algorithm <ref> [Worthington94] </ref>. In my experiments, the on-board disk cache services only read I/O requests because the HP C2247A disk drive used in the experimental system was configured to store data on the magnetic media before signaling completion for a write I/O request. <p> Among the cross-product of these options, the best storage subsystem performance (for these workloads) is offered by SPTF scheduling and aggressive prefetching after cache hits. The interactions that cause cache-awareness are obscure and easy to overlook without some indication that they represent a problem. <ref> [Worthington94] </ref> compared SPCTF and 4 For the example in the next chapter, increased request response times do not translate into increased elapsed times for the tasks. <p> I use the former algorithm because it performs no storage subsystem performance optimizations. The latter is used because it has reasonable implementation costs, is used in many existing systems, and has been shown to outperform other seek-reducing algorithms for many real workloads <ref> [Worthington94] </ref>. In addition to these conventional algorithms, this chapter examines disk scheduling algorithms that prioritize requests based on request criticality. From a short-term viewpoint, time-critical and time-limited are clearly more important to system performance than time-noncritical requests. <p> The former list is almost always very short because time-critical and time-limited requests tend to block processes, preventing them from generating additional requests. It is well-established that disk scheduling algorithms all perform equally for consistently small lists of pending requests <ref> [Teorey72, Worthington94] </ref>. On the other hand, the second list, which contains the time-noncritical requests, tends to grow large and presents solid opportunities for improvement. Indeed, the results show that C-LOOK results in lower average response times for these requests. <p> The simulator currently supports 18 different disk scheduling algorithms, including First-Come-First-Served, LBN-based and cylinder-based versions of common seek-reducing algorithms (LOOK, C-LOOK, Shortest-Seek-Time-First and V-SCAN (R)), and a variety of Shortest-Positioning-Time-First algorithms (combinations of cache-aware, aged, weighted and transfer-aware). Most of the supported scheduling algorithms are described and evaluated in <ref> [Worthington94] </ref>. 2. the information used to translate logical block numbers (LBNs) to cylinder numbers. This translation is used only for the cylinder-based seek-reducing algorithms. <p> Together with the current offset and rotation speed, these values are used to calculate the required estimate. 4. the age factor for the aged and weighted versions of the SPTF algorithm. These algorithms are described in <ref> [Seltzer90, Jacobson91, Worthington94] </ref>. 5. values that configure algorithm enhancements for exploiting request sequentiality. The enhancements fall into two categories, concatenation and sequencing, that can be applied selectively to reads and/or writes. A parameter value provides the four bits in this cross-product. <p> This would not have been possible without the support of NCR Corporation (now AT&T/GIS). While these traces are not used for the experiments reported in this dissertation, they are a crucial component of this simulation infrastructure and have been central to some of our previous storage subsystem research <ref> [Ganger93a, Worthington94] </ref>. Table A.1 lists basic statistics for these traces. Three of the traces come from Hewlett-Packard systems running HP-UX, a version of the UNIX operating system. [Ruemmler93] describes these traces in detail. Cello comes from a server at HP Labs used primarily for program development, simulation, mail and news.
Reference: [Worthington94a] <author> B. Worthington, G. Ganger, Y. Patt, </author> <title> "Scheduling for Modern Disk Drives and Non-Random Workloads", </title> <type> Report CSE-TR-194-94, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Table 5.3 lists some basic performance characteristics of this disk drive and <ref> [Worthington94a] </ref> provides a thorough breakdown of simulator configuration parameter values. The operating system is UNIX SVR4 MP, AT&T/GIS's production operating system for symmetric multiprocessing. The default file system, ufs, is based on the Berkeley fast file system [McKusick84].
Reference: [Worthington95] <author> B. Worthington, G. Ganger, Y. Patt, J. Wilkes, </author> <title> "On-Line Extraction of SCSI Disk Drive Parameters", </title> <booktitle> ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. 146-156. </pages>
Reference-contexts: Modern disk drives use a wide variety of cache management policies, many of which are described below. Although the policies used by a given disk are rarely outlined in publically available specifications, there are approaches to identifying them empirically <ref> [Worthington95] </ref>. Most disks partition the available memory into multiple cache lines, referred to as segments, to better service multiple streams of sequential requests. Commonly, each segment is the same size. In some disks, this size matches the number of sectors per track.
Reference: [Worthington95a] <author> B. Worthington, </author> <title> "Aggressive Centralized and Distributed Scheduling of Disk Requests", </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, </address> <year> 1995. </year>
Reference-contexts: While present in many systems, such support is generally not exploited by system software. Currently, researchers are exploring how such support can be utilized by a distributed disk request scheduler that concerns itself with both mechanical latencies and system priorities <ref> [Worthington95a] </ref>. 3.2 System-Level Modeling This dissertation, in part, proposes the use of system-level models for evaluating I/O subsystem designs. <p> In a growing number of environments, computer system performance is determined largely by I/O performance. This dictates a change in paradigm with a much greater focus on data movement. 1 Some of these factors are explored in <ref> [Worthington95a] </ref>. APPENDICES 84 85 APPENDIX A Detailed Description of the Simulation Infrastructure This appendix describes the simulation infrastructure used for performance evaluation of storage subsystem designs. The simulator itself is described, including the general simulation environment, the storage subsystem components and the host system components. <p> Of course, there are dependencies between these components (e.g., write data cannot be transferred to the media before it is received from the host or controller), but the disk can still exploit concurrency between separate requests <ref> [Worthington95a] </ref>. B.2 Disk Drive Module The disk drive module models all of the aspects of modern disk drives described in section B.1. It is organized as five submodules (see figure B.5) and uses instances of the general queue/scheduler module (see section A.1.2).
References-found: 124

