URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn7.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: MATRIX MULTIPLICATION ON THE INTEL TOUCHSTONE DELTA*  
Author: STEVEN HUSS-LEDERMAN ELAINE M. JACOBSON ANNA TSAO and GUODONG ZHANG 
Affiliation: Center  
Note: Appeared in Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing, Norfolk, Virginia, March 22-24, 1993. Expanded version appeared as Supercomputing Research  
Date: January 4, 1993  
Pubnum: Technical Report SRC-TR-93-101.  
Abstract: Matrix multiplication is a key primitive in block matrix algorithms such as those found in LAPACK. We present results from our study of matrix multiplication algorithms on the Intel Touchstone Delta, a distributed memory message-passing architecture with a two-dimensional mesh topology. We obtain an implementation that uses communications primitives highly suited to the Delta and exploits the single node assembly-coded matrix multiplication. Our algorithm is completely general, able to deal with arbitrary mesh aspect ratios and matrix dimensions, and has achieved parallel efficiency of 86% with overall peak performance in excess of 8 Gflops on 256 nodes for an 8800 fi 8800 matrix. We describe our algorithm design and implementation, and present performance results that demonstrate scalability and robust behavior over varying mesh topologies. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, E., Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, & D. Sorensen, </author> <title> LAPACK: A portable linear algebra library for high-performance computers, </title> <booktitle> Proceedings, Supercomputing `90, </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990, </year> <pages> pp. 2-11. </pages>
Reference-contexts: A stable and fairly uniform set of appropriate kernels well-suited to most serial machines makes these implementations hard to beat. This is not yet the case for parallel architectures, where the set of primitives can so readily change from one machine to another, but the block algorithms of LAPACK <ref> [1] </ref> are one step in this direction. In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology, and develop an implementation on one such machine, the i860-based Intel Touchstone Delta.
Reference: 2. <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigensolvers, </title> <journal> Adv. Appl. Math. </journal> <volume> 13 (1992), </volume> <pages> 253-261. </pages>
Reference-contexts: First, as part of the Parallel Research on Invariant Subspace Methods (PRISM) project, we are currently investigating the symmetric Invariant Subspace Decomposition Algorithm (symmetric ISDA), a new technique for finding all the eigenvalues and eigenvectors of a dense real symmetric matrix, which is based predominantly on matrix multiply operations <ref> [2, 13, 17] </ref>.
Reference: 3. <author> Cannon, L.E., </author> <title> A cellular computer to implement the Kalman filter algorithm, </title> <type> Ph.D. Thesis (1969), </type> <institution> Montana State University. </institution>
Reference-contexts: Thus, our first general rule is to strive for large granularity in local computations. We consider three standard algorithms: 1D-systolic (1D) [11], 2D-systolic or Cannon's algorithm (2D) <ref> [3, 11] </ref>, and Broadcast-Multiply-Roll (BMR) [10]. We quickly review the algorithms while examining their communication requirements. Descriptions of the algorithms could be given from the matrix point-of-view, i.e., seeing first the matrix layout and marking entries with the processor location where the entry is stored.
Reference: 4. <author> Choi, J., J. J. Dongarra, R. Pozo, & D. W. Walker, </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proceedings, Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, </booktitle> <address> Virginia, </address> <month> October 19-21, </month> <title> 1992), </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: Finally, while our orientation was machine-specific and aimed at optimization rather than portability, the resulting kernel could still be useful to future portable distributed memory software such as ScaLAPACK <ref> [4] </ref>, being an optimized version of one of the core Level 3 BLAS building blocks [6]. We analyze three matrix multiply algorithms and their suitability for the Delta, describing our design objectives, decisions, and implementations. Performance results are presented that demonstrate scalability and robust behavior even on non-optimal mesh configurations. <p> We do not consider these here, but expect to report on them in future papers. Data Distribution. As mentioned earlier, the above descriptions use a contiguous distribution of data across processors. Another technique is the block scattered layout <ref> [4] </ref>, where successive matrix blocks of size r fi s are assigned to successive physical processors in both matrix dimensions.
Reference: 5. <author> Choi, J., J. J. Dongarra, & D. W. Walker, </author> <title> Level 3 BLAS for distributed memory concurrent computers, </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing (Saint Hilaire du Touvet, </booktitle> <address> France, September 7-8, 1992), </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology, and develop an implementation on one such machine, the i860-based Intel Touchstone Delta. While some studies of distributed matrix multiplication have been made <ref> [5, 10, 19] </ref>, we felt that a separate focused effort was required in our work for several reasons. <p> The r and s are application-dependent panel widths determined to be advantageous for local computation, while scattering the blocks across physical processors helps maintain good load balance in applications such as LU factorization. However, as noted in <ref> [5] </ref>, for the important basic operation of matrix multiplication, the contiguous layout is optimal; spreading the data to the level achieved by the block scattered scheme does not aid in load balancing and generally incurs significant extra communication overhead.
Reference: 6. <author> Dongarra, J. J., J. Du Croz, S. Hammarling, & I. Duff, </author> <title> A set of level 3 basic linear algebra subprograms, </title> <journal> ACM Transactions on Mathematical Software 16 (1990), </journal> <pages> 1-17. </pages>
Reference-contexts: Finally, while our orientation was machine-specific and aimed at optimization rather than portability, the resulting kernel could still be useful to future portable distributed memory software such as ScaLAPACK [4], being an optimized version of one of the core Level 3 BLAS building blocks <ref> [6] </ref>. We analyze three matrix multiply algorithms and their suitability for the Delta, describing our design objectives, decisions, and implementations. Performance results are presented that demonstrate scalability and robust behavior even on non-optimal mesh configurations.
Reference: 7. <author> Dongarra, J. J., I. S. Duff, D. C. Sorensen, & H. A. van der Vorst, </author> <title> Solving Linear Systems on Vector and Shared Memory Computers, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Second, our present target machine represented a new architecture on which the costs of data motion were not clear. Just as reducing the cost of hierarchical memory data access is crucial to the design of efficient algorithms for vector and shared memory machines <ref> [7] </ref>, the same is true in a multicomputer environment, where we want to reduce the cost of data movement across the distributed memory.
Reference: 8. <author> Dongarra, J., C. B. Moler, J. R. Bunch, & G. W. Stewart, </author> <title> LINPACK User's Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: 1. Introduction Multiplication of two matrices is one of the most basic operations of scientific computing. Versions for serial computers have long been based on primitives embodied in the kernels of standard software packages, such as LINPACK <ref> [8] </ref>. A stable and fairly uniform set of appropriate kernels well-suited to most serial machines makes these implementations hard to beat.
Reference: 9. <author> Dunigan, T. H., </author> <title> Communication performance of the Intel Touchstone Delta mesh, </title> <type> Technical Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory (January, </institution> <year> 1992). </year>
Reference-contexts: These choices result in communication costs for matrix multiplication equal to that of the optimal contiguous layout. 3. The Intel Touchstone Delta In this section we present an overview of the Intel Delta architecture, focusing primarily on its characteristics that influenced our choice of algorithm. For further details, see <ref> [9] </ref>, [12], [14], and [18]. The Delta is a descendant of the earlier Intel iPSC/1, iPSC/2, and iPSC/860 (Gamma) machines.
Reference: 10. <author> Fox, G., S. Otto, & A. Hey, </author> <title> Matrix algorithms on a hypercube I: Matrix multiplication, </title> <booktitle> Parallel Computing 4 (1987), </booktitle> <pages> 17-31. </pages>
Reference-contexts: In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology, and develop an implementation on one such machine, the i860-based Intel Touchstone Delta. While some studies of distributed matrix multiplication have been made <ref> [5, 10, 19] </ref>, we felt that a separate focused effort was required in our work for several reasons. <p> Thus, our first general rule is to strive for large granularity in local computations. We consider three standard algorithms: 1D-systolic (1D) [11], 2D-systolic or Cannon's algorithm (2D) [3, 11], and Broadcast-Multiply-Roll (BMR) <ref> [10] </ref>. We quickly review the algorithms while examining their communication requirements. Descriptions of the algorithms could be given from the matrix point-of-view, i.e., seeing first the matrix layout and marking entries with the processor location where the entry is stored.
Reference: 11. <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: Thus, our first general rule is to strive for large granularity in local computations. We consider three standard algorithms: 1D-systolic (1D) <ref> [11] </ref>, 2D-systolic or Cannon's algorithm (2D) [3, 11], and Broadcast-Multiply-Roll (BMR) [10]. We quickly review the algorithms while examining their communication requirements. <p> Thus, our first general rule is to strive for large granularity in local computations. We consider three standard algorithms: 1D-systolic (1D) [11], 2D-systolic or Cannon's algorithm (2D) <ref> [3, 11] </ref>, and Broadcast-Multiply-Roll (BMR) [10]. We quickly review the algorithms while examining their communication requirements. Descriptions of the algorithms could be given from the matrix point-of-view, i.e., seeing first the matrix layout and marking entries with the processor location where the entry is stored.
Reference: 12. <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Optimizing communication primitives on the Intel Touchstone Delta, </title> <type> Technical Report, </type> <note> Supercomputing Research Center (to appear). </note>
Reference-contexts: The Intel Touchstone Delta In this section we present an overview of the Intel Delta architecture, focusing primarily on its characteristics that influenced our choice of algorithm. For further details, see [9], <ref> [12] </ref>, [14], and [18]. The Delta is a descendant of the earlier Intel iPSC/1, iPSC/2, and iPSC/860 (Gamma) machines. <p> For our purposes here it is enough to note that skew is thus costly, with its cost increasing with p. More details on these primitives can be found in <ref> [12] </ref>. 4. Design and Implementation In this section we compute comparative cost models for the three candidate algorithms and choose the most promising one for initial implementation. For further simplification, we neglect skewing in our costing of 2D; our conclusions remain the same when it is taken into account. <p> We determined techniques for efficient implementation of these primitives using basic information on Delta behavior, such as that reported in Section 3, as well as examination of its specific performance on these operations. See <ref> [12] </ref> for details on the design and optimization of these communication kernels on the Delta.
Reference: 13. <author> Huss-Lederman, S., A. Tsao, & G. Zhang, </author> <title> A parallel implementation of the Invariant Subspace Decomposition Algorithm for dense symmetric matrices, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Norfolk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <pages> pp. 367-374, </pages> <note> (also PRISM Working Note #9, also appears as Technical Report SRC-TR-93-091, </note> <institution> Supercomputing Research Center, </institution> <year> 1993). </year> <month> 8 </month>
Reference-contexts: First, as part of the Parallel Research on Invariant Subspace Methods (PRISM) project, we are currently investigating the symmetric Invariant Subspace Decomposition Algorithm (symmetric ISDA), a new technique for finding all the eigenvalues and eigenvectors of a dense real symmetric matrix, which is based predominantly on matrix multiply operations <ref> [2, 13, 17] </ref>.
Reference: 14. <author> Intel Supercomputing Systems Division, </author> <title> Intel Touchstone Delta System Description, Advanced Information, Intel Corpo--ration (February, </title> <year> 1991). </year>
Reference-contexts: The Intel Touchstone Delta In this section we present an overview of the Intel Delta architecture, focusing primarily on its characteristics that influenced our choice of algorithm. For further details, see [9], [12], <ref> [14] </ref>, and [18]. The Delta is a descendant of the earlier Intel iPSC/1, iPSC/2, and iPSC/860 (Gamma) machines. While these had a hypercube topology, the Delta is a two-dimensional mesh, and represents the alternate design choice of having more wires between connected processors, but less connectivity than the earlier machines.
Reference: 15. <institution> Intel Supercomputing Systems Division, Paragon Supercomputers, Order Number 203/6/92/10K/GA, Intel Corporation (1992). </institution>
Reference-contexts: While these had a hypercube topology, the Delta is a two-dimensional mesh, and represents the alternate design choice of having more wires between connected processors, but less connectivity than the earlier machines. The Delta is a prototye for Intel's follow-on machine, the Paragon <ref> [15] </ref>. Developed for the Concurrent Supercomputing Consortium and housed at the California Institute of Technology, the Delta consists of 528 i860 XR compute processors. One important feature of the i860 that influenced our design is that it has a write-back cache, requiring the processor to mediate all memory requests.
Reference: 16. <author> Kuck & Associates, Inc., CLASSPACK, </author> <title> Basic Math Library User's Guide, Release 1.2, Document # 9202003, </title> <year> 1992. </year>
Reference-contexts: As usual, the crucial issue for high efficiency is data locality; we must strive for maximum use of data in floating point computations, while minimizing costs of data transfer. To this end, we utilize the highly optimized assembly-coded DGEMM on single nodes <ref> [16] </ref>. It is capable of sustaining 36:5 Mflops/sec of the 40 Mflops/sec possible for standard double precision matrix multiplication (empirically determined using Release 1.4 of Intel NX/M OS and related software), and the likelihood of attaining this rate increases sharply as the size of the matrices is increased.
Reference: 17. <author> Huss-Lederman, S., A. Tsao, & T. Turnbull, </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eigen-values, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center (1991). </institution>
Reference-contexts: First, as part of the Parallel Research on Invariant Subspace Methods (PRISM) project, we are currently investigating the symmetric Invariant Subspace Decomposition Algorithm (symmetric ISDA), a new technique for finding all the eigenvalues and eigenvectors of a dense real symmetric matrix, which is based predominantly on matrix multiply operations <ref> [2, 13, 17] </ref>.
Reference: 18. <author> Littlefield, R., </author> <title> Characterizing and tuning communications performance for real applications, presentation overheads, </title> <booktitle> Proceedings, First Intel Delta Applications Workshop, </booktitle> <month> CCSF-14-92 (February, </month> <year> 1992), </year> <title> Caltech Concurrent Supercomputing Facilities, </title> <address> Pasadena, California, </address> <year> 1992, </year> <pages> pp. 179-190. </pages>
Reference-contexts: The Intel Touchstone Delta In this section we present an overview of the Intel Delta architecture, focusing primarily on its characteristics that influenced our choice of algorithm. For further details, see [9], [12], [14], and <ref> [18] </ref>. The Delta is a descendant of the earlier Intel iPSC/1, iPSC/2, and iPSC/860 (Gamma) machines. While these had a hypercube topology, the Delta is a two-dimensional mesh, and represents the alternate design choice of having more wires between connected processors, but less connectivity than the earlier machines.
Reference: 19. <author> Mathur, K. K. & S. L. Johnsson, </author> <title> Multiplication of matrices of arbitrary shape on a data parallel computer (1992), Thinking Machines Corporation, </title> <type> preprint, </type> <note> also released as Technical Report TR-216. 9 </note>
Reference-contexts: In this paper we study algorithms for matrix multiplication on distributed-memory message-passing architectures with a two-dimensional mesh topology, and develop an implementation on one such machine, the i860-based Intel Touchstone Delta. While some studies of distributed matrix multiplication have been made <ref> [5, 10, 19] </ref>, we felt that a separate focused effort was required in our work for several reasons.
References-found: 19

