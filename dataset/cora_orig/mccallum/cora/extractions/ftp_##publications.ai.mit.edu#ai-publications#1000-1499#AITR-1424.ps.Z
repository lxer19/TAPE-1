URL: ftp://publications.ai.mit.edu/ai-publications/1000-1499/AITR-1424.ps.Z
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Title: Explorations of the Practical Issues of Learning Prediction-Control Tasks Using Temporal Difference Learning Methods  
Author: Charles L. Isbell Tomaso Poggio, Campbell L. Searle, 
Degree: submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Computer Science at the  all rights reserved Author Charles Isbell Department of Electrical Engineering and Computer Science Certified by  Thesis Supervisor Accepted by  Chairman, EECS Committee on Graduate Students  
Date: December 1992  
Affiliation: Massachusetts Institute of Technology  Massachusetts Institute of Technology  
Abstract-found: 0
Intro-found: 0
Reference: <author> D. H. Ackley, G. E. Hinton and T. J. Sejnowski. </author> <title> A Learning Algorithm for Boltzman Machines. </title> <journal> Cognitive Systems, </journal> (9):147-169, 1985. 
Reference: <author> A. G. Barto. </author> <title> Learning by Statistical Cooperation of Self-Interested Neuron-Like Computing Elements. </title> <journal> Human Neurobiology, </journal> (4):229-256, 1985. 
Reference: <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike Elements That Can Solve Difficult Learning Control Problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> (13):834-846, 1983. 
Reference: <author> D. S. Broomhead and David Lowe. </author> <title> Multivariate Functional Interpolation and Adaptive Networks. </title> <journal> Complex Systems, </journal> (2):321-355, 1988. 
Reference-contexts: Only the coefficients, c i , must be learned in this case. This reduces the process of learning to a simple linear problem solvable by matrix inversion <ref> (Broomhead and Lowe, 1988) </ref>. 8 The RBF network can be generalized by allowing fewer centers than training examples. The centers of this generalized radial basis function network (GRBF) are usually initialized to some subset of the training examples. The centers can then remain fixed or be allowed to change.
Reference: <author> J. Christensen. </author> <title> Learning Static Evaluation Functions by Linear Regression. </title> <booktitle> In T. </booktitle>
Reference: <author> M. Mitchell, J. G. Carbonell and R. S. Michalski (Eds.), </author> <title> Machine Learning: A Guide to Current Research, </title> <address> Boston: </address> <publisher> Kluwer Academic, </publisher> <year> 1986. </year>
Reference: <author> J. Christensen and R. E. Korf. </author> <title> A Unified Theory of Heuristic Evaluation Functions and its Application to Learning. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> 148-152. </pages> <address> Philadelphia: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference: <author> P. Dayan. </author> <title> Temporal Differences: TD (l ) for General l . Machine Learning, </title> <publisher> in press, </publisher> <year> 1991. </year>
Reference-contexts: For other values of l :0 &lt; l &lt; 1, it is harder to characterize exactly what is happening; however, there is some interpolation between the maximum- likelihood estimate and the minimial root squared error calculated by the traditional supervised learning procedure <ref> (Dayan, 1991) </ref>. 2.4 Prediction versus Control From Samuels checkers-playing program to Tesauros backgammon player, temporal difference learning algorithms have been used with some success in the domain of games. Learning in this domain, however, is not just a matter of prediction.
Reference: <author> E. V. Denardo. </author> <title> Dynamic Programming: Models and Applications. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference: <author> T. G. Dietterich and R. S. Michalski. </author> <title> Learning to Predict Sequences. </title> <editor> In R. </editor> <publisher> S. </publisher>
Reference: <editor> Michalski, J. G. Carbonell and T. M. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol II. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year> <note> 73 G. </note> <author> E. Hinton. </author> <title> Connectionist Learning Procedures. </title> <type> Technical Report CS-87-115, CMU, </type> <year> 1987. </year>
Reference: <author> J. H. Holland. </author> <title> Escaping Brittleness: The Possibilities of General-Purpose Learning Algorithms Applied to Parallel Rule-Based Systems. </title> <editor> In R. S. Michalski, J. G. Carbonell and T. M. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach Vol II. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference: <author> Anya C. Hurlbert. </author> <title> The Computation of Color. A. I. </title> <type> Technical report 1154, </type> <institution> MIT, </institution> <year> 1989. </year>
Reference: <author> M. Jones. </author> <title> Using Recurrent Networks for Dimensionality Reduction . Masters Thesis. </title> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <year> 1992. </year>
Reference-contexts: There is a class of networks that uses this principle called recurrent networks (see figure 5.1). These networks are equivalent to feedforward networks in that they can approximate any function arbitrarily well. Beyond this, they have been used for various tasks, including dimensionality reduction, with some success <ref> (Jones, 1992) </ref>. This formulation of a function can be useful for other reasons as well. For some problems, v v x) may be an easier function to learn than v v x) and this ease of learning is worth the tradeoff in computation time.
Reference: <author> J. G. Kemeny and J. L. Snell. </author> <title> Finite Markov Chains. </title> <address> New York: SpringerVerlag, </address> <year> 1976. </year>
Reference: <author> R. P. Lippmann. </author> <title> An Introduction to Computing with Neural Nets. </title> <journal> IEEE ASSP Magazine, </journal> <pages> 4-22, </pages> <year> 1987. </year>
Reference: <author> Tomaso Poggio and Federico Girosi. </author> <title> A Theory of Networks for Approximation and Learning. </title> <editor> A. I. </editor> <volume> Memo 1140, </volume> <publisher> MIT, </publisher> <year> 1989. </year>
Reference: <author> Tomaso Poggio and Federico Girosi. </author> <title> Extensions of a Theory of Networks for Approximation and Learning: Dimensionality Reduction and Clustering. </title> <editor> A. I. </editor> <volume> Memo 1167, </volume> <publisher> MIT, </publisher> <year> 1990. </year>
Reference: <author> Tomaso Poggio and Federico Girosi. </author> <title> Regularization Algorithms for Learning that Are Equivalent to Multilayer Networks: Dimensionality Reduction and Clustering. </title>
Reference: <author> A. I. </author> <title> Memo 1167, </title> <publisher> MIT, </publisher> <year> 1990. </year>
Reference: <author> D. E. Rumelhart, G. E. Hinton and R. J. Williams. </author> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D. Rumelhart and J. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing, Vol I. </booktitle> <publisher> MIT Press, 1986 74 A. </publisher> <editor> L. Samuel. </editor> <title> Some Studies in Machine Learning Using the Game of Checkers. </title> <journal> IBM Journal on research and Development </journal> , (3):210-229, 1959. Reprinted in E. 
Reference-contexts: This learning rule is a generalization of the delta or Widrow-Hoff rule <ref> (Rumelhart et al, 1986) </ref>. This is a clear supervised learning algorithm with each D v w t depending directly on z . This falls prey to the disadvantages noted earlier.
Reference: <editor> A. Feigenbaum and J. Feldman (Eds.), </editor> <booktitle> Computers and Thought . New York: </booktitle> <publisher> McGraw-Hill. </publisher>
Reference: <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning . Doctoral Dissertation. </title> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, </institution> <year> 1984. </year>
Reference: <author> R. S. Sutton. </author> <title> Learning to Predict by Methods of Temporal Difference. </title> <journal> Machine Learning, </journal> (3):9-44, 1988. 
Reference-contexts: In this thesis, I will use the General Radial Basis Function (GRBF) and HyperBF networks (Poggio and Girosi, 1990), to discuss algorithms for training neural networks. In particular, I will discuss and propose evaluation criteria for the TD ( l) temporal difference learning algorithm <ref> (Sutton, 1988) </ref>. As a training algorithm it is provably equivalent to the more widely-used supervised learning algorithms; however, questions 2 remain about its usefulness and efficiency with more complex real-world problems. <p> Instead of changing network parameters by means of the difference between predicted and actual outputs, these methods update parameter values by means of the difference between temporally successive predictions <ref> (Sutton, 1988) </ref>. Feedback is usually provided by a teacher at the end of a series of predictions. This combines the principle of temporal (and spatial) coherencethe notion that the environment is stable and smooth and any function that predicts behavior should reflect that notionwith the power of an external teacher. <p> such as speech recognitiona traditional domain of supervised learning methodshumans are not faced with simple pattern-classification pairs, but a series of patterns that all contribute to the same classification. 2.2 Derivation of the TD (l) learning algorithm In the following two subsections we derive the TD (l ) learning algorithm <ref> (Sutton, 1988) </ref>. First we introduce a temporal difference learning procedure that is directly derived from the classical general delta learning rule and induces the same weight changes. <p> The TD (0) procedure moves toward this maximum-likelihood estimate with repeated presentations of the training data <ref> (Sutton, 1988) </ref>.
Reference: <author> R. S. Sutton and A. G. Barto. </author> <title> Toward a Modern Theory of Adaptive Networks: Expectation and Prediction. </title> <journal> Psychological Review, </journal> (88):135-171, 1981(a). 
Reference: <author> R. S. Sutton and A. G. Barto. </author> <title> An Adaptive Network that Constructs and Uses an Internal Model of its Environment. </title> <journal> Cognitive and Brain Theory </journal> , (4):217-246, 1981(b). 
Reference: <author> R. S. Sutton and A. G. Barto. </author> <title> A Temporal Difference Model of Classical Conditioning. </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 355-378. </pages> <address> Seattle, WA: </address> <publisher> Lawrence Erlbaum, </publisher> <year> 1987. </year>
Reference: <author> Gerald Tesauro. </author> <title> Practical Issues in Temporal Difference Learning. </title> <note> To appear in Machine Learning, </note> <year> 1992. </year>
Reference: <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD Thesis. </type> <institution> University of Cambridge, </institution> <address> England, </address> <year> 1989. </year>
Reference: <author> B. Widrow and M. E. Hoff. </author> <title> Adaptive Switching Circuits. </title> <booktitle> 1960 WESCON Convention Record, Part IV, </booktitle> <pages> 96-104, </pages> <year> 1960. </year>
Reference: <author> B. Widrow and S. D. Stearns. </author> <title> Adaptive Signal Processing . Englewood Cliffs, </title> <address> NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1985. </year> <note> 75 R. </note> <author> J. Williams. </author> <title> Reinforcement Learning in Connectionist Networks: </title> <note> A Mathematical analysis . Technical Report 8605, </note> <institution> University of California, San Diego, Institute for Cognitive Science, </institution> <year> 1986. </year>
Reference-contexts: After a finite number of repeated presentations, it is well-known that TD (1) converges in such a way so as to minimize the root squared error between its predictions and the actual outcomes in the training set <ref> (Widrow and Stearns, 1985) </ref>. But what does TD (0) compute after a finite number of repeated presentations? Suppose that one knows that the training data to be used is generated by some Markov process.
Reference: <author> I. H. Witten. </author> <title> An Adaptive Optimal Controller for Discrete-Time Markov Environments. </title> <journal> Information ad Control, </journal> (34):286-295, 1977.
References-found: 32

