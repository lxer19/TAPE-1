URL: http://cag-www.lcs.mit.edu/~rinard/paper/ijhsc97.ps
Refering-URL: http://cag-www.lcs.mit.edu/~rinard/paper/index.html
Root-URL: 
Email: martin@cs.ucsb.edu  
Title: Locality Optimizations for Parallel Computing Using Data Access Information  
Author: Martin C. Rinard 
Address: Santa Barbara, California 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Given the large communication overheads characteristic of modern parallel machines, optimizations that improve locality by executing tasks close to data that they will access may improve the performance of parallel computations. This paper describes our experience automatically applying locality optimizations in the context of Jade, a portable, implicitly parallel programming language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to declare how parts of the program access data. The Jade implementation uses this data access information to automatically extract the concurrency and apply locality optimizations. We present performance results for several Jade applications running on the Stanford DASH machine. We use these results to characterize the overall performance impact of the locality optimizations. In our application set the locality optimization level has little effect on the performance of two of the applications and a large effect on the performance of the rest of the applications. We also found that, if the locality optimization level had a significant effect on the performance, the maximum performance was obtained when the programmer explicitly placed tasks on processors rather than relying on the scheduling algorithm inside the Jade implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: These results suggest that it should be possible to improve the Jade scheduler by making it less eager to move tasks off their target processors in an attempt to improve the load balance. 6. Related Work. Chandra, Gupta and Hennessy <ref> [1] </ref> have designed, implemented and measured a scheduling algorithm for the parallel language COOL running on DASH. The goal of the scheduling algorithm is to enhance the locality of the computation while balancing the load.
Reference: [2] <author> I. Duff, R. Grimes, and J. Lewis. </author> <title> Sparse matrix problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The data set for Ocean is a square 192 by 192 grid. The timing runs omit an initial I/O phase. For Panel Cholesky the timing runs factor the BCSSTK15 matrix from the Harwell-Boeing sparse matrix benchmark set <ref> [2] </ref>. The performance numbers only measure the actual numerical factorization, omitting initial I/O and a symbolic factorization phase. In practice the overhead of the initial I/O and symbolic factorization would be amortized over many factorizations of matrices with identical structure. 5. Experimental Results.
Reference: [3] <author> R. Fowler and L. Kontothanassis. </author> <title> Improving processor and cache locality in fine-grain parallel computations using object-affinity scheduling and continuation passing. </title> <type> Technical Report 411, </type> <institution> Dept. of Computer Science, University of Rochester, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The annotations in Jade programs, on the other hand, provide a complete specification of which objects each task will access and how it will access them. The Jade implementation uses this information for multiple purposes: both to extract the concurrency and to apply locality optimizations. Fowler and Kontothanassis <ref> [3] </ref> have developed an explicitly parallel system that uses object affinity scheduling to enhance locality. The system associates objects with processors. Each task and thread has a location field that specifies the processor on which to execute the task or thread.
Reference: [4] <author> J. Harris, S. Lazaratos, and R. Michelena. </author> <title> Tomographic string inversion. </title> <booktitle> In 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, </booktitle> <pages> pages 82-85, </pages> <year> 1990. </year>
Reference-contexts: Applications. The application set consists of three complete scientific applications and one computational kernel. The complete applications are Water, which evaluates forces and potentials in a system of water molecules in the liquid state, String <ref> [4] </ref>, which computes a velocity model of the geology between two oil wells, and Ocean, which simulates the role of eddy and boundary currents in influencing large-scale ocean movements. The computational kernel is Panel Cholesky, which factors a sparse positive-definite matrix.
Reference: [5] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: It also evaluates the performance impact of the locality heuristic by presenting the results obtained by executing several complete Jade applications. The collected data allow us to characterize the performance impact on this set of applications. Our execution platform is the Stanford DASH machine <ref> [5] </ref>. Although all of the research presented in this paper was performed in the context of Jade, the results should be of interest to several sectors of the parallel computing community. <p> We compute the time spent in tasks by reading the 60ns counter on DASH <ref> [5] </ref> just before each task executes, reading it again just after the task completes, then using the difference to update a variable containing the running sum of the total time spent executing tasks. Figures 6 through 9 plot the time spent executing tasks for each of the applications.
Reference: [6] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Mowry, Lam and Gupta have evaluated the performance impact of prefetching in the context of a shared memory multiprocessor that transfers data at the granularity of cache lines. The prefetch instructions are inserted either directly by the programmer <ref> [6] </ref>, or, for statically analyzable programs, by the compiler [7]. We see this communication optimization as orthogonal to the Jade communication optimizations. It takes place at a fine granularity, with communication operations that transfer a single cache line typically issued every few iterations of a loop.
Reference: [7] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Mowry, Lam and Gupta have evaluated the performance impact of prefetching in the context of a shared memory multiprocessor that transfers data at the granularity of cache lines. The prefetch instructions are inserted either directly by the programmer [6], or, for statically analyzable programs, by the compiler <ref> [7] </ref>. We see this communication optimization as orthogonal to the Jade communication optimizations. It takes place at a fine granularity, with communication operations that transfer a single cache line typically issued every few iterations of a loop.
Reference: [8] <author> M. Rinard. </author> <title> The Design, Implementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: In Section 4 we describe the Jade applications. In Section 5 we present the performance results. In Section 6 we survey related work; we conclude in Section 7. 2. The Jade Programming Language. This section provides a brief overview of the Jade language; other publications contain a complete description <ref> [8, 9, 11] </ref>. Jade is a set of constructs that programmers use to describe how a program written in a sequential, imperative language accesses data. <p> Jade eliminates these limitations by providing a more advanced construct and additional access specification statements <ref> [8] </ref>. These language features allow programmers to express more advanced concurrency patterns with multiple synchronization points within a single task. 3. Locality Optimization Algorithm. Access specifications give the Jade implementation advance information about how each task will access data. The locality heuristic exploits this advance information to optimize the communication. <p> Implementation Overview. The shared memory implementation has three components: a synchronizer, a scheduler and a dispatcher. The synchronizer uses a queue-based algorithm to determine when tasks can execute without violating the dynamic data dependence constraints <ref> [8] </ref>. The scheduler takes the resulting pool of enabled tasks generated by the synchronizer and assigns them to processors for execution, using a distributed task stealing algorithm to dynamically balance the load. The dispatcher on each processor serially executes its set of executable tasks. 3.2. Locality Optimization. <p> The dispatcher on each processor serially executes its set of executable tasks. 3.2. Locality Optimization. We have implemented several variants of the locality heuristic, each tailored for the different memory hierarchies of different machines <ref> [8] </ref>. In this paper we discuss the locality heuristic used on machines such as the Stanford DASH machine with physically distributed memory modules (each associated with a processor or cluster of processors) and hardware coherent caches. 3.2.1. The Shared Memory Scheduler. <p> Performance Analysis for Panel Cholesky. For Panel Cholesky several factors combine to limit the performance, among them an inherent lack of concurrency in the basic parallel computation [12] and the task management overhead, which lengthens the critical path <ref> [8] </ref>. Figure 13 presents the task management percentage for Panel Cholesky. This figure shows that, as the number of processors increases, the Jade implementation spends a substantial amount of time managing tasks. Fig. 13.
Reference: [9] <author> M. Rinard and M. Lam. </author> <booktitle> Semantic foundations of Jade. In Proceedings of the Nineteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 105-118, </pages> <address> Albuquerque, NM, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: In Section 4 we describe the Jade applications. In Section 5 we present the performance results. In Section 6 we survey related work; we conclude in Section 7. 2. The Jade Programming Language. This section provides a brief overview of the Jade language; other publications contain a complete description <ref> [8, 9, 11] </ref>. Jade is a set of constructs that programmers use to describe how a program written in a sequential, imperative language accesses data.
Reference: [10] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Heterogeneous Parallel Programming in Jade. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 245-256, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Improving the locality of the computation by executing tasks close to the data that they will access can improve the communication behavior of the program by reducing the amount of remote data that tasks access. This paper describes our experience automatically applying locality optimizations in the context of Jade <ref> [10, 11] </ref>, a portable, implicitly parallel programming language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to describe how parts of the program access data.
Reference: [11] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Jade: a high-level, machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Improving the locality of the computation by executing tasks close to the data that they will access can improve the communication behavior of the program by reducing the amount of remote data that tasks access. This paper describes our experience automatically applying locality optimizations in the context of Jade <ref> [10, 11] </ref>, a portable, implicitly parallel programming language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to describe how parts of the program access data. <p> In Section 4 we describe the Jade applications. In Section 5 we present the performance results. In Section 6 we survey related work; we conclude in Section 7. 2. The Jade Programming Language. This section provides a brief overview of the Jade language; other publications contain a complete description <ref> [8, 9, 11] </ref>. Jade is a set of constructs that programmers use to describe how a program written in a sequential, imperative language accesses data.
Reference: [12] <author> E. Rothberg. </author> <title> Exploiting the memory hierarchy in sequential and parallel sparse Cholesky factorization. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Performance Analysis for Panel Cholesky. For Panel Cholesky several factors combine to limit the performance, among them an inherent lack of concurrency in the basic parallel computation <ref> [12] </ref> and the task management overhead, which lengthens the critical path [8]. Figure 13 presents the task management percentage for Panel Cholesky. This figure shows that, as the number of processors increases, the Jade implementation spends a substantial amount of time managing tasks. Fig. 13.
Reference: [13] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The computational kernel is Panel Cholesky, which factors a sparse positive-definite matrix. The SPLASH benchmark set <ref> [13] </ref> contains variants of the Water, Ocean and Panel Cholesky applications. We next discuss the parallel behavior of each application. * Water: Water performs an interleaved sequence of parallel and serial phases.
References-found: 13

