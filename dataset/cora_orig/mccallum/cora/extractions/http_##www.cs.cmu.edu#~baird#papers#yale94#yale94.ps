URL: http://www.cs.cmu.edu/~baird/papers/yale94/yale94.ps
Refering-URL: http://www.cs.cmu.edu/~baird/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: rjw@ccs.neu.edu  bairdlc@wL.wpafb.af.mil  
Title: Tight Performance Bounds on Greedy Policies Based on Imperfect Value Functions  
Author: Ronald J. Williams Leemon C. Baird, III 
Address: Boston, MA 02115  OH 45433-6543  
Affiliation: College of Computer Science, 161 CN Northeastern University  Wright Laboratory Wright-Patterson AFB,  
Abstract: Consider a given value function on states of a Markov decision problem, as might result from applying a reinforcement learning algorithm. Unless this value function equals the corresponding optimal value function, at some states there will be a discrepancy, which is natural to call the Bellman residual, between what the value function specifies at that state and what is obtained by a one-step lookahead along the seemingly best action at that state using the given value function to evaluate all succeeding states. This paper derives a tight bound on how far from optimal the discounted return for a greedy policy based on the given value function will be as a function of the maximum norm magnitude of this Bellman residual. A corresponding result is also obtained for value functions defined on state-action pairs, as are used in Q-learning. One significant application of these results is to problems where a function approximator is used to learn a value function, with training of the approxi-mator based on trying to minimize the Bellman residual across states or state-action pairs. When control is based on the use of the resulting value function, this result provides a link between how well the objectives of function approximator training are met and the quality of the resulting control. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L. C. & Klopf, A. H. </author> <year> (1993). </year> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> U.S. </type> <institution> Air Force Technical Report WL-TR-93-1147, Wright Laboratory, Wright-Patterson Air Force Base, OH. </institution>
Reference-contexts: For brevity, some of the straightforward mathematical proofs have been omitted in this paper. A longer version, containing all missing details, is also available <ref> (Williams & Baird, 1993) </ref>.
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Singh, S. P. & Yee, R. C. </author> <title> (To appear). An upper bound on the loss from approximate optimal-value functions. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: And most importantly for applications to learning, when training a function approximator to represent a value function on states (or state-action pairs, as considered below), the approach universally used is based on trying to minimize the individual temporal difference (TD) er-rors <ref> (Sutton, 1988) </ref>, which are closely related to the Bellman residual.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference: <author> Williams, R. J. & Baird, L. C., </author> <title> III (1993). Tight performance bounds on greedy policies based on imperfect value functions (Technical Report NU-CCS-93-14). </title> <institution> Boston: Northeastern University, College of Computer Science. </institution>
Reference-contexts: For brevity, some of the straightforward mathematical proofs have been omitted in this paper. A longer version, containing all missing details, is also available <ref> (Williams & Baird, 1993) </ref>.
References-found: 7

