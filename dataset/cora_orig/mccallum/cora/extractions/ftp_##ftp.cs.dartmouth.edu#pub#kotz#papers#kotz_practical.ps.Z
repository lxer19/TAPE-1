URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:practical.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/practical.html
Root-URL: http://www.cs.dartmouth.edu
Email: David.Kotz@Dartmouth.edu carla@cs.duke.edu  
Title: Practical Prefetching Techniques for Parallel File Systems  
Author: David Kotz Carla Schlatter Ellis 
Address: Hanover, NH 03755-3551 Durham, NC 27706  
Affiliation: Dept. of Math and Computer Science Dept. of Computer Science Dartmouth College Duke University  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/CS-papers/Kotz/kotz:practical.ps.Z  
Note: Copyright 1991 by IEEE. Appeared in Conf. on Parallel and Distributed Information Systems, pages 182-189. Available at  
Abstract: Improvements in the processing speed of multiprocessors are outpacing improvements in the speed of disk hardware. Parallel disk I/O subsystems have been proposed as one way to close the gap between processor and disk speeds. In a previous paper we showed that prefetching and caching have the potential to deliver the performance benefits of parallel file systems to parallel applications. In this paper we describe experiments with practical prefetching policies, and show that prefetching can be implemented efficiently even for the more complex parallel file access patterns. We also test the ability of these policies across a range of architectural parameters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> BBN Advanced Computers. Butterfly Products Overview, </institution> <year> 1987. </year>
Reference-contexts: We feel that this set covers the range of patterns likely to be used by scientific applications. 3.2 Methods The RAPID-Transit testbed is a parallel program implemented on a BBN GP1000 Butterfly parallel processor <ref> [1] </ref>. The testbed is heavily parameterized, and incorporates the synthetic workload, the file system, and a set of simulated disks. The file system allocates and manages a buffer cache to hold disk blocks. See [8] for details. Prefetching is attempted whenever the processor is idle.
Reference: [2] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Over 90% of all files opened are opened read-only or write-only. A classic Unix file system study [12] found that 90% of all files are processed sequentially, either through the whole file (70% of all accesses) or after only one seek. Parallel file access is discussed by Crockett <ref> [2] </ref>. Although he did not study an actual workload, he related file access patterns to possible storage techniques. Many of his basic file access patterns are reflected in our workload model. We concentrate on scientific workloads, characterized by sequential access to large files [13, 11].
Reference: [3] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The files may be interleaved over the disks, but the multiple controllers and independent access to the disks make this technique different from disk striping. Examples of this I/O architecture include the Concurrent File System [15, 6] for the Intel iPSC/2 multiprocessor, and the Bridge file system <ref> [4, 3] </ref> for the BBN Butterfly multiprocessor. Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems [20, 18, 19, 17].
Reference: [4] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The files may be interleaved over the disks, but the multiple controllers and independent access to the disks make this technique different from disk striping. Examples of this I/O architecture include the Concurrent File System [15, 6] for the Intel iPSC/2 multiprocessor, and the Bridge file system <ref> [4, 3] </ref> for the BBN Butterfly multiprocessor. Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems [20, 18, 19, 17].
Reference: [5] <author> Rick Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits. File access patterns have never been studied for parallel computers, but have been studied extensively for uniprocessors <ref> [5, 12] </ref>. Floyd [5] studied file access patterns in a Unix system, and found that 68% of files opened for reading are completely read, usually sequentially. Over 90% of all files opened are opened read-only or write-only. <p> With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits. File access patterns have never been studied for parallel computers, but have been studied extensively for uniprocessors [5, 12]. Floyd <ref> [5] </ref> studied file access patterns in a Unix system, and found that 68% of files opened for reading are completely read, usually sequentially. Over 90% of all files opened are opened read-only or write-only. <p> The file system internals, which are responsible for caching and prefetching, see only the block access pattern. In our research we do not investigate read/write file access patterns, because most files are opened for either reading or writing, with few files updated <ref> [5, 12] </ref>. We expect this to be especially true for the large files used in scientific applications. This paper covers read-only patterns, whereas write-only patterns are covered in [10, 8]. All sequential patterns consist of a sequence of accesses to sequential portions.
Reference: [6] <author> James C. French, Terrence W. Pratt, and Mri-ganka Das. </author> <title> Performance measurement of a parallel input/output system for the Intel iPSC/2 hypercube. </title> <booktitle> Proceedings of the 1991 ACM Sigmet-rics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 178-187, </pages> <year> 1991. </year>
Reference-contexts: The files may be interleaved over the disks, but the multiple controllers and independent access to the disks make this technique different from disk striping. Examples of this I/O architecture include the Concurrent File System <ref> [15, 6] </ref> for the Intel iPSC/2 multiprocessor, and the Bridge file system [4, 3] for the BBN Butterfly multiprocessor. Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems.
Reference: [7] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: Section 6 concludes. 2 Background Much of the previous work in I/O hardware parallelism has involved disk striping. In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [16, 7, 14] </ref>. All of these schemes rely on a single controller to manage all of the disks. For multiprocessors, one form of parallel disk architecture is based on the notion of parallel, independent disks, using multiple conventional disk devices addressed independently and attached to separate processors.
Reference: [8] <author> David Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note>
Reference-contexts: We expect this to be especially true for the large files used in scientific applications. This paper covers read-only patterns, whereas write-only patterns are covered in <ref> [10, 8] </ref>. All sequential patterns consist of a sequence of accesses to sequential portions. A portion is some number of contiguous blocks in the file. Note that the whole file may be considered one large portion. <p> The testbed is heavily parameterized, and incorporates the synthetic workload, the file system, and a set of simulated disks. The file system allocates and manages a buffer cache to hold disk blocks. See <ref> [8] </ref> for details. Prefetching is attempted whenever the processor is idle. Assuming a commonly used processor-allocation strategy of one processor for each user process [21], the processor becomes idle whenever its assigned process is idle, usually waiting for disk activity or synchronization to complete. <p> Once they decide to prefetch, both predictors track all accesses and prefetches, and suggest blocks for prefetch-ing that have not yet been fetched. In this mode they are capable of recognizing sequential portions, much like PORT, with unexpected non-sequential accesses requiring re-evaluation of the pattern. See <ref> [8] </ref> for details on these predictors. 5 Experiments We begin with some details of our experiments and measures, then give results from experiments that compare the practical predictors against EXACT and NONE. <p> In many places we give the maximum cv for a given data set. Normalized Performance: Due to limited data space we cannot present all of the experimental data (but see <ref> [8] </ref>). Instead, we use a summarizing measure. Since EXACT represents the potential for prefetch-ing performance, we evaluate our on-line predictors in terms of their relative performance to EXACT. <p> Thus, it is best to have a normalized performance near 1. The case t &lt; t e is considered an anomaly, since an on-line predictor should not run faster than EXACT (although it did sometimes happen for subtle reasons <ref> [8] </ref>). We assign these cases a normalized performance of 1, since they have certainly reached the full potential of EXACT. The normalized performance is undefined for the rnd pattern, in which t e t n . <p> In particular, we varied the number of processors, the number of disks, and the ratio of processor speed to disk speed. We give a sample of the results here, along with the key conclusions; see <ref> [8] </ref> for a full presentation. Number of processors: We varied the number of processors to test the scalability of the file system software, including the predictors.
Reference: [9] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Prefetch-ing in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 218-230, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: This extension to caching is known as prefetching. Prefetching does not work for all access patterns, of course, but it should be beneficial for common sequential patterns. In <ref> [9] </ref>, we showed that prefetching has significant potential to improve read performance in multiprocessor file systems. We measured the potential using an idealistic prefetching policy that was provided with the complete file access pattern in advance. <p> This question is the primary focus of this paper. * Can our practical policies achieve their full potential, as determined in <ref> [9] </ref> by our unrealizable "full-knowledge" policy? * Can we design general policies that are practical for many different types of access patterns? 1 * Do the prefetching policies and implementation scale well, given more processors, more disks, or a wider gap between processor speed and disk ac cess speed? To answer <p> design general policies that are practical for many different types of access patterns? 1 * Do the prefetching policies and implementation scale well, given more processors, more disks, or a wider gap between processor speed and disk ac cess speed? To answer these questions, we used the testbed developed for <ref> [9] </ref>. The testbed implemented many prefetching and caching policies on a real multiprocessor, and simulated the parallel disk I/O. We evaluated many prefetching policies on a wide variety of workloads and architectural parameters. In the next section we provide more background information. <p> The base for all of our evaluations of prefetching policies is the simple NONE policy, which is equivalent to not prefetching. We also use an off-line predictor called EXACT, which is provided with the entire access pattern in advance. (This is the approach used in <ref> [9] </ref>.) The advance knowledge makes it a perfect predictor, since it makes no mistakes and requires little overhead. However it is not realistic, since a real predictor does not know the entire access pattern in advance. <p> This, and all time measures in the testbed, is real time, including all forms of overhead. We also record the average time to read a block, the total synchronization time, the cache hit ratio, prefetch overhead, and many others. In <ref> [9] </ref> we found that measures such as cache hit rate and average block 4 read time are improved with prefetching, but are not good indicators of overall performance. <p> In the few cases where their prefetching was not beneficial, the resulting performance loss was minor. They were remarkably successful at reaching the potential for prefetching, as determined with the EXACT predictor and originally reported in <ref> [9] </ref>. In addition, we found that these predictors were robust across variations in architectural parameters, such as the number of disks, number of processors, and disk access time.
Reference: [10] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <booktitle> In 1991 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: We expect this to be especially true for the large files used in scientific applications. This paper covers read-only patterns, whereas write-only patterns are covered in <ref> [10, 8] </ref>. All sequential patterns consist of a sequence of accesses to sequential portions. A portion is some number of contiguous blocks in the file. Note that the whole file may be considered one large portion.
Reference: [11] <author> Ethan Miller. </author> <title> Input/Output behavior of supercomputing applications. </title> <type> Technical Report UCB/CSD 91/616, </type> <institution> University of California, Berkeley, </institution> <year> 1991. </year> <note> Submitted to Supercomputing '91. </note>
Reference-contexts: Although he did not study an actual workload, he related file access patterns to possible storage techniques. Many of his basic file access patterns are reflected in our workload model. We concentrate on scientific workloads, characterized by sequential access to large files <ref> [13, 11] </ref>.
Reference: [12] <author> John Ousterhout, Herve Da Costa, David Har-rison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits. File access patterns have never been studied for parallel computers, but have been studied extensively for uniprocessors <ref> [5, 12] </ref>. Floyd [5] studied file access patterns in a Unix system, and found that 68% of files opened for reading are completely read, usually sequentially. Over 90% of all files opened are opened read-only or write-only. <p> Floyd [5] studied file access patterns in a Unix system, and found that 68% of files opened for reading are completely read, usually sequentially. Over 90% of all files opened are opened read-only or write-only. A classic Unix file system study <ref> [12] </ref> found that 90% of all files are processed sequentially, either through the whole file (70% of all accesses) or after only one seek. Parallel file access is discussed by Crockett [2]. Although he did not study an actual workload, he related file access patterns to possible storage techniques. <p> The file system internals, which are responsible for caching and prefetching, see only the block access pattern. In our research we do not investigate read/write file access patterns, because most files are opened for either reading or writing, with few files updated <ref> [5, 12] </ref>. We expect this to be especially true for the large files used in scientific applications. This paper covers read-only patterns, whereas write-only patterns are covered in [10, 8]. All sequential patterns consist of a sequence of accesses to sequential portions.
Reference: [13] <author> John Ousterhout and Fred Douglis. </author> <title> Beating the I/O bottleneck: A case for log-structured file systems. </title> <journal> ACM Operating Systems Review, </journal> <volume> 23(1) </volume> <pages> 11-28, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Although he did not study an actual workload, he related file access patterns to possible storage techniques. Many of his basic file access patterns are reflected in our workload model. We concentrate on scientific workloads, characterized by sequential access to large files <ref> [13, 11] </ref>.
Reference: [14] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Disk I/O has always been slower than processing speed, and recent trends have shown that improvements in the speed of disk hardware are not keeping up with the increasing raw speed of processors. This widening access-time gap is known as the I/O crisis <ref> [14, 20] </ref>. The problem is compounded in typical parallel architectures that multiply the processing and memory capacity without balancing the I/O capabilities. The most promising solution to the I/O crisis is to extend parallelism into the I/O subsystem. <p> Section 6 concludes. 2 Background Much of the previous work in I/O hardware parallelism has involved disk striping. In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [16, 7, 14] </ref>. All of these schemes rely on a single controller to manage all of the disks. For multiprocessors, one form of parallel disk architecture is based on the notion of parallel, independent disks, using multiple conventional disk devices addressed independently and attached to separate processors.
Reference: [15] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: The files may be interleaved over the disks, but the multiple controllers and independent access to the disks make this technique different from disk striping. Examples of this I/O architecture include the Concurrent File System <ref> [15, 6] </ref> for the Intel iPSC/2 multiprocessor, and the Bridge file system [4, 3] for the BBN Butterfly multiprocessor. Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems.
Reference: [16] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: Section 6 concludes. 2 Background Much of the previous work in I/O hardware parallelism has involved disk striping. In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [16, 7, 14] </ref>. All of these schemes rely on a single controller to manage all of the disks. For multiprocessors, one form of parallel disk architecture is based on the notion of parallel, independent disks, using multiple conventional disk devices addressed independently and attached to separate processors.
Reference: [17] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory heirarchies. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems <ref> [20, 18, 19, 17] </ref>. The central idea behind prefetch-ing is to overlap some of the I/O time with computation by issuing disk operations before they are requested. With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits.
Reference: [18] <author> Alan Jay Smith. </author> <title> Sequentiality and prefetch-ing in database systems. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 3(3) </volume> <pages> 223-247, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems <ref> [20, 18, 19, 17] </ref>. The central idea behind prefetch-ing is to overlap some of the I/O time with computation by issuing disk operations before they are requested. With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits.
Reference: [19] <author> Alan Jay Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems <ref> [20, 18, 19, 17] </ref>. The central idea behind prefetch-ing is to overlap some of the I/O time with computation by issuing disk operations before they are requested. With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits.
Reference: [20] <author> Alan Jay Smith. </author> <title> Disk cache-miss ratio analysis and design considerations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 161-203, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Disk I/O has always been slower than processing speed, and recent trends have shown that improvements in the speed of disk hardware are not keeping up with the increasing raw speed of processors. This widening access-time gap is known as the I/O crisis <ref> [14, 20] </ref>. The problem is compounded in typical parallel architectures that multiply the processing and memory capacity without balancing the I/O capabilities. The most promising solution to the I/O crisis is to extend parallelism into the I/O subsystem. <p> Examples of this I/O architecture include the Concurrent File System [15, 6] for the Intel iPSC/2 multiprocessor, and the Bridge file system [4, 3] for the BBN Butterfly multiprocessor. Caching commonly-used disk blocks can significantly improve file system performance <ref> [20] </ref>, and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems [20, 18, 19, 17]. The central idea behind prefetch-ing is to overlap some of the I/O time with computation by issuing disk operations before they are requested. <p> Caching commonly-used disk blocks can significantly improve file system performance [20], and indeed is a technique used in most modern file systems. Prefetching is also successful in uniprocessor file systems <ref> [20, 18, 19, 17] </ref>. The central idea behind prefetch-ing is to overlap some of the I/O time with computation by issuing disk operations before they are requested. With parallel disk hardware, however, we expect prefetching to also overlap I/O with I/O, obtaining even larger benefits. <p> The fourth is a hybrid of the first three simpler predictors. These predictors monitor the individual process reference patterns, looking for sequential access. Since the process reference patterns are independent, these predictors are totally concurrent. OBL | One-Block Look-ahead: This algorithm (as in <ref> [20] </ref>) always predicts block i + 1 after block i is referenced, and no more. IBL | Infinite-Block Look-ahead: IBL predicts that i + 2; i + 3; : : : will follow a reference to i, and recommends that they all be prefetched in that order.
Reference: [21] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The file system allocates and manages a buffer cache to hold disk blocks. See [8] for details. Prefetching is attempted whenever the processor is idle. Assuming a commonly used processor-allocation strategy of one processor for each user process <ref> [21] </ref>, the processor becomes idle whenever its assigned process is idle, usually waiting for disk activity or synchronization to complete. To decide on a block to prefetch, the prefetching module calls a predictor, which encapsulates a particular policy, a pattern-prediction heuristic.
References-found: 21

