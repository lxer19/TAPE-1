URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9405.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: A Dissertation  Reduction Operations in Parallel Discrete Event Simulations  
Author: Carmen M. Pancerella 
Degree: Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy (Computer Science)  
Date: May 1994  
Note: by  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [ABRI91] <author> Abrams, M. and Richardson, D., </author> <title> Implementing a Global Termination Condition and Collecting Output Measures in Parallel Simulation, </title> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 86-91, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: For example, a window may be enlarged by including this additional knowledge. 3.1.5. Reduced Values as Termination Detection Conditions in Parallel Simulations The challenge of global termination detection and the calculation of output measures in a PDES <ref> [ABRI91] </ref> can be realized easily using reduction operations within our framework. Many global termination conditions for example, sums and boolean operations can be calculated and disseminated efficiently as globally reduced values.
Reference: [AJKS83] <author> Ajtai, M., Komlos, J. and Szemeredi, E., </author> <title> An O(n(log(n)) Sorting Network, </title> <booktitle> Proceedings of the 15th Annual Symposium on Theory of Computing, </booktitle> <address> Boston, Massachusetts, </address> <pages> pp. 1-9, </pages> <year> (1983). </year>
Reference-contexts: This implementation combines sorting circuits, parallel prefix circuits, and merging circuits. The sorting circuit is assumed to be an AKS sorting circuit <ref> [AJKS83] </ref> which has optimal space and time complexities but an impractically large multiplicative constant. The Multiple Criteria n-processor BSR [LIAK93] allows multiple selection of the data items to be reduced. <p> The AKS sorting network <ref> [AJKS83] </ref> is an optimal sorting network with width O (n) and depth O (log n), but the associated constants are too large for this network to be considered practical. 2.4. Directed Graph Theory and Terminology Most PDES applications demonstrate a spatial locality: event messages tend to follow static communication channels.
Reference: [AKST94] <author> Akl, S. G. and Stojmenovic, I., </author> <title> Multiple Criteria BSR: An Implementation and Applications to Computational Geometry Problems, </title> <booktitle> Proceedings of HICSS, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: The sorting circuit is assumed to be an AKS sorting circuit [AJKS83] which has optimal space and time complexities but an impractically large multiplicative constant. The Multiple Criteria n-processor BSR [LIAK93] allows multiple selection of the data items to be reduced. Akl and Stojmenovic <ref> [AKST94] </ref> present a O (n 2 ) switch solution to this problem with time complexity of O (log n) and also state that it is an open problem if there is a Multiple Criteria n-processor BSR implementation that requires a number of switches asymptotically smaller than O (n 2 ).
Reference: [AYAN89] <author> Ayani, R., </author> <title> A Parallel Simulation Scheme Based on Distances Between Objects, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> Tampa, Florida, </address> <pages> pp. 113-118, </pages> <month> (March </month> <year> 1989). </year>
Reference-contexts: Examples of iterative PDES synchronization protocols include the Bounded Lag protocol [LUBA88a], the Moving Time Window protocol [SOBW88], Chandy and Shermans protocol for converting conditional events into unconditional events [CHSH89], an iterative algorithm based on the distance between objects <ref> [AYAN89] </ref>, Nicols iterative algorithm [NICO90] [NICO91], and the Global Windowing Algorithm [DICK93]. 2.1.5. Hardware Support for PDES The need for special-purpose hardware to support PDES is well established.
Reference: [BATC68] <author> Batcher, K. E., </author> <title> Sorting Networks and Their Applications, </title> <booktitle> Proceedings of the AFIPS 1968 Joint Computing Conference, </booktitle> <address> Atlantic City, New Jersey, </address> <pages> pp. 307-314, </pages> <month> (April </month> <year> 1968). </year>
Reference-contexts: Sorting Networks There is a multitude of literature on parallel sorting and sorting networks. We limit our discussion to a class of practical sorting networks and a theoretically optimal sorting network. Batchers two sorting networks <ref> [BATC68] </ref>, the odd-even merge network and the bitonic sorting network, are both based on parallel merge sort and have similar properties. 23 Both of these networks have width O (n) and depth O (log 2 n), so the time complexity to sort n items is O (log 2 n).
Reference: [BELL90] <author> Bellenot, S., </author> <title> Global Virtual Time Algorithms, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 122-127, </pages> <month> (January </month> <year> 1990). </year>
Reference-contexts: The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], [SAMA85], [LILA89b], <ref> [BELL90] </ref>, [COKE91], and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. <p> GVT, by definition, is computed as the minimum of these two values: GVT computation and dissemination in this framework is a significant improvement in algorithm complexity and implementation efficiency over existing GVT maintenance schemes ([JEFF85], [JESO85], [SAMA85], [LILA89], <ref> [BELL90] </ref>, [COKE91], [TOGA93]). 3.1.3. Reduced Values as Lookahead Values in Parallel Simulations Minimum event processing times and lookahead values can be computed as globally reduced values. For example, the smallest future time that an LP can send event messages can be computed as a globally reduced value.
Reference: [BERR86] <author> Berry, O. </author> <title> Performance Evaluation of the Time Warp Distributed Simulation Mechanism, </title> <type> PhD Thesis, </type> <institution> University of Southern California, </institution> <address> Los Angeles, California, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol <ref> [BERR86] </ref>, [JEBH85], [GILM88], [LOCU88].
Reference: [BLEL89] <author> Blelloch, G. E., </author> <title> Scans as Primitive Parallel Operations, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, No. 11, pp.1526-1538, </volume> <month> (November </month> <year> 1989). </year>
Reference-contexts: Blelloch <ref> [BLEL89] </ref> proposed a tree-structured hardware implementation of parallel prefix operations with O (n) components and O (log n) time complexity. The computation of parallel prefix reductions is a subset of our problem of computing and disseminating target-specific synchronization information, as discussed in Chapters 5 and 6. 20 2.2.2.
Reference: [BLEL90] <author> Blelloch, G. E., </author> <title> Prefix Sums and Their Applications, </title> <type> CMU-CS-90-190, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The segmented scan can be defined by the following recurrence: x i = a 0 , if i = 0 x i = (x i-1 a i ), if f i = 0, 0 &lt; i &lt; n These formal definitions and applications of scans have been published by Blelloch <ref> [BLEL90] </ref>, although scans were first introduced for the language APL [IVER62], and segmented scans were first suggested by Schwartz [SCHW80]. Blelloch [BLEL89] proposed a tree-structured hardware implementation of parallel prefix operations with O (n) components and O (log n) time complexity.
Reference: [BROW93] <author> Brown, M. S., </author> <title> The Hardware Design and Implementation of a Parallel Reduction Network, </title> <type> Masters Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, Charlottesville, Virginia, </institution> <year> 1993. </year> <month> 165 </month>
Reference-contexts: Details of the electrical design for the prototype hardware have been completed by McGraw [MCGR93] and Brown <ref> [BROW93] </ref>. In Section 3.6. we present some algorithms which execute on the framework hardware and support parallel simulations. Throughout this chapter we make it evident how the framework supports a wide range of PDES synchronization protocols. 27 3.1.
Reference: [BRYA77] <author> Bryant, R. E., </author> <title> Simulation of Packet Communications Architecture Computer Systems, </title> <institution> MIT-LCS-TR-188, Massachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <year> 1977. </year>
Reference-contexts: We present a brief overview of parallel simulation 12 synchronization protocols categorized with respect to aggressiveness, inaccuracy, risk and synchrony. The names of the categories are common in the literature. 2.1.2. Conservative PDES Synchronization Protocols Chandy and Misra [CHMI79] and Bryant <ref> [BRYA77] </ref> performed pioneering work in this area independently. This class of protocols will never permit an LP to do incorrect work; hence, each LP cannot proceed until it is guaranteed not to receive a message in its logical past.
Reference: [BURO90] <author> Buzzell, C. A. and Robb, M. J., </author> <title> Modular VME Rollback Hardware for Time Warp, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 153-156, </pages> <month> (January </month> <year> 1990). </year>
Reference-contexts: The rollback chip is a memory management unit that facilitates the state saving and restoration that is inherent in aggressive protocols such as Time Warp. As reported in a study by Buzzell and Robb <ref> [BURO90] </ref>, the chip has excellent performance capabilities. Filoque, et. al., [FIGP91] proposed the use of a processor network with programmable logic for efficient global computations, such as the computation of GVT in a Time Warp simulation.
Reference: [CHIE94] <author> Chien, A., </author> <type> Personal Communication, </type> <month> May 1, </month> <year> 1994. </year>
Reference-contexts: This is competitive with the current technology for existing communication systems; for example, a zero byte message sent from node to node on the Intel Paragon [INTE93] can take 30-70 microseconds using commercial messaging layers and the latency can be reduced to about 5 microseconds with a lower overhead layer <ref> [CHIE94] </ref>. In comparable time our acknowledgments are processed at the software level on APs; process to process acknowledgments an a Paragon can be orders of magnitude more expensive.
Reference: [CHLA85] <author> Chandy, K. M. and Lamport, L., </author> <title> Distributed Snapshots: Determining Global States of Distributed Systems, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 3, No. 1, pp.63-75, </volume> <month> (February </month> <year> 1985). </year>
Reference-contexts: Many global termination conditions for example, sums and boolean operations can be calculated and disseminated efficiently as globally reduced values. Unlike Chandy and Lamports distributed snapshot algorithm <ref> [CHLA85] </ref>, a framework 31 consisting of synchronization values and related algorithms can be used to evaluate termination conditions even when there are outstanding messages in the parallel simulation.
Reference: [CHMI79] <author> Chandy, K. M. and Misra, J., </author> <title> Distributed Simulation: A Case Study in Design and Verification of Distributed Programs, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-5, No. 5, </volume> <pages> pp. 440-452, </pages> <month> (September </month> <year> 1979). </year>
Reference-contexts: We present a brief overview of parallel simulation 12 synchronization protocols categorized with respect to aggressiveness, inaccuracy, risk and synchrony. The names of the categories are common in the literature. 2.1.2. Conservative PDES Synchronization Protocols Chandy and Misra <ref> [CHMI79] </ref> and Bryant [BRYA77] performed pioneering work in this area independently. This class of protocols will never permit an LP to do incorrect work; hence, each LP cannot proceed until it is guaranteed not to receive a message in its logical past. <p> Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See <ref> [CHMI79] </ref>, [MISR86], [PEWM79], [PEWM79b], [REYN82], [NICO84], [CHMI81], [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [CHMI81] <author> Chandy, K. M. and Misra, J., </author> <title> Asynchronous Distributed Simulation via a Sequence of Parallel Computations, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 24, No. 11, </volume> <pages> pp. 198-206, </pages> <month> (April </month> <year> 1981). </year>
Reference-contexts: Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], [PEWM79b], [REYN82], [NICO84], <ref> [CHMI81] </ref>, [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [CHMI87] <author> Chandy, K. M. and Misra, J., </author> <title> Conditional Knowledge as a Basis for Distributed Simulation, </title> <type> Technical Report 5251:TR:87, </type> <institution> Computer Science Department, California Institute of Technology, Pasadena, California, </institution> <year> 1987. </year>
Reference-contexts: A protocol that employs aggressiveness without risk guarantees that all rollbacks are strictly local to that LP. A risk message [REYN88] is a message that is the product of actions taken based on incomplete (conditional; see <ref> [CHMI87] </ref>) knowledge or as a result of processing that leads to the transmission of out-of-order messages. An LP is at risk if there exists a risk message at the head of at least one of its input queues or if at least one of its input queues is empty.
Reference: [CHSH89] <author> Chandy, K. M. and Sherman, R., </author> <title> The Conditional Event Approach to Distributed Simulation, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> Tampa, Florida, </address> <pages> pp. 93-99, </pages> <month> (March </month> <year> 1989). </year>
Reference-contexts: Iterative protocols are accurate, aggressive or non-aggressive, with or without risk, and loosely synchronous. Examples of iterative PDES synchronization protocols include the Bounded Lag protocol [LUBA88a], the Moving Time Window protocol [SOBW88], Chandy and Shermans protocol for converting conditional events into unconditional events <ref> [CHSH89] </ref>, an iterative algorithm based on the distance between objects [AYAN89], Nicols iterative algorithm [NICO90] [NICO91], and the Global Windowing Algorithm [DICK93]. 2.1.5. Hardware Support for PDES The need for special-purpose hardware to support PDES is well established.
Reference: [COKE91] <author> Concepcion, A. I. and Kelly, S. G., </author> <title> Computing Global Virtual Time Using the Multi-Level Token Passing Algorithm, </title> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 63-68, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], [SAMA85], [LILA89b], [BELL90], <ref> [COKE91] </ref>, and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. <p> GVT, by definition, is computed as the minimum of these two values: GVT computation and dissemination in this framework is a significant improvement in algorithm complexity and implementation efficiency over existing GVT maintenance schemes ([JEFF85], [JESO85], [SAMA85], [LILA89], [BELL90], <ref> [COKE91] </ref>, [TOGA93]). 3.1.3. Reduced Values as Lookahead Values in Parallel Simulations Minimum event processing times and lookahead values can be computed as globally reduced values. For example, the smallest future time that an LP can send event messages can be computed as a globally reduced value.
Reference: [DAFU93] <author> Das, S. R. and Fujimoto, R. M., </author> <title> A Performance Study of the Cancelback Protocol for Time Warp, </title> <booktitle> Proceedings of the 1993 Workshop on Parallel and Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 135-142, </pages> <month> (May </month> <year> 1993). </year>
Reference-contexts: Since state saving is such an integral part of the Time Warp protocol, efficient memory management is critical to its performance. The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies <ref> [DAFU93] </ref> have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], [SAMA85], [LILA89b], [BELL90], [COKE91], and [TOGA93]. <p> This can be important, for example, to the cancelback protocol of [JEFF90], a memory management protocol for Time Warp, executing on a shared memory multiprocessor. Performance studies <ref> [DAFU93] </ref> have shown that the global computation of GVT on the Kendall Square Research Machine (KSR) [KEND92] in support of the cancelback protocol has a high cost. 5.1.3. Target-specific Acknowledgment of Messages in PDESs In Chapter 4 we discussed how globally computed minimum operations support message acknowledgments in a PDES. <p> Second, the amount of state space in optimistic PDESs was reduced when TSVT was used in fossil collections. The efficient computation of TSVT will support the rollback chip [FUTG92] and the cancelback Globally reduced information TS reduced information 30.6 Execution time (seconds) 10 30 155 protocol <ref> [DAFU93] </ref>, as discussed in Section 5.1.2. It is a topic of future research to investigate both larger systems and a broader class of protocols. We have concluded that target-specific synchronization information offers significant benefits to conservative PDESs.
Reference: [DEGY91] <author> DeBenedictis, E. and Ghosh, S., </author> <title> A Novel Algorithm for Discrete-Event Simulation, </title> <journal> IEEE Computer, </journal> <volume> Vol. 24, No. 6, </volume> <pages> pp. 21-33, </pages> <month> (June </month> <year> 1991). </year>
Reference-contexts: Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], [PEWM79b], [REYN82], [NICO84], [CHMI81], [YUGD91], <ref> [DEGY91] </ref>, and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [DICK93] <author> Dickens, P. </author> <title> M.,Analysis of the Aggressive Global Windowing Algorithm, </title> <type> PhD Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, Charlottesville, Virginia, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Examples of iterative PDES synchronization protocols include the Bounded Lag protocol [LUBA88a], the Moving Time Window protocol [SOBW88], Chandy and Shermans protocol for converting conditional events into unconditional events [CHSH89], an iterative algorithm based on the distance between objects [AYAN89], Nicols iterative algorithm [NICO90] [NICO91], and the Global Windowing Algorithm <ref> [DICK93] </ref>. 2.1.5. Hardware Support for PDES The need for special-purpose hardware to support PDES is well established. In a recent survey on the state-of-the-art in parallel simulation, Nicol and Fujimoto recognize hardware support as one of six important areas of future research [NIFU92]. <p> We have a framework for disseminating this information easily. 3.1.4. Reduced Values in Iterative Parallel Simulations Iterative PDES synchronization protocols, such as Bounded Lag [LUBA88a], Moving Time Window [SOBW88], and the aggressive Global Windowing Algorithm proposed by Dickens <ref> [DICK93] </ref>, require the computation and dissemination of ceiling values or fault values. Lubachevsky [LUBA89] requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing protocols, such as those proposed by Nicol [NICO93] and Dickens [DIRE92], require establishment of parameters for the window.
Reference: [DIRE92] <author> Dickens, P. M. and Reynolds Jr., P. F., </author> <title> State Saving and Rollback Costs for an Aggressive Global Windowing Algorithm, </title> <note> Computer Science Report 166 No. </note> <institution> TR-92-18, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Lubachevsky [LUBA89] requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing protocols, such as those proposed by Nicol [NICO93] and Dickens <ref> [DIRE92] </ref>, require establishment of parameters for the window. These values are defined as reductions across all LPs. Furthermore, additional global reduction values, such as a minimum outstanding message time or a minimum next event time, could enhance iterative PDES synchronization protocols.
Reference: [FEKL92] <author> Felderman, R. and Kleinrock, L., </author> <title> Two Processor Time Warp Analysis: Capturing the Effects of Message Queueing and Rollback/State Saving Costs, </title> <type> Technical Report 920035, </type> <institution> Computer Science Department, University of California at Los Angeles, </institution> <address> Los Angeles, California, </address> <year> 1992. </year>
Reference-contexts: If each LP submits a current estimate of its rate of simulation, the fastest (or slowest) LP (with respect to logical time) can be identified. Felderman and Kleinrock <ref> [FEKL92] </ref> show analytically that a Time Warp simulation can be more efficient if a faster LP is slowed down; they do not propose how the information might be propagated. We have a framework for disseminating this information easily. 3.1.4.
Reference: [FEKL92b] <author> Felderman, R. and Kleinrock, L., </author> <title> Two Processor Conservative Simulation Analysis, </title> <booktitle> Proceedings of the 1992 Western Simulation MultiConference on Parallel and Distributed Simulation, </booktitle> <address> Newport Beach, California, </address> <pages> pp. 169-177, </pages> <month> (January </month> <year> 1992). </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], [FUJI88], [REMM88], [NICO88], [NICO88b], [LILA89], [WALA89], and <ref> [FEKL92b] </ref> for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [FIGP91] <author> Filoque, J. M., Gautrin, E. and Pottier, B., </author> <title> Efficient Global Computations on a Processors Network with Programmable Logic, </title> <type> Report 1374, </type> <institution> Institut National de Recherche en Informatique et en Anutomatique, France, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The rollback chip is a memory management unit that facilitates the state saving and restoration that is inherent in aggressive protocols such as Time Warp. As reported in a study by Buzzell and Robb [BURO90], the chip has excellent performance capabilities. Filoque, et. al., <ref> [FIGP91] </ref> proposed the use of a processor network with programmable logic for efficient global computations, such as the computation of GVT in a Time Warp simulation. This hardware is not a single network like we introduce in Chapter 3; it is, however, a distributed system of sockets, one per processor. <p> When the token returns to the controller, the global computation is complete. Therefore, their proposed hardware performs global computations in O (n) time whereas, our synchronization network computes reductions in O (log n) time. (See Chapter 3.) Furthermore, the proposed synchronization algorithms for computing GVT in Filoques network <ref> [FIGP91] </ref> rely on the host communication network for message acknowledgments and our framework uses the framework hardware for this purpose. (See Chapter 4.) The goals of both approaches are similar, but our framework is more efficient, more exible, and more scalable, as will be shown throughout this thesis. 2.2.
Reference: [FOJO88] <author> Fox, G., Johnson, M., Lyzenga, G., et. al., </author> <title> Solving Problems on Concurrent Processors, Volume 1, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: The degradation of performance in this case is a result of both state saving overhead and the increased frequency of fossil collection. 2.1.4. Iterative PDES Synchronization Protocols Conservative and optimistic protocols are both asynchronous protocols. Several researchers have introduced PDES synchronization protocols which are loosely synchronous (See <ref> [FOJO88] </ref>), such that LPs process asynchronously and synchronize periodically at barriers. These protocols proceed iteratively and synchronize at the end of each of three phases. Phase one requires LPs to determine the event with the smallest timestamp in the system.
Reference: [FRWW84] <author> Franklin, M. A., Wann, D. F. and Wong, K. F., </author> <title> Parallel Machines and Algorithms for Discrete-Event Simulation, </title> <booktitle> Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pp. 449-458, </pages> <month> (August </month> <year> 1984). </year>
Reference-contexts: The use of special-purpose hardware to improve the performance of simulation programs is not novel. Logic simulation engines have been constructed that yield significant speedups <ref> [FRWW84] </ref>. Lubachevsky suggests using a special-purpose network to broadcast a minimum event time in his Bounded Lag protocol [LUBA88]. This network is a binary tree implemented in hardware in order to support synchronization barriers and to compute and broadcast a minimum next event time.
Reference: [FUJI87] <author> Fujimoto, R. M., </author> <title> Performance Measurements of Distributed Simulation Strategies, </title> <type> Technical Report No. </type> <institution> UUCS-87-026a, Computer Science Department, University of Utah, </institution> <address> Salt Lake City, Utah, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], <ref> [FUJI87] </ref>, [FUJI88], [REMM88], [NICO88], [NICO88b], [LILA89], [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [FUJI88] <author> Fujimoto, R. M., </author> <title> Lookahead in Parallel Discrete Event Simulation, </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> University Park, Pennsylvania, </address> <pages> pp. 34-41, </pages> <month> (August </month> <year> 1988). </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], <ref> [FUJI88] </ref>, [REMM88], [NICO88], [NICO88b], [LILA89], [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [FUJI89] <author> Fujimoto, R. M., </author> <title> The Virtual Time Machine, </title> <booktitle> Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Santa Fe, New Mexico, </address> <pages> pp. 199-208, </pages> <month> (June </month> <year> 1989). </year>
Reference-contexts: One trend in hardware support for Time Warp is to design a high performance, 18 discrete event simulation engine. Fujimoto initially targeted the Virtual Time Machine <ref> [FUJI89] </ref> as hardware support for discrete event simulation, but this machine is now intended to utilize an aggressive style of execution in a general purpose parallel computer. Fujimoto, et. al., developed the rollback chip [FUTG92] as a hardware enhancement to a Time Warp engine.
Reference: [FUJI89b] <author> Fujimoto, R. M., </author> <title> Time Warp on a Shared Memory Multiprocessor, </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> University Park, Pennsylvania, </address> <pages> pp. 242-249, </pages> <month> (August </month> <year> 1989). </year>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], <ref> [FUJI89b] </ref>, [LILA89c], [LILA89d], [LILA89e], [LILA90], and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking. <p> This advantage, however, comes at the cost of extra memory requirements and a more complex mechanism for state saving and rollback. Fujimoto found that as the size of the state increased by a modest size of 2000 bytes, the degradation of performance was reported to be 50% <ref> [FUJI89b] </ref>. The degradation of performance in this case is a result of both state saving overhead and the increased frequency of fossil collection. 2.1.4. Iterative PDES Synchronization Protocols Conservative and optimistic protocols are both asynchronous protocols.
Reference: [FUJI90] <author> Fujimoto, R. M., </author> <title> Parallel Discrete Event Simulation, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 33, No. 10, </volume> <pages> pp. 30-53, </pages> <month> (October </month> <year> 1990). </year>
Reference-contexts: This makes parallel simulation a challenging problem. During his keynote address at the Seventh Workshop on Parallel and Distributed Simulation (May 1993), Mani Chandy identified two key research contributions that parallel discrete event simulation (PDES) <ref> [FUJI90] </ref> has made and is making to parallel computing: 1) the development of techniques for efficient asynchronous computation and 2) the exploration of reduction operations (binary, associative operations). <p> We review the relevant literature and introduce terminology in each of these areas. Parallel discrete event simulation is the application which we support with the computation of efficient reduction operations. Fujimoto has written an excellent survey of PDES research prior to 1990 <ref> [FUJI90] </ref>, and Nicol and Fujimoto have published current PDES research topics since 1990 [NIFU92]. We present background research and related work in PDES in Section 2.1. Our research applies the efficient computation of reduction operations in hardware to parallel simulation synchronization protocols. <p> A parallel simulation will be correct if and only if each LP ultimately processes events equivalent to a nondecreasing timestamp order. Adherence to this local causality constraint is sufficient, though not always necessary, to guarantee the absence of causality errors <ref> [FUJI90] </ref>. However, due to both the asynchronous 10 nature of LPs logical clocks and communication delays among LPs, there is no way of guaranteeing that messages received by LP i occur in a specific order. It would appear that parallel simulation is a natural candidate for parallel processing. <p> Most of the research in PDES has been directed at solving the synchronization problem such as the one depicted in Figure 2.1. Many parallel simulation synchronization protocols have been proposed; Fujimoto <ref> [FUJI90] </ref> gives a good survey of this literature. Each of the proposed protocols performs well under certain conditions and poorly under 30 60 Global logical time LP 1 LP 2 50 80 Global logical time LP 1 LP 2 (a) (b) 11 others.
Reference: [FUJI90b] <author> Fujimoto, R. M., </author> <title> Performance of Time Warp Under Synthetic Workloads, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 23-28, </pages> <month> (January </month> <year> 1990). </year> <month> 167 </month>
Reference-contexts: Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol [BERR86], [JEBH85], [GILM88], [LOCU88]. It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters <ref> [FUJI90b] </ref> and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], [FUJI89b], [LILA89c], [LILA89d], [LILA89e], [LILA90], and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization
Reference: [FUTG92] <author> Fujimoto, R. M., Tsai, J. J. and Gopalakrishnan, </author> <title> G.C., Design and Evaluation of the Rollback Chip: Special Purpose Hardware for Time Warp, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 41, No. 1, </volume> <pages> pp. 68-82, </pages> <month> (January </month> <year> 1992). </year>
Reference-contexts: Furthermore, GVT can be used in termination detection, crash recovery, and input and output handling. There are three major challenges in optimistic protocols. First, the cost of periodic state saving can be very high. The use of special-purpose hardware, the rollback chip <ref> [FUTG92] </ref>, can almost eliminate this cost. The problem with the rollback chip is that its memory capacity is limited. Second, optimistic protocols use more memory than sequential simulations due to state saving and aggressive processing. Fossil collection, or garbage collection, is necessary for an efficient implementation of Time Warp. <p> Fujimoto initially targeted the Virtual Time Machine [FUJI89] as hardware support for discrete event simulation, but this machine is now intended to utilize an aggressive style of execution in a general purpose parallel computer. Fujimoto, et. al., developed the rollback chip <ref> [FUTG92] </ref> as a hardware enhancement to a Time Warp engine. The rollback chip is a memory management unit that facilitates the state saving and restoration that is inherent in aggressive protocols such as Time Warp. <p> This framework, a low-level characterization of all PDES synchronization protocols, provides a model for the efficient computation of critical synchronization values, such as GVT, particularly in support of the rollback chip <ref> [FUTG92] </ref> and adaptive aggressive PDES synchronization protocols [SRIN93]. The original framework, as proposed by Reynolds [REYN91] [REYN92], outlined the three components of the framework and provided low-level algorithms to support a conservative PDES. Our research contributions are built on this work. <p> Therefore, fossil collection can be done with more accurate state knowledge. This can lead to better utilization of state saving memory. (See Chapter 6 for performance results on the reduction in state space.) If the state space were limited, as it is in Fujimotos high-speed rollback chip <ref> [FUTG92] </ref>, this can be a significant benefit. u s 111 The efficient computation of target-specific virtual times, i.e., in a high-speed reduction network, can provide near-perfect state information at a low cost. <p> State saving is performed after each event. There is no cost associated with state saving or fossil collection. There is an unbounded amount of state saving memory. This is a reasonable assumption, assuming the rollback chip of Fujimoto <ref> [FUTG92] </ref>. States are counted each time fossil collection occurs. A state is either a snapshot of the current state or a message in the output message list, which is used to determine where to send antimessages. <p> The total amount of state space is computed for all LPs, in this case for four LPs, and not on a per LP basis. If the state space were limited, as it is in Fujimotos high-speed rollback chip <ref> [FUTG92] </ref>, this can be a significant savings. 6.4.2. Topologies of Eight Logical Processes For the simulations of size eight, we used more interesting topologies. <p> Second, the amount of state space in optimistic PDESs was reduced when TSVT was used in fossil collections. The efficient computation of TSVT will support the rollback chip <ref> [FUTG92] </ref> and the cancelback Globally reduced information TS reduced information 30.6 Execution time (seconds) 10 30 155 protocol [DAFU93], as discussed in Section 5.1.2. It is a topic of future research to investigate both larger systems and a broader class of protocols.
Reference: [GAFN88] <author> Gafni, A., </author> <title> Rollback Mechanisms for Optimistic Distributed Simulation Systems, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 61-67, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See <ref> [GAFN88] </ref>, [FUJI89b], [LILA89c], [LILA89d], [LILA89e], [LILA90], and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking.
Reference: [GILM88] <author> Gilmer, J. B., </author> <title> An Assessment of Time Warp Parallel Discrete Event Simulation Algorithm Performance, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 45-49, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol [BERR86], [JEBH85], <ref> [GILM88] </ref>, [LOCU88].
Reference: [GIRY88] <author> Gibbons, A. and Rytter, W., </author> <title> Efficient Parallel Algorithms, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain, </address> <year> 1988. </year>
Reference-contexts: The computation of parallel prefix reductions is a subset of our problem of computing and disseminating target-specific synchronization information, as discussed in Chapters 5 and 6. 20 2.2.2. Minima of Interval Computation A problem related to the segmented scan operation is the minima of intervals operation <ref> [GIRY88] </ref>.
Reference: [HOSH85] <author> Hoshino, T., </author> <title> PAX Computer: High-Speed Parallel Processing and Scientific Computing, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: In this section we discuss related hardware efforts for barrier synchronization, computing reductions and sorting. 2.3.1. Hardware for Barrier Synchronization Several researchers have proposed the use of hardware to implement barrier synchronization. Hoshino <ref> [HOSH85] </ref> has an efficient barrier synchronization in the PAX computer. Stone [STON90] suggests the use of global busses to compute maximum values and to implement fetch-and-increment. The hardware that we propose, on the other hand, provides support for a larger class of algorithms than barrier synchronization algorithms. 2.3.2.
Reference: [INTE89] <author> Intel Corporation, </author> <title> iPSC2 Programmers Reference Manual, Intel Scientific Computers, </title> <institution> Beaverton, Oregon, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: The hardware that we propose, on the other hand, provides support for a larger class of algorithms than barrier synchronization algorithms. 2.3.2. Intel iPSC/2 Many parallel architectures provide for global binary, associative operations across all processors. Global operations on the Intel iPSC/2 <ref> [INTE89] </ref> are provided for arithmetic and logical operations. There is no separate network to support this computation. All computation is performed on the host processors and all communication is done in the data network. 2.3.3.
Reference: [INTE93] <author> Intel Corporation, </author> <title> Paragon Userss Guide, </title> <institution> Intel Supercomputer Systems Division, Beaverton, Oregon, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: This is competitive with the current technology for existing communication systems; for example, a zero byte message sent from node to node on the Intel Paragon <ref> [INTE93] </ref> can take 30-70 microseconds using commercial messaging layers and the latency can be reduced to about 5 microseconds with a lower overhead layer [CHIE94].
Reference: [IVER62] <author> Iverson, K. E., </author> <title> A Programming Language, </title> <publisher> Wiley, </publisher> <address> New York, New York, </address> <year> 1962. </year>
Reference-contexts: x i = a 0 , if i = 0 x i = (x i-1 a i ), if f i = 0, 0 &lt; i &lt; n These formal definitions and applications of scans have been published by Blelloch [BLEL90], although scans were first introduced for the language APL <ref> [IVER62] </ref>, and segmented scans were first suggested by Schwartz [SCHW80]. Blelloch [BLEL89] proposed a tree-structured hardware implementation of parallel prefix operations with O (n) components and O (log n) time complexity.
Reference: [JEBH85] <author> Jefferson, D., Beckman, B., Hughes, S., et. al., </author> <title> Implementation of Time Warp on the Caltech Hypercube, </title> <booktitle> Proceedings of the Conference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <month> (January </month> <year> 1985). </year>
Reference-contexts: Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol [BERR86], <ref> [JEBH85] </ref>, [GILM88], [LOCU88].
Reference: [JEFF85] <author> Jefferson, D. R., </author> <title> Virtual Time, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 7, No. 3, </volume> <pages> pp. 404-425, </pages> <month> (July </month> <year> 1985). </year>
Reference-contexts: The second major class is protocols that are accurate, aggressive, and with risk; these protocols are commonly called optimistic protocols, and Time Warp <ref> [JEFF85] </ref> is the most common of the optimistic protocols. <p> This usually requires excessive overheads such as broadcast communication necessary to determine when it is safe to proceed. In general, conservative protocols perform well on simulations with sparse topologies and good lookahead properties. 2.1.3. Optimistic PDES Synchronization Protocols The most common optimistic protocol is Time Warp <ref> [JEFF85] </ref>. Time Warp employs maximal aggressiveness: each LP executes without regard to whether there are synchronization conicts (i.e. potential causality errors) with other LPs. A protocol in this class may do incorrect processing, and at some point, the incorrect results must be undone, and the work redone correctly. <p> One rollback can cause a cascade of rollbacks. An LP, however, can never roll back past the global virtual time (GVT) <ref> [JEFF85] </ref>. GVT is maintained across all LPs: this is a simulation-wide safe time. GVT is the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately. <p> GVT is the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately. In other words, at any real time r, GVT (r) is defined to be the minimum of all local clocks and of the timestamps of all transient messages <ref> [JEFF85] </ref>. GVT 15 limits the amount of saved state information that an LP must have in memory at any time; it also prevents rollback to the beginning of simulation time (unless GVT is equal to this time). <p> The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include <ref> [JEFF85] </ref>, [JESO85], [SAMA85], [LILA89b], [BELL90], [COKE91], and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. <p> The elimination of causality errors allows an LP to recognize when it can commit to processing an irreversible act such as I/O. Details can be found in [REYN92]. 3.1.2. Reduced Values in Optimistic Parallel Simulations In an optimistic PDES synchronization protocol, such as Time Warp <ref> [JEFF85] </ref>, GVT can be efficiently computed by an LP at any time using our framework (See [SRRE93b]). <p> Thus, from the perspective of a given LP, its target-specific inputs and outputs depend on the operation being performed, and different LPs will have different sources of inputs and different targets. 6.1.2. Optimistic Simulation Algorithms The optimistic PDES implemented is a Time Warp simulation <ref> [JEFF85] </ref>. (See Section 2.1.3. for details on Time Warp simulations.) In our simulations, antimessages are cancelled aggressively, such that all events sent in the LPs future are cancelled at the time of rollback.
Reference: [JEFF90] <author> Jefferson, D. R., </author> <title> Virtual Time II: Storage Management in Distributed Simulation, </title> <booktitle> Proceedings of the Ninth Annual Symposium on Principles of Distributed Computing, </booktitle> <address> Quebec City, Quebec, Canada, </address> <pages> pp. 75-89, </pages> <month> (August </month> <year> 1990). </year>
Reference-contexts: Fossil collection involves destroying state information that is older than GVT as a method of freeing up available memory for currently saved states. Since state saving is such an integral part of the Time Warp protocol, efficient memory management is critical to its performance. The cancelback protocol <ref> [JEFF90] </ref>, proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], [SAMA85], [LILA89b], [BELL90], [COKE91], and [TOGA93]. <p> This can be important, for example, to the cancelback protocol of <ref> [JEFF90] </ref>, a memory management protocol for Time Warp, executing on a shared memory multiprocessor. Performance studies [DAFU93] have shown that the global computation of GVT on the Kendall Square Research Machine (KSR) [KEND92] in support of the cancelback protocol has a high cost. 5.1.3.
Reference: [JESO85] <author> Jefferson, D. and Sowizral, H., </author> <title> Fast Concurrent Simulation Using the Time Warp Mechanism, </title> <booktitle> Proceedings of the Conference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 63-69, </pages> <month> (January </month> <year> 1985). </year>
Reference-contexts: These protocols are accurate, aggressive, asynchronous, and without risk. Under the Time Warp protocol, an LP processes messages from its input queue (s) in monotonically non-decreasing timestamp order until it exhausts the queue (s) <ref> [JESO85] </ref>; hence, it never blocks or waits until it can safely process the next message. Time Warp guarantees that a parallel simulation cannot deadlock if and only if all LPs process events 14 unconditionally. <p> Also, GVT is used to show a guarantee of progress in a Time Warp simulation <ref> [JESO85] </ref>. Furthermore, GVT can be used in termination detection, crash recovery, and input and output handling. There are three major challenges in optimistic protocols. First, the cost of periodic state saving can be very high. The use of special-purpose hardware, the rollback chip [FUTG92], can almost eliminate this cost. <p> The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], <ref> [JESO85] </ref>, [SAMA85], [LILA89b], [BELL90], [COKE91], and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. <p> The two values and are maintained locally by each LP i . GVT, by definition, is computed as the minimum of these two values: GVT computation and dissemination in this framework is a significant improvement in algorithm complexity and implementation efficiency over existing GVT maintenance schemes ([JEFF85], <ref> [JESO85] </ref>, [SAMA85], [LILA89], [BELL90], [COKE91], [TOGA93]). 3.1.3. Reduced Values as Lookahead Values in Parallel Simulations Minimum event processing times and lookahead values can be computed as globally reduced values. For example, the smallest future time that an LP can send event messages can be computed as a globally reduced value.
Reference: [JOSC79] <author> Jordan, H. F., Scalabrin, M. and Calvert, W., </author> <title> A Comparison of Three Types of Multiprocessor Algorithms, </title> <booktitle> Proceedings of the 1979 International Conference on Parallel Processing, </booktitle> <pages> pp. 231-238, </pages> <month> (August </month> <year> 1979). </year> <month> 168 </month>
Reference-contexts: Global operations on the Intel iPSC/2 [INTE89] are provided for arithmetic and logical operations. There is no separate network to support this computation. All computation is performed on the host processors and all communication is done in the data network. 2.3.3. Finite Element Machine The Finite Element Machine (FEM) <ref> [JOSC79] </ref>, a NASA prototype, utilizes a binary tree-structured max/summation network to perform the global sum and maximum 22 calculations necessary to support structural analysis algorithms. The sum and max calculations in the FEM are calculated alternately without processor synchronization.
Reference: [KEND92] <institution> Kendall Square Research Corporation, KSR Parallel Programming, Kendall Square Research Corporation, Waltham, Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: This can be important, for example, to the cancelback protocol of [JEFF90], a memory management protocol for Time Warp, executing on a shared memory multiprocessor. Performance studies [DAFU93] have shown that the global computation of GVT on the Kendall Square Research Machine (KSR) <ref> [KEND92] </ref> in support of the cancelback protocol has a high cost. 5.1.3. Target-specific Acknowledgment of Messages in PDESs In Chapter 4 we discussed how globally computed minimum operations support message acknowledgments in a PDES. All reduced values and message acknowledgment algorithms can be modified for target-specific message acknowledgments.
Reference: [KIRK92] <author> Kirks, D. J., </author> <title> A New Approach to Load Sharing, A Research Proposal, </title> <institution> Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Target-Specific Reductions in Other Parallel Computing Applications The benefits of the computation and dissemination of target-specific reduced values are not limited to parallel discrete event simulations. We expect target-specific reductions to enhance a range of parallel computing problems: load balancing <ref> [KIRK92] </ref>, iterative 113 numerical computations, and parallel programming problems that require the computation of binary, associative operations across irregular communication patterns. The impact of the efficient computation target-specific reductions in parallel computations is a topic of future research. 5.2.
Reference: [LAMP79] <author> Lamport, L., </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocessor Programs, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-28, No. 9, pp.690-691, </volume> <month> (September </month> <year> 1979). </year>
Reference-contexts: We have chosen to weaken the desire for total ordering because doing so leads to more efficient s 33 and cost-effective solutions and because we can accomplish all we need with a simpler and less expensive approach. In place of total ordering we consider the concept of sequential consistency <ref> [LAMP79] </ref>, which is defined as follows: for a given sequence of changes to the values of components in the state vector of an LP i , the order in which global reductions appear to be applied to those values must be the same as the order in which the values were
Reference: [LEAD92] <author> Leiserson, C. E., Abuhamdeh, Z. S., Douglas, D. C., et. al., </author> <title> The Network Architecture of the Connection Machine CM-5, </title> <booktitle> Proceedings of the Symposium on Parallel and Distributed Algorithms 92, </booktitle> <address> San Diego, California, </address> <month> (June </month> <year> 1992). </year>
Reference-contexts: Thinking Machines CM-5 Supercomputer The Thinking Machines CM-5 [THIN92] contains two separate networks for different types of communication and synchronization: the data network is the primary message-passing network in the machine and the control network provides hardware support for common cooperative operations. The CM-5 control network <ref> [LEAD92] </ref> supports soft barrier synchronization, global arithmetic and logical reduction operations, parallel prefix and parallel suffix operations, and segmented parallel prefix operations. As in the iPSC/2, the reduction operations the CM-5 require the complete synchronization of all processors.
Reference: [LIAK93] <author> Lindon, L. F. and Akl, S. G., </author> <title> An Optimal Implementation of Broadcasting with Selective Reduction, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. </pages> <address> 256- 269, </address> <month> (March </month> <year> 1993). </year>
Reference-contexts: Broadcasting with Selective Reduction Lindon and Akl <ref> [LIAK93] </ref> introduced the Broadcasting with Selective Reduction (BSR) as an extension to the CRCW PRAM model of parallel computation. <p> This implementation combines sorting circuits, parallel prefix circuits, and merging circuits. The sorting circuit is assumed to be an AKS sorting circuit [AJKS83] which has optimal space and time complexities but an impractically large multiplicative constant. The Multiple Criteria n-processor BSR <ref> [LIAK93] </ref> allows multiple selection of the data items to be reduced.
Reference: [LILA89] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> Exploiting Lookahead in a Parallel Simulation, </title> <type> Technical Report 89-10-06, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], [FUJI88], [REMM88], [NICO88], [NICO88b], <ref> [LILA89] </ref>, [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs. <p> The two values and are maintained locally by each LP i . GVT, by definition, is computed as the minimum of these two values: GVT computation and dissemination in this framework is a significant improvement in algorithm complexity and implementation efficiency over existing GVT maintenance schemes ([JEFF85], [JESO85], [SAMA85], <ref> [LILA89] </ref>, [BELL90], [COKE91], [TOGA93]). 3.1.3. Reduced Values as Lookahead Values in Parallel Simulations Minimum event processing times and lookahead values can be computed as globally reduced values. For example, the smallest future time that an LP can send event messages can be computed as a globally reduced value.
Reference: [LILA89b] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> Determining the Global Virtual Time in a Distributed Simulation, </title> <type> Technical Report 90-01-02, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], [SAMA85], <ref> [LILA89b] </ref>, [BELL90], [COKE91], and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89].
Reference: [LILA89c] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> Optimality Considerations for Time Warp Parallel Simulation, </title> <type> Technical Report 89-07-05, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], [FUJI89b], <ref> [LILA89c] </ref>, [LILA89d], [LILA89e], [LILA90], and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking.
Reference: [LILA89d] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> A Study of Time Warp Rollback Mechanisms, </title> <type> Technical Report 89-09-07, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], [FUJI89b], [LILA89c], <ref> [LILA89d] </ref>, [LILA89e], [LILA90], and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking.
Reference: [LILA89e] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> The Optimal Checkpoint Interval in Time Warp Parallel Simulation, </title> <type> Technical Report 89-09-04, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <year> 1989. </year>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], [FUJI89b], [LILA89c], [LILA89d], <ref> [LILA89e] </ref>, [LILA90], and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking.
Reference: [LILA90] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> Optimality Considerations for Time Warp Parallel Simulation, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 29-34, </pages> <month> (January </month> <year> 1990). </year>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], [FUJI89b], [LILA89c], [LILA89d], [LILA89e], <ref> [LILA90] </ref>, and [LILA90b] for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking.
Reference: [LILA90b] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> Reducing the State Saving Overhead for Time Warp Parallel Simulation, </title> <type> Technical Report 90-02-03, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <year> 1990. </year> <month> 169 </month>
Reference-contexts: It has been demonstrated that Time Warp is a robust protocol across a wide range of workload parameters [FUJI90b] and is not as sensitive to lookahead as the conservative protocols. (See [GAFN88], [FUJI89b], [LILA89c], [LILA89d], [LILA89e], [LILA90], and <ref> [LILA90b] </ref> for additional performance studies on Time Warp.) The major advantage of optimistic PDES synchronization protocols is that more parallelism can be extracted from some applications than with other PDES synchronization protocols. Time Warp does not suffer from artificial blocking.
Reference: [LIMA85] <author> Livny, M. and Manber, U. </author> <title> Distributed Computation Via Active Messages, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-34, No. 12, pp.1185-1190, </volume> <month> (December </month> <year> 1985). </year>
Reference-contexts: His control synchronization network is presented strictly in support of the bounded lag protocol; nonetheless, this has served as a motivating factor in our approach. Hardware enhancements for Time Warp have been prevalent in the research; for example, Livny and Manber suggested using token rings for disseminating GVT <ref> [LIMA85] </ref>. One trend in hardware support for Time Warp is to design a high performance, 18 discrete event simulation engine.
Reference: [LITR90] <author> Liu, L. Z. and Tropper, C., </author> <title> Local Deadlock Detection in Distributed Simulations, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 64-69, </pages> <month> (January </month> <year> 1990). </year>
Reference-contexts: Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], [PEWM79b], [REYN82], [NICO84], [CHMI81], [YUGD91], [DEGY91], and <ref> [LITR90] </ref> for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [LOCU88] <author> Lomow, G., Cleary, J., Unger, B., et. al., </author> <title> A Performance Study of Time Warp, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 50-55, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol [BERR86], [JEBH85], [GILM88], <ref> [LOCU88] </ref>.
Reference: [LUBA88] <author> Lubachevsky, B. D., </author> <title> Bounded Lag Distributed Discrete Event Simulation, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 183-191, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: Most of the research in hardware support for parallel simulations has been in support of state saving and rollback in Time Warp simulations. The need for efficient synchronization in parallel simulations has also been recognized, yet only one known research effort <ref> [LUBA88] </ref> has considered hardware to support the efficient computation of 4 reductions for synchronizing PDES logical processes. Lubachevskys effort is quite limited in scope, supporting of only the parallel simulation protocol he proposes. <p> The use of special-purpose hardware to improve the performance of simulation programs is not novel. Logic simulation engines have been constructed that yield significant speedups [FRWW84]. Lubachevsky suggests using a special-purpose network to broadcast a minimum event time in his Bounded Lag protocol <ref> [LUBA88] </ref>. This network is a binary tree implemented in hardware in order to support synchronization barriers and to compute and broadcast a minimum next event time.
Reference: [LUBA89] <author> Lubachevsky, B. D., </author> <title> Efficient Distributed Event-Driven Simulations of Multiple-Loop Networks, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 32, No. 1, </volume> <pages> pp. 111-123, </pages> <month> (January </month> <year> 1989). </year>
Reference-contexts: Reduced Values in Iterative Parallel Simulations Iterative PDES synchronization protocols, such as Bounded Lag [LUBA88a], Moving Time Window [SOBW88], and the aggressive Global Windowing Algorithm proposed by Dickens [DICK93], require the computation and dissemination of ceiling values or fault values. Lubachevsky <ref> [LUBA89] </ref> requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing protocols, such as those proposed by Nicol [NICO93] and Dickens [DIRE92], require establishment of parameters for the window. These values are defined as reductions across all LPs.
Reference: [LUSW89] <author> Lubachevsky, B., Shwartz, A. and Weiss, A. </author> <title> Rollback Sometimes Works If Filtered, </title> <booktitle> Proceedings of the 1989 Winter Simulation Conference, </booktitle> <address> Washington, DC, </address> <pages> pp. 630-639, </pages> <month> (December </month> <year> 1989). </year>
Reference-contexts: Representative ones include [JEFF85], [JESO85], [SAMA85], [LILA89b], [BELL90], [COKE91], and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound <ref> [LUSW89] </ref>. Echoing is a pattern of self-fueled rollbacks whose amplitude increases without bound [LUSW89]. Turner and Xu [TUXU92] have reported cascading rollbacks and echoing 16 which significantly degrade performance in their telephone switching network simulation. Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. <p> The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound <ref> [LUSW89] </ref>. Echoing is a pattern of self-fueled rollbacks whose amplitude increases without bound [LUSW89]. Turner and Xu [TUXU92] have reported cascading rollbacks and echoing 16 which significantly degrade performance in their telephone switching network simulation. Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. <p> rollbacks where the number of LPs increases without bound <ref> [LUSW89] </ref>. Echoing is a pattern of self-fueled rollbacks whose amplitude increases without bound [LUSW89]. Turner and Xu [TUXU92] have reported cascading rollbacks and echoing 16 which significantly degrade performance in their telephone switching network simulation. Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur. Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol [BERR86], [JEBH85], [GILM88], [LOCU88].
Reference: [MCGR93] <author> McGraw, R. M., </author> <title> The Design and Test of Hardware Support for a Parallel Reduction Network, </title> <type> Masters Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, Charlottesville, Virginia, </institution> <year> 1993. </year>
Reference-contexts: Details of the electrical design for the prototype hardware have been completed by McGraw <ref> [MCGR93] </ref> and Brown [BROW93]. In Section 3.6. we present some algorithms which execute on the framework hardware and support parallel simulations. Throughout this chapter we make it evident how the framework supports a wide range of PDES synchronization protocols. 27 3.1.
Reference: [MISR86] <author> Misra, J., </author> <title> Distributed Discrete-Event Simulation, </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 18, No. 1, </volume> <pages> pp. 39-65, </pages> <month> (March </month> <year> 1986). </year>
Reference-contexts: Our framework for PDES, described in Chapter 3, will support each of the protocols mentioned in this chapter. Finally, we discuss hardware support for parallel simulations. 2.1.1. Model of PDES A general model of a PDES has been described by Misra <ref> [MISR86] </ref>. A PDES consists of a set of logical processes (LPs) that model physical processes in a system. All interactions among physical processes are modeled by event messages, or events, sent among LPs. Each message contains a timestamp indicating the logical time at which a 9 scheduled event will occur. <p> Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], <ref> [MISR86] </ref>, [PEWM79], [PEWM79b], [REYN82], [NICO84], [CHMI81], [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [MIMI84] <author> Mitra, D. and Mitrani, I. </author> <title> Analysis and Optimum Performance of Two Message-Passing Parallel Processors Synchronized by Rollback, PERFORMANCE 84, </title> <publisher> Elsevier Science Pub (North Holland), </publisher> <pages> pp. 35-51, </pages> <year> 1984. </year>
Reference-contexts: Echoing is a pattern of self-fueled rollbacks whose amplitude increases without bound [LUSW89]. Turner and Xu [TUXU92] have reported cascading rollbacks and echoing 16 which significantly degrade performance in their telephone switching network simulation. Mitra and Mitrani <ref> [MIMI84] </ref> and Lubachevsky [LUSW89] have developed models to show that echoing can occur. Many experimental results have been published on the performance of the Time Warp parallel simulation synchronization protocol [BERR86], [JEBH85], [GILM88], [LOCU88].
Reference: [NICO84] <author> Nicol, D. M., </author> <title> Synchronizing Network Performance, </title> <type> Masters Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, Charlottesville, Virginia, </institution> <month> January </month> <year> 1984. </year>
Reference-contexts: Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], [PEWM79b], [REYN82], <ref> [NICO84] </ref>, [CHMI81], [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [NICO88] <author> Nicol, D. M., </author> <title> High Performance Parallelized Discrete Event Simulation of Stochastic Queueing Networks, </title> <booktitle> Proceedings of the 1988 Winter Simulation Conference, </booktitle> <address> San Diego, California, </address> <pages> pp. 306-314, </pages> <month> (December </month> <year> 1988). </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], [FUJI88], [REMM88], <ref> [NICO88] </ref>, [NICO88b], [LILA89], [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [NICO88b] <author> Nicol, D. M., </author> <title> Parallel Discrete-Event Simulation of FCFS Stochastic Queueing Networks, </title> <booktitle> Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems, </booktitle> <pages> pp. 124-137, </pages> <year> (1988). </year> <month> 170 </month>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], [FUJI88], [REMM88], [NICO88], <ref> [NICO88b] </ref>, [LILA89], [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [NICO90] <author> Nicol, D. M., </author> <title> The Cost of Conservative Synchronization in Parallel Discrete Event Simulations, </title> <type> NASA Contractor Report 182034, </type> <institution> Institute for Computer Applications in Science and Engineering, NASA Langley, Hampton, Virginia, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Examples of iterative PDES synchronization protocols include the Bounded Lag protocol [LUBA88a], the Moving Time Window protocol [SOBW88], Chandy and Shermans protocol for converting conditional events into unconditional events [CHSH89], an iterative algorithm based on the distance between objects [AYAN89], Nicols iterative algorithm <ref> [NICO90] </ref> [NICO91], and the Global Windowing Algorithm [DICK93]. 2.1.5. Hardware Support for PDES The need for special-purpose hardware to support PDES is well established. In a recent survey on the state-of-the-art in parallel simulation, Nicol and Fujimoto recognize hardware support as one of six important areas of future research [NIFU92].
Reference: [NICO91] <author> Nicol, D. M., </author> <title> Performance Bounds on Parallel Self-Initiating Discrete-Event Simulations, </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> Vol. 1, No. 1, </volume> <pages> pp. 24-50, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: Examples of iterative PDES synchronization protocols include the Bounded Lag protocol [LUBA88a], the Moving Time Window protocol [SOBW88], Chandy and Shermans protocol for converting conditional events into unconditional events [CHSH89], an iterative algorithm based on the distance between objects [AYAN89], Nicols iterative algorithm [NICO90] <ref> [NICO91] </ref>, and the Global Windowing Algorithm [DICK93]. 2.1.5. Hardware Support for PDES The need for special-purpose hardware to support PDES is well established. In a recent survey on the state-of-the-art in parallel simulation, Nicol and Fujimoto recognize hardware support as one of six important areas of future research [NIFU92].
Reference: [NICO93] <author> Nicol, D. M., </author> <title> The Cost of Conservative Synchronization in Parallel Discrete Event Simulations, </title> <journal> Journal of the ACM, </journal> <volume> Vol. 40, No. 2, </volume> <pages> pp. 304-333, </pages> <month> (April </month> <year> 1993). </year>
Reference-contexts: Lubachevsky [LUBA89] requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing protocols, such as those proposed by Nicol <ref> [NICO93] </ref> and Dickens [DIRE92], require establishment of parameters for the window. These values are defined as reductions across all LPs. Furthermore, additional global reduction values, such as a minimum outstanding message time or a minimum next event time, could enhance iterative PDES synchronization protocols.
Reference: [NIFU92] <author> Nicol, D. and Fujimoto, R., </author> <title> Parallel Simulation Today, </title> <note> to appear in The Annals of Operations Research. </note>
Reference-contexts: In addition, simulation programs, especially those employing aggressive processing, often utilize a large amount of memory. The importance of research in the area of hardware support for PDES has been recognized in a recent article on the state of the art in parallel simulation <ref> [NIFU92] </ref>. Most of the research in hardware support for parallel simulations has been in support of state saving and rollback in Time Warp simulations. <p> Parallel discrete event simulation is the application which we support with the computation of efficient reduction operations. Fujimoto has written an excellent survey of PDES research prior to 1990 [FUJI90], and Nicol and Fujimoto have published current PDES research topics since 1990 <ref> [NIFU92] </ref>. We present background research and related work in PDES in Section 2.1. Our research applies the efficient computation of reduction operations in hardware to parallel simulation synchronization protocols. <p> Hardware Support for PDES The need for special-purpose hardware to support PDES is well established. In a recent survey on the state-of-the-art in parallel simulation, Nicol and Fujimoto recognize hardware support as one of six important areas of future research <ref> [NIFU92] </ref>. The use of special-purpose hardware to improve the performance of simulation programs is not novel. Logic simulation engines have been constructed that yield significant speedups [FRWW84]. Lubachevsky suggests using a special-purpose network to broadcast a minimum event time in his Bounded Lag protocol [LUBA88].
Reference: [NIRE84] <author> Nicol, D. M. and Reynolds Jr., P. F., </author> <title> Problem Oriented Protocol Design, </title> <booktitle> Proceedings of the 1984 Winter Simulation Conference, </booktitle> <address> Dallas, Texas, </address> <pages> pp. 471-474, </pages> <month> (December </month> <year> 1984). </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See <ref> [NIRE84] </ref>, [FUJI87], [FUJI88], [REMM88], [NICO88], [NICO88b], [LILA89], [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [OWGR76] <author> Owicki, S. and Gries, D., </author> <title> An Axiomatic Proof Technique for Parallel Programs I, </title> <journal> Acta Informatica, </journal> <volume> Vol. 6, </volume> <pages> pp. 319-340, </pages> <year> 1976. </year>
Reference-contexts: In Lemma 4.4 we prove that all messages acknowledged during round robin mode are acknowledged properly. In Lemma 4.5 we show the non-interference of round robin acknowledgments. We use the term non-interference in a less rigorous way than Owicki and Gries <ref> [OWGR76] </ref>. By non-interference, we will show that each acknowledgment mode does not violate the correctness of the other. Our strategy is to examine the variables and data structures that are read and/or written while in round robin acknowledgment mode and to show that they will remain in correct states.
Reference: [PANC92] <author> Pancerella, C. M., </author> <title> Improving the Efficiency of a Framework for Parallel Simulations, </title> <booktitle> Proceedings of the 1992 Western Simulation MultiConference on Parallel and Distributed Simulation, </booktitle> <address> Newport Beach, California, </address> <pages> pp. 22-29, </pages> <month> (January </month> <year> 1992). </year>
Reference-contexts: Our framework has been described extensively to date. (See [REYN91], [REYN92], [REPA92], <ref> [PANC92] </ref>, [REPS92], and [REPS93].) In this chapter we discuss our contributions to the framework in three areas: the applicability of the framework to a wide range of PDES synchronization protocols, the correctness criteria on which the framework hardware is based, and the hardware component of the framework. <p> Since this AP-PRN interface does not prevent state vector loss, an alternative, which is the equivalent of observable sequential consistency, is to use two extra input registers and compute tagged selective operations to perform a double handshake <ref> [PANC92] </ref>, as discussed in Chapter 4. We note, however, that it is expensive (in terms of computation time) to implement observable sequential consistency in the framework hardware and it should be avoided when possible. Specific details about our prototype hardware have been published by Reynolds, Pancerella, and Srinivasan [REPS93]. <p> In Section 4.2. we discuss the necessary algorithmic requirements for using a reduction 58 network to acknowledge messages. In Section 4.3. we present a solution which employs a two reduction handshake algorithm in the reduction network. This work was first introduced by Pancerella <ref> [PANC92] </ref>. In Section 4.4. we discuss an alternative to this algorithm, one that requires a single reduction operation to be computed in the reduction network. We prove the correctness of this alternative in Section 4.5. <p> We have presented work using two reduction networks to compute synchronization values: one network produces the PDES state T-values, such as and , and the other network is dedicated to the acknowledgment of messages <ref> [PANC92] </ref>. This has an advantage over the serial acknowledgment algorithm since its best case performance can be shown to be O (1) and this best case O (1) complexity occurs under heavy load.
Reference: [PARE93] <author> Pancerella, C. M. and Reynolds Jr., P. F., </author> <title> Disseminating Critical Target-specific Synchronization Information in Parallel Discrete Event Simulations, </title> <booktitle> Proceedings of the 1993 Workshop on Parallel and Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 52-59, </pages> <month> (May </month> <year> 1993). </year>
Reference-contexts: We expect the time lag between the sending of a message and its acknowledgment to be reduced significantly. 107 5 The Cost of Doing Target-specific Reductions We have established that target-specific reductions can be critical to the performance of both aggressive and non-aggressive parallel simulations <ref> [PARE93] </ref>. In this chapter we present the best known theoretical results on the sequential computation of target-specific binary, associative operations. Our theoretical contributions include the establishment of specific time complexity and space complexity trade-offs. <p> Many of these simulation results were first presented in <ref> [PARE93] </ref>. As established in previous chapters, target-specific reductions allow an LP to receive synchronization information only from those logical processes which may have a direct or indirect impact on its performance.
Reference: [PEWM79] <author> Peacock, J. K., Wong, J. W. and Manning, E., </author> <title> Distributed Simulation Using a Network of Processors, Computer Networks 3, </title> <publisher> North-Holland Publishing Company, </publisher> <pages> pp. 44-56, </pages> <year> 1979. </year>
Reference-contexts: Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], <ref> [PEWM79] </ref>, [PEWM79b], [REYN82], [NICO84], [CHMI81], [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [PEWM79b] <author> Peacock, J. K., Wong, J. W. and Manning, E., </author> <title> A Distributed Approach to Queueing Network Simulation, </title> <booktitle> Proceedings of the 1979 Winter Simulation Conference, </booktitle> <pages> pp. 399-406, </pages> <month> (December </month> <year> 1979). </year>
Reference-contexts: Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], <ref> [PEWM79b] </ref>, [REYN82], [NICO84], [CHMI81], [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [PFBG85] <author> Pfister, G. F., Brantley, W. C., George, D. A., et. al., </author> <title> The IBM Research Parallel Prototype (RP3): Introduction and Architecture, </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Illinois, </publisher> <pages> pp. 764-771, </pages> <month> (August </month> <year> 1985). </year>
Reference-contexts: An x MIN a ( )= 21 optimal solution to the Multiple Criteria n-processor BSR problem is directly applicable to the target-specific dissemination problem. 2.3. Related Architectures Using a separate synchronization network for improving system performance is not a new idea. The IBM RP3 <ref> [PFBG85] </ref> was designed as a shared memory multiprocessor that houses both a combining network for synchronization traffic and a low latency network for regular message traffic. In this section we discuss related hardware efforts for barrier synchronization, computing reductions and sorting. 2.3.1.
Reference: [RABJ88] <author> Ranade, A. G., Bhatt, S. N. and Johnsson, S. L., </author> <title> The Fluent Abstract Machine, </title> <institution> YALEU/Department of Computer Science/Technical Report-573, Department of Computer Science, Yale University, </institution> <address> New Haven, Connecticut, </address> <month> January </month> <year> 1988. </year> <month> 171 </month>
Reference-contexts: This kind of cooperation can be achieved, but at a cost. The cost can be temporal: LPs must continually execute the equivalent of barrier synchronizations, or the cost can be monetary: specialized networks such as those proposed by Ranade, Bhatt, and Johnnson <ref> [RABJ88] </ref> and Reynolds, Williams, and Wagner [REWW89] [REWW92] would be required in addition to our framework.
Reference: [REMM88] <author> Reed, D. A., Malony, A. D., and McCredie, B. D., </author> <title> Parallel Discrete Event Simulation Using Shared Memory, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 14, No. 4, </volume> <pages> pp. 541-553, </pages> <month> (April </month> <year> 1988). </year>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], [FUJI88], <ref> [REMM88] </ref>, [NICO88], [NICO88b], [LILA89], [WALA89], and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [REPA92] <author> Reynolds Jr., P. F. and Pancerella, C. M., </author> <title> Hardware Support for Parallel Discrete Event Simulations, </title> <institution> Computer Science Report No. TR-92-08, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The original framework, as proposed by Reynolds [REYN91] [REYN92], outlined the three components of the framework and provided low-level algorithms to support a conservative PDES. Our research contributions are built on this work. Reynolds and Pancerella <ref> [REPA92] </ref> first described specific functional and implementation details of the hardware component of the framework. We presented the prototype hardware design and applications of the hardware-based framework to a wide range of PDES synchronization protocols [REPS92]. <p> This computation model is designed to ride the technology wave, such that as processors and memories get faster, the framework can exploit these gains.We use the term framework to describe the entire software/hardware ensemble meant to support PDES. Our framework has been described extensively to date. (See [REYN91], [REYN92], <ref> [REPA92] </ref>, [PANC92], [REPS92], and [REPS93].) In this chapter we discuss our contributions to the framework in three areas: the applicability of the framework to a wide range of PDES synchronization protocols, the correctness criteria on which the framework hardware is based, and the hardware component of the framework. <p> In the next few sections, we explore various T-values in parallel simulations. Each of the T-values we present are computed in a reduction network. 3.1.1. Reduced Values in Conservative Parallel Simulations A set of T-values to support a conservative PDES synchronization protocol have been presented in detail [REYN92] <ref> [REPA92] </ref>. The T-values computed are , the minimum next event time, and , the minimum logical timestamp of messages that have been sent but not acknowledged. <p> A high-level hardware description of this implementation is shown in Figure 3.2. The shaded components represent hardware which is built and interfaced to an existing parallel machine or cluster of computers. This hardware description was first proposed by Reynolds and Pancerella <ref> [REPA92] </ref>. The host system in our description is a closely coupled network of high speed processors with its own network for interprocess communication. The host communication network is independent from the synchronization network, or reduction network.
Reference: [REPS92] <author> Reynolds Jr., P. F., Pancerella, C. M. and Srinivasan, S., </author> <title> Making Parallel Simulations Go Fast, </title> <booktitle> Proceedings of the 1992 Winter Simulation Conference, Alexandria, Virginia, </booktitle> <pages> pp. 646-655, </pages> <month> (December </month> <year> 1992). </year>
Reference-contexts: Our research contributions are built on this work. Reynolds and Pancerella [REPA92] first described specific functional and implementation details of the hardware component of the framework. We presented the prototype hardware design and applications of the hardware-based framework to a wide range of PDES synchronization protocols <ref> [REPS92] </ref>. Finally, the framework was further developed by Reynolds, Pancerella, and Srinivasan [REPS93] with criteria for its correct operation, functional characteristics, description of a prototype hardware design, and performance results first reported by Srinivasan [SRIN92]. <p> Our framework has been described extensively to date. (See [REYN91], [REYN92], [REPA92], [PANC92], <ref> [REPS92] </ref>, and [REPS93].) In this chapter we discuss our contributions to the framework in three areas: the applicability of the framework to a wide range of PDES synchronization protocols, the correctness criteria on which the framework hardware is based, and the hardware component of the framework.
Reference: [REPS93] <author> Reynolds Jr., P. F., Pancerella, C. M. and Srinivasan, S., </author> <title> Design and Performance Analysis of Hardware Support for Parallel Simulations, </title> <journal> in a special issue of Journal of Parallel and Distributed Computing on Parallel and Distributed Simulation, </journal> <volume> Vol. 18, No. 4, </volume> <pages> pp. 435-453, </pages> <month> (August </month> <year> 1993). </year>
Reference-contexts: We presented the prototype hardware design and applications of the hardware-based framework to a wide range of PDES synchronization protocols [REPS92]. Finally, the framework was further developed by Reynolds, Pancerella, and Srinivasan <ref> [REPS93] </ref> with criteria for its correct operation, functional characteristics, description of a prototype hardware design, and performance results first reported by Srinivasan [SRIN92]. Our contributions to the framework, as presented in these publications, will be presented in this chapter. <p> Our framework has been described extensively to date. (See [REYN91], [REYN92], [REPA92], [PANC92], [REPS92], and <ref> [REPS93] </ref>.) In this chapter we discuss our contributions to the framework in three areas: the applicability of the framework to a wide range of PDES synchronization protocols, the correctness criteria on which the framework hardware is based, and the hardware component of the framework. <p> The first condition is easily achieved, however, we do not expect LPs will be able to 35 process a constant ow of globally reduced values. Consider the speed at which such values could be produced. Our prototype hardware <ref> [REPS93] </ref> computes global reductions, with pipelining, such that new results are produced on the order of every 150 nanoseconds. The absolute timing is not the factor here; rather it is that time relative to a typical processors instruction cycle time. <p> However, low latency is critical. When an LP computes a new T-value the corresponding global reduction should be completed as quickly as possible. The importance of this is established in our performance analysis of the hardware <ref> [REPS93] </ref>. This analysis concluded that under normal load, GVT computed on our hardware lags behind the actual GVT by 5-10 microseconds. <p> We note, however, that it is expensive (in terms of computation time) to implement observable sequential consistency in the framework hardware and it should be avoided when possible. Specific details about our prototype hardware have been published by Reynolds, Pancerella, and Srinivasan <ref> [REPS93] </ref>. We now discuss the algorithms that execute on the host and auxiliary processors in order to support parallel simulations. 3.6. Framework Algorithms Synchronization algorithms are the third component of our PDES framework. <p> In the next section we present our results on the performance of both TPA and SPA. 4.8. Performance Results We have implemented both TPA and SPA on our four-processor prototype framework hardware <ref> [REPS93] </ref>. We discuss the prototype system and execution parameters prior to presenting our results. 4.8.1. Prototype Framework Hardware The host system is a Sparc cluster: four Sparc 2 equivalent processors with Ethernet (TCP/IP) as the host communication network. The expected host communication latency time is approximately two milliseconds. <p> Simulation Assumptions We have made certain assumptions with respect to our simulations. We believe them to be realistic with respect to current technology and parallel simulations in general. Assumptions about the reduction network and all interfaces are based on the prototype design of our four-processor global reduction network <ref> [REPS93] </ref>. Each logical process LP i in the parallel simulation executes on a dedicated physical host processor HP i . There are two times represented in this simulation. Logical time refers to the logical time of the PDES being simulated. <p> Adaptability The design of the interface to the host computing system (See Section 3.5.3.) isolates the design of the rest of the framework hardware from the host computing system. Our prototype system <ref> [REPS93] </ref> assumes a Sun SBus interface to a Sparc cluster (a network of Sparc-1es); this design is easily adapted to other host systems. Generality The framework hardware contains programmable ALUs which allow it to be used to support a wide variety of applications.
Reference: [REWW89] <author> Reynolds Jr., P. F., Williams, C. and Wagner, R. R., </author> <note> Parallel Operations, Computer Science Report No. </note> <institution> TR-89-16, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: This kind of cooperation can be achieved, but at a cost. The cost can be temporal: LPs must continually execute the equivalent of barrier synchronizations, or the cost can be monetary: specialized networks such as those proposed by Ranade, Bhatt, and Johnnson [RABJ88] and Reynolds, Williams, and Wagner <ref> [REWW89] </ref> [REWW92] would be required in addition to our framework. We have chosen to weaken the desire for total ordering because doing so leads to more efficient s 33 and cost-effective solutions and because we can accomplish all we need with a simpler and less expensive approach.
Reference: [REWW92] <author> Reynolds Jr., P. F., Williams, C. and Wagner, R. R., </author> <title> Empirical Analysis of Isotach Networks, </title> <institution> Computer Science Report No. TR-92-19, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: This kind of cooperation can be achieved, but at a cost. The cost can be temporal: LPs must continually execute the equivalent of barrier synchronizations, or the cost can be monetary: specialized networks such as those proposed by Ranade, Bhatt, and Johnnson [RABJ88] and Reynolds, Williams, and Wagner [REWW89] <ref> [REWW92] </ref> would be required in addition to our framework. We have chosen to weaken the desire for total ordering because doing so leads to more efficient s 33 and cost-effective solutions and because we can accomplish all we need with a simpler and less expensive approach.
Reference: [REYN82] <author> Reynolds Jr., P. F., </author> <title> A Shared Resource Algorithm for Distributed Simulation, </title> <booktitle> Proceedings of the 9th Annual Symposium on Computer Architecture, </booktitle> <address> Austin, Texas, </address> <pages> pp. 259-266, </pages> <month> (April </month> <year> 1982). </year>
Reference-contexts: In a conservative protocol, an LP will block until determines that it can safely process an event. These protocols are accurate, non-aggressive, asynchronous, and without risk. The most significant disadvantage of conservative PDES synchronization protocols is that the potential parallelism is not always fully exploited due to artificial blocking <ref> [REYN82] </ref>. Artificial blocking occurs when LP i is blocked and waiting for a message from an LP whose logical clock has already exceeded the message time of LP i s pending message. <p> Blocking introduces the potential for deadlock. Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], [PEWM79b], <ref> [REYN82] </ref>, [NICO84], [CHMI81], [YUGD91], [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
Reference: [REYN88] <author> Reynolds Jr., P. F., </author> <title> A Spectrum of Options for Parallel Simulations, </title> <booktitle> Proceedings of the 1988 Winter Simulation Conference, </booktitle> <address> San Diego, California, </address> <pages> pp. 167-174, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: Broadly these protocols fall into two major classes, though there is a wide range of synchronization protocols. One class is protocols that are accurate, non-aggressive, and without risk (using terminology developed by Reynolds <ref> [REYN88] </ref>); these protocols are commonly referred to as conservative protocols. <p> Scalability The hardware must be scalable. Adaptability The hardware should be adaptable to current and future parallel computers. Also, the framework should be designed to adapt to current technology with ease. Generality The hardware must be able to support a spectrum of parallel simulation synchronization protocols <ref> [REYN88] </ref>. Low cost The hardware to support parallel simulation synchronization protocols should not be expensive. One of the primary motivations of our effort is the efficient computation of global virtual time in an optimistic PDES protocol. <p> Traditionally, researchers have categorized PDES synchronization protocols as either conservative or optimistic. This classification is too restrictive; Reynolds has developed a set of design variables that define a spectrum of options for parallel simulation synchronization protocols <ref> [REYN88] </ref>. Although nine design variables are defined, the four that are most relevant to our research are aggressiveness, accuracy, risk and synchrony. Aggressiveness involves relaxing the requirement that event messages are processed in a strict monotonic order with respect to message times. <p> Risk is the design variable that allows an LP to send messages that have been processed based on aggressive or inaccurate processing assumptions. A protocol that employs aggressiveness without risk guarantees that all rollbacks are strictly local to that LP. A risk message <ref> [REYN88] </ref> is a message that is the product of actions taken based on incomplete (conditional; see [CHMI87]) knowledge or as a result of processing that leads to the transmission of out-of-order messages.
Reference: [REYN91] <author> Reynolds Jr., P. </author> <title> F.,An Efficient Framework for Parallel Simulations, </title> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 167-174, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: The utility of reduction operations has been greatly overlooked until now. The results presented in this thesis make important contributions to the efficient computation of reduction operations in support of parallel simulation. A novel framework for parallel simulation was presented by Reynolds <ref> [REYN91] </ref>. This framework is a software/hardware ensemble for the efficient computation of reductions and the dissemination of reduced values in support of parallel simulations. <p> Also, it is critical that the algorithms which support synchronization and event message acknowledgments be correct. We have based our framework on formal correctness criteria and have derived correctness proofs for message acknowledgment algorithms. 5 The framework as proposed by Reynolds <ref> [REYN91] </ref> includes the computation of globally reduced values. We believe that in some cases globally reduced information is not sufficient for logical processes in a PDES to make the necessary event processing decisions. <p> In the next chapter we present a reduction-based framework for parallel discrete event simulations as introduced by Reynolds <ref> [REYN91] </ref> [REYN92]. In this chapter we develop this framework with respect the applicability of the framework, the computation model, the detailed hardware design, and correctness criteria. In Chapter 4 we present our contributions with respect to the algorithmic component of the framework. <p> In Chapter 5 and Chapter 6 we present our contributions on the computation of target-specific reductions. 25 3 A Framework for Parallel Discrete Event Simulations Our parallel discrete event simulation framework, first introduced by Reynolds <ref> [REYN91] </ref>, is a combination of hardware and algorithms in support of parallel simulations. <p> This framework, a low-level characterization of all PDES synchronization protocols, provides a model for the efficient computation of critical synchronization values, such as GVT, particularly in support of the rollback chip [FUTG92] and adaptive aggressive PDES synchronization protocols [SRIN93]. The original framework, as proposed by Reynolds <ref> [REYN91] </ref> [REYN92], outlined the three components of the framework and provided low-level algorithms to support a conservative PDES. Our research contributions are built on this work. Reynolds and Pancerella [REPA92] first described specific functional and implementation details of the hardware component of the framework. <p> This computation model is designed to ride the technology wave, such that as processors and memories get faster, the framework can exploit these gains.We use the term framework to describe the entire software/hardware ensemble meant to support PDES. Our framework has been described extensively to date. (See <ref> [REYN91] </ref>, [REYN92], [REPA92], [PANC92], [REPS92], and [REPS93].) In this chapter we discuss our contributions to the framework in three areas: the applicability of the framework to a wide range of PDES synchronization protocols, the correctness criteria on which the framework hardware is based, and the hardware component of the framework. <p> We identify three primary contributions of the work presented in this chapter. First, we have demonstrated the applicability of this framework to a wide range of PDES synchronization protocols. The applicability of the framework to PDES synchronization protocols began in <ref> [REYN91] </ref> with reduced values and low-level algorithms to support conservative protocols. It continued in [SRIN92] with the applicability to optimistic protocols. <p> messages is important to the computation of a minimum outstanding message time, which is useful in non-aggressive PDES synchronization protocols, aggressive PDES synchronization protocols, and adaptive aggressive PDES synchronization protocols. 57 4 Acknowledgment Messages in a Reduction Network A probable source of performance degradation in Reynoldss original PDES framework algorithms <ref> [REYN91] </ref> (See Chapter 3.) (from here on known as Reynoldss framework algorithms) is the communication of synchronization information among LPs, i.e., acknowledgment messages, outside the reduction network. This is undesirable. <p> We summarize our contributions and discuss avenues of future research. 7.1. Summary of Work The framework for parallel discrete event simulations <ref> [REYN91] </ref> was advanced in this thesis, is a novel and efficient combination of both hardware and software to rapidly compute and disseminate reduced values in support of a spectrum of PDES synchronization protocols.
Reference: [REYN92] <author> Reynolds Jr., P. </author> <title> F.,An Efficient Framework for Parallel Simulations, </title> <journal> International Journal in Computer Simulation, </journal> <volume> Vol. 2, No. 4, </volume> <year> (1992). </year>
Reference-contexts: In the next chapter we present a reduction-based framework for parallel discrete event simulations as introduced by Reynolds [REYN91] <ref> [REYN92] </ref>. In this chapter we develop this framework with respect the applicability of the framework, the computation model, the detailed hardware design, and correctness criteria. In Chapter 4 we present our contributions with respect to the algorithmic component of the framework. <p> This framework, a low-level characterization of all PDES synchronization protocols, provides a model for the efficient computation of critical synchronization values, such as GVT, particularly in support of the rollback chip [FUTG92] and adaptive aggressive PDES synchronization protocols [SRIN93]. The original framework, as proposed by Reynolds [REYN91] <ref> [REYN92] </ref>, outlined the three components of the framework and provided low-level algorithms to support a conservative PDES. Our research contributions are built on this work. Reynolds and Pancerella [REPA92] first described specific functional and implementation details of the hardware component of the framework. <p> This computation model is designed to ride the technology wave, such that as processors and memories get faster, the framework can exploit these gains.We use the term framework to describe the entire software/hardware ensemble meant to support PDES. Our framework has been described extensively to date. (See [REYN91], <ref> [REYN92] </ref>, [REPA92], [PANC92], [REPS92], and [REPS93].) In this chapter we discuss our contributions to the framework in three areas: the applicability of the framework to a wide range of PDES synchronization protocols, the correctness criteria on which the framework hardware is based, and the hardware component of the framework. <p> In the next few sections, we explore various T-values in parallel simulations. Each of the T-values we present are computed in a reduction network. 3.1.1. Reduced Values in Conservative Parallel Simulations A set of T-values to support a conservative PDES synchronization protocol have been presented in detail <ref> [REYN92] </ref> [REPA92]. The T-values computed are , the minimum next event time, and , the minimum logical timestamp of messages that have been sent but not acknowledged. <p> The maintenance of takes into account the messages that are in transit in the host communication network. The elimination of causality errors allows an LP to recognize when it can commit to processing an irreversible act such as I/O. Details can be found in <ref> [REYN92] </ref>. 3.1.2. Reduced Values in Optimistic Parallel Simulations In an optimistic PDES synchronization protocol, such as Time Warp [JEFF85], GVT can be efficiently computed by an LP at any time using our framework (See [SRRE93b]). <p> When an LP completes an event and sends a message to another LP, both its simulation time and its smallest unreceived message time may change. A simulation-wide invariant that must be maintained, as demonstrated by Reynolds <ref> [REYN92] </ref>, is that the event or message in the system with the smallest logical time must always be represented in at least one LPs simulation time or smallest unreceived message time. <p> We discuss the algorithmic requirements of both the host and auxiliary processors in the following sections. Specific details about framework algorithms can be found in <ref> [REYN92] </ref> (conservative PDES algorithms), [SRRE93] (optimistic PDES algorithms), and the next chapter (message acknowledgment algorithms). 3.6.1. Host Processor Algorithms In our parallel simulation framework, all processing of an LP event processing and event message sending and receiving occurs on the HP. <p> In order to maintain all event messages must be acknowledged. In the algorithms proposed by Reynolds <ref> [REYN92] </ref> acknowledgment messages are sent through the host network of the parallel machine. There are three problems with using the host communication network, and not the high-speed reduction network, for acknowledging messages. We discuss these next. 4.1.1. <p> The only difference between the two configurations is the type of reductions computed in the reduction network. 6.1.1. Conservative Simulation Algorithms The conservative PDES is based on the synchronization algorithms in <ref> [REYN92] </ref> (See Section 3.1.1.). LPs maintain a next event time and a smallest unreceived message time ; two globally reduced minimum operations are performed on these inputs, giving and , respectively.
Reference: [SAMA85] <author> Samadi, B., </author> <title> Distributed Simulation, Algorithms, and Performance Analysis, </title> <type> PhD Thesis, </type> <institution> Computer Science Department, University of California at Los Angeles, </institution> <address> Los Angeles, California, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], <ref> [SAMA85] </ref>, [LILA89b], [BELL90], [COKE91], and [TOGA93]. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. <p> The two values and are maintained locally by each LP i . GVT, by definition, is computed as the minimum of these two values: GVT computation and dissemination in this framework is a significant improvement in algorithm complexity and implementation efficiency over existing GVT maintenance schemes ([JEFF85], [JESO85], <ref> [SAMA85] </ref>, [LILA89], [BELL90], [COKE91], [TOGA93]). 3.1.3. Reduced Values as Lookahead Values in Parallel Simulations Minimum event processing times and lookahead values can be computed as globally reduced values. For example, the smallest future time that an LP can send event messages can be computed as a globally reduced value.
Reference: [SBUS90] <institution> Sun Microsystems, SBus Specification B.0, Sun Microsystems, Inc., Mountain View, California, </institution> <year> 1990. </year> <month> 172 </month>
Reference-contexts: The expected host communication latency time is approximately two milliseconds. Each auxiliary processor is a 25 MHz Motorola 68020 microprocessor with 256 Kbytes of RAM. The host-auxiliary processor interface is implemented with a dual-ported RAM, where a Sparc 2 accesses the dual-ported RAM through a Sun SBus interface <ref> [SBUS90] </ref>. The parallel reduction network consists of three ALUs in a binary tree-shaped network. The minor cycle time is 150 nanoseconds.
Reference: [SCHW80] <author> Schwartz, J. T., </author> <title> Ultracomputers, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 2, No. 4, </volume> <pages> pp. 484-521, </pages> <month> (October </month> <year> 1980). </year>
Reference-contexts: 0 x i = (x i-1 a i ), if f i = 0, 0 &lt; i &lt; n These formal definitions and applications of scans have been published by Blelloch [BLEL90], although scans were first introduced for the language APL [IVER62], and segmented scans were first suggested by Schwartz <ref> [SCHW80] </ref>. Blelloch [BLEL89] proposed a tree-structured hardware implementation of parallel prefix operations with O (n) components and O (log n) time complexity. The computation of parallel prefix reductions is a subset of our problem of computing and disseminating target-specific synchronization information, as discussed in Chapters 5 and 6. 20 2.2.2.
Reference: [SOBW88] <author> Sokol, L. M., Briscoe, D. P. and Wieland, A. P., MTW: </author> <title> A Strategy for Scheduling Discrete Simulation Events for Concurrent Execution, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 34-42, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: Iterative protocols are accurate, aggressive or non-aggressive, with or without risk, and loosely synchronous. Examples of iterative PDES synchronization protocols include the Bounded Lag protocol [LUBA88a], the Moving Time Window protocol <ref> [SOBW88] </ref>, Chandy and Shermans protocol for converting conditional events into unconditional events [CHSH89], an iterative algorithm based on the distance between objects [AYAN89], Nicols iterative algorithm [NICO90] [NICO91], and the Global Windowing Algorithm [DICK93]. 2.1.5. Hardware Support for PDES The need for special-purpose hardware to support PDES is well established. <p> We have a framework for disseminating this information easily. 3.1.4. Reduced Values in Iterative Parallel Simulations Iterative PDES synchronization protocols, such as Bounded Lag [LUBA88a], Moving Time Window <ref> [SOBW88] </ref>, and the aggressive Global Windowing Algorithm proposed by Dickens [DICK93], require the computation and dissemination of ceiling values or fault values. Lubachevsky [LUBA89] requires global establishment of minimum next event time and other values to compute opaque periods.
Reference: [SRIN92] <author> Srinivasan, S., </author> <title> Modeling a Framework for Parallel Simulations, </title> <type> Masters Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, Charlottesville, Virginia, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Finally, the framework was further developed by Reynolds, Pancerella, and Srinivasan [REPS93] with criteria for its correct operation, functional characteristics, description of a prototype hardware design, and performance results first reported by Srinivasan <ref> [SRIN92] </ref>. Our contributions to the framework, as presented in these publications, will be presented in this chapter. The framework itself is comprised of both hardware and software to support PDES. <p> First, we have demonstrated the applicability of this framework to a wide range of PDES synchronization protocols. The applicability of the framework to PDES synchronization protocols began in [REYN91] with reduced values and low-level algorithms to support conservative protocols. It continued in <ref> [SRIN92] </ref> with the applicability to optimistic protocols. <p> r r r 68 To implement batched acknowledgments on the reduction network, a concatenation of the smallest sequence number of a contiguous batch and the number of messages acknowledged in the batch are placed in the tag register and the message timestamp in the data register. (See Section 3.5.4.) Simulations <ref> [SRIN92] </ref> demonstrate that the batching of acknowledgments makes the system more robust. As the load increases on an AP, its unacknowledged message lists start growing. As a consequence, contiguous batches of messages form, and therefore, with batching of acknowledgments, APs perform more work per unit time than with single acknowledgments. <p> A proof of correctness for TPA can be found in [SRRE93]. 4.3.1. Performance In practice, we expect good performance from the version of TPA in Figure 4.4 since there are dedicated auxiliary processors monitoring the high-speed output from the reduction network and executing all acknowledgment algorithms. Furthermore, simulations <ref> [SRIN92] </ref> show that under normal load, the mean time to complete a two-phase acknowledgment is around 10 microseconds in a 32-processor system. <p> It is an open question how each acknowledgment algorithm will perform in a larger system. The simulation results in <ref> [SRIN92] </ref>, however, indicate that TPA is scalable to up to 32 processors with essentially no growth in the time required to acknowledge messages. 0.000 0.010 0.020 0.030 0.040 Mean Time Between Received Messages (seconds) 1.0 3.0 5.0 7.0 Maximum batch size Two-phase acknowledgment Single phase acknowledgment 105 4.9. <p> We compared the two with respect to execution time of the simulation and sizes of the message lists on auxiliary 106 processor. We conclude that TPA performs as well as SPA, and that memory utilization on the auxiliary processors is better. The simulation performance studies presented in <ref> [SRIN92] </ref> and performance studies presented in Section 4.8. are encouraging. They show significant potential for reduction-based acknowledgment algorithms. These experimental results, however, assume that reductions are computed globally. In the next chapter we introduce target-specific reductions to more accurately depict the state of a PDES. <p> Our presentation of acknowledgment algorithms included discussions and observations on the performance of the simulations executing in conjunction with each algorithm. We have developed the batched acknowledgment enhancement as a method of acknowledging several messages in a single reduced value. Simulations <ref> [SRIN92] </ref> have shown that batched acknowledgments allow our framework hardware to support smaller 160 granules of both event message processing times and host network communication latencies. We have implemented both the two-phase acknowledgment (TPA) and the single phase acknowledgment (SPA) on our four-node prototype framework hardware. <p> We have implemented two different message acknowledgment algorithms on our prototype hardware. Based on performance results, we have concluded that the two-phase acknowledgment is better. This conclusion is not drawn from a large simulation, however. Simulations <ref> [SRIN92] </ref> have demonstrated the scalability of the two-phase acknowledgment to simulations of size 32. It is open question how the two message acknowledgment algorithms will compare when the number of processors is increased. The empirical results presented in Chapter 6 are encouraging results for small simulations.
Reference: [SRIN93] <author> Srinivasan, S., </author> <title> Adaptive Synchronization Algorithms for Parallel Discrete Event Simulation, A Research Proposal, </title> <institution> Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: This framework, a low-level characterization of all PDES synchronization protocols, provides a model for the efficient computation of critical synchronization values, such as GVT, particularly in support of the rollback chip [FUTG92] and adaptive aggressive PDES synchronization protocols <ref> [SRIN93] </ref>. The original framework, as proposed by Reynolds [REYN91] [REYN92], outlined the three components of the framework and provided low-level algorithms to support a conservative PDES. Our research contributions are built on this work. <p> Finally, we expect the dissemination of near-perfect state information to support adaptive PDES synchronization protocols, those that combine the strengths of both aggressive and non-aggressive protocols while limiting the weaknesses. This is a topic of current research <ref> [SRIN93] </ref>. If target-specific synchronization information is available to LPs in a PDES, all LPs receive more accurate state information and can process events accordingly. The final result is a framework for PDES that efficiently supports a wide range of PDESs. 5.1.5. <p> Since TSVT is based only on information relevant to the target LP, thus, in a sense, making it more accurate than GVT, fossils (state information that precedes a TSVT) will be reduced. In <ref> [SRIN93] </ref>, it was proposed that effective PDES protocols will be those that do adaptive aggressive processing, characterized by controlled aggressiveness, where the benefits of aggressiveness are maximized and its costs are minimized. The impact on state saving is clearly evident.
Reference: [SRRE93] <author> Srinivasan, S. and Reynolds Jr., P. F., </author> <title> Hardware Support for Aggressive Parallel Discrete Event Simulation, </title> <institution> Computer Science Report No. TR-93-07, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: We discuss the algorithmic requirements of both the host and auxiliary processors in the following sections. Specific details about framework algorithms can be found in [REYN92] (conservative PDES algorithms), <ref> [SRRE93] </ref> (optimistic PDES algorithms), and the next chapter (message acknowledgment algorithms). 3.6.1. Host Processor Algorithms In our parallel simulation framework, all processing of an LP event processing and event message sending and receiving occurs on the HP. <p> Each AP i maintains data structures in support of the event message acknowledgments and reduction operations supporting the PDES synchronization protocol. Specific details regarding the correctness of an aggressive PDES executing on a processor pair in the framework hardware can be found in <ref> [SRRE93] </ref>. 4.2.1. Host Processor Requirements for Acknowledgment Algorithms Recall that the communication channel between an HP and its AP is functionally a FIFO (See Section 3.5.3.). An HP enqueues tagged entries into the FIFO with the tag indicating the nature of the communication. <p> As discussed, all acknowledgments will be processed by both sending and receiving APs. This comes at the cost of having to do two reductions in the reduction network. A proof of correctness for TPA can be found in <ref> [SRRE93] </ref>. 4.3.1. Performance In practice, we expect good performance from the version of TPA in Figure 4.4 since there are dedicated auxiliary processors monitoring the high-speed output from the reduction network and executing all acknowledgment algorithms. <p> Proof of Correctness of SPA A proof of correctness for an aggressive PDES synchronization protocol executing on the framework hardware was presented by Srinivasan and Reynolds <ref> [SRRE93] </ref>. <p> Properties of the Hardware and Algorithms Before proving the correctness of the single phase acknowledgment, we present some properties of the framework hardware and corresponding algorithms. Some of these properties appeared first in <ref> [SRRE93] </ref>. 4.5.1.1. Properties of the Framework Hardware Property P1 (no loss): No communication from the HP to AP is lost. 86 Property P2 (reduction operation): The PRN computes reductions on state vectors. Acknowledgment T-values are computed with a minimum operation that is a tagged selective operation. 4.5.1.2. <p> If we show that GVT p (t) is monotonically non-decreasing for all times t when a reduction cycle is started, then GVT c (t) will be monotonically non-decreasing for all real times t. We show this. r 88 Srinivasan and Reynolds <ref> [SRRE93] </ref> showed that GVT p (t) is strictly non-decreasing when a single processor, and not a host-auxiliary processor pair, was used to execute simulation events and interface with the reduction network. <p> The asynchronous nature of the auxiliary processors and the FIFOs between the processors in a HP-AP pair make our proof more complex. We build on the proofs of Srinivasan and Reynolds <ref> [SRRE93] </ref> and use similar proof techniques. The methods for maintaining the local T-values and and the computation of both and do not change in this algorithm. The difference between TPA, proven correct in [SRRE93], and SPA, which we prove correct next, is the method of acknowledging messages. <p> We build on the proofs of Srinivasan and Reynolds <ref> [SRRE93] </ref> and use similar proof techniques. The methods for maintaining the local T-values and and the computation of both and do not change in this algorithm. The difference between TPA, proven correct in [SRRE93], and SPA, which we prove correct next, is the method of acknowledging messages. In the following proof the only assumption made about acknowledgments is that there is a mechanism for the receiver of each message to notify the sender of the receipt of the message. <p> Therefore, G = = . (ii) AP j has not processed the FIFO entry SENT_MSG. In <ref> [SRRE93] </ref>, it was shown that if a rollback chain (or possibly several rollback chains) are followed towards the root of the chain (or the root of the smallest rollback chain), there exists an AP k such that . (The LP at the root of the rollback chain is the LP that <p> Furthermore, the algorithms are correct when the reductions are being computed asynchronously with the execution of the simulation. A correctness proof for TPA was presented in <ref> [SRRE93] </ref>, and correctness proofs for SPA were presented here. Finally, we have implemented both the two-phase acknowledgment and the single phase acknowledgment on our prototype framework hardware. We compared the two with respect to execution time of the simulation and sizes of the message lists on auxiliary 106 processor.
Reference: [SRRE93b] <author> Srinivasan, S. and Reynolds Jr., P. </author> <title> F.,Non-interfering GVT Computation Via Asynchronous Global Reductions, </title> <booktitle> Proceedings of the 1993 Winter Simulation Conference, </booktitle> <address> Los Angeles, California, </address> <pages> pp. 740-749, </pages> <month> (December </month> <year> 1993). </year>
Reference-contexts: Details can be found in [REYN92]. 3.1.2. Reduced Values in Optimistic Parallel Simulations In an optimistic PDES synchronization protocol, such as Time Warp [JEFF85], GVT can be efficiently computed by an LP at any time using our framework (See <ref> [SRRE93b] </ref>).
Reference: [STON90] <author> Stone, H. S., </author> <title> High-Performance Computer Architecture, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: In this section we discuss related hardware efforts for barrier synchronization, computing reductions and sorting. 2.3.1. Hardware for Barrier Synchronization Several researchers have proposed the use of hardware to implement barrier synchronization. Hoshino [HOSH85] has an efficient barrier synchronization in the PAX computer. Stone <ref> [STON90] </ref> suggests the use of global busses to compute maximum values and to implement fetch-and-increment. The hardware that we propose, on the other hand, provides support for a larger class of algorithms than barrier synchronization algorithms. 2.3.2.
Reference: [TANE89] <author> Tanenbaum, A. S., </author> <title> Computer Networks - Second Edition, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: This enhancement improves the efficiency of employing the reduction network to acknowledge messages. Messages arriving from the same sender can be acknowledged as sequences by acknowledging the message ID of the largest in-sequence message; this same scheme is common in computer networks <ref> [TANE89] </ref>. Batched acknowledgments can be implemented by adding a third component to and . Now these message acknowledgment T-values have the form: message time, message ID, batch size-.
Reference: [THIN92] <author> Thinking Machines Corporation, </author> <title> The Connection Machine CM-5 Technical Summary, </title> <institution> Thinking Machines Corporation, Cambridge, Massachusetts, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Thinking Machines CM-5 Supercomputer The Thinking Machines CM-5 <ref> [THIN92] </ref> contains two separate networks for different types of communication and synchronization: the data network is the primary message-passing network in the machine and the control network provides hardware support for common cooperative operations.
Reference: [TOGA93] <author> Tomlinson, A. I. and Garg, V. K., </author> <title> An Algorithm for Minimally Latent Global Virtual Time, </title> <booktitle> Proceedings of the 1993 Workshop on Parallel and Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 35-42, </pages> <month> (May </month> <year> 1993). </year>
Reference-contexts: The cancelback protocol [JEFF90], proposed by Jefferson, provides optimal storage management. Performance studies [DAFU93] have shown that the GVT maintenance scheme is critical to this memory management protocol. There are many proposed GVT computation schemes. Representative ones include [JEFF85], [JESO85], [SAMA85], [LILA89b], [BELL90], [COKE91], and <ref> [TOGA93] </ref>. The third main problem with a rollback-based simulation is that the process of rolling back computation can degrade performance. A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. <p> GVT, by definition, is computed as the minimum of these two values: GVT computation and dissemination in this framework is a significant improvement in algorithm complexity and implementation efficiency over existing GVT maintenance schemes ([JEFF85], [JESO85], [SAMA85], [LILA89], [BELL90], [COKE91], <ref> [TOGA93] </ref>). 3.1.3. Reduced Values as Lookahead Values in Parallel Simulations Minimum event processing times and lookahead values can be computed as globally reduced values. For example, the smallest future time that an LP can send event messages can be computed as a globally reduced value.
Reference: [TUXU92] <author> Turner, S. and Xu, M., </author> <title> Performance Evaluation of the Bounded Time Warp Algorithm, </title> <booktitle> Proceedings of the 1992 Western Simulation MultiConference on Parallel and Distributed Simulation, </booktitle> <address> Newport Beach, California, </address> <pages> pp. 117-126, </pages> <month> (January </month> <year> 1992). </year>
Reference-contexts: A cascading rollback is a chain reaction of rollbacks where the number of LPs increases without bound [LUSW89]. Echoing is a pattern of self-fueled rollbacks whose amplitude increases without bound [LUSW89]. Turner and Xu <ref> [TUXU92] </ref> have reported cascading rollbacks and echoing 16 which significantly degrade performance in their telephone switching network simulation. Mitra and Mitrani [MIMI84] and Lubachevsky [LUSW89] have developed models to show that echoing can occur.
Reference: [WALA89] <author> Wagner, D. B. and Lazowska, E. D., </author> <title> Parallel Simulation of Queueing Networks: Limitations and Potentials, </title> <booktitle> Proceedings of the 1989 ACM SIGMETRICS and PERFORMANCE 89: International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Berkeley, California, </address> <pages> pp. 146-155, </pages> <month> (May </month> <year> 1989). </year> <month> 173 </month>
Reference-contexts: Researchers have shown that minimum event processing times and lookahead values can produce significant performance improvements in a non-aggressive PDES synchronization protocol. (See [NIRE84], [FUJI87], [FUJI88], [REMM88], [NICO88], [NICO88b], [LILA89], <ref> [WALA89] </ref>, and [FEKL92b] for performance results on the effects of lookahead values.) Conservative protocols require the communication topology to be static and known a priori. Furthermore, these protocols do not efficiently support fully-connected communication graphs.
Reference: [YUGD91] <author> Yu, M. L., Ghosh, S. and DeBenedictis, E., </author> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 39-43, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: Deadlock occurs when a parallel simulation has cycles where each LP in the cycle is blocked and waiting for a message from another LP in the same cycle. There are many research efforts in deadlock handling. (See [CHMI79], [MISR86], [PEWM79], [PEWM79b], [REYN82], [NICO84], [CHMI81], <ref> [YUGD91] </ref>, [DEGY91], and [LITR90] for different approaches to dealing with the deadlock problem.) Either deadlock avoidance or deadlock detection and recovery is an overhead for the simulation.
References-found: 108

