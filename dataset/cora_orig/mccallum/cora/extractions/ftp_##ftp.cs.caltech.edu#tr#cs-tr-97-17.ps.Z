URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-97-17.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Title: Validation of Average Error Rate Over Classifiers  
Author: Eric Bax 
Keyword: Key words machine learning, Vapnik-Chervonenkis, validation.  
Date: July 1, 1997  
Abstract: We examine methods to estimate the average and variance of test error rates over a set of classifiers. We begin with the process of drawing a classifier at random for each example. Given validation data, the average test error rate can be estimated as if validating a single classifier. Given the test example inputs, the variance can be computed exactly. Next, we consider the process of drawing a classifier at random and using it on all examples. Once again, the expected test error rate can be validated as if validating a single classifier. However, the variance must be estimated by validating all classifers, which yields loose or uncertain bounds. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bax, E., Z. Cataltepe, and J. Sill. </author> <year> (1997). </year> <title> Alternative error bounds for the classifier chosen by early stopping. </title> <publisher> CalTech-CS-TR-97-08. </publisher>
Reference-contexts: In the simplest scheme, the error rate of the classifier from the large set is estimated by inference from the most similar core classifier. In <ref> (Bax, Cataltepe, and Sill, 1997) </ref>, this scheme is used to estimate the test error rate of the classifier chosen by early stopping. A more complex method employs linear programming to estimate the error rate by inference from all core classifiers. <p> In (Bax, Cataltepe, and Sill, 1997), this scheme is used to estimate the test error rate of the classifier chosen by early stopping. A more complex method employs linear programming to estimate the error rate by inference from all core classifiers. In <ref> (Bax, 1997) </ref>, linear programming is used to estimate the test error rates of voting committees and other ensembles. If there are few core classifiers, their error rates can be uniformly estimated with high confidence.
Reference: <author> Bax, E. </author> <year> (1997). </year> <title> Validation of voting committees. </title> <publisher> CalTech-CS-TR-97-13. </publisher>
Reference-contexts: In the simplest scheme, the error rate of the classifier from the large set is estimated by inference from the most similar core classifier. In <ref> (Bax, Cataltepe, and Sill, 1997) </ref>, this scheme is used to estimate the test error rate of the classifier chosen by early stopping. A more complex method employs linear programming to estimate the error rate by inference from all core classifiers. <p> In (Bax, Cataltepe, and Sill, 1997), this scheme is used to estimate the test error rate of the classifier chosen by early stopping. A more complex method employs linear programming to estimate the error rate by inference from all core classifiers. In <ref> (Bax, 1997) </ref>, linear programming is used to estimate the test error rates of voting committees and other ensembles. If there are few core classifiers, their error rates can be uniformly estimated with high confidence.
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition pp. </title> <address> 364-371. </address> <publisher> Oxford University Press. </publisher>
Reference: <author> Feller, W. </author> <year> (1968). </year> <title> An Introduction to Probability Theory and Its Applications p. 128. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: To bound the error due to estimating the test error variance over classifiers by the validation error variance over classifiers, first expand the test error variance using a well-known identity from probability theory <ref> (Feller, 1968, p.128) </ref>. Var (-0 ) = E (-02 ) (E (-0 )) 2 (26) Replace test error rates by validation error rates and residuals. Var (-0 ) = E ((- + ffi) 2 ) (E (- + ffi)) 2 (27) Expectation is a linear operator.
Reference: <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Am. Stat. Assoc. J. </journal> <pages> pp. 13-30. </pages>
Reference-contexts: We follow the development found in (Vapnik, 1982), in which a result due to Hoeffding <ref> (Hoeffding, 1963) </ref> is used to bound the confidence of estimating the test error rate by the validation error rate. Let g m 2 fg 1 ; : : : ; g M g be a classifier chosen without reference to validation error rate.
Reference: <author> Jacobs, R. A., M. I. Jordan, S. J. Nowlan, and G E Hinton. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <booktitle> Neural Computation 3 (1) pp. </booktitle> <pages> 79-87. </pages>
Reference: <author> Jordan, M. I. and R. A. Jacobs. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation 6 (2) pp. </booktitle> <pages> 181-214. </pages>
Reference: <author> Perrone, M. P., and L. N. Cooper. </author> <year> (1993). </year> <title> When networks disagree: ensemble methods for hybrid neural networks. </title> <editor> in R. J. Mammone (Ed.). </editor> <booktitle> Artificial Neural Networks for Speech and Vision pp. </booktitle> <pages> 126-142. </pages> <publisher> Chapman & Hall, London. </publisher>
Reference: <author> Vapnik, V. N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data p.31. </title> <publisher> Springer-Verlag New York. Inc. </publisher>
Reference-contexts: We follow the development found in <ref> (Vapnik, 1982) </ref>, in which a result due to Hoeffding (Hoeffding, 1963) is used to bound the confidence of estimating the test error rate by the validation error rate. <p> We follow the derivation for VC analysis found in <ref> (Vapnik, 1982) </ref>. Consider the probability of failure for at least one single-classifier estimate. Prfj-0 1 -1 j * or : : : or j-0 Bound the probability of the union event by the sum of event probabilities.
Reference: <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks. </booktitle> <pages> 5 (2) pp. 241-259. </pages>
References-found: 10

