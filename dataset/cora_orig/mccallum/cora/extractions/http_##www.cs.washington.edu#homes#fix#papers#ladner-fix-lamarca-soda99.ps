URL: http://www.cs.washington.edu/homes/fix/papers/ladner-fix-lamarca-soda99.ps
Refering-URL: http://www.cs.washington.edu/homes/fix/papers/
Root-URL: http://www.cs.washington.edu
Title: Cache Performance Analysis of Traversals and Random Accesses  
Author: Richard E. Ladner James D. Fix Anthony LaMarca 
Note: To appear in the Tenth Annual ACM-SIAM Symposium on Discrete Algorithms  
Abstract: This paper describes a model for studying the cache performance of algorithms in a direct-mapped cache. Using this model, we analyze the cache performance of several commonly occurring memory access patterns: (i) sequential and random memory traversals, (ii) systems of random accesses, and (iii) combinations of each. For each of these, we give exact expressions for the number of cache misses per memory access in our model. We illustrate the application of these analyses by determining the cache performance of two algorithms: the traversal of a binary search tree and the counting of items in a large array. Trace driven cache simulations validate that our analyses accurately predict cache performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, M. Horowitz, and J. Hennessy. </author> <title> An analytical cache model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(2) </volume> <pages> 184-215, </pages> <year> 1989. </year>
Reference-contexts: In direct-mapped caches, the data item brought into the cache can only go to one place. The result is that our analyses are both different and likely more useful in practice than cache analyses for fully associative caches [17]. There have been several studies <ref> [1, 17, 19, 20] </ref> that have tried to quantify the cache performance of programs by summarizing or analyzing actual memory access traces. Our work differs from this in that we do not examine the trace of a program, but just the actual algorithm itself.
Reference: [2] <author> A. Aggarwal, K. Chandra, and M. Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In 28th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 204-216, </pages> <year> 1987. </year>
Reference-contexts: A number of authors have established models of memory hierarchies then designed and analyzed algorithms for these models <ref> [2, 3, 4, 6] </ref>. In addition, there is a long history of external memory algorithms, for example, for sorting and searching [8, 15, 12, 21, 22, 23].
Reference: [3] <author> A. Aggarwal, K. Chandra, and M. Snir. </author> <title> A model for hierarchical memory. </title> <booktitle> In 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 305-314, </pages> <year> 1987. </year>
Reference-contexts: A number of authors have established models of memory hierarchies then designed and analyzed algorithms for these models <ref> [2, 3, 4, 6] </ref>. In addition, there is a long history of external memory algorithms, for example, for sorting and searching [8, 15, 12, 21, 22, 23].
Reference: [4] <author> A. Aggarwal and J. Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: A number of authors have established models of memory hierarchies then designed and analyzed algorithms for these models <ref> [2, 3, 4, 6] </ref>. In addition, there is a long history of external memory algorithms, for example, for sorting and searching [8, 15, 12, 21, 22, 23].
Reference: [5] <author> A. V. Aho, P. J. Denning, and J. D. Ullman. </author> <title> Principles of optimal page replacement. </title> <journal> Journal of the ACM, </journal> <volume> 18(1) </volume> <pages> 80-93, </pages> <year> 1971. </year>
Reference-contexts: Our work differs from this in that we do not examine the trace of a program, but just the actual algorithm itself. A number of studies have analyzed different memory hierarchy alternatives by modeling program or multiprogram access patterns by a simple stochastic model, the Independent Reference Model (IRM) <ref> [5, 16, 11] </ref>. Rather than apply such a model to entire programs, we apply and extend the IRM cache analysis of Rao [16] to algorithm subcomponents that exhibit random access behavior. 3 The Model We choose a simple model of cache memory for our analysis.
Reference: [6] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The uniform memory hierarchy model of computation. </title> <address> Algo-rithmica, 12(2-3):72-109, </address> <year> 1994. </year>
Reference-contexts: A number of authors have established models of memory hierarchies then designed and analyzed algorithms for these models <ref> [2, 3, 4, 6] </ref>. In addition, there is a long history of external memory algorithms, for example, for sorting and searching [8, 15, 12, 21, 22, 23].
Reference: [7] <author> E. Coffman and P. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: However, often a random access pattern can be analyzed approximately by assuming the inde 0.0 0.4 0.8 2e+04 4e+04 6e+04 8e+04 1e+05 1.2e+05 misses per tree node number of tree nodes predicted measured by our model. pendent reference assumption <ref> [7] </ref>. In this model each access is independent of all previous accesses. In our previous paper [13] we called the analysis of a set of random access patterns collective analysis and in this paper we adopt the collective analysis approach in order to examine algorithms that exhibit random access.
Reference: [8] <author> L. Gotlieb. </author> <title> Optimal multi-way search trees. </title> <journal> SIAM Journal of Computing, </journal> <volume> 10(3) </volume> <pages> 422-433, </pages> <month> Aug </month> <year> 1981. </year>
Reference-contexts: A number of authors have established models of memory hierarchies then designed and analyzed algorithms for these models [2, 3, 4, 6]. In addition, there is a long history of external memory algorithms, for example, for sorting and searching <ref> [8, 15, 12, 21, 22, 23] </ref>. One characteristic of much of this previous work is that in addition to performing calculations, the algorithms can explicitly move data to and from the cache.
Reference: [9] <author> J. Hennesey and D. Patterson. </author> <booktitle> Computer Architecture </booktitle>
Reference-contexts: A read or write to a variable that occupies multiple blocks is modeled as a sequence of accesses to the blocks. We do not distinguish between reads and writes because we assume a copy back architecture with a write buffer for the cache <ref> [9] </ref>. In the copy back architecture writes to the cache are not immediately passed to the memory. <p> While the cache performance of traversals and uniformly random access patterns do not benefit from set-associativity [11], set associativity tends to reduce the cache misses per access for other access patterns and in practice <ref> [9] </ref>. It is our hope that this paper will encourage other researchers to begin studying the cache performance of algorithms with interesting memory access patterns and to study them in realistic cache architectures including direct-mapped caches and set-associative caches with a low degree of associativity.
References-found: 9

