URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94469.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Problem Formulations and Other Optimization Issues in Muldisciplinary Optimization  
Author: J. E. Dennis, Jr and Robert Michael Lewis. 
Keyword: Constrained optimization, multidisciplinary design optimization, optimal design, computational engineering, parallel computation.  
Abstract: This paper is about multidisciplinary (design) optimization, or MDO, the coupling of two or more analysis disciplines with numerical optimization. The "individual discipline feasible" (IDF) approaches introduced here make use of existing specialized analysis codes, and they introduce significant opportunities for coarse-grained computational parallelism particularly well-suited to heterogeneous computing environments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> AIAA Technical Committee on Multidisciplinary Design Optimization (MDO). White paper on current state of the art. American Institute of Aeronautics and Astronautics, </institution> <year> 1991. </year>
Reference-contexts: Sobieszczanski-Sobieski argued for its significance at least as far back as 1982 [22]. The status of this research area is well summarized in <ref> [1] </ref>. The observation that lies at the root of the formulations we propose is that the disciplinary analyses of MDO|the simulations of the response of the physical system in the MDO problem to those parameters we can control|constitute equality constraints in the optimization problem.
Reference: [2] <author> Natalia Alexandrov. </author> <title> Multilevel algorithms for nonlinear equations and equality constrained optimization. </title> <type> Technical Report TR93-20, </type> <institution> Department of Computational and Applied Mathematics, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In that work, the "disciplines" corresponded to the sub-domains produced by applying a domain decomposition method to the governing differential equations. We are also very hopeful of the success of a nonlinear multilevel approach being developed by Natalia Alexandrov of ICASE <ref> [2, 3] </ref>. The decomposition of MDO problems along the lines of the constituent disciplines also leads one to a natural coarse-grained computational parallelism, a point worth noting but one we will not discuss here in any detail. <p> The multilevel trust region generalizations of the methods of Brown and Brent to optimization are completely new, having been introduced in the thesis <ref> [2] </ref>. They are motivated by the potential expense of computing derivatives in MDO, and by the possible convenience of being able to process each discipline, or each block of cross disciplinary constraints, independently in the optimization. <p> Brent [5] and other authors have suggested different perspectives and variants, but no one had previously globalized these methods. The multilevel methods being developed at ICASE and Rice, see <ref> [2, 3] </ref>, can be viewed as globalizations of the Brown-Brent methods for nonlinear equations and their extension to constrained optimization. The basic idea amounts to successive minimization of progressively smaller-dimensional (reduced) models of the arbitrarily partitioned constraint blocks and finally the reduced model of the objective function.
Reference: [3] <author> Natalia Alexandrov and J. E. Dennis, Jr. </author> <title> Multilevel trust-region algorithms for nonlinear equations and equality constrained optimization. </title> <type> Technical report, </type> <institution> ICASE, </institution> <year> 1994. </year> <note> in progress. </note>
Reference-contexts: In that work, the "disciplines" corresponded to the sub-domains produced by applying a domain decomposition method to the governing differential equations. We are also very hopeful of the success of a nonlinear multilevel approach being developed by Natalia Alexandrov of ICASE <ref> [2, 3] </ref>. The decomposition of MDO problems along the lines of the constituent disciplines also leads one to a natural coarse-grained computational parallelism, a point worth noting but one we will not discuss here in any detail. <p> Brent [5] and other authors have suggested different perspectives and variants, but no one had previously globalized these methods. The multilevel methods being developed at ICASE and Rice, see <ref> [2, 3] </ref>, can be viewed as globalizations of the Brown-Brent methods for nonlinear equations and their extension to constrained optimization. The basic idea amounts to successive minimization of progressively smaller-dimensional (reduced) models of the arbitrarily partitioned constraint blocks and finally the reduced model of the objective function.
Reference: [4] <author> J.-F. M. Barthelemy and R. T. Haftka. </author> <title> Approximation concepts for optimum structural design. </title> <journal> Structural Optimization, </journal> <volume> 5 </volume> <pages> 129-144, </pages> <year> 1993. </year>
Reference-contexts: A variation on this straightforward 3 MDF approach that might reduce the computational expense would be to mimic for MDO what has been done in single discipline structural optimization by using so-called approximation concepts <ref> [4] </ref> for each analysis code.
Reference: [5] <author> Richard P. Brent. </author> <title> Some efficient algorithms for solving systems of nonlinear equations. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 10(2) </volume> <pages> 327-344, </pages> <year> 1973. </year>
Reference-contexts: Brown's method is locally q-quadratically convergent, the same rapid rate of convergence as Newton's method, and its variant that uses finite-difference derivatives can be implemented to require the calculation of fewer|roughly half as many|sensitivities than we would need to calculate for Newton's method. Brent <ref> [5] </ref> and other authors have suggested different perspectives and variants, but no one had previously globalized these methods. The multilevel methods being developed at ICASE and Rice, see [2, 3], can be viewed as globalizations of the Brown-Brent methods for nonlinear equations and their extension to constrained optimization.
Reference: [6] <author> Kenneth M. Brown. </author> <title> A quadratically convergent Newton-like method based upon Gaussian elimination. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 6(4) </volume> <pages> 560-569, </pages> <month> December </month> <year> 1969. </year>
Reference-contexts: The multilevel algorithms can be applied to all classes of MDO formulations. Brown <ref> [6] </ref> suggested a nonlinear generalization of Gaussian elimination for solving square systems of nonlinear equations.
Reference: [7] <author> Maria R. Celis. </author> <title> A Trust Region Strategy for Nonlinear Equality Constrained Optimization. </title> <type> PhD thesis, </type> <institution> Department of Mathematical Sciences, Rice University, Houston, TX, </institution> <note> 1985; also available as TR85-4, </note> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <address> Houston, TX 77251-1892. </address>
Reference-contexts: We will discuss two related classes of trust region algorithms for the equality constrained MDO formulations. The first of these is a class of large-scale constrained optimization algorithms that are descendents of the Celis-Dennis-Tapia (CDT) algorithm <ref> [7] </ref>. The other class of trust region methods are generalizations to optimization of the methods of Brown and Brent for nonlinear equations.
Reference: [8] <author> E. J. Cramer, J. E. Dennis, Jr., P. D. Frank, R. M. Lewis, and G. R. Shubin. </author> <title> On alternative problem formulations for multidisciplinary design optimization. </title> <booktitle> Proceedings of the Fourth AIAA/USAF/NASA/OAI Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <month> September </month> <year> 1992. </year> <institution> Cleveland, Ohio. </institution>
Reference-contexts: The key is not to introduce so many new variables that the problem grows too large. We will give an abbreviated presentation of our taxonomy of the formulations of MDO, using notation that differs slightly from that used in the AIAA paper <ref> [8] </ref> or the more developed paper [9]. In the latter paper, we give a complete framework for describing MDO problems. For purposes of the present exposition, the example we will discuss in this section will be an MDO problem with two disciplines.
Reference: [9] <author> E. J. Cramer, J. E. Dennis, Jr., P. D. Frank, R. M. Lewis, and G. R. Shubin. </author> <title> Problem formulation for multidisciplinary design optimization. </title> <note> SIAM Journal on Optimization, to appear 1994. </note>
Reference-contexts: We were able to do this, and we have extended these ideas to MDO in joint work with Evin Cramer, Paul Frank, and Greg Shubin of Boeing Computer Services. A detailed version can be found in <ref> [9] </ref>. In this paper we will present a sketch of that work together with a brief discussion of some optimization algorithms likely to be useful for MDO. We recommend Greg Shubin's computational study [21] of these formulations applied to a model problem in aircraft design. <p> The key is not to introduce so many new variables that the problem grows too large. We will give an abbreviated presentation of our taxonomy of the formulations of MDO, using notation that differs slightly from that used in the AIAA paper [8] or the more developed paper <ref> [9] </ref>. In the latter paper, we give a complete framework for describing MDO problems. For purposes of the present exposition, the example we will discuss in this section will be an MDO problem with two disciplines. <p> We say that we have decomposed the problem at a low bandwidth interdisciplinary interface. Additional IDF formulations are described in <ref> [9] </ref>, including the sequenced IDF formulations, which were anticipated in the literature by equation (12) in [18]. The more detailed presentation given in [9] discusses further how we might use the IDF approach to obtain constrained MDO formulations that are much smaller than in the AAO approach. <p> We say that we have decomposed the problem at a low bandwidth interdisciplinary interface. Additional IDF formulations are described in <ref> [9] </ref>, including the sequenced IDF formulations, which were anticipated in the literature by equation (12) in [18]. The more detailed presentation given in [9] discusses further how we might use the IDF approach to obtain constrained MDO formulations that are much smaller than in the AAO approach. <p> The method avoids the cost of simultaneous sensitivity analyses. In addition, this approach enables us to determine optimization steps while computing fewer sensitivities than in Newton's method. Another important application for the multilevel algorithms is MDA. As mentioned previously and discussed in detail in <ref> [9] </ref>, MDA, or the procedure of bringing all the disciplines into equilibrium, can be extremely expensive because of the need to execute the analysis codes and to compute the associated sensitivities repeatedly.
Reference: [10] <author> J. E. Dennis, Jr., Mahmoud El-Alem, and Maria Cristina Maciel. </author> <title> A global convergence theory for general trust-region-based algorithms for equality constrained optimization. </title> <type> Technical Report TR92-28, </type> <institution> Rice University, Department of Computational and Applied Mathematics, Houston, Texas, </institution> <year> 1992. </year> <month> Revised April </month> <year> 1994. </year>
Reference-contexts: The quantity c is chosen to ensure convergence to feasibility; in theory we need only require a fraction of Cauchy decrease on the sum of squares of the linearized con straints <ref> [10] </ref>. Having approximately solved this subproblem, we apply an acceptance test to the new iterate. This acceptance test currently uses the augmented La-grangian as its merit function. The augmented La-grangian combines the objective and the constraints to measure progress towards the competing goals of optimality and feasibility.
Reference: [11] <author> J. E. Dennis, Jr. and Virginia Torczon. </author> <title> Direct search methods on parallel machines. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 448-474, </pages> <month> Novem-ber </month> <year> 1991. </year>
Reference-contexts: For this reason we have placed the discussion of these methods in Section 3.2, where we discuss the issue of sensitivities in MDO. 3.1.1 Parallel direct search methods We can apply to the MDF formulation the parallel direct search methods developed by Dennis and Tor-czon <ref> [11] </ref>. Direct search methods are optimization methods that neither require nor estimate derivatives, so they avoid one of the major difficulties in 7 the MDF approach to MDO. Instead of trying to compute gradients, direct search methods move towards optimality by working directly with values of the objective function. <p> Moreover, when the objective function is differentiable, the direct search methods are supported by a convincing convergence theory [26, 28], making them an attractive alternative to more ad hoc methods such as genetic algorithms or neural networks. Dennis and Torczon <ref> [11] </ref> have developed direct search methods specifically for parallel computers, and the parallelism is of an uncommon type insofar as in principle, there is no upper limit to the number of processors that can be used effectively in the algorithm.
Reference: [12] <author> Mahmoud M. El-Alem. </author> <title> A global convergence theory for the Celis-Dennis-Tapia trust region algorithm for constrained optimization. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 28(1) </volume> <pages> 266-290, </pages> <year> 1991. </year>
Reference-contexts: The optimization algorithm would then guide the refinement of the accuracy of the analyses as the optimization progresses towards an optimal and feasible solution. For the supporting theory, see <ref> [12] </ref>. <p> This acceptance test currently uses the augmented La-grangian as its merit function. The augmented La-grangian combines the objective and the constraints to measure progress towards the competing goals of optimality and feasibility. The choice of the penalty parameter in the augmented Lagrangian follows that in <ref> [12] </ref>. In contrast to augmented Lagrangian methods, the penalty weight in the augmented Lagrangian does not directly enter into the computation of the optimization step. Instead, it only figures in the step acceptance test.
Reference: [13] <author> Samuel K. Eldersveld. </author> <title> Automated part nesting for just-in-time manufacturing. TechNet: </title> <journal> Briefs on Computing Technology, </journal> <volume> 7(3) </volume> <pages> 7-8, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Recently it was inserted as a crucial component of a Boeing parts nesting code developed by Eldersveld and Grandine <ref> [13, 17] </ref> to plan for minimizing waste while cutting aircraft parts from sheet metal. 3.1.2 The CDT algorithm for large-scale equality constrained optimization We have implemented an algorithm that can handle very large equality constrained optimization problems and successfully used it in the parallel solution of parameter estimation for flow in
Reference: [14] <author> P. D. Frank, A. J. Booker, T. P. Caudell, and M. J. Healy. </author> <title> Optimization and search methods for multidisciplinary design. </title> <booktitle> Proceedings of the Fourth AIAA/USAF/NASA/OAI Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <month> September </month> <year> 1992. </year> <institution> Cleveland, Ohio. </institution> <month> 10 </month>
Reference-contexts: Space permits us to give little detail concerning these topics. We recommend <ref> [14] </ref> for an informative study on the use of optimization methods in MDO. Exploratory optimization and objective synthesis are interactive procedures involving the designer. 6 These procedures should prove very useful in the pre-liminary stages of the MDO solution process. <p> This is motivated, frankly, by our suspicion of the notion of "push-button" design, especially for problems as complex as MDO. 3.1 NLP algorithms While there are many computational optimization approaches that present themselves as candidates to solve MDO problems, the most promising involve using calculus-based quasi-Newton methods for numerical optimization <ref> [14] </ref>. We will discuss several optimization algorithms that we believe will prove effective in solving the different formulations of the MDO problem.
Reference: [15] <author> P. D. Frank and G. R. Shubin. </author> <title> A comparison of optimization-based approaches for a model computational aerodynamics design problem. </title> <institution> Technical Report Engineering Computing and Analysis Division ECA-TR-136-R1, Boeing Computer Services, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: In IDF, it is the less expensive y 1 (x; y 12 ) and y 2 (x; y 21 ) with y ij treated as independent variables. In AAO, y is an independent variable. The AAO approach has been investigated in a number of engineering fields, for instance, aerodynamics <ref> [15] </ref>, structural optimization [19], where it is called SAD or SAND, for simultaneous analysis and design, and chemical engineering [24], where it is called the open equations or nonlinear programming approach. We have applied this approach to parameter estimation for flow in porous media and for chemical kinetics problems. <p> More generally, the optimization algorithms applied to the AAO formulation can experience difficulty converging to feasibility if started far from feasibility <ref> [15] </ref>. 2.3 The Individual Feasible (IDF) ap proach The AAO approach shifts the difficulty of multidisciplinary feasibility from the level of the analysis codes up to the level of the optimization algorithm. This may be inconvenient if it requires extensive rearrangement of the analysis software.
Reference: [16] <author> Philip E. Gill, Walter Murray, and Margaret H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Finally, the structure of the IDF formulations should assist in using various components from the analysis codes to construct effective block precon-ditioners for use in iterative solvers. The major issue in large scale optimization is how best to deal with inequality constraints. Any active set method <ref> [16] </ref> is compatible with our CDT algorithm, but no one has much experience with the use of active set methods on problems as large as MDO.
Reference: [17] <author> Thomas A. Grandine. </author> <title> Private communication, </title> <month> November </month> <year> 1992. </year>
Reference-contexts: Recently it was inserted as a crucial component of a Boeing parts nesting code developed by Eldersveld and Grandine <ref> [13, 17] </ref> to plan for minimizing waste while cutting aircraft parts from sheet metal. 3.1.2 The CDT algorithm for large-scale equality constrained optimization We have implemented an algorithm that can handle very large equality constrained optimization problems and successfully used it in the parallel solution of parameter estimation for flow in
Reference: [18] <author> R. T. Haftka, J. Sobieszczanski-Sobieski, and S. L. Padula. </author> <title> On options for interdisciplinary analysis and design optimization. </title> <journal> Structural Optimization, </journal> <volume> 4(2) </volume> <pages> 65-74, </pages> <year> 1992. </year>
Reference-contexts: We say that we have decomposed the problem at a low bandwidth interdisciplinary interface. Additional IDF formulations are described in [9], including the sequenced IDF formulations, which were anticipated in the literature by equation (12) in <ref> [18] </ref>. The more detailed presentation given in [9] discusses further how we might use the IDF approach to obtain constrained MDO formulations that are much smaller than in the AAO approach.
Reference: [19] <author> Raphael T. Haftka, Zafer Gurdal, and Manohar P. Kamat. </author> <title> Elements of Structural Optimization. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: In AAO, y is an independent variable. The AAO approach has been investigated in a number of engineering fields, for instance, aerodynamics [15], structural optimization <ref> [19] </ref>, where it is called SAD or SAND, for simultaneous analysis and design, and chemical engineering [24], where it is called the open equations or nonlinear programming approach. We have applied this approach to parameter estimation for flow in porous media and for chemical kinetics problems.
Reference: [20] <author> W. P. E. Gill, Murray, M. A. Saunders, and M. H. Wright. </author> <title> User's guide for npsol (version 4.0): a fortran package for nonlinear programming. </title> <type> Technical Report SOL 86-2, </type> <institution> Department of Operations Research, Stanford University, </institution> <year> 1986. </year>
Reference-contexts: Thus, there must be a choice only of the first move limits to try for the initial step, and from that point on, the algorithm sets the move limits. There is certainly other applicable work on NLP algorithms. The Stanford Optimization Laboratory has produced the program NPSOL <ref> [20] </ref> which has 1 been used successfully in aerospace applications at Boeing. It uses a more conventional line search approach. In order to be computationally effective, optimization algorithms must exploit the pronounced block structure of MDO problems. Our large-scale optimization algorithms are designed to do this.
Reference: [21] <author> G. R. Shubin. </author> <title> Application of alternate multidisciplinary optimization formulations to a model problem for static aeroelasticity. </title> <type> Technical Report BCSTECH-93-022, </type> <institution> Boeing Computer Services, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: A detailed version can be found in [9]. In this paper we will present a sketch of that work together with a brief discussion of some optimization algorithms likely to be useful for MDO. We recommend Greg Shubin's computational study <ref> [21] </ref> of these formulations applied to a model problem in aircraft design. The IDF formulation given in section 2 has a simplicity of structure that allows easy incorporation of existing disciplinary analysis codes without the need for a multidisciplinary analysis procedure. <p> This makes the sensitivities more expensive than those of the AAO formulation. * Any special methods required to handle cross disciplinary consistency must be built into the optimizer. Shubin <ref> [21] </ref> contains a useful practical discussion of the issues involved in choosing a formulation for a particular problem. 3 Optimization A major thrust of our approach is our work on optimization tools and algorithms for MDO.
Reference: [22] <author> J. Sobieszczanski-Sobieski. </author> <title> A linear decomposition method for large optimization problems| Blueprint for development. </title> <type> Technical Report TM 83248, </type> <institution> NASA, </institution> <month> February </month> <year> 1982. </year>
Reference-contexts: Sobieszczanski-Sobieski argued for its significance at least as far back as 1982 <ref> [22] </ref>. The status of this research area is well summarized in [1].
Reference: [23] <author> Trond Steihaug. </author> <title> The conjugate gradient method and trust regions in large scale optimization. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20(3) </volume> <pages> 626-637, </pages> <year> 1983. </year>
Reference-contexts: This decision is of importance for the parallel solution of large-scale optimization problems. Solving the subtasks of the optimization iteration by use of iterative methods such as conjugate gradients is particularly efficient. This choice allows us to use the ideas of Steihaug and Toint <ref> [23, 25] </ref> for truncated conjugate direction iterations to reduce the amount of work in an optimization iteration when we are far from a solution.
Reference: [24] <author> Iauw-Bhieng Tjoa and Lorenz T. Biegler. </author> <title> Simultaneous solution and optimization strategies for parameter estimation of differential-algebraic equation systems. </title> <journal> Industrial and Engineering Chemistry Research, </journal> <volume> 30(2) </volume> <pages> 376-385, </pages> <year> 1991. </year>
Reference-contexts: In AAO, y is an independent variable. The AAO approach has been investigated in a number of engineering fields, for instance, aerodynamics [15], structural optimization [19], where it is called SAD or SAND, for simultaneous analysis and design, and chemical engineering <ref> [24] </ref>, where it is called the open equations or nonlinear programming approach. We have applied this approach to parameter estimation for flow in porous media and for chemical kinetics problems.
Reference: [25] <author> Phillipe L. Toint. </author> <title> Towards an efficient sparsity exploiting Newton method for minimization. </title> <editor> In Iain S. Duff, editor, </editor> <title> Sparse matrices and their uses. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: This decision is of importance for the parallel solution of large-scale optimization problems. Solving the subtasks of the optimization iteration by use of iterative methods such as conjugate gradients is particularly efficient. This choice allows us to use the ideas of Steihaug and Toint <ref> [23, 25] </ref> for truncated conjugate direction iterations to reduce the amount of work in an optimization iteration when we are far from a solution.
Reference: [26] <author> Virginia Torczon. </author> <title> On the convergence of the multidirectional search algorithm. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(1) </volume> <pages> 123-145, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: They are also robust in the presence of noise and well-defined even when the objective function is neither continuous nor differentiable. Moreover, when the objective function is differentiable, the direct search methods are supported by a convincing convergence theory <ref> [26, 28] </ref>, making them an attractive alternative to more ad hoc methods such as genetic algorithms or neural networks.
Reference: [27] <author> Virginia Torczon. </author> <title> PDS: Direct search methods for unconstrained optimization on either sequential or parallel machines. </title> <type> Technical Report TR92-09, </type> <institution> Rice University, Department of Computational and Applied Mathematics, </institution> <address> Houston, TX 77251-1892, </address> <year> 1992. </year>
Reference-contexts: Dennis and Torczon [11] have developed direct search methods specifically for parallel computers, and the parallelism is of an uncommon type insofar as in principle, there is no upper limit to the number of processors that can be used effectively in the algorithm. A code developed by Torczon <ref> [27] </ref> has solved a variety of problem, including a velocity estimation problem in oil exploration, a problem in tumor modeling, and a parameter estimation problem in modeling hearing loss.
Reference: [28] <author> Virginia Torczon. </author> <title> On the convergence of pattern search algorithms. </title> <type> Technical Report TR93-10, </type> <institution> Rice University, Department of Computational and Applied Mathematics, Houston, Texas, </institution> <year> 1993. </year> <month> 11 </month>
Reference-contexts: They are also robust in the presence of noise and well-defined even when the objective function is neither continuous nor differentiable. Moreover, when the objective function is differentiable, the direct search methods are supported by a convincing convergence theory <ref> [26, 28] </ref>, making them an attractive alternative to more ad hoc methods such as genetic algorithms or neural networks.
References-found: 28

