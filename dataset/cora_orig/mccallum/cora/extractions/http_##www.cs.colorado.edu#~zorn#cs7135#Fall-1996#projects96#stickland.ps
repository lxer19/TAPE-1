URL: http://www.cs.colorado.edu/~zorn/cs7135/Fall-1996/projects96/stickland.ps
Refering-URL: http://www.cs.colorado.edu/~zorn/cs7135/Fall-1996/fall1996.html
Root-URL: http://www.cs.colorado.edu
Title: Final Project HTML document compression and Active Web page sizes  
Author: Mark Strickland 
Note: Release: I give my consent to have this document reprinted, copied, or used for any purpose as long as proper acknowledgement is given to the author  
Date: Fall 1996  
Address: CSCI7135  
Affiliation: Internet Systems  
Abstract: The simplicity of the World Wide Web has caused its rapid acceptance and exponential growth. But this growth has lead to severe performance problems because of its abuse of the TCP protocol. Many improvements have been proposed to the http protocol, but html document compression is never mentioned even though the http 1.0 specification has provisions for its support. This paper first determines what is the current, typical, active Web page size. From this information, we determine how html compression could improve response time and cut unnecessary network traffic for average, size html documents. 
Abstract-found: 1
Intro-found: 1
Reference: [BERN92] <author> Tim Berners-Lee, </author> <title> "World-Wide Web, </title> <journal> the Information Universe"; Electronic Networking: Research, Applications, and Policy; Volume 1, </journal> <volume> No. 2, </volume> <pages> pp. 52-58, </pages> <publisher> Meckler Publications, </publisher> <address> Wesport, Connecticut, </address> <month> Spring </month> <year> 1992 </year>
Reference: [BERN94a] <author> Tim Berners-Lee, </author> <title> "The WorldWide Web"; CACM, </title> <address> v37n9, Aug. 94; p.76-82 </address>
Reference-contexts: Of that 10 Terabytes of data, about 750 Gigabytes consisted of http traffic according to figure 3 given in <ref> [BERN94a] </ref>. This computes to 7.5% of the data traffic (this seems low). In [CERF95], he states that traffic rates measured on the retired NSFNET backbone approached 20 Terabytes per month. Unfortunately, since the NSF turned over the control of the Internet to commercial providers, Internet statistics are difficult to obtain.
Reference: [BERN95] <author> Tim Berners-Lee, Daniel W. </author> <title> Connolly; "HyperText Markup Language Specification - 2.0"; IETF RFC 1866; MIT/W3C; November, </title> <booktitle> 1995; 77 pages </booktitle>
Reference: [BERN96] <author> Tim Berners-Lee, R. Fielding, H. </author> <title> Nielsen; "Hypertext Transfer Protocol - HTTP/1.0", IETF RFC 1945; May 1996; MIT/LCS, </title> <institution> University of California - Irvine; 60 pages </institution>
Reference-contexts: Images are much larger than the associated html documents but why not compress the html documents also and save the bandwidth that is possible? 2.4 The http protocol The http 1.0 protocol specification <ref> [BERN96] </ref> already contains support of a header field definition for accept-encoding and content-encoding. The specification for accept-encoding, allows a client (browser) to specify in a get request that the client is able and willing to accept data in compressed format.
Reference: [BRAU94] <author> H. W. Braun, K. Claffy, </author> <title> "Web Traffic Characterization: An Assessment of the Impact of Caching Documents from NCSA's Web Server, </title> <booktitle> Second International Conf. on the World Wide Web, </booktitle> <address> Chicago, IL, </address> <month> October 20-23, </month> <year> 1994, </year> <pages> pages 1007-1027 </pages>
Reference-contexts: These results were reported in 1994, which in Web years, is a long time ago. Two other papers, <ref> [BRAU94] </ref> and [SELT96], measured documents retrieved from a server for the purposes of developing a caching mechanism. In [BRAU94], the authors discovered that the mean document size was 17 KB and the median was 3 KB. <p> These results were reported in 1994, which in Web years, is a long time ago. Two other papers, <ref> [BRAU94] </ref> and [SELT96], measured documents retrieved from a server for the purposes of developing a caching mechanism. In [BRAU94], the authors discovered that the mean document size was 17 KB and the median was 3 KB.
Reference: [BRAY96] <author> Tim Bray, </author> <title> Measuring the Web, </title> <booktitle> Fifth International World Wide Web Conference, </booktitle> <address> May 6-10, 1996, Paris, France </address>
Reference-contexts: The first 3 study is from 1994 and represents stale information. In addition, both studies only reflect data for documents from a single Web server / cache. Finally [WOOD96] and <ref> [BRAY96] </ref> took measurements of the Web utilizing a Web crawler. In [WOOD96], the authors collected via the Inktomi Web crawler, statistical data for over 2.6 million HTML documents on the World Wide Web. One of the statistics was the document size. <p> Woodruff et. all report that, after all markup had been extracted, the size of each HTML document was measured. For the entire data set, the mean size was 4.4 KB, the median size was 2.0 KB, and the maximum size was 1.6 MB. <ref> [BRAY96] </ref> extracted data in November 1995, when the Open Text Index covered the content of about 1.5 million textual objects retrieved from the WWW. Bray discovered that the size of the average page has consistently been between 6 KB and 7 KB during the entire lifetime of the Index. <p> Any graphic for which I could not determine its size was not counted. My desire was to error on the low side since <ref> [BRAY96] </ref> found just under 50% of the Web documents contained no graphics. I wanted to discover if this condition was also true for active Web documents as well.
Reference: [CACE91] <author> R. Caceres, P. B. Danzig, S. Jamin, D. J. Mitzel, </author> <title> "Characteristics of Wide-Area TCP/IP Conversations", </title> <booktitle> ACM SIGCOMM, </booktitle> <address> Zurich, Switzerland, </address> <pages> pp. 101-112, </pages> <year> 1991 </year>
Reference: [CATL95] <author> L. Catledge and J. Pitkow, </author> <title> "Characterizing Browsing Strategies in the WorldWide Web", </title> <journal> Journal of Computer Networks and ISDN systems, </journal> <volume> Vol. 27, No. 6, </volume> <year> 1995 </year>
Reference: [CERF] <author> V. </author> <title> Cerf; A Brief History of the Internet and Related Network; http://www.skywriting.com/cerf.html </title>
Reference: [CERF95] <author> V. </author> <title> Cerf; "Computer Networking: Global Infrastructure Driver for the 21st Century", </title> <address> OntheInternet; V1n5; Nov-Dec 95; p 18-24; http://www.cs.washington.edu/homes/lazowska/cra/networks.html </address>
Reference-contexts: Of that 10 Terabytes of data, about 750 Gigabytes consisted of http traffic according to figure 3 given in [BERN94a]. This computes to 7.5% of the data traffic (this seems low). In <ref> [CERF95] </ref>, he states that traffic rates measured on the retired NSFNET backbone approached 20 Terabytes per month. Unfortunately, since the NSF turned over the control of the Internet to commercial providers, Internet statistics are difficult to obtain.
Reference: [CLAF94] <author> K. Claffy, G. C. Polyzos, H. W. </author> <title> Braun; Tracking Long-term Growth of the NSFNET; Communications of the ACM; 1994; http://www.nlanr.net./Papers/tlg.html </title>
Reference: [COMP] <author> Newsgroup comp.compression, </author> <note> faq parts 1, 2, 3; http://www.cs.ruu.nl/wais/html/nadir/compression-faq/.html </note>
Reference-contexts: The use of slow speed network lines by many Internet users suggests that a reduction in the amount of data received from html data compression will improve their response time. The faq (frequently asked questions) for the newsgroup comp.compression <ref> [COMP] </ref> discusses many different types of data compression programs. The group also has 14 files from the standard Calgary Text Compression Corpus, which are used for benchmarking compression programs.
Reference: [DANZ93] <author> P. B. Danzig, R. S. Hall, and M. F. Schwartz, </author> <title> "A case for caching file objects inside internetworks", </title> <booktitle> Proceedings of ACM SIGCOMM 1993, </booktitle> <month> September </month> <year> 1993, </year> <pages> pp. 29-248 </pages>
Reference-contexts: Other proposals suggest new, pipelining http methods be implemented such as mget [VILE95], getall, or getlist [PADM95]. Caching has been called a necessary improvement, since the performance of all distributed systems depends on caching <ref> [DANZ93] </ref>. All of these are currently under consideration and addressed in the http 1.1 specification. While all of these proposals will help to solve the http protocol performance deficiency, none suggest html data compression as an additional enhancement. <p> Finally, Ill will review and estimate the current http traffic on the Internet and how a little savings will have a significant impact on reducing backbone traffic. 2.1 Previous compression study Danzig, Hall, and Schwartz <ref> [DANZ93] </ref> performed a study on internet ftp traffic and concluded that besides caching, that on-the-fly compression would also reduce the amount of NSFNET backbone traffic. At the time of the study, ftp traffic amounted for 50% of the backbones traffic.
Reference: [FLUC96] <author> F. Fluckiger, </author> <title> "From WorldWide Web to Information Superhighway", Computer Networks and ISDN Systems, </title> <booktitle> Selected Papers from the 6th Joint European Networking Conference, </booktitle> <volume> Volume 28, Numer 4, </volume> <month> February </month> <year> 1996 </year>
Reference: [GLAS94] <author> S. Glassman, </author> <title> "A caching relay for the World Wide Web", </title> <booktitle> Proc. 1st Int. WorldWide Web Conf., </booktitle> <address> Geneva, Switzerland, </address> <month> May </month> <year> 1994, </year> <pages> pp. 69-76 18 </pages>
Reference: [GOEL95] <author> K. E. </author> <title> Goeller; Tim Berners-Lee Speaks on the History and Future of the Web:; </title> <booktitle> 4th International WorldWide Web Conference; Dec. </booktitle> <pages> 11-14, </pages> <address> 1995; Boston, MA; http://info.bellcore.com/WWWCONF/ARTICLES/11/bernlee.html </address>
Reference: [GRAY95] <author> M. T. Gray, </author> <title> "Measuring the Growth of the Web, </title> <note> June 1993 to June 1995; MIT; 1995;http://www.mit.edu/people/mkgray/growth/ </note>
Reference: [LOTT92] <author> M. </author> <title> Lottor; "Internet Growth (1981-1991). Req. For Com. </title> <institution> 1296; Network Information Systems Center; SRI Int.; </institution> <month> Jan. </month> <year> 1992 </year>
Reference: [MOGU95] <author> Jeffrey C. Mogul, </author> <title> "The case for persistent-connection HTTP", Proc. </title> <booktitle> SIGCOMM 1995, </booktitle> <address> Cambridge, Mass., </address> <month> August </month> <year> 1995, </year> <pages> pp. 299-313 </pages>
Reference-contexts: Of course, this simplicity has also caused the World Wide Web to be rapidly accepted and caused its exponential growth. To correct the performance difficulties of the http protocol, many proposals have been made. One proposal suggests a persistent tcp connection [PADM95], <ref> [MOGU95] </ref> which remains after the server has sent all of the requested data to the web client. Other proposals suggest new, pipelining http methods be implemented such as mget [VILE95], getall, or getlist [PADM95].
Reference: [NUCK92] <author> Neal Nuckolls, </author> <title> How to Use DLPI, </title> <institution> Sun Corp., </institution> <month> June 30, </month> <year> 1992, </year> <month> ftp://opcom.sun.ca/pub/drivers/dlpi/how_to_use_DLPI.ps </month>
Reference-contexts: The other connections, from their naming, appear to be T1 or greater speeds (note that were using the Universitys new OC3 ATM connection). For the intranet measurement tool, I used a previously developed sniffer program that runs on Solaris UNIX using the dlpi protocol <ref> [NUCK92] </ref>. The program captures http packets for both inbound and outbound data traffic, and displays key fields from the TCP and IP header in addition to a user specified amount of the packet data.
Reference: [PADM94] <author> V. N. Padmanabhan, J. C. Mogul, </author> <title> "Improving HTTP Latency", </title> <booktitle> Second International Conf. on the World Wide Web, </booktitle> <address> Chicago, IL., </address> <month> October 20-23, </month> <year> 1994 </year>
Reference-contexts: 1. Introduction One of the biggest problems with the World Wide Web is its performance. This has been well documented in [SPER94], <ref> [PADM94] </ref>, [TOUC96] and [VILE95]. Much of this performance problem is due to the simplicity of the http protocol and its use (or abuse) of the TCP protocol. Of course, this simplicity has also caused the World Wide Web to be rapidly accepted and caused its exponential growth. <p> (bytes) File type HTML 6 K ASCII text Web page 6+2+2 K HTML and links to two icons text 60 K ASCII text icon 2 K small GIF (icons) image 20 K large GIF (clickable map) photo 200 K very large GIF (photo) Table 1: ** reproduced from [TOUC96] In <ref> [PADM94] </ref>, the authors note that a sample of 200,000 HTTP retrieved documents (unscientifically chosen) had a mean size of about 13 KB bytes and a median of under 2 KB bytes. These results were reported in 1994, which in Web years, is a long time ago.
Reference: [PITK95] <author> J. Pitkow and C. M. </author> <title> Kehoe; "Results from the Third WWW User Survey"; The World Wide Web Journal; Vol. </title> <type> 1, </type> <note> No. 1; 1995; http://www.cc.gatech.edu/gvu/user_surveys/papers/survey_3_paper.html </note>
Reference: [POST81a] <author> J. B. Postel, </author> <title> "Internet Protocol", </title> <type> IETF RFC 791, </type> <year> 1981, </year> <pages> 45 pages </pages>
Reference: [POST81b] <author> J. B. Postel, </author> <title> "Transmission Control Protocol", </title> <type> RFC 793, </type> <institution> Network Information Center, SRI International, </institution> <month> September </month> <year> 1981, </year> <pages> 85 pages </pages>
Reference: [RELI94] <author> L. Relihan, T. Cahill, and M. G. </author> <note> Hinchey, "Untangling the WorldWide Web"; 1994; http://itdsrv1.ul.ie/Research/WWW/utwww.ps </note>
Reference: [RUTK94] <institution> Growth of Internet Browsing Services; A. M. Rutkowski and Internet Society; Copyright 1994; http://ftp.freebsd.org/pub/internet/inet/internet-society/charts/traffic-gifs/www-pcnt.gif </institution>
Reference-contexts: At the time of the study, ftp traffic amounted for 50% of the backbones traffic. By May 1994, the NSFNET backbone carried more Web network traffic than any other kind of data. <ref> [RUTK94] </ref>.
Reference: [SELT96] <author> M. Seltzer & J. Gwertzman, </author> <title> WorldWide Web Cache Consistency; Proceedings for the 1996 USENIX Technical Conference; San Diego, </title> <address> CA; January, </address> <year> 1996 </year>
Reference-contexts: These results were reported in 1994, which in Web years, is a long time ago. Two other papers, [BRAU94] and <ref> [SELT96] </ref>, measured documents retrieved from a server for the purposes of developing a caching mechanism. In [BRAU94], the authors discovered that the mean document size was 17 KB and the median was 3 KB. In [SELT96], a Microsoft proxy cache, which receives approximately 150,000 web object requests on an average week <p> Two other papers, [BRAU94] and <ref> [SELT96] </ref>, measured documents retrieved from a server for the purposes of developing a caching mechanism. In [BRAU94], the authors discovered that the mean document size was 17 KB and the median was 3 KB. In [SELT96], a Microsoft proxy cache, which receives approximately 150,000 web object requests on an average week day, discovered that 22% of these accesses were for html type objects and these html objects had an average file size of 4,786 bytes.
Reference: [SPER94] <author> S. Spero, </author> <title> "Analysis of HTTP Performance Problems"; 1994; http://www.eit.com/mailinglists/www.lists/www-talk.1994q3/0159.html </title>
Reference-contexts: 1. Introduction One of the biggest problems with the World Wide Web is its performance. This has been well documented in <ref> [SPER94] </ref>, [PADM94], [TOUC96] and [VILE95]. Much of this performance problem is due to the simplicity of the http protocol and its use (or abuse) of the TCP protocol. Of course, this simplicity has also caused the World Wide Web to be rapidly accepted and caused its exponential growth.
Reference: [STEV94] <author> W. Richard Stevens, </author> <title> "TCP/IP Illustrated, Volume 1: The Protocols", </title> <publisher> Addison-Wesley, </publisher> <year> 1994 </year>
Reference: [STEV90] <author> W. Richard Stevens, </author> <title> "UNIX Network Programming", </title> <publisher> Prentice-Hall, </publisher> <year> 1990 </year>
Reference-contexts: In contrast to the Web crawler, this is a strategic difference since the crawler counts each Web page only once and each equally. The resultant output file was then used to drive a program to retrieve each of these Web documents. A Web client program was coded <ref> [STEV90] </ref> to read the above input file, retrieve a Web page, and place the html source in a file. From this file, the html size was determined. I determined the html size by seeing if the retrieved information included a http response header.
Reference: [TOUC96] <author> J. Touch, J. Heidemann, & K. Obraczka, </author> <note> Analysis of HTTP Performance; USC/Information Sciences Institute; August 16, 1996; http://www.isi.edu/lsam/ib/http-perf </note>
Reference-contexts: 1. Introduction One of the biggest problems with the World Wide Web is its performance. This has been well documented in [SPER94], [PADM94], <ref> [TOUC96] </ref> and [VILE95]. Much of this performance problem is due to the simplicity of the http protocol and its use (or abuse) of the TCP protocol. Of course, this simplicity has also caused the World Wide Web to be rapidly accepted and caused its exponential growth. <p> File size (bytes) File type HTML 6 K ASCII text Web page 6+2+2 K HTML and links to two icons text 60 K ASCII text icon 2 K small GIF (icons) image 20 K large GIF (clickable map) photo 200 K very large GIF (photo) Table 1: ** reproduced from <ref> [TOUC96] </ref> In [PADM94], the authors note that a sample of 200,000 HTTP retrieved documents (unscientifically chosen) had a mean size of about 13 KB bytes and a median of under 2 KB bytes. These results were reported in 1994, which in Web years, is a long time ago. <p> I also needed the amount and sizes of the html documents being retrieved on the Web. Also required for this study was the amount and sizes of images imbedded in those documents. 2.3 Data compression In <ref> [TOUC96] </ref>, these same authors also state that, the TCP MSS is 536 bytes for data, although most current implementations round this down to 512 bytes. Touch et. all continue by stating that many users are connected to the Internet via low-bandwidth lines. <p> Document count: 24,900 Median page size: 5,099 Mean page size: 8,577 Table 2: Page Measurement Results Next I created and ran a C program which tabulated all of the image file information. Since <ref> [TOUC96] </ref> reported that icons were about 2 KB in size and images about 20 KB in size. I decided to classify the graphics based upon size.
Reference: [VILE95] <author> C. L. Viles and J. C. </author> <title> French, "Availability and Latency of World Wide Web Information Servers", </title> <booktitle> USENIX Computing Systems, </booktitle> <volume> Volume 8, No. 1, </volume> <month> Winter </month> <year> 1995, </year> <pages> pages 61-90 </pages>
Reference-contexts: 1. Introduction One of the biggest problems with the World Wide Web is its performance. This has been well documented in [SPER94], [PADM94], [TOUC96] and <ref> [VILE95] </ref>. Much of this performance problem is due to the simplicity of the http protocol and its use (or abuse) of the TCP protocol. Of course, this simplicity has also caused the World Wide Web to be rapidly accepted and caused its exponential growth. <p> One proposal suggests a persistent tcp connection [PADM95], [MOGU95] which remains after the server has sent all of the requested data to the web client. Other proposals suggest new, pipelining http methods be implemented such as mget <ref> [VILE95] </ref>, getall, or getlist [PADM95]. Caching has been called a necessary improvement, since the performance of all distributed systems depends on caching [DANZ93]. All of these are currently under consideration and addressed in the http 1.1 specification.
Reference: [WOOD96] <author> A. Woodruff, P. Aoki, E. Brewer, P. Gauthier, & L. Rowe, </author> <title> "An Investigation of Documents from the World Wide Web", </title> <institution> Computer Science Division, University of California at Berkeley 19 </institution>
Reference-contexts: The first 3 study is from 1994 and represents stale information. In addition, both studies only reflect data for documents from a single Web server / cache. Finally <ref> [WOOD96] </ref> and [BRAY96] took measurements of the Web utilizing a Web crawler. In [WOOD96], the authors collected via the Inktomi Web crawler, statistical data for over 2.6 million HTML documents on the World Wide Web. One of the statistics was the document size. <p> The first 3 study is from 1994 and represents stale information. In addition, both studies only reflect data for documents from a single Web server / cache. Finally <ref> [WOOD96] </ref> and [BRAY96] took measurements of the Web utilizing a Web crawler. In [WOOD96], the authors collected via the Inktomi Web crawler, statistical data for over 2.6 million HTML documents on the World Wide Web. One of the statistics was the document size. Woodruff et. all report that, after all markup had been extracted, the size of each HTML document was measured. <p> Many novice users, enthralled with the mystic of the Web, have created their own home page. These pages represent much of the Web that exists, is small in size, but probably never referenced. I also note that the 1.6 maximum size from <ref> [WOOD96] </ref> seems unusually low and suspect. To perform this study, accurate, up-to-date, and additional information was required that current references do not provide. Therefore, a Web page measurement was done to obtain this information. The mean and median html document sizes for typical, active pages on the Web was required.
References-found: 33

