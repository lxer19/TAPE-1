URL: http://www.cs.toronto.edu/~ftp/pub/reports/na/hayes-95-msc/thesis.ps.Z
Refering-URL: http://www.cs.toronto.edu/NA/reports.html
Root-URL: http://www.cs.toronto.edu
Title: Efficient Shadowing of High Dimensional Chaotic Systems with the Large Astrophysical N -body Problem as
Author: by Wayne Hayes 
Degree: A thesis submitted in conformity with the requirements for the degree of Master of Science  
Note: c Copyright by Wayne Hayes 1995  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sverre J. Aarseth. </author> <title> Direct Methods for N -Body Simulations. </title> <booktitle> In Multiple Time Scales, </booktitle> <pages> pages 377-418. </pages> <publisher> Academic Press, Inc., </publisher> <year> 1985. </year>
Reference-contexts: We are interested 6 in the case where a and b are finite integers. For a chaotic map, f may be a simple equation, such as the logistic equation f (x) = 1 2x 2 , which always maps the interval <ref> [1; 1] </ref> onto itself. For an ODE system like the N -body problem, f (x) represents the true solution of integrating the phase-space co-ordinates x for one timestep. <p> For example, Aarseth's <ref> [1] </ref> N -body integrator is a popular one for collisional systems. It includes regularization, in which close encounters between 2 particles are solved using the analytical 2-body solution with perturbations from the other particles. <p> Eventually, shadowing should be attempted while using the same noisy integrator that astronomers commonly use | often leapfrog for collisionless systems, and Aarseth's <ref> [1] </ref> for collisional systems, and usually with individual timesteps for each particle.
Reference: [2] <author> D. V. Anosov. </author> <booktitle> Proc. </booktitle> <institution> Steklov Inst. Math, 90:1, </institution> <year> 1967. </year>
Reference-contexts: The first group of chaotic systems for which it was proven that shadow orbits exist was hyperbolic systems <ref> [2, 7] </ref>. In a 2-dimensional hyperbolic system, there are two special directions called the unstable (or expanding) and the stable (or contracting) directions, which are generally not orthogonal.
Reference: [3] <author> Josh Barnes and Piet Hut. </author> <title> A hierarchical O(N log N ) force-calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <month> 4 December </month> <year> 1986. </year>
Reference-contexts: For example, Hernquist, Hut and Makino [15], Barnes and Hut [5], and Sellwood [31] explicitly say this; Singh, Hennessy and Gupta [32] have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl [27], Jernigan and Porter [17], and Barnes and Hut <ref> [3] </ref> all imply that using the largest possible N is a desirable characteristic. * force softening, i.e., replacing r 2 with (r 2 + " 2 soft ) in the denominator of the gravitational force computation for some small constant " soft , usually chosen to approximate the average inter-particle separation. <p> This is not done as often as in the past. Implementation approximations measure how well the implementation simulates the model, and include such things as * numerical integration truncation error. * machine roundoff error. * Using approximate force computation algorithms like the Barnes-Hut tree code <ref> [3] </ref> or the Fast Multipole Method [12]. <p> Perhaps discrete positions could be used for particles with large timesteps, rather than their interpolated positions, and still produce shadowable solutions. Another example of a system with perturbations is the Barnes and Hut <ref> [3] </ref> O (N log N ) force computation algorithm. This algorithm produces a force function that is discontinuous in both space and time, thus introducing artificial, relatively large perturbations; often the force function for each particle is computed only to about one part in 10 3 or 10 4 [5].
Reference: [4] <author> Joshua Barnes, Jeremy Goodman, and Piet Hut. </author> <title> Dynamical instabilities in spherical stellar systems. </title> <journal> The Astrophysical Journal, </journal> <volume> 300 </volume> <pages> 112-131, </pages> <year> 1986. </year>
Reference-contexts: The proliferation of titles such as "Simulations of Sinking Galaxies" [33], "Dissipationless Collapse of Galaxies and Initial Conditions" [23], "A Numerical Study of the Stability of Spherical Galaxies" [24], "The Global Stability of Our Galaxy" [30], and "Dynamical Instabilities in Spherical Stellar Systems" <ref> [4] </ref> shows that much trust is placed in the results of these simulations.
Reference: [5] <author> Joshua E. Barnes and Piet Hut. </author> <title> Error analysis of a tree code. </title> <journal> Astrophysical Journal Supplement Series, </journal> <volume> 70 </volume> <pages> 389-417, </pages> <year> 1989. </year>
Reference-contexts: The general consensus seems to be that this is the limiting source of error in current large N -body simulations. For example, Hernquist, Hut and Makino [15], Barnes and Hut <ref> [5] </ref>, and Sellwood [31] explicitly say this; Singh, Hennessy and Gupta [32] have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl [27], Jernigan and Porter [17], and Barnes and Hut [3] all imply that using the largest possible N is a desirable characteristic. * <p> Barnes and Hut <ref> [5] </ref> claim that astrophysical N -body simulations require only "modest" accuracy levels, but also concede that quoting energy conservation isn't enough, and that more stringent tests are needed. An example of conservation of macroscopic properties is given by Kandrup and Smith [19]. <p> This algorithm produces a force function that is discontinuous in both space and time, thus introducing artificial, relatively large perturbations; often the force function for each particle is computed only to about one part in 10 3 or 10 4 <ref> [5] </ref>. Furthermore these kicks have been shown not to be random, but instead have a high correlation [5]. To test whether such a system is shadowable, we would need many moving particles (preferably thousands), and then we should try shadowing it using the O (N 2 ) force computation algorithm. <p> is discontinuous in both space and time, thus introducing artificial, relatively large perturbations; often the force function for each particle is computed only to about one part in 10 3 or 10 4 <ref> [5] </ref>. Furthermore these kicks have been shown not to be random, but instead have a high correlation [5]. To test whether such a system is shadowable, we would need many moving particles (preferably thousands), and then we should try shadowing it using the O (N 2 ) force computation algorithm.
Reference: [6] <author> James Binney and Scott Tremaine. </author> <title> Galactic Dynamics. Princeton Series in Astrophysics. </title> <publisher> Princeton University Press, </publisher> <year> 1987. </year>
Reference-contexts: This is done because it allows a smaller N to approximate a larger N , and also to eliminate the singularity at r = 0 <ref> [6] </ref>. * reducing the dimensionality of the problem from 3 to 2, if applicable. This is not done as often as in the past. <p> For their "cheap" integrator, they used a 5th-order predictor-corrector routine that is commonly used by astronomers [Appendix B of <ref> [6] </ref>], arranged to have 1-step errors of about 10 5 . For their expensive integrator, they used a Bulirsch-Stoer method [28] with tolerance 10 13 . They found they could shadow the particle for several crossing times.
Reference: [7] <author> R. Bowen. J. </author> <title> Differential Equations, </title> <address> 18:333, </address> <year> 1975. </year>
Reference-contexts: The first group of chaotic systems for which it was proven that shadow orbits exist was hyperbolic systems <ref> [2, 7] </ref>. In a 2-dimensional hyperbolic system, there are two special directions called the unstable (or expanding) and the stable (or contracting) directions, which are generally not orthogonal.
Reference: [8] <author> Silvina Dawson, Celso Grebogi, Tim Sauer, and James A. Yorke. </author> <title> Obstructions to Shadowing When a Lyapunov Exponent Fluctuates about Zero. </title> <journal> Physical Review Letters, </journal> <volume> 73(14) </volume> <pages> 1927-1930, </pages> <month> 3 Oct </month> <year> 1994. </year>
Reference-contexts: This trouble spot will become even more of a problem as we attempt to build a longer shadow, because the volume of phase space around the trouble spot that can contain shadows shrinks as the attempted shadowing distance increases (see the discussion on "brittleness" of orbits in <ref> [8] </ref>). <p> This would smoothen the gravitational potential, which we know from other studies [20, 10] decreases the Lyapunov exponent, and thus may lengthen the average shadow length. Despite the above caveats, there is reason to believe that high-dimensional shadowing may be difficult. Dawsen et al. <ref> [8] </ref> show that shadowing becomes extremely difficult in systems where a Lyapunov exponent fluctuates about zero. They claim that such fluctuating Lyapunov exponents occur frequently in high-dimensional systems.
Reference: [9] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sun-deram. </author> <title> PVM: Parallel Virtual Machine:A Users' Guide and Tutorial for Network Parallel Computing. </title> <note> Available via anonymous ftp from netlib2.cs.utk.edu:/pvm3/book/pvm-book.ps. </note>
Reference-contexts: Thus, each could be computed on a separate processer, for example using the PVM (Parallel Virtual Machinne) software package <ref> [9] </ref>. * When using constant RUS, and iterations are failing, is there some way to decide if the problem is being caused by one (or a small number) of RUS i 's? If so, then only these RUS i 's need be recomputed, rather than the RUS for the entire trajectory.
Reference: [10] <author> Jeremy Goodman, Douglas C. Heggie, and Piet Hut. </author> <title> On the exponential instability of N -body systems. </title> <journal> The Astrophysical Journal, </journal> <volume> 415 </volume> <pages> 715-733, </pages> <year> 1993. </year>
Reference-contexts: Goodman, Heggie, and Hut <ref> [10] </ref> developed a detailed theory of the growth of small perturbations, and verified it with simulation to show that the exponential growth of small errors results mostly from close encounters, which occur infrequently. <p> Amazing as it may seem, there is currently no clear definition of simulation accuracy [29]. Obviously, attempting to follow the individual paths of all N particles is infeasible; Goodman, Heggie and Hut <ref> [10] </ref> show that this would require O (N ) digits of precision. On the other hand, most astronomical publications quote energy conservation as their only measure of output error, even though there are infinitely many solutions with equal energy but vastly different phase-space trajectories. <p> instead with the evolution of the distribution of particles. 2 Most practitioners know that the exponential magnification of errors means they cannot possibly trust the microscopic details, but they believe that the statistical results are independent of the microscopic errors, although little work has been done to test this belief <ref> [10] </ref>. Barnes and Hut [5] claim that astrophysical N -body simulations require only "modest" accuracy levels, but also concede that quoting energy conservation isn't enough, and that more stringent tests are needed. An example of conservation of macroscopic properties is given by Kandrup and Smith [19]. <p> Unfortunately, ODE integrations have truncation errors, so the magnitude of these errors will be magnified exponentially on a short time scale. Goodman, Heggie and Hut <ref> [10] </ref> offer some solace in that the exact evolution could be closely followed for a long time if O (N ) digits are kept, but this is currently infeasible. There seems little hope of obtaining valid simulations by this criterion. 2. <p> Perhaps a more realistic scaling of the problem, at least from the astronomer's point of view, would be to have M particles move amongst 100M fixed ones. This would smoothen the gravitational potential, which we know from other studies <ref> [20, 10] </ref> decreases the Lyapunov exponent, and thus may lengthen the average shadow length. Despite the above caveats, there is reason to believe that high-dimensional shadowing may be difficult. Dawsen et al. [8] show that shadowing becomes extremely difficult in systems where a Lyapunov exponent fluctuates about zero. <p> There seems to be an ongoing debate between Kandrup and Smith [19, 21, 20], who argue that the growth of errors is due both to the global potential and to collisional effects, while Goodman, Heggie, and Hut <ref> [10] </ref> argue that collisional encounters are the only process for the magnification of errors. 43 attempts to shadow noisy orbits in the unsoftened Plummer model of Fig. 1 [their figure 1, not included in this thesis] (each point represents a different orbit): (a) maximum time T for which the orbit could
Reference: [11] <author> Celso Grebogi, Stephen M. Hammel, James A. Yorke, and Tim Sauer. </author> <title> Shadowing of Physical Trajectories in Chaotic Dynamics: Containment and Refinement. </title> <journal> Physical Review Letters, </journal> <volume> 65(13) </volume> <pages> 1527-1530, </pages> <month> 24 September </month> <year> 1990. </year>
Reference-contexts: Scalars are written in small letters and matrices in CAPITALS. Some of the following definitions are taken, with minor modifications, from Grebogi, Hammel, Yorke, and Sauer <ref> [11] </ref>, hereinafter referred to as GHYS. The terms trajectory, orbit, and solution are used interchangably throughout this thesis. Definition. A true trajectory fx i g b i=a of f satisfies x i+1 = f (x i ) for a i &lt; b. <p> With various numerical tricks and physical insights, our algorithm runs, depending on the problem, between 5 and 100 times faster than the original algorithm. 2.0 Introduction 2.0.0 Background As described in the previous chapter, Grebogi, Hammel, Yorke, and Sauer (GHYS) <ref> [11] </ref>, invented a two-dimensional shadowing procedure consisting of two parts: 0. containment, which, if successful, rigourously proves the existence of a true shadow of a noisy trajectory. It is not, however, guaranteed to find a shadow if one exists.
Reference: [12] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> A fast algorithm for particle simulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 73:325, </volume> <year> 1987. </year>
Reference-contexts: Implementation approximations measure how well the implementation simulates the model, and include such things as * numerical integration truncation error. * machine roundoff error. * Using approximate force computation algorithms like the Barnes-Hut tree code [3] or the Fast Multipole Method <ref> [12] </ref>. Hernquist, Hut, and Makino [15] try to show that the effect of this error is negligible, by showing that the energy of each individual particle is conserved to a high degree regardless of whether the Barnes-Hut or the direct O (N 2 ) algorithm is used.
Reference: [13] <author> Ernst Hairer, Syvert Paul Norsett, and Gerhard Wanner. </author> <title> Solving Ordinary Differential Equations I - Nonstiff Problems. </title> <publisher> Springer-Verlag, </publisher> <year> 1980. </year> <month> 59 </month>
Reference-contexts: perturbation ffip at time t 0 gets mapped to a perturbation at time t 2 by the matrix-matrix and matrix-vector multiplication R 2 ffip = R 1 R 0 ffip.) Finally, the linear map in the GHYS refinement procedure is L i = R (t i+1 ; t i ) <ref> [13] </ref>. 9 As will be seen later, it is the computation of the linear maps L i , called resolvents, that takes most of the CPU time during a refinement, because the resolvent has O (N 2 ) terms in it, and it needs to be computed to high accuracy.
Reference: [14] <author> D. C. Heggie and R. D. Mathieu. </author> <title> Standardized units and time scales. </title> <editor> In Piet Hut and S. L. W. McMillan, editors, </editor> <booktitle> The Use of Supercomputers in Stellar Dynamics, </booktitle> <pages> pages 233-235. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Thus, for each shadow step, the Adams's method must restart with extremely small stepsizes. Estimating the largest shadow steps that can be used For the N -body simulations reported in this thesis, a set of "standardized" units was used <ref> [14] </ref> such that the scale of the system in all units of interest was of order 1 | i.e., the system had a diameter of order 1, the crossing time was of order 1, and the N particles each had mass 1=N . <p> Forces were not softened. The 32 orbits were chosen by generating random 3-dimensional positions for all particles from the uniform distribution on (0; 1); a 3-dimensional random initial velocity for the moving particle was also chosen uniformly on (0; 1). 9 The standardized units of Heggie and Mathieu <ref> [14] </ref> were used, in which each particle has mass 1=N . The pseudo-random number generator was the popular 48-bit drand48 (), with seeds 1 through 32.
Reference: [15] <author> Lars Hernquist, Piet Hut, and Jun Makino. </author> <title> Discreteness noise versus force errors in N -body simulations. </title> <journal> Astrophysical Journal Letters, </journal> <volume> 402:L85-L88, </volume> <year> 1993. </year>
Reference-contexts: The general consensus seems to be that this is the limiting source of error in current large N -body simulations. For example, Hernquist, Hut and Makino <ref> [15] </ref>, Barnes and Hut [5], and Sellwood [31] explicitly say this; Singh, Hennessy and Gupta [32] have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl [27], Jernigan and Porter [17], and Barnes and Hut [3] all imply that using the largest possible N is <p> Implementation approximations measure how well the implementation simulates the model, and include such things as * numerical integration truncation error. * machine roundoff error. * Using approximate force computation algorithms like the Barnes-Hut tree code [3] or the Fast Multipole Method [12]. Hernquist, Hut, and Makino <ref> [15] </ref> try to show that the effect of this error is negligible, by showing that the energy of each individual particle is conserved to a high degree regardless of whether the Barnes-Hut or the direct O (N 2 ) algorithm is used. <p> There has already been work to show that energy distributions are preserved between the Barnes-Hut and O (N 2 ) algorithms <ref> [15] </ref>, but no shadowing was attempted. More generally, we would like to study the shadowability of solution methods that introduce arbitrary "kicks" of various magnitudes. Roundoff and truncation error are the only kinds of kicks that have been studied in shadowing of N -body systems thus far.
Reference: [16] <author> Alan C. Hindmarsh. LSODE and LSODI, </author> <title> two new initial value ordinary differential equation solvers. </title> <journal> ACM-SIGNUM Newsletter, </journal> <volume> 15(4) </volume> <pages> 10-11, </pages> <year> 1980. </year>
Reference-contexts: Dependence of shadowing on the choice of accurate integrator I have tried two integrators as my "accurate" integrator, although both were variable-order, variable-timestep Adams's methods called SDRIV2 and LSODE <ref> [18, 16] </ref>. QT used a Bulirsch-Stoer integra 13 tor by Press et al. [28]. <p> Once the initial conditions were set, each noisy orbit was generated by integrating for 1.28 standard time units (about 1 crossing time) using LSODE <ref> [16] </ref> with pure relative error control of 10 6 . This figure agreed well with the magnitude of the initial 1-step errors computed during refinement.
Reference: [17] <author> J. Garrett Jernigan and David H. Porter. </author> <title> A tree code with logarithmic reduction of force terms, hierarchical regularization of all variables, and explicit accuracy controls. </title> <journal> Astrophysical Journal Supplement Series, </journal> <volume> 71 </volume> <pages> 871-893, </pages> <year> 1989. </year>
Reference-contexts: For example, Hernquist, Hut and Makino [15], Barnes and Hut [5], and Sellwood [31] explicitly say this; Singh, Hennessy and Gupta [32] have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl [27], Jernigan and Porter <ref> [17] </ref>, and Barnes and Hut [3] all imply that using the largest possible N is a desirable characteristic. * force softening, i.e., replacing r 2 with (r 2 + " 2 soft ) in the denominator of the gravitational force computation for some small constant " soft , usually chosen to
Reference: [18] <author> David Kahaner, Cleve Moler, and Stephen Nash. </author> <title> Numerical Methods and Software. </title> <booktitle> Prentice-Hall series in Computational Mathematics. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: Dependence of shadowing on the choice of accurate integrator I have tried two integrators as my "accurate" integrator, although both were variable-order, variable-timestep Adams's methods called SDRIV2 and LSODE <ref> [18, 16] </ref>. QT used a Bulirsch-Stoer integra 13 tor by Press et al. [28].
Reference: [19] <author> Henry E. Kandrup and Haywood Smith. </author> <title> On the sensitivity of the N -body problem to small changes in initial conditions. </title> <journal> The Astrophysical Journal, </journal> <volume> 374 </volume> <pages> 255-265, </pages> <year> 1991. </year>
Reference-contexts: They found that different algorithms and computers gave results in which some measures differed by as much as 100%. More recent work on the growth of errors includes Kandrup and Smith <ref> [19] </ref>, who showed that under a large range of parameters, the time scale over which small perturbations grow by a factor of e (Euler's constant, 2:7182 : : :), called the e-folding time 1 , is comparable to the crossing time (the average time it takes a particle to cross the <p> Barnes and Hut [5] claim that astrophysical N -body simulations require only "modest" accuracy levels, but also concede that quoting energy conservation isn't enough, and that more stringent tests are needed. An example of conservation of macroscopic properties is given by Kandrup and Smith <ref> [19] </ref>. They show that a histogram of the e-folding times of individual particles stays constant within statistical uncertainties, even though the phase-space distribution of those particles is vastly different for different initial conditions. <p> This may explain why QT's graph 6 (b) has less scatter than their graph 6 (a). 1 There is some disagreement that these two factors exist. There seems to be an ongoing debate between Kandrup and Smith <ref> [19, 21, 20] </ref>, who argue that the growth of errors is due both to the global potential and to collisional effects, while Goodman, Heggie, and Hut [10] argue that collisional encounters are the only process for the magnification of errors. 43 attempts to shadow noisy orbits in the unsoftened Plummer model
Reference: [20] <author> Henry E. Kandrup and Haywood Smith. </author> <title> On the sensitivity of the N -body problem to small changes in initial conditions. III. </title> <journal> The Astrophysical Journal, </journal> <volume> 399 </volume> <pages> 627-633, </pages> <year> 1992. </year>
Reference-contexts: Perhaps the results of collisionless systems can be trusted for longer than collisional ones, since close encounters have a much smaller effect in the former. Kandrup and Smith <ref> [20] </ref> showed that as softening is increased (i.e., the collisionality is decreased), the e-folding time grows slightly faster than linearly. (i.e., the Lyapunov exponent decreases, so errors are magnified more slowly.) They agree with Goodman et al. that the error magnification is due more to the rare individual particles whose errors <p> Perhaps a more realistic scaling of the problem, at least from the astronomer's point of view, would be to have M particles move amongst 100M fixed ones. This would smoothen the gravitational potential, which we know from other studies <ref> [20, 10] </ref> decreases the Lyapunov exponent, and thus may lengthen the average shadow length. Despite the above caveats, there is reason to believe that high-dimensional shadowing may be difficult. Dawsen et al. [8] show that shadowing becomes extremely difficult in systems where a Lyapunov exponent fluctuates about zero. <p> This may explain why QT's graph 6 (b) has less scatter than their graph 6 (a). 1 There is some disagreement that these two factors exist. There seems to be an ongoing debate between Kandrup and Smith <ref> [19, 21, 20] </ref>, who argue that the growth of errors is due both to the global potential and to collisional effects, while Goodman, Heggie, and Hut [10] argue that collisional encounters are the only process for the magnification of errors. 43 attempts to shadow noisy orbits in the unsoftened Plummer model
Reference: [21] <author> Henry E. Kandrup and Haywood Smith. </author> <title> On the sensitivity of the N -body problem to small changes in initial conditions. II. </title> <journal> The Astrophysical Journal, </journal> <volume> 386 </volume> <pages> 635-645, </pages> <year> 1992. </year>
Reference-contexts: This may explain why QT's graph 6 (b) has less scatter than their graph 6 (a). 1 There is some disagreement that these two factors exist. There seems to be an ongoing debate between Kandrup and Smith <ref> [19, 21, 20] </ref>, who argue that the growth of errors is due both to the global potential and to collisional effects, while Goodman, Heggie, and Hut [10] argue that collisional encounters are the only process for the magnification of errors. 43 attempts to shadow noisy orbits in the unsoftened Plummer model
Reference: [22] <author> M. Lecar. Bull. Astron., 3:91, </author> <year> 1968. </year>
Reference-contexts: Lecar <ref> [22] </ref> co-ordinated a study between many researchers, each of whom independently computed the solution to an N -body problem with identical initial conditions. They found that different algorithms and computers gave results in which some measures differed by as much as 100%.
Reference: [23] <author> Thomas A. McGlynn. </author> <title> Dissipational collapse of galaxies and initial conditions. </title> <journal> The Astrophysical Journal, </journal> <volume> 281 </volume> <pages> 13-30, </pages> <year> 1984. </year>
Reference-contexts: The astronomical literature is brimming with results of large N -body simulations. The proliferation of titles such as "Simulations of Sinking Galaxies" [33], "Dissipationless Collapse of Galaxies and Initial Conditions" <ref> [23] </ref>, "A Numerical Study of the Stability of Spherical Galaxies" [24], "The Global Stability of Our Galaxy" [30], and "Dynamical Instabilities in Spherical Stellar Systems" [4] shows that much trust is placed in the results of these simulations.
Reference: [24] <author> David Merritt and Luis A. Aguilar. </author> <title> A numerical study of the stability of spherical galaxies. </title> <journal> Monthly Notices of the Royal Astronomical Society, </journal> <volume> 217 </volume> <pages> 787-804, </pages> <year> 1985. </year>
Reference-contexts: The astronomical literature is brimming with results of large N -body simulations. The proliferation of titles such as "Simulations of Sinking Galaxies" [33], "Dissipationless Collapse of Galaxies and Initial Conditions" [23], "A Numerical Study of the Stability of Spherical Galaxies" <ref> [24] </ref>, "The Global Stability of Our Galaxy" [30], and "Dynamical Instabilities in Spherical Stellar Systems" [4] shows that much trust is placed in the results of these simulations.
Reference: [25] <author> Dimitri Mihalas and James Binney. </author> <title> Galactic Astronomy | Structure and Kinematics. </title> <booktitle> Prince-ton Series in Astrophysics. </booktitle> <publisher> Freeman, </publisher> <year> 1981. </year>
Reference-contexts: reasonable estimate for the time for this simulation would take on a Sun SPARCstation IPC would be about 100 times longer | about 10 weeks. (If 99% vectorization could be achieved, it would be about 1,000 times longer | about 2 years.) 0 See the discussion on stellar kinematics in <ref> [25] </ref>, particularly pages 431-438. 42 As with all the simulations in this thesis, the particles each had mass 1=N , were distributed uniformly in the unit cube, and the initial velocity of the moving particle was also generated in the uniform unit cube in velocity space.
Reference: [26] <author> R. H. Miller. </author> <note> The Astrophysical Journal, 140:250, </note> <year> 1964. </year>
Reference-contexts: Given that numerical computations are constantly introducing small errors to the computed solution, we must naturally ask what effect these errors have on computed solutions. 0 0.1 History of exponential divergence in N -body systems Miller <ref> [26] </ref> was the first to show that small changes in the initial conditions of an N -body system result in exponentially diverging solutions. Lecar [22] co-ordinated a study between many researchers, each of whom independently computed the solution to an N -body problem with identical initial conditions.
Reference: [27] <author> D. Pfenniger and D. Friedli. </author> <title> Computational issues connect with 3D N-body simulations. </title> <journal> Astronomy and Astrophysics, </journal> <volume> 270 </volume> <pages> 561-572, </pages> <year> 1993. </year>
Reference-contexts: For example, Hernquist, Hut and Makino [15], Barnes and Hut [5], and Sellwood [31] explicitly say this; Singh, Hennessy and Gupta [32] have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl <ref> [27] </ref>, Jernigan and Porter [17], and Barnes and Hut [3] all imply that using the largest possible N is a desirable characteristic. * force softening, i.e., replacing r 2 with (r 2 + " 2 soft ) in the denominator of the gravitational force computation for some small constant " soft
Reference: [28] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: Dependence of shadowing on the choice of accurate integrator I have tried two integrators as my "accurate" integrator, although both were variable-order, variable-timestep Adams's methods called SDRIV2 and LSODE [18, 16]. QT used a Bulirsch-Stoer integra 13 tor by Press et al. <ref> [28] </ref>. It would be interesting to study whether the choice of accurate integrator influences the numerical shadow found, because if one true shadow exists, then infinitely many (closely packed) true shadows exist. 1 However, if the boundary conditions are the same, then the solutions should be the same. <p> For their "cheap" integrator, they used a 5th-order predictor-corrector routine that is commonly used by astronomers [Appendix B of [6]], arranged to have 1-step errors of about 10 5 . For their expensive integrator, they used a Bulirsch-Stoer method <ref> [28] </ref> with tolerance 10 13 . They found they could shadow the particle for several crossing times. Unfortunately, their refinement procedure was too inefficient to work on significantly higher dimensional systems.
Reference: [29] <author> Gerald D. Quinlan and Scott Tremaine. </author> <title> On the reliability of gravitational N -body integrations. </title> <journal> Monthly Notices of the Royal Astronomical Society, </journal> <volume> 259 </volume> <pages> 505-518, </pages> <year> 1992. </year>
Reference-contexts: Given that N -body systems are chaotic, and that their simulation introduces the above input errors, we must now ask precisely what we mean by the "accuracy" of a simulation. Amazing as it may seem, there is currently no clear definition of simulation accuracy <ref> [29] </ref>. Obviously, attempting to follow the individual paths of all N particles is infeasible; Goodman, Heggie and Hut [10] show that this would require O (N ) digits of precision. <p> Chapter 2 is the main body of the work, which introduces the optimizations to the shadowing algorithm of Quinlan and Tremaine <ref> [29] </ref>, and talks about high-dimensional shadowing in general. Chapter 3 contains some preliminary results on high-dimensional shadowing that will be of interest both to practicing N -body astronomers, and to those interested in shadowing in general. <p> Although this is a good area for further work, containment is beyond the scope of this thesis. 7 The refinement algorithm that concerns us in this thesis is the one first presented in two dimen-sions by GHYS, and generalized to handle arbitrary Hamiltonian systems by Quinlan and Tremaine <ref> [29] </ref>, hereinafter referred to as QT. QT make the distinction between dynamical noise and observational noise. Observational noise does not effect the future evolution of the system. <p> Thus it cannot be used to disprove the existence of a shadow. 1. refinement, which GHYS intended simply as a method to reduce the noise of a pseudo trajectory to increase the chances of success of their containment procedure. 19 Quinlan and Tremaine (QT) <ref> [29] </ref>, wanted to test for the existence of shadows in large N -body systems. They generalized the refinement portion of the GHYS algorithm to work on arbitrary-dimensional Hamiltonian systems. <p> QT: original code of Quinlan & Tremaine <ref> [29] </ref> L: Large shadow steps I: backward resolvent by Inverting forward resolvent C: Cheaper accurate integrator R: constant RUS U: reUse RUS from previous successful shadow (appears only in combinations) 32 than average in some columns. <p> This leads naturally to the idea of what I call Fixed Motion Shadowing. 4.0.3 Fixed Motion Shadowing Shadowing of particular particles in large fixed-motion N -body systems, or simply Fixed Motion Shadowing, is more general than the form of shadowing done by Quinlan and Tremaine (QT) <ref> [29] </ref>. In their system, one particle moves amongst many particles whose positions are fixed.
Reference: [30] <author> J. A. Sellwood. </author> <title> The global stability of our Galaxy. </title> <journal> Monthly Notices of the Royal Astronomical Society, </journal> <volume> 217 </volume> <pages> 127-148, </pages> <year> 1985. </year>
Reference-contexts: The astronomical literature is brimming with results of large N -body simulations. The proliferation of titles such as "Simulations of Sinking Galaxies" [33], "Dissipationless Collapse of Galaxies and Initial Conditions" [23], "A Numerical Study of the Stability of Spherical Galaxies" [24], "The Global Stability of Our Galaxy" <ref> [30] </ref>, and "Dynamical Instabilities in Spherical Stellar Systems" [4] shows that much trust is placed in the results of these simulations.
Reference: [31] <author> J. A. Sellwood. </author> <title> The art of N -body building. </title> <booktitle> Annual Review of Astronomy and Astrophysics, </booktitle> <volume> 25 </volume> <pages> 151-86, </pages> <year> 1987. </year>
Reference-contexts: The general consensus seems to be that this is the limiting source of error in current large N -body simulations. For example, Hernquist, Hut and Makino [15], Barnes and Hut [5], and Sellwood <ref> [31] </ref> explicitly say this; Singh, Hennessy and Gupta [32] have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl [27], Jernigan and Porter [17], and Barnes and Hut [3] all imply that using the largest possible N is a desirable characteristic. * force softening, i.e.,
Reference: [32] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Implications of Hierarchical N - body Methods for Multiprocessor Architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Computer Systems Lab, Stanford University, Stanford, </institution> <address> CA 94305, </address> <year> 1992. </year>
Reference-contexts: The general consensus seems to be that this is the limiting source of error in current large N -body simulations. For example, Hernquist, Hut and Makino [15], Barnes and Hut [5], and Sellwood [31] explicitly say this; Singh, Hennessy and Gupta <ref> [32] </ref> have a "master error equation" in which clearly discreteness noise is dominant; and Pfenniger and Friedl [27], Jernigan and Porter [17], and Barnes and Hut [3] all imply that using the largest possible N is a desirable characteristic. * force softening, i.e., replacing r 2 with (r 2 + "
Reference: [33] <author> Simon D. White. </author> <title> Simulations of sinking galaxies. </title> <journal> The Astrophysical Journal, </journal> <volume> 274 </volume> <pages> 53-61, </pages> <year> 1983. </year> <month> 61 </month>
Reference-contexts: The astronomical literature is brimming with results of large N -body simulations. The proliferation of titles such as "Simulations of Sinking Galaxies" <ref> [33] </ref>, "Dissipationless Collapse of Galaxies and Initial Conditions" [23], "A Numerical Study of the Stability of Spherical Galaxies" [24], "The Global Stability of Our Galaxy" [30], and "Dynamical Instabilities in Spherical Stellar Systems" [4] shows that much trust is placed in the results of these simulations.
References-found: 33

