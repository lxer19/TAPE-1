URL: http://www.hds.utc.fr/~grandval/icann98.ps.gz
Refering-URL: http://www.hds.utc.fr/~grandval/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Yves.Grandvalet@hds.utc.fr  
Title: Least Absolute Shrinkage is Equivalent to Quadratic Penalization  
Author: Yves Grandvalet 
Address: BP 20.529, 60205 Compiegne Cedex, France  
Affiliation: Heudiasyc, UMR CNRS 6599, Universite de Technologie de Compiegne,  
Abstract: Adaptive ridge is a special form of ridge regression, balancing the quadratic penalization on each parameter of the model. This paper shows the equivalence between adaptive ridge and lasso (least absolute shrinkage and selection operator). This equivalence states that both procedures produce the same estimate. Least absolute shrinkage can thus be viewed as a particular quadratic penalization. From this observation, we derive an EM algorithm to compute the lasso solution. We finally present a series of applications of this type of algorithm in regres sion problems: kernel regression, additive modeling and neural net training.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman. </author> <title> Heuristics of instability and stabilization in model selection. </title> <journal> The Annals of Statistics 1996, </journal> <volume> 24(6) </volume> <pages> 2350-2383. </pages>
Reference-contexts: It is also expected to produce models with few non-zero coefficients if interpretation is planned. Ridge regression and subset selection are the two main penalization procedures. The former is stable, but does not shrink parameters to zero, the latter gives simple models, but it is unstable <ref> [1] </ref>. These observations motivated the search for new penalization techniques such as garrotte, non-negative garrotte [1], and lasso (least absolute shrinkage and selection operator) [2]. Adaptative noise injection [3] was proposed as a means to automatically balance penalization on different coefficients. Adaptive ridge regression is its deterministic version. <p> Ridge regression and subset selection are the two main penalization procedures. The former is stable, but does not shrink parameters to zero, the latter gives simple models, but it is unstable <ref> [1] </ref>. These observations motivated the search for new penalization techniques such as garrotte, non-negative garrotte [1], and lasso (least absolute shrinkage and selection operator) [2]. Adaptative noise injection [3] was proposed as a means to automatically balance penalization on different coefficients. Adaptive ridge regression is its deterministic version. Section 2 presents adaptive ridge. <p> The Bayes prior distribution is a centered normal distribution, with variance proportional to 1=. This prior distribution treats all covariates similarly. It is not appropriate when we know that all covariates are not equally relevant. The garrotte estimate <ref> [1] </ref> is based on the OLS estimate b fi ffi . The standard quadratic constraint is replaced by P d j b fi ffi 2 j C. The coefficients with smaller OLS estimate are thus more heavily penalized. Other modifications are better explained with the prior distribution viewpoint.
Reference: [2] <author> R.J. Tibshirani. </author> <title> Regression shrinkage and selection via the lasso. </title> <type> Technical report, </type> <institution> University of Toronto, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The former is stable, but does not shrink parameters to zero, the latter gives simple models, but it is unstable [1]. These observations motivated the search for new penalization techniques such as garrotte, non-negative garrotte [1], and lasso (least absolute shrinkage and selection operator) <ref> [2] </ref>. Adaptative noise injection [3] was proposed as a means to automatically balance penalization on different coefficients. Adaptive ridge regression is its deterministic version. Section 2 presents adaptive ridge. The equivalence of this special form of ridge regression with the lasso is briefly shown in section 3. <p> definition is that the adaptive ridge estimate uses the constraint ( P d ffi instead of P d 2 Adaptive ridge, as ridge or lasso, is not scale invariant, so that the covariates should be normalized to produce sensible estimates. 4 A new algorithm for finding the lasso solution Tibshirani <ref> [2] </ref> proposes to use quadratic programming to find the lasso solution, with 2d variables (positive and negative parts of fi j ) and 2d+1 constraints (signs of positive and negative parts of fi j plus constraint (8)). The writing (2) suggests to use the EM algorithm.
Reference: [3] <author> Y. Grandvalet and S. Canu. </author> <title> Adaptive noise injection for input variables relevance determination. In: </title> <publisher> ICANN'97, Springer-Verlag 1997, </publisher> <pages> pp 463-468. </pages>
Reference-contexts: The former is stable, but does not shrink parameters to zero, the latter gives simple models, but it is unstable [1]. These observations motivated the search for new penalization techniques such as garrotte, non-negative garrotte [1], and lasso (least absolute shrinkage and selection operator) [2]. Adaptative noise injection <ref> [3] </ref> was proposed as a means to automatically balance penalization on different coefficients. Adaptive ridge regression is its deterministic version. Section 2 presents adaptive ridge. The equivalence of this special form of ridge regression with the lasso is briefly shown in section 3. <p> Mixtures of Gaussians may be used to cluster different set of covariates. Several models have been proposed, with data dependent clusters [4], or classes defined a priori [5]. The automatic relevance determination model [6] ranks in the latter type. Follow ing <ref> [3] </ref>, we propose to use such a mixture, in the form b fi = Argmin fi i=1 d X fi j x ij y i + j=1 j : (2) Here, each coefficient has its own prior distribution.
Reference: [4] <author> S.J. Nowlan and G.E. Hinton. </author> <title> Simplifying neural networks by soft weight-sharing. </title> <booktitle> Neural Computation 1992, </booktitle> <volume> 4(4) </volume> <pages> 473-493. </pages>
Reference-contexts: The coefficients with smaller OLS estimate are thus more heavily penalized. Other modifications are better explained with the prior distribution viewpoint. Mixtures of Gaussians may be used to cluster different set of covariates. Several models have been proposed, with data dependent clusters <ref> [4] </ref>, or classes defined a priori [5]. The automatic relevance determination model [6] ranks in the latter type.
Reference: [5] <author> D.J.C. MacKay. </author> <title> A practical Bayesian framework for backprop networks. </title> <booktitle> Neural Computation 1992, </booktitle> <volume> 4(3) </volume> <pages> 448-472. </pages>
Reference-contexts: The coefficients with smaller OLS estimate are thus more heavily penalized. Other modifications are better explained with the prior distribution viewpoint. Mixtures of Gaussians may be used to cluster different set of covariates. Several models have been proposed, with data dependent clusters [4], or classes defined a priori <ref> [5] </ref>. The automatic relevance determination model [6] ranks in the latter type.
Reference: [6] <author> R. M. Neal. </author> <title> Bayesian Learning for Neural Networks. </title> <booktitle> Lecture Notes in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Other modifications are better explained with the prior distribution viewpoint. Mixtures of Gaussians may be used to cluster different set of covariates. Several models have been proposed, with data dependent clusters [4], or classes defined a priori [5]. The automatic relevance determination model <ref> [6] </ref> ranks in the latter type. Follow ing [3], we propose to use such a mixture, in the form b fi = Argmin fi i=1 d X fi j x ij y i + j=1 j : (2) Here, each coefficient has its own prior distribution.
Reference: [7] <author> W. Hardle. </author> <title> Applied Nonparametric Regression, </title> <booktitle> volume 19 of Economic Society Monographs. </booktitle> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The EM algorithm can be applied to these other problems, without modifying the E-step. 5 Applications Adaptive ridge regression may be applied to a variety of problems. They include kernel regression, additive modeling and neural net training. Kernel smoothers <ref> [7] </ref> can benefit from the sparse representation given by soft-thresholding methods like lasso. For these regressors, b f (x) = P ` there are as many covariates as pairs in the sample. <p> The quadratic procedure of lasso with 2` + 1 constraints becomes computationally expensive, but the EM algorithm of adaptive ridge is reasonably fast to converge. An example of result is shown on figure 1 for the motorcycle dataset <ref> [7] </ref>. On this example, the final fitting uses only a few kernels (17 out of 133). The plus are data points, and dots are the prototypes corresponding to the kernels with non-zero coefficients. The Gaussian kernel used is repre sented dotted in the lower right-hand corner.
Reference: [8] <author> F. Girosi. </author> <title> An equivalence between sparse approximation and support vector machines. </title> <type> Technical Report 1606, </type> <institution> M.I.T. AI Laboratory, </institution> <address> Cambridge, MA., </address> <year> 1997. </year>
Reference-contexts: On this example, the final fitting uses only a few kernels (17 out of 133). The plus are data points, and dots are the prototypes corresponding to the kernels with non-zero coefficients. The Gaussian kernel used is repre sented dotted in the lower right-hand corner. Girosi <ref> [8] </ref> showed an equivalence between a version of least absolute shrinkage applied to kernel smoothing, and Support Vector Machine (SVM) [9]. However, adaptive ridge, as applied here, is not equivalent to SVM fitting, because the response variables are noisy and the cost minimized is different.
Reference: [9] <author> V.N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: The Gaussian kernel used is repre sented dotted in the lower right-hand corner. Girosi [8] showed an equivalence between a version of least absolute shrinkage applied to kernel smoothing, and Support Vector Machine (SVM) <ref> [9] </ref>. However, adaptive ridge, as applied here, is not equivalent to SVM fitting, because the response variables are noisy and the cost minimized is different. The prototypes shown on figure 1 are thus very different from the support vectors that would be obtained by SVM fitting.
Reference: [10] <author> T.J. Hastie and R.J. Tibshirani. </author> <title> Generalized Additive Models, </title> <booktitle> volume 43 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: However, adaptive ridge, as applied here, is not equivalent to SVM fitting, because the response variables are noisy and the cost minimized is different. The prototypes shown on figure 1 are thus very different from the support vectors that would be obtained by SVM fitting. Additive models <ref> [10] </ref> are sums of univariate functions, b f (x) = P d b f j (x j ), where the b f j are smooth but unspecified functions.
Reference: [11] <author> S. Canu, Y. Grandvalet, and M.-H. </author> <title> Masson. Black-box software sensor design for environmental monitoring. In: </title> <publisher> ICANN'98, Springer-Verlag 1998. </publisher>
Reference-contexts: The goal here is to penalize/select the input variables according to their relevance, and each output variable according to the smoothness of the corresponding mapping <ref> [11] </ref>. The solid curves are the estimates of the univariate functions (Gaussian kernel smoothers) for different values of the overall tuning parameter . The true function is represented in dotted line.
References-found: 11

