URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9301.ps.Z
Refering-URL: http://www.cs.virginia.edu/~isotach/pubs.html
Root-URL: http://www.cs.virginia.edu
Title: Concurrency Control in Asynchronous Computations  
Author: Carolyn Craig Williams 
Degree: A Dissertation Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy (Computer Science)  
Date: January 1993  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [AdH90] <author> S. V. Adve and M. D. Hill, </author> <title> Implementing Sequential Consistency in Cache-Based Systems, </title> <booktitle> 1990 ICPP, </booktitle> <month> August, </month> <year> 1990, </year> <month> I:47-50. </month>
Reference-contexts: Causal memory [AHJ90] allows a read to return any of a set of writes that logically precede the read with the result that concurrent reads can return different values. Weak ordering <ref> [AdH90, BNR89, DSB86] </ref> ensures sequential consistency only at synchronization points, and release consistency [Gha90] enforces a yet weaker form of sequential consistency around release and acquire synchronization operations such as operations on locks. Sequential consistency is most often discussed in relation to cache protocols.
Reference: [Aga88] <author> A. Agarwal, et al., </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proc. of the 15th International Symp. Computer Architecture, </booktitle> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: When a block is written, the information needed to maintain coherence can be sent point-to-point to only those PE's with a copy of the affected block. Some protocols are directory/snoopy hybrids, e.g., the limited directory protocols proposed by Agarwal, et al., <ref> [Aga88] </ref> and several protocols for multiple bus or 97 hybrid bus/MIN architectures, such as the protocol proposed by Algudady, et al., [ADT90] and the DASH [Len90], VMP-MC [CGB89], Aquarius [CaD90], and Galactica Net [WiL92] protocols. <p> The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [AhB89] <author> M. L. Ahuja and J. C. Browne, </author> <title> Concurrency Control by Transactions Carrying States and Preordering Multiversioned Entities, </title> <booktitle> Information Sciences 48(1989), </booktitle> <pages> 157-193. </pages>
Reference-contexts: The path protocol has been generalized to a tree-structured ordering [Mon78, SiK80, Sil82]. Several protocols have been designed to reduce the expected number of extra nodes a transaction must lock <ref> [AhB89, BuS85, Yan82] </ref>, but these protocols have other drawbacks: large messages and complex tests at each node [AhB89], reliance on static mode information about transaction access patterns [BuS85], or the potential for cascading rollback [Yan82]. <p> The path protocol has been generalized to a tree-structured ordering [Mon78, SiK80, Sil82]. Several protocols have been designed to reduce the expected number of extra nodes a transaction must lock [AhB89, BuS85, Yan82], but these protocols have other drawbacks: large messages and complex tests at each node <ref> [AhB89] </ref>, reliance on static mode information about transaction access patterns [BuS85], or the potential for cascading rollback [Yan82]. In systems in which the network topology matches the hierarchical structure of the data, the tree protocols can be viewed as implementing ordered multicasts.
Reference: [AHJ90] <author> M. Ahuja, P. W. Hutto and R. John, </author> <title> Implementing and Programming Causal Distributed Shared Memory, </title> <institution> GIT-CC-90-499, Georgia Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Processor consistency [Goo89] requires that writes issued by the same process appear to be executed in the order specified by the program, but allows writes issued by different processes to be seen by different processes in an inconsistent order. Causal memory <ref> [AHJ90] </ref> allows a read to return any of a set of writes that logically precede the read with the result that concurrent reads can return different values.
Reference: [ADT90] <author> M. S. Algudady, C. R. Das and M. J. Thazhuthaveetil, </author> <title> A Write Update Cache Coherence Protocol for Min-Based Multiprocessors with Accessibility Based Split Caches, </title> <booktitle> Proc. Supercomputing '90, </booktitle> <address> New York, New York, </address> <month> November </month> <year> 1990, </year> <pages> 544-553. </pages>
Reference-contexts: Some protocols are directory/snoopy hybrids, e.g., the limited directory protocols proposed by Agarwal, et al., [Aga88] and several protocols for multiple bus or 97 hybrid bus/MIN architectures, such as the protocol proposed by Algudady, et al., <ref> [ADT90] </ref> and the DASH [Len90], VMP-MC [CGB89], Aquarius [CaD90], and Galactica Net [WiL92] protocols. Directory protocols are so named because they maintain a directory for each block listing the location of all copies of the block. The first directory protocol, due to Tang [Tan76], specified a single centralized directory.
Reference: [And90] <author> T. E. Anderson, </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1,1 (January 1990), </journal> <pages> 6-16. </pages>
Reference-contexts: Before executing an atomic action that accesses more than one variable or that accesses a single variable more than one time, the process obtains locks on the accessed variable (s). Existing hardware support for atomicity is designed to make locking more efficient <ref> [And90, GVW89, Jay88, MeS90] </ref>. Hardware support for atomicity has also taken the form of adding simple ALU's to the memory modules (MM's) to permit atomic actions consisting of a single read-modify-write (RMW) operation to be implemented without locking [KRS88].
Reference: [ABP92] <author> J. B. Andrews, C. J. Beckmann and D. K. Poulsen, </author> <title> Notification and Multicast Networks for Synchronization and Coherence, </title> <journal> Journal of Parallel and Distributed Computing 15(August 1992), </journal> <pages> 332-350. </pages>
Reference-contexts: Split Operations Heap Implementation of Access Sequences hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh the read mask to determine whether to route the response on one or both outputs. Designs for switches that can decode read masks have already been proposed in support of unordered multicasts <ref> [ABP92, Ste89] </ref>. An additional benefit of using the network to fan-out responses is a reduction in traffic in the stages closest to memory. In computing the space overhead for the heap implementation we assume, for simplicity, that each variable is of the same size vsize. <p> Some protocols, e.g., read broadcast protocols [KMR86], are write-invalidate/write-update hybrids. Because the concurrency of write-invalidate protocols is inherently limited, the delta-cache protocols use a write-update policy. A disadvantage of the update policy is the cost of distributing cache updates. Hardware support for multi-casting <ref> [ABP92, Ste89] </ref> can reduce this cost in a way that is compatible with isotach networks. To maintain consistency among the copies of the same block and ensure processes observe updates to different blocks in a consistent order, updates must appear to be executed as an indivisible step.
Reference: [ArB84] <author> J. Archibald and J. L. Baer, </author> <title> An Economical Solution to the Cache Coherence Problem, </title> <booktitle> Proc. 11th International Symp. Computer Architecture, </booktitle> <year> 1984, </year> <pages> 355-362. </pages>
Reference-contexts: The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [ArB86] <author> J. Archibald and J. L. Baer, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Transactions on Computer Systems 4,4 (November 1986), </journal> <pages> 273-298. </pages>
Reference-contexts: One way to obtain this consistent ordering is illustrated by the snoopy protocols for the DEC Firefly and Xerox Dragon (described by Archibald and Baer <ref> [ArB86] </ref>). These protocols allow concurrent readers and writers to the same cache block. All PE's receive updates in the same order because the shared bus serializes the update broadcasts.
Reference: [ACE88] <author> Arvind, D. E. Culler and K. Ekanadham, </author> <title> The Price of Asynchronous Parallelism: An Analysis of Dataflow Architectures, </title> <type> RC 13889 (#62181), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> July, </month> <year> 1988. </year>
Reference-contexts: This chapter gives correctness criteria for concurrency control and describes existing solutions to the concurrency control problem. Our discussion is in SMM terms but is applicable to the message-based model (MBM). 2.1. ASSUMPTIONS AND PRELIMINARY DEFINITIONS We begin by distinguishing the basic work <ref> [ACE88] </ref> operations executed to fulfill the function of the program from the concurrency control operations executed to control the order in which basic work operations are executed.
Reference: [ANP89] <author> Arvind, R. S. Nikhil and K. K. Pingali, I-Structures: </author> <title> Data Structures for Parallel Computing, </title> <journal> ACM Trans. Prog. Lang. and Systems 11,4 (October 1989), </journal> <pages> 598-632. </pages>
Reference-contexts: Data dependences are typically enforced with semaphores or barriers. Hardware support includes synchronization flags or key fields for controlling access to the associated variable, e.g., the full/empty flags on the HEP [Jor83], the empty bit associated with each element of the I-structure proposed by Arvind for dataflow computation <ref> [ANP89] </ref>, and the synchronization key fields proposed by Zhu and Yew and by Peir [Pei83, ZhY84]. Other proposals for hardware support of version consistency include algorithms for improving the efficiency of barrier synchronization [Bro86, HFM88, Jay87]. 21 2.4.
Reference: [Awe85] <author> B. Awerbuch, </author> <title> Complexity of Network Synchronization, </title> <journal> J. ACM 32,4 (October 1985), </journal> <pages> 804-823. </pages>
Reference-contexts: In more vivid terms, the network pulses like a heart. These pulses supply the timing mechanism for a distributed logical clock. Each pulse of the network advances the logical clocks at each node by one logical time pulse. Local synchrony was first proposed by Awerbuch <ref> [Awe85] </ref>, who uses local synchrony (the a-synchronizer in Awerbuch's terms) to support execution of SIMD graph algorithms on asynchronous networks. Local synchrony has also been used by Gibbons to support barrier synchronization [BGS89, Gib89] and by Ranade to emulate a concurrent-read, concurrent-write (CRCW) PRAM [Ran87, RBJ88].
Reference: [BaR88] <author> B. R. Badrinath and K. Ramamritham, </author> <title> Synchronizing Transactions on Objects, </title> <journal> IEEE Trans. on Computers 37,5 (1988), </journal> <pages> 541-547. </pages>
Reference-contexts: There have been several proposals for using semantic information provided by the programmer either about the transactions [FaO89, Gar83] or about the objects accessed <ref> [BaR88, WeL85] </ref> to increase the concurrency of the scheduler. These protocols have significant disadvantages in that they are application dependent, require significant programmer effort to specify the semantics, and have more overhead than protocols using only syntactic information. A second alternative to serializability as a correctness criterion is linearizability [HeW89].
Reference: [BaT88] <author> H. E. Bal and A. S. Tanenbaum, </author> <title> Distributed Programming with Shared Data, </title> <booktitle> Proc. of the IEEE CS 1988 Int. Conf. on Computer Languages, </booktitle> <address> Miami, Florida, </address> <month> October 9-13, </month> <year> 1988, </year> <pages> 82-91. </pages>
Reference-contexts: Using totally ordered multicasts to send updates to all the nodes with a copy of replicated data ensures all nodes receive the updates in the same order. This way of maintaining consistency of replicated data has been proposed for distributed databases [Cha84, PBS89] and virtual shared memory <ref> [BaT88] </ref>, and is the principle on which snoopy cache coherence protocols work (See Chapter 6). Consistency of replicated data can also be maintained using a causal multicast in place of a totally ordered multicast [JoB86].
Reference: [Bar92] <author> P. S. Barth, </author> <title> Using Atomic Data Structures for Parallel Simulation, </title> <booktitle> Proc. Scalable High Performance Computing Conference, </booktitle> <month> April </month> <year> 1992, </year> <pages> 30-37. </pages>
Reference-contexts: The idea of using multiple versions to eliminate false dependences has appeared in many contexts, for example in parallel databases [BeG83, Pap86], dataflow computation [Cha71], compilation of loops for parallel execution <ref> [Bar92, Uht87] </ref>, and software pipelining [Lam88]. Most systems maintain only a single version of each shared variable and so can satisfy false dependences only by executing the operations in the specified order. A common source of version constraints in SMM programs is in the parallel execution of loops.
Reference: [BMR89] <author> S. J. Baylor, K. P. McAuliffe and B. D. Rathi, </author> <title> Cache Coherence Protocols for MIN-Based Multiprocessors, </title> <type> RC 15221, </type> <institution> IBM Research Report, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: In choosing to focus on hardware protocols, we do not dismiss the benefits of using static information in reducing the cost of cache coherence. Several software/hardware hybrid protocols that use static information about variable access patterns to improve hardware protocols have been proposed <ref> [BMR89, BCZ90, LiY91] </ref>, and we see a role for such static information in delta-cache protocols. Cache protocols are also classified as snoopy or directory protocols.
Reference: [BaR89] <author> S. J. Baylor and B. D. Rathi, </author> <title> A Study of the Memory Reference Behavior of Engineering/Scientific Applications in Parallel Processors, </title> <booktitle> ICPP, </booktitle> <year> 1989, </year> <month> I-78-82. </month>
Reference-contexts: Eliminating the memory copy allows an optimization for the special case in which only the owner has a cache copy. 117 w r r H H hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Studies of parallel programs suggest this case occurs frequently in actual applications <ref> [BaR89, EgK88] </ref>. (This case may not, however, occur as frequently in isotach programs.
Reference: [BBG89] <author> C. Beeri, P. A. Bernstein and N. Goodman, </author> <title> A Model for Concurrency in Nested Transactions Systems, </title> <editor> J. </editor> <booktitle> ACM 36,2 (1989), </booktitle> <pages> 230-269. </pages>
Reference-contexts: One potential application for nested isochrons is as a protocol for nested transactions <ref> [BBG89, Mos85] </ref>. In a nested transaction, a message may represent either a primitive operation or trigger the issuance of a sub-transaction. The need for a nested transaction protocol arises in distributed databases and in parallel programs, such as object-oriented programs, in which information-hiding is a goal.
Reference: [BCZ90] <author> J. K. Bennett, J. B. Carter and W. Zwaenepoel, </author> <title> Adaptive Software Cache Management for Distributed Shared Memory Architectures, </title> <booktitle> Proc. 17th Annual International Symp. on Computer Architecture, </booktitle> <month> May, </month> <year> 1990, </year> <pages> 1125-1135?. 183 </pages>
Reference-contexts: In choosing to focus on hardware protocols, we do not dismiss the benefits of using static information in reducing the cost of cache coherence. Several software/hardware hybrid protocols that use static information about variable access patterns to improve hardware protocols have been proposed <ref> [BMR89, BCZ90, LiY91] </ref>, and we see a role for such static information in delta-cache protocols. Cache protocols are also classified as snoopy or directory protocols.
Reference: [BeS80] <author> P. A. Bernstein and D. W. Shipman, </author> <title> The Correctness of Concurrency Control Mechanisms in a System for Distributed Databases (SDD-1), </title> <journal> ACM Trans. Database Systems 5,1 (March 1980), </journal> <pages> 52-68. </pages>
Reference-contexts: Given its high communication cost, conservative T/O is typically considered only for fully replicated databases, where high communication costs are unavoidable, or where statically available information 25 about communication patterns allows some pruning of the required communications <ref> [BeS80] </ref>. Unless a conservative T/0 protocol operates in declaration mode, care must be taken to avoid deadlock [McL81]. The basic isotach-based concurrency control technique for enforcing atomicity (the time slice technique described in Chapter 5) is analogous to a multiversion, conservative timestamp ordering protocol operating in declaration mode.
Reference: [BeG80] <author> P. A. Bernstein and N. Goodman, </author> <title> Timestamp Based Algorithms for Concurrency Control in Distributed Database Systems, </title> <booktitle> Proc. 6th Int. Conf. Very Large Data Bases, </booktitle> <month> October </month> <year> 1980. </year>
Reference-contexts: A transaction must still be aborted if one of its prewrites arrives at a DM after a younger read for the same entity. (3) Conservative T/O <ref> [BeG80, HeV79, KNT79, Mil79] </ref>. In a conservative T/O protocol, the DM executes an operation only if it can determine that no older conflicting operation can arrive later. Each transaction receives a unique timestamp and each operation carries the timestamp assigned to the transaction of which it is a part.
Reference: [BeG81] <author> P. A. Bernstein and N. Goodman, </author> <title> Concurrency Control in Distributed Database Systems, </title> <journal> Computing Surveys 13,2 (June 1981), </journal> <pages> 185-221. </pages>
Reference-contexts: We say an execution equivalent to a literally correct serial execution is serializable. Serializability is the most widely used correctness criterion for concurrency control in databases <ref> [BeG81, Pap86] </ref>. Our definition of serializability is an adaptation to parallel programming of the term serializability as it is used in databases. The principal difference is in the greater emphasis on sequencing constraints. <p> Timestamp Ordering Techniques In a timestamp ordering (T/O) protocol, each transaction is assigned a timestamp and conflicting operations on each entity are required to be executed in timestamp order. In the basic T/O algorithm as described by Bernstein <ref> [BeG81] </ref>, two timestamps are associated with each entity. The read timestamp for an entity records the timestamp of the youngest read to access the entity, i.e., the read with the largest timestamp, and the write timestamp records the timestamp of the youngest write. <p> The problem posed by mixing isochron and lock-based techniques is similar to the problem that arises in databases when T/O protocols are combined with locking protocols <ref> [BeG81] </ref>. In both contexts, mixing techniques that use different criteria to resolve the order among atomic actions with conflicting operations requires care. 5.3. VERSION CONSISTENCY Version consistency requires that processes synchronize to ensure that their execution respects the read/write, write/write, and write/read data dependences implied by the program. <p> In other words, the presence of caches must not change the meaning of the program. This correctness criterion is analogous to the criterion typically used for replicated databases <ref> [BeG81] </ref>. This generalization of the correctness criteria for concurrency control from single-copy systems to multiple-copy systems is appropriate for the delta-cache protocols because the protocols are themselves generalizations of the isotach concurrency control techniques for systems without caches to systems with caches.
Reference: [BeG82] <author> P. A. Bernstein and N. Goodman, </author> <title> A Sophisticate's Introduction to Distributed Concurrency Control, </title> <booktitle> Proc. 8th Int. Conf. on Very Large Data Bases, </booktitle> <address> Mexico City, </address> <month> September </month> <year> 1982. </year>
Reference-contexts: If the transaction is certified, the write phase is executed. In the write phase the transaction's updates are written to the database. There are several types of OCC protocols depending on the type of test used in the certification phase <ref> [BeG82] </ref>. Most OCC protocols require execution be equivalent to a serial execution of the same transactions executed in the order in which they enter the certification phase.
Reference: [BeG83] <author> P. A. Bernstein and N. Goodman, </author> <title> Multiversion Concurrency Control, </title> <journal> ACM Trans. Database Systems 8,4 (December, </journal> <year> 1983), </year> <pages> 465-483. </pages>
Reference-contexts: The idea of using multiple versions to eliminate false dependences has appeared in many contexts, for example in parallel databases <ref> [BeG83, Pap86] </ref>, dataflow computation [Cha71], compilation of loops for parallel execution [Bar92, Uht87], and software pipelining [Lam88]. Most systems maintain only a single version of each shared variable and so can satisfy false dependences only by executing the operations in the specified order.
Reference: [BGS89] <author> Y. Birk, P. B. Gibbons, J. L. C. Sanz and D. Soroker, </author> <title> A Simple Mechanism for Efficient Barrier Synchronization in MIMD Machines, </title> <type> Tech. Rep. RJ 7078, </type> <institution> IBM, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Local synchrony was first proposed by Awerbuch [Awe85], who uses local synchrony (the a-synchronizer in Awerbuch's terms) to support execution of SIMD graph algorithms on asynchronous networks. Local synchrony has also been used by Gibbons to support barrier synchronization <ref> [BGS89, Gib89] </ref> and by Ranade to emulate a concurrent-read, concurrent-write (CRCW) PRAM [Ran87, RBJ88]. The networks proposed by these researchers can all be described as isotach networks.
Reference: [BiJ87] <author> K. P. Birman and T. A. Joseph, </author> <title> Reliable Communication in the Presence of Failures, </title> <journal> ACM Trans. </journal> <note> Computer Systems 5,1 (February, </note> <year> 1987), </year> <pages> 47-76. </pages>
Reference-contexts: Several protocols for the causal delivery of messages or groups of messages in systems that communicate via point-to-point links have been proposed, all requiring multiple message rounds 40 <ref> [BiJ87, BSS91, Gol89, PBS89, SES89] </ref>. Some of these protocols are discussed in Chapter 4. We know of only one point-to-point network that has been shown to ensure causal message delivery a type of tree-structured network called race-free networks [LHH91]. <p> These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations [GaS91, LHH91, Mon78, SiK80]. 59 A second approach is based on groups <ref> [BiJ87, BSS91, GaS91, PBS89] </ref>. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group. These protocols use the fact that every process sees every message to simplify the task of ordering multicasts. <p> The worst-case latency algorithm 60 appears to have all the properties of the isochron except combinability, but the imposition of worst-case latency on all messages makes this algorithm unsuitable for general use. 3) Highest bid algorithm <ref> [BiJ87, Das92, Gol89] </ref>. This algorithm is the basis for the first version of the ABCAST protocol in ISIS [BiJ87], where it is attributed to Dale Skeen. The algorithm has two phases. In the first, the source process sends the multicast. <p> This algorithm is the basis for the first version of the ABCAST protocol in ISIS <ref> [BiJ87] </ref>, where it is attributed to Dale Skeen. The algorithm has two phases. In the first, the source process sends the multicast. <p> Consistency of replicated data can also be maintained using a causal multicast in place of a totally ordered multicast [JoB86]. Another commonly proposed application for ordered multicasts in concurrency control is in acquiring multiple locks without risk of deadlock, see e.g., <ref> [BiJ87] </ref>. Instead of acquiring locks in a linear order, processes can issue all lock requests in totally ordered multicasts. The total ordering ensures that no cyclic dependence exists among lock requests.
Reference: [BSS91] <author> K. P. Birman, A. Schiper and P. Stephenson, </author> <title> Lightweight Causal and Atomic Group Multicast, </title> <journal> ACM TOCS, </journal> <month> August </month> <year> 1991, </year> <pages> 272-314. </pages>
Reference-contexts: Message delivery is causal if for any two messages, m and m, s (m) fi s (m) place (r (m)) = place (r (m)) fi m is received before m [SES89]. Causal delivery has been found to be useful in distributed programming applications <ref> [BSS91] </ref>. Causal message delivery is easily obtained in a system in which processes communicate via a broadcast medium because the events of sending and receiving a given message are logically simultaneous. <p> Several protocols for the causal delivery of messages or groups of messages in systems that communicate via point-to-point links have been proposed, all requiring multiple message rounds 40 <ref> [BiJ87, BSS91, Gol89, PBS89, SES89] </ref>. Some of these protocols are discussed in Chapter 4. We know of only one point-to-point network that has been shown to ensure causal message delivery a type of tree-structured network called race-free networks [LHH91]. <p> We base causality on the logical precedes relation among multicasts instead of on the fi relation more commonly used to define causal multicasts, e.g., <ref> [BSS91] </ref>, because we do not assume messages are sent at the time 53 P 2 I M2 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh the multicast is issued. <p> These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations [GaS91, LHH91, Mon78, SiK80]. 59 A second approach is based on groups <ref> [BiJ87, BSS91, GaS91, PBS89] </ref>. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group. These protocols use the fact that every process sees every message to simplify the task of ordering multicasts.
Reference: [BNR89] <author> R. Bisiani, A. Nowatzyk and M. Ravishankar, </author> <title> Coherent Shared Memory on a Distributed Memory Machine, </title> <booktitle> Int. Conf. on Parallel Processing 1(1989), </booktitle> <pages> 133-141. </pages>
Reference-contexts: Causal memory [AHJ90] allows a read to return any of a set of writes that logically precede the read with the result that concurrent reads can return different values. Weak ordering <ref> [AdH90, BNR89, DSB86] </ref> ensures sequential consistency only at synchronization points, and release consistency [Gha90] enforces a yet weaker form of sequential consistency around release and acquire synchronization operations such as operations on locks. Sequential consistency is most often discussed in relation to cache protocols.
Reference: [BiD86] <author> P. Bitar and A. M. Despain, </author> <title> Multiprocessor Cache Synchronization: Issues, Innovations, Evolution, </title> <booktitle> Proc. 13th International Symp. Computer Architecture, </booktitle> <year> 1986, </year> <pages> 424-433. </pages>
Reference-contexts: They assume that a process issuing an operation that is part of an atomic action has already acquired exclusive rights to the accessed variables though a separate mechanism. The few protocols that include support for the execution of atomic actions <ref> [BiD86, GVW89, LeR90] </ref> are based on locking. A third way in which the delta-cache protocols are highly concurrent is that they allow more pipe-lining than other directory protocols. In most cache protocols, processes cannot pipeline operations except by sacrificing sequential consistency.
Reference: [Bol67] <author> L. J. Boland, et al., </author> <title> The IBM System/360 Model 91: Storage System, </title> <journal> IBM Journal of Research and Development 11,1 (January, </journal> <year> 1967), </year> <pages> 54-68. </pages>
Reference-contexts: In parallel architecture, a two-step write is used in some designs to reduce delays in the instruction pipeline. The IBM 360/91 allowed a store to be issued before the value to be written had been determined <ref> [Bol67] </ref>. Store instructions awaiting data were held in a queue at the memory. Any read instruction with the same address as a store instruction in the queue was delayed until the store was executed. A more recent computer that uses this anticipatory two-step write is the Astronautics ZS-1 [Smi89].
Reference: [Bro86] <author> E. D. Brooks, III, </author> <title> The Butterfly Barrier, </title> <booktitle> International Journal of Parallel Processing 15,4 (1986), </booktitle> <pages> 295-307. </pages>
Reference-contexts: Other proposals for hardware support of version consistency include algorithms for improving the efficiency of barrier synchronization <ref> [Bro86, HFM88, Jay87] </ref>. 21 2.4. LOGICAL TIME CONCURRENCY CONTROL Concurrency control systems can be classified depending on whether they enforce the ordering constraints specified by the program in real time or in logical time. A real-time concurrency control system ensures that execution is literally correct.
Reference: [BuS85] <author> G. N. Buckley and A. Silberschatz, </author> <title> Beyond Two-Phase Locking, </title> <journal> J. ACM 32,2 (April 1985), </journal> <pages> 314-326. </pages>
Reference-contexts: The path protocol has been generalized to a tree-structured ordering [Mon78, SiK80, Sil82]. Several protocols have been designed to reduce the expected number of extra nodes a transaction must lock <ref> [AhB89, BuS85, Yan82] </ref>, but these protocols have other drawbacks: large messages and complex tests at each node [AhB89], reliance on static mode information about transaction access patterns [BuS85], or the potential for cascading rollback [Yan82]. <p> Several protocols have been designed to reduce the expected number of extra nodes a transaction must lock [AhB89, BuS85, Yan82], but these protocols have other drawbacks: large messages and complex tests at each node [AhB89], reliance on static mode information about transaction access patterns <ref> [BuS85] </ref>, or the potential for cascading rollback [Yan82]. In systems in which the network topology matches the hierarchical structure of the data, the tree protocols can be viewed as implementing ordered multicasts. Ordered multicasts and their use in concurrency control are discussed in Chapter 4. 2.4.2.
Reference: [CaD90] <author> M. Carlton and A. Despain, </author> <title> Multiple-Bus Shared-Memory System: Aquarius Project, </title> <journal> IEEE Computer 23,6 (June 1990), </journal> <pages> 80-83. </pages>
Reference-contexts: Since they rely on broadcasting, snoopy protocols scale poorly. Researchers are exploring ways to improve the scalability of snoopy protocols by using multiple buses arranged hierarchically [CGB89, HLH92, Wil87] or in a grid <ref> [CaD90, GoW88] </ref>. This approach is promising for programs with access patterns that allow most broadcasts to be restricted to a local cluster of PE's. Directory protocols represent a more general approach to the scalability problem. These protocols do not require broadcasting. <p> Some protocols are directory/snoopy hybrids, e.g., the limited directory protocols proposed by Agarwal, et al., [Aga88] and several protocols for multiple bus or 97 hybrid bus/MIN architectures, such as the protocol proposed by Algudady, et al., [ADT90] and the DASH [Len90], VMP-MC [CGB89], Aquarius <ref> [CaD90] </ref>, and Galactica Net [WiL92] protocols. Directory protocols are so named because they maintain a directory for each block listing the location of all copies of the block. The first directory protocol, due to Tang [Tan76], specified a single centralized directory.
Reference: [CeF78] <author> L. M. Censier and P. Feautrier, </author> <title> A New Solution to Coherence Problems in Multicache Systems, </title> <journal> IEEE Trans. on Computers, </journal> <month> December </month> <year> 1978, </year> <pages> 1112-1118. </pages>
Reference-contexts: The first directory protocol, due to Tang [Tan76], specified a single centralized directory. Censier and Feautrier improved on this protocol by distributing the directory so that the directory for each block is located in the MM containing the memory copy of that block <ref> [CeF78] </ref>. The directory for each block in Censier and Feautrier's protocol is a bit vector of length n, where n is the number of PE's and bit k of a block's vector is set if PE k holds a copy of that block. <p> Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols. For simplicity, we assume the bit vector representation proposed by Censier and Feautrier <ref> [CeF78] </ref>, but delta-cache protocols are compatible with many of the proposals for improving the scalability of the directory representation, e.g. with the linked-list directory representation proposed for the Alewife machine [CKA91] or with the similar, hardware-supported representation discussed by Simoni and Horowitz [SiH91]. <p> The traditional definition of memory coherence is that a memory is coherent if a load operation on a shared variable returns the value written by the latest store operation on the same variable <ref> [CeF78] </ref>. Dubois, Scheurich, and Briggs have noted difficulty applying the concept of latest store to systems that do not broadcast cache updates [DSB86]. They propose sequential consistency as an alternative correctness criterion and give restrictions on pipe-lining sufficient to enforce sequential consistency for several cache-based systems [ScD87].
Reference: [CKA91] <author> D. Chaiken, J. Kubiatowicz and A. Agarwal, </author> <title> LimitLeSS Directories: A Scalable Cache Coherence Scheme, </title> <booktitle> 4th ASPLOS, </booktitle> <month> April, </month> <year> 1991, </year> <pages> 224-234. </pages>
Reference-contexts: The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols. <p> For simplicity, we assume the bit vector representation proposed by Censier and Feautrier [CeF78], but delta-cache protocols are compatible with many of the proposals for improving the scalability of the directory representation, e.g. with the linked-list directory representation proposed for the Alewife machine <ref> [CKA91] </ref> or with the similar, hardware-supported representation discussed by Simoni and Horowitz [SiH91]. A third way in which cache protocols are classified is as write-update or write-invalidate protocols. A write-update protocol maintains coherence by keeping all copies of the same block up-to-date.
Reference: [Cha90] <author> D. Chaiken, et al., </author> <title> Directory-Based Cache Coherence in Large-Scale Multiprocessors, </title> <booktitle> Computer 23,6 (June 1990), </booktitle> <pages> 49-58. </pages>
Reference-contexts: Directory protocols follow a standard pattern, differing principally in their scheme for representing the direc 99 tory (see e.g., the survey in <ref> [Cha90] </ref>). In a typical directory protocol, cache blocks are held either in read-only status or in unrestricted status, a status that allows writing.
Reference: [Cha71] <author> D. D. Chamberlin, </author> <title> Approach to Parallel Processing"" The "Single-Assignment" Approach to Parallel Processing, </title> <booktitle> AFIPS Conf. Proc., Fall Joint Computer Conf. </booktitle> <pages> 39(1971). </pages>
Reference-contexts: The idea of using multiple versions to eliminate false dependences has appeared in many contexts, for example in parallel databases [BeG83, Pap86], dataflow computation <ref> [Cha71] </ref>, compilation of loops for parallel execution [Bar92, Uht87], and software pipelining [Lam88]. Most systems maintain only a single version of each shared variable and so can satisfy false dependences only by executing the operations in the specified order.
Reference: [ChM84] <author> J. Chang and N. F. Maxemchuk, </author> <title> Reliable Broadcast Protocols, </title> <journal> ACM Trans. Computer Systems 2,3 (August 1984), </journal> <pages> 251-273. </pages>
Reference-contexts: The same result can be achieved in systems with no physical serialization point in the network by funneling all multicasts through a single node <ref> [ChM84, KTH89, NCN88, Toi92] </ref>. Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root.
Reference: [Cha84] <author> J. Chang, </author> <title> Simplifying Distributed Database Systems Design by Using a Broadcast Network, </title> <booktitle> Proc. of the ACM SIGMOD, </booktitle> <address> Boston, Mass., </address> <month> June, </month> <year> 1984, </year> <pages> 223-233. </pages>
Reference-contexts: Using totally ordered multicasts to send updates to all the nodes with a copy of replicated data ensures all nodes receive the updates in the same order. This way of maintaining consistency of replicated data has been proposed for distributed databases <ref> [Cha84, PBS89] </ref> and virtual shared memory [BaT88], and is the principle on which snoopy cache coherence protocols work (See Chapter 6). Consistency of replicated data can also be maintained using a causal multicast in place of a totally ordered multicast [JoB86]. <p> Chang has proposed other ways of using ordered multicasts in concurrency 62 control: in checking the continued validity of read locks in a 2PL protocol with replicated data; in implementing a nonblocking commit; and in simplifying log-based recovery from crashes <ref> [Cha84] </ref>. 4.5. EXTENSIONS This section briefly explores ways to extend isochrons by relaxing assumptions made earlier in the chapter. Three extensions are discussed: complex operations; meta-isochrons; and nested isochrons. Each has applications in concurrency control. 4.5.1. <p> The idea of using an ordered multicast to obtain locks in a consistent order was proposed by Chang as an application to databases of a broadcast medium based ordered multicast <ref> [Cha84] </ref>. The atomic lock request technique is for static structured atomic actions. When a process can predeclare its access sets, it can avoid deadlock by using an isochron to request all its locks concurrently instead of by acquiring its locks sequentially in a linear order.
Reference: [CNL89] <author> S. T. Chanson, G. W. Neufeld and L. Liang, </author> <title> A Bibliography on Multicast and Group Communication, Operating Systems Review 23,4 (October, </title> <booktitle> 1989), </booktitle> <pages> 20-25. </pages>
Reference-contexts: Note that this definition allows a process to send different messages to different destinations in the same multicast and to send a multicast containing more than one message per destination. Many multicast protocols have been proposed <ref> [CNL89] </ref>. The next section describes those most closely related to the isochron. Most work on multicasts assumes MBM computations, so we define the isochron initially in MBM terms and then adapt the definition to the SMM. The isochron is a totally ordered, issue consistent multicast.
Reference: [ChV88] <author> H. Cheong and A. Veidenbaum, </author> <booktitle> Proc. 15th International Symp. Computer Architecture, </booktitle> <year> 1988. </year> <month> 184 </month>
Reference-contexts: RELATED WORK Delta-cache protocols are hardware, directory protocols. Hardware protocols manage caches dynamically without direction from the programmer. They require run-time communication to maintain memory coherence, but are less conservative than software protocols, e.g., <ref> [ChV88, MiB89] </ref>, protocols that use static analysis by the programmer or compiler to manage caches with little or no hardware support. In choosing to focus on hardware protocols, we do not dismiss the benefits of using static information in reducing the cost of cache coherence.
Reference: [CGB89] <author> D. R. Cheriton, H. A. Goosen and P. D. Boyle, </author> <title> Multi-Level Shared Caching Techniques for Scalability in VMP-MC, </title> <booktitle> Proc. 16th ISCA, </booktitle> <month> June </month> <year> 1989, </year> <pages> 16-24. </pages>
Reference-contexts: Since they rely on broadcasting, snoopy protocols scale poorly. Researchers are exploring ways to improve the scalability of snoopy protocols by using multiple buses arranged hierarchically <ref> [CGB89, HLH92, Wil87] </ref> or in a grid [CaD90, GoW88]. This approach is promising for programs with access patterns that allow most broadcasts to be restricted to a local cluster of PE's. Directory protocols represent a more general approach to the scalability problem. These protocols do not require broadcasting. <p> Some protocols are directory/snoopy hybrids, e.g., the limited directory protocols proposed by Agarwal, et al., [Aga88] and several protocols for multiple bus or 97 hybrid bus/MIN architectures, such as the protocol proposed by Algudady, et al., [ADT90] and the DASH [Len90], VMP-MC <ref> [CGB89] </ref>, Aquarius [CaD90], and Galactica Net [WiL92] protocols. Directory protocols are so named because they maintain a directory for each block listing the location of all copies of the block. The first directory protocol, due to Tang [Tan76], specified a single centralized directory.
Reference: [Cri89] <author> F. Cristian, </author> <title> Synchronous Atomic Broadcast for Redundant Broadcast Channels, </title> <type> Tech. Rep. RJ 7203 (67682), </type> <institution> IBM, </institution> <month> December, </month> <year> 1989. </year>
Reference-contexts: Although the algorithm has no bottleneck, its high message cost, due to the fact that each destination node must communicate with every other node before it can deliver a multicast message, limits the algorithm to small systems. 2) Worst-case latency algorithm <ref> [Cri89] </ref>. This algorithm was developed for fault-critical real time systems. We describe only the version that tolerates zero faults.
Reference: [Das92] <author> M. Dasser, TOMP: </author> <title> A Total Ordering Multicast Protocol, </title> <note> Operating Systems Review 26,1 (January 1992). </note>
Reference-contexts: The worst-case latency algorithm 60 appears to have all the properties of the isochron except combinability, but the imposition of worst-case latency on all messages makes this algorithm unsuitable for general use. 3) Highest bid algorithm <ref> [BiJ87, Das92, Gol89] </ref>. This algorithm is the basis for the first version of the ABCAST protocol in ISIS [BiJ87], where it is attributed to Dale Skeen. The algorithm has two phases. In the first, the source process sends the multicast. <p> A recent version of this algorithm reduces the average time a message marked deliverable spends on the delay queue by increasing the amount of information each participant in a multicast receives about the state of the delay queues of the other participants <ref> [Das92] </ref>. Although it is claimed to be only pair wise ordered, the multicast is actually totally ordered. As shown in Figure 4.7, pair-wise ordering is a weaker property than total ordering.
Reference: [DDS87] <author> D. Dolev, C. Dwork and L. Stockmeyer, </author> <title> On the Minimal Synchronization Needed for Distributed Consensus, </title> <journal> J. </journal> <note> ACM 34,1 (January 1987), 77-97. </note>
Reference-contexts: It has been shown that the existence of a wait-free, totally ordered multicast implies a solution to the n-process consensus problem that is tolerant of process fail-stop faults <ref> [DDS87, Gol89] </ref>. Since Her-lihy and others [Her88, Plo89] have shown that consensus is the fundamental problem in wait-free synchronization, the isochron could serve as a primitive for wait-free synchronization. We believe the iso-chron may be useful in developing practical wait-free algorithms. 4.4.
Reference: [DSB86] <author> M. Dubois, C. Scheurich and F. Briggs, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. 13th International Symp. Computer Architecture, </booktitle> <year> 1986, </year> <pages> 434-442. </pages>
Reference-contexts: Causal memory [AHJ90] allows a read to return any of a set of writes that logically precede the read with the result that concurrent reads can return different values. Weak ordering <ref> [AdH90, BNR89, DSB86] </ref> ensures sequential consistency only at synchronization points, and release consistency [Gha90] enforces a yet weaker form of sequential consistency around release and acquire synchronization operations such as operations on locks. Sequential consistency is most often discussed in relation to cache protocols. <p> Sequential consistency is most often discussed in relation to cache protocols. Maintaining multiple copies of shared variables complicates the problem of ensuring sequential consistency because it requires coordinating operations accessing different copies of the same variable. Sequential consistency has been proposed as the correctness criterion for cache protocols <ref> [DSB86] </ref>. We return to this topic in Chapter 6 in the discussion of isotach-based cache protocols. Version consistency. Version consistency means operations appear to be executed in the order specified by the program's version constraints. <p> Dubois, Scheurich, and Briggs have noted difficulty applying the concept of latest store to systems that do not broadcast cache updates <ref> [DSB86] </ref>. They propose sequential consistency as an alternative correctness criterion and give restrictions on pipe-lining sufficient to enforce sequential consistency for several cache-based systems [ScD87].
Reference: [DuS90] <author> M. Dubois and C. Scheurich, </author> <title> Memory Access Dependencies in Shared-Memory Multiprocessors, </title> <journal> IEEE Trans. on Software Eng. </journal> <month> 16,6 (June </month> <year> 1990), </year> <pages> 660-673. </pages>
Reference-contexts: Returning the read immediately can cause a violation of sequential consistency. If the forwarded write is delayed in the ICN, a second read to the same variable that logically succeeds the write may be executed before the write <ref> [DuS90, KRS88] </ref>. On an isotach network the read can be satisfied immediately because any operation on the same variable that logically succeeds the write will have a greater execution timestamp and so will be executed after the write. 7.2.
Reference: [EgK88] <author> S. Eggers and R. Katz, </author> <title> A Characterization of Sharing in Parallel Programs and Its Application to Coherency Protocol Evaluation, </title> <booktitle> Proc. 15th International Symp. Computer Architecture, </booktitle> <month> May </month> <year> 1988, </year> <pages> 373-382. </pages>
Reference-contexts: The relative merits of the write-update and write-invalidate policies depend on several factors: block size, cache size, the probability of an access being a write, and the extent to which accesses by different processes to the same block are interleaved <ref> [EgK88] </ref>. Write-update protocols can waste bandwidth by sending updates to processes that no longer need to access the updated block. On the other 98 hand, write-invalidate protocols may invalidate actively accessed copies of a block. Some protocols, e.g., read broadcast protocols [KMR86], are write-invalidate/write-update hybrids. <p> Eliminating the memory copy allows an optimization for the special case in which only the owner has a cache copy. 117 w r r H H hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Studies of parallel programs suggest this case occurs frequently in actual applications <ref> [BaR89, EgK88] </ref>. (This case may not, however, occur as frequently in isotach programs.
Reference: [EGL76] <author> K. P. Eswaran, J. N. Gray, R. A. Lorie and I. L. Traiger, </author> <title> The Notions of Consistency and Predicate Locks in a Database System, </title> <journal> Comm. ACM 19,11 (November 1976), </journal> <pages> 624-633. </pages>
Reference-contexts: A generalized atomic action containing operations issued by one or more processes has been proposed by Sha [Sha83]. The concept of the atomic action was developed in the context of database concurrency control by Eswaran where it corresponds to the concept of transaction <ref> [EGL76] </ref>. Early proposals for using the atomic action as a device for structuring parallel programs include those of Owicki and Gries [OwG76] and Lomet [Lom77].
Reference: [FaO89] <author> A. A. Farrag and M. T. Ozsu, </author> <title> Using Semantic Knowledge of Transactions to Increase Concurrency, </title> <journal> ACM Trans. Database Systems 14,4 (December 1989), </journal> <pages> 503-525. </pages>
Reference-contexts: There have been several proposals for using semantic information provided by the programmer either about the transactions <ref> [FaO89, Gar83] </ref> or about the objects accessed [BaR88, WeL85] to increase the concurrency of the scheduler. These protocols have significant disadvantages in that they are application dependent, require significant programmer effort to specify the semantics, and have more overhead than protocols using only syntactic information.
Reference: [Fid88] <author> C. Fidge, </author> <title> Timestamps in Message-Passing Systems that Preserve the Partial Ordering, </title> <booktitle> Proc. 11th Australian Computer Science Conference, </booktitle> <year> 1988, </year> <pages> 56-66. </pages>
Reference-contexts: In the resulting ordering a fi b fi t (a) &lt; t (b), but the converse, t (a) &lt; t (b) implies a fi b, is not true because a and b may be concurrent. The vector clock developed independently by Mattern [Mat88], Schmuck [Sch88], and Fidge <ref> [Fid88, Fid91] </ref> implements a logical time system in which a fi b iff t (a) &lt; t (b).
Reference: [Fid91] <author> C. Fidge, </author> <title> Logical Time in Distributed Computing Systems, </title> <booktitle> Computer, </booktitle> <month> August </month> <year> 1991, </year> <pages> 28-33. </pages>
Reference-contexts: In the resulting ordering a fi b fi t (a) &lt; t (b), but the converse, t (a) &lt; t (b) implies a fi b, is not true because a and b may be concurrent. The vector clock developed independently by Mattern [Mat88], Schmuck [Sch88], and Fidge <ref> [Fid88, Fid91] </ref> implements a logical time system in which a fi b iff t (a) &lt; t (b).
Reference: [Fra86] <author> N. Francez, </author> <title> in Fairness, </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Fairness is a concurrency control issue because a shared variable is a resource for which operations may compete. The concurrency control acts as a scheduler granting or denying access to this resource. Several different types of fairness can be distinguished <ref> [Fra86] </ref>.
Reference: [Fuj89] <author> R. M. Fujimoto, </author> <title> The Virtual Time Machine, </title> <booktitle> Proc. of the ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <address> Santa Fe, New Mexico, </address> <month> June 18-21, </month> <year> 1989, </year> <pages> 199-208. </pages>
Reference-contexts: The potential for rollback means each process must keep a history of all its internal states younger than the GVT, including messages sent and received. 27 Fujimoto <ref> [Fuj89] </ref> and Tinker [TiK88, Tin89] have proposed extending Time Warp techniques to other types of parallel computation. In Fujimoto's proposal, a SMM computation is decomposed into a number of units called tasks that can access shared variables and spawn new tasks.
Reference: [Gar83] <author> H. Garcia-Molina, </author> <title> Using Semantic Knowledge for Transaction Processing in a Distributed Database, </title> <journal> ACM Trans. </journal> <note> Database Systems 8,2 (June 1983). </note>
Reference-contexts: There have been several proposals for using semantic information provided by the programmer either about the transactions <ref> [FaO89, Gar83] </ref> or about the objects accessed [BaR88, WeL85] to increase the concurrency of the scheduler. These protocols have significant disadvantages in that they are application dependent, require significant programmer effort to specify the semantics, and have more overhead than protocols using only syntactic information.
Reference: [GaS91] <author> H. Garcia-Molina and A. Spauster, </author> <title> Ordered and Reliable Multicast Communication, </title> <journal> TOCS 9,3 (August 1991), </journal> <pages> 242-271. </pages>
Reference-contexts: Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root. These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations <ref> [GaS91, LHH91, Mon78, SiK80] </ref>. 59 A second approach is based on groups [BiJ87, BSS91, GaS91, PBS89]. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group. <p> These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations [GaS91, LHH91, Mon78, SiK80]. 59 A second approach is based on groups <ref> [BiJ87, BSS91, GaS91, PBS89] </ref>. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group. These protocols use the fact that every process sees every message to simplify the task of ordering multicasts.
Reference: [Gha90] <author> K. Gharachorloo, et al., </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proc. 17th International Symp. Computer Architecture, </booktitle> <year> 1990, </year> <pages> 15-25. </pages>
Reference-contexts: Causal memory [AHJ90] allows a read to return any of a set of writes that logically precede the read with the result that concurrent reads can return different values. Weak ordering [AdH90, BNR89, DSB86] ensures sequential consistency only at synchronization points, and release consistency <ref> [Gha90] </ref> enforces a yet weaker form of sequential consistency around release and acquire synchronization operations such as operations on locks. Sequential consistency is most often discussed in relation to cache protocols.
Reference: [Gib89] <author> P. B. Gibbons, </author> <title> The Asynchronous PRAM: A Semi-Synchronous Model for Shared Memory MIMD Machines, </title> <type> 89-062, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> December, </month> <year> 1989. </year>
Reference-contexts: Local synchrony was first proposed by Awerbuch [Awe85], who uses local synchrony (the a-synchronizer in Awerbuch's terms) to support execution of SIMD graph algorithms on asynchronous networks. Local synchrony has also been used by Gibbons to support barrier synchronization <ref> [BGS89, Gib89] </ref> and by Ranade to emulate a concurrent-read, concurrent-write (CRCW) PRAM [Ran87, RBJ88]. The networks proposed by these researchers can all be described as isotach networks.
Reference: [Gol89] <author> K. J. Goldman, </author> <title> Highly Concurrent Logically Synchronous Multicast, in Distributed Computing, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin-Heidelburg-New York, </address> <year> 1989, </year> <pages> 94-108. </pages>
Reference-contexts: Several protocols for the causal delivery of messages or groups of messages in systems that communicate via point-to-point links have been proposed, all requiring multiple message rounds 40 <ref> [BiJ87, BSS91, Gol89, PBS89, SES89] </ref>. Some of these protocols are discussed in Chapter 4. We know of only one point-to-point network that has been shown to ensure causal message delivery a type of tree-structured network called race-free networks [LHH91]. <p> It has been shown that the existence of a wait-free, totally ordered multicast implies a solution to the n-process consensus problem that is tolerant of process fail-stop faults <ref> [DDS87, Gol89] </ref>. Since Her-lihy and others [Her88, Plo89] have shown that consensus is the fundamental problem in wait-free synchronization, the isochron could serve as a primitive for wait-free synchronization. We believe the iso-chron may be useful in developing practical wait-free algorithms. 4.4. <p> The worst-case latency algorithm 60 appears to have all the properties of the isochron except combinability, but the imposition of worst-case latency on all messages makes this algorithm unsuitable for general use. 3) Highest bid algorithm <ref> [BiJ87, Das92, Gol89] </ref>. This algorithm is the basis for the first version of the ABCAST protocol in ISIS [BiJ87], where it is attributed to Dale Skeen. The algorithm has two phases. In the first, the source process sends the multicast. <p> The figure shows receive events for each of 3 multicasts with a dotted line connecting the receive events for each multicast. P 2 4 6 1 3 P P M hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 61 The multicast as described is not causal, but a variation proposed by Goldman <ref> [Gol89] </ref> appears to convert it to a causal multicast at the cost of an additional round of messages per multicast. Another modification of the highest-bid multicast makes it fault-tolerant. The version of the algorithm we described is for fault-free conditions.
Reference: [GoW88] <author> J. R. Goodman and P. Woest, </author> <title> The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor, </title> <booktitle> Proc. 15th International Conf. Computer Architecture, </booktitle> <year> 1988, </year> <pages> 422-431. </pages>
Reference-contexts: Since they rely on broadcasting, snoopy protocols scale poorly. Researchers are exploring ways to improve the scalability of snoopy protocols by using multiple buses arranged hierarchically [CGB89, HLH92, Wil87] or in a grid <ref> [CaD90, GoW88] </ref>. This approach is promising for programs with access patterns that allow most broadcasts to be restricted to a local cluster of PE's. Directory protocols represent a more general approach to the scalability problem. These protocols do not require broadcasting.
Reference: [GVW89] <author> J. R. Goodman, M. K. Vernon and P. J. Woest, </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors, </title> <booktitle> 3rd International Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April, </month> <year> 1989, </year> <pages> 84-75. </pages>
Reference-contexts: Before executing an atomic action that accesses more than one variable or that accesses a single variable more than one time, the process obtains locks on the accessed variable (s). Existing hardware support for atomicity is designed to make locking more efficient <ref> [And90, GVW89, Jay88, MeS90] </ref>. Hardware support for atomicity has also taken the form of adding simple ALU's to the memory modules (MM's) to permit atomic actions consisting of a single read-modify-write (RMW) operation to be implemented without locking [KRS88]. <p> They assume that a process issuing an operation that is part of an atomic action has already acquired exclusive rights to the accessed variables though a separate mechanism. The few protocols that include support for the execution of atomic actions <ref> [BiD86, GVW89, LeR90] </ref> are based on locking. A third way in which the delta-cache protocols are highly concurrent is that they allow more pipe-lining than other directory protocols. In most cache protocols, processes cannot pipeline operations except by sacrificing sequential consistency.
Reference: [Goo89] <author> J. R. Goodman, </author> <title> Cache Consistency and Sequential Consistency, </title> <type> 61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: The high cost of enforcing sequential consistency has led researchers to propose enforcing less strict forms of sequential consistency. Processor consistency <ref> [Goo89] </ref> requires that writes issued by the same process appear to be executed in the order specified by the program, but allows writes issued by different processes to be seen by different processes in an inconsistent order.
Reference: [GGK83] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph and M. Snir, </author> <title> The NYU Ultracomputer --- Designing an MIMD Shared Memory Parallel Computer, </title> <journal> IEEE 185 Transactions on Computers 32,2 (February 1983), </journal> <pages> 175-189. </pages>
Reference-contexts: Thus dest receives m before m. ` Combinability. Several machines in which the switches in the ICN have the ability to combine operations and fan-out responses to the resultant composite operations have been proposed or built including the NYU Ultracomputer <ref> [GGK83] </ref>, IBM's RP3 [Pfi85], and the Yale Fluent [RBJ88]. The pur pose of combining is to maintain good performance in the presence of multiple operations accessing the 57 1 4 I dest M b &gt;i i &gt;=i m hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh same shared variable. <p> Several recombining networks have been built or proposed, including the NYU Ultracomputer <ref> [GGK83] </ref>, IBM's RP3 [Pfi85], and the Yale Fluent [RBJ88]. Kruskal, Rudolph, and Snir have laid the foundation for reasoning about combining networks [KRS88] and Ranade has described an algorithm for implementing combining on a network of the type we call an isotach network [Ran87]. <p> Combining is recursive, i.e., an operation combined 136 at one stage may itself be the product of combining at a previous stage. We consider associative Read-Modify-Write (RMW) operations [KRS88], also known as a fetch and-f operations <ref> [GGK83] </ref>. An RMW operation indivisibly reads a shared variable, assigns it a new value that is a specified function of the old, and returns the old value. Reads and writes, as well as swaps, test-and-sets, and fetch-and-adds, are all special cases of the RMW and are all combinable.
Reference: [Gra79] <author> J. N. Gray, </author> <booktitle> Notes on Data Base Operating Systems, in Operating Systems: An Advanced Course, </booktitle> <volume> vol. 60 , R. </volume> <editor> Bayer, R. M. Graham and G. Seegmuller (editors), </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1979, </year> <pages> 393-481. </pages>
Reference-contexts: Split Operations. In database concurrency control, writes are routinely executed in two parts to ensure execution on an all-or-nothing basis, a tentative prewrite followed by a write to stable storage when the transaction commits <ref> [Gra79] </ref>. The operation from the database literature most closely related to split operations is the prewrite proposed by Reed [Ree78] and discussed in Chapter 2.
Reference: [GWM90] <author> A. Gupta, W. Weber and T. Mowry, </author> <title> Reducing Memory and Traffic Requirements for Scalable Directory-Based Cache Coherence Schemes, </title> <booktitle> Proc. 1990 ICPP, </booktitle> <month> August </month> <year> 1990, </year> <month> I-312-I-321. </month>
Reference-contexts: The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [HLH92] <author> E. Hagersten, A. Landin and S. Haridi, </author> <title> DDM A Cache-Only Memory Architecture, </title> <booktitle> Computer 25,9 (September 1992), </booktitle> <pages> 44-54. </pages>
Reference-contexts: Since they rely on broadcasting, snoopy protocols scale poorly. Researchers are exploring ways to improve the scalability of snoopy protocols by using multiple buses arranged hierarchically <ref> [CGB89, HLH92, Wil87] </ref> or in a grid [CaD90, GoW88]. This approach is promising for programs with access patterns that allow most broadcasts to be restricted to a local cluster of PE's. Directory protocols represent a more general approach to the scalability problem. These protocols do not require broadcasting.
Reference: [Hal85] <author> R. H. Halstead, Jr., </author> <title> Multilisp: A Language for Concurrent Symbolic Computation, </title> <journal> ACM Trans. Prog. Lang. and Systems 7,4 (October 1985), </journal> <pages> 501-538. </pages>
Reference-contexts: In the case of a blocking read the process issues no further operations until it receives the value of the accessed variable in response to the read. In the case of a non-blocking read, the process can continue to issue operations, perhaps treating the expected value as a future <ref> [Hal85] </ref>. Split operations can accommodate either assumption, but we will assume reads are non-blocking.
Reference: [HFM88] <author> D. Hensgen, R. Finkel and U. Manber, </author> <title> Two Algorithms for Barrier Synchronization, </title> <booktitle> International Journal of Parallel Programming 17,1 (1988), </booktitle> <pages> 1-17. </pages>
Reference-contexts: Other proposals for hardware support of version consistency include algorithms for improving the efficiency of barrier synchronization <ref> [Bro86, HFM88, Jay87] </ref>. 21 2.4. LOGICAL TIME CONCURRENCY CONTROL Concurrency control systems can be classified depending on whether they enforce the ordering constraints specified by the program in real time or in logical time. A real-time concurrency control system ensures that execution is literally correct.
Reference: [Her88] <author> M. P. Herlihy, </author> <title> Impossibility and Universality Results for Wait-Free Synchronization, </title> <booktitle> Proc. 7th Annual ACM Symp. on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: Wait-freedom. An action is wait- free if a process attempting to execute the action will succeed in a finite number of steps, regardless of the rate at which other processes make progress (see, e.g., <ref> [Her88] </ref>). Wait-freedom implies a measure of fault-tolerance since an action that can complete in the presense of arbitrarily slow processes will also tolerate stopped processes. <p> It has been shown that the existence of a wait-free, totally ordered multicast implies a solution to the n-process consensus problem that is tolerant of process fail-stop faults [DDS87, Gol89]. Since Her-lihy and others <ref> [Her88, Plo89] </ref> have shown that consensus is the fundamental problem in wait-free synchronization, the isochron could serve as a primitive for wait-free synchronization. We believe the iso-chron may be useful in developing practical wait-free algorithms. 4.4.
Reference: [HeW89] <author> M. P. Herlihy and J. M. Wing, </author> <title> Linearizable Concurrent Objects, </title> <journal> SIGPLAN Notices 24,4 (April 1989), </journal> <pages> 133-135. </pages>
Reference-contexts: These protocols have significant disadvantages in that they are application dependent, require significant programmer effort to specify the semantics, and have more overhead than protocols using only syntactic information. A second alternative to serializability as a correctness criterion is linearizability <ref> [HeW89] </ref>.
Reference: [HeV79] <author> D. Herman and J. P. Verjus, </author> <title> An Algorithm for Maintaining the Consistency of Multiple Copies, </title> <booktitle> Proc. 1st Int. Conf. on Distributed Computing Systems, </booktitle> <month> October </month> <year> 1979, </year> <pages> 625-631. </pages>
Reference-contexts: A transaction must still be aborted if one of its prewrites arrives at a DM after a younger read for the same entity. (3) Conservative T/O <ref> [BeG80, HeV79, KNT79, Mil79] </ref>. In a conservative T/O protocol, the DM executes an operation only if it can determine that no older conflicting operation can arrive later. Each transaction receives a unique timestamp and each operation carries the timestamp assigned to the transaction of which it is a part.
Reference: [Jam90] <author> D. V. James, et al., </author> <title> Scalable Coherent Interface, </title> <booktitle> Computer 23,6 (June 1990), </booktitle> <pages> 74-77. </pages>
Reference-contexts: The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [Jay87] <author> D. N. Jayasimha, </author> <title> Parallel Access to Synchronization Variables, </title> <booktitle> Int. Conf. on Parallel Processing, </booktitle> <year> 1987, </year> <pages> 97-99. </pages>
Reference-contexts: Other proposals for hardware support of version consistency include algorithms for improving the efficiency of barrier synchronization <ref> [Bro86, HFM88, Jay87] </ref>. 21 2.4. LOGICAL TIME CONCURRENCY CONTROL Concurrency control systems can be classified depending on whether they enforce the ordering constraints specified by the program in real time or in logical time. A real-time concurrency control system ensures that execution is literally correct.
Reference: [Jay88] <author> D. N. Jayasimha, </author> <title> Distributed Synchronizers, </title> <booktitle> Int. Conf. on Parallel Processing 1(1988), </booktitle> <pages> 23-27. </pages>
Reference-contexts: Before executing an atomic action that accesses more than one variable or that accesses a single variable more than one time, the process obtains locks on the accessed variable (s). Existing hardware support for atomicity is designed to make locking more efficient <ref> [And90, GVW89, Jay88, MeS90] </ref>. Hardware support for atomicity has also taken the form of adding simple ALU's to the memory modules (MM's) to permit atomic actions consisting of a single read-modify-write (RMW) operation to be implemented without locking [KRS88].
Reference: [Jef85] <author> D. R. Jefferson, </author> <title> Virtual Time, </title> <journal> ACM Trans. Prog. Lang. and Systems 7,3 (July 1985), </journal> <pages> 404-426. </pages>
Reference-contexts: Time Warp The logical time concurrency control techniques described so far were developed for databases. Time Warp, a logical time based protocol proposed by Jefferson <ref> [Jef85] </ref>, was developed for parallel simulations. In terms of database concurrency control, Time Warp is a multiversion T/O algorithm that is starvation-free but subject to cascading rollback.
Reference: [Jor83] <author> H. F. Jordan, </author> <title> Performance Measurements on HEP A Pipelined MIMD Computer, </title> <booktitle> Proc. of the 10th ISCA, </booktitle> <year> 1983, </year> <pages> 207-212. </pages>
Reference-contexts: Data dependences are typically enforced with semaphores or barriers. Hardware support includes synchronization flags or key fields for controlling access to the associated variable, e.g., the full/empty flags on the HEP <ref> [Jor83] </ref>, the empty bit associated with each element of the I-structure proposed by Arvind for dataflow computation [ANP89], and the synchronization key fields proposed by Zhu and Yew and by Peir [Pei83, ZhY84].
Reference: [JoB86] <author> T. Joseph and K. Birman, </author> <title> Low Cost Management of Replicated Data in Fault-Tolerant Distributed Systems, </title> <journal> ACM Trans. </journal> <note> Computer Systems 4,1 (February, </note> <year> 1986), </year> <pages> 54-70. </pages>
Reference-contexts: Consistency of replicated data can also be maintained using a causal multicast in place of a totally ordered multicast <ref> [JoB86] </ref>. Another commonly proposed application for ordered multicasts in concurrency control is in acquiring multiple locks without risk of deadlock, see e.g., [BiJ87]. Instead of acquiring locks in a linear order, processes can issue all lock requests in totally ordered multicasts.
Reference: [KTH89] <author> M. F. Kaashoek, A. S. Tanenbaum, S. F. Hummell and H. E. Bal, </author> <title> An Efficient Reliable Broadcast Protocol, </title> <booktitle> Operating Systems Review 23,4 (October 1989), </booktitle> <pages> 5-19. </pages>
Reference-contexts: The same result can be achieved in systems with no physical serialization point in the network by funneling all multicasts through a single node <ref> [ChM84, KTH89, NCN88, Toi92] </ref>. Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root.
Reference: [KNT79] <author> A. Kaneko, Y. Nishihara, K. Tsuruoka and M. Hattori, </author> <title> Logical Clock Synchronization Methods for Duplicated Database Control, </title> <booktitle> Proc. 1st Int. Conf. on Distributed Computer Systems, </booktitle> <month> October, </month> <year> 1979, </year> <pages> 601-611. </pages>
Reference-contexts: A transaction must still be aborted if one of its prewrites arrives at a DM after a younger read for the same entity. (3) Conservative T/O <ref> [BeG80, HeV79, KNT79, Mil79] </ref>. In a conservative T/O protocol, the DM executes an operation only if it can determine that no older conflicting operation can arrive later. Each transaction receives a unique timestamp and each operation carries the timestamp assigned to the transaction of which it is a part.
Reference: [KMR86] <author> A. Karlin, M. S. Manasse, L. Rudolph and D. D. Sleater, </author> <title> Competitive Snoopy Caching, </title> <booktitle> Proc. 27th Annual Symp. on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1986, </year> <pages> 244-254. </pages>
Reference-contexts: Write-update protocols can waste bandwidth by sending updates to processes that no longer need to access the updated block. On the other 98 hand, write-invalidate protocols may invalidate actively accessed copies of a block. Some protocols, e.g., read broadcast protocols <ref> [KMR86] </ref>, are write-invalidate/write-update hybrids. Because the concurrency of write-invalidate protocols is inherently limited, the delta-cache protocols use a write-update policy. A disadvantage of the update policy is the cost of distributing cache updates.
Reference: [KHM87] <author> M. J. Karol, M. G. Hluchyj and S. P. Morgan, </author> <title> Input Versus Output Queueing on a Space-Division Packet Switch, </title> <journal> IEEE Transactions on Communications 35,12 (December 1987), </journal> <pages> 347-356. </pages>
Reference-contexts: In the context of a conventional network, the z-switch is similar to switch designs with output buffers <ref> [KHM87] </ref> or internal buffers [KuJ84]. We assume switches in C1 and I1 have the same cycle time, i. e., that the time required in the absence of conflicts for a message to travel through a switch in C1 is the same as in I1.
Reference: [Knu73] <author> D. E. Knuth, </author> <title> in Fundamental Algorithms, vol. 3, Sorting and Searching , Addison-Wesley, </title> <booktitle> 1973, </booktitle> <pages> 397. </pages>
Reference-contexts: A warm-spot 173 traffic model has several warm variables instead of a single hot variable. The warm-spot traffic model is based on the 80/20 rule (see e.g. <ref> [Knu73] </ref>), a rule of thumb widely used in describing the pattern of accesses to a pool of shared objects. The 80/20 rule says that 80% of the accesses are to 20% of the variables.
Reference: [KRS88] <author> C. P. Kruskal, L. Rudolph and M. Snir, </author> <title> Efficient Synchronization on Multiprocessors with Shared Memory, </title> <journal> ACM Trans. Prog. Lang. and Systems 10,4 (October 1988), </journal> <pages> 579-601. </pages>
Reference-contexts: Hardware support for atomicity has also taken the form of adding simple ALU's to the memory modules (MM's) to permit atomic actions consisting of a single read-modify-write (RMW) operation to be implemented without locking <ref> [KRS88] </ref>. Atomi-city is typically specified in the program by operations on locks or by monitors or other high-level constructs implemented using locks. <p> Complex Operations The assumption that each operation is either a read or a write can be relaxed. An operation can, consistent with our assumption of FIFO indivisible execution, be any operation executed as an indivisible step by memory. These operations include read-modify-write (RMW) operations <ref> [KRS88] </ref> if implemented in memory. RMW operations extend the utility of isochrons by allowing an operation to modify the value of a variable based on its current value in a single indivisible operation. <p> Several recombining networks have been built or proposed, including the NYU Ultracomputer [GGK83], IBM's RP3 [Pfi85], and the Yale Fluent [RBJ88]. Kruskal, Rudolph, and Snir have laid the foundation for reasoning about combining networks <ref> [KRS88] </ref> and Ranade has described an algorithm for implementing combining on a network of the type we call an isotach network [Ran87]. The contribution of this chapter is to prove that combining is consistent with the semantics of isochrons. <p> Combining is recursive, i.e., an operation combined 136 at one stage may itself be the product of combining at a previous stage. We consider associative Read-Modify-Write (RMW) operations <ref> [KRS88] </ref>, also known as a fetch and-f operations [GGK83]. An RMW operation indivisibly reads a shared variable, assigns it a new value that is a specified function of the old, and returns the old value. <p> In general, any pair of operations that can be combined can be combined in either orientation <ref> [KRS88] </ref>. A read and a write can be combined, in that order, by forwarding a swap that assigns the value supplied by the write, satisfying the read with the value returned from memory in response to the swap. <p> Kruskal, Rudolph, and Snir prove this recombining network algorithm is correct by showing that the result of executing each composite operation OP c is equivalent to the result of a serial execution of the operations OP c represents <ref> [KRS88] </ref>. An original operation, i.e., an operation issued by a process, represents itself. A composite operation produced by combining OP i and OP j represents the operations that OP i and OP j represent. <p> An efficient combining technique is critical in emulating a CRCW PRAM and the FIFO's reduce the complexity of combining to a constant time. Ranade also shows that the network switches hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This statement of the result differs from that in original paper <ref> [KRS88] </ref> because we use f i f f j to denote f i ( f j (x )), whereas in the original paper it denotes f j ( f i (x )). 138 need to store less information than switches in a conventional recombining network. <p> Returning the read immediately can cause a violation of sequential consistency. If the forwarded write is delayed in the ICN, a second read to the same variable that logically succeeds the write may be executed before the write <ref> [DuS90, KRS88] </ref>. On an isotach network the read can be satisfied immediately because any operation on the same variable that logically succeeds the write will have a greater execution timestamp and so will be executed after the write. 7.2. <p> Requiring that switches combine operations in order by the pId's of the issuing PE's has been proposed by other researchers as a way to compute parallel prefixes within the ICN <ref> [KRS88, RBJ88] </ref>. <p> Later switches can further combine OP ij but cannot change the orientation of OP i and OP j within it. Since f j precedes f i in the composition of functions specified by OP i , OP i precedes OP j in the recursive expansion of OP i <ref> [KRS88] </ref>. Therefore execution of OP i is equivalent to a serial execution in which OP i is executed before OP j . ` THEOREM . Operations can be combined in an isotach network consistently with the semantics of isochrons. PROOF.
Reference: [Kuc78] <author> D. J. Kuck, </author> <title> in The Structure of Computers and Computations, vol. </title> <publisher> 1 , John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1978. </year> <month> 186 </month>
Reference-contexts: A version constraint specifies the execution order of conflicting operations accessing the same variable and establishes a data dependence between the operations. Data dependences are of three types: write/read, write/write, and read/write dependences, also known as flow, output, and anti-dependences <ref> [Kuc78] </ref>, respectively. A write/read dependence means the read must return the value written by the write. A read/write dependence means the read operation must logically precede the write, i.e., it must return the value of the variable before it is overwritten by the write.
Reference: [KuJ84] <author> M. Kumar and J. R. </author> <title> Jump, Performance Enhancement of Buffered Delta Networks Using Crossbar Switches and Multiple Links, </title> <journal> Journal of Parallel and Distributed Computing 1(1984), </journal> <pages> 81-103. </pages>
Reference-contexts: In the context of a conventional network, the z-switch is similar to switch designs with output buffers [KHM87] or internal buffers <ref> [KuJ84] </ref>. We assume switches in C1 and I1 have the same cycle time, i. e., that the time required in the absence of conflicts for a message to travel through a switch in C1 is the same as in I1.
Reference: [KuR81] <author> H. T. Kung and J. Robinson, </author> <title> On Optimistic Methods for Concurrency Control, </title> <journal> ACM Trans. Database Systems 6,2 (June 1981), </journal> <pages> 213-226. </pages>
Reference-contexts: Optimistic protocols Instead of testing each operation as it is submitted, an optimistic concurrency control (OCC) protocol tests the serializability of each transaction when it is ready to commit. The basic OCC protocol has three phases: read, certification, and write <ref> [KuR81] </ref>. In the read phase, all of the read operations are submitted and executed without restriction. Writes are executed only on local, private copies. In the certification phase, also known as the validation phase, the transaction as a whole is submitted to a serialization test.
Reference: [KuP83] <author> H. T. Kung and C. H. Papadimitriou, </author> <title> An Optimality Theory of Concurrency Control for Databases, </title> <journal> Acta Informatica 19(1983), </journal> <pages> 1-11. </pages>
Reference-contexts: Serializability is not a necessary condition for correct execution. Kung and Papidimitriou have shown that serializability is the optimally concurrent correctness criterion for schedulers implementing concurrency control in databases limited to syntactic knowledge, but that serializability ceases to be optimally concurrent if the scheduler has semantic knowledge <ref> [KuP83] </ref>. In other words, a scheduler that uses semantic knowledge can accommodate some schedules that are correct but that are unserializable and thus could not be recognized as correct by a scheduler with only syntactic knowledge.
Reference: [Lam88] <author> M. Lam, </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> Proc. of the SIGPLAN Notices '88 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1988, </year> <pages> 318-328. </pages>
Reference-contexts: The idea of using multiple versions to eliminate false dependences has appeared in many contexts, for example in parallel databases [BeG83, Pap86], dataflow computation [Cha71], compilation of loops for parallel execution [Bar92, Uht87], and software pipelining <ref> [Lam88] </ref>. Most systems maintain only a single version of each shared variable and so can satisfy false dependences only by executing the operations in the specified order. A common source of version constraints in SMM programs is in the parallel execution of loops.
Reference: [Lam78] <author> L. Lamport, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Comm. ACM 21,7 (July 1978), </journal> <pages> 558-565. </pages>
Reference-contexts: order of operations on different variables results in an equivalent execution only because we assume variables are disjoint. a) b) c) B C A 1 3 5 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 13 Another way to understand serializability is in terms of a system of logical time such as the system defined by Lamport <ref> [Lam78] </ref>, discussed below. A logical time system is a set of rules for assigning times to events of interest, here execution events, where the times assigned are consistent with causality, i.e., with the order in which events can be observed to occur. <p> The numbers labeling the operations in part b represent logical times and show that the execution in part b is serializable. In discussing a similar pair of diagrams, Lamport provides an alternative way to view the equivalence among the executions in the figure <ref> [Lam78] </ref>: Without introducing the concept of time into the system (which would require introducing physical clocks), there is no way to decide which of these pictures is the better representation. <p> Lamport's classic paper on logical time systems <ref> [Lam78] </ref> gives a distributed implementation of a logical time system in which the times assigned to events are consistent with the happened before relation, a relation over the events of sending and receiving messages that captures the notion of potential causality.
Reference: [Lam79] <author> L. Lamport, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocessor Programs, </title> <journal> IEEE Trans. on Computers 28(1979), </journal> <pages> 690-691. </pages>
Reference-contexts: Our definition of serializability is an adaptation to parallel programming of the term serializability as it is used in databases. The principal difference is in the greater emphasis on sequencing constraints. Our definition of serializability is strongly influenced by Lamport's concept of sequential consistency <ref> [Lam79] </ref> and by Shasha and Snir's definition of correct parallel executions [ShS88]. In databases, serial-izability serves to separate the problem of proving the correctness of database concurrency control protocols from that of proving the correctness of individual transactions [Pap86]. <p> During this lock acquisition phase, the variables controlled by already acquired locks are unavailable to other processes. Sequential consistency. An execution is sequentially consistent if the order in which operations are executed is consistent with the order specified by each individual process's sequential program <ref> [Lam79] </ref>. If the program specifies that operation A be executed before B, then B must not appear to be executed before A. Sequencing within the program of an individual process is typically indicated by separating instructions with a semicolon. <p> Lamport proposes a technique that eases the restriction on pipelining by permitting a process to issue an operation as soon as it is notified that its last operation is queued awaiting execution at the MM <ref> [Lam79] </ref>. On the IBM RP3, sequential consistency is not enforced unless the programmer explicitly requests it be enforced by issuing a special instruction called a fence instruction.
Reference: [LHH91] <author> A. Landin, E. Hagersten and S. Haridi, </author> <title> Race-Free Interconnection Networks and Multiprocessor Consistency, </title> <booktitle> 18th ISCA, </booktitle> <year> 1991, </year> <pages> 106-115. </pages>
Reference-contexts: Landin has shown that in a tree-structured network, an intermediate node on the path to the MM may be able to issue the acknowledgement, decreasing the delay before a process can issue its next operation <ref> [LHH91] </ref>. Stenstrom has proposed augmenting a multiprocessor in which n PE's are connected to n MM's by a multistage ICN 19 by adding n buses each connected to each MM where each bus is dedicated to carrying consistency information about one PE [Ste92]. <p> Some of these protocols are discussed in Chapter 4. We know of only one point-to-point network that has been shown to ensure causal message delivery a type of tree-structured network called race-free networks <ref> [LHH91] </ref>. We show that a class of isotach networks supports causal message delivery. Not all isotach networks support causal message delivery. <p> Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root. These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations <ref> [GaS91, LHH91, Mon78, SiK80] </ref>. 59 A second approach is based on groups [BiJ87, BSS91, GaS91, PBS89]. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group.
Reference: [LeR90] <author> J. Lee and U. Ramachandran, </author> <title> Synchronization with Multiprocessor Caches, </title> <booktitle> Proc. 17th International Symp. Computer Architecture, </booktitle> <year> 1990, </year> <pages> 27-37. </pages>
Reference-contexts: They assume that a process issuing an operation that is part of an atomic action has already acquired exclusive rights to the accessed variables though a separate mechanism. The few protocols that include support for the execution of atomic actions <ref> [BiD86, GVW89, LeR90] </ref> are based on locking. A third way in which the delta-cache protocols are highly concurrent is that they allow more pipe-lining than other directory protocols. In most cache protocols, processes cannot pipeline operations except by sacrificing sequential consistency.
Reference: [Len90] <author> D. Lenoski, et al., </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor, </title> <booktitle> Proc. 17th International Symp. Computer Architecture, </booktitle> <year> 1990, </year> <pages> 148-159. </pages>
Reference-contexts: Some protocols are directory/snoopy hybrids, e.g., the limited directory protocols proposed by Agarwal, et al., [Aga88] and several protocols for multiple bus or 97 hybrid bus/MIN architectures, such as the protocol proposed by Algudady, et al., [ADT90] and the DASH <ref> [Len90] </ref>, VMP-MC [CGB89], Aquarius [CaD90], and Galactica Net [WiL92] protocols. Directory protocols are so named because they maintain a directory for each block listing the location of all copies of the block. The first directory protocol, due to Tang [Tan76], specified a single centralized directory.
Reference: [Li89] <author> K. Li and P. Hudak, </author> <title> Memory Coherency in Shared Virtual Memory Systems, </title> <journal> ACM TOCS 7,4 (November 1989), </journal> <pages> 321-359. </pages>
Reference-contexts: among delta-cache protocols on a per block or per reference basis; describing on-time protocols; changing the migration algorithm so that it migrates the hot copy adaptively; exploring the applicability of the protocols and migration algorithm to the related problem of implementing a virtual shared memory on a distributed memory machine <ref> [Li89] </ref>; and designing protocols for other network topologies. 134 CHAPTER 7 COMBINING A recombining network is a network in which the switches combine operations on the same variable and then fan-out responses to the processes that issued the original operations.
Reference: [LiY90] <author> D. J. Lilja and P. Yew, </author> <title> A Compiler-Assisted Directory-Based Cache Coherence Scheme, </title> <type> CSRD 990, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: The solutions that have been proposed to this problem, in particular the proposals for recording the information in a cache associated with each MM <ref> [LiY90, OKN90] </ref>, suggest alternative ways to implement access sequences. We leave the exploration of these alternatives for future work. 5.1.3. Related Work Data structures similar to the access sequence and operations similar to the split operation have appeared previously in proposals for managing access to shared data. Access Sequence. <p> The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [LiY91] <author> D. J. Lilja and P. Yew, </author> <title> Combining Hardware and Software Cache Coherence Strategies, </title> <booktitle> Proc. 1991 Conf. on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In choosing to focus on hardware protocols, we do not dismiss the benefits of using static information in reducing the cost of cache coherence. Several software/hardware hybrid protocols that use static information about variable access patterns to improve hardware protocols have been proposed <ref> [BMR89, BCZ90, LiY91] </ref>, and we see a role for such static information in delta-cache protocols. Cache protocols are also classified as snoopy or directory protocols.
Reference: [LiS83] <author> B. Liskov and R. Scheifler, </author> <title> Guardians and Actions: Linguistic Support for Robust, Distributed Programs, </title> <journal> ACM Trans. Prog. Lang. and Systems 5,3 (July 1983), </journal> <pages> 381-404. </pages>
Reference-contexts: Most parallel computers support indivisible reads and writes at the word level. The atomic action allows the programmer to define a virtual machine supporting indivisible execution at any level of granularity. In databases, as well as in some message based model (MBM) programming languages designed for geographically distributed systems <ref> [LiS83, WeL85] </ref>, the atomic action is also a unit of recovery, i.e., the instructions in an atomic 17 action are executed on an all-or-nothing basis, even in the presence of hardware failures. As stated previously, we assume a failure-free system.
Reference: [Lom77] <author> D. B. Lomet, </author> <title> Process Structuring, Synchronization, and Recovery Using Atomic Actions, </title> <booktitle> Proc. Conf. on Language Design for Reliable Software, SIGPLAN Notices Notices 12,3 (March 1977), </booktitle> <pages> 128-137. </pages>
Reference-contexts: The concept of the atomic action was developed in the context of database concurrency control by Eswaran where it corresponds to the concept of transaction [EGL76]. Early proposals for using the atomic action as a device for structuring parallel programs include those of Owicki and Gries [OwG76] and Lomet <ref> [Lom77] </ref>. The atomic action is a group of one or more instructions issued by the same process that appears to be executed indivisibly, without interleaving with other instructions. Most parallel computers support indivisible reads and writes at the word level.
Reference: [Mat88] <author> F. Mattern, </author> <title> Virtual Time and Global States of Distributed Systems, </title> <booktitle> Parallel and Distributed Algorithms, </booktitle> <year> 1988, </year> <pages> 215-226. </pages>
Reference-contexts: In the resulting ordering a fi b fi t (a) &lt; t (b), but the converse, t (a) &lt; t (b) implies a fi b, is not true because a and b may be concurrent. The vector clock developed independently by Mattern <ref> [Mat88] </ref>, Schmuck [Sch88], and Fidge [Fid88, Fid91] implements a logical time system in which a fi b iff t (a) &lt; t (b).
Reference: [McL81] <author> G. McLean, </author> <title> Comments on SDD-1 Concurrency Control Mechanism, </title> <journal> ACM Trans. Database Systems 6,2 (1981), </journal> <pages> 347-350. </pages>
Reference-contexts: Unless a conservative T/0 protocol operates in declaration mode, care must be taken to avoid deadlock <ref> [McL81] </ref>. The basic isotach-based concurrency control technique for enforcing atomicity (the time slice technique described in Chapter 5) is analogous to a multiversion, conservative timestamp ordering protocol operating in declaration mode. It is free of deadlock, no transaction is ever aborted, but each transaction must predeclare its access set.
Reference: [MMA90] <author> P. M. Melliar-Smith, L. E. Moser and V. Agrawala, </author> <title> Broadcast Protocols for Distributed Systems, </title> <journal> IEEE Trans. </journal> <note> Parallel and Distributed Systems 1,1 (January 1990). </note>
Reference-contexts: These algorithms scale poorly. For example, a bus-based multicast can use the bus-induced serialization of messages to enforce a total order over multi-casts, e.g. <ref> [MMA90] </ref>, but bus saturation limits concurrency to a low level. The same result can be achieved in systems with no physical serialization point in the network by funneling all multicasts through a single node [ChM84, KTH89, NCN88, Toi92].
Reference: [MeS90] <author> J. M. Mellor-Crummey and M. L. Scott, </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <institution> TR342, Computer Science Department, University of Rochester, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Before executing an atomic action that accesses more than one variable or that accesses a single variable more than one time, the process obtains locks on the accessed variable (s). Existing hardware support for atomicity is designed to make locking more efficient <ref> [And90, GVW89, Jay88, MeS90] </ref>. Hardware support for atomicity has also taken the form of adding simple ALU's to the memory modules (MM's) to permit atomic actions consisting of a single read-modify-write (RMW) operation to be implemented without locking [KRS88].
Reference: [Mil79] <author> M. Milenkovic, </author> <title> Update Synchronization in Multiaccess Systems, </title> <type> Ph.D. Thesis, </type> <institution> University of Massachusetts, Amherst, </institution> <month> May </month> <year> 1979. </year>
Reference-contexts: A transaction that attempts to write an entity with a younger read timestamp must still be aborted. A difficulty with this solution is in determining when the space occupied by old versions can be safely reused [Ree83, Wei87]. (2) Operation in predeclaration mode <ref> [Mil79, Ree78] </ref>. Reed has proposed that each transaction in a multiversion T/O protocol issue its prewrites as soon as possible, even before the value to be written is determined, in order to diminish the risk that the prewrite will arrive too late, after a read with a younger timestamp. <p> A transaction must still be aborted if one of its prewrites arrives at a DM after a younger read for the same entity. (3) Conservative T/O <ref> [BeG80, HeV79, KNT79, Mil79] </ref>. In a conservative T/O protocol, the DM executes an operation only if it can determine that no older conflicting operation can arrive later. Each transaction receives a unique timestamp and each operation carries the timestamp assigned to the transaction of which it is a part.
Reference: [MiB89] <author> S. L. Min and J. Baer, </author> <title> A Timestamp-based Cache Coherence Scheme, </title> <booktitle> Int. Conf. on Parallel Processing 1(1989), </booktitle> <pages> 23-32. </pages>
Reference-contexts: RELATED WORK Delta-cache protocols are hardware, directory protocols. Hardware protocols manage caches dynamically without direction from the programmer. They require run-time communication to maintain memory coherence, but are less conservative than software protocols, e.g., <ref> [ChV88, MiB89] </ref>, protocols that use static analysis by the programmer or compiler to manage caches with little or no hardware support. In choosing to focus on hardware protocols, we do not dismiss the benefits of using static information in reducing the cost of cache coherence.
Reference: [Mon78] <author> W. A. Montgomery, </author> <title> Robust Concurrency Control for a Distributed Information System, </title> <type> Ph.D. Thesis, </type> <institution> Lab for Computer Science, M.I.T., </institution> <address> Cambridge, Mass., </address> <month> December, </month> <year> 1978. </year> <month> 187 </month>
Reference-contexts: A significant drawback of this protocol is that a process must obtain locks on variables that it does not need to access if its access set is not a consecutive range of variables according to the predetermined ordering. The path protocol has been generalized to a tree-structured ordering <ref> [Mon78, SiK80, Sil82] </ref>. <p> Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root. These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations <ref> [GaS91, LHH91, Mon78, SiK80] </ref>. 59 A second approach is based on groups [BiJ87, BSS91, GaS91, PBS89]. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group.
Reference: [Mos85] <author> J. E. B. Moss, </author> <title> Nested Transactions: An Approach to Reliable Distributed Copmputing, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <month> March </month> <year> 1985. </year>
Reference-contexts: One potential application for nested isochrons is as a protocol for nested transactions <ref> [BBG89, Mos85] </ref>. In a nested transaction, a message may represent either a primitive operation or trigger the issuance of a sub-transaction. The need for a nested transaction protocol arises in distributed databases and in parallel programs, such as object-oriented programs, in which information-hiding is a goal.
Reference: [NCN88] <author> S. Navaratnam, S. Chanson and G. Neufeld, </author> <title> Reliable Group Communication in Distributed Systems, </title> <booktitle> Proc. 8th International Conf. on Distributed Computing Systems, </booktitle> <address> San Jose, Calif, </address> <month> June </month> <year> 1988, </year> <pages> 439-446. </pages>
Reference-contexts: The same result can be achieved in systems with no physical serialization point in the network by funneling all multicasts through a single node <ref> [ChM84, KTH89, NCN88, Toi92] </ref>. Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root.
Reference: [OKN90] <author> B. W. OKrafka and A. R. </author> <title> Newton, An Empirical Evaluation of Two Memory-Efficient Methods, </title> <booktitle> Proc. 17th International Symp. Computer Architecture, </booktitle> <year> 1990, </year> <pages> 138-147. </pages>
Reference-contexts: The solutions that have been proposed to this problem, in particular the proposals for recording the information in a cache associated with each MM <ref> [LiY90, OKN90] </ref>, suggest alternative ways to implement access sequences. We leave the exploration of these alternatives for future work. 5.1.3. Related Work Data structures similar to the access sequence and operations similar to the split operation have appeared previously in proposals for managing access to shared data. Access Sequence. <p> The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [OwG76] <author> S. Owicki and D. Gries, </author> <title> An Axiomatic Proof Technique for Parallel Programs I, </title> <journal> Acta Informatica 6(1976), </journal> <pages> 319-340. </pages>
Reference-contexts: The concept of the atomic action was developed in the context of database concurrency control by Eswaran where it corresponds to the concept of transaction [EGL76]. Early proposals for using the atomic action as a device for structuring parallel programs include those of Owicki and Gries <ref> [OwG76] </ref> and Lomet [Lom77]. The atomic action is a group of one or more instructions issued by the same process that appears to be executed indivisibly, without interleaving with other instructions. Most parallel computers support indivisible reads and writes at the word level.
Reference: [Pap86] <author> C. Papadimitriou, </author> <title> Database Concurrency Control, </title> <publisher> Computer Science Press, </publisher> <year> 1986. </year>
Reference-contexts: We say an execution equivalent to a literally correct serial execution is serializable. Serializability is the most widely used correctness criterion for concurrency control in databases <ref> [BeG81, Pap86] </ref>. Our definition of serializability is an adaptation to parallel programming of the term serializability as it is used in databases. The principal difference is in the greater emphasis on sequencing constraints. <p> In databases, serial-izability serves to separate the problem of proving the correctness of database concurrency control protocols from that of proving the correctness of individual transactions <ref> [Pap86] </ref>. The use of serializability as the correctness criterion in parallel programming provides a similar benefit by separating the problem of proving the correctness of a concurrency control system from the formidable problem of verifying that a parallel program conforms to its specification. <p> Operations conflict if they access the same shared variable and are not both reads. This type of equivalence is called conflict equivalence in the database literature <ref> [Pap86] </ref>. Note that we require that variables be disjoint. No part of any variable can coincide with that of any other variable. <p> The idea of using multiple versions to eliminate false dependences has appeared in many contexts, for example in parallel databases <ref> [BeG83, Pap86] </ref>, dataflow computation [Cha71], compilation of loops for parallel execution [Bar92, Uht87], and software pipelining [Lam88]. Most systems maintain only a single version of each shared variable and so can satisfy false dependences only by executing the operations in the specified order. <p> A mechanism for controlling concurrency in parallel programs must be simple and fast to avoid unacceptable increases in the memory access time. (3) Knowledge available. A database protocol can operate in either of three different information modes: dynamic, declaration, or static <ref> [Pap86] </ref>. In the dynamic mode, the schedulers discover the access set of each transaction, i.e., the set of variables accessed by the transaction, dynamically as operations are issued. <p> A non-2PL protocol constrains the order in which a transaction may acquire its locks and make its accesses, but allows the transaction to release 23 some locks before it obtains others. The simplest such protocol is the path protocol (see e.g., <ref> [Pap86] </ref>), a protocol that imposes a linear ordering on the shared variables and requires that each process obtain locks, in order, on all variables from the first (in the linear order) variable in its access set to the last. <p> The correctness of the time slice technique can be demonstrated more formally using the approach used in databases to prove the serializability of multiversion protocols <ref> [Pap86] </ref>. A multiversion database protocol 83 is proven correct by showing that each execution is equivalent to some single version serial execution of the same transactions in which operations from each transaction are executed without interleaving with other operations. A similar argument shows that the time slice technique is correct.
Reference: [Pei83] <author> J. Peir, </author> <title> An Efficient Synchronization Method for Multiprocessor Systems, Cedar Document 27, </title> <institution> Lab. for Advanced Supercomputers,Univ. of Ill. at Urb.-Champ., </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: synchronization flags or key fields for controlling access to the associated variable, e.g., the full/empty flags on the HEP [Jor83], the empty bit associated with each element of the I-structure proposed by Arvind for dataflow computation [ANP89], and the synchronization key fields proposed by Zhu and Yew and by Peir <ref> [Pei83, ZhY84] </ref>. Other proposals for hardware support of version consistency include algorithms for improving the efficiency of barrier synchronization [Bro86, HFM88, Jay87]. 21 2.4.
Reference: [PBS89] <author> L. L. Peterson, N. C. Bucholz and R. D. Schlichting, </author> <title> Preserving and Using Context Information in Interprocess Communication, </title> <journal> ACM Trans. Computer Systems 7,3 (August 1989), </journal> <pages> 217-246. </pages>
Reference-contexts: Several protocols for the causal delivery of messages or groups of messages in systems that communicate via point-to-point links have been proposed, all requiring multiple message rounds 40 <ref> [BiJ87, BSS91, Gol89, PBS89, SES89] </ref>. Some of these protocols are discussed in Chapter 4. We know of only one point-to-point network that has been shown to ensure causal message delivery a type of tree-structured network called race-free networks [LHH91]. <p> These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations [GaS91, LHH91, Mon78, SiK80]. 59 A second approach is based on groups <ref> [BiJ87, BSS91, GaS91, PBS89] </ref>. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group. These protocols use the fact that every process sees every message to simplify the task of ordering multicasts. <p> Using totally ordered multicasts to send updates to all the nodes with a copy of replicated data ensures all nodes receive the updates in the same order. This way of maintaining consistency of replicated data has been proposed for distributed databases <ref> [Cha84, PBS89] </ref> and virtual shared memory [BaT88], and is the principle on which snoopy cache coherence protocols work (See Chapter 6). Consistency of replicated data can also be maintained using a causal multicast in place of a totally ordered multicast [JoB86].
Reference: [PfN85] <author> G. F. Pfister and V. A. Norton, </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks, </title> <journal> IEEE Transactions on Computers 34,10 (October, </journal> <year> 1985), </year> <pages> 943-948. </pages>
Reference-contexts: Recombining networks also reduce network load and may reduce the incidence of hot-spots <ref> [PfN85] </ref>. With one exception (discussed below), the applicability and thus the potential benefit of existing proposals for combining is restricted to singletons, operations that are part of only trivial atomic actions. The restriction is implicit. <p> The request rate at which C2 is saturated is the highest, 0.65; and I1 the lowest, 0.25. C1 and I2 are saturated at about the same request rate, 0.45 and 0.43, respectively. Hot-Spot Traffic Graphs A-hot_spot give performance results for the hot-spot traffic model. The hot-spot traffic model <ref> [PfN85] </ref> is a simple variation on the uniform traffic model in which one variable, the hot-spot, is accessed more often than the other variables. The hot-spot is chosen randomly such that for each trial, each global variable may be the hot-spot with equal probability.
Reference: [Pfi85] <author> G. F. Pfister, et al., </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture, </title> <booktitle> Int. Conf. on Parallel Processing, </booktitle> <year> 1985, </year> <pages> 764-771. </pages>
Reference-contexts: Thus dest receives m before m. ` Combinability. Several machines in which the switches in the ICN have the ability to combine operations and fan-out responses to the resultant composite operations have been proposed or built including the NYU Ultracomputer [GGK83], IBM's RP3 <ref> [Pfi85] </ref>, and the Yale Fluent [RBJ88]. The pur pose of combining is to maintain good performance in the presence of multiple operations accessing the 57 1 4 I dest M b &gt;i i &gt;=i m hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh same shared variable. <p> Several recombining networks have been built or proposed, including the NYU Ultracomputer [GGK83], IBM's RP3 <ref> [Pfi85] </ref>, and the Yale Fluent [RBJ88]. Kruskal, Rudolph, and Snir have laid the foundation for reasoning about combining networks [KRS88] and Ranade has described an algorithm for implementing combining on a network of the type we call an isotach network [Ran87].
Reference: [Plo89] <author> S. A. Plotkin, </author> <title> Sticky Bits and Universality of Consensus, </title> <institution> STAN-CS-89-1280, Stanford University, Dept. of Computer Science, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: It has been shown that the existence of a wait-free, totally ordered multicast implies a solution to the n-process consensus problem that is tolerant of process fail-stop faults [DDS87, Gol89]. Since Her-lihy and others <ref> [Her88, Plo89] </ref> have shown that consensus is the fundamental problem in wait-free synchronization, the isochron could serve as a primitive for wait-free synchronization. We believe the iso-chron may be useful in developing practical wait-free algorithms. 4.4.
Reference: [Ran87] <author> A. G. Ranade, </author> <title> How to Emulate Shared Memory, </title> <booktitle> IEEE Annual Symp. on Foundations of Computer Science, </booktitle> <address> Los Angeles, </address> <year> 1987, </year> <pages> 185-194. </pages>
Reference-contexts: Local synchrony has also been used by Gibbons to support barrier synchronization [BGS89, Gib89] and by Ranade to emulate a concurrent-read, concurrent-write (CRCW) PRAM <ref> [Ran87, RBJ88] </ref>. The networks proposed by these researchers can all be described as isotach networks. Ranade's is most closely related to the isotach networks described here because his network can be viewed as implementing an n-tuple isotach logical time system where n is greater than one. <p> Kruskal, Rudolph, and Snir have laid the foundation for reasoning about combining networks [KRS88] and Ranade has described an algorithm for implementing combining on a network of the type we call an isotach network <ref> [Ran87] </ref>. The contribution of this chapter is to prove that combining is consistent with the semantics of isochrons. We show that when operations from different atomic actions are combined within an isotach recombining network the resulting execution is sequentially consistent and atomic on the isochron level. <p> We use this result below in proving the algorithm for combining isochrons. Ranade <ref> [Ran87] </ref> shows that associative lookup queues located at the switches can be replaced by simple FIFO's given a network of the type we call an isotach network. <p> Each switch in I1 153 and I2 routes messages within the same pulse in order by route-tag. Isotach networks carry two types of messages in addition to operations: tokens and ghosts. As described in Chapter 3, tokens are control signals that divide pulses. A ghost <ref> [Ran87] </ref> is a copy of an operation with a bit set to indicate it is not a real operation. In other respects, ghosts look like operations. In particular, ghosts have route-tags.
Reference: [RBJ88] <author> A. G. Ranade, S. N. Bhatt and S. L. Johnsson, </author> <title> The Fluent Abstract Machine, </title> <type> Tech. Rep. 573, </type> <institution> Yale University, Dept. of Computer Science, </institution> <month> January, </month> <year> 1988. </year>
Reference-contexts: Local synchrony has also been used by Gibbons to support barrier synchronization [BGS89, Gib89] and by Ranade to emulate a concurrent-read, concurrent-write (CRCW) PRAM <ref> [Ran87, RBJ88] </ref>. The networks proposed by these researchers can all be described as isotach networks. Ranade's is most closely related to the isotach networks described here because his network can be viewed as implementing an n-tuple isotach logical time system where n is greater than one. <p> Thus dest receives m before m. ` Combinability. Several machines in which the switches in the ICN have the ability to combine operations and fan-out responses to the resultant composite operations have been proposed or built including the NYU Ultracomputer [GGK83], IBM's RP3 [Pfi85], and the Yale Fluent <ref> [RBJ88] </ref>. The pur pose of combining is to maintain good performance in the presence of multiple operations accessing the 57 1 4 I dest M b &gt;i i &gt;=i m hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh same shared variable. <p> Several recombining networks have been built or proposed, including the NYU Ultracomputer [GGK83], IBM's RP3 [Pfi85], and the Yale Fluent <ref> [RBJ88] </ref>. Kruskal, Rudolph, and Snir have laid the foundation for reasoning about combining networks [KRS88] and Ranade has described an algorithm for implementing combining on a network of the type we call an isotach network [Ran87]. <p> Requiring that switches combine operations in order by the pId's of the issuing PE's has been proposed by other researchers as a way to compute parallel prefixes within the ICN <ref> [KRS88, RBJ88] </ref>.
Reference: [Ree78] <author> D. Reed, </author> <title> Naming and Synchronization in a Decentralized Computer System, MIT/LCS/Tech. </title> <type> Rep. 205, </type> <institution> Laboratory for Computer Science, MIT, </institution> <month> September </month> <year> 1978. </year>
Reference-contexts: If a read is submitted that is younger than the prewrite then the read must also be buffered. Some techniques proposed to reduce the risk of abortion and starvation in T/O protocols are as follows: (1) Multiple versions <ref> [Ree78, Ree83] </ref>. If each DM retains old versions of each entity, tardy reads, i.e., reads with a timestamp less than the write timestamp of the entity accessed, can be accommodated using old versions. A transaction that attempts to write an entity with a younger read timestamp must still be aborted. <p> A transaction that attempts to write an entity with a younger read timestamp must still be aborted. A difficulty with this solution is in determining when the space occupied by old versions can be safely reused [Ree83, Wei87]. (2) Operation in predeclaration mode <ref> [Mil79, Ree78] </ref>. Reed has proposed that each transaction in a multiversion T/O protocol issue its prewrites as soon as possible, even before the value to be written is determined, in order to diminish the risk that the prewrite will arrive too late, after a read with a younger timestamp. <p> The operation from the database literature most closely related to split operations is the prewrite proposed by Reed <ref> [Ree78] </ref> and discussed in Chapter 2. The principal difference between the prewrite and the SCHED is that a prewrite may arrive after a read that logically succeeds it, causing the atomic action of which it is a part to be aborted.
Reference: [Ree83] <author> D. Reed, </author> <title> Implementing Atomic Actions on Decentralized Data, </title> <journal> ACM Trans. </journal> <note> Computer Systems 1,1 (February, </note> <year> 1983), </year> <pages> 3-23. </pages>
Reference-contexts: If a read is submitted that is younger than the prewrite then the read must also be buffered. Some techniques proposed to reduce the risk of abortion and starvation in T/O protocols are as follows: (1) Multiple versions <ref> [Ree78, Ree83] </ref>. If each DM retains old versions of each entity, tardy reads, i.e., reads with a timestamp less than the write timestamp of the entity accessed, can be accommodated using old versions. A transaction that attempts to write an entity with a younger read timestamp must still be aborted. <p> A transaction that attempts to write an entity with a younger read timestamp must still be aborted. A difficulty with this solution is in determining when the space occupied by old versions can be safely reused <ref> [Ree83, Wei87] </ref>. (2) Operation in predeclaration mode [Mil79, Ree78].
Reference: [ReF87] <author> D. A. Reed and R. M. Fujimoto, </author> <title> Multicomputer Networks; Message-Based Parallel Processing, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Each network is composed of 2x2 switches interconnected in the same baseline network topology. A diagram of a multistage interconnection network with this topology was shown in Figure 7.1. The message transmission protocol is store-and-forward using a send-acknowledge protocol <ref> [ReF87] </ref>. To allow the use of time-stepped simulation, we assume the networks are clocked, i. e., switches begin each cycle simultaneously. We believe the results are also applicable to self-timed networks. The networks differ only in the design of the individual switches and in the algorithms the switches execute.
Reference: [RWW89] <author> P. F. Reynolds, Jr., C. Williams and R. R. Wagner, Jr., </author> <title> Parallel Operations, </title> <type> Tech. Rep. 89-16, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> December, </month> <year> 1989. </year>
Reference-contexts: Some physical paths may not be routable paths for a given routing algorithm. The basic isotach network algorithm can be used for an equidistant network. Alternatively, a wave algorithm can be used in which the SIU's generate tokens and the switches propagate them <ref> [RWW89] </ref>. Periodically each SIU generates and sends a token. When a switch receives a token on each input it sends 43 out a token on each of its outputs.
Reference: [ReW91] <author> P. F. Reynolds, Jr. and R. R. Wagner, Jr., </author> <title> A Local Synchrony Implementation: Banyan Networks, </title> <type> Tech. Rep. 91-38, </type> <institution> University of Virginia, Department of Computer Science, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: This knowledge may enable the switch to route an operation on its other input. Ghosts improve network performance and are necessary in some networks for deadlock freedom <ref> [ReW91] </ref>. 8.1.2.1. Isotach network I1 In network I1 the switches have the same structure as the switches in C1, i.e., each switch has two input queues, two output buffers, and a router. Each switch in I1 records the route-tag of the last message it routed as last_tag.
Reference: [RWW92] <author> P. F. Reynolds, Jr., C. Williams and R. R. Wagner, Jr., </author> <title> Empirical Analysis of Isotach Networks, </title> <type> Tech. Rep. 92-19, </type> <institution> University of Virginia, Dept. of Computer Science, </institution> <month> June, </month> <year> 1992. </year>
Reference-contexts: We show that when operations from different atomic actions are combined within an isotach recombining network, the resulting execution is sequentially consistent and atomic on the isochron level. Chapter 8 presents results from a simulation study <ref> [RWW92] </ref> comparing the performance of iso-tach and conventional networks. The study shows conventional networks have higher raw power, i.e., more throughput with less delay, than isotach networks, but that under a workload that includes atomicity and sequencing constraints, isotach networks outperform conventional networks under the assumptions of the study. <p> We expect, therefore, that an isotach network will have lower raw power, i.e., lower throughput and higher delay, than a comparable conventional network. This chapter summarizes the results of a simulation study comparing the performance of isotach and conventional systems <ref> [RWW92] </ref>.
Reference: [ScD87] <author> C. Scheurich and M. Dubois, </author> <title> Correct Memory Operation of Cache-Based Multiprocessors, </title> <booktitle> Proc. 14th Int. Symp. Computer Architecture, </booktitle> <month> June </month> <year> 1987, </year> <pages> 234-243. </pages>
Reference-contexts: Dubois, Scheurich, and Briggs have noted difficulty applying the concept of latest store to systems that do not broadcast cache updates [DSB86]. They propose sequential consistency as an alternative correctness criterion and give restrictions on pipe-lining sufficient to enforce sequential consistency for several cache-based systems <ref> [ScD87] </ref>. We view the problem of maintaining a coherent memory in a system in which shared variables may be cached as an aspect of the concurrency control problem.
Reference: [SES89] <author> A. Schiper, J. Eggli and A. Sandoz, </author> <title> A New Algorithm to Implement Causal Ordering, </title> <booktitle> in Distributed Computing, </booktitle> <volume> vol. </volume> <publisher> 89 , Springer-Verlag, </publisher> <address> Berlin-Heidelburg-New York, </address> <year> 1989, </year> <pages> 219-232. </pages>
Reference-contexts: We define place (a) for any event a, as the pId of the process at which a occurs. Message delivery is causal if for any two messages, m and m, s (m) fi s (m) place (r (m)) = place (r (m)) fi m is received before m <ref> [SES89] </ref>. Causal delivery has been found to be useful in distributed programming applications [BSS91]. Causal message delivery is easily obtained in a system in which processes communicate via a broadcast medium because the events of sending and receiving a given message are logically simultaneous. <p> Several protocols for the causal delivery of messages or groups of messages in systems that communicate via point-to-point links have been proposed, all requiring multiple message rounds 40 <ref> [BiJ87, BSS91, Gol89, PBS89, SES89] </ref>. Some of these protocols are discussed in Chapter 4. We know of only one point-to-point network that has been shown to ensure causal message delivery a type of tree-structured network called race-free networks [LHH91].
Reference: [Sch88] <author> F. Schmuck, </author> <title> The Use of Efficient Broadcast in Asynchronous Distributed Systems, </title> <type> Ph.D Thesis, </type> <institution> Cornell University, </institution> <year> 1988. </year> <month> 188 </month>
Reference-contexts: In the resulting ordering a fi b fi t (a) &lt; t (b), but the converse, t (a) &lt; t (b) implies a fi b, is not true because a and b may be concurrent. The vector clock developed independently by Mattern [Mat88], Schmuck <ref> [Sch88] </ref>, and Fidge [Fid88, Fid91] implements a logical time system in which a fi b iff t (a) &lt; t (b).
Reference: [Sed88] <author> R. Sedgewick, </author> <title> in Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Identifying the minimum message requires that each input hold either a message or a token. As it routes each message, the switch increments the pulse component of the message's timestamp. The switch algorithm can be viewed as a basic multi-way merge algorithm (see e.g., <ref> [Sed88] </ref>). Because each SIU sends messages in timestamp order and each switch maintains that order, a switch receives messages on each input in timestamp order.
Reference: [Sha83] <editor> L. Sha, et al., </editor> <booktitle> Distributed Co-operating Processes and Transaction, Proc. ACM SIGCOMM Symp., </booktitle> <year> 1983. </year>
Reference-contexts: We assume each atomic action consists of operations issued by the same process, but this restriction is not inherent in the notion of atomicity. A generalized atomic action containing operations issued by one or more processes has been proposed by Sha <ref> [Sha83] </ref>. The concept of the atomic action was developed in the context of database concurrency control by Eswaran where it corresponds to the concept of transaction [EGL76].
Reference: [ShS88] <author> D. Shasha and M. Snir, </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory, </title> <journal> ACM Trans. Prog. Lang. and Systems 10,4 (October, </journal> <year> 1988), </year> <pages> 282-312. </pages>
Reference-contexts: The principal difference is in the greater emphasis on sequencing constraints. Our definition of serializability is strongly influenced by Lamport's concept of sequential consistency [Lam79] and by Shasha and Snir's definition of correct parallel executions <ref> [ShS88] </ref>. In databases, serial-izability serves to separate the problem of proving the correctness of database concurrency control protocols from that of proving the correctness of individual transactions [Pap86]. <p> In order to determine where fence instructions are needed, Shasha and Snir describe an algorithm for determining which operations can be safely pipelined on the basis of compile-time information <ref> [ShS88] </ref>. Landin has shown that in a tree-structured network, an intermediate node on the path to the MM may be able to issue the acknowledgement, decreasing the delay before a process can issue its next operation [LHH91].
Reference: [SiK80] <author> A. Silberschatz and Z. Kedem, </author> <title> Consistency in Hierarchical Database Systems, </title> <journal> J. </journal> <note> ACM 27,1 (January, </note> <year> 1980), </year> <pages> 72-80. </pages>
Reference-contexts: A significant drawback of this protocol is that a process must obtain locks on variables that it does not need to access if its access set is not a consecutive range of variables according to the predetermined ordering. The path protocol has been generalized to a tree-structured ordering <ref> [Mon78, SiK80, Sil82] </ref>. <p> Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root. These algorithms achieve the desired ordering by propagating each multi-cast from the lowest common ancestor of all its destinations <ref> [GaS91, LHH91, Mon78, SiK80] </ref>. 59 A second approach is based on groups [BiJ87, BSS91, GaS91, PBS89]. A group-based multicast is a hybrid multicast/broadcast. Each process belongs to one or more groups and each multicast goes to the full membership of a single process group.
Reference: [Sil82] <author> A. Silberschatz, </author> <title> A Multi-Version Concurrency Control Scheme with No Rollbacks, </title> <booktitle> Proc. ACM Symp. on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1982, </year> <pages> 216-223. </pages>
Reference-contexts: A significant drawback of this protocol is that a process must obtain locks on variables that it does not need to access if its access set is not a consecutive range of variables according to the predetermined ordering. The path protocol has been generalized to a tree-structured ordering <ref> [Mon78, SiK80, Sil82] </ref>.
Reference: [SiH91] <author> R. Simoni and M. Horowitz, </author> <title> Modeling the Performance of Limited Pointer Directories for Cache Coherence, </title> <booktitle> 18th ISCA, </booktitle> <year> 1991, </year> <pages> 309-318. </pages>
Reference-contexts: The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols. <p> bit vector representation proposed by Censier and Feautrier [CeF78], but delta-cache protocols are compatible with many of the proposals for improving the scalability of the directory representation, e.g. with the linked-list directory representation proposed for the Alewife machine [CKA91] or with the similar, hardware-supported representation discussed by Simoni and Horowitz <ref> [SiH91] </ref>. A third way in which cache protocols are classified is as write-update or write-invalidate protocols. A write-update protocol maintains coherence by keeping all copies of the same block up-to-date. Whenever any copy of a block is written, all copies of the block are updated.
Reference: [SWP86] <author> J. E. Smith, S. Weiss and N. Pang, </author> <title> A Simulation Study of Decoupled Architecture Computers, </title> <booktitle> IEEE Computer 35(August 1986), </booktitle> <pages> 692-702. </pages>
Reference-contexts: Any read instruction with the same address as a store instruction in the queue was delayed until the store was executed. A more recent computer that uses this anticipatory two-step write is the Astronautics ZS-1 [Smi89]. Decoupled access processors use the anticipatory write in a similar way <ref> [SWP86] </ref>. These designs separate the functions of accessing memory from executing instructions, giving each function its own processor.
Reference: [Smi89] <author> J. E. Smith, </author> <title> Dynamic Instruction Scheduling and the Astronautics ZS-1, </title> <journal> IEEE Computer 22,7 (July, </journal> <year> 1989), </year> <pages> 21-35. </pages>
Reference-contexts: Store instructions awaiting data were held in a queue at the memory. Any read instruction with the same address as a store instruction in the queue was delayed until the store was executed. A more recent computer that uses this anticipatory two-step write is the Astronautics ZS-1 <ref> [Smi89] </ref>. Decoupled access processors use the anticipatory write in a similar way [SWP86]. These designs separate the functions of accessing memory from executing instructions, giving each function its own processor.
Reference: [Ste89] <author> P. Stenstrom, </author> <title> A Cache Consistency Protocol for Multiprocessors with Multistage Networks, </title> <booktitle> Proc. 16th International Symp. Computer Architecture, </booktitle> <month> May </month> <year> 1989, </year> <pages> 407-415. </pages>
Reference-contexts: Split Operations Heap Implementation of Access Sequences hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh the read mask to determine whether to route the response on one or both outputs. Designs for switches that can decode read masks have already been proposed in support of unordered multicasts <ref> [ABP92, Ste89] </ref>. An additional benefit of using the network to fan-out responses is a reduction in traffic in the stages closest to memory. In computing the space overhead for the heap implementation we assume, for simplicity, that each variable is of the same size vsize. <p> The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols. <p> Some protocols, e.g., read broadcast protocols [KMR86], are write-invalidate/write-update hybrids. Because the concurrency of write-invalidate protocols is inherently limited, the delta-cache protocols use a write-update policy. A disadvantage of the update policy is the cost of distributing cache updates. Hardware support for multi-casting <ref> [ABP92, Ste89] </ref> can reduce this cost in a way that is compatible with isotach networks. To maintain consistency among the copies of the same block and ensure processes observe updates to different blocks in a consistent order, updates must appear to be executed as an indivisible step.
Reference: [Ste92] <author> P. Stenstrom, </author> <title> A Latency-Hiding Scheme for Multiprocessors with Buffered Multistage Networks, </title> <booktitle> International Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992, </year> <pages> 39-41. </pages>
Reference-contexts: Stenstrom has proposed augmenting a multiprocessor in which n PE's are connected to n MM's by a multistage ICN 19 by adding n buses each connected to each MM where each bus is dedicated to carrying consistency information about one PE <ref> [Ste92] </ref>. The high cost of enforcing sequential consistency has led researchers to propose enforcing less strict forms of sequential consistency.
Reference: [Tan76] <author> C. K. Tang, </author> <title> Cache Design in the Tightly Coupled Multiprocessor System, </title> <booktitle> AFIPS Conf. Preceedings National Computer Conference, </booktitle> <year> 1976, </year> <pages> 749-753. </pages>
Reference-contexts: Directory protocols are so named because they maintain a directory for each block listing the location of all copies of the block. The first directory protocol, due to Tang <ref> [Tan76] </ref>, specified a single centralized directory. Censier and Feautrier improved on this protocol by distributing the directory so that the directory for each block is located in the MM containing the memory copy of that block [CeF78].
Reference: [ThD91] <author> M. Thapar and B. Delagi, </author> <title> Scalable Cache Coherence for Large Shared Memory Multiprocessors, </title> <booktitle> Computer Architecture News 19,1 (March 1991), </booktitle> <pages> 114-119. </pages>
Reference-contexts: The principal focus of the subsequent work on directory protocols has been on improving the scalability of the directory representation <ref> [Aga88, ArB84, CKA91, GWM90, Jam90, LiY90, OKN90, SiH91, Ste89, ThD91] </ref>. Although reducing the space complexity of the directory representation is an important problem, our focus is different on improving the concurrency of cache coherence protocols.
Reference: [ThR89] <author> A. Thomasian and E. Rahm, </author> <title> A New Distributed Optimistic Concurrency Control Method and a Comparison of its Performance with Two-Phase Locking, </title> <type> RC 15073(#67304), </type> <institution> IBM, </institution> <month> October, </month> <year> 1989. </year>
Reference-contexts: Since a given pair of transactions can enter the certification phase in a different order at different DM's, this rule requires additional work to ensure the DM's use a consistent serial schedule in the test for serializability. An OCC protocol proposed by Thomasian and Rahm <ref> [ThR89] </ref> uses a shared bus to ensure that certification requests arrive in a consistent order at all DM's. The protocol is a hybrid. <p> false; repeat c^.next:read (n',-) || c^.next:sched (-,-) || n^.next:read (t,-); the scheduling isochron if (n == n') done := true; else initial and confirming reads disagree c^.next:cancel (-) || c^.next:read (n,-); until (done); c^.next:assign (-,t); The repeated read technique is similar to the OCC protocol proposed by Thomasian and Rahm <ref> [ThR89] </ref> (discussed in Chapter 2) but uses isochrons in place of a broadcast medium based ordered multi-cast and access sequences in place of locks. The technique is deadlock-free, but it is subject to starvation.
Reference: [TiK88] <author> P. Tinker and M. Katz, </author> <title> Parallel Execution of Sequential Scheme with ParaTran, </title> <booktitle> Proc. of the 1988 ACM Conf. on Lisp and Functional Programming, </booktitle> <address> Snowbird, Utah, </address> <month> July, </month> <year> 1988. </year>
Reference-contexts: The potential for rollback means each process must keep a history of all its internal states younger than the GVT, including messages sent and received. 27 Fujimoto [Fuj89] and Tinker <ref> [TiK88, Tin89] </ref> have proposed extending Time Warp techniques to other types of parallel computation. In Fujimoto's proposal, a SMM computation is decomposed into a number of units called tasks that can access shared variables and spawn new tasks. <p> Fujimoto has designed the Rollback Chip to provide hardware support for remembering and rolling back to past states. Tinker and Katz have proposed using a mechanism similar to Time Warp for automatic paralleliza-tion of programs written in Scheme <ref> [TiK88] </ref>, and Tinker has proposed a stack-based approach to scheduling tasks to improve the performance of the Time Warp approach [Tin89].
Reference: [Tin89] <author> P. Tinker, </author> <title> Task Scheduling for General Rollback Computing, </title> <booktitle> Int. Conf. on Parallel Processing 2(1989), </booktitle> <pages> 180-183. </pages>
Reference-contexts: The potential for rollback means each process must keep a history of all its internal states younger than the GVT, including messages sent and received. 27 Fujimoto [Fuj89] and Tinker <ref> [TiK88, Tin89] </ref> have proposed extending Time Warp techniques to other types of parallel computation. In Fujimoto's proposal, a SMM computation is decomposed into a number of units called tasks that can access shared variables and spawn new tasks. <p> Tinker and Katz have proposed using a mechanism similar to Time Warp for automatic paralleliza-tion of programs written in Scheme [TiK88], and Tinker has proposed a stack-based approach to scheduling tasks to improve the performance of the Time Warp approach <ref> [Tin89] </ref>.
Reference: [Toi92] <author> G. F. C. Toinard, </author> <title> A New Way to Design Causally and Totally Ordered Multicast Protocols, </title> <booktitle> Operating Systems Review 26,4 (October 1992), </booktitle> <pages> 77-83. </pages>
Reference-contexts: The same result can be achieved in systems with no physical serialization point in the network by funneling all multicasts through a single node <ref> [ChM84, KTH89, NCN88, Toi92] </ref>. Multicasts for hierarchically structured networks or for applications with a hierarchical communication pattern scale more successfully, but still have a potential bottleneck at the root.
Reference: [Uht87] <author> A. K. Uht, </author> <title> Incremental Performance Contributions of Hardware Concurrency Extraction Techniques, </title> <booktitle> in 1st International Conf. on Supercomputing, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1987, </year> <pages> 355-376. </pages>
Reference-contexts: The idea of using multiple versions to eliminate false dependences has appeared in many contexts, for example in parallel databases [BeG83, Pap86], dataflow computation [Cha71], compilation of loops for parallel execution <ref> [Bar92, Uht87] </ref>, and software pipelining [Lam88]. Most systems maintain only a single version of each shared variable and so can satisfy false dependences only by executing the operations in the specified order. A common source of version constraints in SMM programs is in the parallel execution of loops.
Reference: [WeL85] <author> W. Weihl and B. Liskov, </author> <title> Implementation of Resilient Atomic Data Types, </title> <journal> ACM Trans. Prog. Lang. and Systems 7,2 (April 1985), </journal> <pages> 244-269. </pages>
Reference-contexts: There have been several proposals for using semantic information provided by the programmer either about the transactions [FaO89, Gar83] or about the objects accessed <ref> [BaR88, WeL85] </ref> to increase the concurrency of the scheduler. These protocols have significant disadvantages in that they are application dependent, require significant programmer effort to specify the semantics, and have more overhead than protocols using only syntactic information. A second alternative to serializability as a correctness criterion is linearizability [HeW89]. <p> Most parallel computers support indivisible reads and writes at the word level. The atomic action allows the programmer to define a virtual machine supporting indivisible execution at any level of granularity. In databases, as well as in some message based model (MBM) programming languages designed for geographically distributed systems <ref> [LiS83, WeL85] </ref>, the atomic action is also a unit of recovery, i.e., the instructions in an atomic 17 action are executed on an all-or-nothing basis, even in the presence of hardware failures. As stated previously, we assume a failure-free system.
Reference: [Wei87] <author> W. E. Weihl, </author> <title> Distributed Version Management for Read Only Actions, </title> <journal> IEEE Trans. on Software Eng. </journal> <month> 13,1 (January </month> <year> 1987), </year> <pages> 55-64. </pages>
Reference-contexts: A transaction that attempts to write an entity with a younger read timestamp must still be aborted. A difficulty with this solution is in determining when the space occupied by old versions can be safely reused <ref> [Ree83, Wei87] </ref>. (2) Operation in predeclaration mode [Mil79, Ree78].
Reference: [Wil87] <author> A. W. Wilson, Jr., </author> <title> Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors, </title> <booktitle> Proc. 14th Int. Symp. Computer Architecture, </booktitle> <year> 1987, </year> <pages> 244-252. </pages>
Reference-contexts: Since they rely on broadcasting, snoopy protocols scale poorly. Researchers are exploring ways to improve the scalability of snoopy protocols by using multiple buses arranged hierarchically <ref> [CGB89, HLH92, Wil87] </ref> or in a grid [CaD90, GoW88]. This approach is promising for programs with access patterns that allow most broadcasts to be restricted to a local cluster of PE's. Directory protocols represent a more general approach to the scalability problem. These protocols do not require broadcasting.
Reference: [WiL92] <author> A. W. Wilson, Jr. and R. P. LaRowe, Jr., </author> <title> Hiding Shared Memory Reference Latency on the Galactica Net Distributed Shared Memory Architecture, </title> <journal> Journal of Parallel and Distributed Computing 15(August 1992), </journal> <pages> 351-367. </pages>
Reference-contexts: Some protocols are directory/snoopy hybrids, e.g., the limited directory protocols proposed by Agarwal, et al., [Aga88] and several protocols for multiple bus or 97 hybrid bus/MIN architectures, such as the protocol proposed by Algudady, et al., [ADT90] and the DASH [Len90], VMP-MC [CGB89], Aquarius [CaD90], and Galactica Net <ref> [WiL92] </ref> protocols. Directory protocols are so named because they maintain a directory for each block listing the location of all copies of the block. The first directory protocol, due to Tang [Tan76], specified a single centralized directory. <p> By shrinking the number of copies to one, the write is trivially performed atomically. A third way, illustrated by the write-update protocol used in the Galactica Net <ref> [WiL92] </ref>, is to use 2PL locking. Each update appears to occur as an indivisible step because the writer locks all copies of the affected block until the update has been performed on all the copies.
Reference: [Wu80] <author> C. Wu and T. Feng, </author> <title> On a Class of Multistage Interconnection Networks, </title> <journal> IEEE Trans. on Computers 29,8 (August 1980), </journal> <pages> 694-702. 189 </pages>
Reference-contexts: This interval is S's pId-interval for V. The reverse baseline network (see Fig. 7.1.) is convex as are all networks equivalent to the reverse baseline network under renumbering of inputs and outputs, such as the indirect n-cube, banyan, and omega networks <ref> [Wu80] </ref>. Some non-convex networks can be made convex by changing the routing algorithm or the pId assignment, but some networks, such as the network in Figure 7.2, are inherently non-convex.
Reference: [Yan82] <author> M. Yannakakis, </author> <title> A Theory of Safe Locking Policies in Database Systems, </title> <journal> J. ACM 29,4 (July, </journal> <year> 1982), </year> <pages> 718-740. </pages>
Reference-contexts: The path protocol has been generalized to a tree-structured ordering [Mon78, SiK80, Sil82]. Several protocols have been designed to reduce the expected number of extra nodes a transaction must lock <ref> [AhB89, BuS85, Yan82] </ref>, but these protocols have other drawbacks: large messages and complex tests at each node [AhB89], reliance on static mode information about transaction access patterns [BuS85], or the potential for cascading rollback [Yan82]. <p> protocols have been designed to reduce the expected number of extra nodes a transaction must lock [AhB89, BuS85, Yan82], but these protocols have other drawbacks: large messages and complex tests at each node [AhB89], reliance on static mode information about transaction access patterns [BuS85], or the potential for cascading rollback <ref> [Yan82] </ref>. In systems in which the network topology matches the hierarchical structure of the data, the tree protocols can be viewed as implementing ordered multicasts. Ordered multicasts and their use in concurrency control are discussed in Chapter 4. 2.4.2.
Reference: [ZhY84] <author> C. Q. Zhu and P. C. Yew, </author> <title> A Synchronization Scheme and its Application for Large Multiprocessor Systems, </title> <booktitle> Proc. of the 4th Int. Conf. on Distributed Computing Systems, </booktitle> <year> 1984, </year> <pages> 486-493. </pages>
Reference-contexts: synchronization flags or key fields for controlling access to the associated variable, e.g., the full/empty flags on the HEP [Jor83], the empty bit associated with each element of the I-structure proposed by Arvind for dataflow computation [ANP89], and the synchronization key fields proposed by Zhu and Yew and by Peir <ref> [Pei83, ZhY84] </ref>. Other proposals for hardware support of version consistency include algorithms for improving the efficiency of barrier synchronization [Bro86, HFM88, Jay87]. 21 2.4.
References-found: 150

