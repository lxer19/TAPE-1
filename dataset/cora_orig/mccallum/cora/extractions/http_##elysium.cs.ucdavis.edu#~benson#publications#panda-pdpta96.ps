URL: http://elysium.cs.ucdavis.edu/~benson/publications/panda-pdpta96.ps
Refering-URL: http://elysium.cs.ucdavis.edu/~benson/publications/publications.html
Root-URL: http://www.cs.ucdavis.edu
Title: Experience with a Portability Layer for Implementing Parallel Programming Systems  
Author: Tim Ruhl Henri Bal Gregory Benson Raoul Bhoedjang Koen Langendoen 
Affiliation: Dept. of Mathematics and Computer Science Dept. of Computer Science Vrije Universiteit, Amsterdam University of California, Davis  
Abstract: Panda is a virtual machine designed to support portable implementations of parallel programming systems. It provides communication primitives and thread support to higher-level layers (such as a runtime system). We have used Panda to implement four parallel programming systems: Orca, data parallel Orca, PVM, and SR. The paper describes our experiences in implementing these systems using Panda and it eval uates the performance of the Panda-based implementations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.R. Andrews and R.A. Olsson. </author> <title> The SR Programming Language: Concurrency in Practice. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1993. </year>
Reference-contexts: systems: * the Orca parallel language, for which the Panda system was originally designed [4], * a system based on Orca, but with extensions to support both task and data parallelism in an integrated way [9], * the PVM message passing system [15], and * the SR concurrent programming language <ref> [1] </ref>. The outline of the paper is as follows. In Section 2 we first look at other approaches for portable implementations of programming systems. In Section 3 we outline the design of the Panda system. <p> We resolve this difference by optimistically letting the program continue. Usually, the send buffer will be reinitialized using another Panda message, but sometimes another operation may be invoked on the same message. Only in that exceptional case, we block the invoker until the acknowledgement arrives. 4.3 SR SR <ref> [1] </ref> is a task-parallel language for writing concurrent programs that run on both shared memory multiprocessors and distributed memory systems. The main language elements are operations, processes, resources, and virtual machines. Operations represent communication channels through which processes communicate. Resources are dynamic modules which contain processes and variables.
Reference: [2] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A Language for Parallel Program--ming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The portability of parallel applications can be enhanced by using portable programming systems, but this leaves many of the problems to the implementor of such systems. In particular, it is difficult to obtain both portability and efficiency. In our research on the Orca <ref> [2] </ref> programming system, we use the well-known implementation technique of a virtual machine to achieve portability. We have designed a virtual machine, called Panda [4], that hides machine specific details from its users. <p> If two processes simultaneously multicast a message, either all receivers get the first message first or they all get the second message first. These strong semantics are useful for implementing replicated shared data, because the replicas can be kept consistent by multicasting the updates using a total ordering <ref> [2] </ref>. Panda's group communication primitives allow processes to join (or leave) a given group and to send a message to a group. <p> Its programming model is a form of object-based Distributed Shared Memory <ref> [2] </ref>. Objects in Orca are variables of abstract data types that can be shared by different processes. A recent extension to Orca [9] also supports data parallel programming, by allowing objects to be partitioned among multiple machines. <p> A recent extension to Orca [9] also supports data parallel programming, by allowing objects to be partitioned among multiple machines. Below, we first discuss the implementation of the original Orca language on Panda; after that, we discuss the data parallel extension. The initial implementation of Orca <ref> [2] </ref> ran directly on top of the Amoeba distributed operating system, so it was not portable. The Panda system was designed as a cornerstone of a portable implementation of Orca. Orca is implemented on top of Panda as follows. <p> All these machines execute the operation on their local copy, thus updating the replica. The total ordering guarantees that all machines receive all updates in the same order, so all replicas will remain consistent <ref> [2] </ref>. The data parallel extension of Orca is based on partitioned objects, which are objects containing arrays that are partitioned among multiple processors. Operations on these partitioned objects conceptually update all elements at the same time. <p> The Panda system uses Amoeba's kernel threads; the Panda synchronization primitives (mutexes and condition variables) are implemented on top of Amoeba mutexes. We compare the performance of this system with that of the original (nonportable) Orca system, which was implemented directly on top of Amoeba <ref> [2] </ref>. The two systems use the same compiler, but entirely different runtime systems. The basic communication primitive in Orca is object invocation.
Reference: [3] <author> R.A.F. Bhoedjang and K. Langendoen. </author> <title> Friendly and Efficient Message Handling. </title> <booktitle> In Proc of the 29th Hawaii International Conference on System Sciences, </booktitle> <pages> pages 121-130, </pages> <address> Maui, Hawaii, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: To allow an efficient and deadlock-free implementation with daemon threads, the upcall function is not allowed to block on a condition that depends on the arrival of new messages <ref> [3] </ref>. Three types of reliable communication are provided: point-to-point message passing (MP), Remote Procedure Call (RPC), and group communication. With message passing, the sender can either wait until the message has been delivered at the destination processor (synchronous MP) or it can continue immediately (asynchronous MP). <p> Whenever the object is changed, its continuations are picked up and the operations are tried again. An alternative solution (which we originally used) is to create a new thread of control for the upcall handler, but this has a high context switching and memory overhead <ref> [3] </ref>. For operations on replicated objects, we use an entirely different strategy. If the operation does not modify the object, it is executed locally, without any communication.
Reference: [4] <author> R.A.F. Bhoedjang, T. Ruhl, R. Hofman, K. Langendoen, H.E. Bal, and M.F. Kaashoek. Panda: </author> <title> A Portable Platform to Support Parallel Programming Languages. </title> <booktitle> Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 213-226, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: In particular, it is difficult to obtain both portability and efficiency. In our research on the Orca [2] programming system, we use the well-known implementation technique of a virtual machine to achieve portability. We have designed a virtual machine, called Panda <ref> [4] </ref>, that hides machine specific details from its users. Panda provides support for threads and communication to higher software layers (such as the Orca runtime system), in a machine and operating system independent way. <p> In this paper, we describe our experiences using Panda to implement four programming systems: * the Orca parallel language, for which the Panda system was originally designed <ref> [4] </ref>, * a system based on Orca, but with extensions to support both task and data parallelism in an integrated way [9], * the PVM message passing system [15], and * the SR concurrent programming language [1]. The outline of the paper is as follows.
Reference: [5] <author> The PORTS Consortium. </author> <title> PORTS: POrtable RunTime System. </title> <note> documents available from http://www.cs.uoregon.edu/paracomp/ports/, 1996. </note>
Reference-contexts: These two low-level primitives can be used to implement higher-level abstractions, such as message passing or Remote Procedure Calls. PORTS <ref> [5] </ref> is perhaps the most ambitious undertaking to obtain a portable common runtime system. A consortium of several researchers is currently working on the specification of PORTS.
Reference: [6] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> Supercomputing'93, </booktitle> <pages> pages 878-883, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Finally, in Section 6, we present our conclusions. 2 Related Work Several other systems exist that, like Panda, support portable implementations of different programming environments. We discuss several approaches below. Other related efforts include portable communication interfaces (e.g., PVM [15] and MPI <ref> [6] </ref>) and portable threads packages (e.g., Pthreads [12], Chant [8], and uThread [14]). Converse [11] is a framework that allows interoperability between different programming systems. Using Converse, it is possible to write different modules of a parallel program in different languages, and have these cooperate.
Reference: [7] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus Approach to Integrating Multithread-ing and Communication. </title> <note> Journal of Parallel and Distributed Computing (to appear), </note> <year> 1996. </year>
Reference-contexts: Both Converse and Panda provide point-to-point and multicast communication, although in Panda the semantics of multicast are stronger. Panda supports totally-ordered multicast [10], which is useful for implementing data replication techniques that are used by Distributed Shared Memory systems. Nexus <ref> [7] </ref> is a portable library that has been used to implement a range of programming systems, such as Compositional C++, Fortran-M, MPI, and others. An important goal in Nexus is to provide flexible mechanisms for threads and communication.
Reference: [8] <author> M. Haines, D. Cronk, and P. Mehrota. </author> <title> On the Design of Chant: a Talking Threads Package. </title> <booktitle> In Supercomputing '94, </booktitle> <pages> pages 350-359, </pages> <address> Washington D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: We discuss several approaches below. Other related efforts include portable communication interfaces (e.g., PVM [15] and MPI [6]) and portable threads packages (e.g., Pthreads [12], Chant <ref> [8] </ref>, and uThread [14]). Converse [11] is a framework that allows interoperability between different programming systems. Using Converse, it is possible to write different modules of a parallel program in different languages, and have these cooperate.
Reference: [9] <author> S. Ben Hassen and H.E. Bal. </author> <title> Integrating Task and Data Parallelism Using Shared Objects. </title> <booktitle> In 10th ACM International Conference on Supercomputing, </booktitle> <pages> pages 317-324, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In this paper, we describe our experiences using Panda to implement four programming systems: * the Orca parallel language, for which the Panda system was originally designed [4], * a system based on Orca, but with extensions to support both task and data parallelism in an integrated way <ref> [9] </ref>, * the PVM message passing system [15], and * the SR concurrent programming language [1]. The outline of the paper is as follows. In Section 2 we first look at other approaches for portable implementations of programming systems. In Section 3 we outline the design of the Panda system. <p> Its programming model is a form of object-based Distributed Shared Memory [2]. Objects in Orca are variables of abstract data types that can be shared by different processes. A recent extension to Orca <ref> [9] </ref> also supports data parallel programming, by allowing objects to be partitioned among multiple machines. Below, we first discuss the implementation of the original Orca language on Panda; after that, we discuss the data parallel extension.
Reference: [10] <author> M.F. Kaashoek. </author> <title> Group Communication in Distributed Computer Systems. </title> <type> PhD thesis, </type> <institution> Vrije Universiteit, </institution> <address> Amsterdam, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Also, Converse provides functionality (e.g., load balancers) that in our approach would be implemented in the language-dependent RTS. Both Converse and Panda provide point-to-point and multicast communication, although in Panda the semantics of multicast are stronger. Panda supports totally-ordered multicast <ref> [10] </ref>, which is useful for implementing data replication techniques that are used by Distributed Shared Memory systems. Nexus [7] is a portable library that has been used to implement a range of programming systems, such as Compositional C++, Fortran-M, MPI, and others. <p> So, RPC is 2-way communication, whereas MP is 1-way. Group communication in Panda provides totally-ordered multicast, which guarantees that all multicast messages arrive in the same order at all destinations <ref> [10] </ref>. If two processes simultaneously multicast a message, either all receivers get the first message first or they all get the second message first. These strong semantics are useful for implementing replicated shared data, because the replicas can be kept consistent by multicasting the updates using a total ordering [2]. <p> The protocols are implemented on top of Amoeba's unreliable unicast and multicast primitives. The RPC protocol is a 2-way stop-and-wait protocol (although Panda sometimes uses a larger packet size than that provided by the underlying hardware). The broadcast protocol is based on the sequencer protocols described in <ref> [10] </ref>. The Panda system uses a daemon thread that receives packets, reassembles them into messages, and then makes an upcall. The Panda system uses Amoeba's kernel threads; the Panda synchronization primitives (mutexes and condition variables) are implemented on top of Amoeba mutexes.
Reference: [11] <author> L.V. Kale, M. Bhandarkar, N. Jagathesan, S. Krishnan, and J. Yelon. </author> <title> An Interoper-able Framework for Parallel Programming. </title> <booktitle> In Proc. International Parallel Processing Symposium, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: We discuss several approaches below. Other related efforts include portable communication interfaces (e.g., PVM [15] and MPI [6]) and portable threads packages (e.g., Pthreads [12], Chant [8], and uThread [14]). Converse <ref> [11] </ref> is a framework that allows interoperability between different programming systems. Using Converse, it is possible to write different modules of a parallel program in different languages, and have these cooperate.
Reference: [12] <author> F. Mueller. </author> <title> A Library Implementation of POSIX Threads under UNIX. </title> <booktitle> In Proc. USENIX Conference Winter'93, </booktitle> <pages> pages 29-41, </pages> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Finally, in Section 6, we present our conclusions. 2 Related Work Several other systems exist that, like Panda, support portable implementations of different programming environments. We discuss several approaches below. Other related efforts include portable communication interfaces (e.g., PVM [15] and MPI [6]) and portable threads packages (e.g., Pthreads <ref> [12] </ref>, Chant [8], and uThread [14]). Converse [11] is a framework that allows interoperability between different programming systems. Using Converse, it is possible to write different modules of a parallel program in different languages, and have these cooperate. <p> PORTS [5] is perhaps the most ambitious undertaking to obtain a portable common runtime system. A consortium of several researchers is currently working on the specification of PORTS. The PORTS interface supports threads (based on a subset of Pthreads <ref> [12] </ref>) Table 1: Overview of the Panda interface Threads create, exit, join, yield, self, set priority Synchronization mutex lock, trylock, unlock condition variable wait, timed wait, signal, broadcast Communication Message Passing register, send, receive, poll Remote Procedure Call register, call, reply Group Communication join, leave, send and communication.
Reference: [13] <author> M. Oey, K. Langendoen, and H.E. Bal. </author> <title> Comparing Kernel-Space and User-Space Communication Protocols on Amoeba. </title> <booktitle> In Proc. of the 15th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 238-245, </pages> <address> Vancouver, British Columbia, Canada, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Each processor is on a single board (a Tatung board, equivalent to the Sun Classic) and contains 32 MB of local memory. The boards are connected by a 10 Mbit/sec switched Ethernet. The Panda implementation on Amoeba is described in <ref> [13] </ref>. Panda is implemented with communication protocols in user space. The protocols are implemented on top of Amoeba's unreliable unicast and multicast primitives. The RPC protocol is a 2-way stop-and-wait protocol (although Panda sometimes uses a larger packet size than that provided by the underlying hardware). <p> The overhead for GOI is mainly attributed to running the broadcast protocol (including the sequencer) in user space, so more crossings between user space and kernel space are needed compared to the original implementation <ref> [13] </ref>. In addition to these benchmarks, we have also run a large number of Orca applications on both systems. The results show that the Panda-based system achieves about the same absolute and relative performance. Table 3: Null latency (in msecs) for PVM roundtrip messages. <p> In general, the usage of Panda somewhat increases the latency of the basic communication operations. In many cases, the overhead is related to threads (thread creation, context switching, or thread-safety). Also, the Panda protocols run in user space, which has some overhead <ref> [13] </ref>. The great advantages of using Panda, however, are increased portability, modularity, and the flexibility of user-space protocols. Acknowledgments This research is supported in part by a PIONIER grant from the Netherlands Organization for Scientific Research (N.W.O.).
Reference: [14] <author> W. Shu. </author> <title> Runtime Support for User-Level Ultra Lightweight Threads on Massively Parallel Distributed Memory Machines. </title> <booktitle> In Proc. Frontiers 1995, </booktitle> <pages> pages 448-455, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: We discuss several approaches below. Other related efforts include portable communication interfaces (e.g., PVM [15] and MPI [6]) and portable threads packages (e.g., Pthreads [12], Chant [8], and uThread <ref> [14] </ref>). Converse [11] is a framework that allows interoperability between different programming systems. Using Converse, it is possible to write different modules of a parallel program in different languages, and have these cooperate.
Reference: [15] <author> V.S. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: our experiences using Panda to implement four programming systems: * the Orca parallel language, for which the Panda system was originally designed [4], * a system based on Orca, but with extensions to support both task and data parallelism in an integrated way [9], * the PVM message passing system <ref> [15] </ref>, and * the SR concurrent programming language [1]. The outline of the paper is as follows. In Section 2 we first look at other approaches for portable implementations of programming systems. In Section 3 we outline the design of the Panda system. <p> Finally, in Section 6, we present our conclusions. 2 Related Work Several other systems exist that, like Panda, support portable implementations of different programming environments. We discuss several approaches below. Other related efforts include portable communication interfaces (e.g., PVM <ref> [15] </ref> and MPI [6]) and portable threads packages (e.g., Pthreads [12], Chant [8], and uThread [14]). Converse [11] is a framework that allows interoperability between different programming systems. Using Converse, it is possible to write different modules of a parallel program in different languages, and have these cooperate.
Reference: [16] <author> R. van Renesse, K.P. Birman, and S. Maffeis. Horus: </author> <title> A flexible group communication system. </title> <journal> Comm. of the ACM, </journal> <volume> 39(4) </volume> <pages> 76-83, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Other functions in the PORTS interface support timers, tracing, and processor allocation. Since the PORTS interface is still under development, a comparison with our work on Panda would be premature. Horus <ref> [16] </ref> is a system that provides group communication to support distributed applications. It uses a number of "microprotocols" with standardized interfaces, that can be stacked on each other (much like Lego blocks) to form a group protocol. The system is very flexible, and even allows dynamic configuration of protocol stacks.
References-found: 16

