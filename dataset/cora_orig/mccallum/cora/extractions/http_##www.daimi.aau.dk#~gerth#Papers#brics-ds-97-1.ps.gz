URL: http://www.daimi.aau.dk/~gerth/Papers/brics-ds-97-1.ps.gz
Refering-URL: http://www.daimi.aau.dk/~gerth/
Root-URL: http://www.daimi.aau.dk
Title: Case Efficient Data  Worst Case Efficient Data Structures  
Author: S. odal: W Structur es Gerth Stlting Brodal 
Date: orst  1396-7002 January 1997  
Web: DS-97-1  
Note: BRICS  BRICS Basic Research in Computer Science  BRICS Dissertation Series DS-97-1 ISSN  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> The Design And Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Performing ParUnlink once guarantees that A 2 is satisfied, especially that the new minimum element is contained in Q:L [0], because the new minimum element was either already contained in Q:L [0] or it was the minimum element in Q:L <ref> [1] </ref>. Finally ParLink performed once reestablishes A 1 . A pseudo code implementation for a CREW PRAM based on the previous discussion is shown in Figure 7.2. <p> In parallel we now construct a rank one tree from 75 each block. The remaining 1-6 elements are stored in Q:L [0]. The same block partitioning and linking is now done for the rank one trees. The remaining rank one trees are stored in Q:L <ref> [1] </ref>. This process continues until no tree remains. There are at most O (log n) iterations because each iteration reduces the number of trees by a factor six. The resulting forest satisfies B 1 and B 3 .
Reference: [2] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: We present the corresponding data structure of size O (nm) for the 1-query case. We present a simple data structure based on tries <ref> [2, 50] </ref> which has optimal size O (nm) and supports 1-queries in time O (m). Unfortunately, we do not know how to construct the data structure in time O (nm) and we leave this as an open problem.
Reference: [3] <author> Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. </author> <title> Network Flows. </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: We mention here the assignment problem, the minimum cost flow problem, (for definitions see <ref> [3] </ref>), and the single-source shortest path problem in planar digraphs. The minimum cost flow problem (which is P-complete [57]) can be solved by O (m log n) calls to Dijkstra's algorithm (see e.g. [3, Section 10.7]). <p> We mention here the assignment problem, the minimum cost flow problem, (for definitions see [3]), and the single-source shortest path problem in planar digraphs. The minimum cost flow problem (which is P-complete [57]) can be solved by O (m log n) calls to Dijkstra's algorithm (see e.g. <ref> [3, Section 10.7] </ref>). Using our implementation, we obtain a parallel algorithm that runs in O (nm log n) time and performs O (m 2 log 2 n) work. The assignment problem can be solved by n calls to Dijkstra's algorithm (see e.g. [3, Section 12.4]). <p> Using our implementation, we obtain a parallel algorithm that runs in O (nm log n) time and performs O (m 2 log 2 n) work. The assignment problem can be solved by n calls to Dijkstra's algorithm (see e.g. <ref> [3, Section 12.4] </ref>). Using our implementation, we obtain a parallel algorithm that runs in O (n 2 ) time and performs O (nm log n) work.
Reference: [4] <author> Susanne Albers and Torben Hagerup. </author> <title> Improved parallel integer sorting without concurrent writing. </title> <booktitle> In Proc. 3rd ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 463-472, </pages> <year> 1992. </year>
Reference-contexts: In the following we w.l.o.g. assume that we can apply Practical RAM operations to a list of O (k) integers stored in O (1) words in worst cast constant time. Together with each integer we store a test bit, as in <ref> [4, 5, 102] </ref>. An integer together with the associated test bit is denoted a field. Figure 9.1 illustrates the structure of a list of maximum capacity k containing ` k integers x 1 ; : : : ; x ` . <p> A field containing the integer x i has a test bit equal to zero. The remaining k ` empty fields store the integer zero and a test bit equal to one. Essential to the data structure to be described is the following lemma due to Albers and Hagerup <ref> [4] </ref>. Lemma 12 (Albers and Hagerup) On a Practical RAM two sorted lists each of at most O (k) integers stored in O (1) words can be merged into a single sorted list stored in O (1) words in O (log k) time. <p> Tedious implementation details are omitted. First a mask is constructed corresponding to the integers only appearing once in C. This can be done in worst case constant time by performing the comparisons between neighbor integers in C by subtraction like the mask construction described in <ref> [4] </ref>. The integers appearing only once in C are compressed to form a single list as follows. First a prefix sum computation is performed to calculate how many fields each integer has to be shifted to the right. <p> A similar approach has been applied in <ref> [4] </ref> to reverse a list of integers. 2 The main component of our data structure is a search tree T where all leaves have equal depth and all internal nodes have degree at least one and at most k= log 4 n.
Reference: [5] <author> Arne Andersson. </author> <title> Sublogarithmic searching without multiplications. </title> <booktitle> In Proc. 36th Ann. Symp. on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 655-663, </pages> <year> 1995. </year>
Reference-contexts: The space requirement of Thorup's data structure is O (n2 *w ) (if the time bounds are amortized the space requirement is O (n + 2 *w )). Andersson <ref> [5] </ref> has presented a Practical RAM implementation supporting insertions, deletions and predecessor queries in worst case O ( p log n) time and minimum and maximum queries in worst case constant time. The space requirement of Andersson's data structure is O (n + 2 *w ). <p> The space requirement of Andersson's data structure is O (n + 2 *w ). Several data structures can achieve the same time bounds as Andersson <ref> [5] </ref>, but they all require constant time multiplication [6, 54, 93]. The main result of this paper is Theorem 22 stated below. The theorem requires the notion of smooth functions. <p> The data structure is the first allowing predecessor queries in O (log n= log log n) time while having O (log log n) update time. If f (n) = p achieve time bounds matching those of Andersson <ref> [5] </ref>. The basic idea of our construction is to apply the data structure of van Emde Boas et al. [104, 106] for O (f (n)) levels and then switch to a packed search tree of height O (log n=f (n)). This is very similar to the data structure of Andersson [5]. <p> <ref> [5] </ref>. The basic idea of our construction is to apply the data structure of van Emde Boas et al. [104, 106] for O (f (n)) levels and then switch to a packed search tree of height O (log n=f (n)). This is very similar to the data structure of Andersson [5]. But where Andersson uses O (log n=f (n)) time to update his packed B-tree, we only need O (f (n)) time. <p> In the following we w.l.o.g. assume that we can apply Practical RAM operations to a list of O (k) integers stored in O (1) words in worst cast constant time. Together with each integer we store a test bit, as in <ref> [4, 5, 102] </ref>. An integer together with the associated test bit is denoted a field. Figure 9.1 illustrates the structure of a list of maximum capacity k containing ` k integers x 1 ; : : : ; x ` . <p> This part of the data structure is quite similar to the packed B-tree described by Andersson <ref> [5] </ref>. To achieve faster update times for Insert and Delete than Andersson, we add buffers of delayed Insert and Delete operations to each internal node of the tree. <p> Otherwise we have to search for the predecessor of e in T . We first perform a search for e in the search tree T . The implementation of the search for e in T is identical to how Andersson searches in a packed B-tree <ref> [5] </ref>. We refer to [5] for details. Let be the leaf reached and w 1 ; : : : ; w h1 be the internal nodes on the path from the root to . Define w h = . <p> Otherwise we have to search for the predecessor of e in T . We first perform a search for e in the search tree T . The implementation of the search for e in T is identical to how Andersson searches in a packed B-tree <ref> [5] </ref>. We refer to [5] for details. Let be the leaf reached and w 1 ; : : : ; w h1 be the internal nodes on the path from the root to . Define w h = . <p> If the parent p of w now has + 1 children we split p into two nodes of degree =2 while distributing the buffers I p and D p among the two nodes w.r.t. the new search key. The details of how to split a node is described in <ref> [5] </ref>. If the parent of p gets degree + 1 we recursively split the parent of p. The implementation of inserting e in T takes worst case O (h log k) time. <p> This is similar to the data structure of Andersson <ref> [5] </ref>, and for details we refer to [5]. We w.l.o.g. assume w 2 f (n) log n. The idea is to use the topmost f (n) levels of the data structure of van Emde Boas et al. and then switch to our packed search trees. <p> This is similar to the data structure of Andersson <ref> [5] </ref>, and for details we refer to [5]. We w.l.o.g. assume w 2 f (n) log n. The idea is to use the topmost f (n) levels of the data structure of van Emde Boas et al. and then switch to our packed search trees.
Reference: [6] <author> Arne Andersson. </author> <title> Faster deterministic sorting and searching in linear space. </title> <booktitle> In Proc. 37th Ann. Symp. on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 135-141, </pages> <year> 1996. </year>
Reference-contexts: The space requirement of Andersson's data structure is O (n + 2 *w ). Several data structures can achieve the same time bounds as Andersson [5], but they all require constant time multiplication <ref> [6, 54, 93] </ref>. The main result of this paper is Theorem 22 stated below. The theorem requires the notion of smooth functions. Overmars [86] defines a nondecreasing function f to be smooth if and only if f (O (n)) = O (f (n)).
Reference: [7] <author> Lars Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In Proc. 4th Workshop on Algorithms and Data Structures (WADS), volume 955 of Lecture Notes in Computer Science, </booktitle> <pages> pages 334-345. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: The idea of adding buffers to a search tree has in the context of designing I/O efficient data structures been applied by Arge <ref> [7] </ref>. Throughout this paper we w.l.o.g. assume Delete only deletes integers actually contained in the set and Insert never inserts an already inserted integer. This can be satisfied by tabulating the multiplicity of each inserted integer.
Reference: [8] <author> Mikhail J. Atallah and S. Rao Kosaraju. </author> <title> An adversary-based lower bound for sorting. </title> <journal> Information Processing Letters, </journal> <volume> 13 </volume> <pages> 55-57, </pages> <year> 1981. </year>
Reference: [9] <author> Michael D. Atkinson, Jorg-Rudiger Sack, Nicola Santoro, and Thomas Strothotte. </author> <title> Min-max heaps and generalized priority queues. </title> <journal> Communications of the ACM, </journal> <volume> 29(10) </volume> <pages> 996-1000, </pages> <year> 1986. </year>
Reference-contexts: As a simple consequence of our construction we get a new implementation of meldable double ended priority queues, which is a data type that allows both FindMin/FindMax and Delete-Min/ DeleteMax <ref> [9, 38] </ref>. For each queue we just have to maintain two heap ordered trees as described in Section 5.1. One tree ordered with respect to minimum and the other with respect to maximum.
Reference: [10] <author> Kenneth E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proc. AFIPS Spring Joint Computer Conference, </booktitle> <volume> 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: Albers and Hagerup's proof of Lemma 12 is a description of how to implement the bitonic merging algorithm of Batcher <ref> [10] </ref> in a constant number of words on the Practical RAM.
Reference: [11] <author> Jit Biswas and James C. Browne. </author> <title> Simultaneous update of priority structures. </title> <booktitle> In Int. Conference on Parallel Processing, </booktitle> <pages> pages 124-131, </pages> <year> 1987. </year>
Reference: [12] <author> Burton H. Bloom. </author> <title> Space/time trade-offs in hash coding with allowable errors. </title> <journal> Communications of the ACM, </journal> <volume> 13 </volume> <pages> 422-426, </pages> <year> 1970. </year>
Reference-contexts: Recently a sequence of papers have considered how to solve this problem efficiently [41, 42, 59, 74, 112]. Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters <ref> [12] </ref> and uses hashing techniques. Dolev et al. [41, 42] and Greene, Parnas and Yao [59] considered approximate dictionary queries for the case where d is large. The initial effort towards a theoretical study of the small d case was given by Yao and Yao in [112].
Reference: [13] <author> Bela Bollobas and Istvan Simon. </author> <title> Repeated random insertion into a priority queue. </title> <journal> Journal of Algorithms, </journal> <volume> 6 </volume> <pages> 466-477, </pages> <year> 1985. </year>
Reference: [14] <author> Allan Borodin, Leonidas J. Guibas, Nancy A. Lynch, and Andrew C. Yao. </author> <title> Efficient searching using partial ordering. </title> <journal> Information Processing Letters, </journal> <volume> 12 </volume> <pages> 71-75, </pages> <year> 1981. </year> <month> 115 </month>
Reference: [15] <author> Gerth Stolting Brodal. </author> <title> Fast meldable priority queues. </title> <booktitle> In Proc. 4th Workshop on Algorithms and Data Structures (WADS), volume 955 of Lecture Notes in Computer Science, </booktitle> <pages> pages 282-290. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Program of the EC under contract no. 7141 (project ALCOM II) and by the Danish Natural Science Research Council (Grant No. 9400044). y BRICS (Basic Research in Computer Science), a Centre of the Danish National Research Foundation. 57 Amortized Worst case Fredman et al. [53] Driscoll et al. [43] Brodal <ref> [15] </ref> New result MakeQueue O (1) O (1) O (1) O (1) FindMin O (1) O (1) O (1) O (1) Insert O (1) O (1) O (1) O (1) Meld O (1) O (log n) O (1) O (1) DecreaseKey O (1) O (1) O (log n) O (1) Delete/DeleteMin <p> They achieve amortized constant time for all operations except for the two delete operations which require amortized time O (log n). The data structure we present achieves matching worst case time bounds for all operations. Previously, this was only achieved for various strict subsets of the listed operations <ref> [15, 27, 43, 107] </ref>. <p> The data structure we present achieves matching worst case time bounds for all operations. Previously, this was only achieved for various strict subsets of the listed operations [15, 27, 43, 107]. For example the relaxed heaps of Driscoll et al. [43] and the priority queues in <ref> [15] </ref> achieve the above time bounds in the worst case sense except that in [43] Meld requires worst case time fi (log n) and in [15] DecreaseKey requires worst case time fi (log n). Refer to Table 6.1. <p> For example the relaxed heaps of Driscoll et al. [43] and the priority queues in <ref> [15] </ref> achieve the above time bounds in the worst case sense except that in [43] Meld requires worst case time fi (log n) and in [15] DecreaseKey requires worst case time fi (log n). Refer to Table 6.1. If we ignore the Delete operation our results are optimal in the following sense. A lower bound for DeleteMin in the comparison model is proved in [15] where it is proved that if Meld can be performed in <p> [43] Meld requires worst case time fi (log n) and in <ref> [15] </ref> DecreaseKey requires worst case time fi (log n). Refer to Table 6.1. If we ignore the Delete operation our results are optimal in the following sense. A lower bound for DeleteMin in the comparison model is proved in [15] where it is proved that if Meld can be performed in time o (n) then DeleteMin cannot be performed in time o (log n). The data structure presented in this paper originates from the same ideas as the relaxed heaps of Driscoll et al. [43]. <p> The guide's job is to tell us which operations to perform. This problem also arises implicitly in <ref> [15, 27, 60, 67] </ref>. But the solution presented in [60] requires time fi (k) to find which Reduce operations to perform whereas the problems in the other papers are simpler because only x 1 can be forced to increase and decrease. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95].
Reference: [16] <author> Gerth Stolting Brodal. </author> <title> Partially persistent data structures of bounded degree with constant update time. </title> <journal> Nordic Journal of Computing, </journal> <volume> 3(3) </volume> <pages> 238-255, </pages> <year> 1996. </year>
Reference: [17] <author> Gerth Stolting Brodal. </author> <title> Priority queues on parallel machines. </title> <booktitle> In Proc. 5th Scandinavian Workshop on Algorithm Theory (SWAT), volume 1097 of Lecture Notes in Computer Science, </booktitle> <pages> pages 416-427. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [18] <author> Gerth Stolting Brodal. </author> <title> Worst-case efficient priority queues. </title> <booktitle> In Proc. 7th ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 52-58, </pages> <year> 1996. </year>
Reference-contexts: Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. <p> in O (m + n log n) operations by using efficient priority queues like Fibonacci heaps [53] for maintaining tentative distances, or other priority queue implementations supporting deletion of the minimum key element in amortized or worst case logarithmic time, and decrease key in amortized or worst case constant time <ref> [18, 43, 63] </ref>. fl This work was partially supported by the EU ESPRIT LTR Project No. 20244 (ALCOM-IT), and by the DFG project SFB 124-D6 (VLSI Entwurfsmethoden und Parallelitat). y Supported by the Danish Natural Science Research Council (Grant No. 9400044). z BRICS (Basic Research in Computer Science), a Centre of
Reference: [19] <author> Gerth Stolting Brodal. </author> <title> Predecessor queries in dynamic integer sets. </title> <booktitle> In Proc. 14th Symposium on Theoretical Aspects of Computer Science (STACS), volume 1200 of Lecture Notes in Computer Science, </booktitle> <pages> pages 21-32. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference: [20] <author> Gerth Stolting Brodal, Shiva Chaudhuri, and Jaikumar Radhakrishnan. </author> <title> The randomized complexity of maintaining the minimum. </title> <booktitle> Nordic Journal of Computing, Selected Papers of the 5th Scandinavian Workshop on Algorithm Theory (SWAT'96), </booktitle> <volume> 3(4) </volume> <pages> 337-351, </pages> <year> 1996. </year>
Reference-contexts: For the comparison model a tradeoff between the operations has been shown by Brodal et al. <ref> [20] </ref>. The tradeoff shown in [20] is that if Insert and Delete take worst case O (t (n)) time then FindMin (and FindMax) requires at least worst case n=2 O (t (n)) time. <p> For the comparison model a tradeoff between the operations has been shown by Brodal et al. <ref> [20] </ref>. The tradeoff shown in [20] is that if Insert and Delete take worst case O (t (n)) time then FindMin (and FindMax) requires at least worst case n=2 O (t (n)) time. <p> Miltersen [78] refers to this model as a Practical RAM. We assume the elements are integers in the range 0::2 w 1. A tradeoff similar to the one for the comparison model <ref> [20] </ref> is not known for a Practical RAM. A data structure of van Emde Boas et al. [104, 106] supports the operations Insert, Delete, Pred, FindMin and FindMax on a Practical RAM in worst case O (log w) time.
Reference: [21] <author> Gerth Stolting Brodal and Leszek Gasieniec. </author> <title> Approximate dictionary queries. </title> <booktitle> In Proc. 7th Combinatorial Pattern Matching (CPM), volume 1075 of Lecture Notes in Computer Science, </booktitle> <pages> pages 65-74. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference: [22] <author> Gerth Stolting Brodal and Chris Okasaki. </author> <title> Optimal purely functional priority queues. </title> <journal> Journal of Functional Programming, </journal> <month> December </month> <year> 1996. </year>
Reference: [23] <author> Gerth Stolting Brodal, Jesper Larsson Traff, and Christos D. Zaroliagis. </author> <title> A parallel priority data structure with applications. </title> <booktitle> In Proc. 11th Int. Parallel Processing Symposium (IPPS), </booktitle> <pages> pages 689-693, </pages> <year> 1997. </year>
Reference: [24] <author> Mark R. Brown. </author> <title> Implementation and analysis of binomial queue algorithms. </title> <journal> SIAM Journal of Computing, </journal> <volume> 7 </volume> <pages> 298-319, </pages> <year> 1978. </year>
Reference: [25] <author> Adam L. Buchsbaum and Robert Endre Tarjan. </author> <title> Confluently persistent deques via data-structural bootstrapping. </title> <booktitle> In Proc. 4th ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 155-164, </pages> <year> 1993. </year>
Reference: [26] <author> Svante Carlsson. </author> <title> Heaps. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Lund University, Lund, Sweden, </institution> <year> 1986. </year>
Reference: [27] <author> Svante Carlsson, Patricio V. Poblete, and J. Ian Munro. </author> <title> An implicit binomial queue with constant insertion time. </title> <booktitle> In Proc. 1st Scandinavian Workshop on Algorithm Theory (SWAT), volume 318 of Lecture Notes in Computer Science, </booktitle> <pages> pages 1-13. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: The priority queue of van Leeuwen [107], the implicit priority queues of Carlsson et al. <ref> [27] </ref> and the relaxed heaps of Driscoll et al. [43], but neither of these support Meld efficiently. However, the last two do support MakeQueue, FindMin and Insert in worst case constant time and Delete and DeleteMin in worst case time O (log n). <p> They achieve amortized constant time for all operations except for the two delete operations which require amortized time O (log n). The data structure we present achieves matching worst case time bounds for all operations. Previously, this was only achieved for various strict subsets of the listed operations <ref> [15, 27, 43, 107] </ref>. <p> The guide's job is to tell us which operations to perform. This problem also arises implicitly in <ref> [15, 27, 60, 67] </ref>. But the solution presented in [60] requires time fi (k) to find which Reduce operations to perform whereas the problems in the other papers are simpler because only x 1 can be forced to increase and decrease.
Reference: [28] <author> Danny Z. Chen and Xiaobo Hu. </author> <title> Fast and efficient operations on parallel priority queues (preliminary version). </title> <booktitle> In Algorithms and Computation: 5th International Symposium, ISAAC '93, volume 834 of Lecture Notes in Computer Science, </booktitle> <pages> pages 279-287. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> The operations we support are all the commonly needed priority queue operations from the sequential setting [76] and the parallel insertion of several elements at the same time <ref> [28, 89] </ref>. MakeQueue Creates and returns a new empty priority queue. Insert (Q; e) Inserts element e into priority queue Q. Meld (Q 1 ; Q 2 ) Melds priority queues Q 1 and Q 2 . The resulting priority queue is stored in Q 1 . <p> The bounds we achieve matches those of <ref> [28] </ref> for k equal one and those of [88]. See Table 7.1. <p> This extends the result of [94]. The priority queues we present in this paper do not support the operation MultiDelete, that deletes the k smallest elements from a priority queue (where k is fixed <ref> [28, 89] </ref>). However, a possible solution is to apply the k-bandwidth idea used in [28, 89], by letting each node contain k elements instead of one. <p> This extends the result of [94]. The priority queues we present in this paper do not support the operation MultiDelete, that deletes the k smallest elements from a priority queue (where k is fixed <ref> [28, 89] </ref>). However, a possible solution is to apply the k-bandwidth idea used in [28, 89], by letting each node contain k elements instead of one. If we apply the idea to the data structure in Section 7.2 we get the time bounds in Theorem 15, improving upon the bounds achieved in [89], see Table 7.1. <p> We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 [90] [88] [89] <ref> [28] </ref> [94] This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert log log n log log n - 1 1 DeleteMin log log n log log n - 1 1 Meld log log n log n k + log log <p> Because Meld (Q; Build (e 1 ; : : : ; e k )) implements the priority queue operation MultiInsert (Q; e 1 ; : : : ; e k ) we have the corollary below. Notice that k does not have to be fixed as in <ref> [28, 89] </ref>. Corollary 7 On a CREW PRAM MultiInsert can be performed in time O (log k) with O ((log n+ k)= log k) processors. 7.5 Pipelined priority queue operations The priority queues in Section 7.2, 7.3 and 7.4 require the CREW PRAM to achieve constant time per operation. <p> Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [29] <author> Richard Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17(4) </volume> <pages> 130-145, </pages> <year> 1988. </year>
Reference-contexts: This enables us to delete all occurrences of v in adjacency lists of such vertices u j 2 S = V nS in constant time. Sorting of the adjacency lists takes O (log n) time and O (m log n) work <ref> [29] </ref>. Constructing links and building the required arrays can then be done in constant time using O (m) operations. This completes the description of the Init operation. The processors associated with vertices in S at any given instant are organized in a linear pipeline.
Reference: [30] <author> Sajal K. Das, Maria C. Pinotti, and Falguni Sarkar. </author> <title> Optimal and load balanced mapping of parallel priority queues in hypercubes. </title> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference: [31] <author> Paul F. Dietz. </author> <title> Fully persistent arrays. </title> <booktitle> In Proc. 1st Workshop on Algorithms and Data Structures (WADS), volume 382 of Lecture Notes in Computer Science, </booktitle> <pages> pages 67-74. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference: [32] <author> Paul F. Dietz. </author> <title> Heap construction in the parallel comparison tree model. </title> <booktitle> In Proc. 3rd Scandi-navian Workshop on Algorithm Theory (SWAT), volume 621 of Lecture Notes in Computer Science, </booktitle> <pages> pages 140-150. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> Table 7.1 lists the performance of different implementations adopting parallelism to priority queues. Several papers consider how to build heaps [49] optimally in parallel <ref> [32, 35, 70, 95] </ref>. On an EREW PRAM an optimal construction time of O (log n) is achieved in [95] and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in [35].
Reference: [33] <author> Paul F. Dietz and Rajeev Raman. </author> <title> Persistence, amortization and randomization. </title> <booktitle> In Proc. 2nd ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 78-88, </pages> <year> 1991. </year>
Reference: [34] <author> Paul F. Dietz and Rajeev Raman. </author> <title> A constant update time finger search tree. </title> <journal> Information Processing Letters, </journal> <volume> 52 </volume> <pages> 147-154, </pages> <year> 1994. </year>
Reference-contexts: This shows that the construction described in the previous sections is optimal among all implementations where Meld takes sublinear time. If Meld is allowed to take linear time it is possible to support DeleteMin in worst case constant time by using the finger search trees of Dietz and Raman <ref> [34] </ref>. By using their data structure MakeQueue, FindMin, DeleteMin, Delete can be supported in worst case time O (1), Insert in worst case time O (log n) and Meld in worst case time O (n).
Reference: [35] <author> Paul F. Dietz and Rajeev Raman. </author> <title> Very fast optimal parallel algorithms for heap construction. </title> <booktitle> In Proc. 6th Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 514-521, </pages> <year> 1994. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> Table 7.1 lists the performance of different implementations adopting parallelism to priority queues. Several papers consider how to build heaps [49] optimally in parallel <ref> [32, 35, 70, 95] </ref>. On an EREW PRAM an optimal construction time of O (log n) is achieved in [95] and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in [35]. <p> Several papers consider how to build heaps [49] optimally in parallel [32, 35, 70, 95]. On an EREW PRAM an optimal construction time of O (log n) is achieved in [95] and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in <ref> [35] </ref>. <p> Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [36] <author> Paul F. Dietz and Daniel D. Sleator. </author> <title> Two algorithms for maintaining order in a list. </title> <booktitle> In Proc. 19th Ann. ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 365-372, </pages> <year> 1987. </year>
Reference-contexts: Essentially what we need to know is if given the handles of two strings from W , which one of the two strings is the lexicographically smallest. A solution to this problem was given by Dietz and Sleator <ref> [36] </ref>. They presented a data structure that allows a new element to be inserted into a linked list in constant time if the new element's position is known, and that can answer order queries in constant time.
Reference: [37] <author> Edsger W. Dijkstra. </author> <title> A note on two problems in connexion with graphs. </title> <journal> Numerische Mathe-matik, </journal> <volume> 1 </volume> <pages> 269-271, </pages> <year> 1959. </year>
Reference-contexts: A notorious example is the single-source shortest path problem. The best sequential algorithm for the single-source shortest path problem on directed graphs with non-negative real valued edge weights is Dijkstra's algorithm <ref> [37] </ref>.
Reference: [38] <author> Yuzheng Ding and Mark Allen Weiss. </author> <title> The relaxed min-max heap. </title> <journal> Acta Informatica, </journal> <volume> 30 </volume> <pages> 215-231, </pages> <year> 1993. </year>
Reference-contexts: As a simple consequence of our construction we get a new implementation of meldable double ended priority queues, which is a data type that allows both FindMin/FindMax and Delete-Min/ DeleteMax <ref> [9, 38] </ref>. For each queue we just have to maintain two heap ordered trees as described in Section 5.1. One tree ordered with respect to minimum and the other with respect to maximum.
Reference: [39] <author> Ernst E. Doberkat. </author> <title> Deleting the root of a heap. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 245-265, </pages> <year> 1982. </year>
Reference: [40] <author> Ernst E. Doberkat. </author> <title> An average case analysis of Floyd's algorithm to compute heaps. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 114-131, </pages> <year> 1984. </year>
Reference: [41] <author> Danny Dolev, Yuval Harari, Nathan Linial, Noam Nisan, and Michael Parnas. </author> <title> Neighborhood preserving hashing and approximate queries. </title> <booktitle> In Proc. 5th ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 251-259, </pages> <year> 1994. </year>
Reference-contexts: Minsky and Papert originally raised this problem in [80]. Recently a sequence of papers have considered how to solve this problem efficiently <ref> [41, 42, 59, 74, 112] </ref>. Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. <p> Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. Dolev et al. <ref> [41, 42] </ref> and Greene, Parnas and Yao [59] considered approximate dictionary queries for the case where d is large. The initial effort towards a theoretical study of the small d case was given by Yao and Yao in [112].
Reference: [42] <author> Danny Dolev, Yuval Harari, and Michael Parnas. </author> <title> Finding the neighborhood of a query in a dictionary. </title> <booktitle> In Proc. 2nd Israel Symposium on Theory of Computing and Systems, </booktitle> <pages> pages 33-42, </pages> <year> 1993. </year>
Reference-contexts: Minsky and Papert originally raised this problem in [80]. Recently a sequence of papers have considered how to solve this problem efficiently <ref> [41, 42, 59, 74, 112] </ref>. Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. <p> Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. Dolev et al. <ref> [41, 42] </ref> and Greene, Parnas and Yao [59] considered approximate dictionary queries for the case where d is large. The initial effort towards a theoretical study of the small d case was given by Yao and Yao in [112].
Reference: [43] <author> James R. Driscoll, Harold N. Gabow, Ruth Shrairman, and Robert Endre Tarjan. </author> <title> Relaxed heaps: An alternative to fibonacci heaps with applications to parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 31(11) </volume> <pages> 1343-1354, </pages> <year> 1988. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> The priority queue of van Leeuwen [107], the implicit priority queues of Carlsson et al. [27] and the relaxed heaps of Driscoll et al. <ref> [43] </ref>, but neither of these support Meld efficiently. However, the last two do support MakeQueue, FindMin and Insert in worst case constant time and Delete and DeleteMin in worst case time O (log n). <p> Another interesting operation to consider is DecreaseKey. Our data structure supports DecreaseKey in worst case time O (log n), because DecreaseKey can be implemented in terms of a Delete operation followed by an Insert operation. Relaxed heaps <ref> [43] </ref> support Decrease-Key in worst case time O (1) but do not support Meld. But it is easy to see that relaxed heaps can be extended to support Meld in worst case time O (log n). <p> Research Actions Program of the EC under contract no. 7141 (project ALCOM II) and by the Danish Natural Science Research Council (Grant No. 9400044). y BRICS (Basic Research in Computer Science), a Centre of the Danish National Research Foundation. 57 Amortized Worst case Fredman et al. [53] Driscoll et al. <ref> [43] </ref> Brodal [15] New result MakeQueue O (1) O (1) O (1) O (1) FindMin O (1) O (1) O (1) O (1) Insert O (1) O (1) O (1) O (1) Meld O (1) O (log n) O (1) O (1) DecreaseKey O (1) O (1) O (log n) O <p> They achieve amortized constant time for all operations except for the two delete operations which require amortized time O (log n). The data structure we present achieves matching worst case time bounds for all operations. Previously, this was only achieved for various strict subsets of the listed operations <ref> [15, 27, 43, 107] </ref>. <p> The data structure we present achieves matching worst case time bounds for all operations. Previously, this was only achieved for various strict subsets of the listed operations [15, 27, 43, 107]. For example the relaxed heaps of Driscoll et al. <ref> [43] </ref> and the priority queues in [15] achieve the above time bounds in the worst case sense except that in [43] Meld requires worst case time fi (log n) and in [15] DecreaseKey requires worst case time fi (log n). Refer to Table 6.1. <p> Previously, this was only achieved for various strict subsets of the listed operations [15, 27, 43, 107]. For example the relaxed heaps of Driscoll et al. <ref> [43] </ref> and the priority queues in [15] achieve the above time bounds in the worst case sense except that in [43] Meld requires worst case time fi (log n) and in [15] DecreaseKey requires worst case time fi (log n). Refer to Table 6.1. If we ignore the Delete operation our results are optimal in the following sense. <p> The data structure presented in this paper originates from the same ideas as the relaxed heaps of Driscoll et al. <ref> [43] </ref>. In [43] the data structure is based on heap ordered trees where fi (log n) nodes may violate heap order. <p> The data structure presented in this paper originates from the same ideas as the relaxed heaps of Driscoll et al. <ref> [43] </ref>. In [43] the data structure is based on heap ordered trees where fi (log n) nodes may violate heap order. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. <p> in O (m + n log n) operations by using efficient priority queues like Fibonacci heaps [53] for maintaining tentative distances, or other priority queue implementations supporting deletion of the minimum key element in amortized or worst case logarithmic time, and decrease key in amortized or worst case constant time <ref> [18, 43, 63] </ref>. fl This work was partially supported by the EU ESPRIT LTR Project No. 20244 (ALCOM-IT), and by the DFG project SFB 124-D6 (VLSI Entwurfsmethoden und Parallelitat). y Supported by the Danish Natural Science Research Council (Grant No. 9400044). z BRICS (Basic Research in Computer Science), a Centre of <p> For this parallelization it is important that the priority queue operations have worst case running time, and therefore the original Fibonacci heap cannot be used to implement the local queues. This was first observed in <ref> [43] </ref> where a new data structure, called relaxed heaps, was developed to overcome this problem. Using relaxed heaps, an O (n log n) time and O (m + n log n) work (-optimal) parallel implementation of Dijkstra's algorithm is obtained. <p> Our bounds are strongly polynomial and speed up the best previous ones <ref> [43] </ref> by a logarithmic factor.
Reference: [44] <author> James R. Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert Endre Tarjan. </author> <title> Making data structures persistent. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 38 </volume> <pages> 86-124, </pages> <year> 1989. </year> <month> 117 </month>
Reference: [45] <author> James R. Driscoll, Daniel D. K. Sleator, and Robert Endre Tarjan. </author> <title> Fully persistent lists with catenation. </title> <booktitle> In Proc. 2nd ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 89-99, </pages> <year> 1991. </year>
Reference: [46] <author> Rolf Fagerberg. </author> <title> A generalization of binomial queues. </title> <journal> Information Processing Letters, </journal> <volume> 57 </volume> <pages> 109-114, </pages> <year> 1996. </year>
Reference: [47] <author> Michael J. Fischer and Michael S. Paterson. Fishspear: </author> <title> A priority queue algorithm. </title> <journal> Journal of the ACM, </journal> <volume> 41(1) </volume> <pages> 3-30, </pages> <year> 1994. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time.
Reference: [48] <author> Rudolf Fleischer. </author> <title> A simple balanced search tree with O(1) worst-case update time. </title> <booktitle> In Algorithms and Computation: 4th International Symposium, ISAAC '93, volume 762 of Lecture Notes in Computer Science, </booktitle> <pages> pages 138-146. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference: [49] <author> Robert W. Floyd. </author> <title> Algorithm 245: Treesort3. </title> <journal> Communications of the ACM, </journal> <volume> 7(12):701, </volume> <year> 1964. </year>
Reference-contexts: Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. <p> Table 7.1 lists the performance of different implementations adopting parallelism to priority queues. Several papers consider how to build heaps <ref> [49] </ref> optimally in parallel [32, 35, 70, 95]. On an EREW PRAM an optimal construction time of O (log n) is achieved in [95] and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in [35].
Reference: [50] <author> Edward Fredkin. </author> <title> Trie memory. </title> <journal> Communications of the ACM, </journal> <volume> 3 </volume> <pages> 490-499, </pages> <year> 1962. </year>
Reference-contexts: We present the corresponding data structure of size O (nm) for the 1-query case. We present a simple data structure based on tries <ref> [2, 50] </ref> which has optimal size O (nm) and supports 1-queries in time O (m). Unfortunately, we do not know how to construct the data structure in time O (nm) and we leave this as an open problem. <p> The strings in the dictionary W are called dictionary strings. We let dist H (u; v) denote the Hamming distance between the two strings u and v. The basic component of our data structure is a trie <ref> [50] </ref>. A trie, also called a digital search tree, is a tree representation of a set of strings. In a trie all edges are labeled by symbols such that every string corresponds to a path in the trie.
Reference: [51] <author> Michael L. Fredman, Janos Komlos, and Endre Szemeredi. </author> <title> Storing a sparse table with O(1) worst case access time. </title> <journal> Journal of the ACM, </journal> <volume> 31(3) </volume> <pages> 538-544, </pages> <year> 1984. </year>
Reference-contexts: This research was done while visiting the Max-Planck Institut fur Informatik, Saabrucken, Germany. y Basic Research in Computer Science, a Centre of the Danish National Research Foundation. z On leave from Institute of Informatics, Warsaw University, ul. Banacha 2, 02-097, Warszawa, Poland. WWW: http://zaa.mimuw.edu.pl/lechu/lechu.html. 107 and Szemeredi <ref> [51] </ref>. On the other hand d-queries can be answered in time O (m) when the size of the data structure can be O (n P d m ). We present the corresponding data structure of size O (nm) for the 1-query case.
Reference: [52] <author> Michael L. Fredman, Robert Sedgewick, Daniel D. Sleator, and Robert Endre Tarjan. </author> <title> The pairing heap: A new form of self-adjusting heap. </title> <journal> Algorithmica, </journal> <volume> 1 </volume> <pages> 111-129, </pages> <year> 1986. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95].
Reference: [53] <author> Michael L. Fredman and Robert Endre Tarjan. </author> <title> Fibonacci heaps and their uses in improved network optimization algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 34(3) </volume> <pages> 596-615, </pages> <year> 1987. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> In Section 5.3 we show that our construction is optimal. Section 5.4 contains some final remarks. 5.1 The Data Structure Our basic representation of a priority queue is a heap ordered tree where each node contains one element. This is slightly different from binomial heaps [108] and Fibonacci heaps <ref> [53] </ref> where the representation is a forest of heap ordered trees. With each node we associate a rank and we partition the children of a node into two types, type i and type ii. <p> By increasing the rank of the node with the smallest element to r + 1 the properties a) to d) are satisfied. The operation is illustrated in Figure 5.3. This is similar to the linking of trees in binomial heaps and Fibonacci heaps <ref> [108, 53] </ref>. C C fl fl " fl fl C C fl fl C C fl fl C C r r r + 1 We now describe how to implement the operations. * MakeQueue is trivial. <p> the ESPRIT II Basic Research Actions Program of the EC under contract no. 7141 (project ALCOM II) and by the Danish Natural Science Research Council (Grant No. 9400044). y BRICS (Basic Research in Computer Science), a Centre of the Danish National Research Foundation. 57 Amortized Worst case Fredman et al. <ref> [53] </ref> Driscoll et al. [43] Brodal [15] New result MakeQueue O (1) O (1) O (1) O (1) FindMin O (1) O (1) O (1) O (1) Insert O (1) O (1) O (1) O (1) Meld O (1) O (log n) O (1) O (1) DecreaseKey O (1) O (1) <p> In the amortized sense, [101], the best performance for these operations is achieved by Fibonacci heaps <ref> [53] </ref>. They achieve amortized constant time for all operations except for the two delete operations which require amortized time O (log n). The data structure we present achieves matching worst case time bounds for all operations. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. <p> For an n-vertex, m-edge digraph the algorithm can be implemented to run in O (m + n log n) operations by using efficient priority queues like Fibonacci heaps <ref> [53] </ref> for maintaining tentative distances, or other priority queue implementations supporting deletion of the minimum key element in amortized or worst case logarithmic time, and decrease key in amortized or worst case constant time [18, 43, 63]. fl This work was partially supported by the EU ESPRIT LTR Project No. 20244
Reference: [54] <author> Michael L. Fredman and Dan E. Willard. </author> <title> Surpassing the information theoretic bound with fusion trees. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 424-436, </pages> <year> 1993. </year>
Reference-contexts: The space requirement of Andersson's data structure is O (n + 2 *w ). Several data structures can achieve the same time bounds as Andersson [5], but they all require constant time multiplication <ref> [6, 54, 93] </ref>. The main result of this paper is Theorem 22 stated below. The theorem requires the notion of smooth functions. Overmars [86] defines a nondecreasing function f to be smooth if and only if f (O (n)) = O (f (n)).
Reference: [55] <author> Harold N. Gabow and Robert Endre Tarjan. </author> <title> A linear-time algorithm for a special case of disjoint set union. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 30 </volume> <pages> 209-221, </pages> <year> 1985. </year>
Reference-contexts: We denote such an element to be marked. For this purpose we use the Find-Split-Add data structure of Imai and Asano [65], an extension of a data structure by Gabow and Tarjan <ref> [55] </ref>. The data structure supports the following operations: Given a pointer to an element in a list, to find the closest marked element (Find); to mark an unmarked list element (Split); and to insert a new unmarked element into the list adjacent to an element in the list (Add).
Reference: [56] <author> Andrew V. Goldberg, Serge A. Plotkin, and Pravin M. Vaidya. </author> <title> Sublinear-time parallel algorithms for matching and related problems. </title> <journal> Journal of Algorithms, </journal> <volume> 14(2) </volume> <pages> 180-213, </pages> <year> 1993. </year>
Reference-contexts: special case of unary weights [81, 69], and a weakly polynomial CRCW PRAM algorithm exists that that runs in O (n 2=3 log 2 n log (nC)) time and performs O (n 11=3 log 2 n log (nC)) work for the case of integer weights in the range [C; C] <ref> [56] </ref>. Our bounds are strongly polynomial and speed up the best previous ones [43] by a logarithmic factor.
Reference: [57] <author> Leslie M. Goldschlager, Ralph A. Shaw, and John Staples. </author> <title> The maximum flow problem is LOGSPACE complete for P. </title> <journal> Theoretical Computer Science, </journal> <volume> 21 </volume> <pages> 105-111, </pages> <year> 1982. </year>
Reference-contexts: We mention here the assignment problem, the minimum cost flow problem, (for definitions see [3]), and the single-source shortest path problem in planar digraphs. The minimum cost flow problem (which is P-complete <ref> [57] </ref>) can be solved by O (m log n) calls to Dijkstra's algorithm (see e.g. [3, Section 10.7]). Using our implementation, we obtain a parallel algorithm that runs in O (nm log n) time and performs O (m 2 log 2 n) work.
Reference: [58] <author> Gaston H. Gonnet and J. Ian Munro. </author> <title> Heaps on heaps. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15(4) </volume> <pages> 964-971, </pages> <year> 1986. </year>
Reference: [59] <author> Dan Greene, Michal Parnas, and Frances Yao. </author> <title> Multi-index hashing for information retrieval. </title> <booktitle> In Proc. 35th Ann. Symp. on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 722-731, </pages> <year> 1994. </year>
Reference-contexts: Minsky and Papert originally raised this problem in [80]. Recently a sequence of papers have considered how to solve this problem efficiently <ref> [41, 42, 59, 74, 112] </ref>. Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. <p> Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. Dolev et al. [41, 42] and Greene, Parnas and Yao <ref> [59] </ref> considered approximate dictionary queries for the case where d is large. The initial effort towards a theoretical study of the small d case was given by Yao and Yao in [112].
Reference: [60] <author> Leo J. Guibas, Edward M. McCreight, Michael F. Plass, and Janet R. Roberts. </author> <title> A new representation for linear lists. </title> <booktitle> In Proc. 9th Ann. ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 49-60, </pages> <year> 1977. </year>
Reference-contexts: Lemma 9 shows that the size is at least exponential in the rank. The last two properties are essential to achieve Meld in worst case constant time. The regularity constraint c) is a variation of the regularity constraint that Guibas et al. <ref> [60] </ref> used in their construction of finger search trees. The idea is that between two ranks where three children have equal rank there is a rank of which there only is one child. <p> The table shows all the possible cases. Recall that c) states that between every two n i = 3 there is at least one n i = 1. The different cases are also considered in <ref> [60] </ref>. y 1 1 fl y 1 2 y 2 23y 1 1 fl y 2 31y 1 2 x3y 3 1y 2 23y 1 1 fl x3y 3 1y 2 31y 1 2 y 1 22 fl y 1 31 x3y 2 1y 1 22 fl x3y 2 1y 1 <p> The guide's job is to tell us which operations to perform. This problem also arises implicitly in <ref> [15, 27, 60, 67] </ref>. But the solution presented in [60] requires time fi (k) to find which Reduce operations to perform whereas the problems in the other papers are simpler because only x 1 can be forced to increase and decrease. <p> The guide's job is to tell us which operations to perform. This problem also arises implicitly in [15, 27, 60, 67]. But the solution presented in <ref> [60] </ref> requires time fi (k) to find which Reduce operations to perform whereas the problems in the other papers are simpler because only x 1 can be forced to increase and decrease.
Reference: [61] <author> Y. Han, Victor Pan, and John H. Reif. </author> <title> Algorithms for computing all pair shortest paths in directed graphs. </title> <booktitle> In Proc. 4th ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 353-362, </pages> <year> 1992. </year> <month> 118 </month>
Reference-contexts: The best NC algorithm runs in O (log 2 n) time and performs O (n 3 (log log n= log n) 1=3 ) work on an EREW PRAM <ref> [61] </ref>. Moreover, work efficient algorithms which are (at least) sublinearly fast are also not known for general digraphs. Dijkstra's algorithm is highly sequential, and can probably not be used as a basis for a fast (NC) parallel algorithm.
Reference: [62] <author> Godfrey H. Hardy, John E. Littlewood, and Gyorgy Polya. </author> <title> Inequalities. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1952. </year>
Reference: [63] <author> Peter Hoyer. </author> <title> A general technique for implementation of efficient priority queues. </title> <booktitle> In Proc. 3rd Israel Symposium on Theory of Computing and Systems, </booktitle> <pages> pages 57-66, </pages> <year> 1995. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> in O (m + n log n) operations by using efficient priority queues like Fibonacci heaps [53] for maintaining tentative distances, or other priority queue implementations supporting deletion of the minimum key element in amortized or worst case logarithmic time, and decrease key in amortized or worst case constant time <ref> [18, 43, 63] </ref>. fl This work was partially supported by the EU ESPRIT LTR Project No. 20244 (ALCOM-IT), and by the DFG project SFB 124-D6 (VLSI Entwurfsmethoden und Parallelitat). y Supported by the Danish Natural Science Research Council (Grant No. 9400044). z BRICS (Basic Research in Computer Science), a Centre of
Reference: [64] <author> Scott Huddleston and Kurt Mehlhorn. </author> <title> A new data structure for representing sorted lists. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 157-184, </pages> <year> 1982. </year>
Reference-contexts: In the comparison model Insert, Delete and Pred can be supported in worst case O (log n) time and FindMin and FindMax in worst case constant time by a balanced search tree, say an (a; b)-tree <ref> [64] </ref>. For the comparison model a tradeoff between the operations has been shown by Brodal et al. [20]. The tradeoff shown in [20] is that if Insert and Delete take worst case O (t (n)) time then FindMin (and FindMax) requires at least worst case n=2 O (t (n)) time.
Reference: [65] <author> Hiroshi Imai and Taka Asano. </author> <title> Dynamic orthogonal segment intersection search. </title> <journal> Journal of Algorithms, </journal> <volume> 8 </volume> <pages> 1-18, </pages> <year> 1987. </year>
Reference-contexts: We can now find the closest element to e in L x by finding the closest element in L v that has a non null pointer. We denote such an element to be marked. For this purpose we use the Find-Split-Add data structure of Imai and Asano <ref> [65] </ref>, an extension of a data structure by Gabow and Tarjan [55].
Reference: [66] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Because our priority queues can report a minimum element in constant time and that there is lower bound of (log n) for finding the minimum of a set of elements on a CREW PRAM <ref> [66] </ref> we have an (log n) lower bound on the construction time on a CREW PRAM. We now give a matching upper bound on an EREW PRAM. First a collection of trees is constructed satisfying B 1 and B 3 but not B 2 .
Reference: [67] <author> Haim Kaplan and Robert Endre Tarjan. </author> <title> Persistent lists with catenation via recursive slowdown. </title> <booktitle> In Proc. 27th Ann. ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 93-102, </pages> <year> 1995. </year>
Reference-contexts: The guide's job is to tell us which operations to perform. This problem also arises implicitly in <ref> [15, 27, 60, 67] </ref>. But the solution presented in [60] requires time fi (k) to find which Reduce operations to perform whereas the problems in the other papers are simpler because only x 1 can be forced to increase and decrease.
Reference: [68] <author> Haim Kaplan and Robert Endre Tarjan. </author> <title> Purely functional representations of catenable sorted lists. </title> <booktitle> In Proc. 28th Ann. ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 202-211, </pages> <year> 1996. </year>
Reference: [69] <author> Richard M. Karp, Eli Upfal, and Avi Wigderson. </author> <title> Constructing a maximum matching is in random NC. </title> <journal> Combinatorica, </journal> <volume> 6(1) </volume> <pages> 35-38, </pages> <year> 1986. </year>
Reference-contexts: Using our implementation, we obtain a parallel algorithm that runs in O (n 2 ) time and performs O (nm log n) work. The assignment problem is not known to be in NC, but an RNC algorithm exists for the special case of unary weights <ref> [81, 69] </ref>, and a weakly polynomial CRCW PRAM algorithm exists that that runs in O (n 2=3 log 2 n log (nC)) time and performs O (n 11=3 log 2 n log (nC)) work for the case of integer weights in the range [C; C] [56].
Reference: [70] <author> Chan M. Khoong. </author> <title> Optimal parallel construction of heaps. </title> <journal> Information Processing Letters, </journal> <volume> 48 </volume> <pages> 159-161, </pages> <year> 1993. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> Table 7.1 lists the performance of different implementations adopting parallelism to priority queues. Several papers consider how to build heaps [49] optimally in parallel <ref> [32, 35, 70, 95] </ref>. On an EREW PRAM an optimal construction time of O (log n) is achieved in [95] and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in [35].
Reference: [71] <author> Donald E. Knuth. </author> <title> The Art of Computer Programming, Volume III: Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference: [72] <author> Frank Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: That a systolic processor array with fi (n) processors can implement a priority queue supporting the operations Insert and DeleteMin in constant time is parallel computing folklore, see Exercise 1.119 in <ref> [72] </ref>. Recently Ranade et al. [94] showed how to achieve the same bounds on a processor array with only O (log n) processors. In Section 7.5 we describe how the priority queues can be modified to allow operations to be performed via pipelining.
Reference: [73] <author> Christos Levcopoulos and Mark H. Overmars. </author> <title> A balanced search tree with O(1) worst-case update time. </title> <journal> Acta Informatica, </journal> <volume> 26 </volume> <pages> 269-277, </pages> <year> 1988. </year>
Reference: [74] <author> Udi Manber and Sun Wu. </author> <title> An algorithm for approximate membership checking with application to password security. </title> <journal> Information Processing Letters, </journal> <volume> 50 </volume> <pages> 191-197, </pages> <year> 1994. </year>
Reference-contexts: Minsky and Papert originally raised this problem in [80]. Recently a sequence of papers have considered how to solve this problem efficiently <ref> [41, 42, 59, 74, 112] </ref>. Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques. <p> Minsky and Papert originally raised this problem in [80]. Recently a sequence of papers have considered how to solve this problem efficiently [41, 42, 59, 74, 112]. Manber and Wu <ref> [74] </ref> considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files. Their method is based on Bloom filters [12] and uses hashing techniques.
Reference: [75] <author> Colin McDiarmid. </author> <title> Average-case lower bounds for searching. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17(5) </volume> <pages> 1044-1060, </pages> <year> 1988. </year>
Reference: [76] <author> Kurt Mehlhorn and Athanasios K. Tsakalidis. </author> <title> Data structures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity. </booktitle> <publisher> MIT Press/Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: Category: E.1 Keywords: priority queues, meld, decrease key, worst case complexity 6.1 Introduction We consider the problem of implementing priority queues which are efficient in the worst case sense. The operations we want to support are the following commonly needed priority queue operations <ref> [76] </ref>. MakeQueue creates and returns an empty priority queue. FindMin (Q) returns the minimum element contained in priority queue Q. Insert (Q; e) inserts an element e into priority queue Q. <p> The construction of priority queues is a classical topic in data structures [15, 27, 43, 46, 49, 52, 53, 71, 97, 107, 108, 109]. A historical overview of implementations can be found in <ref> [76] </ref>. There are many applications of priority queues. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in <ref> [76] </ref>. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. The operations we support are all the commonly needed priority queue operations from the sequential setting <ref> [76] </ref> and the parallel insertion of several elements at the same time [28, 89]. MakeQueue Creates and returns a new empty priority queue. Insert (Q; e) Inserts element e into priority queue Q. Meld (Q 1 ; Q 2 ) Melds priority queues Q 1 and Q 2 .
Reference: [77] <author> Peter Bro Miltersen. </author> <title> Lower bounds for Union-Split-Find related problems on random access machines. </title> <booktitle> In Proc. 26th Ann. ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 625-634, </pages> <year> 1994. </year>
Reference-contexts: The data structure of Thorup does not support predecessor queries but Thorup mentions that an (log 1=3o (1) n) lower bound for Pred can be extracted from <ref> [77, 79] </ref>. The space requirement of Thorup's data structure is O (n2 *w ) (if the time bounds are amortized the space requirement is O (n + 2 *w )).
Reference: [78] <author> Peter Bro Miltersen. </author> <title> Lower bounds for static dictionaries on RAMs with bit operations but no multiplications. </title> <booktitle> In Proc. 23rd Int. Colloquium on Automata, Languages and Programming (ICALP), volume 1099 of Lecture Notes in Computer Science, </booktitle> <pages> pages 442-453. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year> <month> 119 </month>
Reference-contexts: Miltersen <ref> [78] </ref> refers to this model as a Practical RAM. We assume the elements are integers in the range 0::2 w 1. A tradeoff similar to the one for the comparison model [20] is not known for a Practical RAM.
Reference: [79] <author> Peter Bro Miltersen, Noam Nisan, Shmuel Safra, and Avi Wigderson. </author> <title> On data structures and asymmetric communication complexity. </title> <booktitle> In Proc. 27th Ann. ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 103-111, </pages> <year> 1995. </year>
Reference-contexts: The data structure of Thorup does not support predecessor queries but Thorup mentions that an (log 1=3o (1) n) lower bound for Pred can be extracted from <ref> [77, 79] </ref>. The space requirement of Thorup's data structure is O (n2 *w ) (if the time bounds are amortized the space requirement is O (n + 2 *w )).
Reference: [80] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1969. </year>
Reference-contexts: We are interested in answering d-queries, i.e., for any query string ff 2 f0; 1g m to decide if there is a string w i in W with at most Hamming distance d of ff. Minsky and Papert originally raised this problem in <ref> [80] </ref>. Recently a sequence of papers have considered how to solve this problem efficiently [41, 42, 59, 74, 112]. Manber and Wu [74] considered the application of approximate dictionary queries to password security and spelling correction of bibliographic files.
Reference: [81] <author> Ketan Mulmuley, Umesh V. Vazirani, and Vijay V. Vazirani. </author> <title> Matching is as easy as matrix inversion. </title> <journal> Combinatorica, </journal> <volume> 7(1) </volume> <pages> 105-113, </pages> <year> 1987. </year>
Reference-contexts: Using our implementation, we obtain a parallel algorithm that runs in O (n 2 ) time and performs O (nm log n) work. The assignment problem is not known to be in NC, but an RNC algorithm exists for the special case of unary weights <ref> [81, 69] </ref>, and a weakly polynomial CRCW PRAM algorithm exists that that runs in O (n 2=3 log 2 n log (nC)) time and performs O (n 11=3 log 2 n log (nC)) work for the case of integer weights in the range [C; C] [56].
Reference: [82] <author> J. Ian Munro and Hendra Suwanda. </author> <title> Implicit data structures for fast search and update. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 236-250, </pages> <year> 1980. </year>
Reference: [83] <author> Chris Okasaki. </author> <title> Purely functional random-access lists. </title> <booktitle> In Functional Programming Languages and Computer Architecutre, </booktitle> <pages> pages 86-95, </pages> <year> 1995. </year>
Reference-contexts: Note that the only restructuring required is to make e an incoming edge of the new node w. The described approach for relinking has recently been applied in a different context to construct purely functional random-access lists <ref> [83] </ref>. In [83] it is proved that a sequence of trees satisfying (8.1) is unique for a given number of nodes. 8.3.2 A work efficient implementation In the following we let the output queue of processor P i be denoted Q out (i) . <p> Note that the only restructuring required is to make e an incoming edge of the new node w. The described approach for relinking has recently been applied in a different context to construct purely functional random-access lists <ref> [83] </ref>. In [83] it is proved that a sequence of trees satisfying (8.1) is unique for a given number of nodes. 8.3.2 A work efficient implementation In the following we let the output queue of processor P i be denoted Q out (i) .
Reference: [84] <author> Chris Okasaki. </author> <title> Simple and efficient purely functional queues and deques. </title> <journal> Journal of Functional Programming, </journal> <month> October </month> <year> 1995. </year>
Reference: [85] <author> Chris Okasaki. </author> <title> Purely Functional Data Structures. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1996. </year> <note> Tech report CMU-CS-96-177. </note>
Reference: [86] <author> Mark H. Overmars. </author> <title> The Design of Dynamic Data Structures, </title> <booktitle> volume 156 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1983. </year>
Reference-contexts: Several data structures can achieve the same time bounds as Andersson [5], but they all require constant time multiplication [6, 54, 93]. The main result of this paper is Theorem 22 stated below. The theorem requires the notion of smooth functions. Overmars <ref> [86] </ref> defines a nondecreasing function f to be smooth if and only if f (O (n)) = O (f (n)). <p> In the description of our data structure we in the following assume n is a constant such that the current number of integers in the set is fi (n). This can be satisfied by using the general dynamization technique described by Overmars <ref> [86] </ref>, which requires f (n) to be smooth. <p> The rebuilding of T when O (n) Delete operations have been performed can be handled by the general dynamization technique of Overmars <ref> [86] </ref> in worst case O (log k) time per operation. For details refer to [86]. What remains to be described is how to handle the cases when L or R becomes empty and when I r or D r becomes full. <p> The rebuilding of T when O (n) Delete operations have been performed can be handled by the general dynamization technique of Overmars <ref> [86] </ref> in worst case O (log k) time per operation. For details refer to [86]. What remains to be described is how to handle the cases when L or R becomes empty and when I r or D r becomes full. The basic idea is to handle these cases by simply avoiding them. Below we outline the necessary changes to the amortized solution.
Reference: [87] <author> Richard C. Paige and Clyde P. Kruskal. </author> <title> Parallel algorithms for shortest path problems. </title> <booktitle> In Int. Conference on Parallel Processing, </booktitle> <pages> pages 14-20, </pages> <year> 1985. </year>
Reference-contexts: Dijkstra's algorithm is highly sequential, and can probably not be used as a basis for a fast (NC) parallel algorithm. However, it is easy to give a parallel implementation of the algorithm that runs in O (n log n) time <ref> [87] </ref>. The idea is to perform the distance updates within each iteration in parallel by associating a local priority queue with each processor. The vertex of minimum distance for the next iteration is determined (in parallel) as the minimum of the minima in the local priority queues. <p> Hence, the approach with processor local priority queues does not seem to make it possible to improve the running time beyond O (n log n) without resorting to a more powerful PRAM model. This was considered in <ref> [87] </ref> where two faster (but not work efficient) implementations of Dijkstra's algorithm were given on a CRCW PRAM: the first (resp. second) algorithm runs in O (n log log n) (resp. O (n)) time, and performs O (n 2 ) (resp.
Reference: [88] <author> Maria C. Pinotti, Sajal K. Das, and Vincenzo A. Crupi. </author> <title> Parallel and distributed meldable priority queues based on binomial heaps. </title> <booktitle> In Int. Conference on Parallel Processing, </booktitle> <year> 1996. </year>
Reference-contexts: The bounds we achieve matches those of [28] for k equal one and those of <ref> [88] </ref>. See Table 7.1. Corollary 6 On an EREW PRAM priority queues exist supporting FindMin in constant time with one processor, and supporting MakeQueue, Insert, Meld, DeleteMin, Delete and DecreaseKey in time O (log log n) with O (log n= log log n) processors. <p> We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 [90] <ref> [88] </ref> [89] [28] [94] This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert log log n log log n - 1 1 DeleteMin log log n log log n - 1 1 Meld log log n log n k + <p> Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [89] <author> Maria C. Pinotti and Geppino Pucci. </author> <title> Parallel priority queues. </title> <journal> Information Processing Letters, </journal> <volume> 40 </volume> <pages> 33-40, </pages> <year> 1991. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> The operations we support are all the commonly needed priority queue operations from the sequential setting [76] and the parallel insertion of several elements at the same time <ref> [28, 89] </ref>. MakeQueue Creates and returns a new empty priority queue. Insert (Q; e) Inserts element e into priority queue Q. Meld (Q 1 ; Q 2 ) Melds priority queues Q 1 and Q 2 . The resulting priority queue is stored in Q 1 . <p> This extends the result of [94]. The priority queues we present in this paper do not support the operation MultiDelete, that deletes the k smallest elements from a priority queue (where k is fixed <ref> [28, 89] </ref>). However, a possible solution is to apply the k-bandwidth idea used in [28, 89], by letting each node contain k elements instead of one. <p> This extends the result of [94]. The priority queues we present in this paper do not support the operation MultiDelete, that deletes the k smallest elements from a priority queue (where k is fixed <ref> [28, 89] </ref>). However, a possible solution is to apply the k-bandwidth idea used in [28, 89], by letting each node contain k elements instead of one. If we apply the idea to the data structure in Section 7.2 we get the time bounds in Theorem 15, improving upon the bounds achieved in [89], see Table 7.1. <p> If we apply the idea to the data structure in Section 7.2 we get the time bounds in Theorem 15, improving upon the bounds achieved in <ref> [89] </ref>, see Table 7.1. We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 [90] [88] [89] [28] [94] This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert <p> If we apply the idea to the data structure in Section 7.2 we get the time bounds in Theorem 15, improving upon the bounds achieved in <ref> [89] </ref>, see Table 7.1. We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 [90] [88] [89] [28] [94] This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert log log n log log n - 1 1 DeleteMin log log n <p> idea to the data structure in Section 7.2 we get the time bounds in Theorem 15, improving upon the bounds achieved in <ref> [89] </ref>, see Table 7.1. We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 [90] [88] [89] [28] [94] This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert log log n log log n - 1 1 DeleteMin log log n log log n - 1 1 Meld log log n log n k + log <p> Because Meld (Q; Build (e 1 ; : : : ; e k )) implements the priority queue operation MultiInsert (Q; e 1 ; : : : ; e k ) we have the corollary below. Notice that k does not have to be fixed as in <ref> [28, 89] </ref>. Corollary 7 On a CREW PRAM MultiInsert can be performed in time O (log k) with O ((log n+ k)= log k) processors. 7.5 Pipelined priority queue operations The priority queues in Section 7.2, 7.3 and 7.4 require the CREW PRAM to achieve constant time per operation. <p> Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [90] <author> Maria C. Pinotti and Geppino Pucci. </author> <title> Parallel algorithms for priority queue operations. </title> <journal> Theoretical Computer Science, </journal> <volume> 148(1) </volume> <pages> 171-180, </pages> <year> 1995. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting <ref> [90, 94] </ref>. The operations we support are all the commonly needed priority queue operations from the sequential setting [76] and the parallel insertion of several elements at the same time [28, 89]. MakeQueue Creates and returns a new empty priority queue. <p> We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 <ref> [90] </ref> [88] [89] [28] [94] This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert log log n log log n - 1 1 DeleteMin log log n log log n - 1 1 Meld log log n log n k <p> Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [91] <author> Thomas Porter and Istvan Simon. </author> <title> Random insertion into a priority queue structure. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 1(3) </volume> <pages> 292-298, </pages> <year> 1975. </year>
Reference: [92] <author> Rajeev Raman. </author> <title> Eliminating Amortization: On Data Structures with Guaranteed Response Time. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <address> New York, </address> <year> 1992. </year> <institution> Computer Science Dept., U. Rochester, </institution> <note> tech report TR-439. </note>
Reference: [93] <author> Rajeev Raman. </author> <title> Priority queues: Small, monotone and trans-dichotomous. </title> <booktitle> In ESA '96, Algorithms, volume 1136 of Lecture Notes in Computer Science, </booktitle> <pages> pages 121-137. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: The space requirement of Andersson's data structure is O (n + 2 *w ). Several data structures can achieve the same time bounds as Andersson [5], but they all require constant time multiplication <ref> [6, 54, 93] </ref>. The main result of this paper is Theorem 22 stated below. The theorem requires the notion of smooth functions. Overmars [86] defines a nondecreasing function f to be smooth if and only if f (O (n)) = O (f (n)).
Reference: [94] <author> Abhiram Ranade, Szu-Tsung Cheng, Etienne Deprit, Jeff Jones, and Sun-Inn Shih. </author> <title> Parallelism and locality in priority queues. </title> <booktitle> In Proc. 6th Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 490-496, </pages> <year> 1994. </year>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting <ref> [90, 94] </ref>. The operations we support are all the commonly needed priority queue operations from the sequential setting [76] and the parallel insertion of several elements at the same time [28, 89]. MakeQueue Creates and returns a new empty priority queue. <p> That a systolic processor array with fi (n) processors can implement a priority queue supporting the operations Insert and DeleteMin in constant time is parallel computing folklore, see Exercise 1.119 in [72]. Recently Ranade et al. <ref> [94] </ref> showed how to achieve the same bounds on a processor array with only O (log n) processors. In Section 7.5 we describe how the priority queues can be modified to allow operations to be performed via pipelining. <p> As a result we get an implementation of priority queues on a processor array with O (log n) processors, supporting the operations MakeQueue, Insert, Meld, FindMin, DeleteMin, Delete and DecreaseKey in constant time. This extends the result of <ref> [94] </ref>. The priority queues we present in this paper do not support the operation MultiDelete, that deletes the k smallest elements from a priority queue (where k is fixed [28, 89]). <p> We omit the details and refer the reader to [89]. 1 All logarithms in this paper are to the base two. 70 [90] [88] [89] [28] <ref> [94] </ref> This paper Model EREW EREW 2 CREW EREW Array CREW FindMin 1 log log n 1 1 1 1 Insert log log n log log n - 1 1 DeleteMin log log n log log n - 1 1 Meld log log n log n k + log log k <p> As a consequence we get an implementation of priority queues on a processor array of size O (log n) supporting priority queue operations in constant time. On a processor array we assume that all requests are entered at processor zero and that output is generated at processor zero too <ref> [94] </ref>. The basic idea is to represent a priority queue by a forest of heap ordered binomial trees as in Section 7.2, and to perform the operations sequentially in a loop that does constant work for each rank in increasing rank order. <p> Unfortunately, no known parallel priority queues support such an operation; they only support a multi-delete operation which assumes that the k elements to be deleted are the k elements with smallest priority in the priority queue (see e.g., <ref> [17, 28, 35, 88, 89, 90, 94] </ref>). A different idea is required to improve upon the running time. We present a parallel priority data structure that speeds up the parallel implementation of Dijkstra's algorithm, by supporting the operations required at each iteration in O (1) time.
Reference: [95] <author> Nageswara S. V. Rao and Weixiong Zhang. </author> <title> Building heaps in parallel. </title> <journal> Information Processing Letters, </journal> <volume> 37 </volume> <pages> 355-358, </pages> <year> 1991. </year> <month> 120 </month>
Reference-contexts: Some references are [15, 18, 43, 49, 52, 53, 108, 109]. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines <ref> [28, 32, 35, 70, 89, 90, 94, 95] </ref>. In this paper we focus on how to achieve optimal speedup for the individual priority queue operations known from the sequential setting [90, 94]. <p> Table 7.1 lists the performance of different implementations adopting parallelism to priority queues. Several papers consider how to build heaps [49] optimally in parallel <ref> [32, 35, 70, 95] </ref>. On an EREW PRAM an optimal construction time of O (log n) is achieved in [95] and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in [35]. <p> Table 7.1 lists the performance of different implementations adopting parallelism to priority queues. Several papers consider how to build heaps [49] optimally in parallel [32, 35, 70, 95]. On an EREW PRAM an optimal construction time of O (log n) is achieved in <ref> [95] </ref> and on a CRCW PRAM an optimal construction time of O (log log n) is achieved in [35].
Reference: [96] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Concurrent access of priority queues. </title> <journal> IEEE Transac--tions on Computers, </journal> <volume> 37(12) </volume> <pages> 1657-1665, </pages> <year> 1988. </year>
Reference: [97] <author> Jorg-Rudiger Sack and Thomas Strothotte. </author> <title> An algorithm for merging heaps. </title> <journal> Acta Informat-ica, </journal> <volume> 22 </volume> <pages> 171-186, </pages> <year> 1985. </year>
Reference: [98] <author> Neil Sarnak and Robert Endre Tarjan. </author> <title> Planar point location using persistent search trees. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 669-679, </pages> <year> 1986. </year>
Reference: [99] <author> Daniel Dominic Sleator and Robert Endre Tarjan. </author> <title> Self adjusting heaps. </title> <journal> SIAM Journal of Computing, </journal> <volume> 15(1) </volume> <pages> 52-68, </pages> <year> 1986. </year>
Reference: [100] <author> Robert Endre Tarjan. </author> <title> Data Structures and Network Algortihms. </title> <booktitle> Society for Industrial and Applied Matehmatics, </booktitle> <address> Philadelphia, Pennsylvania, </address> <year> 1983. </year>
Reference-contexts: A historical overview of implementations can be found in [76]. There are many applications of priority queues. Two of the most prominent examples are sorting problems and network optimization problems <ref> [100] </ref>. fl This work was partially supported by the ESPRIT II Basic Research Actions Program of the EC under contract no. 7141 (project ALCOM II) and by the Danish Natural Science Research Council (Grant No. 9400044). y BRICS (Basic Research in Computer Science), a Centre of the Danish National Research Foundation.
Reference: [101] <author> Robert Endre Tarjan. </author> <title> Amortized computational complexity. </title> <journal> SIAM Journal on Algebraic and Discrete Methods, </journal> <volume> 6 </volume> <pages> 306-318, </pages> <year> 1985. </year>
Reference-contexts: The implementation of priority queues is a classical problem in data structures. A few references are [43, 47, 52, 53, 63, 108, 109]. In the amortized sense, <ref> [101] </ref>, the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> In the amortized sense, <ref> [101] </ref>, the best performance for these operations is achieved by Fibonacci heaps [53]. They achieve amortized constant time for all operations except for the two delete operations which require amortized time O (log n). The data structure we present achieves matching worst case time bounds for all operations.
Reference: [102] <author> Mikkel Thorup. </author> <title> On RAM priority queues. </title> <booktitle> In Proc. 7th ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 59-67, </pages> <year> 1996. </year>
Reference-contexts: A data structure of van Emde Boas et al. [104, 106] supports the operations Insert, Delete, Pred, FindMin and FindMax on a Practical RAM in worst case O (log w) time. For word size log O (1) n this implies an O (log log n) time implementation. Thorup <ref> [102] </ref> recently presented a priority queue supporting Insert and DeleteMin in worst case O (log log n) time independently of the word size w. <p> In the following we w.l.o.g. assume that we can apply Practical RAM operations to a list of O (k) integers stored in O (1) words in worst cast constant time. Together with each integer we store a test bit, as in <ref> [4, 5, 102] </ref>. An integer together with the associated test bit is denoted a field. Figure 9.1 illustrates the structure of a list of maximum capacity k containing ` k integers x 1 ; : : : ; x ` .
Reference: [103] <author> Jesper Larsson Traff and Christos D. Zaroliagis. </author> <title> Simple parallel algorithm for the single-source shortest path problem on planar digraphs. In A Parallel Algorithms for Irregularly Structured Problems (IRREGULAR'96), </title> <booktitle> volume 1117 of Lecture Notes in Computer Science, </booktitle> <pages> pages 183-194. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: Our bounds are strongly polynomial and speed up the best previous ones [43] by a logarithmic factor. Greater parallelism for the single-source shortest path problem in the case of planar digraphs can be achieved by plugging our implementation of Dijkstra's algorithm into the algorithm of <ref> [103] </ref> resulting in an algorithm which runs O (n 2* + n 1* ) time and performs O (n 1+* ) work on a CREW PRAM.
Reference: [104] <author> Peter van Emde Boas. </author> <title> Preserving order in a forest in less than logarithmic time and linear space. </title> <journal> Information Processing Letters, </journal> <volume> 6 </volume> <pages> 80-82, </pages> <year> 1977. </year>
Reference-contexts: Miltersen [78] refers to this model as a Practical RAM. We assume the elements are integers in the range 0::2 w 1. A tradeoff similar to the one for the comparison model [20] is not known for a Practical RAM. A data structure of van Emde Boas et al. <ref> [104, 106] </ref> supports the operations Insert, Delete, Pred, FindMin and FindMax on a Practical RAM in worst case O (log w) time. For word size log O (1) n this implies an O (log log n) time implementation. <p> If f (n) = p achieve time bounds matching those of Andersson [5]. The basic idea of our construction is to apply the data structure of van Emde Boas et al. <ref> [104, 106] </ref> for O (f (n)) levels and then switch to a packed search tree of height O (log n=f (n)). This is very similar to the data structure of Andersson [5]. <p> In Section 9.3 we describe how to perform queries in a packed search tree and in Section 9.4 how to update a packed search tree. In Section 9.5 we combine the packed search trees with a range reduction based on the data structure of van Emde Boas et al. <ref> [104, 106] </ref> to achieve the result stated in Theorem 22. <p> This finishes our description of how to achieve the bounds stated in Lemma 11. 102 9.5 Range reduction To prove Theorem 22 we combine Lemma 11 with a range reduction based on a data structure of van Emde Boas et al. <ref> [104, 106] </ref>. This is similar to the data structure of Andersson [5], and for details we refer to [5]. We w.l.o.g. assume w 2 f (n) log n.
Reference: [105] <author> Peter van Emde Boas. </author> <title> Machine models and simulations. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity. </booktitle> <publisher> MIT Press/Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: Their solution was described in the cell-probe model of Yao [111] with word size equal to 1. In this paper we adopt the standard unit cost RAM model <ref> [105] </ref>. For the general case where d &gt; 1, d-queries can be answered in optimal space O (nm) doing P d m exact queries each requiring time O (m) by using the data structure of Fredman, Komlos fl Supported by the Danish Natural Science Research Council (Grant No. 9400044).
Reference: [106] <author> Peter van Emde Boas, R. Kaas, and E. Zijlstra. </author> <title> Design and implementation of an efficient priority queue. </title> <journal> Mathematical Systems Theory, </journal> <volume> 10 </volume> <pages> 99-127, </pages> <year> 1977. </year>
Reference-contexts: Miltersen [78] refers to this model as a Practical RAM. We assume the elements are integers in the range 0::2 w 1. A tradeoff similar to the one for the comparison model [20] is not known for a Practical RAM. A data structure of van Emde Boas et al. <ref> [104, 106] </ref> supports the operations Insert, Delete, Pred, FindMin and FindMax on a Practical RAM in worst case O (log w) time. For word size log O (1) n this implies an O (log log n) time implementation. <p> If f (n) = p achieve time bounds matching those of Andersson [5]. The basic idea of our construction is to apply the data structure of van Emde Boas et al. <ref> [104, 106] </ref> for O (f (n)) levels and then switch to a packed search tree of height O (log n=f (n)). This is very similar to the data structure of Andersson [5]. <p> In Section 9.3 we describe how to perform queries in a packed search tree and in Section 9.4 how to update a packed search tree. In Section 9.5 we combine the packed search trees with a range reduction based on the data structure of van Emde Boas et al. <ref> [104, 106] </ref> to achieve the result stated in Theorem 22. <p> This finishes our description of how to achieve the bounds stated in Lemma 11. 102 9.5 Range reduction To prove Theorem 22 we combine Lemma 11 with a range reduction based on a data structure of van Emde Boas et al. <ref> [104, 106] </ref>. This is similar to the data structure of Andersson [5], and for details we refer to [5]. We w.l.o.g. assume w 2 f (n) log n.
Reference: [107] <author> Jan van Leeuwen. </author> <title> The composition of fast priority queues. </title> <type> Technical Report RUU-CS-78-5, </type> <institution> Department of Computer Science, University of Utrecht, </institution> <year> 1978. </year>
Reference-contexts: The priority queue of van Leeuwen <ref> [107] </ref>, the implicit priority queues of Carlsson et al. [27] and the relaxed heaps of Driscoll et al. [43], but neither of these support Meld efficiently. <p> They achieve amortized constant time for all operations except for the two delete operations which require amortized time O (log n). The data structure we present achieves matching worst case time bounds for all operations. Previously, this was only achieved for various strict subsets of the listed operations <ref> [15, 27, 43, 107] </ref>.
Reference: [108] <author> Jean Vuillemin. </author> <title> A data structure for manipulating priority queues. </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 309-315, </pages> <year> 1978. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> The implementation of priority queues is a classical problem in data structures. A few references are [43, 47, 52, 53, 63, 108, 109]. In the amortized sense, [101], the best performance is achieved by binomial heaps <ref> [108] </ref>. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> In Section 5.3 we show that our construction is optimal. Section 5.4 contains some final remarks. 5.1 The Data Structure Our basic representation of a priority queue is a heap ordered tree where each node contains one element. This is slightly different from binomial heaps <ref> [108] </ref> and Fibonacci heaps [53] where the representation is a forest of heap ordered trees. With each node we associate a rank and we partition the children of a node into two types, type i and type ii. <p> By increasing the rank of the node with the smallest element to r + 1 the properties a) to d) are satisfied. The operation is illustrated in Figure 5.3. This is similar to the linking of trees in binomial heaps and Fibonacci heaps <ref> [108, 53] </ref>. C C fl fl " fl fl C C fl fl C C fl fl C C r r r + 1 We now describe how to implement the operations. * MakeQueue is trivial. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95]. <p> In Section 7.3 we describe how to extend the repertoire of priority queue operations to include Delete and DecreaseKey. The priority queues in this section are based on heap ordered binomial trees <ref> [108] </ref>. Throughout this paper we assume a one to one mapping between tree nodes and priority queue elements. Binomial trees are defined as follows. A binomial tree of rank zero is a single node.
Reference: [109] <author> John William Joseph Williams. </author> <title> Algorithm 232: Heapsort. </title> <journal> Communications of the ACM, </journal> <volume> 7(6) </volume> <pages> 347-348, </pages> <year> 1964. </year>
Reference-contexts: Delete (Q; e) Deletes element e from priority queue Q provided that it is known where e is stored in Q (priority queues do not support the searching for an element). The implementation of priority queues is a classical problem in data structures. A few references are <ref> [43, 47, 52, 53, 63, 108, 109] </ref>. In the amortized sense, [101], the best performance is achieved by binomial heaps [108]. They support Delete and DeleteMin in amortized time O (log n) and all other operations in amortized constant time. <p> Category: E.1, F.1.2 Keywords: priority queues, meld, PRAM, worst case complexity 7.1 Introduction The construction of priority queues is a classical topic in data structures. Some references are <ref> [15, 18, 43, 49, 52, 53, 108, 109] </ref>. A historical overview of implementations can be found in [76]. Recently several papers have also considered how to implement priority queues on parallel machines [28, 32, 35, 70, 89, 90, 94, 95].
Reference: [110] <author> A. C-C. Yao. </author> <title> Probabilistic computations: Towards a unified measure of complexity. </title> <booktitle> In Proc. 17th Ann. Symp. on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 222-227, </pages> <year> 1977. </year>
Reference: [111] <author> Andrew C. Yao. </author> <title> Should tables be sorted? Journal of the ACM, </title> <booktitle> 28(3) </booktitle> <pages> 615-628, </pages> <year> 1981. </year>
Reference-contexts: They present for the case d = 1 a data structure supporting queries in time O (m log log n) with space requirement O (nm log m). Their solution was described in the cell-probe model of Yao <ref> [111] </ref> with word size equal to 1. In this paper we adopt the standard unit cost RAM model [105].

References-found: 111

