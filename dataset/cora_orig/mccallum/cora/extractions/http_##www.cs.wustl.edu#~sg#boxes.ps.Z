URL: http://www.cs.wustl.edu/~sg/boxes.ps.Z
Refering-URL: http://www.cs.wustl.edu/~sg/
Root-URL: 
Email: bshouty@cpsc.ucalgary.ca  pwgoldb@cs.sandia.gov  sg@cs.wustl.edu  dmath@cs.wustl.edu  
Title: Exact Learning of Discretized Geometric Concepts  
Author: Nader H. Bshouty Paul W. Goldberg Sally A. Goldman H. David Mathias 
Date: March 2, 1996  
Address: Calgary, Alberta, Canada T2N 1N4  MS 1110 P.O. Box 5800 Albuquerque, NM 87185-1110  St. Louis, MO 63130  St. Louis, MO 63130  
Affiliation: Department of Computer Science The University of Calgary  Department 1423 Sandia National Labs,  Dept. of Computer Science Washington University  Dept. of Computer Science Washington University  
Abstract: We first present an algorithm that uses membership and equivalence queries to exactly identify a discretized geometric concept defined by the union of m axis-parallel boxes in d- dimensional discretized Euclidean space where each coordinate can have n discrete values. This algorithm receives at most md counterexamples and uses time and membership queries polynomial in m and log n for d any constant. Furthermore, all equivalence queries can be formulated as the union of O(md log m) axis-parallel boxes. Next, we show how to extend our algorithm to efficiently learn, from only equivalence queries, any discretized geometric concept generated from any number of halfspaces with any number of known (to the learner) slopes in a constant dimensional space. In particular, our algorithm exactly learns (from equivalence queries only) unions of discretized axis-parallel boxes in constant dimensional space in polynomial time. Furthermore, this algorithm can be modified to handle a polynomial number of lies in the counterexamples provided by the environment. Finally, we introduce a new complexity measure that better captures the complexity of the union of m boxes than simply the number of boxes and the dimension. Our new measure, , is the number of segments in the target where a segment is a maximum portion of one of the sides of the target that lies entirely inside or entirely outside each of the other halfspaces defining the target. We present a modification of our first algorithm that uses time and queries polynomial in and log n. In fact, the time and queries (both membership and equivalence) used by this single algorithm are polynomial for either m or d constant. fl Portions of this paper appear in preliminary form in [18] and [8]. y This research was supported in part by the NSERC of Canada. z This research was performed while visiting Washington University. Currently supported by the U.S. Department of Energy under contract DE-AC04-76AL85000. x Supported in part by NSF Grant CCR-9110108 and an NSF NYI Grant CCR-9357707 with matching funds provided by Xerox PARC and WUTA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research [4, 11, 13, 22, 24, 25, 26, 27, 28, 30]. We study the problem of learning geometric concepts under the model of learning with queries <ref> [1] </ref> in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. To apply such a learning model to a geometric domain, it is necessary to look at a discretized (or digitalized) version of the domain. <p> This paper subsumes the results presented by Goldberg et al. [18] and includes several results given by Bshouty et al. [8]. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An- gluin <ref> [1] </ref>. When applied to our class of discretized geometric concepts, the learner's goal is to learn exactly how an unknown target concept, g, drawn from the concept class G 2 N d n , classifies as positive or negative all instances from the instance space N d n . <p> The learner is permitted time polynomial in 1=*, 1=ffi, the size of an example, and the size of the target concept to formulate a hypothesis. The relationship between the PAC model and the query model is well understood. Angluin <ref> [1] </ref> showed that any class that is learnable using only equivalence queries is also PAC learnable. The relationship is unchanged by the addition of membership queries to each model. Blum [5] showed that PAC learnability does not imply query learnability.
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [2, 21, 33, 20] </ref>, there has been little research on learning geometric concepts with noise. Auer [4] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [3] <author> Esther Arkin, Patrice Belleville, Joseph Mitchell, David Mount, Kathleen Romanik, Steven Salzberg, and Diana Souvaine. </author> <title> Testing simple polygons. </title> <booktitle> In Proceedings of the 5th Canadian Conference on Computational Geometry, </booktitle> <pages> pages 387-392, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: However, when working with non-convex objects, the probe used in such work is more powerful than a membership query. Finally, there has been some work on geometric testing (for example see <ref> [31, 3] </ref>) in which there is no learning task but rather a probe equivalent to a membership query is used to determine whether two geometric objects are equivalent.
Reference: [4] <author> Peter Auer. </author> <title> On-line learning of rectangles in noisy environments. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 253-261, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> Auer <ref> [4] </ref> improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> While there has been some work addressing the general issue of mislabeled training examples in the PAC model [2, 21, 33, 20], there has been little research on learning geometric concepts with noise. Auer <ref> [4] </ref> investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [5] <author> Avrim Blum. </author> <title> Separating PAC and mistake-bound learning models over the Boolean domain. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: The relationship between the PAC model and the query model is well understood. Angluin [1] showed that any class that is learnable using only equivalence queries is also PAC learnable. The relationship is unchanged by the addition of membership queries to each model. Blum <ref> [5] </ref> showed that PAC learnability does not imply query learnability. <p> One must select hypotheses for the equivalence queries so that sufficient progress is made with each counterexample. This requirement of selecting a "smart" hypothesis makes the problem of obtaining an efficient algorithm to exactly learn the class S n significantly harder than obtaining the corresponding PAC result. Also Blum <ref> [5] </ref> has shown that if one-way functions exist then there exist functions that are PAC-learnable but not exactly learnable.
Reference: [6] <author> Avrim Blum and Steven Rudich. </author> <title> Fast learning of k-term DNF formulas with queries. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-389, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: it would be interesting to see if S n can be efficiently learned in time polynomial in m and log n for d = O (log m) or in time polynomial in d and log n for m = O (log d) (i.e. a generalization of the Blum and Rudich <ref> [6] </ref> result that O (log n)-term DNF formulas are exactly learnable). Of course, since S n generalizes the class of DNF formulas, it seems very unlikely that one could develop an algorithm for the unrestricted case of m box d n that is polynomial in m, log n, and d.
Reference: [7] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model [34]. Blumer et al. <ref> [7] </ref> present an algorithm to PAC-learn an m-fold union of boxes in E d by drawing a sufficiently large sample of size m 0 = poly * ; lg 1 , and then performing a greedy covering over the 6 at most 2d boxes defined by the sample. <p> Thus n counterexamples may be needed. However, if one uses the "smarter" hypothesis of r = (h+n)=2 then at most dlog ne counterexamples are needed. More generally, the results from Blumer et al. <ref> [7] </ref> show that under the PAC model any concise hypothesis that is consistent with the data is satisfactory. In other words, the PAC model provides no suitable basis for distinction among different consistent hypotheses.
Reference: [8] <author> Nader H. Bshouty, Zhixiang Chen, and Steve Homer. </author> <title> On learning discretized geometric concepts. </title> <booktitle> In 35th Annual Symposium on Foundations of Computer Science, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Finally, in Section 10 we conclude with some open problems. This paper subsumes the results presented by Goldberg et al. [18] and includes several results given by Bshouty et al. <ref> [8] </ref>. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An- gluin [1]. <p> This algorithm appears in <ref> [8] </ref> along with the equivalence-query algorithms presented here in Sections 6 and 7. While the Chen and Homer result is very similar to our result of Section 6, they use a very different technique to obtain the result.
Reference: [9] <author> Nader H. Bshouty, Sally A. Goldman, Thomas R. Hancock, and Sleiman Matar. </author> <title> Asking questions to minimize errors. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 41-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: We present this algorithm, in part, because it introduces the approach used to obtain our other results and also because it uses very few equivalence queries, which is of interest if one's goal is to minimize the number of prediction errors made by the learner <ref> [9] </ref>. Next in Section 6 we describe a modification of this algorithm that efficiently learns the union of boxes in constant dimensional space with only equivalence queries. <p> serves two purposes: (1) the other algorithms presented build upon this basic algorithm and thus for ease of exposition we present it here, and (2) it uses very few equivalence queries, which is of interest if one's goal is to minimize the number of prediction errors made by the learner <ref> [9] </ref>.
Reference: [10] <author> William J. Bultman and Wolfgang Maass. </author> <title> Fast identification of geometric objects with membership queries. </title> <booktitle> In Fourth Workshop on Computational Learning Theory, </booktitle> <pages> pages 337-353, </pages> <year> 1991. </year>
Reference-contexts: There has also been work on learning non axis-parallel discretized rectangles with only equivalence queries. Maass and Turan [28] show an (n) information theoretic lower bound on the number of equivalence queries when the hypotheses must be drawn from the concept class. Contrasting this lower bound, Bultman and Maass <ref> [10] </ref> give an efficient algorithm that uses membership and equivalence queries to learn this class using O (log n) equivalence queries.
Reference: [11] <author> Zhixiang Chen. </author> <title> Learning unions of two rectangles in the plane with equivalence queries. </title> <booktitle> In Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 243-252. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> More recently, Chen <ref> [11] </ref> gave an algorithm that used equivalence queries to learn general unions of two boxes in the (discretized) plane. The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles.
Reference: [12] <author> Zhixiang Chen and Steven Homer. </author> <title> The bounded injury priority method and the learnability of unions of rectangles. </title> <type> Unpublished manuscript, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: The hypothesis class of their algorithm is the union of 8m 2 2 rectangles. In work independent of ours, Chen and Homer <ref> [12] </ref> have improved upon their earlier result by giving an algorithm that learns any concept from S n using O (m 2 (d+1) d 2 log 2d+1 n) equivalence queries by efficiently applying the bounded injury method from recursive function theory.
Reference: [13] <author> Zhixiang Chen and Wolfgang Maass. </author> <title> On-line learning of rectangles. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 16-27. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan [24, 26]. The best known result for learning the class box d n was provided by the work of Chen and Maass <ref> [13] </ref> in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [14] <author> Joseph C. Culberson and Robert A. Reckhow. </author> <title> Covering Polygons is hard. </title> <booktitle> In 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 601-611, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Note that it is NP-hard to find a minimum covering of a concept from S n by individual boxes <ref> [14] </ref>. Recall that a consistent hypothesis h essentially encodes the set of positive regions. Thus our goal is to find the union of as few boxes as possible that "cover" all of the positive regions.
Reference: [15] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: Thus our goal is to find the union of as few boxes as possible that "cover" all of the positive regions. We now describe how to formulate this problem as a set covering problem for which we can then use the standard greedy set covering heuristic <ref> [15] </ref> to perform the conversion. The set X of objects to cover will simply contain all positive regions in h. Thus jXj (4m + 1) d . Then the set F of subsets of X will be made as follows.
Reference: [16] <author> H. Edelsbrunner. </author> <title> Algorithms in Combinatorial Geometry. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, Germany, </address> <year> 1987. </year> <month> 31 </month>
Reference-contexts: We use the following lemma to bound the maximum number of regions that will be contained in G H . Lemma 2 Any t, d-dimensional hyperplanes in a (d + 1)-dimensional space divide the space into at most t d+1 + 1 regions. This result is well-known. See Edelsbrunner <ref> [16] </ref> for a proof.
Reference: [17] <author> Mike Frazier, Sally Goldman, Nina Mishra, and Leonard Pitt. </author> <title> Learning from a con--sistently ignorant teacher. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Also Blum [5] has shown that if one-way functions exist then there exist functions that are PAC-learnable but not exactly learnable. Finally, under a variation of the PAC model in which membership queries can be made, Frazier et al. <ref> [17] </ref> have given an algorithm to PAC-learn the m-fold union of boxes in E d for which each box is entirely contained within the positive quadrant and contains the origin. Furthermore, their algorithm learns this subclass of general unions of boxes in time polynomial in both m and d.
Reference: [18] <author> Paul W. Goldberg, Sally A. Goldman, and H. David Mathias. </author> <title> Learning unions of boxes with membership and equivalence queries. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: In Section 9 we present our new complexity measure and describe a modification of our first algorithm that runs in polynomial time with respect to this complexity measure. Finally, in Section 10 we conclude with some open problems. This paper subsumes the results presented by Goldberg et al. <ref> [18] </ref> and includes several results given by Bshouty et al. [8]. 2 Learning Model The learning model we use in this paper is that of learning with queries developed by An- gluin [1].
Reference: [19] <author> Steven Homer and Zhixiang Chen. </author> <title> Fast learning unions of rectangles with queries. </title> <type> Unpublished manuscript, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: More recently, Chen [11] gave an algorithm that used equivalence queries to learn general unions of two boxes in the (discretized) plane. The algorithm uses O (log 2 n) equivalence queries, and involves a detailed case analysis of the shapes formed by the two rectangles. Chen and Homer <ref> [19] </ref> presented an algorithm to learn the union of m rectangles in the plane using O (m 3 log n) queries (both membership and equivalence) and O (m 5 log n) time. The hypothesis class of their algorithm is the union of 8m 2 2 rectangles.
Reference: [20] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 807-837, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [2, 21, 33, 20] </ref>, there has been little research on learning geometric concepts with noise. Auer [4] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [21] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [2, 21, 33, 20] </ref>, there has been little research on learning geometric concepts with noise. Auer [4] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [22] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> They showed that if the learner was restricted to only make equivalence queries 3 By size we mean the number of bits to encode the example. 5 in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [22, 27] </ref>. Auer [4] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries.
Reference: [23] <author> Philip M. Long and Manfred K. Warmuth. </author> <title> Composite geometric concepts and poly-nomial predictability. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 273-287. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Thus for d constant this algorithm runs in polynomial time. Long and Warmuth <ref> [23] </ref> present an algorithm to PAC-learn this same class by again drawing a sufficiently large sample and constructing a hypothesis that consists of at most m (2d) m boxes consistent with the sample.
Reference: [24] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexam-ples. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan <ref> [24, 26] </ref>. The best known result for learning the class box d n was provided by the work of Chen and Maass [13] in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [25] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexam-ples and membership queries. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 203-210, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> The concept class studied here has also been considered in the PAC model, as summarized in the next section. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [25, 26, 27, 28] </ref>. One of the geometric concepts that they studied was the class box d n .
Reference: [26] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <type> Technical Report IIG-Report 316, </type> <institution> Technische Universitat Graz, TU Graz, Austria, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> The concept class studied here has also been considered in the PAC model, as summarized in the next section. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [25, 26, 27, 28] </ref>. One of the geometric concepts that they studied was the class box d n . <p> If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. An algorithm making O (2 d log n) equivalence queries was given by Maass and Turan <ref> [24, 26] </ref>. The best known result for learning the class box d n was provided by the work of Chen and Maass [13] in which they gave an algorithm making O (d 2 log n) equivalence queries.
Reference: [27] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> The concept class studied here has also been considered in the PAC model, as summarized in the next section. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [25, 26, 27, 28] </ref>. One of the geometric concepts that they studied was the class box d n . <p> They showed that if the learner was restricted to only make equivalence queries 3 By size we mean the number of bits to encode the example. 5 in which each hypothesis was drawn from box d n then (d log n) queries are needed to achieve exact identification <ref> [22, 27] </ref>. Auer [4] improves this lower bound to ( d 2 log d log n). If one always makes an equivalence query using the simple hypothesis that produces the smallest box consistent with the previously seen examples, then the resulting algorithm makes O (dn) equivalence queries. <p> So for m constant this yields an efficient PAC algorithm. We note that either of these PAC algorithms can be applied to the class S n giving efficient PAC algorithms for this class for either d constant or m constant. As discussed by Maass and Turan <ref> [27] </ref>, the task of a concept learning algorithm is to provide a "smart" hypothesis based on the data available. In other words, the hypothesis must be carefully chosen so that as much information as possible is obtained from each counterexample.
Reference: [28] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> Algorithms and lower bounds for on-line learning of geometrical concepts. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 251-269, </pages> <year> 1994. </year> <month> 32 </month>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> The concept class studied here has also been considered in the PAC model, as summarized in the next section. 3 Previous Work The problem of learning geometric concepts over a discrete domain was extensively studied by Maass and Turan <ref> [25, 26, 27, 28] </ref>. One of the geometric concepts that they studied was the class box d n . <p> There has also been work on learning non axis-parallel discretized rectangles with only equivalence queries. Maass and Turan <ref> [28] </ref> show an (n) information theoretic lower bound on the number of equivalence queries when the hypotheses must be drawn from the concept class.
Reference: [29] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> How fast can a threshold gate learn? Computational Learning Theory and Natural Learning Systems: Constraints and Prospects, </title> <editor> G. Drastal, S.J. Hanson and R. Rivest eds., </editor> <publisher> MIT Press, to appear. </publisher>
Reference-contexts: There has also been some work on learning discretized geometric concepts defined by non axis-parallel hyperplanes. Maass and Turan <ref> [29] </ref> study the problem of learning a single discretized halfspace using only equivalence queries. They give an efficient algorithm using O (d 2 (log d+log n)) queries and give an information theoretic lower bound of d on the number of queries when all hypotheses are discretized halfspaces.
Reference: [30] <author> Wolfgang Maass and Manfred K. Warmuth. </author> <title> Efficient learning with virtual threshold gates. </title> <type> Personal communication. </type>
Reference-contexts: 1 Introduction Recently, learning geometric concepts in d-dimensional Euclidean space has been the subject of much research <ref> [4, 11, 13, 22, 24, 25, 26, 27, 28, 30] </ref>. We study the problem of learning geometric concepts under the model of learning with queries [1] in which the learner is required to output a final hypothesis that correctly classifies every point in the domain. <p> While the Chen and Homer result is very similar to our result of Section 6, they use a very different technique to obtain the result. Also our algorithm only uses O ((8d 2 m log n) d ) equivalence queries. Finally, in other independent work, Maass and Warmuth <ref> [30] </ref> have developed, as part of a more general result, an algorithm to learn any concept from S n using O (md log n) equivalence queries and O computation time.
Reference: [31] <author> Kathleen Romanik and Carl Smith. </author> <title> Testing geometric objects. </title> <booktitle> In Proceedings of the 2nd Canadian Conference on Computational Geometry, </booktitle> <pages> pages 14-19, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: However, when working with non-convex objects, the probe used in such work is more powerful than a membership query. Finally, there has been some work on geometric testing (for example see <ref> [31, 3] </ref>) in which there is no learning task but rather a probe equivalent to a membership query is used to determine whether two geometric objects are equivalent.
Reference: [32] <author> S. Skiena. </author> <title> Problems in geometric probing. </title> <journal> Algorithmica, </journal> <volume> 4 </volume> <pages> 599-605, </pages> <year> 1989. </year>
Reference-contexts: Observe that the class considered by Frazier et al. is a generalization of the class of DNF formulas in which all variables only appear negated. Computational geometry researchers have looked at the slightly related problem of geometric probing (for example see <ref> [32] </ref>). Here one aims to construct (or learn) an unknown 7 convex polygon given a point inside the polygon along with the ability to make a probe in which the algorithm can "shoot" a ray in a specified direction to find out the location where the ray hits the polygon.
Reference: [33] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: While there has been some work addressing the general issue of mislabeled training examples in the PAC model <ref> [2, 21, 33, 20] </ref>, there has been little research on learning geometric concepts with noise. Auer [4] investigates exact learning of boxes where some of the counterexamples, given in response to equivalence queries, are noisy.
Reference: [34] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11):11341142, </volume> <month> November </month> <year> 1984. </year> <month> 33 </month>
Reference-contexts: We seek a more efficient approach than testing each counterexample we receive using a membership query, especially in view of the fact that we are interested in algorithms that may only use equivalence queries. Another important learning model is the PAC model introduced by Valiant <ref> [34] </ref>. In this model the learner is presented with labeled examples chosen at random according to an unknown, arbitrary distribution D over the instance space. <p> Closely related to the problem of learning the union of discretized boxes, is the problem of learning the union of non-discretized boxes in the PAC model <ref> [34] </ref>.
References-found: 34

