URL: file://cindy.di.unito.it/articles/fonn.submitted.icml97.ps
Refering-URL: http://www.di.unito.it/WWW/MLgroup/attiliobib.html
Root-URL: 
Email: fbotta,attilio,piolag@di.unito.it  
Title: FONN: Combining First Order Logic with Connectionist Learning  
Author: M. Botta, A. Giordana and R. Piola 
Note: Submitted to the ICML-97, and accepted THIS IS NOT THE FINAL VERSION  
Address: Torino  
Affiliation: Dept. of Computer Science, University of  
Abstract: This paper presents a neural network architecture that can manage structured data and refine knowledge bases expressed in a first order logic language. The presented framework is well suited to classification problems in which concept de scriptions depend upon numerical features of the data. In fact, the main goal of the neural architecture is that of refining the numerical part of the knowledge base, without changing its structure. In particular, we discuss a method to translate a set of classification rules into neural computation units. Here, we focus our attention on the translation method and on algorithms to refine network weights on struc tured data. The classification theory to be refined can be manually handcrafted or automatically acquired by a symbolic relational learning system able to deal with numerical features. As a matter of fact, the primary goal is to bring into a neural network architecture the capability of dealing with structured data of unrestricted size, by allowing to dynamically bind the classification rules to different items occur ring in the input data. An extensive experimentation on a challenging artificial case study shows that the network converges quite fastly and generalizes much better than propositional learners on an equivalent task definition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Baroglio, A. Giordana, M. Kaiser, M. Nuttin, and R. Piola. </author> <title> Learn ing controllers for industrial robots. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 221-250, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: 1 INTRODUCTION Several papers appeared in the last decade, both in the field of machine learn ing <ref> [16, 15, 10, 1] </ref> and in the field of connectionism [17, 8], have shown that 1 combining knowledge based methods with connectionist learning, produced algorithms exhibiting excellent performances on non trivial cases studies. <p> The method developed by [16, 15] is based on multi-layer perceptron, whereas the one described in <ref> [17, 1] </ref> is based on Radial Basis Function Networks (RBFNs) [12]. An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. <p> An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. This paper extends the methods proposed in <ref> [1, 17] </ref> to first order logics Horn clause theories, in order to deal with structured data and numeric attributes in concept recognition (classification) problems. <p> absence of reference benchmarks for testing algorithms learning from both numerical and categorical features in FOL, we intend to propose this testbed to the Machine Learning community. 2 MAPPING FIRST ORDER LOGICS TO NEURAL NETWORKS Algorithms for mapping propositional Horn clauses to neural networks have been presented by several authors <ref> [16, 17, 8, 1] </ref>. All of them share the same basic idea that a propositional theory can be functionally described by an AND/OR graph having atomic expressions in the leaves. <p> For instance, Towell and Shavlik [16, 15] transform structured propositional theories into a multi-layer perceptron and encode both boolean operators ^ and _ by means of the perceptron activation function. On the contrary, <ref> [1] </ref> only consider flat theories which are encoded as Fac torizable Radial Basis Function Networks [12], in which atomic conditions are represented as gaussian functions, and boolean operators ^ and _ are encoded with the arithmetic product and the perceptron, respectively. <p> The method of representing logical theories by means of AND/OR graphs can be extended to First Order Theories. Here we will follow this line in order to generalize the method described in <ref> [1] </ref> to FOL classification theories. For the sake of simplicity we restrict ourselves to FOL flat theories, with functions, and negation on single literals. This is not too restrictive since it is the format of concept definitions generated by most relational learners. <p> On the other hand, the eval uation network in Figure 1 is a static structure that can be translated into a neural network in a similar way as done for the propositional case <ref> [1, 15] </ref>. 5 3 FIRST ORDER NEURAL NETWORKS Starting from the AND/OR graph introduced in Figure 1, we will now precisely define the First Order Neural Network architecture. <p> e v i i=1 e v i fl (v 1 ; : : : ; v s ) = 1 + e i=1 w i v i +~ The reason for this choice was due to the fact that the network obtained is a straightforward generalization of a Factorizable RBFN <ref> [12, 1] </ref>, whose behavior has been widely investigated in the literature. In fact, the only difference is represented by the additional layer implementing the existential quantification, which is not necessary in the propositional version.
Reference: [2] <author> H.R. Berenji. </author> <title> Fuzzy logic controllers. In R.R. </title> <editor> Yager and L.A. Zadeh, ed itors, </editor> <title> An Introduction to Fuzzy Logic Applications in Intelligent Systems. </title> <address> Kluver, </address> <year> 1992. </year>
Reference-contexts: Anyhow, it is worth noting that this choice is in some sense arbitrary and in no way pretends to be considered as an optimal one. For instance, many alternative proposals can be found in the fuzzy logic controller literature <ref> [2, 18] </ref>, which have not been explored in this work and which deserve attention. 4 NETWORK REFINEMENT As all activation functions in FONNs are continuous and derivable in the whole parameter space, it is immediate to apply the classical error gradient descent technique in order to finely tune the weights w
Reference: [3] <author> H.R. Berenji and P. Khedkar. </author> <title> Learning and tuning fuzzy con trollers through reinforcements. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(5) </volume> <pages> 724-740, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: of the max function is pro vided by the so called sof tmax (S M ), computed by the expression S M (v 1 : : : ; v n ) = 8 P n P n i=1 e v i The properties of S M have been investigated in <ref> [3] </ref> where it has been applied in a fuzzy neural network. An alternative to S M could be a simple arithmetic sum or an arithmetic sum followed by a squashing function, such as a sigmoid.
Reference: [4] <author> E.B. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learn ability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Smaller networks have a smaller number of weights to be tuned, and in general, require smaller learning sets because the Vapnik-Chervonenkis dimension of a neural network increases with the number of weights <ref> [4] </ref>. We will present results obtained for data of definite size showing that, also in this case, FONN perform significantly better than C4.5, using small learning sets. The experimentation is performed on an artificial problem inspired to the well known trains going east problem proposed by Michalski [11].
Reference: [5] <author> M. Botta and A. Giordana. </author> <title> SMART+: A multi-strategy learning tool. </title> <booktitle> In IJCAI-93, Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 937-943, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The first attempt to deal with continuous attributes in FOL is due to INDUCE with the Closing Interval rule [11]. The same solution has been reproposed in other systems [7] inspired to INDUCE. Another proposal has been made in <ref> [5] </ref>, where an algorithm has been proposed in order to learn continuous intervals in SMART+.
Reference: [6] <author> R. Davis, B.G. Buchanan, and E.H. Shortliffe. </author> <title> Production systems as a representationfor a knowledge-based consultation program. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 15-45, </pages> <year> 1977. </year>
Reference-contexts: An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN <ref> [6] </ref>. This paper extends the methods proposed in [1, 17] to first order logics Horn clause theories, in order to deal with structured data and numeric attributes in concept recognition (classification) problems.
Reference: [7] <author> F. Esposito, D. Malerba, and G. Semeraro. </author> <title> Classification in noisy environ ments using a distance measure between structural symbolic descriptions. </title> <journal> IEEE trans. on Pattern Analisys and Machine Intelligence, </journal> <volume> 14(3):390 402, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The first attempt to deal with continuous attributes in FOL is due to INDUCE with the Closing Interval rule [11]. The same solution has been reproposed in other systems <ref> [7] </ref> inspired to INDUCE. Another proposal has been made in [5], where an algorithm has been proposed in order to learn continuous intervals in SMART+.
Reference: [8] <author> L.M. Fu. </author> <title> Knowledge-based connectionism for revising domain theories. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 23(1) </volume> <pages> 173-182, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION Several papers appeared in the last decade, both in the field of machine learn ing [16, 15, 10, 1] and in the field of connectionism <ref> [17, 8] </ref>, have shown that 1 combining knowledge based methods with connectionist learning, produced algorithms exhibiting excellent performances on non trivial cases studies. <p> The method developed by [16, 15] is based on multi-layer perceptron, whereas the one described in [17, 1] is based on Radial Basis Function Networks (RBFNs) [12]. An alternative line is followed by [10] and <ref> [8] </ref> which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. This paper extends the methods proposed in [1, 17] to first order logics Horn clause theories, in order to deal with structured data and numeric attributes in concept recognition (classification) problems. <p> absence of reference benchmarks for testing algorithms learning from both numerical and categorical features in FOL, we intend to propose this testbed to the Machine Learning community. 2 MAPPING FIRST ORDER LOGICS TO NEURAL NETWORKS Algorithms for mapping propositional Horn clauses to neural networks have been presented by several authors <ref> [16, 17, 8, 1] </ref>. All of them share the same basic idea that a propositional theory can be functionally described by an AND/OR graph having atomic expressions in the leaves.
Reference: [9] <author> D. Haussler. </author> <title> Learning conjunctive concepts in structural domains. </title> <journal> Ma chine Learning, </journal> <volume> 4 </volume> <pages> 70-40, </pages> <year> 1989. </year>
Reference-contexts: This point hinders a potential pitfall which does not rise up in the propositional case. In fact, matching First Order formulas is per se a combinatorial problem, as explained by <ref> [9] </ref>. Nevertheless, in many practical cases the complexity can be tamed by a smart theorem prover by exploiting the knowledge gained during the matching process in order to avoid testing subformulas which depend upon conditions which are known to be false.
Reference: [10] <author> J.J. Mahoney and R.J. Mooney. </author> <title> Comparing methods for refining certainity-factor rule-bases. </title> <booktitle> In Proc. of the Eleventh Internetional Work shop on Machine Learning ML-94, </booktitle> <institution> Rutgers University, NJ, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION Several papers appeared in the last decade, both in the field of machine learn ing <ref> [16, 15, 10, 1] </ref> and in the field of connectionism [17, 8], have shown that 1 combining knowledge based methods with connectionist learning, produced algorithms exhibiting excellent performances on non trivial cases studies. <p> The method developed by [16, 15] is based on multi-layer perceptron, whereas the one described in [17, 1] is based on Radial Basis Function Networks (RBFNs) [12]. An alternative line is followed by <ref> [10] </ref> and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. This paper extends the methods proposed in [1, 17] to first order logics Horn clause theories, in order to deal with structured data and numeric attributes in concept recognition (classification) problems.
Reference: [11] <author> R. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 83-134, </pages> <address> Los Altos, CA, 1983. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 18 </pages>
Reference-contexts: The first attempt to deal with continuous attributes in FOL is due to INDUCE with the Closing Interval rule <ref> [11] </ref>. The same solution has been reproposed in other systems [7] inspired to INDUCE. Another proposal has been made in [5], where an algorithm has been proposed in order to learn continuous intervals in SMART+. <p> We will present results obtained for data of definite size showing that, also in this case, FONN perform significantly better than C4.5, using small learning sets. The experimentation is performed on an artificial problem inspired to the well known trains going east problem proposed by Michalski <ref> [11] </ref>. In the present case a set of continuous attributes has been added and a generation program which automatically builds trains according to user assigned proba bility distributions has been implemented. Trains are partitioned into positive and negative instances according to a user defined rule set. <p> On the contrary, by defining different learning problems on the same dataset we can a-priori control their complexity. 1 All data are available; the ftp site is not reported in order to hide author's identity 11 5.1 THE TRAIN CHECK-OUT PROBLEM Starting from the well-known train-going-east problem defined by Michalsky <ref> [11] </ref> we extended the dataset by introducing continuous features.
Reference: [12] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <journal> Pro ceedings of the IEEE, </journal> <volume> 78(9) </volume> <pages> 1481-1497, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The method developed by [16, 15] is based on multi-layer perceptron, whereas the one described in [17, 1] is based on Radial Basis Function Networks (RBFNs) <ref> [12] </ref>. An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. <p> For instance, Towell and Shavlik [16, 15] transform structured propositional theories into a multi-layer perceptron and encode both boolean operators ^ and _ by means of the perceptron activation function. On the contrary, [1] only consider flat theories which are encoded as Fac torizable Radial Basis Function Networks <ref> [12] </ref>, in which atomic conditions are represented as gaussian functions, and boolean operators ^ and _ are encoded with the arithmetic product and the perceptron, respectively. The method of representing logical theories by means of AND/OR graphs can be extended to First Order Theories. <p> e v i i=1 e v i fl (v 1 ; : : : ; v s ) = 1 + e i=1 w i v i +~ The reason for this choice was due to the fact that the network obtained is a straightforward generalization of a Factorizable RBFN <ref> [12, 1] </ref>, whose behavior has been widely investigated in the literature. In fact, the only difference is represented by the additional layer implementing the existential quantification, which is not necessary in the propositional version.
Reference: [13] <author> R.J. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: An example of such a rule base is shown in Fig. 3 (a). For task 1, we tried two versions of the handcrafted knowledge base: one (marked as (a) in the experiments) with completely wrong parameters and one (b) more accurate. We performed the following experiments: C4.5 <ref> [13] </ref> has been run on the propositional definition of each problem; handcrafted sets of rules, correspond ing to the ones used for classifying the instances, have been fed to the FONN; thirdly, Smart+ |a relational learner capable of learning numeric intervals in the form (2) with reasonable approximation| has been run
Reference: [14] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing : Explorations in the Microstructure of Cognition, Parts I & II. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Even if the methods proposed in the literature differ for many substantial aspects, all of them share the basic approach of converting a propositional theory into a neural network which can then be refined using numeric algo rithms such as error backpropagation <ref> [14] </ref>. The method developed by [16, 15] is based on multi-layer perceptron, whereas the one described in [17, 1] is based on Radial Basis Function Networks (RBFNs) [12]. <p> In the method proposed in this paper, a FOL classification theory is trans lated into a network of elementary computational units which can be refined by performing an error gradient descent <ref> [14] </ref>, while preserving the symbolic 2 readability. We will call such a kind of network First Order Neural Networks (FONN). The classification theory can be manually handcrafted, or acquired using a relational learner capable of providing at least a rough estimate of the numeric constants occurring in the classification rules. <p> Moreover, let v (!= y ) denote the ideal output that is 1 if != y belongs to relation and 0 otherwise. The refinement procedure proceeds by epochs <ref> [14] </ref>.
Reference: [15] <author> G. Towell and J.W. Shavlik. </author> <title> Knowledge based artificial neural networks. </title> <journal> Artficial Intelligence, </journal> <volume> 70(4) </volume> <pages> 119-166, </pages> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION Several papers appeared in the last decade, both in the field of machine learn ing <ref> [16, 15, 10, 1] </ref> and in the field of connectionism [17, 8], have shown that 1 combining knowledge based methods with connectionist learning, produced algorithms exhibiting excellent performances on non trivial cases studies. <p> Even if the methods proposed in the literature differ for many substantial aspects, all of them share the basic approach of converting a propositional theory into a neural network which can then be refined using numeric algo rithms such as error backpropagation [14]. The method developed by <ref> [16, 15] </ref> is based on multi-layer perceptron, whereas the one described in [17, 1] is based on Radial Basis Function Networks (RBFNs) [12]. An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. <p> Nevertheless, substantial differences among the approaches concern the for mat of the theory, and the functions used for encoding the logical connectives. For instance, Towell and Shavlik <ref> [16, 15] </ref> transform structured propositional theories into a multi-layer perceptron and encode both boolean operators ^ and _ by means of the perceptron activation function. <p> On the other hand, the eval uation network in Figure 1 is a static structure that can be translated into a neural network in a similar way as done for the propositional case <ref> [1, 15] </ref>. 5 3 FIRST ORDER NEURAL NETWORKS Starting from the AND/OR graph introduced in Figure 1, we will now precisely define the First Order Neural Network architecture.
Reference: [16] <author> G.G. Towell, J.W. Shavlik, and M.O. Noordwier. </author> <title> Refinement of approxi mate domain theories by knowledge-based neural networks. </title> <booktitle> In AAAI'90, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Several papers appeared in the last decade, both in the field of machine learn ing <ref> [16, 15, 10, 1] </ref> and in the field of connectionism [17, 8], have shown that 1 combining knowledge based methods with connectionist learning, produced algorithms exhibiting excellent performances on non trivial cases studies. <p> Even if the methods proposed in the literature differ for many substantial aspects, all of them share the basic approach of converting a propositional theory into a neural network which can then be refined using numeric algo rithms such as error backpropagation [14]. The method developed by <ref> [16, 15] </ref> is based on multi-layer perceptron, whereas the one described in [17, 1] is based on Radial Basis Function Networks (RBFNs) [12]. An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. <p> absence of reference benchmarks for testing algorithms learning from both numerical and categorical features in FOL, we intend to propose this testbed to the Machine Learning community. 2 MAPPING FIRST ORDER LOGICS TO NEURAL NETWORKS Algorithms for mapping propositional Horn clauses to neural networks have been presented by several authors <ref> [16, 17, 8, 1] </ref>. All of them share the same basic idea that a propositional theory can be functionally described by an AND/OR graph having atomic expressions in the leaves. <p> Nevertheless, substantial differences among the approaches concern the for mat of the theory, and the functions used for encoding the logical connectives. For instance, Towell and Shavlik <ref> [16, 15] </ref> transform structured propositional theories into a multi-layer perceptron and encode both boolean operators ^ and _ by means of the perceptron activation function.
Reference: [17] <author> V. Tresp, J. Hollatz, and S. Ahmad. </author> <title> Network structuring and training us ing rule-based knowledge. </title> <booktitle> In Advances in Neural Information Processing Systems 5 (NIPS-5), </booktitle> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION Several papers appeared in the last decade, both in the field of machine learn ing [16, 15, 10, 1] and in the field of connectionism <ref> [17, 8] </ref>, have shown that 1 combining knowledge based methods with connectionist learning, produced algorithms exhibiting excellent performances on non trivial cases studies. <p> The method developed by [16, 15] is based on multi-layer perceptron, whereas the one described in <ref> [17, 1] </ref> is based on Radial Basis Function Networks (RBFNs) [12]. An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. <p> An alternative line is followed by [10] and [8] which adapt the backpropagation to knowledge bases with certainty factors in the style of MYCIN [6]. This paper extends the methods proposed in <ref> [1, 17] </ref> to first order logics Horn clause theories, in order to deal with structured data and numeric attributes in concept recognition (classification) problems. <p> absence of reference benchmarks for testing algorithms learning from both numerical and categorical features in FOL, we intend to propose this testbed to the Machine Learning community. 2 MAPPING FIRST ORDER LOGICS TO NEURAL NETWORKS Algorithms for mapping propositional Horn clauses to neural networks have been presented by several authors <ref> [16, 17, 8, 1] </ref>. All of them share the same basic idea that a propositional theory can be functionally described by an AND/OR graph having atomic expressions in the leaves.
Reference: [18] <author> L.A. Zadeh. </author> <title> Knowledge representation in fuzzy logic. In R.R. </title> <editor> Yager and L.A. Zadeh, editors, </editor> <title> An Introduction to Fuzzy Logic Applications in Intelligent Systems. </title> <publisher> Kluver Academic Publishers, </publisher> <year> 1992. </year> <month> 19 </month>
Reference-contexts: Anyhow, it is worth noting that this choice is in some sense arbitrary and in no way pretends to be considered as an optimal one. For instance, many alternative proposals can be found in the fuzzy logic controller literature <ref> [2, 18] </ref>, which have not been explored in this work and which deserve attention. 4 NETWORK REFINEMENT As all activation functions in FONNs are continuous and derivable in the whole parameter space, it is immediate to apply the classical error gradient descent technique in order to finely tune the weights w
References-found: 18

