URL: http://www.iro.umontreal.ca/~lisa/pointeurs/bengio_1996_npl.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/bib/journals/journals.html
Root-URL: http://www.iro.umontreal.ca
Title: Use of Modular Architectures for Time Series Prediction  
Abstract: Recently, there has been a lot of papers published in the field of time series prediction using connectionist models. Nevertheless we think that one of the major problem which is rarely treated in the literature is related to the choice of input parameters (embedding dimension and delay). In this paper, we propose two modular approaches to this problem and apply them to a sunspot-related time series. Experimental results are then compared to a simple multi-layer perceptron in order to estimate performances of these models. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Takens, </author> <title> Detecting strange attractors in turbulence, in Dynamical Systems and Turbulence, </title> <editor> D. A. Rand and L.-S. Young, eds., </editor> <volume> vol. </volume> <booktitle> 898 of Lecture Notes in Mathematics, Warwick 1980, 1981, </booktitle> <publisher> Springer-Verlag, Berlin, </publisher> <pages> pp. 366-381. </pages>
Reference-contexts: Prediction consists to find the future values fx N+1 , x N+2 , g. If the series is obtained from a deterministic dynamical system, Takens <ref> [1] </ref> showed that there exists an integer d (which is called the embedding dimension), an integer o (which is an arbitrary delay) and a function f () such that for any t &gt; (d o ): x t = f (x to ; x t2o ; ; x tdo ) (1)
Reference: [2] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> Learning internal representations by error propagation, in Parallel Distributed Processing, </title> <editor> D. E. Rumelhart and J. L. McClelland, eds., </editor> <volume> vol. 1, </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: arbitrary delay) and a function f () such that for any t &gt; (d o ): x t = f (x to ; x t2o ; ; x tdo ) (1) Although one can approximate f () using a simple connectionist model such as a multi-layer perceptron trained by backpropagation <ref> [2] </ref>, there exists no exact method to find neither d nor o when N is too small (less than 10 d samples).
Reference: [3] <author> M. Hu, </author> <title> Application of the adaline system to weather forecasting, </title> <type> Electrical Engineering Degree Thesis Technical Report 6775-1, </type> <institution> Stanford Electronic Laboratory, </institution> <year> 1964. </year>
Reference-contexts: Since the pioneering work of Hu <ref> [3] </ref> and the first model using backpropagation for time series prediction by Lapedes and Farber [4], numerous papers have been published on the subject [5]. Most of them have overlooked the subject of input parameter selection (namely d and o ).
Reference: [4] <author> A. Lapedes and R. Farber, </author> <title> Nonlinear signal processing using neural networks: prediction and system modelling, </title> <type> Tech. Rep. </type> <institution> LA-UR-87-2662, Los Alamos National Laboratory, Theoretical Division, </institution> <year> 1987. </year>
Reference-contexts: Since the pioneering work of Hu [3] and the first model using backpropagation for time series prediction by Lapedes and Farber <ref> [4] </ref>, numerous papers have been published on the subject [5]. Most of them have overlooked the subject of input parameter selection (namely d and o ).
Reference: [5] <author> A. S. Weigend and N. A. Gershenfeld, eds., </author> <title> Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Since the pioneering work of Hu [3] and the first model using backpropagation for time series prediction by Lapedes and Farber [4], numerous papers have been published on the subject <ref> [5] </ref>. Most of them have overlooked the subject of input parameter selection (namely d and o ).
Reference: [6] <author> H. Abarbanel, R. Brown, J. Sidorowich, and L. Tsimring, </author> <title> The analysis of observed chaotic data in physical systems, </title> <journal> Reviews of Modern Physics, </journal> <volume> 65 (1993), </volume> <pages> pp. 1331-1392. 4 </pages>
Reference-contexts: Most of them have overlooked the subject of input parameter selection (namely d and o ). In this paper, we do not propose yet another heuristic to select d and o (there are in effect already numerous heuristics, see a good review in <ref> [6] </ref>), which could lead to suboptimal solution in case of unsufficient data. We propose to mix many sets of parameters (each chosen using personal prefered heuristic for instance), using a modular connectionist approach. 2 Problem Description Solar activity is usually measured as the number of sunspots R [7].
Reference: [7] <author> A. Izenman, J. R. </author> <title> Wolf and the Zurich sunspot relative numbers, </title> <journal> The Mathematical Intelligencer, </journal> <volume> 7 (1985), </volume> <pages> pp. 27-33. </pages>
Reference-contexts: We propose to mix many sets of parameters (each chosen using personal prefered heuristic for instance), using a modular connectionist approach. 2 Problem Description Solar activity is usually measured as the number of sunspots R <ref> [7] </ref>. The sunspots time series is known to be difficult to predict and has served as a benchmark in the statistics and connectionist literature [8].
Reference: [8] <author> A. S. Weigend, B. A. Huberman, and D. E. Rumelhart, </author> <title> Predicting sunspots and exchange rates with connectionist networks, in Nonlinear modeling and forecasting, </title> <editor> M. Casdagli and S. Eubank, eds., </editor> <publisher> Addison Wesley, </publisher> <year> 1992, </year> <pages> pp. 395-431. </pages>
Reference-contexts: The sunspots time series is known to be difficult to predict and has served as a benchmark in the statistics and connectionist literature <ref> [8] </ref>. The France Telecom CNET ionospheric prediction service involves the IR5 time series which is a non-centered five-month mean of the sunspots number R: IR5 t = 5 where MR t is the mean sunspots number of month t.
Reference: [9] <author> F. Fogelman Souli e, </author> <title> Neural network architectures for pattern recognition, in From Statistics to Neural Networks, Theory and Pattern Recognition Applications, </title> <editor> V. Cherkassky, J. Friedman, and H. Wechsler, eds., </editor> <publisher> Springer Verlag, </publisher> <year> 1994, </year> <pages> pp. 243-262. </pages>
Reference-contexts: Modularity is heavily used in computer science: by decomposing a problem into modules, one expects each module to be simpler than the overall task; which can be called the divide and conquer strategy <ref> [9] </ref>. Since the time series length is too small to adequately determine correct values for d and o , an alternate solution is to use on each small networks different d and o values. Thus knowledge is incorporated into the structure.
Reference: [10] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, </author> <title> Adaptive mixtures of local experts, </title> <booktitle> Neural Computation, 3 (1991), </booktitle> <pages> pp. 79-87. </pages>
Reference-contexts: to train faster, and the ratio should change smoothly in order at the end of the training to let the global error be the only important term. 4 Adaptative Mixture of Experts The second modular architecture we propose is an adaptative mixture of experts, as introduced by Jacobs and Jordan <ref> [10] </ref>. Since its introduction, many papers has been published using this model or its extensions (such as Hierarchical Mixtures of Experts) for time series prediction. This 2 architecture combines associative and competitive learning.
Reference: [11] <author> T. Meyer and N. Packard, </author> <title> Local forecasting of high-dimensional chaotic dynamics, in Nonlinear Modeling and Forecasting, </title> <editor> M. Casdagli and S. Eubank, eds., </editor> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1992, </year> <pages> pp. 249-263. </pages>
Reference-contexts: Expert 3: d = 24, o = 3, hidden units = 8. Gating: uses all three input parameter sets, with 12 hidden units. The main assumption for the use of this model is that, as it has been suggested in <ref> [11] </ref>, there may be more than one underlying attractor generating the system; a dynamical noise could locally switch the system from one attractor to another. The mixture of experts could then select locally the best reconstruction space.
Reference: [12] <author> L. Xu, </author> <title> Signal segmentation by finite mixture model and EM algorithm, </title> <booktitle> in International Symposium on Artificial Neural Networks, </booktitle> <year> 1994, </year> <pages> pp. </pages> <month> 453-458. </month> <title> [13] , Channel equalization by finite mixtures and the EM algorithm, in Neural Networks for Signal Processing V, </title> <editor> F. Girosi, J. Makhoul, E. Manolakos, and E. Wilson, eds., </editor> <publisher> IEEE Press, </publisher> <year> 1995, </year> <pages> pp. 603-612. </pages>
Reference-contexts: The mixture of experts could then select locally the best reconstruction space. This model has been used for instance in <ref> [12, 13] </ref> to select automatically the attractor generating each part of a signal made of random parts of many different signals.
Reference: [14] <author> J. Hamilton, </author> <title> Analysis of time series subject to changes in regime, </title> <journal> Journal of Econometrics, </journal> <volume> 45 (1990), </volume> <pages> pp. 39-79. </pages>
Reference-contexts: The main difference with our work is our proposition to let each expert have a different view of the input signal in order to help the specialization of each experts. The model can also be compared to regime switching models <ref> [14, 15] </ref>: these models are based on the idea of piecewise linearization of non linear models over the state space by the introduction of tresholds.
Reference: [15] <author> H. Tong, </author> <title> Non-linear Time Series, A Dynamical System Approach, </title> <publisher> Clarendon Press, </publisher> <year> 1990. </year>
Reference-contexts: The main difference with our work is our proposition to let each expert have a different view of the input signal in order to help the specialization of each experts. The model can also be compared to regime switching models <ref> [14, 15] </ref>: these models are based on the idea of piecewise linearization of non linear models over the state space by the introduction of tresholds.
Reference: [16] <author> Y. Bengio and P. Frasconi, </author> <title> An Input Output HMM architecture, </title> <booktitle> in Advances in Neural Information Processing Systems: Proceedings of the 1994 Conference, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, eds., </editor> <publisher> MIT Press, </publisher> <year> 1995. </year> <month> 5 </month>
Reference-contexts: Both gave us better results than a simple multi-layer perceptron for a specific and known to be difficult time series. Other modular architectures could be tested, such as the newly proposed IOHMM model <ref> [16] </ref>. Moreover, another direction which should be explored is related to the stationarity of a time series: there is currently no good method to preprocess a series such that it always results in a stationary process, which is important in order to expect good generalization performance.
References-found: 15

