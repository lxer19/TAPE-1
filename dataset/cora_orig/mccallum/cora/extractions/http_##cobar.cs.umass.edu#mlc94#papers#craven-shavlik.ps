URL: http://cobar.cs.umass.edu/mlc94/papers/craven-shavlik.ps
Refering-URL: 
Root-URL: 
Email: email: fcraven, shavlikg@cs.wisc.edu  
Phone: phone: (608) 263-0475  
Title: Investigating the Value of a Good Input Representation  
Author: Mark W. Craven Jude W. Shavlik U. S. A. 
Address: 1210 West Dayton St. Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: This paper is reprinted from Computational Learning Theory and Natural Learning Systems, vol. 3, T. Petsche, S. Judd, and S. Hanson, (eds.), forthcoming 1995. Copyrighted 1995 by MIT Press Abstract The ability of an inductive learning system to find a good solution to a given problem is dependent upon the representation used for the features of the problem. A number of factors, including training-set size and the ability of the learning algorithm to perform constructive induction, can mediate the effect of an input representation on the accuracy of a learned concept description. We present experiments that evaluate the effect of input representation on generalization performance for the real-world problem of finding genes in DNA. Our experiments that demonstrate that: (1) two different input representations for this task result in significantly different generalization performance for both neural networks and decision trees; and (2) both neural and symbolic methods for constructive induction fail to bridge the gap between these two representations. We believe that this real-world domain provides an interesting challenge problem for the machine learning subfield of constructive induction because the relationship between the two representations is well known, and because conceptually, the representational shift involved in constructing the better representation should not be too imposing. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, E. B. & Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160. </pages>
Reference-contexts: The hidden units are arranged in a single layer and each unit is fully-connected to the set of input units. The number of free parameters (weights + biases) in a neural network gives a rough indication of the capacity of the network <ref> (Baum & Haussler, 1989) </ref>. The 40-hidden-unit networks used in this experiment have 2481 parameters.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993). </year> <title> Learning to predict reading frames in E. coli DNA sequences. </title> <booktitle> In Proceedings of the 26th Hawaii International Conference on System Sciences, </booktitle> <pages> (pp. 773-782), </pages> <address> Maui, HI. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Mural, 1991) to the problem of finding genes in DNA sequences. 3 The Effect of Input Representation As part of the Wisconsin E. coli Genome Project (Daniels et al., 1992), we have been investigating the use of neural networks to find genes in DNA sequences of the bacterium E. coli <ref> (Craven & Shavlik, 1993) </ref>. In the course of this research, we have found that the choice of input representation has a significant effect on how well the task is learned; other researchers have reached the same conclusion (Lapedes et al., 1989).
Reference: <author> Daniels, D. L., Plunkett III, G., Burland, V. D., & Blattner, F. R. </author> <year> (1992). </year> <title> Analysis of the Escherichia coli genome: DNA sequence of the region from 84.5 to 86.5 minutes. </title> <journal> Science, </journal> <volume> 257 </volume> <pages> 771-778. </pages>
Reference-contexts: Other researchers have also investigated neural-network approaches (Farber et al., 1992; Lapedes et al., 1989; Uberbacher & Mural, 1991) to the problem of finding genes in DNA sequences. 3 The Effect of Input Representation As part of the Wisconsin E. coli Genome Project <ref> (Daniels et al., 1992) </ref>, we have been investigating the use of neural networks to find genes in DNA sequences of the bacterium E. coli (Craven & Shavlik, 1993).
Reference: <author> Farber, R., Lapedes, A., & Sirotkin, K. </author> <year> (1992). </year> <title> Determination of eucaryotic protein coding regions using neural networks and information theory. </title> <journal> Journal of Molecular Biology, </journal> <volume> 226 </volume> <pages> 471-479. </pages>
Reference: <author> Flann, N. S. & Dietterich, T. G. </author> <year> (1986). </year> <title> Selecting appropriate representations for learning from examples. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 460-466), </pages> <address> Philadelphia, PA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction The ability of an inductive learning system to find a good solution to a given problem is often dependent upon the representation used for the features of the problem <ref> (Flann & Dietterich, 1986) </ref>. However, there are a number of factors that can mediate the effect of an input representation on the accuracy of a learned concept description. These factors include the training-set size and the ability of the learning algorithm to perform constructive induction (Michalski, 1983).
Reference: <author> Hinton, G. E., McClelland, J. L., & Rumelhart, D. E. </author> <year> (1986). </year> <title> Distributed representations. </title> <editor> In Rumelhart, D. E. & McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: It is important to note that we are not particularly concerned with exactly what features the networks learn to encode with their hidden units. It is possible that the hidden units could learn a local encoding of codons, or they may learn a distributed representation <ref> (Hinton et al., 1986) </ref> of codons, or some entirely different, but useful, set of features. In order to investigate this question, we construct a set of generalization curves using networks with different numbers of hidden units.
Reference: <author> Kramer, A. H. & Sangiovanni-Vincentelli, A. </author> <year> (1989). </year> <title> Efficient parallel learning algorithms for neural networks. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 1). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: When we use the networks to classify instances, however, we treat the output units as threshold units, and hence the networks can be considered to be perceptrons. We train networks until convergence using a conjugate-gradient algorithm <ref> (Kramer & Sangiovanni-Vincentelli, 1989) </ref>. Conjugate-gradient learning obviates the need for learning-rate and momentum parameters. A tuning set consisting of 10% of each training set is used to determine when the network weights are saved so that networks do not "overfit" the training data.
Reference: <author> Lapedes, A., Barnes, C., Burks, C., Farber, R., & Sirotkin, K. </author> <year> (1989). </year> <title> Application of neural networks and other machine learning algorithms to DNA sequence analysis. </title> <editor> In Bell, G. & Marr, T., editors, </editor> <title> Computers and DNA, </title> <booktitle> SFI Studies in the Sciences of Complexity, vol. VII. </booktitle> <publisher> Addison-Wesley. </publisher>
Reference-contexts: In the course of this research, we have found that the choice of input representation has a significant effect on how well the task is learned; other researchers have reached the same conclusion <ref> (Lapedes et al., 1989) </ref>. It was this finding that motivated us to explore the effect of input representation on generalization performance. In this section we describe the two different representations that are used for DNA sequences in our experiments. Both representations that we investigate represent DNA sequences as feature-value pairs.
Reference: <author> Matheus, C. J. </author> <year> (1990a). </year> <title> Adding domain knowledge to SBL through feature construction. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 803-808), </pages> <address> Boston, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: One of the touted virtues of multi-layer artificial neural networks is that their hidden units are able to construct new features from the given input features (Rumelhart et al., 1986). The other system that we investigate is Citre <ref> (Matheus, 1990a) </ref>, which performs constructive induction on decision trees. In our experiments, neither of these approaches are able to construct the features needed for good generalization in the gene-finding domain. The next section provides a brief introduction to the biology underlying the problem of finding genes in DNA. <p> The selection process involves forming conjunctions of pairs of Boolean features. Matheus describes a number of ways in which pairs of features can be selected <ref> (Matheus, 1990a) </ref>. In this experiment we use the adjacent method, which selects all adjacent pairs of tests that occur on decision-tree branches that lead to positively-labelled leaves. For example, consider a branch in a decision tree that leads to a leaf labelled coding.
Reference: <author> Matheus, C. J. </author> <year> (1990b). </year> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference-contexts: We have also conducted this experiment using the fringe feature selection method (Pagallo & Haussler, 1990), and competitive evaluation of features <ref> (Matheus, 1990b) </ref>. However, we found that the adjacent selection method and information-based evaluation provided the best results.
Reference: <author> Matheus, C. J. & Rendell, L. A. </author> <year> (1989). </year> <title> Constructive induction on decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 645-650), </pages> <address> Detroit, MI. </address>
Reference-contexts: The Citre system <ref> (Matheus & Rendell, 1989) </ref> provides a general approach for constructive induction on decision trees. In this section we describe an experiment in which we train decision trees using the nucleotides representation, and then use Citre to construct features on these trees.
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161. </pages>
Reference-contexts: However, there are a number of factors that can mediate the effect of an input representation on the accuracy of a learned concept description. These factors include the training-set size and the ability of the learning algorithm to perform constructive induction <ref> (Michalski, 1983) </ref>. Constructive induction involves automatically constructing new features from given ones, thereby changing the problem representation. In this paper we investigate, for a real-world problem, the difference in generalization performance that results from using two different input representations.
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99. </pages>
Reference-contexts: We have also conducted this experiment using the fringe feature selection method <ref> (Pagallo & Haussler, 1990) </ref>, and competitive evaluation of features (Matheus, 1990b). However, we found that the adjacent selection method and information-based evaluation provided the best results.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: A generalization curve plots test-set error on the y-axis against training-set size on the x-axis. We construct these curves using the C4.5 decision tree algorithm <ref> (Quinlan, 1993) </ref>, and perceptrons (Rosenblatt, 1958). A perceptron is a neural network with no hidden units and a linear-threshold output unit. A window size of 15 nucleotides is used to determine the input features for both representations. <p> Error is not backward-propagated and weights are not updated for members of the tuning set; they are simply classified by the network in order to estimate the accuracy of the network on unseen examples. For the decision trees, pessimistic pruning <ref> (Quinlan, 1993) </ref> is used to avoid overfitting. and codons input representations. Figure 5 shows the observed generalization curves for representations. The solid line shows the observed generalization curve for the nucleotides representation. The dashed line shows the observed generalization curve for the codons representation.
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The perceptron: A probabilistic model for information storage and orga-nization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65(6) </volume> <pages> 386-407. </pages>
Reference-contexts: A generalization curve plots test-set error on the y-axis against training-set size on the x-axis. We construct these curves using the C4.5 decision tree algorithm (Quinlan, 1993), and perceptrons <ref> (Rosenblatt, 1958) </ref>. A perceptron is a neural network with no hidden units and a linear-threshold output unit. A window size of 15 nucleotides is used to determine the input features for both representations. For both representations, classifiers are trained on example sets that range from 100 to 10,000 examples.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The first approach to constructive induction that we consider involves simply adding hidden units to the networks used in the previous experiment. One of the touted virtues of multi-layer artificial neural networks is that their hidden units are able to construct new features from the given input features <ref> (Rumelhart et al., 1986) </ref>. The other system that we investigate is Citre (Matheus, 1990a), which performs constructive induction on decision trees. In our experiments, neither of these approaches are able to construct the features needed for good generalization in the gene-finding domain.
Reference: <author> Uberbacher, E. C. & Mural, R. J. </author> <year> (1991). </year> <title> Locating protein coding regions in human DNA sequences by a multiple sensor neural network approach. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 88 </volume> <pages> 11261-11265. </pages>
Reference: <author> Watson, J. D., Hopkins, N. H., Roberts, J. W., Steitz, J. A., & Weiner, A. M. </author> <year> (1987). </year> <title> Molecular Biology of the Gene (volume I). </title> <address> Benjamin/Cummings, Menlo Park, CA, </address> <note> fourth edition. </note>
Reference-contexts: The final section discusses the significance of these experiments and provides conclusions. 2 The Problem Domain This section provides a brief description of the problem that serves as a testbed for our experiments. A more thorough treatment of the biology underlying the problem can be found elsewhere <ref> (Watson et al., 1987) </ref>. A DNA strand is a linear sequence of nucleotides composed from the alphabet fA, G, T, Cg. A DNA molecule comprises two strands organized as a double helix. Certain subsequences of a DNA strand, called genes, encode proteins.
References-found: 18

