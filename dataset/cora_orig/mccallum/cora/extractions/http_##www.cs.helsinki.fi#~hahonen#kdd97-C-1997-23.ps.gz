URL: http://www.cs.helsinki.fi/~hahonen/kdd97-C-1997-23.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~hahonen/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fhahonen,oheinone,mklemett,verkamog@cs.helsinki.fi  
Title: Applying Data Mining Techniques in Text Analysis  
Author: Helena Ahonen Oskari Heinonen Mika Klemettinen A. Inkeri Verkamo 
Keyword: data mining, text, text analysis, applications, experiments  
Address: P.O. Box 26, FIN-00014 University of Helsinki, Finland  
Affiliation: University of Helsinki, Department of Computer Science  
Abstract: A number of recent data mining techniques have been targeted especially for the analysis of sequential data. Traditional examples of sequential data involve telecommunication alarms, Www log files, user action registration for Hci studies, or any other series of events consisting of an event type and a time of occurrence. Text can also be seen as sequential data, in many respects similar to the data collected by sensors, or other observation systems. Traditionally, texts have been analysed using various information retrieval related methods, such as full-text analysis, and natural language processing. However, only few examples of data mining in text, particularly in full text, are available. In this paper we show that general data mining methods are applicable to text analysis tasks under certain conditions. Moreover, we present a general framework for text mining. The framework follows the general Kdd process, thus containing steps from preprocessing to the utilization of the results. The data mining method that we apply is based on generalized episodes and episode rules. We consider preprocessing of the text to be essential in text mining: by shifting the focus in the preprocessing phase, data mining can be used to obtain results for various purposes. We give concrete examples of how to preprocess texts based on the intended use of the discovered results and how to balance preprocessing with post-processing. We also present example applications including search for key words, key phrases and other co-occurring words, e.g. collocations and generalized concordances. These applications are both common and relevant tasks in information retrieval and natural language processing. We also present results from real-life data experiments to show that our approach is applicable in practice. 
Abstract-found: 1
Intro-found: 1
Reference: [AHKV97] <author> Helena Ahonen, Oskari Heinonen, Mika Klemettinen, and A. Inkeri Verkamo. </author> <title> Mining in the Phrasal Frontier. </title> <type> Technical Report C-1997-14, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Multiple interpretations of the words are presented on consecutive lines. The selected features are concatenated as one entity (see examples in Section 2.1). 4.2 Phrases and Co-occurring Terms In our preliminary experiments (see <ref> [AHKV97] </ref>), we selected certain interesting parts of speech and considered two simple test cases, the discovery of phrases and co-occurring terms. The latter case is particularly important in Finnish, where the word order is, in general, very flexible.
Reference: [AMS + 96] <author> Rakesh Agrawal, Heikki Mannila, Ramakrishnan Srikant, Hannu Toivonen, and A. Inkeri Verkamo. </author> <title> Fast discovery of association rules. </title> <editor> In Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthur-usamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, California, USA, </address> <year> 1996. </year>
Reference-contexts: explain the strategic decisions of the preprocessing and postprocessing phases that are necessary to focus our discovery process. 2 2.1 Episodes Episode rules and episodes are a modification of the concept of association rules and frequent sets, applied to sequential data. (For the basic definitions of association rules, see, e.g., <ref> [AMS + 96] </ref>.) Sequential data, such as text, can be seen as a sequence of pairs (feature vector, index) where feature vector consists of an ordered set of features and index contains information about the position of the word in the sequence.
Reference: [Bib93] <author> Douglas Biber. </author> <title> Co-occurrence patterns among collocations: a tool for corpus-based lexical knowledge. </title> <journal> Computational Linguistics, </journal> <volume> 19(3) </volume> <pages> 531-538, </pages> <year> 1993. </year>
Reference-contexts: If the collection is large, however, concordances can provide too much data. One way to group different word uses or to rank them in order of importance is to sort concordance lines according to the collocations surrounding the word <ref> [Bib93] </ref>. We consider an even more advanced approach, so-called generalized concordances: frequent patterns that may contain both words and grammatical features. Preprocessing may drop all sentences that do not contain the given word.
Reference: [CKPT92] <author> Douglass R. Cutting, David Karger, Jan Pedersen, and John W. Tukey. Scatter/Gather: </author> <title> A cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of the 15th Annual International ACM/SIGIR Conference, </booktitle> <pages> pages 318-329, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In addition to simple queries, con 5 tent descriptors can be used for various text classification tasks. For instance, documents can be clustered according to their similarity, e.g., to visualize a large document collection <ref> [CKPT92] </ref>. Although indexing and selecting key words are well-studied within information retrieval, new challenges have been recently set by the sudden appearance of very large heterogeneous full text document collections.
Reference: [FDK96] <author> R. Feldman, I. Dagan, and W. Klosgen. </author> <title> Efficient algorithms for mining and manipulating associations in texts. </title> <booktitle> In Cybernetics and Systems, Volume II, The Thirteenth European Meeting on Cybernetics and Systems Research, </booktitle> <address> Vi-enna, Austria, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Hence, a data mining approach should be appropriate. Surprisingly enough, only a few examples of data mining in text, or text mining, are available. The most notable are the Kdt and Fact systems <ref> [FDK96] </ref> used in mining Reuters news articles. Their approach, however, requires a substantial amount of background knowledge, and is not applicable as such to text analysis in general.
Reference: [LSJ96] <author> David D. Lewis and Karen Sparck Jones. </author> <title> Natural language processing for information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 39(1) </volume> <pages> 92-101, </pages> <year> 1996. </year>
Reference-contexts: fixed representation of the episodes and rules, much of the postprocessing can be performed by general tools such as grep and sort. 3 Applications 3.1 Information Retrieval Tasks In information retrieval | or more specifically text retrieval | key words and key phrases are commonly used to boost query processing <ref> [Sal88, LSJ96] </ref>. Consider a common information retrieval task: The user expresses his/her information needs, e.g., by giving a query, and the system executes the search by matching the query with the documents. With large collections simply scanning the text is not feasible. <p> Although indexing and selecting key words are well-studied within information retrieval, new challenges have been recently set by the sudden appearance of very large heterogeneous full text document collections. Lewis and Sparck Jones <ref> [LSJ96] </ref> consider compound key terms as one essential possibility to improve the quality of text retrieval in this new situation.
Reference: [Man96] <author> Heikki Mannila. </author> <title> Data mining: machine learning, statistics, and databases. </title> <booktitle> In Proceedings of the 8th International Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 1-6, </pages> <address> Stockholm, Sweden, </address> <year> 1996. </year>
Reference-contexts: In particular, the preprocessing phase is crucial to the efficiency of the process, since according to the results in different domain areas and applications, preprocessing can require as much as 80 per cent of the total effort <ref> [Man96] </ref>. There are certain special aspects in the preprocessing of textual data. Text consists of words, special characters, and structural information. The preprocessing required depends heavily on the intended use of the results. Typically, the data is homogenized by replacing special characters and structural information (e.g., Sgml tags) with symbols.
Reference: [MT96] <author> Heikki Mannila and Hannu Toivonen. </author> <title> Discovering generalized episodes using minimal occurrences. </title> <booktitle> In Proceedings of the Second International Conference 11 on Knowledge Discovery and Data Mining (KDD'96), </booktitle> <pages> pages 146-151, </pages> <address> Port--land, Oregon, USA, </address> <month> August </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: Moreover, since there may be several partially differing occurrences of the episode within the substring S 0 , we restrict ourselves to the distinct minimal occurrences of the episode; for a formal definition of a minimal occurrence, see <ref> [MT96] </ref>. As an example, let us think of a text sequence where the feature vector contains the base form of the word, the part of speech, and the number of the word (when appropriate). <p> The method that we have used to discover frequent episodes and episode rules in our data is described in <ref> [MT96] </ref>.
Reference: [Ril95] <author> Ellen Riloff. </author> <title> Little words can make a big diffrence for text classification. </title> <booktitle> In Proceedings of the 18th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'95), </booktitle> <pages> pages 130-136, </pages> <address> Seattle, Washington, USA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: It has been shown <ref> [Ril95] </ref> that some types of collocations are domain-dependent and, hence, good indicators of the topics covered by the document. What kind of collocations are considered interesting depends on the intended application. If they are used as content descriptors in information retrieval, their discriminating ability is one criterion.
Reference: [Sal88] <author> Gerard Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, USA, </address> <year> 1988. </year>
Reference-contexts: fixed representation of the episodes and rules, much of the postprocessing can be performed by general tools such as grep and sort. 3 Applications 3.1 Information Retrieval Tasks In information retrieval | or more specifically text retrieval | key words and key phrases are commonly used to boost query processing <ref> [Sal88, LSJ96] </ref>. Consider a common information retrieval task: The user expresses his/her information needs, e.g., by giving a query, and the system executes the search by matching the query with the documents. With large collections simply scanning the text is not feasible. <p> Common function words (prepositions, articles, etc.) are pruned. A phrase is considered interesting according to its discriminating ability. Additionally, according to Salton <ref> [Sal88] </ref>, parts of an interesting phrase should have different frequencies: the phrase head should have a frequency exceeding a stated threshold, while the other components should have a medium or low frequency; common function words should not be used.
Reference: [Sma93] <author> Frank Smadja. </author> <title> Retrieving collocations from text: </title> <journal> Xtract. Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 143-177, </pages> <year> 1993. </year> <month> 12 </month>
Reference-contexts: Postprocessing may include sorting and grouping of rules according to some features. Some examples of rules can be found in Section 4. Collocations are recurrent combinations of words corresponding to arbitrary word usages. Unlike typical phrases used in information retrieval, collocations often contain prepositions and inflected words. Smadja <ref> [Sma93] </ref> distincts three types of collocations: 1. predicative relations; e.g., frequent predicate-object pairs like make-decision, 2. rigid noun phrases; e.g., The Dow Jones average of 30 industrial stocks, and 3. phrasal templates that may contain empty slots; e.g., The average finished the week with a net loss of [NUMBER]. 6 In
References-found: 11

