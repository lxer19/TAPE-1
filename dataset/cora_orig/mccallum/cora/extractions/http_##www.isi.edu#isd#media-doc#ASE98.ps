URL: http://www.isi.edu/isd/media-doc/ASE98.ps
Refering-URL: http://www.isi.edu/isd/media-doc/media-doc-body.html
Root-URL: http://www.isi.edu
Email: ferdem,johnson,marsellag@isi.edu  
Phone: +1 310 822 1511  
Title: Task Oriented Software Understanding  
Author: Ali Erdem, W. Lewis Johnson, Stacy Marsella 
Keyword: software explanation, software understanding, user model, task model, knowledge representation  
Address: 4676 Admiralty Way Marina del Rey, CA 90292-6695  
Affiliation: USC Information Sciences Institute Computer Science Dept.  
Abstract: The main factors that affect software understanding are the complexity of the problem solved by the program, the program text, the user's mental ability and experience and the task being performed. This paper describes a planning approach solution to the software understanding problem that focuses on the user's task and expertise. First, user questions about software artifacts have been studied and the most commonly asked questions are identified. These questions are organized into a question model and procedures for answering them are developed. Then, the patterns in user questions while performing certain tasks have been studied and these patterns are used to build generic task models. The explanation system uses these task models in several ways. The task model, along with a user model, is used to generate explanations tailored to the user's task and expertise. In addition, the task model allows the system to provide explicit task support in its interface. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agnar Aamodt, Bart Benus, Cuno Duursma, Christine Tomlinson, Ronald Schrooten, and Wal-ter Van de Velde. </author> <title> Task features and their use in CommonKADS. </title> <type> Technical report, </type> <institution> Vrije Universi-tiet Brussel, University of Amsterdam, </institution> <year> 1993. </year>
Reference-contexts: The taxonomy in figure 3 is prepared after reviewing the task definitions in Serbanati Analysis identification comprehension classification assessment Synthesis design planning modeling Fault finding diagnosis repair verification Configuration Modification Informative Specification Documentation Explanation Management [14], CommonKADS <ref> [1] </ref> and Programmer's Apprentice [13]. In this taxonomy, gather is an identification task, comprehend is a comprehension task and test is an assessment task. IMPLICATIONS FOR MediaDoc We are incorporating the results of the question and the task studies into MediaDoc.
Reference: [2] <author> Deborah A. Boehm-Davis. </author> <title> Software comprehension. </title> <editor> In M. Helander, editor, </editor> <title> Handbook of Human-Computer Interaction, </title> <booktitle> chapter 5, </booktitle> <pages> pages 107-121. </pages> <publisher> Elsevier Science Publishers B.V. (North Holland), </publisher> <year> 1988. </year>
Reference-contexts: Software Understanding is the reconstruction of logic, structure and goals that were used in writing a program in order to understand what the program does and how it does it <ref> [2, 3] </ref>. This reconstruction process is typically composed of inquiry episodes [3, 15] which involve the following steps: read some code, ask a question about the code, form an hypothesis and search the documentation and the code to confirm the hypothesis.
Reference: [3] <author> Ruven Brooks. </author> <title> Towards a theory of the comprehension of computer programs. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 18 </volume> <pages> 543-554, </pages> <year> 1983. </year>
Reference-contexts: Software Understanding is the reconstruction of logic, structure and goals that were used in writing a program in order to understand what the program does and how it does it <ref> [2, 3] </ref>. This reconstruction process is typically composed of inquiry episodes [3, 15] which involve the following steps: read some code, ask a question about the code, form an hypothesis and search the documentation and the code to confirm the hypothesis. <p> Software Understanding is the reconstruction of logic, structure and goals that were used in writing a program in order to understand what the program does and how it does it [2, 3]. This reconstruction process is typically composed of inquiry episodes <ref> [3, 15] </ref> which involve the following steps: read some code, ask a question about the code, form an hypothesis and search the documentation and the code to confirm the hypothesis. The generation and verification of the hypothesis are influenced by the salient features in the code and the documentation [3]. <p> The generation and verification of the hypothesis are influenced by the salient features in the code and the documentation <ref> [3] </ref>. Attempts to solve the software understanding problem has focused on two methods: improving the search and automating the recognition of features in the code. The search process was improved either by providing an automated search capability or by changing the organization of the documents. <p> These improved search and automated recognition methods ignored one important factor, the user. Software understanding is affected by the complexity of the problem solved by the program, the program text , the user's mental ability and experience, and the task being performed <ref> [3] </ref>. The methods described above focused on the first two factors, but ignored the latter two. Even with good documentation, users still prefer asking questions to system experts or other users before consulting the documentation. There are several reasons for this behavior. <p> if we had such a tool it could have been used to answer these types of questions. * Hierarchical and multi-layered mental representations: The programming process constructs mappings from the problem domain to the programming domain, possibly through several intermediate domains and software understanding is the reconstruction of these mappings <ref> [3] </ref>. As a result, the experts' mental representation of computer programs are hierarchical and multi-layered [18]. In our studies, we observed confirming evidence for this hypothesis.
Reference: [4] <author> Sandra Carberry. </author> <title> Plan Recognition in Natural Language Dialogue. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The interactive nature of the dialogue also permits the users to ask follow-on questions and clarify the parts they did not understand. In addition, the human experts can recognize the users' plans and provide answers to satisfy their goals <ref> [4] </ref>. Finally, The experts also rec 1 ognize the users' level of expertise and provide tailored answers that are easier to understand.
Reference: [5] <author> T.A. Corbi. </author> <title> Program understanding: Challenge for the 1990s. </title> <journal> IBM Systems Journal, </journal> <volume> 28(2) </volume> <pages> 294-306, </pages> <year> 1990. </year>
Reference-contexts: INTRODUCTION AND MOTIVATION Software maintenance has become an important activity in the software industry. Maintenance of existing systems consumes 50% to 75% of the total programming effort [16] and a significant portion of this maintenance activity (30% to 60%) is spent on software understanding <ref> [5, 6] </ref>. Software Understanding is the reconstruction of logic, structure and goals that were used in writing a program in order to understand what the program does and how it does it [2, 3].
Reference: [6] <author> Premkumar T. Devanbu. </author> <title> Software Information Systems. </title> <type> PhD thesis, </type> <institution> Rutgers, The State University of New Jersey, </institution> <address> New Brunswick, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: INTRODUCTION AND MOTIVATION Software maintenance has become an important activity in the software industry. Maintenance of existing systems consumes 50% to 75% of the total programming effort [16] and a significant portion of this maintenance activity (30% to 60%) is spent on software understanding <ref> [5, 6] </ref>. Software Understanding is the reconstruction of logic, structure and goals that were used in writing a program in order to understand what the program does and how it does it [2, 3]. <p> Attempts to solve the software understanding problem has focused on two methods: improving the search and automating the recognition of features in the code. The search process was improved either by providing an automated search capability or by changing the organization of the documents. For example, Devanbu's LaSSIE system <ref> [6] </ref> used description logic to represent the domain and the basic software knowledge. Users could do searches by forming queries using the predefined domain and software concepts. Linked and layered documentation organizations have also been used to improve the search process.
Reference: [7] <author> Arthur C. Graesser, Paul J. Byrne, and Michael L. Behrens. </author> <title> Answering questions about information in databases. </title> <editor> In Thomas W. Lauer, Eileen Peacock, and Arthur C. Graesser, editors, </editor> <booktitle> Questions and Information Systems, chapter 12, </booktitle> <pages> pages 229-252. </pages> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1992. </year>
Reference-contexts: In addition, since the USENET data set was biased and did not include questions for all software engineering tasks [9], we decided to survey the research literature for other studies on questions. After reviewing Lehnert's [10] and Graesser's <ref> [7] </ref> question answering systems, Swartout's research on questions asked during expert system explanations [17], Letovsky's research on questions asked during inquiry episodes [11] and Serba-nati's list of most commonly asked types of information by programmers [14], we developed a question model.
Reference: [8] <author> John Hartman. </author> <title> Automatic Control Understanding for Natural Programs. </title> <type> PhD thesis, </type> <institution> The University of Texas at Austin, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Rajlich [12] organized documentation into a problem domain layer, an algorithm layer and a representation layer. Users easily guessed which layer would have the answer to their queries and restricted their search to that layer. The Programmer's Apprentice [13], Hartman's UN-PROG <ref> [8] </ref> and Will's GRASPR [20] programs tried to automate the recognition of the standard programming plans in the code. This approach improved both recognition of the features in the code and reduced the search space from the actual code to the programming plans.
Reference: [9] <author> W. Lewis Johnson and Ali Erdem. </author> <title> Interactive explanation of software systems. </title> <booktitle> In KBSE '95. IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: Such a tool needs to have the necessary knowledge to answer the user questions. To identify what type of questions are asked by the users and what governs the answers the experts provide, we studied the questions posted to the USENET Tcl/Tk newsgroup <ref> [9] </ref>. Based on this study and a survey of the research literature on questions, we later developed a domain independent question model for software understanding questions. The results of the USENET study are being used in the implementation of an online documentation tool called MediaDoc. <p> Wright claimed that people's interaction with documentation starts with formulating a question, therefore the documentation content needs to be determined by the questions users ask [21]. We studied the questions asked by Tcl/Tk programmers in a USENET newsgroup and identified the most commonly asked question types <ref> [9] </ref>. It was possible to request the same information in many different ways in natural language. For our purposes, What does X do?, What is the function of X?, What does X cause? all requested the same type of information. <p> We classified the questions in the data set based on the type of information requested. In addition, since the USENET data set was biased and did not include questions for all software engineering tasks <ref> [9] </ref>, we decided to survey the research literature for other studies on questions.
Reference: [10] <author> Wendy G. Lehnert. </author> <title> The Process of Question Answering: A Computer Simulation of Cognition. </title> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <year> 1978. </year>
Reference-contexts: In addition, since the USENET data set was biased and did not include questions for all software engineering tasks [9], we decided to survey the research literature for other studies on questions. After reviewing Lehnert's <ref> [10] </ref> and Graesser's [7] question answering systems, Swartout's research on questions asked during expert system explanations [17], Letovsky's research on questions asked during inquiry episodes [11] and Serba-nati's list of most commonly asked types of information by programmers [14], we developed a question model.
Reference: [11] <author> Stanley Letovsky. </author> <title> Cognitive processes in program comprehension. </title> <journal> The Journal of Systems and Software, </journal> (7):325-339, July 1987. 
Reference-contexts: After reviewing Lehnert's [10] and Graesser's [7] question answering systems, Swartout's research on questions asked during expert system explanations [17], Letovsky's research on questions asked during inquiry episodes <ref> [11] </ref> and Serba-nati's list of most commonly asked types of information by programmers [14], we developed a question model. In this model, a question is represented based on its topic, question type and the relation type. * Topic: The question topic is the entity referenced in the question.
Reference: [12] <author> V. Rajlich, J. Doran, and R.T.S. Gudla. </author> <title> Layered explanation of software: A methodology for program comprehension. </title> <booktitle> In Proceedings of the Workshop on Program Comprehension, </booktitle> <year> 1994. </year>
Reference-contexts: Linked and layered documentation organizations have also been used to improve the search process. Soloway linked parts of the documentation to delocalize the programming plans [15]. SODOS [19] project at USC linked all the Software Life Cycle documents of a software project and also provided search capabilities. Rajlich <ref> [12] </ref> organized documentation into a problem domain layer, an algorithm layer and a representation layer. Users easily guessed which layer would have the answer to their queries and restricted their search to that layer.
Reference: [13] <author> Charles Rich and Richard C. Waters. </author> <title> The Programmer's Apprentice. </title> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: Rajlich [12] organized documentation into a problem domain layer, an algorithm layer and a representation layer. Users easily guessed which layer would have the answer to their queries and restricted their search to that layer. The Programmer's Apprentice <ref> [13] </ref>, Hartman's UN-PROG [8] and Will's GRASPR [20] programs tried to automate the recognition of the standard programming plans in the code. This approach improved both recognition of the features in the code and reduced the search space from the actual code to the programming plans. <p> The taxonomy in figure 3 is prepared after reviewing the task definitions in Serbanati Analysis identification comprehension classification assessment Synthesis design planning modeling Fault finding diagnosis repair verification Configuration Modification Informative Specification Documentation Explanation Management [14], CommonKADS [1] and Programmer's Apprentice <ref> [13] </ref>. In this taxonomy, gather is an identification task, comprehend is a comprehension task and test is an assessment task. IMPLICATIONS FOR MediaDoc We are incorporating the results of the question and the task studies into MediaDoc.
Reference: [14] <author> L.D. Serbanati. </author> <title> Integrating Tools for Software Development. </title> <publisher> Yourdon Press, </publisher> <year> 1993. </year>
Reference-contexts: After reviewing Lehnert's [10] and Graesser's [7] question answering systems, Swartout's research on questions asked during expert system explanations [17], Letovsky's research on questions asked during inquiry episodes [11] and Serba-nati's list of most commonly asked types of information by programmers <ref> [14] </ref>, we developed a question model. In this model, a question is represented based on its topic, question type and the relation type. * Topic: The question topic is the entity referenced in the question. It can easily be identified as the subject of the question. <p> The taxonomy in figure 3 is prepared after reviewing the task definitions in Serbanati Analysis identification comprehension classification assessment Synthesis design planning modeling Fault finding diagnosis repair verification Configuration Modification Informative Specification Documentation Explanation Management <ref> [14] </ref>, CommonKADS [1] and Programmer's Apprentice [13]. In this taxonomy, gather is an identification task, comprehend is a comprehension task and test is an assessment task. IMPLICATIONS FOR MediaDoc We are incorporating the results of the question and the task studies into MediaDoc.
Reference: [15] <author> E. Soloway, J. Pinto, S.I. Letovsky, D. Littman, and R. Lampert. </author> <title> Designing documentation to compensate for delocalized plans. </title> <journal> Communications of the ACM, </journal> <volume> 31(11), </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: Software Understanding is the reconstruction of logic, structure and goals that were used in writing a program in order to understand what the program does and how it does it [2, 3]. This reconstruction process is typically composed of inquiry episodes <ref> [3, 15] </ref> which involve the following steps: read some code, ask a question about the code, form an hypothesis and search the documentation and the code to confirm the hypothesis. The generation and verification of the hypothesis are influenced by the salient features in the code and the documentation [3]. <p> Users could do searches by forming queries using the predefined domain and software concepts. Linked and layered documentation organizations have also been used to improve the search process. Soloway linked parts of the documentation to delocalize the programming plans <ref> [15] </ref>. SODOS [19] project at USC linked all the Software Life Cycle documents of a software project and also provided search capabilities. Rajlich [12] organized documentation into a problem domain layer, an algorithm layer and a representation layer.
Reference: [16] <author> I. Sommerville. </author> <title> Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <address> 4th edition, </address> <year> 1992. </year>
Reference-contexts: INTRODUCTION AND MOTIVATION Software maintenance has become an important activity in the software industry. Maintenance of existing systems consumes 50% to 75% of the total programming effort <ref> [16] </ref> and a significant portion of this maintenance activity (30% to 60%) is spent on software understanding [5, 6].
Reference: [17] <author> William R. Swartout and Johanna D. Moore. </author> <title> Explanation in second generation expert systems. </title> <editor> In Jean Marc David, Jean-Paul Krivine, and Reid Simmons, editors, </editor> <booktitle> Second Generation Expert Systems, chapter 24, </booktitle> <pages> pages 543-585. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: After reviewing Lehnert's [10] and Graesser's [7] question answering systems, Swartout's research on questions asked during expert system explanations <ref> [17] </ref>, Letovsky's research on questions asked during inquiry episodes [11] and Serba-nati's list of most commonly asked types of information by programmers [14], we developed a question model.
Reference: [18] <author> Susan Wiedenbeck and Vikki Fix. </author> <title> Characteristics of the mental representations of novice and expert programmers: an empirical study. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 39 </volume> <pages> 793-812, </pages> <year> 1993. </year>
Reference-contexts: As a result, the experts' mental representation of computer programs are hierarchical and multi-layered <ref> [18] </ref>. In our studies, we observed confirming evidence for this hypothesis. The first subject constructed mappings from the vehicle behaviors of the running program to ModSAF tasks, from ModSAF tasks to finite state machines and finally from the finite state machines to the code.
Reference: [19] <author> Ronald Charles Williamson. </author> <title> SODOS A Software Documentation Support Environment. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <address> Los Angeles, </address> <month> De-cember </month> <year> 1984. </year>
Reference-contexts: Users could do searches by forming queries using the predefined domain and software concepts. Linked and layered documentation organizations have also been used to improve the search process. Soloway linked parts of the documentation to delocalize the programming plans [15]. SODOS <ref> [19] </ref> project at USC linked all the Software Life Cycle documents of a software project and also provided search capabilities. Rajlich [12] organized documentation into a problem domain layer, an algorithm layer and a representation layer.
Reference: [20] <author> Linda Mary Wills. </author> <title> Automated Program Recognition by Graph Parsing. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Rajlich [12] organized documentation into a problem domain layer, an algorithm layer and a representation layer. Users easily guessed which layer would have the answer to their queries and restricted their search to that layer. The Programmer's Apprentice [13], Hartman's UN-PROG [8] and Will's GRASPR <ref> [20] </ref> programs tried to automate the recognition of the standard programming plans in the code. This approach improved both recognition of the features in the code and reduced the search space from the actual code to the programming plans.
Reference: [21] <author> P. Wright. </author> <title> Issues of content and presentation in document design. </title> <editor> In M. Helander, editor, </editor> <title> Handbook of Human-Computer Interaction, </title> <booktitle> chapter 28, </booktitle> <pages> pages 629-652. </pages> <publisher> Elsevier Science Publishers B.V. (North Holland), </publisher> <year> 1988. </year>
Reference-contexts: Even with good documentation, users still prefer asking questions to system experts or other users before consulting the documentation. There are several reasons for this behavior. First, the dialogue between the user and the expert facilitates the refinement of the questions <ref> [21] </ref>. The interactive nature of the dialogue also permits the users to ask follow-on questions and clarify the parts they did not understand. In addition, the human experts can recognize the users' plans and provide answers to satisfy their goals [4]. <p> QUESTION MODEL Questions are the basis of user's interaction with the documentation and the system experts. Wright claimed that people's interaction with documentation starts with formulating a question, therefore the documentation content needs to be determined by the questions users ask <ref> [21] </ref>. We studied the questions asked by Tcl/Tk programmers in a USENET newsgroup and identified the most commonly asked question types [9]. It was possible to request the same information in many different ways in natural language.
References-found: 21

