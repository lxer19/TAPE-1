URL: ftp://cse.ogi.edu/pub/dsrg/HOPE/pvm94.ps.Z
Refering-URL: http://www.cse.ogi.edu/~crispin/
Root-URL: http://www.cse.ogi.edu
Email: crispin@csd.uwo.ca  
Title: Optimistic Programming in PVM  
Author: Crispin Cowan 
Note: This research is supported by the National Science and Engineering Research Council of Canada (NSERC).  
Date: May 17, 1994  
Address: Middlesex College  Ontario London, Ontario N6A 5B7  
Affiliation: Computer Science Department  University of Western  
Abstract: Optimism is a powerful technique for increasing concurrency. A program can gain concurrency by making an optimistic assumption about its future state, and verifying the assumption in parallel with computations based on the optimistic assumption. To date, use of optimism has been restricted to specialized systems due to the difficulty of writing optimistic programs. In this paper, we define and justify the definition of optimism as any computation that uses rollback. Optimism is effective in avoiding the latency of predictable computations. Performing computations remotely incurs the additional latency of communications time, and so remote computations make good candidates for the application of optimism. By assuming the outcome of a remote computation, a task can proceed in parallel with confirmation of the guess. By guessing correctly, a task can save the amount of time that remote confirmation would have required. HOPE is our model of optimistic computation built upon these notions. HOPE consists of one data type and four operations that provide for the concise and portable expression of optimistic algorithms. HOPE can be embedded in any system providing concurrent task that communicate by exchanging messages. This paper describes our current effort to construct a HOPE prototype on top of PVM. The basic requirements are to integrate PVM with a task checkpoint and rollback mechanism, make the PVM 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jonathan R. Agre and Divyakant Agrawal. </author> <title> Recovering From Process Failures in the Time Warp Mechanism. </title> <booktitle> In 8th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 53-61, </pages> <address> Seattle, WA, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: Optimistically assuming that such a contributing factor does not exist increases available parallelism. Jefferson's Virtual Time [13] is a algorithm for distributed process synchronization that has been widely studied <ref> [1, 12, 14, 18, 22] </ref>.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities. </title> <booktitle> In Proc. AFIPS 1967 Spring Joint Computer Conference 30, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: However, it should also be noted that whenever rollback occurs, other rollback-free algorithms would require blocking for an amount of time equal to that spent on wasted computation [13]. Amdahl's Law <ref> [2] </ref> states that the speedup due to an enhancement is: Speedup = 1 (1fraction enhanced)+ fraction enhanced speedup of enhancement This equation shows that the speedup due to parallelism is limited by the fraction of a program that can actually be parallelized.
Reference: [3] <author> David F. Bacon and Robert E. Strom. </author> <title> Optimistic Parallelization of Communicating Sequential Processes. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: A 100 MIPS CPU can execute over 3 million instructions while waiting for a response from the opposite coast. Bacon and Strom <ref> [3] </ref> present an optimistic transformation for the parallelization of sequential statements S1; S2 based on ideas in [24].
Reference: [4] <author> Daniel J. Bernstein. </author> <title> Poor Man's Checkpointer. Public Domain Software, </title> <year> 1990. </year>
Reference-contexts: This relative scarcity is to be expected, because of the global consistency problems that rollback in a distributed environment introduces, requiring either a global snapshot [17], or a dependency tracking mechanism such as HOPE. In view of this scarcity, we decided to use Pmckpt <ref> [4] </ref>, a source-level checkpoint and restart facility, modify it to provide rollback, and then integrate it with PVM. Pmckpt works by inserting variables into the source code to gather the start and end addresses of the data, stack, and heap areas, and then writing them out to disk.
Reference: [5] <author> R.G. Bubenik. </author> <title> Optimistic Computation. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: To overcome these obstacles and make optimism more attractive to developers and researchers, the rollback and bookkeeping tasks of optimism must be automated. In this section, we review two previous efforts to automate parts of optimism. 12 5.1 Bubenik's Optimistic Computation In <ref> [5, 6] </ref>, Bubenik et al describe a formalism for automatically transforming pessimistic algorithms into optimistic algorithms based on concepts in [23, 24]. Bubenik translates the algorithm into a program dependence graph, and then performs optimistic transformations on the graph. <p> Bubenik translates the algorithm into a program dependence graph, and then performs optimistic transformations on the graph. Bubenik also constructed an optimistic run-time environment in which to exercise these transformations <ref> [5, 7] </ref>. Bubenik provides an operating systems facility to execute encapsulations whose outputs are concealed until the computation is mandated. The scope of an optimistically executed encapsulation is statically bounded. An encapsulation can be mandated or aborted asynchronously with its execution. Bubenik's system does not perform dependency tracking.
Reference: [6] <author> Rick Bubenik and Willy Zwaenepoel. </author> <title> Semantics of Optimistic Computation. </title> <booktitle> In 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 20-27, </pages> <year> 1990. </year>
Reference-contexts: To overcome these obstacles and make optimism more attractive to developers and researchers, the rollback and bookkeeping tasks of optimism must be automated. In this section, we review two previous efforts to automate parts of optimism. 12 5.1 Bubenik's Optimistic Computation In <ref> [5, 6] </ref>, Bubenik et al describe a formalism for automatically transforming pessimistic algorithms into optimistic algorithms based on concepts in [23, 24]. Bubenik translates the algorithm into a program dependence graph, and then performs optimistic transformations on the graph.
Reference: [7] <author> Rick Bubenik and Willy Zwaenepoel. </author> <title> Optimistic Make. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(2) </volume> <pages> 207-217, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Bubenik translates the algorithm into a program dependence graph, and then performs optimistic transformations on the graph. Bubenik also constructed an optimistic run-time environment in which to exercise these transformations <ref> [5, 7] </ref>. Bubenik provides an operating systems facility to execute encapsulations whose outputs are concealed until the computation is mandated. The scope of an optimistically executed encapsulation is statically bounded. An encapsulation can be mandated or aborted asynchronously with its execution. Bubenik's system does not perform dependency tracking. <p> Non-optimistic segments of a program cannot benefit from the optimism of an encapsulation if it does not propagate its results until it is mandated. Bubenik and Zwaenepoel argue their granularity is deliberately large for efficiency reasons <ref> [7] </ref>.
Reference: [8] <author> Crispin Cowan. </author> <title> Optimistic Replication in HOPE. </title> <booktitle> In Proceedings of the 1992 CAS Conference, </booktitle> <pages> pages 269-282, </pages> <address> Toronto, Ontario, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance [23], replication <ref> [8, 11, 25] </ref>, concurrency control [15, 14], and discrete event simulation [13]. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task. <p> In each case, we will isolate the serial bottleneck being addressed, the optimistic assumption (s) being made, the procedure to verify the optimistic assumption, and the contingency plan should the assumption prove false. In <ref> [8, 10] </ref>, we provide greater detail on the applications of optimism. 3.1 Kung and Robinson's Optimistic Concurrency Control Concurrency control is the problem of scheduling concurrent operations in such a way that they appear to have been performed in a single sequence with respect to the data objects that they share,
Reference: [9] <author> Crispin Cowan and Hanan Lutfiyya. </author> <title> Formal Semantics for Expressing Optimism. </title> <type> Report 414, </type> <institution> Computer Science Department, </institution> <address> UWO, London, Ontario, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: This section describes the prototype HOPE system, which is being constructed on top of the C version PVM. 4.1 Overview In <ref> [9] </ref>, we provide an operational semantic definition for the HOPE primitives. The design of the HOPE/PVM prototype basically follows this definition. We define an IDO (I Depend On) set as the set of aid ts that a given task depends on. <p> Adding a printf like interface on top of PVM's primitives would greatly shorten the user code necessary to express algorithms such as Call Streaming. A Formal Semantics of Optimism: A formal semantics of optimism, based on the HOPE programming model, has been developed <ref> [9] </ref>. We believe this to be the first formal semantics for expressing optimism. Such a semantics should enable researchers to formally reason about optimistic algorithms. We also hope to prove the correctness of the HOPE implementation algorithms with respect to the semantics of HOPE.
Reference: [10] <author> Crispin Cowan, Hanan Lutfiyya, and Mike Bauer. </author> <title> Increasing Concurrency Through Optimism: A Reason for HOPE. </title> <booktitle> In Proceedings of the 1994 ACM Computer Science Conference, </booktitle> <pages> pages 218-225, </pages> <address> Phoenix, Arizona, </address> <month> March </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: Sometimes it is quite subtle, such as in fault tolerance applications, where the concurrency introduced is between the volatile and stable-storage components of the system. <ref> [10] </ref> examines ways in which optimism can be used to increase concurrency. It should be noted that while optimism always increases concurrency, it does not always improve performance. <p> In each case, we will isolate the serial bottleneck being addressed, the optimistic assumption (s) being made, the procedure to verify the optimistic assumption, and the contingency plan should the assumption prove false. In <ref> [8, 10] </ref>, we provide greater detail on the applications of optimism. 3.1 Kung and Robinson's Optimistic Concurrency Control Concurrency control is the problem of scheduling concurrent operations in such a way that they appear to have been performed in a single sequence with respect to the data objects that they share,
Reference: [11] <author> Arthur P. Goldberg. </author> <title> Optimistic Algorithms for Distributed Transparent Process Repli--cation. </title> <type> PhD thesis, </type> <institution> University of California at Los Angeles, </institution> <year> 1991. </year> <note> (UCLA Tech. Report CSD-910050). </note>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance [23], replication <ref> [8, 11, 25] </ref>, concurrency control [15, 14], and discrete event simulation [13]. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task.
Reference: [12] <author> Anurag Gupta, Ian F. Akyildiz, and Richard M. Fujimoto. </author> <title> Performance Analysis of Time Warp with Multiple Homogeneous Processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(10) </volume> <pages> 1013-1027, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Optimistically assuming that such a contributing factor does not exist increases available parallelism. Jefferson's Virtual Time [13] is a algorithm for distributed process synchronization that has been widely studied <ref> [1, 12, 14, 18, 22] </ref>.
Reference: [13] <author> D. Jefferson. </author> <title> Virtual Time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(7) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance [23], replication [8, 11, 25], concurrency control [15, 14], and discrete event simulation <ref> [13] </ref>. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task. A computation is a sequence of states that have occurred in the execution of a task. <p> However, it should also be noted that whenever rollback occurs, other rollback-free algorithms would require blocking for an amount of time equal to that spent on wasted computation <ref> [13] </ref>. Amdahl's Law [2] states that the speedup due to an enhancement is: Speedup = 1 (1fraction enhanced)+ fraction enhanced speedup of enhancement This equation shows that the speedup due to parallelism is limited by the fraction of a program that can actually be parallelized. <p> Optimistically assuming that such a contributing factor does not exist increases available parallelism. Jefferson's Virtual Time <ref> [13] </ref> is a algorithm for distributed process synchronization that has been widely studied [1, 12, 14, 18, 22].
Reference: [14] <author> D. Jefferson and A. Motro. </author> <title> The Time Warp Mechanism for Database Concurrency Control. </title> <type> Report Technical Report TR-84-302, </type> <institution> University of Southern California, </institution> <month> January </month> <year> 1984. </year>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance [23], replication [8, 11, 25], concurrency control <ref> [15, 14] </ref>, and discrete event simulation [13]. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task. A computation is a sequence of states that have occurred in the execution of a task. <p> Optimistically assuming that such a contributing factor does not exist increases available parallelism. Jefferson's Virtual Time [13] is a algorithm for distributed process synchronization that has been widely studied <ref> [1, 12, 14, 18, 22] </ref>. <p> HOPE's more general ability to express optimism on Bubenik's optimistic transformation techniques. 5.2 Time Warp Time Warp is specifically designed and marketed as a discrete event simulation system, but it could also be viewed as a general programming environment, and in fact has been examined as a concurrency control mechanism <ref> [14] </ref>. In that it provides concurrent processes and message passing, it is much like a conventional distributed system. In addition, it provides the programmer with global clock to use for synchronization, and then makes optimistic scheduling assumptions about the global clock.
Reference: [15] <author> H.T. Kung and John T. Robinson. </author> <title> On Optimistic Methods for Concurrency Control. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(2) </volume> <pages> 213-226, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance [23], replication [8, 11, 25], concurrency control <ref> [15, 14] </ref>, and discrete event simulation [13]. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task. A computation is a sequence of states that have occurred in the execution of a task. <p> A conservative approach to scheduling would be to look at each operation, and try to find a schedule that will not force any of the operations to roll back. However, there are no deadlock-free conservative protocols that always provide high concurrency <ref> [15] </ref>. Thus the problem of finding a schedule forms a serial bottleneck to the concurrent execution of operations on shared data. Kung and Robinson's Optimistic Concurrency Control [15] is an optimistic algorithm for scheduling concurrent transactions. <p> However, there are no deadlock-free conservative protocols that always provide high concurrency <ref> [15] </ref>. Thus the problem of finding a schedule forms a serial bottleneck to the concurrent execution of operations on shared data. Kung and Robinson's Optimistic Concurrency Control [15] is an optimistic algorithm for scheduling concurrent transactions.
Reference: [16] <author> Leslie Lamport. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Time Warp is a discrete event simulation system based on the Virtual Time concept that provides the illusion of a globally synchronized clock that can be used to preserve a total ordering across the system as defined by Lamport <ref> [16] </ref>, even though processes are actually being executed out of order. Thus the semantics that it guarantees are those of a sequentially executed series of computations.
Reference: [17] <author> Juan Leon. </author> <title> Fail-Safe PVM. </title> <type> TR CMU-CS-93-124, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year>
Reference-contexts: Apparently, rollback facilities that can deal sensibly with message passing environments are virtually non-existent. This relative scarcity is to be expected, because of the global consistency problems that rollback in a distributed environment introduces, requiring either a global snapshot <ref> [17] </ref>, or a dependency tracking mechanism such as HOPE. In view of this scarcity, we decided to use Pmckpt [4], a source-level checkpoint and restart facility, modify it to provide rollback, and then integrate it with PVM.
Reference: [18] <author> Yi-Bing Lin and Edward D. Lazowska. </author> <title> Optimality Considerations of the `Time Warp' Parallel Simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 29-34, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Optimistically assuming that such a contributing factor does not exist increases available parallelism. Jefferson's Virtual Time [13] is a algorithm for distributed process synchronization that has been widely studied <ref> [1, 12, 14, 18, 22] </ref>.
Reference: [19] <author> Sun Microsystems. </author> <title> TMPFS Memory Based File System. section 4 of the SunOS 4.1 manual. </title>
Reference-contexts: The checkpointing of a task involves considerable writing of data to disk. When real disk systems are used, the cost can be considerable. However, on some systems (such as SunOS <ref> [19] </ref>), /tmp is actually implemented as a RAM disk that pages to memory, and other systems provide very large buffering of the file system in the user's address space, and so the latency of actually writing this data to disk can be avoided.
Reference: [20] <author> C. Papadimitriou. </author> <title> The Serializability of Concurrent Updates. </title> <journal> Journal of the ACM, </journal> <volume> 24(4) </volume> <pages> 631-653, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: provide greater detail on the applications of optimism. 3.1 Kung and Robinson's Optimistic Concurrency Control Concurrency control is the problem of scheduling concurrent operations in such a way that they appear to have been performed in a single sequence with respect to the data objects that they share, called serializable <ref> [20] </ref>. A conservative approach to scheduling would be to look at each operation, and try to find a schedule that will not force any of the operations to roll back. However, there are no deadlock-free conservative protocols that always provide high concurrency [15].
Reference: [21] <author> J. Peterson and A. Silberschatz. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: We refer to the time spent in such sequential components of a program as latency. Many techniques have been developed to hide or work around latency. The notion of multiprogramming was first invented to hide the latency of slow I/O devices to get more utilization from expensive CPU's <ref> [21, page 19] </ref>. Multiprogramming is a traditional way to hide latency from a computer by using parallelism.
Reference: [22] <author> Peter Reiher, Richard Fujimoto, Steven Bellenot, and David Jefferson. </author> <title> Cancellation Strategies in Optimistic Execution Systems. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 112-121, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Optimistically assuming that such a contributing factor does not exist increases available parallelism. Jefferson's Virtual Time [13] is a algorithm for distributed process synchronization that has been widely studied <ref> [1, 12, 14, 18, 22] </ref>.
Reference: [23] <author> R.E. Strom and S. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year> <month> 16 </month>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance <ref> [23] </ref>, replication [8, 11, 25], concurrency control [15, 14], and discrete event simulation [13]. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task. <p> However, waiting for writes to stable storage introduces large amounts of latency because I/O devices are very slow parts of a computing system. Better performance is obtained by performing I/O operations concurrent with subsequent computations. Strom and Yemini <ref> [23] </ref> present a system for high-performance fault-tolerant distributed computing based on the optimistic notion that computers mostly do not fail. <p> In this section, we review two previous efforts to automate parts of optimism. 12 5.1 Bubenik's Optimistic Computation In [5, 6], Bubenik et al describe a formalism for automatically transforming pessimistic algorithms into optimistic algorithms based on concepts in <ref> [23, 24] </ref>. Bubenik translates the algorithm into a program dependence graph, and then performs optimistic transformations on the graph. Bubenik also constructed an optimistic run-time environment in which to exercise these transformations [5, 7].
Reference: [24] <author> Rob Strom and Shaula Yemini. </author> <title> Synthesizing Distributed and Parallel Programs through Optimistic Transformations. </title> <editor> In Y. Yemini, editor, </editor> <booktitle> Current Advances in Distrubuted Computing and Communications, </booktitle> <pages> pages 234-256. </pages> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1987. </year>
Reference-contexts: It should be noted that while optimism always increases concurrency, it does not always improve performance. Improved performance depends on the probability of the assumption being correct, the costs of rolling back computations, and the fixed costs of making rollback of computations possible <ref> [24] </ref>. However, it should also be noted that whenever rollback occurs, other rollback-free algorithms would require blocking for an amount of time equal to that spent on wasted computation [13]. <p> A 100 MIPS CPU can execute over 3 million instructions while waiting for a response from the opposite coast. Bacon and Strom [3] present an optimistic transformation for the parallelization of sequential statements S1; S2 based on ideas in <ref> [24] </ref>. <p> In this section, we review two previous efforts to automate parts of optimism. 12 5.1 Bubenik's Optimistic Computation In [5, 6], Bubenik et al describe a formalism for automatically transforming pessimistic algorithms into optimistic algorithms based on concepts in <ref> [23, 24] </ref>. Bubenik translates the algorithm into a program dependence graph, and then performs optimistic transformations on the graph. Bubenik also constructed an optimistic run-time environment in which to exercise these transformations [5, 7].
Reference: [25] <author> P. Triantafillou and D.J. Taylor. </author> <title> A New Paradigm for High Availability and Efficiency in Replicated and Distributed Databases. </title> <booktitle> In 2nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 136-143, </pages> <month> December </month> <year> 1990. </year> <month> 17 </month>
Reference-contexts: Any kind of assumption can be made, as long as a reliable method exists for checking that the assumption was correct. Optimistic algorithms have been used for fault tolerance [23], replication <ref> [8, 11, 25] </ref>, concurrency control [15, 14], and discrete event simulation [13]. For our purposes, a program is composed of concurrent tasks that execute operations that cause events that change the state of a task. <p> Write operations must engage in a protocol with all of the replicas of an object so that the object stays consistent, i.e. appears the same regardless of which replica is accessed. Thus write operations in a replicated system actually suffer greater latency than in non-replicated systems. Triantafillou <ref> [25] </ref> presents a consistency mechanism for replicated data in transaction-based distributed systems. Triantafillou makes two optimistic assumptions: 1. That the selected replica is actually up to date. 2. That a write lock on a replica will be granted by a quorum of the replicas involved.
References-found: 25

