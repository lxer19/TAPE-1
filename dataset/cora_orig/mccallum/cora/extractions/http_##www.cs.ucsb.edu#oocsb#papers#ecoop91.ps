URL: http://www.cs.ucsb.edu/oocsb/papers/ecoop91.ps
Refering-URL: http://www.csd.uu.se/~thomasl/wpo/oo-compilation-papers.html
Root-URL: 
Email: -urs,craig,ungar-@self.stanford.edu  
Title: Optimizing Dynamically-Typed Object-Oriented Languages With Polymorphic Inline Caches  
Author: Urs Hlzle Craig Chambers David Ungar 
Address: Stanford, CA 94305  
Affiliation: Computer Systems Laboratory, Stanford University,  
Abstract: Polymorphic inline caches (PICs) provide a new way to reduce the overhead of polymorphic message sends by extending inline caches to include more than one cached lookup result per call site. For a set of typical object-oriented SELF programs, PICs achieve a median speedup of 11%. As an important side effect, PICs collect type information by recording all of the receiver types actually used at a given call site. The compiler can exploit this type information to generate better code when recompiling a method. An experimental version of such a system achieves a median speedup of 27% for our set of SELF programs, reducing the number of non-inlined message sends by a factor of two. Implementations of dynamically-typed object-oriented languages have been limited by the paucity of type information available to the compiler. The abundance of the type information provided by PICs suggests a new compilation approach for these languages, adaptive compilation. Such compilers may succeed in generating very efficient code for the time-critical parts of a program without incurring distracting compilation pauses. 
Abstract-found: 1
Intro-found: 1
Reference: [BI82] <author> A. H. Borning and D. H. H. Ingalls, </author> <title> A Type Declaration and Inference System for Smalltalk. </title> <booktitle> In Conference Record of the Ninth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 133-139, </pages> <year> 1982. </year>
Reference-contexts: In this scenario, type information would flow from the system to the user, in contrast to other approaches where type information flows from the user to the compiler <ref> [Suz81, BI82, JGZ88] </ref>. In our system, the programmer benefits from type information even for programs which do not contain any type declarations, and the declarations are not needed to obtain good performance.
Reference: [Cha91] <author> Craig Chambers, </author> <title> The Design and Implementation of the SELF Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University. </institution> <note> In preparation. </note>
Reference-contexts: techniques such as type analysis, customization, and splitting have been shown to be very effective in reducing this disparity: for example, these techniques applied to the SELF language bring its performance to within a factor of two of optimized C for small C-like programs such as the Stanford integer benchmarks <ref> [CU90, CU91, Cha91] </ref>. However, larger, object-oriented SELF programs benefit less from these techniques. For example, the Richards operating system benchmark in SELF is four times slower than optimized C. In addition, techniques like type analysis lengthen compile time.
Reference: [CPL83] <author> Thomas J. Conroy and Eduardo Pelegri-Llopart, </author> <title> An Assessment of Method-Lookup Caches for Smalltalk-80 Implementations. </title> <booktitle> In [Kra83]. </booktitle>
Reference: [CU89] <author> Craig Chambers and David Ungar, </author> <title> Customization: Optimizing Compiler Technology for SELF, a Dynamically-Typed Object-Oriented Programming Language. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(7), </note> <month> July, </month> <year> 1989. </year>
Reference: [CUL89] <author> Craig Chambers, David Ungar, and Elgin Lee, </author> <title> An Efficient Implementation of SELF, a Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> In OOPSLA 89 Conference Proceedings, </booktitle> <pages> pp. 49-70, </pages> <address> New Orleans, LA, </address> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October, </month> <year> 1989. </year>
Reference-contexts: SOAR (a Smalltalk implementation for a RISC processor) would be 33% slower without inline caching [Ung86]. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does our SELF system <ref> [CUL89] </ref>. 3. Handling Polymorphic Sends Inline caches are effective only if the receiver type (and thus the call target) remains relatively constant at a call site. <p> The performance impact of inline cache misses becomes more severe in highly efficient systems, where it can no longer be ignored. For example, measurements for the SELF system show that the Richards benchmark spends about 25% of its time handling inline cache misses <ref> [CUL89] </ref>. An informal examination of polymorphic call sites in the SELF system showed that in most cases the degree of polymorphism is small, typically less than ten. <p> Implementation and Results We implemented PICs for the SELF system, an efficient implementation of a dynamically-typed object-oriented language <ref> [CUL89, CU90, CU91] </ref>. All measurements were done on a lightly-loaded Sun-4/260 with 48 MB of memory. The base system used for comparison was the current SELF system as of September 1990. <p> For example, the Sun-4/260 on which our measurements were made executes about 10 million native instructions per second. The optimal calling sequence consists of two instructions per call. This would seem to limit SELF programs to significantly less than 5 million message sends per second (MiMS; see <ref> [CUL89] </ref>) even if every send was implemented optimally. However, many programs execute at 5 MiMS in our current system, and some benchmarks exceed 20 MiMS. <p> Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. 4.2. Customization Customization is another technique for determining the types of many message receivers in a method <ref> [CUL89] </ref>. Customization extends dynamic compilation by exploiting the fact that many messages within a method are sent to self. The compiler creates a separate compiled version of a given source method for each receiver type. This duplication allows the compiler to customize each version to the specific receiver type. <p> Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph <ref> [CUL89, CU90, CU91] </ref>. For example, suppose that an object is known to be an integer in one branch of an if statement and a oating-point number in the other branch.
Reference: [CU90] <author> Craig Chambers and David Ungar, </author> <title> Iterative Type Analysis and Extended Message Splitting: Optimizing Dynamically-Typed Object-Oriented Programs. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June, </month> <year> 1990. </year> <note> Published as SIGPLAN Notices 25(6), </note> <month> June, </month> <year> 1990. </year>
Reference-contexts: techniques such as type analysis, customization, and splitting have been shown to be very effective in reducing this disparity: for example, these techniques applied to the SELF language bring its performance to within a factor of two of optimized C for small C-like programs such as the Stanford integer benchmarks <ref> [CU90, CU91, Cha91] </ref>. However, larger, object-oriented SELF programs benefit less from these techniques. For example, the Richards operating system benchmark in SELF is four times slower than optimized C. In addition, techniques like type analysis lengthen compile time. <p> Implementation and Results We implemented PICs for the SELF system, an efficient implementation of a dynamically-typed object-oriented language <ref> [CUL89, CU90, CU91] </ref>. All measurements were done on a lightly-loaded Sun-4/260 with 48 MB of memory. The base system used for comparison was the current SELF system as of September 1990. <p> Type Analysis and Splitting Type analysis tries to get the most out of the available type information by propagating it through the control ow graph and by performing ow-sensitive analysis <ref> [CU90, CU91] </ref>. The compiler uses the type information obtained through the analysis to inline additional message sends and to reduce the cost of primitives (either by constant-folding the primitive or by avoiding run-time type checks of the primitives arguments). <p> Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph <ref> [CUL89, CU90, CU91] </ref>. For example, suppose that an object is known to be an integer in one branch of an if statement and a oating-point number in the other branch.
Reference: [CU91] <author> Craig Chambers and David Ungar, </author> <title> Making Pure Object-Oriented Languages Practical. To be presented at OOPSLA 91, </title> <address> Phoenix, AZ, </address> <month> October, </month> <year> 1991. </year>
Reference-contexts: techniques such as type analysis, customization, and splitting have been shown to be very effective in reducing this disparity: for example, these techniques applied to the SELF language bring its performance to within a factor of two of optimized C for small C-like programs such as the Stanford integer benchmarks <ref> [CU90, CU91, Cha91] </ref>. However, larger, object-oriented SELF programs benefit less from these techniques. For example, the Richards operating system benchmark in SELF is four times slower than optimized C. In addition, techniques like type analysis lengthen compile time. <p> Implementation and Results We implemented PICs for the SELF system, an efficient implementation of a dynamically-typed object-oriented language <ref> [CUL89, CU90, CU91] </ref>. All measurements were done on a lightly-loaded Sun-4/260 with 48 MB of memory. The base system used for comparison was the current SELF system as of September 1990. <p> Type Analysis and Splitting Type analysis tries to get the most out of the available type information by propagating it through the control ow graph and by performing ow-sensitive analysis <ref> [CU90, CU91] </ref>. The compiler uses the type information obtained through the analysis to inline additional message sends and to reduce the cost of primitives (either by constant-folding the primitive or by avoiding run-time type checks of the primitives arguments). <p> Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph <ref> [CUL89, CU90, CU91] </ref>. For example, suppose that an object is known to be an integer in one branch of an if statement and a oating-point number in the other branch.
Reference: [Deu83] <author> L. Peter Deutsch, </author> <title> The Dorado Smalltalk-80 Implementation: Hardware Architectures Impact on Software Architecture. </title> <booktitle> In [Kra83]. </booktitle>
Reference: [DMSV89] <author> R. Dixon, T. McKee, P. Schweitzer, and M. Vaughan, </author> <title> A Fast Method Dispatcher for Compiled Languages with Multiple Inheritance. </title> <booktitle> In OOPSLA 89 Conference Proceedings, </booktitle> <pages> pp. 211-214, </pages> <address> New Orleans, LA, </address> <month> October, </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October, </month> <year> 1989. </year>
Reference-contexts: Related Work Statically-typed languages can handle polymorphic sends in constant time by indexing into a type-specific function table, thus reducing the lookup to an indirect procedure call. In C++, for example, a dynamically-bound call takes between 5 and 9 cycles on a SPARC <ref> [Ros88, DMSV89, ES90, PW90] </ref>. This is possible because static type checking can guarantee the success of the lookup, i.e. the result of the table lookup need not be verified.
Reference: [DS84] <author> L. Peter Deutsch and Alan Schiffman, </author> <title> Efficient Implementation of the Smalltalk-80 System. </title> <booktitle> Proceedings of the 11th Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <year> 1984. </year>
Reference-contexts: All of these techniques have been used by Smalltalk-80 implementations. 2.1. Dynamic Compilation Early implementations of Smalltalk interpreted the byte codes produced by the Smalltalk compiler [GR83]. The interpretation overhead was significant, so later implementations doubled performance by dynamically compiling and caching machine code <ref> [DS84] </ref>. This technique is known as dynamic compilation (called dynamic translation in [DS84]). Translation to native code is the basis for efficient implementations; most of the techniques described here would not make sense in an interpreted system. <p> Dynamic Compilation Early implementations of Smalltalk interpreted the byte codes produced by the Smalltalk compiler [GR83]. The interpretation overhead was significant, so later implementations doubled performance by dynamically compiling and caching machine code <ref> [DS84] </ref>. This technique is known as dynamic compilation (called dynamic translation in [DS84]). Translation to native code is the basis for efficient implementations; most of the techniques described here would not make sense in an interpreted system. <p> This form of caching is called inline caching since the target address is stored at the send point, i.e. in the callers code <ref> [DS84] </ref>. Inline caching is surprisingly effective, with a hit ratio of 95% for Smalltalk code [DS84, Ung86, UP87]. SOAR (a Smalltalk implementation for a RISC processor) would be 33% slower without inline caching [Ung86]. <p> This form of caching is called inline caching since the target address is stored at the send point, i.e. in the callers code [DS84]. Inline caching is surprisingly effective, with a hit ratio of 95% for Smalltalk code <ref> [DS84, Ung86, UP87] </ref>. SOAR (a Smalltalk implementation for a RISC processor) would be 33% slower without inline caching [Ung86]. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does our SELF system [CUL89]. 3. <p> Along the branch where the type test succeeds, the compiler has precise information about the type of the receiver and can statically bind and inline a copy of the message. For example, existing SELF and Smalltalk systems predict that + will be sent to an integer <ref> [UP82, GR83, DS84] </ref>, since measurements indicate that this occurs 90% of the time [UP87]. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. 4.2.
Reference: [ES90] <author> Margaret A. Ellis and Bjarne Stroustrup, </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: Related Work Statically-typed languages can handle polymorphic sends in constant time by indexing into a type-specific function table, thus reducing the lookup to an indirect procedure call. In C++, for example, a dynamically-bound call takes between 5 and 9 cycles on a SPARC <ref> [Ros88, DMSV89, ES90, PW90] </ref>. This is possible because static type checking can guarantee the success of the lookup, i.e. the result of the table lookup need not be verified.
Reference: [GJ90] <author> Justin Graver and Ralph Johnson, </author> <title> A Type System for Smalltalk. </title> <booktitle> In Conference Record of the 17th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> San Francisco, CA, </address> <month> January, </month> <year> 1990. </year>
Reference: [GR83] <author> Adele Goldberg and David Robson, </author> <title> Smalltalk-80: The Language and Its Implementation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Background To present PICs in context, we first review existing well-known techniques for improving the efficiency of dynamically-typed object-oriented languages. All of these techniques have been used by Smalltalk-80 implementations. 2.1. Dynamic Compilation Early implementations of Smalltalk interpreted the byte codes produced by the Smalltalk compiler <ref> [GR83] </ref>. The interpretation overhead was significant, so later implementations doubled performance by dynamically compiling and caching machine code [DS84]. This technique is known as dynamic compilation (called dynamic translation in [DS84]). <p> Along the branch where the type test succeeds, the compiler has precise information about the type of the receiver and can statically bind and inline a copy of the message. For example, existing SELF and Smalltalk systems predict that + will be sent to an integer <ref> [UP82, GR83, DS84] </ref>, since measurements indicate that this occurs 90% of the time [UP87]. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. 4.2.
Reference: [Han74] <author> Gilbert J. Hansen, </author> <title> Adaptive Systems for the Dynamic Run-Time Optimization of Programs. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1974. </year>
Reference-contexts: In this section we outline the benefits and problems of a new framework for efficient implementations of dynamically-typed object-oriented languages, based on the idea of incremental optimization of compiled code in an environment where type information is relatively abundant. In the tradition of Hansen <ref> [Han74] </ref> we will call this mode of compilation adaptive compilation. One goal of such a system is to maximize overall system performance, i.e. to minimize the sum of compile time and execution time over the life of the system. <p> In the general case, the lookup uses specific hash tables specific to the message names. The concept of adaptive systems is not new. For example, Hansen describes an adaptive compiler in <ref> [Han74] </ref>. His compiler optimized the inner loops of Fortran programs at run-time. The main goal of his work was to minimize the total cost of running a program which presumably was executed only once.
Reference: [Hei90] <author> Richard L. Heintz, Jr., </author> <title> Low Level Optimizations for an Object-Oriented Programming Language. </title> <type> Masters Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference: [Ing86] <author> Daniel H. Ingalls, </author> <title> A Simple Technique for Handling Multiple Polymorphism. </title> <booktitle> In OOPSLA 86 Conference Proceedings, </booktitle> <address> Portland, OR, </address> <year> 1986. </year> <note> Published as SIGPLAN Notices 21(11), </note> <month> November, </month> <year> 1986. </year>
Reference: [JGZ88] <author> Ralph E. Johnson, Justin O. Graver, and Lawrence W. Zurawski, </author> <title> TS: An Optimizing Compiler for Smalltalk. </title> <booktitle> In OOPSLA 88 Conference Proceedings, </booktitle> <pages> pp. 18-26, </pages> <address> San Diego, CA, </address> <month> October, </month> <year> 1988. </year> <note> Published as SIGPLAN Notices 23(11), </note> <month> November, </month> <year> 1988. </year>
Reference-contexts: One approach to solving this problem is to insert type tests before the message send and create a separate branch for each of the possible types. This technique, type casing, is similar to type prediction and to the case analysis technique implemented as part of the Typed Smalltalk system <ref> [JGZ88] </ref>. Splitting is another way to turn a polymorphic message into several separate monomorphic messages. It avoids type tests by copying parts of the control ow graph [CUL89, CU90, CU91]. <p> In this scenario, type information would flow from the system to the user, in contrast to other approaches where type information flows from the user to the compiler <ref> [Suz81, BI82, JGZ88] </ref>. In our system, the programmer benefits from type information even for programs which do not contain any type declarations, and the declarations are not needed to obtain good performance.
Reference: [Joh87] <author> Ralph Johnson, ed., </author> <title> Workshop on Compiling and Optimizing Object-Oriented Programming Languages. </title> <booktitle> In Addendum to the OOPSLA 87 Conference Proceedings, </booktitle> <pages> pp. 59-65, </pages> <address> Orlando, FL, </address> <month> October, </month> <year> 1987. </year> <note> Published as SIGPLAN Notices 23(5), </note> <month> May, </month> <year> 1988. </year>
Reference-contexts: However, inlining requires that the type of the target of a message send be known at compile time so that its definition can be looked up and inlined. Hence, many optimization techniques have focused on ways to obtain and exploit type information <ref> [Joh87] </ref>. The remainder of this section describes existing techniques to extract, preserve, and exploit this precious type information. 4.1. Type Prediction Certain messages are almost exclusively sent to particular receiver types.
Reference: [KiRo89] <author> Gregor Kiczales and Luis Rodriguez, </author> <title> Efficient Method Dispatch in PCL. </title> <type> Technical Report SSL-89-95, </type> <note> Xerox PARC, </note> <year> 1989. </year>
Reference-contexts: If the number of types is large, a shared PIC implemented with a hash table should be faster than the global lookup cache because the message name need not be verified and because the hit ratio will approach 100%. The CLOS implementation described in <ref> [KiRo89] </ref> uses a similar technique to speed up access methods (called reader methods in CLOS). <p> Inline caching techniques are less attractive in this context because a direct call plus a 14 type test take about the same time (6 cycles) as a full lookup. However, statically-typed object-oriented languages could benefit from customization, type casing, and inlining [Lea90]. Kiczales and Rodriguez <ref> [KiRo89] </ref> describe a mechanism similar to PICs for a CLOS implementation. Their implementation of message dispatch does not use inline caching per se but it does use special dispatch handlers for some cases, e.g. when a call site uses only one or two distinct classes.
Reference: [Kra83] <author> Glenn Krasner, ed., </author> <title> Smalltalk-80: Bits of History and Words of Advice. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference: [Lea90] <editor> Douglas Lea, Customization in C++. </editor> <booktitle> In Proceedings of the 1990 Usenix C++ Conference, </booktitle> <pages> pp. 301-314, </pages> <address> San Francisco, CA, </address> <month> April, </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Inline caching techniques are less attractive in this context because a direct call plus a 14 type test take about the same time (6 cycles) as a full lookup. However, statically-typed object-oriented languages could benefit from customization, type casing, and inlining <ref> [Lea90] </ref>. Kiczales and Rodriguez [KiRo89] describe a mechanism similar to PICs for a CLOS implementation. Their implementation of message dispatch does not use inline caching per se but it does use special dispatch handlers for some cases, e.g. when a call site uses only one or two distinct classes.
Reference: [MIPS86] <institution> MIPS Computer Systems, MIPS Language Programmers Guide. MIPS Computer Systems, </institution> <address> Sunny-vale, CA, </address> <year> 1986. </year>
Reference-contexts: All optimizations could be applied statically, but Hansens system tried to allocate compile time wisely in order to minimize total execution time, i.e. the sum of compile and run-time. Some modern compilers for conventional languages use profiling information to perform branch scheduling and to reduce cache conicts <ref> [MIPS86] </ref>. The optimizations enabled by this form of feedback are typically very low-level and machine-dependent. Mitchell [Mit70] converted parts of dynamically-typed interpreted programs into compiled form, assuming that the types of variables remained constant.
Reference: [Mit70] <author> J. G. Mitchell, </author> <title> Design and Construction of Flexible and Efficient Interactive Programming Systems. </title> <type> Ph.D. Thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1970. </year>
Reference-contexts: Some modern compilers for conventional languages use profiling information to perform branch scheduling and to reduce cache conicts [MIPS86]. The optimizations enabled by this form of feedback are typically very low-level and machine-dependent. Mitchell <ref> [Mit70] </ref> converted parts of dynamically-typed interpreted programs into compiled form, assuming that the types of variables remained constant. Whenever the type of a variable changed, all compiled code which depended on its type was discarded.
Reference: [PW90] <author> William Pugh and Grant Weddell, </author> <title> Two-Directional Record Layout for Multiple Inheritance. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 85-91, </pages> <address> White Plains, NY, </address> <month> June, </month> <year> 1990. </year> <note> Published as SIGPLAN Notices 25(6), </note> <month> June, </month> <year> 1990. </year>
Reference-contexts: Related Work Statically-typed languages can handle polymorphic sends in constant time by indexing into a type-specific function table, thus reducing the lookup to an indirect procedure call. In C++, for example, a dynamically-bound call takes between 5 and 9 cycles on a SPARC <ref> [Ros88, DMSV89, ES90, PW90] </ref>. This is possible because static type checking can guarantee the success of the lookup, i.e. the result of the table lookup need not be verified.
Reference: [Ros88] <author> John R. Rose, </author> <title> Fast Dispatch Mechanisms for Stock Hardware. </title> <booktitle> In OOPSLA 88 Conference Proceedings, </booktitle> <pages> pp. 27-35, </pages> <address> San Diego, CA, </address> <month> October, </month> <year> 1988. </year> <note> Published as SIGPLAN Notices 23(11), </note> <month> November, </month> <year> 1988. </year>
Reference-contexts: Related Work Statically-typed languages can handle polymorphic sends in constant time by indexing into a type-specific function table, thus reducing the lookup to an indirect procedure call. In C++, for example, a dynamically-bound call takes between 5 and 9 cycles on a SPARC <ref> [Ros88, DMSV89, ES90, PW90] </ref>. This is possible because static type checking can guarantee the success of the lookup, i.e. the result of the table lookup need not be verified.
Reference: [ST84] <author> Norihisa Suzuki and Minoru Terada, </author> <title> Creating Efficient Systems for Object-Oriented Languages. </title> <booktitle> In Proceedings of the 11th Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, </address> <month> January, </month> <year> 1984. </year>
Reference: [Suz81] <author> Norihisa Suzuki, </author> <title> Inferring Types in Smalltalk. </title> <booktitle> In Proceedings of the 8th Symposium on the Principles of Programming Languages, </booktitle> <year> 1981. </year>
Reference-contexts: In this scenario, type information would flow from the system to the user, in contrast to other approaches where type information flows from the user to the compiler <ref> [Suz81, BI82, JGZ88] </ref>. In our system, the programmer benefits from type information even for programs which do not contain any type declarations, and the declarations are not needed to obtain good performance. <p> Since the language did not support polymor-phism and was not object-oriented, the main motivation for this scheme was to reduce interpretation overhead and to replace generic built-in operators by simpler, specialized code sequences (e.g. to replace generic addition by integer addition). Suzuki <ref> [Suz81] </ref> reports that a type accumulation phase for Smalltalk-80 was suggested to him by Alan Perlis as an alternative to type analysis. In this approach, a program would be run in interpreted form against some examples and then compiled into more efficient code.
Reference: [UBF+84] <author> D. Ungar, R. Blau, P. Foley, D. Samples, and D. Patterson, </author> <title> Architecture of SOAR: Smalltalk on a RISC. </title> <booktitle> In Eleventh Annual International Symposium on Computer Architecture, </booktitle> <address> Ann Arbor, MI, </address> <month> June, </month> <year> 1984. </year>
Reference: [Ung86] <author> David Ungar, </author> <title> The Design and Evaluation of a High Performance Smalltalk System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This form of caching is called inline caching since the target address is stored at the send point, i.e. in the callers code [DS84]. Inline caching is surprisingly effective, with a hit ratio of 95% for Smalltalk code <ref> [DS84, Ung86, UP87] </ref>. SOAR (a Smalltalk implementation for a RISC processor) would be 33% slower without inline caching [Ung86]. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does our SELF system [CUL89]. 3. <p> Inline caching is surprisingly effective, with a hit ratio of 95% for Smalltalk code [DS84, Ung86, UP87]. SOAR (a Smalltalk implementation for a RISC processor) would be 33% slower without inline caching <ref> [Ung86] </ref>. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does our SELF system [CUL89]. 3. Handling Polymorphic Sends Inline caches are effective only if the receiver type (and thus the call target) remains relatively constant at a call site.
Reference: [UP83] <author> David Ungar and David Patterson, </author> <title> Berkeley Smalltalk: Who Knows Where the Time Goes? In [Kra83]. </title>
Reference-contexts: Lookup caches are very effective in reducing the lookup overhead. Berkeley Smalltalk, for example, would have been 37% slower without a cache <ref> [UP83] </ref>. In this paper, we will consistently use speedups when comparing performance; for instance, X is 52% faster than Y means that Ys execution time is 1.52 times Xs execution time. Smalltalk-80 is a trademark of ParcPlace Systems, Inc. 3 2.3.
Reference: [UP87] <author> David Ungar and David Patterson, </author> <title> What Price Smalltalk? In IEEE Computer 20(1), </title> <month> January, </month> <year> 1987. </year>
Reference-contexts: This form of caching is called inline caching since the target address is stored at the send point, i.e. in the callers code [DS84]. Inline caching is surprisingly effective, with a hit ratio of 95% for Smalltalk code <ref> [DS84, Ung86, UP87] </ref>. SOAR (a Smalltalk implementation for a RISC processor) would be 33% slower without inline caching [Ung86]. All compiled implementations of Smalltalk that we know of incorporate inline caches, as does our SELF system [CUL89]. 3. <p> For example, existing SELF and Smalltalk systems predict that + will be sent to an integer [UP82, GR83, DS84], since measurements indicate that this occurs 90% of the time <ref> [UP87] </ref>. Type prediction improves performance if the cost of the test is low and the likelihood of a successful outcome is high. 4.2. Customization Customization is another technique for determining the types of many message receivers in a method [CUL89].
Reference: [WM89] <author> Paul R. Wilson and Thomas G. Mohler, </author> <title> Design of the Opportunistic Garbage Collector. </title> <booktitle> In OOPSLA 89 Conference Proceedings, </booktitle> <pages> pp. 23-35, </pages> <address> New Orleans, LA, </address> <month> October, </month> <year> 1989. </year> <note> Published as SIGPLAN Notices 24(10), </note> <month> October, </month> <year> 1989. </year> <month> 17 </month>
Reference-contexts: Also, recompilation could be performed in the background or during the users think pauses, similar to Wilsons opportunistic garbage collection <ref> [WM89] </ref>. Of course, recompilation has costs. Some time will be wasted by executing unoptimized code, and some time will be wasted because some work is repeated with every recompilation (e.g. code generation).
References-found: 32

