URL: http://www.cs.cmu.edu/~lafferty/LS/tex/dirichlet.ps
Refering-URL: 
Root-URL: 
Email: mackay@mrao.cam.ac.uk  peto@cs.toronto.edu  
Title: A Hierarchical Dirichlet Language Model new algorithm is compared with smoothing on a two million
Author: David J.C. MacKay Linda C. Bauman Petoy 
Address: Cambridge CB3 0HE United Kingdom  Canada  
Affiliation: Cavendish Laboratory  Department of Computer Science University of Toronto  
Note: Natural Language Engineering 1 (1): 000-000 c 1994 Cambridge University Press 1  The  computational resources. Contents  
Abstract: We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as `smoothing'. A number of interesting differences from smoothing emerge. The insights gained from a probabilistic view of this problem point towards new directions for language modelling. The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech, and DNA and protein sequences in molecular biology. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Antoniak, C. E. </author> <title> (1974) Mixtures of Dirichlet processes with applications to non parametric problems. </title> <journal> Annals of Statistics 2: </journal> <pages> 1152-1174. </pages>
Reference-contexts: question that remains before the two inferences above are fully defined is `what is the prior over Q?' In particular, this paper examines the question, what prior P (QjH) would give us predictive distributions of the `smoothed' form (1)? 2.4 A convenient family of priors: Dirichlet distributions The Dirichlet distribution <ref> (Antoniak 1974) </ref> for a probability vector p with I components is parameterized by a measure u (a vector with all coefficients u i &gt; 0) which we will write here as u = ffm, where m is a normalized measure over the I components ( P m i = 1), and
Reference: <author> Bahl, L. R., Brown, P., de Souza, P., Mercer, R. L., and Nahamoo, D. </author> <title> (1991) A fast algorithm for deleted interpolation. </title> <booktitle> In Proc. Eurospeech '91 Genoa, </booktitle> <pages> pp. 1209 1212. </pages>
Reference: <author> Bahl, L. R., Jelinek, F., and Mercer, R. L. </author> <title> (1983) A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Trans PAMI-5 (2): </journal> <pages> 179-190. </pages>
Reference: <author> Bell, T. C., Cleary, J. G., and Witten, I. H. </author> <title> (1990) Text compression. </title> <address> Englewood Cliffs: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: Text compression is a similar prediction task in which character sequences are to be predicted (adaptively, or otherwise). In text compression, the smoothing technique is known as `blending' and is used to combine the predictions obtained using contexts of different orders <ref> (Bell et al. 1990) </ref>. This paper's aim is to reverse-engineer the underlying model which gives a probabilistic meaning to smoothing, allowing it to be better understood, objectively tuned and sensibly modified. <p> i and of the conditional probability of word i following word j are f i = (F i + ff=W )=(T + ff) and f ijj = (F ijj + fi=W )=(F j + fi), where the `initial counts' ff=W and fi=W are commonly set to 0, 1=2 or 1 <ref> (Bell et al. 1990) </ref>. The subscripts i and j run from 1 to W , the total number of distinct words in the language. <p> The appropriate variation of with F j is automatically present in (24). [Not that this is a new idea: the `blending' method in text compression <ref> (Bell et al. 1990) </ref> uses the same variation with F j .] 2.6.2 Level 2 inference At the second level of inference, we infer the hyperparameters given the data. <p> It should be emphasized that this failure of the smoothing formula is not because of any inadequacy of the bigram model; a Markov process can easily capture the couplet in the data set above. [In text compression, the method known as `update exclusion' <ref> (Bell et al. 1990) </ref> avoids the problem described above.] 3 Inferring Dirichlet hyperparameters 3.1 The dice factory An analogy may be useful to describe the inferences we will now make. Imagine that a factory produces biased I-sided dice.
Reference: <author> Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Lai, J. C., and Mercer, R. L. </author> <title> (1992) An estimate of an upper bound for the entropy of English. </title> <booktitle> Computational Linguistics 18 (1): </booktitle> <pages> 31-40. </pages>
Reference: <author> Brown, P. F., DellaPietra, S. A., DellaPietra, V. J., and Mercer, R. L. </author> <title> (1993) The mathematics of statistical machine translation: Parameter estimation. </title> <booktitle> Compu tational Linguistics 19 (2): </booktitle> <pages> 263-311. </pages>
Reference-contexts: 1 Introduction Speech recognition and automatic translation both depend on a language model that assigns probabilities to word sequences. The automatic translation system implemented at IBM used a crude `trigram' model of language with impressive results <ref> (Brown et al. 1993) </ref>. Similar language models are also used in speech recognition systems (Bahl et al. 1983; Jelinek and Mercer 1980).
Reference: <author> Buntine, W. </author> <title> (1992) Learning classification trees. </title> <journal> Statistics and Computing 2: </journal> <volume> 63 73. </volume>
Reference-contexts: The technique of smoothing is also used in modelling with classification trees, and this literature contains a similar paper in which an `empirical Bayes' approach is used <ref> (Buntine 1992) </ref>. As above, this approach is compromised by the invocation of ad hoc estimators, instead of the derivation of inferences. An estimator for m is given that is not, in fact, the most probable m. No objective procedure for setting ff is given.
Reference: <author> Cox, R. </author> <title> (1946) Probability, frequency, and reasonable expectation. </title> <journal> Am. J. Physics 14: </journal> <pages> 1-13. </pages>
Reference-contexts: Since rational inference can always be mapped onto probabilities <ref> (Cox 1946) </ref>, the aim of this paper is to discover what implicit probabilistic model the above procedure can be related to. The smoothing formula and deleted interpolation were originally conceived as a way of combining together the predictions of different models.
Reference: <author> Gale, W., and Church, K. </author> <title> (1991) A program for aligning sentences in bilingual corpora. </title> <booktitle> In Proceedings of 29th Annual Meeting of the ACL, </booktitle> <pages> pp. 177-184. </pages>
Reference: <author> Gull, S. F. </author> <title> (1989) Developments in maximum entropy data analysis. In Maximum Entropy and Bayesian Methods, Cambridge 1988 , ed. by J. </title> <booktitle> Skilling, </booktitle> <pages> pp. </pages> <month> 53-71, </month> <title> Dordrecht. Kluwer. Hierarchical Dirichlet Language Model 19 Hanson, </title> <editor> R., Stutz, J., and Cheeseman, P. </editor> <title> (1991) Bayesian classification with cor relation and inheritance. </title> <booktitle> In Proceedings of the 12th International Joint Confer ence on Artificial Intelligence, Sydney, Australia, </booktitle> <volume> volume 2, </volume> <pages> pp. 692-698. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jelinek, F., and Mercer, R. L. </author> <title> (1980) Interpolated estimation of Markov source parameters from sparse data. In Pattern recognition in practice, </title> <editor> ed. by E. </editor> <publisher> S. </publisher>
Reference-contexts: For the Dirichlet model, this meant solving the simultaneous equations given by equation (35) to obtain u MP . For the smoothing method, separate frequencies were first calculated for each block, and then the 's were obtained using deleted interpolation <ref> (Jelinek and Mercer 1980) </ref>. The optimization was halted when on average each parameter of the model had converged to eight decimal places. The optimized parameter values for each model were then used to compute predictive probabilities ^ P (ijj) for each bigram in the test data.
Reference: <author> Gelsema and L. N. </author> <booktitle> Kanal, </booktitle> <pages> pp. 381-402. </pages> <publisher> North-Holland publishing company. </publisher>
Reference: <author> Katz, S. M. </author> <title> (1987) Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <booktitle> IEEE Transactions on Acoustics, Speech and Signal Processing 35 (3): </booktitle> <pages> 400-401. </pages>
Reference-contexts: Another generally used language model employs a similar procedure known as `backing off' <ref> (Katz 1987) </ref>. Text compression is a similar prediction task in which character sequences are to be predicted (adaptively, or otherwise). In text compression, the smoothing technique is known as `blending' and is used to combine the predictions obtained using contexts of different orders (Bell et al. 1990).
Reference: <author> MacKay, D. J. C. </author> <title> (1995a) Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research, </title> <booktitle> Section A 354 (1): </booktitle> <pages> 73-80. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1995b) Density networks and protein modelling. In Maximum Entropy and Bayesian Methods, Cambridge 1994 , ed. </title> <editor> by J. Skilling and S. Sibisi, </editor> <publisher> Dordrecht. Kluwer. </publisher>
Reference: <author> MacKay, D. J. C. </author> <year> (1995c) </year> <month> Hyperparameters: </month> <title> Optimize, or integrate out? In Maxi mum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. Heidbreder, </editor> <publisher> Dordrecht. Kluwer. </publisher>
Reference: <author> MacKay, D. J. C., </author> <title> (1995d) Models for dice factories and amino acid probability vectors. </title> <note> In preparation. </note>
Reference-contexts: Various algorithms can be used to implement mixture models: both Monte Carlo methods (Neal 1992) and Gaussian approximations (Hanson et al. 1991). Mixture models are applied to the modelling of amino acid probabilities in <ref> (MacKay 1995d) </ref>. Alternatively a model might define the context to be the last two words, with the type of the context being defined by the most recent word. With a coupled prior for the context hyperparameters, this model would give predictions similar to those of the smoothed trigram language model. <p> This motivates the development of componential models (a type of latent variable model), in which the type of a context is represented with several continuous or discrete dimensions. A componential model is described and applied to the modelling of amino acid Hierarchical Dirichlet Language Model 17 probabilities in <ref> (MacKay 1995d) </ref>. It has been generalized to the modelling of joint distributions of multiple amino acids in (MacKay 1995a; MacKay 1995b). 5.2 Relationship to previous `empirical Bayes' approaches An approach similar in spirit to the one advocated in this paper has been described by Nadas (1984). <p> part of a series expansion, gives an approximation to the difference (F +u)(u) that is accurate to within 2% for all u and all positive integers F : (F + u) (u) ' u F + u 1=2 (43) This approximation is useful for gradient-based optimization of Dirichlet distribu tions <ref> (MacKay 1995d) </ref>.
Reference: <author> Nadas, A. </author> <title> (1984) Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Trans ASSP-32 (4): </journal> <pages> 859-861. </pages>
Reference: <author> Neal, R. M. </author> <title> (1992) Bayesian mixture modelling. In Maximum Entropy and Bayesian Methods, </title> <note> Seattle 1991 , ed. by C. </note> <author> Smith, G. Erickson, and P. </author> <month> Neudorfer, </month> <pages> pp. 197 211, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: Various algorithms can be used to implement mixture models: both Monte Carlo methods <ref> (Neal 1992) </ref> and Gaussian approximations (Hanson et al. 1991). Mixture models are applied to the modelling of amino acid probabilities in (MacKay 1995d). Alternatively a model might define the context to be the last two words, with the type of the context being defined by the most recent word.
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic inference using Markov chain Monte Carlo meth ods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Peto, L. B. </author> <title> (1994) A comparison of two smoothing methods for word bigram models. </title> <type> Technical Report CSRI-304, </type> <institution> Computer Systems Research Institute, University of Toronto. </institution>
Reference-contexts: For example, as discussed later in section 5, we might believe that contexts come in equivalence classes or types|this would motivate a mixture model for the vectors q j . 4 Application to a small corpus We conducted an experiment to compare deleted interpolation with the new method empirically <ref> (Peto 1994) </ref>. We used each algorithm to construct an alternative model from the training corpus. We then compared the predictive accuracy of the algorithms by evaluating the perplexity of the test data under each of the competing models: the better the model, the smaller the perplexity.
Reference: <author> Press, W., Flannery, B., Teukolsky, S. A., and Vetterling, W. T. </author> <title> (1988) Numerical Recipes in C . Cambridge. </title>
Reference-contexts: A conjugate gradients algorithm <ref> (Press et al. 1988) </ref>, for example, easily finds the optimum.
Reference: <author> Skilling, J. </author> <title> (1989) Classic maximum entropy. In Maximum Entropy and Bayesian Methods, Cambridge 1988 , ed. by J. Skilling, </title> <publisher> Dordrecht. Kluwer. </publisher>
Reference: <author> West, M. </author> <title> (1992) Hyperparameter estimation in Dirichlet process mixture models. </title> <note> Working paper 92-A03, </note> <institution> Duke Inst. of Stats. and Decision Sciences. </institution>
Reference: <author> Williams, C. K. I., and Hinton, G. E. </author> <title> (1991) Mean field networks that learn to discriminate temporally distorted strings. In Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School , ed. </booktitle> <editor> by D. S. Touretzky, J. L. Elman, and T. J. Sejnowski. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
References-found: 25

