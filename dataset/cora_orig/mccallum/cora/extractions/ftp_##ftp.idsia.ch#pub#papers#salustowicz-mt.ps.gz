URL: ftp://ftp.idsia.ch/pub/papers/salustowicz-mt.ps.gz
Refering-URL: http://www.idsia.ch/reports.html
Root-URL: 
Title: Diplomarbeit A Genetic Algorithm for the Topological Optimization of Neural Networks  
Author: Rafa l Sa lustowicz Matr. Nr.: Aufgabensteller: Prof. Dr. E. Konrad Betreuer: Prof. Dr. D.E. Rumelhart 
Date: Abgabe: 18. April 1995  
Affiliation: Technische Universitat Berlin Fachbereich 13 Informatik Institut fur Wissensbasierte Systeme (WBS)  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Boers, E.J.W. and Kuiper, H. </author> <title> Biological metaphors and the design of modular artificial neural networks Master's Thesis, </title> <institution> Leiden University, The Netherlands. </institution> <note> [In ftp.cs.tu-berlin.de: pub/doc/neuroprose/boers.biological-metaphors.ps.gz] </note>
Reference-contexts: NEURAL NETWORKS 12 independent sub-structures (A and B), which are not connected to each other. Modularity is therefore used here in the context of dividing networks into sub-networks that are expected to solve parts of the problem by itself. Modular feedforward networks have some advantages over their non-modular counterparts <ref> [1] </ref> and a few of those advantages will be discussed here. One major advantage is the lower complexity 6 of modular networks. <p> An advantage of modular neural networks is also the limitation of equal solutions that can be found by such a network <ref> [1] </ref>. Modular neural networks are able to find less equally good solutions than their non-modular counterparts. This can be demonstrated on the example depicted in figure 2.5. <p> An example can be found in the thesis of Boers and Kuiper <ref> [1] </ref>. It is based on the the "what" and "where" categorization of Rueckl et al. [27]. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. <p> This was not crucial for this study as only problems that need hidden units could exhibit the modularity features of the restricted unit-cluster model, but it should be changed as sometimes good solutions without a hidden layer exist and a hidden layer can make the problem artificially harder <ref> [1] </ref>. * genetic adjustment of learn parameters A learn algorithm like backpropagation usually contains different crucial parameters that have to be defined by the user. It is however not always trivial to find good values for those parameters as can be seen 3 see suggestions in section 4.1.6 CHAPTER 5.
Reference: [2] <editor> Brause, R. </editor> <publisher> Neuronale Netze Teubner, </publisher> <address> Stuttgart 1991. </address>
Reference-contexts: NEURAL NETWORKS 10 networks that are needed to solve complex problems which cannot be solved by a single neuron. A detailed description of different models of neurons and a wide variety of neural networks can be found in <ref> [2, 7, 8, 26] </ref>. 2.2 Feedforward Neural Networks Feedforward neural networks are a special class of artificial neural networks that can only propagate an input stimulus in one direction through the network. An example of a multilayered feedforward network is shown in figure 2.4.
Reference: [3] <author> De Jong, K.A. </author> <title> An analysis of the behavior of a class of genetic adaptive systems Doctoral dissertation, </title> <institution> University of Michigan, </institution> <note> Dissertation Abstracts International 36(10), 5140B. (University Microfilms No. 76-9381) </note>
Reference: [4] <author> Harp, S.A., Samad T. and Guha A. </author> <title> Towards the genetic synthesis of neural networks. </title> <editor> In J.D. Schaffer, Editor, </editor> <booktitle> Proc. of the Third Int'l Conf. on Genetic Algorithms and Their Applications, </booktitle> <pages> pp. 360-369. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989 </year>
Reference-contexts: The idea is to encode just the most important parts of an artificial neural network in the genotype and then use predefined rules to extract the phenotype. The IES was split by Yao into three different classes: 1. Blueprints as used by Harp et al. <ref> [4, 5, 6] </ref> that encode parameters for constructing the connectivity and important parameters for the evaluation algorithm. The connectivity parameters guide predefined developmental rules. Furthermore Harp et al. use backpropagation as the evaluation algorithm and encode the learn rate and the momentum on the genotype. 2.
Reference: [5] <author> Harp, S.A., Samad T. and Guha A. </author> <title> Designing application-specific neural networks using the genetic algorithm. </title> <editor> In D.S. Touretzky, Editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pp. 447-454, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990 </year>
Reference-contexts: The idea is to encode just the most important parts of an artificial neural network in the genotype and then use predefined rules to extract the phenotype. The IES was split by Yao into three different classes: 1. Blueprints as used by Harp et al. <ref> [4, 5, 6] </ref> that encode parameters for constructing the connectivity and important parameters for the evaluation algorithm. The connectivity parameters guide predefined developmental rules. Furthermore Harp et al. use backpropagation as the evaluation algorithm and encode the learn rate and the momentum on the genotype. 2.
Reference: [6] <author> Harp, S.A. and Samad T. </author> <title> Genetic synthesis of neural network architecture. </title> <editor> In L. Davis, Editor, </editor> <booktitle> Handbook of Genetic Algorithms, </booktitle> <pages> pp. 203-221. </pages> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY 10003, </address> <year> 1991. </year>
Reference-contexts: The idea is to encode just the most important parts of an artificial neural network in the genotype and then use predefined rules to extract the phenotype. The IES was split by Yao into three different classes: 1. Blueprints as used by Harp et al. <ref> [4, 5, 6] </ref> that encode parameters for constructing the connectivity and important parameters for the evaluation algorithm. The connectivity parameters guide predefined developmental rules. Furthermore Harp et al. use backpropagation as the evaluation algorithm and encode the learn rate and the momentum on the genotype. 2.
Reference: [7] <editor> Hecht-Nielsen, R. </editor> <publisher> Neurocomputing Addison-Wesley 1990. </publisher>
Reference-contexts: NEURAL NETWORKS 10 networks that are needed to solve complex problems which cannot be solved by a single neuron. A detailed description of different models of neurons and a wide variety of neural networks can be found in <ref> [2, 7, 8, 26] </ref>. 2.2 Feedforward Neural Networks Feedforward neural networks are a special class of artificial neural networks that can only propagate an input stimulus in one direction through the network. An example of a multilayered feedforward network is shown in figure 2.4.
Reference: [8] <author> Hertz, J., Krogh, A. and Palmer, R.G. </author> <title> Introduction to the theory of neural computation Addison-Wesley Publishing Company, 1991. </title> <type> 123 BIBLIOGRAPHY 124 </type>
Reference-contexts: NEURAL NETWORKS 10 networks that are needed to solve complex problems which cannot be solved by a single neuron. A detailed description of different models of neurons and a wide variety of neural networks can be found in <ref> [2, 7, 8, 26] </ref>. 2.2 Feedforward Neural Networks Feedforward neural networks are a special class of artificial neural networks that can only propagate an input stimulus in one direction through the network. An example of a multilayered feedforward network is shown in figure 2.4. <p> CHAPTER 2. NEURAL NETWORKS 13 2.4 The Backpropagation Algorithm There are several learn algorithms with which feedforward networks can be trained (their weights can be adjusted) to solve a problem. Some of them can e.g. be found in Hertz/Krough/Palmer <ref> [8] </ref>. This section gives a short overview over the backpropagation algorithm because it will be used in the evolution of feedforward networks presented in this study to train the networks and contribute to the calculation of their quality measure. <p> It is based on the the "what" and "where" categorization of Rueckl et al. [27]. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation [28, 29], conjugate gradient [13, 24], simulated annealing <ref> [8] </ref>, genetic and evolutionary approaches [21, 33]. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process.
Reference: [9] <author> Holland, J.H. </author> <title> Adaptation in natural and artificial systems University of Michigan Press, </title> <journal> Ann Harbor, </journal> <year> 1975 </year>
Reference-contexts: Then the importance of the genetic coding is emphasized and criteria are established that can be used to measure the quality of a genetic coding. Furthermore an overview is given over the simple genetic algorithm as it was introduced by Holland <ref> [9] </ref>. Then the three different variations of the simple genetic algorithm used in this research are presented. The variations include the extended simple genetic algorithm (ESGA), the semantic changing genetic algorithm (SCGA) and the extended semantic changing genetic algorithm (ESCGA). First, the ESGA is described. <p> This criterion cannot always be assured, as it sometimes requires a deep insight into the problem. Nevertheless it is of great advantage to put those building blocks close together on the genotype. A detailed description of the schemata theory 3 can be found in <ref> [9] </ref>. * compactness A genetic coding is compact, if the length of the genotype is minimal 2 Marti encoded three different kinds of connections binary inhibitory, non-existent and excitatory in two different ways. <p> This list does not claim to be complete as more criteria can probably be found. Nevertheless the criteria presented here can be viewed upon as a minimum requirement for a genetic coding to be efficient and promising. 3.3 Simple Genetic Algorithm The original genetic algorithm introduced by Holland <ref> [9] </ref> can be written down in the following way: 4 Let's say we have a coding that allows one to define a feedforward network that consists of two serially chained networks aXOR-net and an AND-net in two different ways: genotype decodes to phenotype XOR-net,AND-net ! XOR-net - AND-net Combining those two <p> Firstly there are two genetic operators that are used implicitly and have not been mentioned yet. The operators are duplication and deletion <ref> [9, 10] </ref>. Duplication and deletion are used implicitly as they are not directly applied to the genetic code. Yet, the process of automatically defining modules makes use of them. <p> GENETIC ALGORITHMS 55 * completeness * closure * proximity * short schemata * compactness * non-isomorphism * modularity Then an outline of the simple genetic algorithm (SGA) as introduced by Holland <ref> [9, 10] </ref> was presented, the function of the genetic operators selection, crossover, mutation and reduction that are used in this research was described and the extended simple genetic algorithm (ESGA) was derived that could operate on genotypes of variable length. <p> The importance of the genetic coding is discussed first and the following criteria are established to measure the quality of a genetic coding: * completeness * closure * proximity * short schemata * compactness * non-isomorphism * modularity An outline of the simple genetic algorithm (SGA) as introduced by Hol-land <ref> [9, 10] </ref> is presented, the function of the genetic operators selection, crossover, mutation and reduction that are used in this research is described and the extended simple genetic algorithm (ESGA) is derived from the simple genetic algorithm. The extended simple genetic algorithm can operate on genotypes of variable length.
Reference: [10] <author> Goldberg, D.E. </author> <title> Genetic Algorithms in search, optimization and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1989. </year>
Reference-contexts: Chapter 3 Genetic Algorithms This chapter first gives a short introduction to genetic algorithms (for a detailed description see Goldberg <ref> [10] </ref>). Then the importance of the genetic coding is emphasized and criteria are established that can be used to measure the quality of a genetic coding. Furthermore an overview is given over the simple genetic algorithm as it was introduced by Holland [9]. <p> The power of a genetic algorithm to solve a specific problem very much depends on how and in which order the parameters of the phenotype are coded on the genotype <ref> [10, 17] </ref>. Several criteria can be found to decide, whether a genetic coding is efficient and promising: * completeness A genetic coding is considered complete if every point in the search space can be encoded [11]. This ensures that all solutions are accessible by the genetic algorithm. <p> For further and more detailed information on genetic operators and different reduction strategies see <ref> [10] </ref>. In order to show the properties of the genetic operators used in this research the genotype is chosen to be a bit string. 3.4.1 Selection The process of selection ensures that individuals of higher quality are more likely to be chosen for reproduction than those of lower quality. <p> CHAPTER 3. GENETIC ALGORITHMS 25 has been selected, as crossovers with multiple points do not seem to improve the optimization process <ref> [10] </ref>. During the crossover operation two points in the symbol strings (genotypes) are arbitrarily chosen and the part, which is enclosed by those two points is swapped. <p> Firstly there are two genetic operators that are used implicitly and have not been mentioned yet. The operators are duplication and deletion <ref> [9, 10] </ref>. Duplication and deletion are used implicitly as they are not directly applied to the genetic code. Yet, the process of automatically defining modules makes use of them. <p> GENETIC ALGORITHMS 55 * completeness * closure * proximity * short schemata * compactness * non-isomorphism * modularity Then an outline of the simple genetic algorithm (SGA) as introduced by Holland <ref> [9, 10] </ref> was presented, the function of the genetic operators selection, crossover, mutation and reduction that are used in this research was described and the extended simple genetic algorithm (ESGA) was derived that could operate on genotypes of variable length. <p> The importance of the genetic coding is discussed first and the following criteria are established to measure the quality of a genetic coding: * completeness * closure * proximity * short schemata * compactness * non-isomorphism * modularity An outline of the simple genetic algorithm (SGA) as introduced by Hol-land <ref> [9, 10] </ref> is presented, the function of the genetic operators selection, crossover, mutation and reduction that are used in this research is described and the extended simple genetic algorithm (ESGA) is derived from the simple genetic algorithm. The extended simple genetic algorithm can operate on genotypes of variable length.
Reference: [11] <author> Gruau, F. </author> <title> Neural Networks Synthesis using Cellular Encoding and the Genetic Algorithm [PhD. </title> <booktitle> thesis in 140.77.1.11: </booktitle> <address> pub/Rapports/PhD/PhD94-01-E.ps.Z], </address> <year> 1994 </year>
Reference-contexts: Several criteria can be found to decide, whether a genetic coding is efficient and promising: * completeness A genetic coding is considered complete if every point in the search space can be encoded <ref> [11] </ref>. This ensures that all solutions are accessible by the genetic algorithm. If e.g. the task is to find the topology of a feedforward neural network, it should be possible to encode all possible feedforward neural networks. <p> Ignoring this criterion could result in loss of possibly valuable solutions. * closure A genetic coding is closed, if all individuals produced by applying genetic operators can be decoded to valid phenotypes <ref> [11] </ref> (provided that their parents have been valid individuals). If this criterion is met, the search space can be kept minimal. No points, which do not represent a solution, can be accessed by the genetic algorithm. <p> Thus it is reasonable to encode parameters that constitute a unity (work on the same part of the problem) close to each other. CHAPTER 3. GENETIC ALGORITHMS 22 <ref> [11] </ref>. Compactness can also be used as a measure. <p> coding may easily lead to the loss of valuable information in both of them 4 . * modularity A genetic coding is modular, when partial solutions (modules/building blocks) are written out just once and a mechanism is provided so that they can be referenced at different points in the coding <ref> [11] </ref>. It is much like in a program where a subroutine is defined just once and then referenced many times throughout the program. Modularity is necessary to ensure compactness. This list does not claim to be complete as more criteria can probably be found. <p> The ESGA-crossover may distribute a substring to many individuals and various locations, but a copy of the substring (redefinition) will then be present at all the locations and not just a reference. That references can be used successfully has been shown in the works of Koza [14] and Gruau <ref> [11] </ref>. * extended module distribution Once a valuable module is found it should be possible to distribute it quickly to individuals of lower fitness. The idea is very simple and introduces a technique, which is found in society and in natural evolution. <p> In the context of neural networks this seems to be the case <ref> [11] </ref>, but, of course, it depends very much on the genetic coding how well the special features of the SCGA are exploited. Chapter 4 Evolving Topologies In this chapter the evolution of topologies of feedforward neural networks is discussed and a promising approach to this undertaking is presented. <p> They all have fan-ins and fan-outs connecting them to exactly the same units and that makes them functionally equal 5 . The 3 Another approach that is based on the same idea can be found in <ref> [11] </ref>. 4 More parts can be found such as e.g. the activation function (s) of the units. 5 All units in such a cluster can implement equally well all parts of the task that is being solved by that cluster. CHAPTER 4.
Reference: [12] <author> Kitano, H. </author> <title> Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476, </pages> <year> 1990. </year>
Reference-contexts: The connectivity parameters guide predefined developmental rules. Furthermore Harp et al. use backpropagation as the evaluation algorithm and encode the learn rate and the momentum on the genotype. 2. Coding of developmental rules as done by Mjolsness et al. [19] and Kitano <ref> [12] </ref>. The developmental rules themselves are encoded in the genotype. This is done by encoding recursive equations (Mjolsness et al.) or generation rules (Kitano). 1 Mutation e.g. becomes quickly meaningless, unless it is permanently adjusted.
Reference: [13] <author> Kramer, A.H. and Sangiovanni-Vincentelli, A. </author> <title> Efficient Parallel Learning Algorithms for Neural Networks. </title> <booktitle> In Advances in Neural Information Processing Systems I (Denver 1988), </booktitle> <editor> ed. D.S. </editor> <booktitle> Touretzky, </booktitle> <pages> pp. 40-48. </pages> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher> <year> 1989. </year>
Reference-contexts: It is based on the the "what" and "where" categorization of Rueckl et al. [27]. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation [28, 29], conjugate gradient <ref> [13, 24] </ref>, simulated annealing [8], genetic and evolutionary approaches [21, 33]. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process.
Reference: [14] <author> Koza, J.R. </author> <title> Genetic Programming: A Paradigm for Genetically Breeding Populations of Computer Programs to Solve Problems Stanford University Computer Science Department Technical Report STAN-CS-90-1314. </title> <month> June </month> <year> 1990. </year>
Reference-contexts: In order to solve a problem using genetic algorithms the parameters of the problem (phenotype) must be coded onto a genotype. A genotype is often represented by a bit string, an integer- or float number string or any other symbol string (trees are also possible <ref> [14] </ref>). The genetic algorithm works on a population of genotypes searching globally for the optimum by applying 19 CHAPTER 3. GENETIC ALGORITHMS 20 genetic operators (see section 3.4) to the genotypes with respect to their quality. <p> The ESGA-crossover may distribute a substring to many individuals and various locations, but a copy of the substring (redefinition) will then be present at all the locations and not just a reference. That references can be used successfully has been shown in the works of Koza <ref> [14] </ref> and Gruau [11]. * extended module distribution Once a valuable module is found it should be possible to distribute it quickly to individuals of lower fitness. The idea is very simple and introduces a technique, which is found in society and in natural evolution. <p> It is possible to create a genetic coding so that this criterion is met 10 and thus an extension to the ESGA (or SGA) is not need. Another option is to limit the crossover points to be of the same kind <ref> [14, 15] </ref>. This means that once a crossover point is selected the matching crossover point of the mating partner must be found in such a way that the offspring will be a valid individual.
Reference: [15] <author> Koza, J.R. and Rice, J.P. </author> <title> Genetic generation of both the weights and architecture for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks 1991 (IJCNN 1991), </booktitle> <pages> pp. </pages> <address> II-397 II-404 </address>
Reference-contexts: (train error) and generalize (test error), its complexity (amount of units and links) and various other criteria. 3.2 Genetic Coding The choice of the genetic representation is crucial to the performance of the genetic algorithm, as the genetic coding defines the window through which the genetic algorithm sees the world <ref> [15] </ref>. The power of a genetic algorithm to solve a specific problem very much depends on how and in which order the parameters of the phenotype are coded on the genotype [10, 17]. <p> It is possible to create a genetic coding so that this criterion is met 10 and thus an extension to the ESGA (or SGA) is not need. Another option is to limit the crossover points to be of the same kind <ref> [14, 15] </ref>. This means that once a crossover point is selected the matching crossover point of the mating partner must be found in such a way that the offspring will be a valid individual. <p> One can make sure that once a crossover point is selected a point of the same type is selected in the other individual <ref> [15] </ref>.
Reference: [16] <author> Lin, E.C.C. Bacteria, plasmids and phages: </author> <title> An introduction to molecular biology Cambridge, </title> <address> Mass.: </address> <publisher> Harvard University Press, </publisher> <year> 1984. </year>
Reference-contexts: It is much like ideas in society spread and like plasmids exchange genetic material 8 <ref> [16] </ref>. This feature should be tested, whether it really has a positive influence as it might force the population to converge to a local minimum.
Reference: [17] <author> Marti, L. </author> <title> Genetically Generated Neural Networks I: Representational Effects CAS/CNS-TR-92-014, </title> <institution> 1992; Boston University Center for Adaptive Systems; 111 Cummington Street Boston, </institution> <address> Ma 02215; [In ftp.cs.tu-berlin.de: pub/doc/neuroprose/marti.ga1.ps.gz] </address>
Reference-contexts: The power of a genetic algorithm to solve a specific problem very much depends on how and in which order the parameters of the phenotype are coded on the genotype <ref> [10, 17] </ref>. Several criteria can be found to decide, whether a genetic coding is efficient and promising: * completeness A genetic coding is considered complete if every point in the search space can be encoded [11]. This ensures that all solutions are accessible by the genetic algorithm. <p> A counterexample is the plain binary coding where two adjacent numbers may differ in almost all their bits (e.g. 100000 and 011111). A work displaying the influence of proximity on genetic coding of neural networks has been done by Marti <ref> [17] </ref>. <p> CHAPTER 3. GENETIC ALGORITHMS 28 proximity criterion is met. Thus it would be advantageous to have a feature that would enable the genetic algorithm to find the right coding by itself (given just an alphabet of all parameters that should be included). Marti <ref> [17] </ref> showed that this is a very important issue. He used a very simple example where it was easy to find the correct coding manually, but this is often not the case 9 . * supporting closure There are several ways of ensuring the closure criterion.
Reference: [18] <author> Miller, G.F., Todd, P.M. and Hegde, S.U. </author> <title> Designing neural networks using genetic algorithms. BIBLIOGRAPHY 125 In J.D. </title> <editor> Schaffer, Editor, </editor> <booktitle> Proc. of the Third Int'l Conf. on Genetic Algorithms and Their Applications, </booktitle> <pages> pp. 379-384. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989 </year>
Reference: [19] <author> Mjolsness, E., Sharp, D.H. and Alpert, B.K. </author> <title> Scaling, </title> <booktitle> machine learning and genetic neural nets. Advances in Applied Mathematics, </booktitle> <volume> 10 </volume> <pages> 137-163, </pages> <year> 1989. </year>
Reference-contexts: The connectivity parameters guide predefined developmental rules. Furthermore Harp et al. use backpropagation as the evaluation algorithm and encode the learn rate and the momentum on the genotype. 2. Coding of developmental rules as done by Mjolsness et al. <ref> [19] </ref> and Kitano [12]. The developmental rules themselves are encoded in the genotype. This is done by encoding recursive equations (Mjolsness et al.) or generation rules (Kitano). 1 Mutation e.g. becomes quickly meaningless, unless it is permanently adjusted.
Reference: [20] <author> Merrill, J.W.L. and Port, </author> <title> R.F. Fractally configured neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 53-60, </pages> <year> 1991. </year>
Reference-contexts: The offspring networks could still have the same amount of units in the layer and the same connectivity, but the mix of biases and weights would not implement the same functionality. CHAPTER 4. EVOLVING TOPOLOGIES 59 3. Fractal coding of the connectivity as done by Merrill and Port <ref> [20] </ref>. The IES is the more advanced encoding strategy. It has the advantages of a relatively small FBP and an easy to control size of genotypes.
Reference: [21] <author> Montana, D. and Davis, L. </author> <title> Training feedforward neural networks using genetic algorithms. </title> <booktitle> In Proc. of Eleventh Int'l Joint Conf. on Artificial Intelligence, </booktitle> <pages> pp. 762-767. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation [28, 29], conjugate gradient [13, 24], simulated annealing [8], genetic and evolutionary approaches <ref> [21, 33] </ref>. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process.
Reference: [22] <author> Ossen, A. </author> <title> Zur Modularisierung und Interpretierbarkeit Neuronaler Netze Dissertation, </title> <address> Berlin 1990. </address>
Reference-contexts: This section gives a short overview over the backpropagation algorithm because it will be used in the evolution of feedforward networks presented in this study to train the networks and contribute to the calculation of their quality measure. A detailed description of the backpropagation algorithm can be found in <ref> [22, 28] </ref>. The backpropagation algorithms is a training algorithm for supervised learning of feedforward networks. A network is presented input and output patterns from an example training data set. Each input pattern in the set has its own desired output pattern. <p> Gradient descent techniques belong therefore to so called local search strategies. The backpropagation algorithm used in this study is extended by a momentum term. The momentum term rates the gradient of the previous cycle. The backpropagation algorithm takes this gradient into consideration when calculating the weight change (see <ref> [22] </ref> for a detailed explanation).
Reference: [23] <author> Prechelt, L. </author> <title> PROBEN1 | A Set of Benchmarks and Benchmarking Rules for Neural Network Training Algorithms. </title> <booktitle> [In ftp.ira.uka.de: </booktitle> <address> /pub/papers/techreports/1994/1994-21.ps.Z] </address>
Reference-contexts: The output is therefore binary, but it has been coded with two output units, where each output unit represents one state. The real world data has been taken from a standard database called "Proben 1" <ref> [23] </ref>. This database contains several data sets for evaluating performances of neural networks. Two separate tests were conducted. The first test was supposed to evolve networks that would be able to learn a larger data set of 699 data samples (fitness defined by the train error).
Reference: [24] <author> Press, W.H., Flannery, B.P., Teukolsky, S.A. and Vetterling, W.T. </author> <title> Numerical Recipes. </title> <publisher> Cambridge: Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: It is based on the the "what" and "where" categorization of Rueckl et al. [27]. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation [28, 29], conjugate gradient <ref> [13, 24] </ref>, simulated annealing [8], genetic and evolutionary approaches [21, 33]. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process.
Reference: [25] <editor> Rechenberg, I. Evolutionstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution Frommberg-Holzboog, </editor> <address> Stuttgart, </address> <year> 1973. </year>
Reference-contexts: Individuals selected for reproduction undergo crossover with a certain probability or advance unchanged to the offspring population. 3.4.3 Mutation Mutation, which is the main engine of the traditional evolution strategy <ref> [25] </ref>, is used with genetic algorithms to introduce new solutions and prevent the population from unrecoverable loss of important information. Mutation is accomplished by modifying single symbols of the symbol string with a certain probability.
Reference: [26] <editor> Rojas, R. Theorie der neuronalen Netze - Eine systematische Einfuhrung Springer-Verlag, </editor> <address> Berlin Heidelberg 1993. </address>
Reference-contexts: NEURAL NETWORKS 10 networks that are needed to solve complex problems which cannot be solved by a single neuron. A detailed description of different models of neurons and a wide variety of neural networks can be found in <ref> [2, 7, 8, 26] </ref>. 2.2 Feedforward Neural Networks Feedforward neural networks are a special class of artificial neural networks that can only propagate an input stimulus in one direction through the network. An example of a multilayered feedforward network is shown in figure 2.4.
Reference: [27] <author> Rueckl, J.G., Cave, K.R. and Kosslyn, </author> <title> S.M. Why are 'what' and 'where' processed by separate cortical visual systems ? A computational investigation. </title> <journal> In Journal of cognitive neuroscience,1 ,pp. </journal> <volume> 171 - 186, </volume> <year> 1989. </year>
Reference-contexts: Furthermore, networks of smaller complexity are less likely to be affected by the overlearning effect 9 . Another advantage is the capability of modular networks to cope with interference problems. Interference problems can e.g. occur when a non-modular network is trained to learn two independent parts of a problem <ref> [27] </ref>. A network, which is originally large enough to capture the problem cannot be trained as it "forgets" one part of the problem while learning the other one. The network shows an oscillating behavior, but is not able to capture both problems at once. <p> An example can be found in the thesis of Boers and Kuiper [1]. It is based on the the "what" and "where" categorization of Rueckl et al. <ref> [27] </ref>. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation [28, 29], conjugate gradient [13, 24], simulated annealing [8], genetic and evolutionary approaches [21, 33].
Reference: [28] <author> Rumelhart D.E., Hinton G.E., Williams R.J. </author> <title> Learning internal representations by error propagation In: </title> <editor> D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol. 1: </volume> <booktitle> Foundations, </booktitle> <publisher> MIT Press 1986. BIBLIOGRAPHY 126 </publisher>
Reference-contexts: This section gives a short overview over the backpropagation algorithm because it will be used in the evolution of feedforward networks presented in this study to train the networks and contribute to the calculation of their quality measure. A detailed description of the backpropagation algorithm can be found in <ref> [22, 28] </ref>. The backpropagation algorithms is a training algorithm for supervised learning of feedforward networks. A network is presented input and output patterns from an example training data set. Each input pattern in the set has its own desired output pattern. <p> It is based on the the "what" and "where" categorization of Rueckl et al. [27]. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation <ref> [28, 29] </ref>, conjugate gradient [13, 24], simulated annealing [8], genetic and evolutionary approaches [21, 33]. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process.
Reference: [29] <author> McClelland, J.L. and Rumelhart D.E. </author> <title> Explorations in parallel distributed processing MIT Press, </title> <year> 1988. </year>
Reference-contexts: The network design is problem specific, meaning that for each problem an individual network must be evolved. The solutions should be at least sub-optimal topologies that can be trained with a learning algorithm the backpropagation algorithm <ref> [29] </ref> has been used here to solve the problem. A special emphasis is put on modular feedforward networks with multiple layers. In order to find modular neural networks two innovative approaches are employed the semantic changing genetic algorithm and the restricted unit-cluster model. <p> It is based on the the "what" and "where" categorization of Rueckl et al. [27]. 29 probably even all CHAPTER 4. EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation <ref> [28, 29] </ref>, conjugate gradient [13, 24], simulated annealing [8], genetic and evolutionary approaches [21, 33]. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process. <p> It lets the genetic algorithm search for solutions locally 52 growing and shrinking the individuals in a moderate way. Parameters for the backpropagation algorithm (for an exact description of their influence on the training of neural networks see <ref> [29] </ref>): * Amount of Epochs The amount of epochs sets how often the training examples are presented to the neural networks before their training and test error are reported. This parameter has to be set individually for each test and will therefore be mentioned later. <p> Weight Range = 0.1 The weight range sets the interval for the random weight initialization 51 Provided that the Gaussian maximum is not set to force the individuals to grow or shrink. 52 on individuals of similar sizes 53 Learn rate and momentum are parameters that are highly problem dependent <ref> [29] </ref>. CHAPTER 4. EVOLVING TOPOLOGIES 84 of the networks. In this case here the weights of the networks are randomly initialized with values from the interval [-0.1, 0.1] before the networks are being trained. It is common to initialize the weights with values from the interval [-0.5, 0.5].
Reference: [30] <author> Salomon, R. Verbesserung konnektionistischer Lernverfahren, </author> <title> die nach der Gradi-entenmethode arbeiten Dissertation, </title> <publisher> TU Berlin 1991. </publisher>
Reference-contexts: The parameters include the topology of the network, the random initial weight initialization, the learn parameters of the backpropagation algorithm and the train and test pattern data sets. 11 There are extensions to the backpropagation algorithm that adapt this parameter automatically <ref> [30] </ref>. CHAPTER 2. NEURAL NETWORKS 15 2.5.1 Topology A well suited topology is highly important for the network to find a solution. Networks which are too small are not able to learn the trainings data set at all or they will just learn it partly. <p> Setting the learn rate to a larger value may speed up the convergence to a certain point, which is problem dependent, but might also keep the network from learning at all. Salomon <ref> [30] </ref> e.g. presents a technique how the learn rate can be adapted automatically. The idea is to adapt the learn rate in relation to the roughness of the surface of the error function. <p> It can be seen that there are several possibilities that can lead to a failure in finding a solution with the SCGA. Most of the possible reasons can be eliminated by adjusting parameters. This however is not always a trivial task. Adaptive learn algorithms 66 <ref> [30] </ref> and longer test runs 67 can help, but do not ensure that a solution will be found. 4.3.5 Breast Cancer Prediction Test The breast cancer prediction is a real world problem. The idea is to predict from 9 input attributes, whether the patient has breast cancer or not.
Reference: [31] <author> Sa lustowicz, </author> <title> R.P. Genetische Algorithmen zur Konstruktion Neuronaler Netze - eine vergleichende Studie 1994, </title> <type> unpublished pre-diploma thesis, </type> <institution> Technical University of Berlin. </institution>
Reference-contexts: Only a very abstract classification will be presented here in order to show the two major different approaches to network coding. This classification has been suggested by Yao <ref> [34, 31] </ref> and classifies genetic codings of neural networks into two categories the direct and the indirect encoding scheme. 57 CHAPTER 4. EVOLVING TOPOLOGIES 58 Direct Encoding Scheme The main idea of the direct encoding scheme (DES) is that the phenotype (neural network) is completely defined by the genotype.
Reference: [32] <author> Schwartz, J. T. </author> <title> The new connectionism Developing relationships between neuro-science and artificial intelligence In: S.R. </title> <editor> Graubard (ed.), </editor> <booktitle> The Artificial Intelligence Debate, </booktitle> <publisher> MIT Press 1988, </publisher> <pages> pp. 123-141. </pages>
Reference-contexts: It is suspected that more than 100 billions biological neurons are connected to each other in a highly complex way so that a large part of the brain can contribute to the processing of a signal <ref> [32] </ref>. Although the functionality of the brain is only known in its rudiments a simple model could be developed. The idea is to try to simulate the learning and the generalization abilities of the brain by developing artificial neural networks.
Reference: [33] <author> Whitley, D., Starkweather, T. </author> <title> und Bogart, C. Genetic algorithms and neural networks: optimizing connections and connectivity. </title> <booktitle> Parallel Computing. </booktitle> <month> 14 (August </month> <year> 1990) </year> <month> 3. </month> <pages> pp. 347-361. </pages>
Reference-contexts: EVOLVING TOPOLOGIES 73 has to be applied. There exist several learning algorithms for feedforward networks - e.g. backpropagation [28, 29], conjugate gradient [13, 24], simulated annealing [8], genetic and evolutionary approaches <ref> [21, 33] </ref>. The choice of the learning algorithm with which the network is then trained has a major impact on the topologies being evolved during the evolutionary process.
Reference: [34] <author> Yao, </author> <note> X. </note>
Reference-contexts: Genetic algorithms are a global optimization technique for high dimensional problems and can be used to solve this task. 1.1 Why Genetic Algorithms ? It is assumed that there is no systematic way 3 to construct problem specific feedforward networks <ref> [34] </ref>. This means that there is no recipe for how to define the optimal topology of a neural network from looking at the trainings and test data sets 4 . Furthermore it is assumed that genetic algorithms are a more powerful approach to topology construction than constructive or destructive algorithms. <p> Only a very abstract classification will be presented here in order to show the two major different approaches to network coding. This classification has been suggested by Yao <ref> [34, 31] </ref> and classifies genetic codings of neural networks into two categories the direct and the indirect encoding scheme. 57 CHAPTER 4. EVOLVING TOPOLOGIES 58 Direct Encoding Scheme The main idea of the direct encoding scheme (DES) is that the phenotype (neural network) is completely defined by the genotype.
References-found: 34

