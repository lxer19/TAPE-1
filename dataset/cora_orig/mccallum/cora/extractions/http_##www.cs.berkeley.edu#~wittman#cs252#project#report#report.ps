URL: http://www.cs.berkeley.edu/~wittman/cs252/project/report/report.ps
Refering-URL: http://www.cs.berkeley.edu/~wittman/cs252/project/report/
Root-URL: 
Email: frichie,wittmang@cs.berkeley.edu  
Title: Optimizing Multigrid for Uniprocessors and SMPs  
Author: Richard Vuduc Michael Wittman 
Date: 252  
Affiliation: University of California, Berkeley  
Pubnum: CS  
Abstract: We discuss a practical implementation of a multigrid solver for sequential machines and SMPs. Our methods are particularly relevant to the class of linear systems arising from finite element problems. We focus our attention on sparse matrix vector multiply (SMVM), the kernel of our solver, and perform detailed analyses of its architectural requirements. In particular, we examine four previously proposed uniprocessor optimization techniques|register blocking, cache blocking, reordering, and symmetry|and quantify their effectiveness in terms of how each affects the overall cache behavior and instruction mix balance. We show that register blocking and symmetry are particularly effective uniprocessor optimizations. We show the overall improvement in the full multigrid solver based on this highly tuned SMVM. We then discuss our current work on a multithreaded version of multigrid for SMPs. Again, we focus on SMVM and show how it is possible to obtain superlinear speedups by using some of the same uniprocessor optimization techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Demmel. </author> <title> Applied Numerical Linear Algebra. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1997. </year>
Reference: [2] <author> S. Toledo. </author> <title> "Improving memory-system performance of sparse matrix-vector multiplication." </title> <booktitle> Proceedings of the 8th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: In our work, we build a solver for the more general case of an arbitrary grid in which a user might be willing to form the matrix explicitly. There have been many attempts at producing optimized, sparse matrix vector multiplication code. Toledo <ref> [2] </ref> implemented some of the techniques described in this paper. He was able to generate a very fast routine, and also investigated the use of special prefetch instructions, which required hand-coding in assembly language; we did not explore this optimization. <p> We believe that this heuristic did not help us because of the already fairly banded structure of the original matrix (fig. 4). Although reordering did not alter the performance, it is possible that in practice, a user would like to reorder the matrix for other numerical reasons <ref> [2] </ref>. Therefore, we wished to see if it was possible to recover the best register blocking performance by applying register blocking to the reordered matrix.
Reference: [3] <author> E. </author> <title> Im. "SPARSity: Building a toolbox for optimizing sparse matrix vector multiplications." </title> <type> Ph.D. Qualifying Exam Report, </type> <month> April </month> <year> 1998. </year>
Reference-contexts: Toledo [2] implemented some of the techniques described in this paper. He was able to generate a very fast routine, and also investigated the use of special prefetch instructions, which required hand-coding in assembly language; we did not explore this optimization. Im <ref> [3] </ref> is currently developing a sparse matrix toolkit, and we used her preliminary research code in creating our solver. <p> The drop-off in performance for larger blocks is due to both the finite number of registers and an increase in fill. For example, we can define the fill ratio as done <ref> [3] </ref>: fill ratio = 1 + no: of filled in zeros original number of nonzeros (1) and compute this for each block size. <p> We use the cache-blocking implementation in the SPARSity toolkit 4 . We tried a variety of cache-blocking sizes but observed only modest (&lt; 5%) improvement for some matrix sizes. These 4 Specifically, we use the variant of cache-blocking referred to as "static" cache-blocking <ref> [3] </ref>. 13 register blocking size of 3fi3 reduces this by about 35% to .35 misses per flop. (Bottom) Similarly, L2 cache misses per useful flop were reduced by about 30% at the optimal register blocking size. 14 For reference, this shows the performance in Mflop/s for various square blocking sizes. (Bottom) <p> At the optimal blocking size for this matrix, 3 fi 3, the mix is approximately "balanced." 15 observations|modest to no improvement|are consistent with the findings of Im in <ref> [3] </ref> for similar matrices that arise in finite element problems. Our performance and cache miss rate results are shown in fig. 10.
Reference: [4] <author> O. Temam and W. Jalby. </author> <title> "Characterizing the behavior of sparse algorithms on caches." </title> <booktitle> Proceedings of Supercomputing, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Im [3] is currently developing a sparse matrix toolkit, and we used her preliminary research code in creating our solver. We also note that some analytical work has been done in <ref> [4] </ref> to suggest how different cache configurations (size, cache line size, etc.) can improve the execution of SMVM code. 2 Uniprocessor optimizations We began by tuning the multigrid kernel operation: sparse matrix vector multiply (SMVM). <p> This suggests that tweaking of the code by combining cache-blocking with register blocking could conceivably have resulted in better performance. 2.3 Reordering Reordering is an optimization which had been previously suggested in <ref> [4] </ref> as a heuristic for improving locality. The idea is to reduce the bandwidth 5 of the matrix, i.e., to rearrange the entries of the matrix so that they lie as close to the diagonal as possible. This would improve the spatial and temporal locality of x.
Reference: [5] <author> D. Bailey, T. Harris, W. Saphir, R. van der Wijngaart, A. Woo, and M. Yarrow. </author> <title> "The NAS Parallel Benchmarks 2.0." </title> <type> Technical Report, </type> <institution> NAS-95-020, NASA Ames Research Center, </institution> <month> December, </month> <year> 1995. </year>
Reference-contexts: However, for this case, we do not need to explicitly represent the matrices, and the memory access patterns are very regular. Another well-known, tuned code is the multigrid solver that is a part of the NAS Parallel Benchmarks <ref> [5] </ref>. Although this is a sample implementation of a solver for the solution of 3-D Poisson's equation, again the problem is defined for regular grids.
Reference: [6] <author> C. Douglas. </author> <title> "Minimizing memory cache usage for multigrid algorithms in two dimensions." </title> <note> http://yellow.ccs.uky.edu/~douglas/Preprints/cache4.ps.gz June 1997. </note>
Reference-contexts: There have been a few previous attempts as well, which we briefly mention. Douglas <ref> [6] </ref> describes how to improve cache behavior of multigrid for two-dimensional problems on a regular, rectangular grid. However, for this case, we do not need to explicitly represent the matrices, and the memory access patterns are very regular.
Reference: [7] <author> F. Wong, R. Martin, R. Arpaci-Dusseau, D. Culler. </author> <title> "Architectural requirements and scalability of the NAS Parallel Benchmarks." </title> <type> Technical Report, </type> <institution> U.C. Berkeley, </institution> <month> January </month> <year> 1998. </year> <month> 29 </month>
References-found: 7

