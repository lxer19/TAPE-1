URL: http://www.cs.colorado.edu/homes/brianb/public_html/mypapers/brianthesis.ps.Z
Refering-URL: http://www.cs.colorado.edu/~brianb/Home.html
Root-URL: http://www.cs.colorado.edu
Title: NONPARAMETRIC SELECTION OF INPUT VARIABLES FOR CONNECTIONIST LEARNING  
Author: by BRIAN BONNLANDER 
Degree: B.S., Stanford University, 1990 M.S., University of Colorado, 1992 A thesis submitted to the Faculty of the Graduate School of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: 1996  
Affiliation: University of Colorado  Department of Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aarts, E. H. L., Korst, J. H. M., and Zweitering, P. J. </author> <year> (1996). </year> <title> Deterministic and randomized local search. </title> <editor> In Smolensky, P., Mozer, M. C., and Rumelhart, D. E., editors, </editor> <booktitle> Mathematical Perspectives on Neural Networks. </booktitle> <publisher> Erlbaum Associates. </publisher>
Reference-contexts: The algorithm terminates when the qualities of all subsets in a neighborhood are no higher than the current subset. 9 The framework is adapted from <ref> (Aarts et al., 1996) </ref>. 21 Neighborhood Sizes Algorithm Largest Smallest Average Forward Selection k 1 k/2 Backward Elimination k 1 k/2 Stepwise Selection k k k Sequential Selection k 2 /4 k k &lt; size &lt; k 2 =4 Table 3.1. Neighborhood sizes for different local search algorithms.
Reference: <author> Abu-Mostafa, Y. </author> <year> (1995). </year> <title> Hints. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 639-671. </pages>
Reference: <author> Allison, J. J. </author> <year> (1994). </year> <title> Explorations of Bayesian input relevance determination in neural networks. </title> <type> Master's thesis, </type> <institution> University of Colorado. </institution>
Reference: <author> Battiti, R. </author> <year> (1994). </year> <title> Using mutual information for selecting features in supervised neural net learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(4) </volume> <pages> 537-550. </pages>
Reference-contexts: Algorithm and Battiti's Approach Fraser (1988) has proposed a method for estimating the joint density p (X 1 ; X 2 ) for two potential input variables in continuous time series analysis, in order to find the first two time lags that are independent; the algorithm is also applied in <ref> (Battiti, 1994) </ref> for performing input variable selection for connectionist networks. Figure 3.1 provides high level pseu-docode for the algorithm. It is a recursive algorithm that creates more histogram splits in areas where the points have finer structure, and it creates fewer splits where there is no evidence for structure.
Reference: <author> Beale, E. M. L. </author> <year> (1970). </year> <title> Note on procedures for variable selection in multiple regression. </title> <journal> Technometrics, </journal> <volume> 12(4) </volume> <pages> 909-914. </pages>
Reference-contexts: The ARD method (Section 3.1.1) can be seen as a generalization of ridge regression to statistical models with intermediate, nonlinear processing components. The SBP algorithm (Section 3.1.3) can be viewed as a modified version of backward elimination <ref> (Beale, 1970) </ref>. The NS p model is an extension of linear regression using a weight regularizer such as Thompson's S p statistic (Thompson, 1978). 3.1.1 Automatic Relevance Determination Derivation of Automatic Relevance Determination (ARD) lies squarely within the realm of Bayesian statistics. <p> This process of training, computing sensitivities, and deleting is repeated; cross validation based techniques are used to decide when this cycle should be stopped. 5 SBP can be viewed as an extension of the backward elimination algorithm from linear regression <ref> (Beale, 1970) </ref>. The step of computing sensitivities in SBP provides a shortcut: whereas, in backward elimination, a separate network would be trained for each input variable's removal, SBP trains only one network and uses the measure to select an input for deletion.
Reference: <author> Bishop, C. M. </author> <year> (1994). </year> <title> Mixture density networks. </title> <type> Unpublished report, </type> <institution> Aston University. </institution>
Reference: <author> Cover, T. M. and Thomas, J. A. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference-contexts: However, the foundation of the theory upon probability distributions still remains. The definitions provided here assume discrete random variables, but there are well defined analogues for continuous random variables. Notation for the definitions follows <ref> (Cover and Thomas, 1991) </ref>. 2.1.1 Entropy The entropy H (p) can be viewed simply as a function of some probability distribution p (X), where X is the set of values with nonzero probability according to p: H (p) = x2X Within information theory, X might describe the possible values produced by <p> In other words, adding input variables cannot decrease mutual information. A proof is given in <ref> (Cover and Thomas, 1991) </ref>, Chapter 2. It hinges on the following facts. <p> The slightly larger error bars for less than five are simply due to the plot involving a percentage error. Exact mutual information for less than five input variables is smaller than for ten, which makes the percent error slightly larger in these cases. 2 See, for example, <ref> (Cover and Thomas, 1991) </ref>, page 237. 29 estimates across twenty different samples. The top plot indicates that the estimate is unbiased and converges to the true, underlying value. The bottom plot shows that accuracy of estimates does not depend strongly on the number of input variables.
Reference: <author> Efroymson, M. A. </author> <year> (1960). </year> <title> Multiple regression analysis. </title> <editor> In Ralston, A. and Wilf, H. S., editors, </editor> <title> Mathematical Methods for Digital Computers. </title> <publisher> Wiley. </publisher>
Reference-contexts: Either algorithm may stop because the addition of any subset in the current neighborhood does not improve overall quality. 3.3.3 Stepwise Selection This algorithm, in its original form, was presented in <ref> (Efroymson, 1960) </ref> for linear networks. The search optimized a particular objective function based on a statistical test of significance for linear network input variables. A more general form of the algorithm can be stated, however, for any objective function.
Reference: <author> Epanechnikov, V. A. </author> <year> (1969). </year> <title> Nonparametric estimation of a multivariate probability density. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 14 </volume> <pages> 153-158. </pages>
Reference-contexts: A kernel based method works naturally with data that are continuous valued, although it also works well when the data consist of values from both discrete and continuous random variables. The method explored in this thesis uses a kernel function known as a multi-dimensional Epanechnikov product kernel <ref> (Epanechnikov, 1969) </ref>.
Reference: <author> Finnoff, W., Hergert, F., and Zimmermann, H. G. </author> <year> (1993). </year> <title> Improving generalization performance by nonconver-gent model selection methods. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 771-783. </pages>
Reference-contexts: The other two methods work best when the number of irrelevant input variables is small. For a certain class of problems, all methods perform well. The chapter goes on to show that problems suggested in <ref> (Finnoff et al., 1993) </ref> are not appropriate for comparing input variable selection algorithms. In general, the number of irrelevant input variables is too small to make a significant difference in performance when they are removed. In addition, two of the problems are too complex for small networks to learn adequately. <p> To avoid confounding the study with the choice of an input variable search algorithm, relevance estimates are performed for all distinct subsets of relevant input variables. Further tests involving forward selection appear in Section 5.4. 37 Table 5.1 summarizes the problems explored. The first five are introduced in <ref> (Finnoff et al., 1993) </ref>. For each problem, the table lists the name used to denote the problem in subsequent analysis, the general form of the output variable as a function of input variables, and the variance of additive, uniform noise added to the output variable to create target values. <p> The input variables selected by adjusted estimation are fX 1 ; X 4 g, again with only the first variable being noise free. 7.3 Artificial Problems of Finnoff et al. The prediction problems presented in <ref> (Finnoff et al., 1993) </ref> are also an interesting subject of study.
Reference: <author> Fraser, A. M. </author> <year> (1988). </year> <title> Information and Entropy in Strange Attractors. </title> <type> PhD thesis, </type> <institution> University of Texas, Austin. </institution>
Reference: <author> Furnival, G. M. and Wilson, R. W. </author> <year> (1974). </year> <title> Regressions by leaps and bounds. </title> <journal> Technometrics, </journal> <volume> 16(4) </volume> <pages> 499-511. </pages>
Reference-contexts: In these cases, it is sometimes possible to take knowledge of the search problem into account to prune the search space. The most commonly assumed knowledge relates to sum-squared error objective functions <ref> (Furnival and Wilson, 1974) </ref>. For sum-squared error, one can show that eliminating input variables can only cause the error to worsen; hence, it is possible to determine a bound on maximum possible improvement in the objective function when input variables are eliminated.
Reference: <author> Hardle, W. </author> <year> (1989). </year> <title> Applied Nonparametric Regression. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Hartigan, J. A. and Hartigan, P. M. </author> <year> (1985). </year> <title> The dip test of unimodality. </title> <journal> The Annals of Statistics, </journal> <volume> 13(1) </volume> <pages> 70-84. </pages>
Reference-contexts: The chapter discusses two univariate mode estimation algorithms: Silverman's (1981) kernel-based mode estimate, and the DIP test for unimodality <ref> (Hartigan and Hartigan, 1985) </ref>. It proposes extensions to the algorithms for multivariate data. It goes on to test the extensions on several artificial data sets, and shows that the accuracy of mode estimates depends to some degree on how dense the points in the density estimate are. <p> Up to now, the algorithms explored here were developed only for univariate data. The two algorithms I extend are a kernel based test (Silverman, 1981), and the DIP test for multimodal-ity <ref> (Hartigan and Hartigan, 1985) </ref>. These algorithms and my proposed multivariate extensions are outlined in Sections 6.1 and 6.2. Sections 6.3 and 6.4 present results for the two respective algorithms. <p> The size of 200 points is chosen because it is the maximum number of points for which empirical significance levels are reported in <ref> (Hartigan and Hartigan, 1985) </ref>. From these data, the DIP statistic is computed. <p> When one input variable is involved (the first and fourth plots from the top), the test succeeds in rejecting the unimodality hypothesis for the bimodal and trimodal output variables. For the unimodal, normal output variable, roughly ten percent of the 3 Each significance level value is reported in <ref> (Hartigan and Hartigan, 1985) </ref> and is based on empirical tests run over dip calculations on 9999 uniformly distributed samples. 58 Histograms show how 100 DIP calculations were distributed for each test; the lightest histograms show DIP estimates for unimodal output variables, and the darkest show values for trimodal output variables. 59 <p> This is unexpected, but it may be due to the use of kernel based, smoothed bootstrap calculations to generate samples for the DIP test. It may be necessary to place less weight with the significance levels found in <ref> (Hartigan and Hartigan, 1985) </ref> when smoothed bootstrap samples are employed. Also, the difference in results between the normal and uniform output variable results suggests that unimodality is more easily recognized in normal random variables than in uniform random variables.
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: By following the negative gradient of the above function, it is possible to find the weight values that maximize the likelihood. A good source for more information on connectionist ideas is <ref> (Hertz et al., 1991) </ref>. Another common definition is the Hessian, which is the matrix of second derivatives of the negative log likelihood function. For weight values that minimize the log likelihood function, the Hessian can be used to describe the curvature of the error surface.
Reference: <author> MacKay, D. J. C. </author> <year> (1993). </year> <title> Bayesian non-linear modeling for the energy prediction competition. </title> <type> Unpublished report. </type>
Reference-contexts: and fi control the balance between the two terms in (3.1), and setting them to proper values can improve generalization performance on future examples. 1 Certain additional assumptions about the shape of M (w) allow the derivation of algebraic formulae for ff and fi; this is the approach taken in <ref> (MacKay, 1993) </ref>. However, the additional assumptions necessary for deriving the equations are rather strong for the neural network paradigm; they include assuming that the error surface is quadratic and that estimates of the Hessian during training accurately reflect the Hessian near the set of weights at the global error minimum.
Reference: <author> Mallows, C. L. </author> <year> (1973). </year> <title> Some comments on C p . Technometrics, </title> <booktitle> 15 </booktitle> <pages> 661-675. </pages>
Reference-contexts: Assume a depth-first search procedure. If, for example, the C p statistic for linear networks is being optimized, then one stores the lowest value of C p for each input variable subset size <ref> (Mallows, 1973) </ref>. At any node in the search tree, one can compute C p for this node, plus the bounds on C p for all nodes below the current one.
Reference: <author> Maron, O. and Moore, A. </author> <year> (1994). </year> <title> Hoeffding races: accelerating model selection search for classification and function approximation. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 59-66. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If the currently "winning" input variable is not winning by a high enough significance level, more bootstraps are performed for the winning input variable and all of its close competitors. A winner is not selected until it is deemed significantly better. This approach is termed "racing" <ref> (Maron and Moore, 1994) </ref>. In summary, a comparison of raw versus adjusted estimates based on all the empirical results of this thesis produces no clear winner. However, a final comparison should involve more explorations of the criteria for stopping forward selection and for selecting an input variable. <p> Differences in data point spacings would no longer throw off mutual information estimates, making the selection of irrelevant or redundant input variables before relevant ones much less likely. In addition, a technique known as racing <ref> (Maron and Moore, 1994) </ref> could be used to make sure that one input variable has a significantly better mutual information estimate before it is selected.
Reference: <author> McCabe, T. M. and Fingersh, L. J. </author> <year> (1995). </year> <title> Cluster-based regression using neural networks. </title> <type> Unpublished report, </type> <institution> University of Colorado. </institution>
Reference-contexts: This is confirmed by doing a linear fit on the seven input variables selected by SBP, which results in a normalized, mean squared error of 0:0575. 7.6 Wind Turbine Data This data set consists of measurements taken from a wind turbine during a five minute period <ref> (McCabe and Fingersh, 1995) </ref>. The goal is to predict the coefficient of force perpendicular to the wind turbine blade.
Reference: <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset Selection in Regression. </title> <publisher> Chapman & Hall, London. </publisher>
Reference-contexts: With such neighborhood structures, one can reliably expect longer running times than stepwise selection, but better solutions. Supporting evidence for this belief, where input variable selection is performed for linear networks, is given in <ref> (Miller, 1990) </ref>. 3.3.5 Branch and Bound Search Algorithms For some problems, the number of input variables may be almost small enough to make exhaustive search tractable, or the described local search algorithms may not yield acceptable solutions.
Reference: <author> Moody, J. </author> <year> (1994). </year> <title> Prediction risk and architecture selection for neural networks. </title> <editor> In Cherkassky, V., Fried-man, J. H., and Wechsler, H., editors, </editor> <title> From Statistics to Neural Networks: Theory and Pattern Recognition Applications, </title> <booktitle> NATO ASI Series F. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Moody, J. E. and Utans, J. </author> <year> (1992). </year> <title> Principled architecture selection for neural networks: application to corporate bond rating prediction. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 683-690. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Neal, R. M. </author> <year> (1995). </year> <title> Bayesian learning for neural networks. </title> <type> PhD thesis, </type> <institution> University of Toronto. </institution> <note> BIBLIOGRAPHY 90 Optican, </note> <author> L. M., Gawne, T. J., Richmond, B. J., and Joseph, P. J. </author> <year> (1991). </year> <title> Unbiased measures of transmitted information and channel capacity from multivariate neuronal data. </title> <journal> Biological Cybernetics, </journal> <volume> 65 </volume> <pages> 305-310. </pages>
Reference-contexts: However, there is a more general, Bayesian approach to network training within the ARD framework <ref> (Neal, 1995) </ref>. Neal's version of ARD is the implementation used in my thesis. 3.1.2 Neal's Implementation of ARD Neal (1995) takes a more general approach to network training and setting the ff and fi, which appears more in line with the traditional Bayesian learning paradigm. <p> Empirical results for ARD appear in (MacKay, 1993; Allison, 1994; Thodberg, 1994; Neal, 1995). For several standard machine learning benchmark problems, it is shown that ARD performs better than networks where all potential input variables are included. In <ref> (Neal, 1995) </ref>, it is shown that ARD does as well as an approach by Quinlan (1993) on the standard Boston housing data set. Neal does not compare performance against other input variable selection methods.
Reference: <author> Quinlan, R. </author> <year> (1993). </year> <title> Combining instance-based and model-based learning. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <address> Amherst, Massachusetts, </address> <pages> pages 236-243. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ross, S. </author> <year> (1991). </year> <title> A First Course in Probability. </title> <publisher> Macmillan Publishing Company, </publisher> <address> New York. </address>
Reference-contexts: The theorem does not cover the case of an arbitrarily constructed sequence of input variable subsets. We cannot expect both measures to order such a sequence in the same way. Figure 4.2 illustrates a situation 1 See, for example, <ref> (Ross, 1991) </ref>. 26 generated using the equation Y = X 1 + Sign (X 2 ) + *, where * is a normal random variable with mean zero and variance 0:009. The top and bottom figures plot the two input variables against the output.
Reference: <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation, Theory, Practice and Visualization. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: Therefore, it is necessary to estimate the distributions from available observations. This is the goal of density estimation. There is an extensive literature on density estimation. It includes methods for forming discrete distribution estimates using histograms <ref> (Scott, 1992) </ref>, continuous estimates using continuous kernel functions (Silverman, 1986; Hardle, 1989), and even estimates using connectionist networks (Bishop, 1994; Weigend and Srivastava, 1995).
Reference: <author> Seber, G. A. F. and Wild, C. J. </author> <year> (1989). </year> <title> Nonlinear Regression. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: Discussion of linearized nonlinear networks can be found in <ref> (Seber and Wild, 1989) </ref>; I use ideas from this reference to extend the analysis in (Thompson, 1978) for minimizing an error function known in the linear regression literature as the S p statistic. 7 As always, the notation uses capital letters (Y ) for random variables and lower case letters (y)
Reference: <author> Silverman, B. W. </author> <year> (1981). </year> <title> Using kernel density estimates to investigate multimodality. </title> <journal> J. R. Statist. Soc. B, </journal> <volume> 43(1) </volume> <pages> 97-99. </pages>
Reference-contexts: The main contribution of this work involves extending two mode estimation algorithms to cases where the input space is multidimensional. Up to now, the algorithms explored here were developed only for univariate data. The two algorithms I extend are a kernel based test <ref> (Silverman, 1981) </ref>, and the DIP test for multimodal-ity (Hartigan and Hartigan, 1985). These algorithms and my proposed multivariate extensions are outlined in Sections 6.1 and 6.2. Sections 6.3 and 6.4 present results for the two respective algorithms.
Reference: <author> Silverman, B. W. </author> <year> (1986). </year> <title> Density Estimation for Statistics and Data Analysis. </title> <address> Chapman-Hall, London. </address>
Reference-contexts: The advantage of a nonparametric approach is the ability to let the data "speak for itself"; it has been proven that many nonparametric density estimation methods are capable of approximating large classes of distributions arbitrarily well, given enough data <ref> (Silverman, 1986) </ref>. 2.2.1 Kernel Density Estimation One nonparametric method I have explored is based on continuous kernels. The estimates are formed by placing a continuous "bump", or kernel function, over each data point sampled from the distribution (Figure 2.1). <p> There are heuristic methods for selecting a good width, however. The following description is adapted from <ref> (Silverman, 1986) </ref>. One way to evaluate the goodness of fit of a density estimate is by examining the estimate's likelihood of generating data. <p> Although this method is reportedly sensitive to outliers, it is one of the few principled approaches for automatically selecting a width h from a finite amount of data <ref> (Silverman, 1986) </ref>. One can compensate for outlier sensitivity by removing outliers from the data in advance. Having a principled approach for kernel width selection is important, since the computation of mutual information depends directly on the kernel width. <p> For each input value, the estimate p (Y jX = x i ) is formed. The DIP statistic requires a set of data, so the next step produces a set of 200 points from the conditional output density estimate using smoothed bootstrapping; <ref> (Silverman, 1986) </ref> discusses an efficient way of generating data from Epanechnikov kernel density estimates. The size of 200 points is chosen because it is the maximum number of points for which empirical significance levels are reported in (Hartigan and Hartigan, 1985). From these data, the DIP statistic is computed.
Reference: <author> Thodberg, H. H. </author> <year> (1994). </year> <title> Bayesian backprop in action: Pruning, committees, error bars and an application to spectroscopy. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 208-215. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thompson, M. L. </author> <year> (1978). </year> <title> Selection of variables in multiple regression: Part I. A review and evaluation. Part II. Chosen procedures. </title> <journal> International Statistical Review, </journal> <volume> 46 </volume> <pages> 1-19 and 129-146. </pages>
Reference-contexts: The SBP algorithm (Section 3.1.3) can be viewed as a modified version of backward elimination (Beale, 1970). The NS p model is an extension of linear regression using a weight regularizer such as Thompson's S p statistic <ref> (Thompson, 1978) </ref>. 3.1.1 Automatic Relevance Determination Derivation of Automatic Relevance Determination (ARD) lies squarely within the realm of Bayesian statistics. A mean zero, Gaussian prior is placed over each set of network weights leading from an input variable to intermediate, nonlinear processing components. <p> Discussion of linearized nonlinear networks can be found in (Seber and Wild, 1989); I use ideas from this reference to extend the analysis in <ref> (Thompson, 1978) </ref> for minimizing an error function known in the linear regression literature as the S p statistic. 7 As always, the notation uses capital letters (Y ) for random variables and lower case letters (y) for individual instances drawn from the random variables.
Reference: <author> Utans, J., Moody, J. E., Rehfuss, S., and Siegelmann, H. </author> <year> (1995). </year> <title> Input variable selection for neural networks: Application to predicting the U.S. business cycle. </title> <booktitle> In IEEE/IAFE Conference on Computational Intelligence for Financial Engineering. </booktitle>
Reference-contexts: The results show that pruning two of ten input variables improves network performance on a test set, although there are no controls to check whether the greatest possible improvement in performance is obtained over all possible input variable subsets. There are also no comparisons to other methods. In <ref> (Utans et al., 1995) </ref>, results are presented for predicting the Index of Industrial production. The data consist of 480 monthly observations obtained from ten financial time series. 4 An explicit goal in Neal's implementation is to make learning performance insensitive to choices for the Gamma distribution parameters.
Reference: <author> Weigend, A. S. and Gershenfeld, N. A., </author> <title> editors (1994). Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: for either algorithm, since the small improvement in performance is overwhelmingly offset by the "spreading out" of data points caused by the addition of the input variable to the density estimates. 7.5 Sleep Apnea Data This one of the data sets used in the Santa Fe time series prediction competition <ref> (Weigend and Gershenfeld, 1994) </ref>. A sleeping patient was connected to several sensors that monitored heart rate, chest 80 81 and adjusted methods represent runs over ten networks started with different initial weights.
Reference: <author> Weigend, A. S. and Srivastava, A. N. </author> <year> (1995). </year> <title> Predicting the probability distributions: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6(2) </volume> <pages> 109-118. </pages>
References-found: 34

