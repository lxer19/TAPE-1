URL: http://www.cis.udel.edu/~case/papers/journal-enum.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: (Email: baliga@gboro.rowan.edu)  (Email: case@cis.udel.edu)  (Email: sanjay@iscs.nus.sg)  
Title: Synthesizing Enumeration Techniques For Language Learning  
Author: Ganesh R. Baliga John Case Sanjay Jain 
Address: New Jersey Mullica Hill, NJ 08024, USA  19716, USA  Singapore  
Affiliation: Computer Science Department Rowan College of  Department of CIS University of Delaware Newark, DE  Department of ISCS National University of Singapore  
Abstract: This paper provides positive and negative results on algorithmically synthesizing, from grammars and from decision procedures for classes of languages, learning machines for identifying, from positive data, grammars for the languages in those classes. In the process, the uniformly decidable classes of recursive languages that can be behaviorally correctly identified from positive data are surprisingly characterized by Angluin's 1980 Condition 2 (the subset principle for preventing over generalization).
Abstract-found: 1
Intro-found: 1
Reference: [Ang80a] <author> D. Angluin. </author> <title> Finding patterns common to a set of strings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 21 </volume> <pages> 46-62, </pages> <year> 1980. </year>
Reference-contexts: Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . Some interesting C x 's are learnable <ref> [Ang80a, Ang80b, Ang82] </ref> and some are not [Gol67]. In Section 3.1 we explore the problem of synthesizing learning machines for learnable C x 's from the corresponding x's.
Reference: [Ang80b] <author> D. Angluin. </author> <title> Inductive inference of formal languages from positive data. </title> <journal> Information and Control, </journal> <volume> 45 </volume> <pages> 117-135, </pages> <year> 1980. </year>
Reference-contexts: Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . Some interesting C x 's are learnable <ref> [Ang80a, Ang80b, Ang82] </ref> and some are not [Gol67]. In Section 3.1 we explore the problem of synthesizing learning machines for learnable C x 's from the corresponding x's. <p> There has been considerable interest in the computational learning theory community in the learnability, from positive data, of uniformly decidable classes of recursive languages. This begins with [Gol67], but especially spans <ref> [Ang80b] </ref> through [ZL95]. In the context of the present paper, one might hope to obtain synthesized learning machines with better mind change complexity if one input decision procedures in place of grammars for the languages to be learned from positive data. <p> The Main Corollaries (Corollaries 2 and 3) of the proof of the Main Theorem (Theorem 11) completely characterize the uniformly decidable classes in TxtBc a . In fact, for the a = 0 case, surprisingly, our characterizing condition is Angluin's Condition 2 from <ref> [Ang80b] </ref>! This condition is the subset principle, a necessary condition preventing overgeneralization in learning from positive data [Ang80b, Ber85, Cas92]. 4 Her Condition 1, also from [Ang80b], was an constructive version of her Condition 2, providing her characterization of uniformly decidable classes in TxtEx. 5 Hence, a uniformly decidable class is <p> In fact, for the a = 0 case, surprisingly, our characterizing condition is Angluin's Condition 2 from [Ang80b]! This condition is the subset principle, a necessary condition preventing overgeneralization in learning from positive data <ref> [Ang80b, Ber85, Cas92] </ref>. 4 Her Condition 1, also from [Ang80b], was an constructive version of her Condition 2, providing her characterization of uniformly decidable classes in TxtEx. 5 Hence, a uniformly decidable class is in (TxtBc TxtEx) , it satisfies Angluin's Condition 2 but not her Condition 1. <p> In fact, for the a = 0 case, surprisingly, our characterizing condition is Angluin's Condition 2 from <ref> [Ang80b] </ref>! This condition is the subset principle, a necessary condition preventing overgeneralization in learning from positive data [Ang80b, Ber85, Cas92]. 4 Her Condition 1, also from [Ang80b], was an constructive version of her Condition 2, providing her characterization of uniformly decidable classes in TxtEx. 5 Hence, a uniformly decidable class is in (TxtBc TxtEx) , it satisfies Angluin's Condition 2 but not her Condition 1. <p> Finally, the discussion following the proof of Theorem 12 clarifies the connection between our learning machine synthesizing algorithm from the proof of Theorem 11 and one implicit in Angluin's proof <ref> [Ang80b] </ref> of her characterization theorem. 2 Preliminaries 2.1 Notation N denotes the set of natural numbers, f0; 1; 2; 3; . . .g. <p> As noted in Section 1, there has been considerable interest in the computational learning theory community in learnability, from positive data, of uniformly decidable classes of recursive languages <ref> [Ang80b, ZL95] </ref>. <p> As noted in Section 1, there has been considerable interest in the computational learning theory community in learnability, from positive data, of uniformly decidable classes of recursive languages [Ang80b, ZL95]. As An-gluin <ref> [Ang80b] </ref> essentially points out, all of the formal language style example classes are uniformly decidable. [Ang80b], for example, deals with so-called exact [LZ93] learning in which, for each learnable class, the programs learned derive naturally from the defining uniform decision procedure for that class. 8 Herein, we will synthesize learning machines <p> As noted in Section 1, there has been considerable interest in the computational learning theory community in learnability, from positive data, of uniformly decidable classes of recursive languages [Ang80b, ZL95]. As An-gluin <ref> [Ang80b] </ref> essentially points out, all of the formal language style example classes are uniformly decidable. [Ang80b], for example, deals with so-called exact [LZ93] learning in which, for each learnable class, the programs learned derive naturally from the defining uniform decision procedure for that class. 8 Herein, we will synthesize learning machines whose hypothesis spaces, in some cases of necessity, go beyond ones naturally associated with the <p> As we noted in Section 1 above, Angluin <ref> [Ang80b] </ref> completely characterized the uniformly decidable classes in TxtEx. 10 Essentially she showed that, for any fixed uniform decision procedure x, U x 2 TxtEx , Condi tion 1 holds, where Condition 1 states: 11 There is an r.e. sequence of (r.e. indices of) finite sets S 0 ; S 1 <p> Our characterization of uniformly decidable classes in TxtBc is, surprisingly, just Angluin's Condition 2! As mentioned above, it is referred to as the subset principle, a necessary condition preventing overgeneralization in learning from positive data <ref> [Ang80b, Ber85, Cas92] </ref>. <p> Suppose x is a uniform decision procedure. Corollary 2, immediately above also provides the following charac terization. U x 2 (TxtBc TxtEx) , U x satisfies Condition 2 but not Condition 1. Hence, since Angluin <ref> [Ang80b] </ref> provided an example U x satisfying Condition 2 and not Condition 1, her example is a uniformly decidable class witnessing that (TxtBc TxtEx) 6= ;.
Reference: [Ang82] <author> D. Angluin. </author> <title> Inference of reversible languages. </title> <journal> Journal of the ACM, </journal> <volume> 29 </volume> <pages> 741-765, </pages> <year> 1982. </year>
Reference-contexts: Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . Some interesting C x 's are learnable <ref> [Ang80a, Ang80b, Ang82] </ref> and some are not [Gol67]. In Section 3.1 we explore the problem of synthesizing learning machines for learnable C x 's from the corresponding x's.
Reference: [AS83] <author> D. Angluin and C. Smith. </author> <title> A survey of inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-289, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions <ref> [KW80, AS83, OSW86b] </ref>, a variety of enumeration techniques [Gol67, BB75, Wie90, Ful90b] are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows.
Reference: [Bar74] <author> J. Barzdin. </author> <title> Two theorems on the limiting synthesis of functions. In Theory of Algorithms and Programs, Latvian State University, </title> <journal> Riga, </journal> <volume> 210 </volume> <pages> 82-88, </pages> <year> 1974. </year> <title> identification of uniformly decidable classes of languages, one can learn grammars , one can learn decision procedures. We can prove this latter is not the case for TxtBc identification (of uniformly decidable classes of languages). </title>
Reference-contexts: Suppose a 2 N [ fflg, where fl is not a natural number. TxtBc a -identification [CL82] is just like TxtBc 2 See also <ref> [Bar74, CS78, CS83] </ref>. 3 One can learn larger classes of languages with TxtBc-identification than with TxtFex n -identification for any n [Cas88, Cas92]; of course this is at a cost of outputting infinitely many distinct grammars in the limit. identification except that, in the final sequence of "correct" programs, each program
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions [KW80, AS83, OSW86b], a variety of enumeration techniques <ref> [Gol67, BB75, Wie90, Ful90b] </ref> are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows.
Reference: [BC93] <author> G. Baliga and J. </author> <title> Case. Learnability: Admissible, co-finite, and hypersimple sets. </title> <booktitle> In Proceedings of the 20th International Colloquium on Automata, Languages and Programming, volume 700 of Lecture Notes in Computer Science, </booktitle> <pages> pages 289-300. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <institution> Lund, Sweden, </institution> <note> Ju-ly 1993. Journal version in press for Journal of Computer and System Sciences, </note> <year> 1996. </year>
Reference-contexts: The a &gt; 0 case is from [CL82], but [OW82a], independently, introduced the a = fl case. For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages <ref> [CL82, Cas88, Cas92, BC93] </ref>.
Reference: [BCJ95] <author> G. Baliga, J. Case, and S. Jain. </author> <title> Language learning with some negative information. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 51 </volume> <pages> 273-285, </pages> <year> 1995. </year>
Reference-contexts: We also know that, in this result, TxtEx (M f (x) ) cannot be improved to TxtEx n (M f (x) ). It is interesting to explore extensions of the present paper for the cases of adding small amounts of negative information to the input data <ref> [BCJ95] </ref>. In the light of Theorem 22 in [BCJ95] and the discussion following the proof of Theorem 12 above, it is reasonable to hope for some resultant improvements in mind change complexity for synthesized machines. <p> It is interesting to explore extensions of the present paper for the cases of adding small amounts of negative information to the input data <ref> [BCJ95] </ref>. In the light of Theorem 22 in [BCJ95] and the discussion following the proof of Theorem 12 above, it is reasonable to hope for some resultant improvements in mind change complexity for synthesized machines.
Reference: [Ber85] <author> R. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: In fact, for the a = 0 case, surprisingly, our characterizing condition is Angluin's Condition 2 from [Ang80b]! This condition is the subset principle, a necessary condition preventing overgeneralization in learning from positive data <ref> [Ang80b, Ber85, Cas92] </ref>. 4 Her Condition 1, also from [Ang80b], was an constructive version of her Condition 2, providing her characterization of uniformly decidable classes in TxtEx. 5 Hence, a uniformly decidable class is in (TxtBc TxtEx) , it satisfies Angluin's Condition 2 but not her Condition 1. <p> Our characterization of uniformly decidable classes in TxtBc is, surprisingly, just Angluin's Condition 2! As mentioned above, it is referred to as the subset principle, a necessary condition preventing overgeneralization in learning from positive data <ref> [Ang80b, Ber85, Cas92] </ref>.
Reference: [BF72] <author> J. Barzdin and R. Freivalds. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference-contexts: The additional input information of a program for generating the tell tales therefore makes a huge difference in the mind change complexity of the synthesized learning machine! 4 Future Directions Barzdin <ref> [BF72] </ref> first considered improvements of archetypal enumeration techniques, involving a majority vote strategy which has vastly better mind-change complexity. See also [LW89]. It would be interesting to look into variants of our algorithms above for synthesizing learning machines with improved mind-change complexity at least for interesting special cases.
Reference: [Blu67] <author> M. Blum. </author> <title> A machine independent theory of the complexity of recursive functions. </title> <journal> Journal of the ACM, </journal> <volume> 14 </volume> <pages> 322-336, </pages> <year> 1967. </year>
Reference-contexts: In such cases we implicitly assume that h; i is used to code the arguments, so, for example, ' i (x; y) stands for ' i (hx; yi). The quantifiers `8 1 ', and `9 1 ' essentially from <ref> [Blu67] </ref>, mean `for all but finitely many' and `there exist infinitely many', respectively. f : N ! N is limiting recursive def , (9 recursive g : (N fi N ) ! N )(8x)[f (x) = lim t!1 g (x; t)].
Reference: [Cas74] <author> J. </author> <title> Case. Periodicity in generations of automata. </title> <journal> Mathematical Systems Theory, </journal> <volume> 8 </volume> <pages> 15-32, </pages> <year> 1974. </year>
Reference-contexts: By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a recursive function p such that the languages W p (i) , i 0, are defined in stages as follows. Initially, the W p (i) 's are empty and is the empty sequence. Go to stage 1. Stage s 1. <p> Proof. We prove here the following restricted version of the theorem only. (8f 2 R)(8n)(9x)[C x 2 TxtEx 1 RecTxtBc n (M f (x) )]: Fix f and n. By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing recursive function p such that the languages W p (i) , i 0, are defined as follows. Enumerate p (1) in W p (0) . W p (1) will be a subset of ODD. The construction will use a set O. <p> Suppose by way of contradiction otherwise. Let f be given. By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing, p such that the following holds. We let L = fL j (9i 1)[ L = ' p (i) ]g. It will be easily seen that, for i &gt; 0, ' p (i) is either a characteristic function or an empty function.
Reference: [Cas86] <author> J. </author> <title> Case. Learning machines. </title> <editor> In W. Demopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: TxtFex a 1 -identification is clearly equivalent to TxtEx a fl . Osherson and Weinstein [OW82a] were the first to define the notions of TxtFex 0 fl and TxtFex fl fl , and the other cases of TxtFex a b -identification are from <ref> [Cas86, Cas88, Cas92] </ref>.
Reference: [Cas88] <author> J. </author> <title> Case. The power of vacillation. </title> <editor> In D. Haussler and L. Pitt, editors, </editor> <booktitle> Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 133-142. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1988. </year> <note> Expanded in [Cas92]. </note>
Reference-contexts: Interestingly, too, M P , in this case, outputs conjectures 1 The criterion requiring a machine, on positive data for a language L, to output eventually no more than n distinct grammars each of which is correct is called TxtFex n - identification <ref> [Cas88, Cas92] </ref>. TxtFex 1 -identification is just TxtEx-identification, but one can learn strictly larger classes of languages with TxtFex n+1 -identification than with TxtFex n -identification [Cas88, Cas92]. from the "hypothesis space" P itself [LZ93]. <p> for a language L, to output eventually no more than n distinct grammars each of which is correct is called TxtFex n - identification <ref> [Cas88, Cas92] </ref>. TxtFex 1 -identification is just TxtEx-identification, but one can learn strictly larger classes of languages with TxtFex n+1 -identification than with TxtFex n -identification [Cas88, Cas92]. from the "hypothesis space" P itself [LZ93]. Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . <p> Suppose a 2 N [ fflg, where fl is not a natural number. TxtBc a -identification [CL82] is just like TxtBc 2 See also [Bar74, CS78, CS83]. 3 One can learn larger classes of languages with TxtBc-identification than with TxtFex n -identification for any n <ref> [Cas88, Cas92] </ref>; of course this is at a cost of outputting infinitely many distinct grammars in the limit. identification except that, in the final sequence of "correct" programs, each program is allowed to make up to a mistakes (where a = fl denotes the unbounded, finite case). <p> The a &gt; 0 case is from [CL82], but [OW82a], independently, introduced the a = fl case. For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages <ref> [CL82, Cas88, Cas92, BC93] </ref>. <p> TxtFex a 1 -identification is clearly equivalent to TxtEx a fl . Osherson and Weinstein [OW82a] were the first to define the notions of TxtFex 0 fl and TxtFex fl fl , and the other cases of TxtFex a b -identification are from <ref> [Cas86, Cas88, Cas92] </ref>. <p> The a 2 f0; flg cases were independently introduced in [OW82a, OW82b]. RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification <ref> [Cas88, Cas92] </ref>. We sometimes write TxtBc for TxtBc 0 , etc. <p> Theorem 2 follows by a direct n-ary recursion theorem argument and also quickly but indirectly from the fact from <ref> [Cas88, Cas92] </ref> that (TxtFex n+1 TxtFex fl n ) 6= ;.
Reference: [Cas92] <author> J. </author> <title> Case. The power of vacillation in language learning. </title> <type> Technical Report 93-08, </type> <institution> University of Delaware, </institution> <year> 1992. </year> <note> Expands on [Cas88]; journal version revised for possible publication. </note>
Reference-contexts: Interestingly, too, M P , in this case, outputs conjectures 1 The criterion requiring a machine, on positive data for a language L, to output eventually no more than n distinct grammars each of which is correct is called TxtFex n - identification <ref> [Cas88, Cas92] </ref>. TxtFex 1 -identification is just TxtEx-identification, but one can learn strictly larger classes of languages with TxtFex n+1 -identification than with TxtFex n -identification [Cas88, Cas92]. from the "hypothesis space" P itself [LZ93]. <p> for a language L, to output eventually no more than n distinct grammars each of which is correct is called TxtFex n - identification <ref> [Cas88, Cas92] </ref>. TxtFex 1 -identification is just TxtEx-identification, but one can learn strictly larger classes of languages with TxtFex n+1 -identification than with TxtFex n -identification [Cas88, Cas92]. from the "hypothesis space" P itself [LZ93]. Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . <p> Suppose a 2 N [ fflg, where fl is not a natural number. TxtBc a -identification [CL82] is just like TxtBc 2 See also [Bar74, CS78, CS83]. 3 One can learn larger classes of languages with TxtBc-identification than with TxtFex n -identification for any n <ref> [Cas88, Cas92] </ref>; of course this is at a cost of outputting infinitely many distinct grammars in the limit. identification except that, in the final sequence of "correct" programs, each program is allowed to make up to a mistakes (where a = fl denotes the unbounded, finite case). <p> In fact, for the a = 0 case, surprisingly, our characterizing condition is Angluin's Condition 2 from [Ang80b]! This condition is the subset principle, a necessary condition preventing overgeneralization in learning from positive data <ref> [Ang80b, Ber85, Cas92] </ref>. 4 Her Condition 1, also from [Ang80b], was an constructive version of her Condition 2, providing her characterization of uniformly decidable classes in TxtEx. 5 Hence, a uniformly decidable class is in (TxtBc TxtEx) , it satisfies Angluin's Condition 2 but not her Condition 1. <p> The a &gt; 0 case is from [CL82], but [OW82a], independently, introduced the a = fl case. For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages <ref> [CL82, Cas88, Cas92, BC93] </ref>. <p> TxtFex a 1 -identification is clearly equivalent to TxtEx a fl . Osherson and Weinstein [OW82a] were the first to define the notions of TxtFex 0 fl and TxtFex fl fl , and the other cases of TxtFex a b -identification are from <ref> [Cas86, Cas88, Cas92] </ref>. <p> The a 2 f0; flg cases were independently introduced in [OW82a, OW82b]. RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification <ref> [Cas88, Cas92] </ref>. We sometimes write TxtBc for TxtBc 0 , etc. <p> Theorem 2 follows by a direct n-ary recursion theorem argument and also quickly but indirectly from the fact from <ref> [Cas88, Cas92] </ref> that (TxtFex n+1 TxtFex fl n ) 6= ;. <p> Our characterization of uniformly decidable classes in TxtBc is, surprisingly, just Angluin's Condition 2! As mentioned above, it is referred to as the subset principle, a necessary condition preventing overgeneralization in learning from positive data <ref> [Ang80b, Ber85, Cas92] </ref>.
Reference: [Cas94] <author> J. </author> <title> Case. Infinitary self-reference in learning theory. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 3-16, </pages> <year> 1994. </year>
Reference-contexts: By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a recursive function p such that the languages W p (i) , i 0, are defined in stages as follows. Initially, the W p (i) 's are empty and is the empty sequence. Go to stage 1. Stage s 1. <p> Proof. We prove here the following restricted version of the theorem only. (8f 2 R)(8n)(9x)[C x 2 TxtEx 1 RecTxtBc n (M f (x) )]: Fix f and n. By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing recursive function p such that the languages W p (i) , i 0, are defined as follows. Enumerate p (1) in W p (0) . W p (1) will be a subset of ODD. The construction will use a set O. <p> Suppose by way of contradiction otherwise. Let f be given. By the Operator Recursion Theorem <ref> [Cas74, Cas94] </ref>, there exists a 1-1 increasing, p such that the following holds. We let L = fL j (9i 1)[ L = ' p (i) ]g. It will be easily seen that, for i &gt; 0, ' p (i) is either a characteristic function or an empty function.
Reference: [CL82] <author> J. Case and C. Lynes. </author> <title> Machine inductive inference and language identification. </title> <editor> In M. Nielsen and E. Schmidt, editors, </editor> <booktitle> Proceedings of the 9th International Colloquium on Automata, Languages and Programming, </booktitle> <volume> volume 140, </volume> <pages> pages 107-115. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference-contexts: The proof of Theorem 3 below (in Section 3.1) presents an algorithm for transforming any x such that C x is TxtEx 0 -identifiable into a (behaviorally correct <ref> [CL82, OW82a] </ref> 2 or TxtBc) learning procedure which, on any listing of a language in C x , eventually converges to an infinite sequence of grammars each of which generates the input language. 3 For this, as well as our other results providing the synthesis of a learning machine, each synthesized <p> Theorem 10 shows the necessity of the cost, from Theorem 9, of passing from no mind changes for the input classes to finitely many in the synthesized learning machines. Suppose a 2 N [ fflg, where fl is not a natural number. TxtBc a -identification <ref> [CL82] </ref> is just like TxtBc 2 See also [Bar74, CS78, CS83]. 3 One can learn larger classes of languages with TxtBc-identification than with TxtFex n -identification for any n [Cas88, Cas92]; of course this is at a cost of outputting infinitely many distinct grammars in the limit. identification except that, in <p> The generalization to the a &gt; 0 cases in Definition 1 is motivated by the observation that humans rarely learn a language perfectly, where the a represents an upper bound on the numer of anomalies permitted in final grammars. The a &gt; 0 case is from <ref> [CL82] </ref>, but [OW82a], independently, introduced the a = fl case. For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages [CL82, Cas88, Cas92, BC93]. <p> The a &gt; 0 case is from [CL82], but [OW82a], independently, introduced the a = fl case. For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages <ref> [CL82, Cas88, Cas92, BC93] </ref>. <p> This motivates the following Definition 5 (1) M RecTxtBc a identifies L (written: L 2 RecTxtBc a (M)) def , (8 recursive texts T for L)(8 1 n)[W M (T [n]) = a L]. (2) RecTxtBc a = fL j (9M)[L RecTxtBc a (M)]g: Definition 4 is from <ref> [CL82] </ref>. The a 2 f0; flg cases were independently introduced in [OW82a, OW82b]. RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification [Cas88, Cas92]. We sometimes write TxtBc for TxtBc 0 , etc. <p> The a 2 f0; flg cases were independently introduced in [OW82a, OW82b]. RecTxtBc a 6= TxtBc a <ref> [CL82, Fre85] </ref>; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification [Cas88, Cas92]. We sometimes write TxtBc for TxtBc 0 , etc.
Reference: [CS78] <author> J. Case and C. Smith. </author> <title> Anomaly hierarchies of mechanized inductive inference. </title> <booktitle> In Symposium on the Theory of Computation, </booktitle> <pages> pages 314-319, </pages> <year> 1978. </year>
Reference-contexts: Suppose a 2 N [ fflg, where fl is not a natural number. TxtBc a -identification [CL82] is just like TxtBc 2 See also <ref> [Bar74, CS78, CS83] </ref>. 3 One can learn larger classes of languages with TxtBc-identification than with TxtFex n -identification for any n [Cas88, Cas92]; of course this is at a cost of outputting infinitely many distinct grammars in the limit. identification except that, in the final sequence of "correct" programs, each program
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: Suppose a 2 N [ fflg, where fl is not a natural number. TxtBc a -identification [CL82] is just like TxtBc 2 See also <ref> [Bar74, CS78, CS83] </ref>. 3 One can learn larger classes of languages with TxtBc-identification than with TxtFex n -identification for any n [Cas88, Cas92]; of course this is at a cost of outputting infinitely many distinct grammars in the limit. identification except that, in the final sequence of "correct" programs, each program
Reference: [dJK96] <author> D. de Jongh and M. </author> <title> Kanazawa. Angluin's tho-erem for indexed families of r.e. sets and applications. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <address> De-senzano del Garda, Italy. </address> <publisher> ACM Press, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: 2 TxtBc , (8U 2 U x )(9S U j S is finite)(8U 0 2 U x j S U 0 )[U 0 6 U ]. 9 It seems pedagogically useful to present the results in this order. 10 Mukouchi [Muk92] characterized the uniformly decidable classes in TxtEx 0 . <ref> [dJK96] </ref> surprisingly characterizes the r.e. classes in TxtEx and presents other interesting results. 11 Recall that the U i 's are defined in Definition 8 above.
Reference: [Fre85] <author> R. Freivalds. </author> <title> Recursiveness of the enumerating functions increases the inferrability of recursively enumerable sets. </title> <journal> Bulletin of the European Association for Theoretical Computer Science, </journal> <volume> 27 </volume> <pages> 35-40, </pages> <year> 1985. </year>
Reference-contexts: The a 2 f0; flg cases were independently introduced in [OW82a, OW82b]. RecTxtBc a 6= TxtBc a <ref> [CL82, Fre85] </ref>; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification [Cas88, Cas92]. We sometimes write TxtBc for TxtBc 0 , etc.
Reference: [Ful85] <author> M. Fulk. </author> <title> A Study of Inductive Inference machines. </title> <type> PhD thesis, </type> <institution> SUNY at Buffalo, </institution> <year> 1985. </year>
Reference: [Ful90a] <author> M. Fulk. </author> <title> Prudence and other conditions on formal language learning. </title> <journal> Information and Computation, </journal> <volume> 85 </volume> <pages> 1-11, </pages> <year> 1990. </year>
Reference: [Ful90b] <author> M. Fulk. </author> <title> Robust separations in inductive infer-ence. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 405-410, </pages> <address> St. Louis, Missouri 1990. </address>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions [KW80, AS83, OSW86b], a variety of enumeration techniques <ref> [Gol67, BB75, Wie90, Ful90b] </ref> are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows.
Reference: [Gle86] <author> L. Gleitman. </author> <title> Biological dispositions to learn language. </title> <editor> In W. Demopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Publ. Co., </publisher> <year> 1986. </year>
Reference: [Gol67] <author> E. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions [KW80, AS83, OSW86b], a variety of enumeration techniques <ref> [Gol67, BB75, Wie90, Ful90b] </ref> are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows. <p> Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . Some interesting C x 's are learnable [Ang80a, Ang80b, Ang82] and some are not <ref> [Gol67] </ref>. In Section 3.1 we explore the problem of synthesizing learning machines for learnable C x 's from the corresponding x's. <p> There has been considerable interest in the computational learning theory community in the learnability, from positive data, of uniformly decidable classes of recursive languages. This begins with <ref> [Gol67] </ref>, but especially spans [Ang80b] through [ZL95]. In the context of the present paper, one might hope to obtain synthesized learning machines with better mind change complexity if one input decision procedures in place of grammars for the languages to be learned from positive data. <p> L def , (written:L 2 TxtEx a b (M)) [L 2 TxtEx a (M) ^ (8 texts T for L)[card (fx j? 6= M (T [x]) ^ M (T [x]) 6= M (T [x + 1])g) b]]. (3) TxtEx a b = fL j (9M)[L TxtEx a b (M)]g: Gold <ref> [Gol67] </ref> introduced the criteria we call TxtEx 0 fl . The generalization to the a &gt; 0 cases in Definition 1 is motivated by the observation that humans rarely learn a language perfectly, where the a represents an upper bound on the numer of anomalies permitted in final grammars. <p> For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages [CL82, Cas88, Cas92, BC93]. Gold's model of language learning from text (positive information) and by machine <ref> [Gol67] </ref> has been very in fluential in contemporary theories of natural language and in mathematical work motivated by its possible connection to human language learning (see, for example, [Pin79, WC80, Wex82, OSW82, OSW84, Ber85, Gle86, Cas86, OSW86a, OSW86b, Ful85, Ful90a, Kir92, We sometimes write TxtEx for TxtEx 0 fl and TxtEx
Reference: [HU79] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory Languages and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1979. </year>
Reference-contexts: The set of all (total) recursive functions of one variable is denoted by R. R 0;1 denotes the class of all (total) recursive 0-1 valued functions. W i denotes domain (' i ). W i is, then, the r.e. set/language ( N ) accepted (or equivalently, gener ated <ref> [HU79] </ref>) by the '-program i. W s i = fx s j x appears in W i in s stepsg. For language L N , L [x] denotes fw x j x 2 Lg, and we use L to denote the characteristic function of L; L denotes complement of L.
Reference: [Kir92] <author> D. Kirsh. </author> <title> PDP learnability and innate knowledge of language. </title> <editor> In S. Davis, editor, </editor> <booktitle> Connectionism: Theory and Practice, </booktitle> <pages> pages 297-322. </pages> <publisher> Oxford University Press, </publisher> <address> NY, </address> <year> 1992. </year>
Reference: [KLHM93] <author> S. Kapur, B. Lust, W. Harbert, and G. Marto-hardjono. </author> <title> Universal grammar and learnability theory: The case of binding domains and the `subset principle'. </title> <editor> In E. Reuland and W. Abra-ham, editors, </editor> <booktitle> Knowledge and Language, </booktitle> <volume> volume I, </volume> <pages> pages 185-216. </pages> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: S 1 = a S 2 means that card (fx j x 2 S 1 S 2 g) a. ODD denotes the set of odd natural numbers. EVEN denotes the set of even natural numbers. 4 See <ref> [KLHM93, Wex93] </ref> for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in [MW87]. 5 We restate these conditions in Section 3.2 below. 6 Decorations are subscripts, superscripts and the like. max (); min () denote the maximum and minimum of a set, respectively,
Reference: [KW80] <author> R. Klette and R. Wiehagen. </author> <title> Research in the theory of inductive inference by GDR mathematicians A survey. </title> <journal> Information Sciences, </journal> <volume> 22 </volume> <pages> 149-169, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions <ref> [KW80, AS83, OSW86b] </ref>, a variety of enumeration techniques [Gol67, BB75, Wie90, Ful90b] are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows.
Reference: [LW89] <author> N. Littlestone and M. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In Symposium on the Theory of Computation, </booktitle> <pages> pages 256-261, </pages> <year> 1989. </year>
Reference-contexts: See also <ref> [LW89] </ref>. It would be interesting to look into variants of our algorithms above for synthesizing learning machines with improved mind-change complexity at least for interesting special cases.
Reference: [LZ93] <author> S. Lange and T. Zeugmann. </author> <title> Language learning in dependence on the space of hypotheses. </title> <booktitle> In Proc. 6th Annual Conf. on Comp. Learning Theory, </booktitle> <pages> pages 156-166, </pages> <year> 1993. </year>
Reference-contexts: TxtFex 1 -identification is just TxtEx-identification, but one can learn strictly larger classes of languages with TxtFex n+1 -identification than with TxtFex n -identification [Cas88, Cas92]. from the "hypothesis space" P itself <ref> [LZ93] </ref>. Suppose x is a procedure for enumerating an r.e. (possibly infinite) set of grammars P . Let C x be the set of languages generated by the grammars in P . Some interesting C x 's are learnable [Ang80a, Ang80b, Ang82] and some are not [Gol67]. <p> input language. 3 For this, as well as our other results providing the synthesis of a learning machine, each synthesized learning machine can be construed as implementing a (perhaps complicated) enumeration technique; however, of necessity, in most cases the conjectures of the synthesized machines go beyond the original hypothesis space <ref> [LZ93] </ref>. Regarding the positive results about C x 's, we also present corresponding lower bound results showing, in many cases, the positive results to be best possible. <p> As An-gluin [Ang80b] essentially points out, all of the formal language style example classes are uniformly decidable. [Ang80b], for example, deals with so-called exact <ref> [LZ93] </ref> learning in which, for each learnable class, the programs learned derive naturally from the defining uniform decision procedure for that class. 8 Herein, we will synthesize learning machines whose hypothesis spaces, in some cases of necessity, go beyond ones naturally associated with the defining uniform decision procedures for the classes <p> learning in which, for each learnable class, the programs learned derive naturally from the defining uniform decision procedure for that class. 8 Herein, we will synthesize learning machines whose hypothesis spaces, in some cases of necessity, go beyond ones naturally associated with the defining uniform decision procedures for the classes <ref> [LZ93] </ref>. The next two theorems (Theorems 7 and 8) deal with the special case where the uniformly decidable classes are finite. The first is an even more positive result than its analog for uniformly r.e. classes (Theorem 1 in Section 3.1 above). It's proof is straightforward, hence, omitted.
Reference: [MR67] <author> A. Meyer and D. Ritchie. </author> <title> The complexity of loop programs. </title> <booktitle> In Proceedings of the 22nd National ACM Conference, </booktitle> <pages> pages 465-469. </pages> <publisher> Thomas Book Co., </publisher> <year> 1967. </year>
Reference-contexts: Clearly for functions f computed by some program in P , this procedure will eventually lock onto a fixed conjecture correct for f . For example, if P is the set of Meyer-Ritchie loop-programs <ref> [MR67, RC94] </ref>, then the enumeration technique we just described learns, in the limit, a correct final program for each primitive recursive function. More generally, enumeration techniques involve some process of matching and elimination based on an initial r.e. set of programs for the objects to be learned.
Reference: [Muk92] <author> Y. Mukouchi. </author> <title> Characterization of finite identification. </title> <editor> In K. P. Jantke, editor, </editor> <booktitle> Proceedings of the Third International Workshop on Analogical and Inductive Inference, Dagstuhl Castle, Ger-many, </booktitle> <pages> pages 260-267, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Corollary 2 U x 2 TxtBc , (8U 2 U x )(9S U j S is finite)(8U 0 2 U x j S U 0 )[U 0 6 U ]. 9 It seems pedagogically useful to present the results in this order. 10 Mukouchi <ref> [Muk92] </ref> characterized the uniformly decidable classes in TxtEx 0 . [dJK96] surprisingly characterizes the r.e. classes in TxtEx and presents other interesting results. 11 Recall that the U i 's are defined in Definition 8 above.
Reference: [MW87] <author> R. Manzini and K. </author> <title> Wexler. Parameters, binding theory and learnability. </title> <journal> Linguistic Inquiry, </journal> <volume> 18 </volume> <pages> 413-444, </pages> <year> 1987. </year>
Reference-contexts: ODD denotes the set of odd natural numbers. EVEN denotes the set of even natural numbers. 4 See [KLHM93, Wex93] for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in <ref> [MW87] </ref>. 5 We restate these conditions in Section 3.2 below. 6 Decorations are subscripts, superscripts and the like. max (); min () denote the maximum and minimum of a set, respectively, where max (;) = 0 and min (;) is undefined.
Reference: [MY78] <author> M. Machtey and P. Young. </author> <title> An Introduction to the General Theory of Algorithms. </title> <publisher> North Hol-land, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: and h with or without decorations range over total functions with arguments and values from N . h; i stands for an arbitrary, computable, one-to-one encoding of all pairs of natural numbers onto N [Rog67]. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Roy87] </ref>. ' i denotes the partial computable function computed by program i in the '-system. The set of all (total) recursive functions of one variable is denoted by R. R 0;1 denotes the class of all (total) recursive 0-1 valued functions. W i denotes domain (' i ).
Reference: [OSW82] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Ideal learning machines. </title> <journal> Cognitive Science, </journal> <volume> 6 </volume> <pages> 277-290, </pages> <year> 1982. </year>
Reference: [OSW84] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Learning theory and natural language. </title> <journal> Cognition, </journal> <volume> 17 </volume> <pages> 1-28, </pages> <year> 1984. </year>
Reference: [OSW86a] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> An analysis of a learning paradigm. </title> <editor> In W. De-mopoulos and A. Marras, editors, </editor> <title> Language Learning and Concept Acquisition. </title> <publisher> Ablex Pub-l. Co., </publisher> <year> 1986. </year>
Reference: [OSW86b] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions <ref> [KW80, AS83, OSW86b] </ref>, a variety of enumeration techniques [Gol67, BB75, Wie90, Ful90b] are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows. <p> Lets see what happens with enumeration techniques and their constructivity in the context of learning (formal) grammars for (formal) languages from positive information about those languages <ref> [OSW86b] </ref>. The present paper was inspired by the following amazingly negative result from [OSW88]. <p> which, from positive data about any one of the languages generated by g and g 0 , finds in the limit, for this language, a correct grammar! By the way, the problem is not that correct programs for finite classes of languages can't be learned in the limit; they can <ref> [OSW86b] </ref>, and by an obvious enumeration technique. The problem is how to pass algorithmically from a list of grammars to a machine which so learns the corresponding languages. <p> In this paper we assume, without loss of generality, that for all t , [M () 6=?] ) [M (t ) 6=?]. M ranges over language learning machines. Lemma 4.2.2B in <ref> [OSW86b] </ref> easily generalizes to cover all learning criteria considered in this paper thereby providing a recursive enumeration M 0 ; M 1 ; . . . of (total) language learning machines such that, for any inference criteria I, every L 2 I is I-identified by some machine in this enumeration. <p> Suppose by way of contradiction that L 2 C x is such that (8S L)(9L 0 j S L 0 )[(L 0 2 C x ) ^ (L 0 6= 2a L)]: Let be a TxtEx a locking sequence for M on L <ref> [OSW86b] </ref>. Let L 0 2 C x be such that L 0 content () and L 0 6= 2a L. Since W M () = a L, it follows that W M () 6= a L 0 . <p> It is surprising and important that the subset principle alone (Angluin's Condition 2) without the added con-structivity conditions of Angluin's Condition 1 charac terizes the uniformly decidable classes U x 2 TxtBc. <ref> [OSW86b] </ref> note that a class of r.e. languages U can be learned in the limit from positive data by a not necessarily algorithmic procedure iff Angluin's Condition 2 holds for U .
Reference: [OSW88] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Synthesizing inductive expertise. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 138-161, </pages> <year> 1988. </year>
Reference-contexts: Lets see what happens with enumeration techniques and their constructivity in the context of learning (formal) grammars for (formal) languages from positive information about those languages [OSW86b]. The present paper was inspired by the following amazingly negative result from <ref> [OSW88] </ref>. <p> W i n g TxtFex fl n (M f (i 0 ;i 1 ;...;i n ) )]. The n = 1 case of Theorem 2 just above, with `limiting recursive' replaced by `recursive' is just the negative result from <ref> [OSW88] </ref> that inspired the present paper, but, of course, Theorem 2 is stronger and more general. Theorem 2 follows by a direct n-ary recursion theorem argument and also quickly but indirectly from the fact from [Cas88, Cas92] that (TxtFex n+1 TxtFex fl n ) 6= ;. <p> This nicely generalizes the special case from <ref> [OSW88] </ref> p resented as Corollary 1 below. in one-shot (i.e., without any mind changes).
Reference: [OW82a] <author> D. Osherson and S. Weinstein. </author> <title> Criteria of language learning. </title> <journal> Information and Control, </journal> <volume> 52 </volume> <pages> 123-138, </pages> <year> 1982. </year>
Reference-contexts: The proof of Theorem 3 below (in Section 3.1) presents an algorithm for transforming any x such that C x is TxtEx 0 -identifiable into a (behaviorally correct <ref> [CL82, OW82a] </ref> 2 or TxtBc) learning procedure which, on any listing of a language in C x , eventually converges to an infinite sequence of grammars each of which generates the input language. 3 For this, as well as our other results providing the synthesis of a learning machine, each synthesized <p> The generalization to the a &gt; 0 cases in Definition 1 is motivated by the observation that humans rarely learn a language perfectly, where the a represents an upper bound on the numer of anomalies permitted in final grammars. The a &gt; 0 case is from [CL82], but <ref> [OW82a] </ref>, independently, introduced the a = fl case. For these and the other success criteria of this paper, we have that tolerating more anomalies leads to being able to learn larger classes of languages [CL82, Cas88, Cas92, BC93]. <p> We sometimes write TxtFex b for TxtFex 0 b . TxtFex a 1 -identification is clearly equivalent to TxtEx a fl . Osherson and Weinstein <ref> [OW82a] </ref> were the first to define the notions of TxtFex 0 fl and TxtFex fl fl , and the other cases of TxtFex a b -identification are from [Cas86, Cas88, Cas92]. <p> The a 2 f0; flg cases were independently introduced in <ref> [OW82a, OW82b] </ref>. RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification [Cas88, Cas92]. We sometimes write TxtBc for TxtBc 0 , etc.
Reference: [OW82b] <author> D. Osherson and S. Weinstein. </author> <title> A note on formal learning theory. </title> <journal> Cognition, </journal> <volume> 11 </volume> <pages> 77-88, </pages> <year> 1982. </year>
Reference-contexts: The a 2 f0; flg cases were independently introduced in <ref> [OW82a, OW82b] </ref>. RecTxtBc a 6= TxtBc a [CL82, Fre85]; however, the restriction to recursive texts doesn't affect learning power for TxtFex a b -identification [Cas88, Cas92]. We sometimes write TxtBc for TxtBc 0 , etc.
Reference: [PEH64] <author> M. Pour-El and W. Howard. </author> <title> A structural criterion for recursive enumeration without repetition. </title> <journal> Zeitschrift fur Mathematische Logik und Grund-lagen der Mathematik, </journal> <volume> 10 </volume> <pages> 105-114, </pages> <year> 1964. </year>
Reference-contexts: Corollary 1 above and, more generally, the footnote to Theorem 1 above suggest to us ones exploring the relevance to our paper's topics of r.e. classes of languages which can be enumerated with n + 1 duplications but not with n <ref> [PEH64, PEP65] </ref>. In this interest we have a preliminary result complementing Theorem 3 above (in Section 3.1) as follows.
Reference: [PEP65] <author> M. Pour-El and H. Putnam. </author> <title> Recursively e-nunumerable classes and their application to recursive sequences of formal theories. </title> <journal> Arch. f. Math. Log. Grund., </journal> <volume> 8 </volume> <pages> 104-121, </pages> <year> 1965. </year>
Reference-contexts: Corollary 1 above and, more generally, the footnote to Theorem 1 above suggest to us ones exploring the relevance to our paper's topics of r.e. classes of languages which can be enumerated with n + 1 duplications but not with n <ref> [PEH64, PEP65] </ref>. In this interest we have a preliminary result complementing Theorem 3 above (in Section 3.1) as follows.
Reference: [Pin79] <author> S. Pinker. </author> <title> Formal models of language learning. </title> <journal> Cognition, </journal> <volume> 7 </volume> <pages> 217-283, </pages> <year> 1979. </year>
Reference: [RC94] <author> J. Royer and J. </author> <title> Case. </title> <booktitle> Subrecursive Programming Systems: Complexity and Succinctness. Progress in Theoretical Computer Science. </booktitle> <address> Birkhauser Boston, </address> <year> 1994. </year>
Reference-contexts: Clearly for functions f computed by some program in P , this procedure will eventually lock onto a fixed conjecture correct for f . For example, if P is the set of Meyer-Ritchie loop-programs <ref> [MR67, RC94] </ref>, then the enumeration technique we just described learns, in the limit, a correct final program for each primitive recursive function. More generally, enumeration techniques involve some process of matching and elimination based on an initial r.e. set of programs for the objects to be learned.
Reference: [Rog58] <author> H. Rogers. </author> <title> Godel numberings of partial recursive functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23 </volume> <pages> 331-341, </pages> <year> 1958. </year>
Reference-contexts: and h with or without decorations range over total functions with arguments and values from N . h; i stands for an arbitrary, computable, one-to-one encoding of all pairs of natural numbers onto N [Rog67]. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Roy87] </ref>. ' i denotes the partial computable function computed by program i in the '-system. The set of all (total) recursive functions of one variable is denoted by R. R 0;1 denotes the class of all (total) recursive 0-1 valued functions. W i denotes domain (' i ).
Reference: [Rog67] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New Y-ork, 1967. </address> <publisher> Reprinted, MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Fix a canonical indexing of the finite sets 1-1 onto N <ref> [Rog67] </ref>. The min () of a collection of finite sets is, then, the finite set in the collection with minimal canonical index. Also, when we compare finite sets by &lt; we are comparing their corresponding canonical indices. <p> We use the symbol # to denote that a computation converges. f; g and h with or without decorations range over total functions with arguments and values from N . h; i stands for an arbitrary, computable, one-to-one encoding of all pairs of natural numbers onto N <ref> [Rog67] </ref>. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N [Rog58, MY78, Roy87]. ' i denotes the partial computable function computed by program i in the '-system. The set of all (total) recursive functions of one variable is denoted by R. <p> Let S L = content (). Let i be the least number such that S L 8 Typically, in the literature one uses i as a "program" for the U i from Definition 8 above. S-m-n in the '-system <ref> [Rog67] </ref> clearly yields corresponding '-decision procedures or grammars as one wishes. U i . Let S 1 L = W M () U i . It is easy to verify that (a) and (b) are satisfied. 2 Claim 4 Suppose T is a text for L 2 U x .
Reference: [Roy87] <author> J. Royer. </author> <title> A Connotational Theory of Program Structure. </title> <booktitle> Lecture Notes in Computer Science 273. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: and h with or without decorations range over total functions with arguments and values from N . h; i stands for an arbitrary, computable, one-to-one encoding of all pairs of natural numbers onto N [Rog67]. ' denotes a fixed acceptable programming system for the partial computable functions: N ! N <ref> [Rog58, MY78, Roy87] </ref>. ' i denotes the partial computable function computed by program i in the '-system. The set of all (total) recursive functions of one variable is denoted by R. R 0;1 denotes the class of all (total) recursive 0-1 valued functions. W i denotes domain (' i ).
Reference: [Sha71] <author> N. Shapiro. </author> <title> Review of "Limiting recursion" by E.M. Gold and "Trial and error predicates and the solution to a problem of Mostowski" by H. Putnam. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 36:342, </volume> <year> 1971. </year>
Reference: [WC80] <author> K. Wexler and P. Culicover. </author> <title> Formal Principles of Language Acquisition. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1980. </year>
Reference: [Wex82] <author> K. </author> <title> Wexler. On extensional learnability. </title> <journal> Cognition, </journal> <volume> 11 </volume> <pages> 89-95, </pages> <year> 1982. </year>
Reference: [Wex93] <author> K. </author> <title> Wexler. The subset principle is an intensional principle. </title> <editor> In E. Reuland and W. Abraham, editors, </editor> <booktitle> Knowledge and Language, </booktitle> <volume> volume I, </volume> <pages> pages 217-239. </pages> <publisher> Kluwer, </publisher> <year> 1993. </year>
Reference-contexts: S 1 = a S 2 means that card (fx j x 2 S 1 S 2 g) a. ODD denotes the set of odd natural numbers. EVEN denotes the set of even natural numbers. 4 See <ref> [KLHM93, Wex93] </ref> for discussion regarding the possible connection between this subset principle and a more traditionally linguistically oriented one in [MW87]. 5 We restate these conditions in Section 3.2 below. 6 Decorations are subscripts, superscripts and the like. max (); min () denote the maximum and minimum of a set, respectively,
Reference: [Wie90] <author> R. Wiehagen. </author> <title> A thesis in inductive inference. </title> <editor> In P. Schmitt J. Dix, K. Jantke, editor, </editor> <booktitle> Non-monotonic and Inductive Logic, 1st International Workshop, volume 543 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 184-207. </pages> <publisher> Springer Verlag, </publisher> <address> Karlsruhe, Germany 1990. </address>
Reference-contexts: 1 Introduction In the context of learning programs in the limit for functions [KW80, AS83, OSW86b], a variety of enumeration techniques <ref> [Gol67, BB75, Wie90, Ful90b] </ref> are important and ubiquitous. Here is the archetypal case. Suppose one has an r.e. set P of programs for computing total functions. One proceeds as follows.
Reference: [ZL95] <author> T. Zeugmann and S. Lange. </author> <title> A guided tour across the boundaries of learning recursive languages. </title> <editor> In Klaus P. Jantke and Steffen Lange, editors, </editor> <booktitle> Algorithmic Learning for Knowledge-Based Systems, volume 961 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 190-258. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: There has been considerable interest in the computational learning theory community in the learnability, from positive data, of uniformly decidable classes of recursive languages. This begins with [Gol67], but especially spans [Ang80b] through <ref> [ZL95] </ref>. In the context of the present paper, one might hope to obtain synthesized learning machines with better mind change complexity if one input decision procedures in place of grammars for the languages to be learned from positive data. <p> As noted in Section 1, there has been considerable interest in the computational learning theory community in learnability, from positive data, of uniformly decidable classes of recursive languages <ref> [Ang80b, ZL95] </ref>.
References-found: 56

