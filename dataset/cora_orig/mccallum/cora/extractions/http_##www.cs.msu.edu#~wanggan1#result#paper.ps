URL: http://www.cs.msu.edu/~wanggan1/result/paper.ps
Refering-URL: http://www.cs.msu.edu/~wanggan1/
Root-URL: http://www.cs.msu.edu
Email: wanggan1@cse.msu.edu mahadeva@cse.msu.edu  
Title: Hierarchical Optimization of Large Manufacturing Systems  
Author: Gang Wang Sridhar Mahadevan 
Address: East Lansing, MI 48823 East Lansing, MI 48823  
Affiliation: Dept. of Computer Science and Engg. Dept. of Computer Science and Engg. Michigan State University Michigan State University  
Abstract: We present a hierarchical adaptive algorithm for intelligent control of a large discrete-part manufacturing system. The algorithm is given as input a discrete-event simulation model of the manufacturing system, and produces as output a control policy that minimizes a given cost function. In particular, the manufacturing system studied here is a transfer line, a sequence of unreliable machines isolated by product buffers. The cost function is to minimize the total work-in-process inventory, and the repair and maintenance charges, while achieving the desired target demand. A key strength of the hierarchical optimization algorithm is that it can be scaled to very large manufacturing systems, such as a 12-machine transfer line with an overall state space of 10 15 states. We present experimental results showing that the optimization algorithm is superior to well-known heuristics for running transfer lines, such as KANBAN and CONWIP. We also show that the hierarchical method is superior to a "flat" version of the same algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Berkley, B. J. </author> <year> 1992. </year> <title> "A review of the kanban production control research literature" in Production and Operations Management 1(4), </title> <type> 393-411. </type>
Reference-contexts: Since it is intractable to analytically compute an optimal control policy for transfer lines with more than 2 machines [6], some heuristic control method must be employed. Many heuristics for controlling a transfer line have been studied and used in real manufacturing sites, the most significant being KANBAN <ref> [1] </ref> and constant work-in-progress (CONWIP) [13]. A key disadvantage of these heuristics is that they only address the problem of minimizing work-in-process inventory. However, in reality, many other costs need to be considered, including the cost of production of parts, and the repair and preventive maintenance costs.
Reference: [2] <author> Bonvik, A. M. Couch, C. Gershwin, S. </author> <year> 1996. </year> <title> "A Comparison of Production-Line Control Mechanism". </title> <note> Accepted for International Journal of Production Research. </note>
Reference: [3] <author> Crites, R. Barto, A. </author> <year> 1996. </year> <title> "Improving elevator performance using reinforcement learning" in Neural Information Processing System (NIPS). </title>
Reference-contexts: The aim of reinforcement learning is to infer a policy that maximizes a long-term measure of the one-step rewards (or costs). Reinforcement learning has generated many successful application in diverse areas, including elevator scheduling <ref> [3] </ref>, and dynamic channel allocation in cellular telephone system [12]. The long-term optimality metric used in these studies is to maximize the discounted sum of rewards. An alternative (and sometimes more natural) met (M) isolated by product inventory buffers (B). ric is to maximize the long-term average reward received [8]. <p> In unichain SMDP's, the average reward is constant across states. Many real-world SMDP's, including the transfer line problem below and the elevator task <ref> [3] </ref> are unichain. 3 The SMART Algorithm It is often too difficult to determine a gain-optimal policy for a given SMDP by analytically solving Equation 3, due to the large size of the state space and the lack of a one-step probability transition model. <p> The number of states in most optimization problems of interest is too large to use a table-based representation of the action value function. Consequently, a number of methods have been developed to approximate the action value function using parametric techniques, such as neural nets <ref> [3] </ref> and CMAC [14].
Reference: [4] <author> Gershwin, S. Caramanis, M. Murray, P. </author> <year> 1988. </year> <title> "Simulation Experience with a Hierarchical Scheduling Policy for a Simple Manufacturing System", </title> <booktitle> in Proceedings of the 27th IEEE Conference on Decision and Control, </booktitle> <address> Austin, Texas. </address>
Reference-contexts: The exponential growth in states make it impossible to apply reinforcement learning to optimize the transfer line as a whole. Instead, we have to employ some method to hierarchically decompose the transfer line into simpler optimization problems. Hierarchical methods of optimizing manufacturing systems have been studied before <ref> [4] </ref> [5] [11]. However, individual machines in a transfer line interact and straightforward decompositions methods can produce sub-optimal results. Our reinforcement learning based approach is quite distinct from these earlier hierarchical methods, and in essence is as follows.
Reference: [5] <author> Gershwin, S. </author> <year> 1987. </year> <title> "A Hierarchical Framework for Manufacturing System Scheduling: A Two-Machine Example." </title> <booktitle> in Proceedings of the 26th IEEE Conference on Decision and Control, </booktitle> <address> Los Angeles, CA. </address>
Reference-contexts: The exponential growth in states make it impossible to apply reinforcement learning to optimize the transfer line as a whole. Instead, we have to employ some method to hierarchically decompose the transfer line into simpler optimization problems. Hierarchical methods of optimizing manufacturing systems have been studied before [4] <ref> [5] </ref> [11]. However, individual machines in a transfer line interact and straightforward decompositions methods can produce sub-optimal results. Our reinforcement learning based approach is quite distinct from these earlier hierarchical methods, and in essence is as follows.
Reference: [6] <author> Gershwin, S. </author> <year> 1994. </year> <title> Manufacturing System Engineering. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: 1 Introduction Discrete-part manufacturing systems consist of a (parallel or serial) combination of distinct machines, whose overall goal is to satisfy target demand while minimizing work-in-process inventory <ref> [6] </ref>. The machines age as they are used, and are subject to breakdown and consequently need repair. Periodic maintenance can avoid unnecessary and costly repair. <p> Since it is intractable to analytically compute an optimal control policy for transfer lines with more than 2 machines <ref> [6] </ref>, some heuristic control method must be employed. Many heuristics for controlling a transfer line have been studied and used in real manufacturing sites, the most significant being KANBAN [1] and constant work-in-progress (CONWIP) [13]. <p> We demonstrate that the hierarchical SMART algorithm produces a transfer line policy that is superior to both CONWIP and KANBAN, and is also superior to the previous "flat" version of SMART. 2 Stochastic Model of a Transfer Line A transfer line <ref> [6] </ref> models an assembly line operation in many factories, where a sequence of machines is needed to make finished parts, which are isolated by product buffers (see figure 1.) The buffers are needed since the machines are unreliable, and a failure of one machine should not disrupt the working of the
Reference: [7] <author> Law, A. Kelton, W. </author> <year> 1991. </year> <title> Simulation Modeling and Analysis. </title> <address> New York, USA: </address> <publisher> McGraw-Hill. </publisher>
Reference: [8] <author> Mahadevan, S. </author> <year> 1996. </year> <title> "Average Reward Reinforcement Learning: </title> <booktitle> Foundations, Algorithms, and Empirical Results" in Machine Learning., </booktitle> <volume> 22, </volume> <pages> 159-195. </pages>
Reference-contexts: The long-term optimality metric used in these studies is to maximize the discounted sum of rewards. An alternative (and sometimes more natural) met (M) isolated by product inventory buffers (B). ric is to maximize the long-term average reward received <ref> [8] </ref>. In particular, we have developed an average-reward reinforcement learning method that is suited to optimize discrete-event simulation models.
Reference: [9] <author> Mahadevan, S.; Marchalleck, N.; Das, T.; and Gosavi, A. </author> <year> 1997. </year> <title> "Self-improving factory simulation using continuous-time average reward reinforcement learning" in Proceedings of the Fourteenth International Machine Learning Conference., </title> <editor> D. Fisher (editor), </editor> <publisher> Morgan Kaufmann. </publisher> <pages> 202-210. </pages>
Reference-contexts: The algorithm called SMART (for Semi-markov Average Reward Technique) was used to learn a flexible maintenance policy for a large multi-product machine with buffers, and shown to be superior to heuristic maintenance procedures, such as coefficient of operational readiness (COR) and age replacement (AR) <ref> [9] </ref>. In this paper, we describe a hierarchical version of the SMART algorithm, and apply it to optimize an even larger 12-machine transfer line. <p> Each sample trajectory provides instances of states and rewards, and can be used to iteratively improve a given policy. In this paper we develop a hierarchical method for solving SDMPs, based on a previously developed algorithm called SMART (for Semi-Markov Average Reward Technique) <ref> [9] </ref>.
Reference: [10] <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Mathematical Statistics. </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference-contexts: However, in reality, many other costs need to be considered, including the cost of production of parts, and the repair and preventive maintenance costs. An analytical approach to optimizing manufacturing systems consists of modeling each machine using a standard stochastic process, such as a Markov decision process (MDP) <ref> [10] </ref>. The optimal procedure can be determined using a dynamic programming algorithm, such as value or policy iteration. <p> We assume here that the evolution of states at decision epochs can be modeled as a semi-Markov decision process (SMDP) <ref> [10] </ref>.
Reference: [11] <author> Sethi, S., and Zhang, Q. </author> <year> 1994. </year> <title> Hierarchical Decision Making in Stochastic Manufacturing Systems., </title> <publisher> Birkhauser. </publisher>
Reference-contexts: The exponential growth in states make it impossible to apply reinforcement learning to optimize the transfer line as a whole. Instead, we have to employ some method to hierarchically decompose the transfer line into simpler optimization problems. Hierarchical methods of optimizing manufacturing systems have been studied before [4] [5] <ref> [11] </ref>. However, individual machines in a transfer line interact and straightforward decompositions methods can produce sub-optimal results. Our reinforcement learning based approach is quite distinct from these earlier hierarchical methods, and in essence is as follows.
Reference: [12] <author> Singh, S. Bertsekas D. </author> <year> 1996. </year> <title> "Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems" in Neural Information Processing Systems (NIPS), </title> <year> 1996. </year>
Reference-contexts: The aim of reinforcement learning is to infer a policy that maximizes a long-term measure of the one-step rewards (or costs). Reinforcement learning has generated many successful application in diverse areas, including elevator scheduling [3], and dynamic channel allocation in cellular telephone system <ref> [12] </ref>. The long-term optimality metric used in these studies is to maximize the discounted sum of rewards. An alternative (and sometimes more natural) met (M) isolated by product inventory buffers (B). ric is to maximize the long-term average reward received [8].
Reference: [13] <author> Spearman, M. L., D. L. Woodruff, and W. J. Hopp. </author> <year> 1990. </year> <note> "CONWIP: a pull alternative to kan-ban" in International Journal of Production Research 28(5), 879-894. </note>
Reference-contexts: Many heuristics for controlling a transfer line have been studied and used in real manufacturing sites, the most significant being KANBAN [1] and constant work-in-progress (CONWIP) <ref> [13] </ref>. A key disadvantage of these heuristics is that they only address the problem of minimizing work-in-process inventory. However, in reality, many other costs need to be considered, including the cost of production of parts, and the repair and preventive maintenance costs.
Reference: [14] <author> Sutton, R., Barto, A. </author> <year> 1998. </year> <title> Reinforcement Learning: An Introduction, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Recently, an alternative approach has become popular, since it addresses both the curse of dimensionality and the lack of an a priori transition model. This approach, called reinforcement learning (RL) <ref> [14] </ref> relies on a discrete-event simulation model and machine learning methods to approximately solve the dynamic programming optimality equations. The aim of reinforcement learning is to infer a policy that maximizes a long-term measure of the one-step rewards (or costs). <p> R (x; a), which is the average-adjusted sum of rewards of doing action a in state x once, and thereafter following the optimal policy. 1 The derivation of this algorithm from the Bellman equation for SMDPs (Equation 3) is fairly straightforward and resembles the derivation of algorithms such as Q-learning <ref> [14] </ref>. The number of states in most optimization problems of interest is too large to use a table-based representation of the action value function. Consequently, a number of methods have been developed to approximate the action value function using parametric techniques, such as neural nets [3] and CMAC [14]. <p> as Q-learning <ref> [14] </ref>. The number of states in most optimization problems of interest is too large to use a table-based representation of the action value function. Consequently, a number of methods have been developed to approximate the action value function using parametric techniques, such as neural nets [3] and CMAC [14].
References-found: 14

