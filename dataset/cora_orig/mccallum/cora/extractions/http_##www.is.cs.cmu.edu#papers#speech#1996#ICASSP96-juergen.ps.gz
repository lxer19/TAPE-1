URL: http://www.is.cs.cmu.edu/papers/speech/1996/ICASSP96-juergen.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: fritsch,rogina@ira.uka.de  
Title: THE BUCKET BOX INTERSECTION (BBI) ALGORITHM FOR FAST APPROXIMATIVE EVALUATION OF DIAGONAL MIXTURE GAUSSIANS  
Author: J. Fritsch, I. Rogina 
Address: USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe Germany Carnegie Mellon University  
Abstract: density functions. Since it is very expensive to evaluate all the Gaussians of the mixture density codebook, many recognizers only compute the M most significant Gaussians (M = 1; : : : ; 8). This paper presents an alternative approach to approximate mixture Gaussians with diagonal covariance matrices, based on a binary feature space partitioning tree. The proposed algorithm is experimentally evaluated in the context of large vocabulary, speaker independent, spontaneous speech recognition using the JANUS-2 speech recognizer. In the case of mixtures with 50 Gaussians, we achieve a speedup of 2-5 in the computation of HMM emission probabilities, without affecting the accuracy of the system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Fritsch, J.; Rogina, I.; Sloboda, T.; Waibel, A.: </author> <title> Speed ing up the Score Computation of HMM Speech Recognizers with the Bucket Voronoi Intersection Algorithm, </title> <booktitle> To appear in Proceedings of the EUROSPEECH, </booktitle> <address> Madrid, Spain 1995. </address>
Reference-contexts: To reduce the computational load, some speech recognizers use Euclidean instead of Mahalanobis distances or restrict the compu <p>- tation of the emission probability to the one Gaussian with the highest value (M = 1). In <ref> [1] </ref> we presented an application of the BVI algorithm [2] that allows for fast nearest-neighbor search when seeking the top-1 Gaus- sian.
Reference: [2] <author> Ramasubramanian, V.; Paliwal, K. K.: </author> <title> Fast Kdimensional Tree Algorithms for Nearest Neighbor Search with Application to Vector Quantization Encoding, </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> Vol. 40, No. 3, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: To reduce the computational load, some speech recognizers use Euclidean instead of Mahalanobis distances or restrict the compu <p>- tation of the emission probability to the one Gaussian with the highest value (M = 1). In [1] we presented an application of the BVI algorithm <ref> [2] </ref> that allows for fast nearest-neighbor search when seeking the top-1 Gaus- sian. However, we found that restricting the probability computation to the evaluation of the Gaussian with the highest value will not be appropriate for most tasks and will result in a lower recognition accuracy. <p> Therefore, it becomes necessary to locally optimize the parameters of the nonterminal nodes independently, starting at the root node. For this purpose, we propose a modification of the Generalized Optimiza- tion Criterion (GOC). See <ref> [2] </ref> for a detailed description of this criterion. The non-terminal nodes in the tree are processed top-down, starting at the root node.
Reference: [3] <author> Bentley, J. L.: </author> <title> Multidimensional binary search trees used for associative searching, </title> <journal> Commun. Ass. Comput. Mach., </journal> <volume> vol. 18, no. 9, pp.509-517, </volume> <month> Sept. </month> <year> 1975. </year>
Reference-contexts: To find the Gaussian boxes containing a vector x we use a K-dimensional space partitioning tree (K-d tree) developed by Bentley <ref> [3] </ref>. A K-d tree is a generalization of the simple one <p>- dimensional binary tree. At every nonterminal node, the current region is divided into two half-spaces by means of a hyperplane orthogonal to one of the K coordinate axes.
Reference: [4] <author> M. Woszczyna, N. Aoki-Waibel, F.D. But, N. Coccaro, K. Horiguchi, T. Kemp, A. Lavie, A. McNair, T. Polzin, I. Rogina, C.P. Rose, T. Schultz, B. Suhm, M. Tomita, A. Waibel: </author> <title> JANUS 93: Towards Spontaneous Speech Translation, </title> <booktitle> Proceedings of the ICASSP 1994, Adelaide, </booktitle> <volume> volume 1, </volume> <pages> pp 345-348. </pages>
Reference-contexts: If so, we can replace a low contributional Gaussian by the missing one, which will decrease the average approximation error while leaving the speed up unchanged. 5. EXPERIMENTS We have conducted experiments with the JANUS- 2 speech recognizer <ref> [4] </ref> on the German Spontaneous Scheduling Task , using the proposed algorithm for the approximation of mixture Gaussian emission probabilities. In our experiments, we were using a continuous- density HMM system with 3300 mixtures and more than 1300 equally sized codebooks, containing 50 vectors with 16 LDA-transformed melscale coefficients.

References-found: 4

