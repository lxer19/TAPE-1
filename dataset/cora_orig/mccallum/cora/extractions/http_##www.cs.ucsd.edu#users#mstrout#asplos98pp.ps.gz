URL: http://www.cs.ucsd.edu/users/mstrout/asplos98pp.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/mstrout/paper.html
Root-URL: http://www.cs.ucsd.edu
Email: fmstrout,carter,ferrante,esimong@cs.ucsd.edu  
Title: Schedule-Independent Storage Mapping for Loops  
Author: Michelle Mills Strout, Larry Carter, Jeanne Ferrante, Beth Simon 
Address: 9500 Gilman Drive, La Jolla, CA 92093-0114  
Affiliation: CSE Department UC, San Diego  
Abstract: This paper studies the relationship between storage requirements and performance. Storage-related dependences inhibit optimizations for locality and parallelism. Techniques such as renaming and array expansion can eliminate all storage-related dependences, but do so at the expense of increased storage. This paper introduces the universal occupancy vector (UOV) for loops with a regular stencil of dependences. The UOV provides a schedule-independent storage reuse pattern that introduces no further dependences (other than those implied by true flow dependences). OV-mapped code requires less storage than full array expansion and only slightly more storage than schedule-dependent minimal storage. We show that determine if a vector is a UOV is NP-complete. However, an easily constructed but possibly non-minimal UOV can be used. We also present a branch and bound algorithm which finds the minimal UOV, while still maintaining a legal UOV at all times. Our experimental results show that the use of OV-mapped storage, coupled with tiling for locality, achieves better performance than tiling after array expansion, and accommodates larger problem sizes than untilable, storage-optimized code. Furthermore, storage mapping based on the UOV introduces negligible runtime overhead. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Alpern, L. Carter, and K.S. Gatlin. </author> <title> Microparallelism and high-performance protein matching. </title> <booktitle> In Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: There is storage for the strings themselves and a 23x23 table which holds comparison weights for the 23 possible string characters. The computation compares each character of one string with all of the characters in the other string. The storage optimized version was taken from <ref> [1] </ref>. See Table 2, for the storage requirements of the different versions.
Reference: [2] <author> Jennifer M. Anderson, Saman P. Amarasinghe, and Monica Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In SIGPLAN PLDI, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Note that since we are taking complete control of temporary storage allocation, it would not be difficult to incorporate data layout techniques such as array padding [3] to improve performance. In addition, the techniques of <ref> [2] </ref> may be of further use in improving our storage mapping. That work first parallelizes sequential programs to minimize communication, then changes the data layout (using strip-mining and permutation) to ensure contiguous storage and take advantage of locality on each processor.
Reference: [3] <author> David F. Bacon, Susan L. Graham, and Oliver J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <journal> Computing Surveys, </journal> <volume> 26(4) </volume> <pages> 345-420, </pages> <year> 1994. </year>
Reference-contexts: Suppose we had started with the code in example, natural subspace expansion would require 3mn storage locations. Not only would we be storing all temporary results, we would also be storing each result in three different locations. However, if we next perform forward substitution <ref> [3] </ref>, each value is stored in one location, and we obtain the original code in Figure 1 (a). Thus we see that using storage expansion techniques on storage-optimized code presents an opportunity to apply our techniques, by removing storage-related dependences. <p> Such data locality loop transformations use data dependence information to determine legality. The presence of storage-related data dependences restrict their applicability. Therefore, we use techniques to remove storage-related dependences <ref> [12, 3, 16, 5] </ref>. Later, when we map storage for reuse, storage-related dependences are reintroduced. <p> Note that since we are taking complete control of temporary storage allocation, it would not be difficult to incorporate data layout techniques such as array padding <ref> [3] </ref> to improve performance. In addition, the techniques of [2] may be of further use in improving our storage mapping. That work first parallelizes sequential programs to minimize communication, then changes the data layout (using strip-mining and permutation) to ensure contiguous storage and take advantage of locality on each processor. <p> In generating code, we remove the overhead introduced by the mod operations by applying loop unrolling <ref> [3] </ref>. 4.3 Storage Allocation We will compute the number of storage-equivalent classes as follows. We first compute a mapping vector ~mv, as described in Section 4.1, which ensures that the iteration points projected along ~ov map to integer points.
Reference: [4] <author> Denis Barthou, Albert Cohen, and Jean-Fran~cois Collard. </author> <title> Maximal static expansion. </title> <booktitle> In Principles of Programming Languages, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: Other related work is in storage expansion and optimiza tion. Array privatization [24, 20] creates separate, per processor storage in a parallel context. In our work, storage expansion is performed on a per iteration basis for better locality. The work in <ref> [4] </ref> creates a maximal expansion of storage that does not require -functions, trading off parallelism for memory usage. The goal of our work is data locality. The work in [17] introduces an Array SSA form that keeps track of values on an element-wise basis.
Reference: [5] <author> Pierre-Yves Calland, Alain Darte, Yves Robert, and Frederic Vivien. </author> <title> Plugging anti and output dependence removal techniques into loop parallelization algorithms. </title> <booktitle> Parallel Computing, </booktitle> <address> 23(1-2):251-266, </address> <year> 1997. </year>
Reference-contexts: Such data locality loop transformations use data dependence information to determine legality. The presence of storage-related data dependences restrict their applicability. Therefore, we use techniques to remove storage-related dependences <ref> [12, 3, 16, 5] </ref>. Later, when we map storage for reuse, storage-related dependences are reintroduced.
Reference: [6] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 252-262, </pages> <address> San Jose, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The array region analysis of [11] allows us to determine which elements in an array are being imported to the loop and which elements are being exported. Using this information we can determine what storage is used for input and output versus temporary results. Data locality loop transformations <ref> [25, 6] </ref> such as tiling change the schedule of a loop to improve the data locality within the loop which in turn can increase performance. Such data locality loop transformations use data dependence information to determine legality. The presence of storage-related data dependences restrict their applicability.
Reference: [7] <author> Larry Carter, Jeanne Ferrante, and S. Flynn Hummel. </author> <title> Efficient parallelism via hierarchical tiling. </title> <booktitle> In Proceedings of SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: In this paper we gave evidence showing one level of tiling along with use of the occupancy vector improves performance. We plan to study which characteristics of the entire memory hierarchy should be taken into account when doing multiple-level optimizations like hierarchical tiling <ref> [7, 8] </ref>. 8 Acknowledgement We would like to thank Ken Lyons at AT&T Labs and Nick Mitchell and Kang Su Gatlin of UCSD for their helpful comments and suggestions. This work is partly supported by NSF grant CCR-9504150, a NSF graduate research fellowship, and an AT&T Labs student grant.
Reference: [8] <author> Larry Carter, Jeanne Ferrante, Susan Flynn Hummel, Bowen Alpern, and Kang Su Gatlin. </author> <title> Hierarchical tiling: A methodology for high performance. </title> <type> Technical Report CS96-508, UCSD, </type> <institution> Department of Computer Science and Engineering, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: In this paper we gave evidence showing one level of tiling along with use of the occupancy vector improves performance. We plan to study which characteristics of the entire memory hierarchy should be taken into account when doing multiple-level optimizations like hierarchical tiling <ref> [7, 8] </ref>. 8 Acknowledgement We would like to thank Ken Lyons at AT&T Labs and Nick Mitchell and Kang Su Gatlin of UCSD for their helpful comments and suggestions. This work is partly supported by NSF grant CCR-9504150, a NSF graduate research fellowship, and an AT&T Labs student grant.
Reference: [9] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> In SIGPLAN PLDI, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year> <month> 9 </month>
Reference-contexts: The goal of our work is data locality. The work in [17] introduces an Array SSA form that keeps track of values on an element-wise basis. Both of these latter works may be useful as we extend our work to more general program structures. In <ref> [9, 10] </ref>, a unified framework and heuristics that consider the combined effect of array layout and loop transformations (described by integer, non-singular matrices [19]) on locality are developed.
Reference: [10] <author> Michal Cierniak. </author> <title> Optimizing Programs by Data and Control Transformations. </title> <type> Ph.d. thesis, </type> <institution> Rochester, Computer Science Department TR670, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: The goal of our work is data locality. The work in [17] introduces an Array SSA form that keeps track of values on an element-wise basis. Both of these latter works may be useful as we extend our work to more general program structures. In <ref> [9, 10] </ref>, a unified framework and heuristics that consider the combined effect of array layout and loop transformations (described by integer, non-singular matrices [19]) on locality are developed.
Reference: [11] <author> Beatrice Creusillet and Fran~cois Irigoin. </author> <title> Interproce-dural array region analyses. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 24(6) </volume> <pages> 513-546, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The method is applicable to regular loops (perfectly nested loops with a regular stencil of data dependences) which produce temporary values that are not used outside the loop. We can determine whether our assumptions are valid for a given loop nest by applying array region analysis <ref> [11] </ref> and value-based Copyright (c) 1998 by the Association for Computing Machinery, Inc. <p> The work in [21] uses the Omega system to obtain value-based dependence analysis, again with greater efficiency than [13]. The array region analysis of <ref> [11] </ref> allows us to determine which elements in an array are being imported to the loop and which elements are being exported. Using this information we can determine what storage is used for input and output versus temporary results.
Reference: [12] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 429-442, </pages> <year> 1988. </year>
Reference-contexts: Such data locality loop transformations use data dependence information to determine legality. The presence of storage-related data dependences restrict their applicability. Therefore, we use techniques to remove storage-related dependences <ref> [12, 3, 16, 5] </ref>. Later, when we map storage for reuse, storage-related dependences are reintroduced.
Reference: [13] <author> Paul Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1), </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: Also, this technique is only applicable in loops which generate temporary values. Value-based dependence analysis is fundamental to our work, as it allows us to summarize which iteration produced the values used by each iteration in the ISG. Precise value-based dependence analysis was first developed in <ref> [13] </ref> over a restricted domain of structured programs, and extended in [20] to obtain the same precision in common cases with greater efficiency. <p> The work in [21] uses the Omega system to obtain value-based dependence analysis, again with greater efficiency than <ref> [13] </ref>. The array region analysis of [11] allows us to determine which elements in an array are being imported to the loop and which elements are being exported. Using this information we can determine what storage is used for input and output versus temporary results.
Reference: [14] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A guide to the theory of NP-Completeness. </title> <institution> Bell Telephone Laboratories, Incorporated, </institution> <year> 1979. </year>
Reference-contexts: In particular: Theorem Given a set V of dependences and an arbitrary vector ~w, determining whether ~w is in U OV (V ) is NP-complete. Proof: The proof is via a reduction of the partition problem <ref> [14] </ref>. Suppose we are given an instance of the partition problem, expressed as a sequence a 0 ; a 1 ; :::a n1 of positive integers. 2 Let h = P a i =2. The partition problem is to determine if there is a subsequence that has sum h. <p> We construct an instance of the UOV membership problem. The stencil V will be a set of two-dimensional vectors. For each 2 We use sequences instead of sets to allow duplicate values, con forming with <ref> [14] </ref>. 3 a i , V will include the vectors ~r i = (0; (n + 1) i + (n + 1) n ) and ~s i = (a i ; (n + 1) i + (n + 1) n ).
Reference: [15] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the 15th Annual ACM SIGPLAN Symposium on Priniciples of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <year> 1988. </year>
Reference-contexts: For simplicity the figure shows only some of the storage-related dependences 1 . Actually because of the assignment to temp2 there are storage-related dependences between all iterations. Because of these storage-related dependences, the schedule of the loop is severely restricted. Consequently, data locality optimizations like tiling <ref> [15, 26] </ref> and loop interchange [27] are not possible without introducing more storage, because they alter the loop schedule. These optimizations are important for achieving good performance [27]. In this paper, we focus on the applicability of tiling, and we give performance results using our techniques in Section 5. <p> In this paper we focus on the use of the UOV in tandem with tiling. Tiling <ref> [15, 26] </ref> is an optimization that partitions the ISG into atomic units of execution called tiles. Tiling changes the execution order of a loop to take better advantage of data locality. It can also be used as a technique to implement parallelism in a loop nest.
Reference: [16] <author> Kathleen Knobe and William J. Dally. </author> <title> The subspace model: A theory of shapes for parallel systems. </title> <booktitle> In 5th Workshop on Compilers for Parallel Computers, Malaga, </booktitle> <address> Spain, </address> <year> 1995. </year>
Reference-contexts: Such data locality loop transformations use data dependence information to determine legality. The presence of storage-related data dependences restrict their applicability. Therefore, we use techniques to remove storage-related dependences <ref> [12, 3, 16, 5] </ref>. Later, when we map storage for reuse, storage-related dependences are reintroduced.
Reference: [17] <author> Kathleen Knobe and Vivek Sarkar. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Principles of Programming Languages, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: In our work, storage expansion is performed on a per iteration basis for better locality. The work in [4] creates a maximal expansion of storage that does not require -functions, trading off parallelism for memory usage. The goal of our work is data locality. The work in <ref> [17] </ref> introduces an Array SSA form that keeps track of values on an element-wise basis. Both of these latter works may be useful as we extend our work to more general program structures.
Reference: [18] <author> V. Lefebvre and Paul Feautrier. </author> <title> Automatic storage management for parallel programs. </title> <journal> Journal on Parallel Computing, </journal> <note> 1997. To be published. </note>
Reference-contexts: From these results we conclude that in codes where the memory latency is the bottleneck, OV-mapped codes help performance by allowing tiling while at the same time keeping the storage minimal. 6 Related Work The most closely related work to ours is <ref> [18] </ref>, which also determines storage reuse for a loop. Their work takes as input a given parallel schedule, but allows direction vectors. In contrast, our UOV-based approach can be used for any legal schedule in the context of a loop with constant distance vectors.
Reference: [19] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <journal> International Journal on Parallel Processing, </journal> <volume> 22(2), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Both of these latter works may be useful as we extend our work to more general program structures. In [9, 10], a unified framework and heuristics that consider the combined effect of array layout and loop transformations (described by integer, non-singular matrices <ref> [19] </ref>) on locality are developed. More specifically, they seek a legal data and control transformation that will result in a stride vector with larger strides at outer loops, and smallest strides 8 at inner loops.
Reference: [20] <author> Dror E. Maydan, Saman P. Amarasinghe, and Mon-ica S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Value-based dependence analysis is fundamental to our work, as it allows us to summarize which iteration produced the values used by each iteration in the ISG. Precise value-based dependence analysis was first developed in [13] over a restricted domain of structured programs, and extended in <ref> [20] </ref> to obtain the same precision in common cases with greater efficiency. This work uses Last Write Trees to represent a mapping from a node in the ISG where a read occurs to another node where the last instance of the value used by the read is written. <p> In contrast, our UOV-based approach can be used for any legal schedule in the context of a loop with constant distance vectors. This allows the flexibility of performing tiling after storage mapping. Other related work is in storage expansion and optimiza tion. Array privatization <ref> [24, 20] </ref> creates separate, per processor storage in a parallel context. In our work, storage expansion is performed on a per iteration basis for better locality. The work in [4] creates a maximal expansion of storage that does not require -functions, trading off parallelism for memory usage.
Reference: [21] <author> William Pugh and David Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Programming Languagesand Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: This work uses Last Write Trees to represent a mapping from a node in the ISG where a read occurs to another node where the last instance of the value used by the read is written. The work in <ref> [21] </ref> uses the Omega system to obtain value-based dependence analysis, again with greater efficiency than [13]. The array region analysis of [11] allows us to determine which elements in an array are being imported to the loop and which elements are being exported.
Reference: [22] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Proceedings Supercomputing 91, </booktitle> <pages> pages 111-120, </pages> <year> 1991. </year>
Reference: [23] <author> Daniel A. Reed and Loyce M. Adams an Merrell L. Patrick. </author> <title> Stencils and problem partitionings: Their influence on the performance of multiple processor systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(7):845-858, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: We assume that each ISG node (except for the input and output nodes) has the same pattern of dependences, which is called a stencil <ref> [23] </ref>. Also, this technique is only applicable in loops which generate temporary values. Value-based dependence analysis is fundamental to our work, as it allows us to summarize which iteration produced the values used by each iteration in the ISG.
Reference: [24] <author> Peng Tu and David Padua. </author> <title> Automatic array privatiza-tion. </title> <booktitle> In Languages and Compilers for Parallel Computing 6th International Workshop, </booktitle> <pages> pages 500-521, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In contrast, our UOV-based approach can be used for any legal schedule in the context of a loop with constant distance vectors. This allows the flexibility of performing tiling after storage mapping. Other related work is in storage expansion and optimiza tion. Array privatization <ref> [24, 20] </ref> creates separate, per processor storage in a parallel context. In our work, storage expansion is performed on a per iteration basis for better locality. The work in [4] creates a maximal expansion of storage that does not require -functions, trading off parallelism for memory usage.
Reference: [25] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: The array region analysis of [11] allows us to determine which elements in an array are being imported to the loop and which elements are being exported. Using this information we can determine what storage is used for input and output versus temporary results. Data locality loop transformations <ref> [25, 6] </ref> such as tiling change the schedule of a loop to improve the data locality within the loop which in turn can increase performance. Such data locality loop transformations use data dependence information to determine legality. The presence of storage-related data dependences restrict their applicability.
Reference: [26] <author> Michael J. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Parallel Processing for Scientific Computing, </booktitle> <pages> pages 357-361, </pages> <year> 1987. </year>
Reference-contexts: For simplicity the figure shows only some of the storage-related dependences 1 . Actually because of the assignment to temp2 there are storage-related dependences between all iterations. Because of these storage-related dependences, the schedule of the loop is severely restricted. Consequently, data locality optimizations like tiling <ref> [15, 26] </ref> and loop interchange [27] are not possible without introducing more storage, because they alter the loop schedule. These optimizations are important for achieving good performance [27]. In this paper, we focus on the applicability of tiling, and we give performance results using our techniques in Section 5. <p> In this paper we focus on the use of the UOV in tandem with tiling. Tiling <ref> [15, 26] </ref> is an optimization that partitions the ISG into atomic units of execution called tiles. Tiling changes the execution order of a loop to take better advantage of data locality. It can also be used as a technique to implement parallelism in a loop nest.
Reference: [27] <author> Michael J. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year> <month> 10 </month>
Reference-contexts: A's two-dimensional storage of temporary values requires nm storage locations. Our aim is to reduce the amount of storage used, but in a way that does not inhibit other optimizations. We can graphically represent our simple example with an iteration space graph (ISG) <ref> [27] </ref>. A loop nest of depth k is represented by a k-dimensional ISG, where the indices in the jth dimension are the values taken on by the jth innermost index variable. <p> Actually because of the assignment to temp2 there are storage-related dependences between all iterations. Because of these storage-related dependences, the schedule of the loop is severely restricted. Consequently, data locality optimizations like tiling [15, 26] and loop interchange <ref> [27] </ref> are not possible without introducing more storage, because they alter the loop schedule. These optimizations are important for achieving good performance [27]. In this paper, we focus on the applicability of tiling, and we give performance results using our techniques in Section 5. <p> Because of these storage-related dependences, the schedule of the loop is severely restricted. Consequently, data locality optimizations like tiling [15, 26] and loop interchange <ref> [27] </ref> are not possible without introducing more storage, because they alter the loop schedule. These optimizations are important for achieving good performance [27]. In this paper, we focus on the applicability of tiling, and we give performance results using our techniques in Section 5. So far we've illustrated how using the UOV reduces storage compared to the use of two-dimensional arrays, and 1 also known as anti- and output dependences [27] 1 allows <p> good performance <ref> [27] </ref>. In this paper, we focus on the applicability of tiling, and we give performance results using our techniques in Section 5. So far we've illustrated how using the UOV reduces storage compared to the use of two-dimensional arrays, and 1 also known as anti- and output dependences [27] 1 allows more flexibility for optimization than the storage--optimized version. There is a second scenario where the UOV can help. Suppose we had started with the code in example, natural subspace expansion would require 3mn storage locations.
References-found: 27

