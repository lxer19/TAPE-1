URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/Papers/Multi_Version.ps
Refering-URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/vouk.se.html
Root-URL: http://www.csc.ncsu.edu
Title: Analysis of Faults Detected in a Large-Scale Multi-Version Software Development Experiment  
Author: Mladen A. Vouk, Alper K. Caglayan David E. Eckhardt David F. McAllister James L. Walker, Jr. John J.P. Kelly John Knight 
Address: Charles River Analytics, Inc.  
Affiliation: NASA Langley Research Center North Carolina State University  University of California, Santa Barbara University of Virginia  
Abstract: Twenty programs were built to the same specification of an inertial navigation problem. The programs were then subjected to a three phase testing and debugging process: an acceptance test, a certification test, and an operational test. Less than 20% of the faults discovered during the certification and operational testing were nonunique, i.e. the same or very similar faults would be found in more than one program. However, some of these "common" faults spanned as many as half of the versions. Faults discovered during the certification testing were due to specification errors and ambiguities, inadequate programmer background knowledge, insufficient programming experience, incomplete analysis, and insufficient acceptance testing. Faults discovered during the operational testing were of a more subtle nature, and were mostly due to various programmer knowledge defects and incomplete analysis errors. Techniques that may be used to avoid the observed fault types are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Avizienis and L. Chen, </author> <title> "On the Implementation of N-version Programming for Software Fault-Tolerance During Program Execution", </title> <booktitle> Proc. COMPSAC 77, </booktitle> <pages> 149-155, </pages> <year> 1977. </year>
Reference: [2] <author> B. Randell, </author> <title> "System structure for software fault-tolerance", </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> Vol. SE-1, </volume> <pages> 220-232, </pages> <year> 1975. </year>
Reference-contexts: 1. Introduction Most common fault-tolerant software mechanisms are based on redundancy <ref> [e.g. 1, 2] </ref>. For acceptable results these techniques require that the failures of redundant software versions are mutually independent, or at least that the positive inter-version failure correlation is low [3].
Reference: [3] <author> D.E. Eckhardt, Jr. and L.D. Lee, </author> <title> "A Theoretical Basis for the Analysis of Multiversion Software Subject to Coincident Errors", </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> Vol. SE-11(12), </volume> <pages> 1511-1517, </pages> <year> 1985. </year>
Reference-contexts: 1. Introduction Most common fault-tolerant software mechanisms are based on redundancy [e.g. 1, 2]. For acceptable results these techniques require that the failures of redundant software versions are mutually independent, or at least that the positive inter-version failure correlation is low <ref> [3] </ref>. Experiments have shown that failure dependence among independently built programs intended to be functionally equivalent may not be negligible in the context of the current software development and testing techniques [e.g. 4, 5, 6, 7].
Reference: [4] <author> R.K. Scott, J.W. Gault, D.F. McAllister and J. Wiggs, </author> <title> "Investigating Version Dependence in Fault-Tolerant Software", </title> <booktitle> AGARD 361, </booktitle> <pages> pp. </pages> <address> 21.1-21.10, </address> <year> 1984. </year>
Reference: [5] <author> P.G. Bishop, D.G. Esp, M. Barnes, P Humphreys, G. Dahl, and J. Lahti, </author> <title> "PODS-A Project on Diverse Software", </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> Vol. SE-12(9), </volume> <pages> 929-940, </pages> <year> 1986. </year>
Reference-contexts: Experiments have shown that failure dependence among independently built programs intended to be functionally equivalent may not be negligible in the context of the current software development and testing techniques <ref> [e.g. 4, 5, 6, 7] </ref>. Therefore, it is important to understand the nature of the faults that may cause dependent failures in such programs. * This research was supported in part by NASA grants NAG-1-511, NAG-1-667,NAG-1-744, and NAG-1-782, and contracts NAS-1-17705 and NAS-1-17964.
Reference: [6] <author> J.C. Knight and N.G. Leveson, </author> <title> "An Experimental Evaluation of the assumption of Independence in Multiversion Programming", </title> <journal> IEEE Trans. Soft. Eng., </journal> <volume> Vol. SE-12(1), </volume> <pages> 96-109, </pages> <year> 1986. </year>
Reference-contexts: Experiments have shown that failure dependence among independently built programs intended to be functionally equivalent may not be negligible in the context of the current software development and testing techniques <ref> [e.g. 4, 5, 6, 7] </ref>. Therefore, it is important to understand the nature of the faults that may cause dependent failures in such programs. * This research was supported in part by NASA grants NAG-1-511, NAG-1-667,NAG-1-744, and NAG-1-782, and contracts NAS-1-17705 and NAS-1-17964.
Reference: [7] <author> J. Kelly, D. Eckhardt, A. Caglayan, J. Knight, D. McAllister, M. Vouk, </author> <title> "A Large Scale Second Generation Experiment in Multi-Version Software: Description and Early Results", </title> <booktitle> Proc. </booktitle> <volume> FTCS 18, </volume> <pages> pp 9-14, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Experiments have shown that failure dependence among independently built programs intended to be functionally equivalent may not be negligible in the context of the current software development and testing techniques <ref> [e.g. 4, 5, 6, 7] </ref>. Therefore, it is important to understand the nature of the faults that may cause dependent failures in such programs. * This research was supported in part by NASA grants NAG-1-511, NAG-1-667,NAG-1-744, and NAG-1-782, and contracts NAS-1-17705 and NAS-1-17964. <p> Participants from industry were: Research Triangle Institute (RTI), Charles River Analytics (CRA). Phase 1 of the experiment was coordinated by RTI with assistance from CRA. Phases 2 and 3 of the experiment was coordinated by CRA. about the experiment can be found in <ref> [7, 8] </ref>. At the end of the original development phase all programs were acceptance tested with a set of 75 test cases. <p> In Figure 2 we illustrate this point by showing the fraction of failures that remain undetected over 500 uniform random test cases given that 2, 3, 4, etc. versions which have gone through acceptance testing are selected at random and tested back-to-back <ref> [7] </ref> In general, for most of the observed faults suitable test cases can be developed which cause them to manifest as failures in the program outputs, although it is not clear such a test case would have systematically been generated through a well defined test strategy.
Reference: [8] <author> D.E. Eckhardt, A.K. Caglayan, J.C. Knight, L.D. Lee, D.F. McAllister, and M.A. Vouk, </author> <title> "An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability," </title> <note> submitted for publication, </note> <year> 1990. </year>
Reference-contexts: Participants from industry were: Research Triangle Institute (RTI), Charles River Analytics (CRA). Phase 1 of the experiment was coordinated by RTI with assistance from CRA. Phases 2 and 3 of the experiment was coordinated by CRA. about the experiment can be found in <ref> [7, 8] </ref>. At the end of the original development phase all programs were acceptance tested with a set of 75 test cases. <p> The fault causing a failure was found through isolated execution of the version code with the input set that resulted in the failure, and dynamic (using a debugger) and static inspections of the code. Additional information about the operational testing process and results can be found in <ref> [8, 9] </ref>. 3. Faults 3.1 Fault Classification We divide the faults into two principal classes: the specification related faults (S), and the design and implementation related faults (D): Specification related faults are those that can be directly traced to problems (errors) within the original, English language, requirements specification document.
Reference: [9] <author> P.R. Lorczak and A.K. Caglayan, </author> <title> "A Large-Scale Second Generation Experiment in Multi-Version Software: Analysis of Software and Specification Faults", </title> <address> Charles River Analytics Inc., </address> <note> Report R8903 under NASA Contract NAS-1-17705, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: The cumulative execution coverage of the code by the test cases was determined through code instrumentation and execution tracing. A more detailed description of the certification testing effort can be found in <ref> [9, 10] </ref>. In the operational testing phase the programs were subjected to an extensive test, totalling over 900,000 input cases, in order to determine software reliability. <p> The fault causing a failure was found through isolated execution of the version code with the input set that resulted in the failure, and dynamic (using a debugger) and static inspections of the code. Additional information about the operational testing process and results can be found in <ref> [8, 9] </ref>. 3. Faults 3.1 Fault Classification We divide the faults into two principal classes: the specification related faults (S), and the design and implementation related faults (D): Specification related faults are those that can be directly traced to problems (errors) within the original, English language, requirements specification document.
Reference: [10] <author> M.A. Vouk, D.F. McAllister, A.M. Paradkar and S.R. Vemulakonda, </author> <title> "Experiments in Fault-Tolerant Software Reliability", </title> <note> Report #5 on NAG-1-667, NCSU, </note> <month> April </month> <year> 1989. </year>
Reference-contexts: The cumulative execution coverage of the code by the test cases was determined through code instrumentation and execution tracing. A more detailed description of the certification testing effort can be found in <ref> [9, 10] </ref>. In the operational testing phase the programs were subjected to an extensive test, totalling over 900,000 input cases, in order to determine software reliability.
References-found: 10

