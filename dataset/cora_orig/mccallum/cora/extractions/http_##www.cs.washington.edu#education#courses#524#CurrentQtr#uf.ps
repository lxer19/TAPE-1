URL: http://www.cs.washington.edu/education/courses/524/CurrentQtr/uf.ps
Refering-URL: http://www.cs.washington.edu/education/courses/524/CurrentQtr/
Root-URL: 
Email: Email address anderson@cs.washington.edu.  Email address woll@cs.ucsd.edu.  
Title: Wait-free Parallel Algorithms for the Union-Find Problem  
Author: Richard J. Anderson Heather Woll 
Note: Supported by an NSF Presidential Young Investigator Award CCR-8657562, Digital Equipment Corporation, NSF CER grant CCR-861966, and NSF/Darpa grant CCR-8907960.  Partially supported by an NSF Research Initiation Award CCR-9009657 and a UCSD Faculty Career Development grant.  
Date: November 1, 1994  
Address: San Diego  
Affiliation: University of Washington  University of California,  
Abstract: We are interested in designing efficient data structures for a shared memory multiprocessor. In this paper we focus on the Union-Find data structure. We consider a fully asynchronous model of computation where arbitrary delays are possible. Thus we require our solutions to the data structure problem have the wait-free property, meaning that each thread continues to make progress on its operations, independent of the speeds of the other threads. In this model efficiency is best measured in terms of the total number of instructions used to perform a sequence of data structure operations, the work performed by the processors. We give a wait-free implementation of an efficient algorithm for Union-Find. In addition we show that the worst case performance of the algorithm can be improved by simulating a synchronized algorithm, or by simulating a larger machine if the data structure requests support sufficient parallelism. Our solutions apply to a much more general adversary model than has been considered by other authors. A preliminary version of this paper was presented at the 23rd STOC, 1991, [AW91]. 
Abstract-found: 1
Intro-found: 1
Reference: [AH90] <author> J. Aspnes and M. Herlihy. </author> <title> Wait-free data structures in the asynchronous PRAM model. </title> <booktitle> In Second Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 340-349, </pages> <year> 1990. </year>
Reference-contexts: Cole and Zajicek do not specify the atomic primitives for the A-PRAM although they implicitly assume that an atomic assignment can be done to several variables simultaneously. Aspnes and Herlihy <ref> [AH90] </ref> consider an A-PRAM that does not support universal primitives. They study the limitations of such machines and classify the computations that can be performed. The works that present a model that is closest to ours are a series of papers considering fault tolerant P-RAMs [KS89, KPS90, MSP90].
Reference: [And90] <author> R. J. Anderson. </author> <title> Parallel algorithms for generating random permutations on a shared memory machine. </title> <booktitle> In Second Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 95-102, </pages> <year> 1990. </year>
Reference-contexts: The adversary both chooses the queries and establishes the interleaving order. If our algorithm is randomized, then the adversary also may look at the results of the random coin before deciding which thread has the next instruction in the interleaving. This adversary model has been used in <ref> [And90] </ref> and [BR90]. The strength of the adversary plays an important role in the results that one can establish. The adversary used by Martel et al. [MSP90] is substantially weaker than the adversary used here.
Reference: [And91] <author> R. J. Anderson. </author> <title> Wait-free primitives for list compression. </title> <booktitle> Work in progress, </booktitle> <year> 1991. </year>
Reference-contexts: Proof: The dominant term is still the cost of traversing long chains. However, if we count the compress failures separately, we can treat the updating of pointers during Finds as atomic operations that splice elements out of a list. The result follows from a bound given in <ref> [And91] </ref>.. 4.4 Adversary strength In our model, we consider a very powerful adversary. The adversary specifies the queries and decides upon the interleaving of instructions.
Reference: [AW91] <author> R. J. Anderson and H. S. Woll. </author> <title> Wait-free parallel algorithms for the union-find problem. </title> <booktitle> In Proceedings of the 23rd ACM Symposium on Theory of Computation, </booktitle> <pages> pages 370-380, </pages> <year> 1991. </year>
Reference: [BR90] <author> J. Buss and P. Ragde. </author> <title> Certified write-all on a strongly asynchronous PRAM. </title> <note> Preliminary Report, </note> <year> 1990. </year>
Reference-contexts: The performance of the algorithm is close to the performance of the sequential Union-Find, except for certain pathological cases. We study two methods for improving the worst case performance. One method involves simulating a sequential machine. Our results include an improved result for the Write-All problem <ref> [BR90, KS89] </ref> with respect to a general adversary. We also introduce a coarse grained technique for improving the worst case performance of the algorithm that does not involve synchronization. <p> The adversary both chooses the queries and establishes the interleaving order. If our algorithm is randomized, then the adversary also may look at the results of the random coin before deciding which thread has the next instruction in the interleaving. This adversary model has been used in [And90] and <ref> [BR90] </ref>. The strength of the adversary plays an important role in the results that one can establish. The adversary used by Martel et al. [MSP90] is substantially weaker than the adversary used here. <p> Our result differs from Martel's, in that our algorithm is applicable against a much stronger adversary. The write-all problem for the general (adaptive) adversary model has been previously considered by Buss and Ragde <ref> [BR90] </ref>. They give an algorithm with O (p log 2 3 ) work (log 2 3 1:79). Our algorithm includes their result as a special case. They also show an (p log p) work lowerbound for the write-all problem.
Reference: [CV86] <author> R. Cole and U. Vishkin. </author> <title> Deterministic coin tossing with applications to optimal parallel list ranking. </title> <journal> Information and Computation, </journal> <volume> 70 </volume> <pages> 32-53, </pages> <year> 1986. </year>
Reference-contexts: The source of inefficiency in our asynchronous algorithm arises from long chains being created. In a synchronized algorithm we can avoid long chains by breaking them up as they are formed. We do this by applying the Cole-Vishkin algorithm 19 for finding an independent set in a linked list <ref> [CV86] </ref>. This causes us to spend O (log fl p) extra time for each linking step. This gives us the following lemma: Lemma 5.10 A synchronous algorithm can answer n Union-Find queries with O (n (ff (n) + log fl p) work.
Reference: [Gib89] <author> P. Gibbons. </author> <title> A more practical PRAM model. </title> <booktitle> In 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <year> 1989. </year>
Reference-contexts: Herlihy [Her90] studied the implementation of a number of data structures in terms of compare&swap. He showed how to implement functional data structures, and discussed wait-free memory management. 2.2 Asynchronous P-RAM models Researchers have recently begun to consider asynchronous versions of the P-RAM <ref> [Gib89, RZ89, RZ90, Nis90, MSP90] </ref>. The papers have introduced several different models, with differing notions of run time.
Reference: [Her88] <author> M. Herlihy. </author> <title> Impossibility and universality results for wait-free synchronization. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 276-291, </pages> <year> 1988. </year>
Reference-contexts: The main thrust of research on wait-free objects has been to relate the powers of various primitives. It has been shown that there are some primitives, referred to as being universal, which are sufficiently powerful that they may be used to implement any wait-free data structure <ref> [Her88] </ref>. We include a universal primitive in our model. In this paper, we are interested in measuring the performance of data structure operations as well as guaranteeing that they have wait-free implementations. We consider the cost of executing a series of data structure requests. <p> The emphasis of this work has primarily been on simple objects such as atomic registers and test&set. A series of papers has established relationships between the primitives, either showing that one primitive can simulate another, or that such a simulation is impossible. Herlihy <ref> [Her88] </ref> unified much of this work by relating primitives to a family of consensus problems and establishing the existence of universal primitives. A primitive is universal if any wait-free object can be constructed from it.
Reference: [Her90] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <year> 1990. </year>
Reference-contexts: Some common data structures, such as stacks and queues fall in intermediate levels. However, most of the complicated data structures used in programming are in the top level. The data structures at the top level have received relatively little attention. Herlihy <ref> [Her90] </ref> studied the implementation of a number of data structures in terms of compare&swap. He showed how to implement functional data structures, and discussed wait-free memory management. 2.2 Asynchronous P-RAM models Researchers have recently begun to consider asynchronous versions of the P-RAM [Gib89, RZ89, RZ90, Nis90, MSP90]. <p> Our choice is compare&swap, which is an assignment that succeeds only if we know the current value of the variable. The compare&swap primitive was used by Herlihy in his work on wait-free data structures, and is included in the instruction set of the IBM 370 <ref> [Her90] </ref>. The following code fragment defines compare&swap. Our main use of this primitive is to ensure that a variable is not overwritten by another thread between reading and updating the variable.
Reference: [HW87] <author> M. P. Herlihy and J. M. Wing. </author> <title> Axioms for concurrent objects. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 13-26, </pages> <year> 1987. </year>
Reference-contexts: In order for an algorithm to be correct it must behave properly for all interleavings <ref> [HW87, Lam79] </ref>. Our definition of what it means to correctly answer the data structure operations is also done in terms of serializability. The answers to the queries are said to be correct if the answers agree with some serialization of atomic queries.
Reference: [Knu68] <author> D. E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1968. </year>
Reference-contexts: The number of left-to-right maxima in a permutation on f1; : : : ; pg is a random variable with mean H p and standard deviation approximately q H p <ref> [Knu68] </ref>. Since the variables are independent, a natural proof would be to apply the central limit theorem. This approach runs into difficulties because the random variables change for each p.
Reference: [KPS90] <author> Z. M. Kedem, K. V. Palem, and P. G. Spirakis. </author> <title> Efficient robust parallel computations. </title> <booktitle> In Proceedings of the 22nd ACM Symposium on Theory of Computation, </booktitle> <pages> pages 138-148, </pages> <year> 1990. </year>
Reference-contexts: Aspnes and Herlihy [AH90] consider an A-PRAM that does not support universal primitives. They study the limitations of such machines and classify the computations that can be performed. The works that present a model that is closest to ours are a series of papers considering fault tolerant P-RAMs <ref> [KS89, KPS90, MSP90] </ref>. These papers consider a model where processors may fail, and the goal is to simulate the computation using the surviving processors. They measure performance in terms of work, the sum of the active times of the processors, the same measure as used in this paper. <p> These papers all look at the problem of how to simulate a single step of an n processor algorithm on a fail-stop or asynchronous machine. Kanellakis and Shvartsman [KS89] introduced the Fail-Stop P-RAM model and the write-all problem. Kedem et al. <ref> [KPS90] </ref> gave a very general simulation result based upon a robust implementation of a variant of the write-all problem. Martel et al. [MSP90] extended the result to an asynchronous model by giving an asynchronous algorithm for the write-all problem. <p> Each round simulates exactly one instruction of each of the p synchronous thread. This type of simulation is discussed for other asynchronous models in several papers <ref> [KS89, KPS90, MSP90] </ref>. It is not difficult to design a synchronized Union-Find algorithm that is almost as efficient as the sequential algorithm. The source of inefficiency in our asynchronous algorithm arises from long chains being created.
Reference: [KR78] <author> B. W. Kernighan and D. M. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice-Hall, </publisher> <year> 1978. </year>
Reference-contexts: We use a notation derived from the C language to express our manipulation of pointers and arrays <ref> [KR78] </ref>. The standard sequential data structure for Union-Find is a collection of records that point to each other to form a forest.
Reference: [KS89] <author> P. Kanellakis and A. Shvartsman. </author> <title> Efficient parallel algorithms can be made robust. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 211-222, </pages> <year> 1989. </year>
Reference-contexts: The performance of the algorithm is close to the performance of the sequential Union-Find, except for certain pathological cases. We study two methods for improving the worst case performance. One method involves simulating a sequential machine. Our results include an improved result for the Write-All problem <ref> [BR90, KS89] </ref> with respect to a general adversary. We also introduce a coarse grained technique for improving the worst case performance of the algorithm that does not involve synchronization. <p> Aspnes and Herlihy [AH90] consider an A-PRAM that does not support universal primitives. They study the limitations of such machines and classify the computations that can be performed. The works that present a model that is closest to ours are a series of papers considering fault tolerant P-RAMs <ref> [KS89, KPS90, MSP90] </ref>. These papers consider a model where processors may fail, and the goal is to simulate the computation using the surviving processors. They measure performance in terms of work, the sum of the active times of the processors, the same measure as used in this paper. <p> These papers all look at the problem of how to simulate a single step of an n processor algorithm on a fail-stop or asynchronous machine. Kanellakis and Shvartsman <ref> [KS89] </ref> introduced the Fail-Stop P-RAM model and the write-all problem. Kedem et al. [KPS90] gave a very general simulation result based upon a robust implementation of a variant of the write-all problem. <p> Each round simulates exactly one instruction of each of the p synchronous thread. This type of simulation is discussed for other asynchronous models in several papers <ref> [KS89, KPS90, MSP90] </ref>. It is not difficult to design a synchronized Union-Find algorithm that is almost as efficient as the sequential algorithm. The source of inefficiency in our asynchronous algorithm arises from long chains being created. <p> In order to simulate a synchronous algorithm, we have each thread execute the instructions of each processor. The abstraction that is used to capture this is a problem introduced by Kanellakis and Shvartsman <ref> [KS89] </ref> called the write-all problem. The write-all problem is to write a fixed value into every cell of an array of size p. When a thread writes a value into location j it executes the instruction associated with processor j in the current round. <p> Since the write-all problem is central to the simulation algorithms, it has been considered previously in different models. Under the fail-stop model, Kanellakis and Shvartsman <ref> [KS89] </ref> gave an O (p log 2 p) work upperbound for an p-processor, p-cell write-all algorithm. Martel et al. gave an O (p) work bound for an p log p log fl p -processor, p-cell write-all algorithm.
Reference: [Lam78] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Cole and Zajicek introduced an asynchronous P-RAM model where the run time is measured in rounds, where a round is a minimal interval of time that allows each processor to complete one step. This measure of run time has also been used in the field of distributed computing <ref> [Lam78, LF81] </ref> One way of viewing this is that time is measured with respect to the slowest processor. This is not an appropriate measure when processors are subject to long delays, since while one processor is delayed, all other computation takes place for free.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocessor programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: In order for an algorithm to be correct it must behave properly for all interleavings <ref> [HW87, Lam79] </ref>. Our definition of what it means to correctly answer the data structure operations is also done in terms of serializability. The answers to the queries are said to be correct if the answers agree with some serialization of atomic queries.
Reference: [LF81] <author> N. A. Lynch and M. J. Fischer. </author> <title> On describing the behavior and implementation of distributed systems. </title> <journal> Theoretical Computer Science, </journal> <volume> 13 </volume> <pages> 17-43, </pages> <year> 1981. </year>
Reference-contexts: Cole and Zajicek introduced an asynchronous P-RAM model where the run time is measured in rounds, where a round is a minimal interval of time that allows each processor to complete one step. This measure of run time has also been used in the field of distributed computing <ref> [Lam78, LF81] </ref> One way of viewing this is that time is measured with respect to the slowest processor. This is not an appropriate measure when processors are subject to long delays, since while one processor is delayed, all other computation takes place for free.
Reference: [MSP90] <author> C. Martel, R. Subramonian, and A. Park. </author> <title> Asynchronous PRAMs are (almost) as good as synchronous PRAMs. </title> <booktitle> In 31st Symposium on Foundations of Computer Science, </booktitle> <pages> pages 590-599, </pages> <year> 1990. </year>
Reference-contexts: Herlihy [Her90] studied the implementation of a number of data structures in terms of compare&swap. He showed how to implement functional data structures, and discussed wait-free memory management. 2.2 Asynchronous P-RAM models Researchers have recently begun to consider asynchronous versions of the P-RAM <ref> [Gib89, RZ89, RZ90, Nis90, MSP90] </ref>. The papers have introduced several different models, with differing notions of run time. <p> Aspnes and Herlihy [AH90] consider an A-PRAM that does not support universal primitives. They study the limitations of such machines and classify the computations that can be performed. The works that present a model that is closest to ours are a series of papers considering fault tolerant P-RAMs <ref> [KS89, KPS90, MSP90] </ref>. These papers consider a model where processors may fail, and the goal is to simulate the computation using the surviving processors. They measure performance in terms of work, the sum of the active times of the processors, the same measure as used in this paper. <p> Kanellakis and Shvartsman [KS89] introduced the Fail-Stop P-RAM model and the write-all problem. Kedem et al. [KPS90] gave a very general simulation result based upon a robust implementation of a variant of the write-all problem. Martel et al. <ref> [MSP90] </ref> extended the result to an asynchronous model by giving an asynchronous algorithm for the write-all problem. Their main result is a probabilistic simulation of an n processor algorithm using n log n log fl n processors and O (n) work. <p> This adversary model has been used in [And90] and [BR90]. The strength of the adversary plays an important role in the results that one can establish. The adversary used by Martel et al. <ref> [MSP90] </ref> is substantially weaker than the adversary used here. Their adversary sets the complete interleaving order before it sees the results of any of the random numbers. <p> If the adversary is not allowed to base its interleavings on the results of a random number generator, then we can get a better worst case bound. Martel et al. <ref> [MSP90] </ref> prevent the adversary taking advantage of the random number generator by having the adversary set the interleaving order before the random numbers are generated. <p> Each round simulates exactly one instruction of each of the p synchronous thread. This type of simulation is discussed for other asynchronous models in several papers <ref> [KS89, KPS90, MSP90] </ref>. It is not difficult to design a synchronized Union-Find algorithm that is almost as efficient as the sequential algorithm. The source of inefficiency in our asynchronous algorithm arises from long chains being created. <p> When we update a tag, we use the Compare&Swap operation to avoid difficulties with concurrent updates. The fact that we have a more powerful model makes it easier for us to resolve this difficulty than have others using weaker models <ref> [MSP90] </ref>. In the next subsection, we show that one round of the synchronous algorithm can be simulated with work O (p 1+* ) for any fixed * &gt; 0.
Reference: [Nis90] <author> N. Nishimura. </author> <title> Asynchronous shared memory parallel computation. </title> <booktitle> In Second Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 76-84, </pages> <year> 1990. </year>
Reference-contexts: Herlihy [Her90] studied the implementation of a number of data structures in terms of compare&swap. He showed how to implement functional data structures, and discussed wait-free memory management. 2.2 Asynchronous P-RAM models Researchers have recently begun to consider asynchronous versions of the P-RAM <ref> [Gib89, RZ89, RZ90, Nis90, MSP90] </ref>. The papers have introduced several different models, with differing notions of run time. <p> This is not an appropriate measure when processors are subject to long delays, since while one processor is delayed, all other computation takes place for free. Nishimura <ref> [Nis90, Nis91] </ref> and Cole and Zajicek [RZ90] analyze run time assuming random interleaving of processors. This model applies best when processors run at close to the same speed as opposed to stopping for long periods of time. <p> One can achieve far better bounds than ours for the Union-Find problem is the adversary is weakened, or if the performance measures are changed. The average case run time (over a worst case set of requests) improves substantially if we take the random interleaving model of Nishimura <ref> [Nis90] </ref>. Under a random interleaving model, it is safe for a thread to stop working when contention is detected, since the other threads are not likely to be stopped indefinitely.
Reference: [Nis91] <author> N. Nishimura. </author> <title> Asynchrony in Shared Memory Parallel Computation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Toronto, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: This is not an appropriate measure when processors are subject to long delays, since while one processor is delayed, all other computation takes place for free. Nishimura <ref> [Nis90, Nis91] </ref> and Cole and Zajicek [RZ90] analyze run time assuming random interleaving of processors. This model applies best when processors run at close to the same speed as opposed to stopping for long periods of time.
Reference: [Plo89] <author> S. A. Plotkin. </author> <title> Sticky bits and universality of consensus. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 159-175, </pages> <year> 1989. </year>
Reference-contexts: A primitive is universal if any wait-free object can be constructed from it. Primitives such as an atomic append to a list and compare&swap are universal while test&set is not. Plotkin <ref> [Plo89] </ref> showed that there is a single bit primitive 4 (called a sticky bit) that is also universal. A main drawback to the universality results is that the simulations of one primitive with another are often inefficient.
Reference: [Rag86] <author> P. Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: approximating packing integer programs. </title> <booktitle> In 27th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 10-18, </pages> <year> 1986. </year>
Reference-contexts: In order to apply the central limit theorem rigorously, it is necessary to bound the rate of convergence to normal distribution. We get a simpler proof by viewing the random variables as a sum of independent Bernoulli trials and applying a bound established by Raghavan <ref> [Rag86] </ref>. For convenience, we restate his result. Lemma 5.14 (Raghavan) Let X 1 ; X 2 ; : : : ; X r be a sequence of independent Bernoulli trials and let = X 1 + X 2 + X r . Suppose Exp [] = m. <p> Suppose Exp [] = m. For ffi &gt; 0, Prob [ &gt; (1 + ffi)m] &lt; (1 + ffi) (1+ffi) : Proof: See <ref> [Rag86] </ref>, Theorem 1.
Reference: [RZ89] <author> R.Cole and O. Zajicek. </author> <title> The APRAM: Incorporating asynchrony into the PRAM model. </title> <booktitle> In 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <year> 1989. </year>
Reference-contexts: Herlihy [Her90] studied the implementation of a number of data structures in terms of compare&swap. He showed how to implement functional data structures, and discussed wait-free memory management. 2.2 Asynchronous P-RAM models Researchers have recently begun to consider asynchronous versions of the P-RAM <ref> [Gib89, RZ89, RZ90, Nis90, MSP90] </ref>. The papers have introduced several different models, with differing notions of run time.
Reference: [RZ90] <author> R.Cole and O. Zajicek. </author> <title> The expected advantage of asynchrony. </title> <booktitle> In Second Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 85-94, </pages> <year> 1990. </year>
Reference-contexts: Herlihy [Her90] studied the implementation of a number of data structures in terms of compare&swap. He showed how to implement functional data structures, and discussed wait-free memory management. 2.2 Asynchronous P-RAM models Researchers have recently begun to consider asynchronous versions of the P-RAM <ref> [Gib89, RZ89, RZ90, Nis90, MSP90] </ref>. The papers have introduced several different models, with differing notions of run time. <p> This is not an appropriate measure when processors are subject to long delays, since while one processor is delayed, all other computation takes place for free. Nishimura [Nis90, Nis91] and Cole and Zajicek <ref> [RZ90] </ref> analyze run time assuming random interleaving of processors. This model applies best when processors run at close to the same speed as opposed to stopping for long periods of time.
Reference: [Tar75] <author> R. E. Tarjan. </author> <title> Efficiency of a good but not linear set union algorithm. </title> <journal> Journal of the ACM, </journal> <volume> 22(4) </volume> <pages> 215-225, </pages> <year> 1975. </year>
Reference-contexts: Union is implemented by making the root of one tree point to the root of the other tree, and Find is implemented by following a path from the given element to the root of its tree. There are a number of implementations of both Union and Find <ref> [Tar75, TvL84] </ref>.
Reference: [Tar83] <author> R. E. Tarjan. </author> <title> Data Structures and Network Algorithms. </title> <publisher> SIAM, </publisher> <year> 1983. </year>
Reference-contexts: There are a number of implementations of both Union and Find [Tar75, TvL84]. In this paper we give an implementation based on a ranked Union with path halving <ref> [Tar83] </ref>. (We can also adapt path compression to a wait-free implementation.) The sequential cost for a sequence of n Union-Find operations with ranked Union and either path halving or path compression is nff (n). There are a number of difficulties that arise in developing a concurrent Union-Find data structure.
Reference: [TvL84] <author> R. E. Tarjan and J. van Leeuwen. </author> <title> Worst-case analysis of set union algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 31(2) </volume> <pages> 245-281, </pages> <year> 1984. </year> <month> 32 </month>
Reference-contexts: Union is implemented by making the root of one tree point to the root of the other tree, and Find is implemented by following a path from the given element to the root of its tree. There are a number of implementations of both Union and Find <ref> [Tar75, TvL84] </ref>. <p> Theorem 4.6 The worst case work for a series of n Union-Find operations is fi (pn+nff (n)). Proof: We begin with the lowerbound. The nff (n) term comes from the sequential lower bound for ranked Union with path-halving <ref> [TvL84] </ref>. The other term arises from having groups of threads simultaneously traverse long chains. The adversary gives a set of n operations, and an interleaving of instructions that takes (pn) work. p=2 threads perform Unions, and p=2 threads perform Finds. <p> The nff (n) term follows from the sequential upperbound. (The results rely on a sequential bound that applies when the number of Finds is less than the number of Unions <ref> [TvL84] </ref>.) The worst case bound of fi (pn) work is not very good when compared with the sequential bound of fi (nff (n)). This says that in the worst case, we get essentially no speedup using p processors. We can put a better spin on the result.
References-found: 27

