URL: ftp://ltpftp.gsfc.nasa.gov/pub/people/smid/papers/padua97/padua.ps
Refering-URL: http://ltpwww.gsfc.nasa.gov/Smid/proj/projects.html
Root-URL: 
Title: ON MCMC METHODS IN BAYESIAN REGRESSION ANALYSIS AND MODEL SELECTION  
Author: Petr Volf Jan Smid Morgan Ales Linka 
Note: Contents:  
Address: Czech Republic, Prague  Baltimore  Liberec  
Affiliation: Academy of Sciences of the  State University,  Technical University,  
Abstract: The objective of statistical data analysis is not only to describe the behaviour of a system, but also to propose, construct (and then to check) a model of observed processes. Bayesian methodology offers one of possible approaches to estimation of unknown components of the model (its parameters or functional components) in a framework of a chosen model type. However, in many instances the evaluation of Bayes posterior distribution (which is basal for Bayesian solutions) is difficult and practically intractable (even with the help of numerical approximations). In such cases the Bayesian analysis may be performed with the help of intensive simulation techniques called the `Markov chain Monte Carlo'. The present paper reviews the best known approaches to MCMC generation. It deals with several typical situations of data analysis and model construction where MCMC methods have been successfully applied. Special attention is devoted to the problem of selection of optimal regression model constructed from regression splines or from other functional units. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andersen P.K., Borgan O., Gill R.D. and Keiding N. </author> <year> (1992). </year> <title> Statistical Models Based on Counting Processes. </title> <publisher> Springer, </publisher> <address> New York. </address>
Reference: [2] <author> Arjas E. and Gasbarra D. </author> <year> (1994). </year> <title> Nonparametric Bayesian inference from right censored survival data using the Gibbs sampler. </title> <journal> Statist. </journal> <volume> Sinica 4, </volume> <pages> 505-524. </pages>
Reference-contexts: We can consider M as an integer-valued random variable and make it the part of Bayesian scheme. For instance, Arjas and Gasbarra <ref> [2] </ref> proposed an approach which generated knots via the Gibbs sampler, using for them the prior distribution given by a Poisson process (its parameter was a priori selected by an analyst actually it was a 'hyperparameter' used as a further tool controlling the sampling procedure). <p> The space of fi's is limited to such that kfi j k = 1. Notice also that a rotation in R p is fully described by an angle having p 1 components, each with values in <ref> [0; 2] </ref>. So that the dimension of nonlinear parameter is actually p 1. On the other hand, it is well known that the projection pursuit is highly sensitive, that the dependence on fi's is rather unsmooth. <p> Therefore, the 14 final estimate (i.e. the average from the chain of histograms) has more or less smooth shape, except at the points where the regression function jumps. In such a way, the points of jumps are easily detected. In <ref> [2] </ref> a similar method is used, also for estimation of a hazard rate. Authors preferred the Gibbs sampler (combined with the sampling-rejection method) and used the conditional priors as proposal distributions.
Reference: [3] <author> Arjas E. and Liu L. </author> <year> (1995). </year> <title> Assessing the losses caused by an industrial intervention a hiearchical Bayesian approach. </title> <editor> J. R. S. S. ser. </editor> <volume> C 44, </volume> <pages> 357-368. </pages>
Reference: [4] <author> Bernardo J.M. and Smith A.F.M. </author> <year> (1994). </year> <title> Bayesian Theory. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: [5] <author> Besag J. </author> <year> (1986). </year> <title> On the statistical analysis of dirty pictures (with discussion). </title> <editor> J. R. S. S. ser. </editor> <volume> B 48, </volume> <pages> 259-302. </pages>
Reference-contexts: We give here only a brief introduction to applications of the MCMC approach in the field of image processing. There exists a number of papers, <ref> [15, 5] </ref>, overviews, [20], monographs, [32] on the subject. Let us consider a very simple example of a "two-valued" image, i. e. a black and white one. We assume that x i ; i 2 I, can be either zero or one.
Reference: [6] <author> Besag J., Green D., Higdon D. and Mengersen K. </author> <year> (1995). </year> <title> Bayesian computation and stochastic systems. </title> <journal> Statist. </journal> <note> Science 10, </note> <year> 1995, </year> <pages> 3-66. </pages>
Reference: [7] <author> Bielza C., Muller P. and Insua D.R. </author> <year> (1997). </year> <title> Markov chain Monte Carlo methods for decision analysis. </title> <booktitle> In: 6-th Intern. Workshop on AI and Statistics, </booktitle> <address> Fort Lauderdale, Florida 1997, </address> <pages> 31-38. </pages>
Reference-contexts: As examples, let us mention Bayesian belief networks [22], probabilistic expert systems, influence diagram as a tool of statistical decision <ref> [7] </ref>. In the framework of Bayesian approach, all unknown quantities (parameters of the model) are regarded as random variables. The analyst specifies (sometimes rather subjectively, using prior information about them) their prior distribution.
Reference: [8] <author> Chipman H., George E.I. and McCulloch R.E. </author> <year> (1997). </year> <title> A Bayesian approach for CART. </title> <booktitle> In: 6-th Intern. Workshop on AI and Statistics, </booktitle> <address> Fort Lauderdale, Florida 1997, </address> <pages> 91-101. </pages>
Reference-contexts: In fact, a Bayesian approach for construction of a classification or regression tree has already been proposed in several papers, even together with the MCMC simulations, e.g. in <ref> [8] </ref>. The spliting of leafs (end nodes) can be accompliched (randomly) in a domain of each input variable (the problem is that this domain is, as a rule, rather wide. The main problem, however, is how to update a structure of a tree in its intermediate level.
Reference: [9] <author> Chen S., Cowan C.F.N. and Grant P.M. </author> <year> (1991). </year> <title> Orthogonal least squares learning for radial basis function networks. </title> <journal> IEEE Trans. Neural. </journal> <volume> Networks 2, </volume> <pages> 302-309. </pages>
Reference-contexts: Thus, an appropriate choice of prior distribution for M supports the reduction of the model complexity. The addition of new units can be complementary controlled by a rule guaranteeing a reasonable minimal distance between them (similarly as in Chen et al. <ref> [9] </ref>) and, eventually, by prescribing a maximal number of units. 4.5 Accuracy of Estimate and of Prediction Likelihood of a parameter value is in mathematical statistics characterized also by the confidence intervals.
Reference: [10] <author> De Boor C. </author> <year> (1978). </year> <title> A Practical Guide to Splines. </title> <publisher> Springer Verlag. </publisher>
Reference: [11] <author> Eubank R.L., Speckman P.L. </author> <year> (1994). </year> <title> Nonparametric estimation of functions with jump discontinuities. In: Change Point Problem, </title> <booktitle> IMS Lecture Notes 23, </booktitle> <pages> 130-143. </pages>
Reference-contexts: Eubank and Speckman <ref> [11] </ref> and others solved the problem in a semiparametric way, estimating a smooth part of the function with the help of kernel nonparametric technique and simultaneously estimating the parameter location of the change-point by the least squares. They also proved consistency and asymptotic normality of the estimator.
Reference: [12] <author> Feller W. </author> <year> (1968). </year> <title> An Introduction to Probability Theory and Its Applications. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: [13] <author> Friedman J.H. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> Annals Statist. </journal> <volume> 19, </volume> <pages> 1-141. </pages>
Reference-contexts: In a non-Bayesian setting, this is often measured by a penalty criterion. For example, it is recommended (among other criteria, e.g. Akaike's AIC, Schwarz's BIC, GCV, see also Friedman <ref> [13] </ref>), to use the criterion ^ 2 M exp ( M N fl ), where fl is a number from (0:5; 1), ^ 2 M is the estimate of residual variance. Quite similarly, we can obtain the penalty as a part of the acceptance probability in the ratio (4). <p> That is why there are attempts to reduce the dimensionality of `decision space' of each single step. Especially the methods based on the idea of regression tree are successful. The most known one is the CART, giving a histogram-like result, and its modification giving a continuous function, the MARS <ref> [13] </ref>. For instance, as a result of MARS-like partitioning of the domain of explaining variables we obtain a sum of relatively simple functions.
Reference: [14] <author> Gelman A. and Rubin D.B. </author> <year> (1992). </year> <title> Inference from iterative simulation using multiple sequences (with discussion). </title> <booktitle> Statist. Science 7, </booktitle> <pages> 457-472. </pages>
Reference: [15] <author> Geman S. and Geman D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Trans. Pattern Anal. Mach. Intell. </journal> <volume> 6, </volume> <pages> 724-741. </pages>
Reference-contexts: We give here only a brief introduction to applications of the MCMC approach in the field of image processing. There exists a number of papers, <ref> [15, 5] </ref>, overviews, [20], monographs, [32] on the subject. Let us consider a very simple example of a "two-valued" image, i. e. a black and white one. We assume that x i ; i 2 I, can be either zero or one.
Reference: [16] <author> Geyer C.J. </author> <year> (1992). </year> <title> Practical Markov chain Monte Carlo (with discussion). </title> <booktitle> Statist. Science 7, </booktitle> <pages> 473-511. </pages>
Reference: [17] <author> Green P.J. </author> <year> (1995). </year> <title> Reversible jump MCMC computation and Bayesian model determination. </title> <journal> Biometrika 82, </journal> <pages> 711-732. </pages>
Reference-contexts: Even more general situation is described in <ref> [17] </ref>, namely the situation when several models are considered simultaneously as "candidate" models. Therefore, we have to deal with several corresponding state spaces. More precisely, let us denote the state spaces corresponding to the i-th model by E i , using the indices i 2 I. <p> For instance, transitions from a 'larger' space to a 'smaller' one can be described by a discrete probability, while the transitions in the opposite direction by a density. P. J. Green in <ref> [17] </ref> formulated a 'dimension-matching' assumption giving the conditions for overcoming such uncosistencies. 8 4 Nonparametric Regression The next parts deal with the models of functional relationships. <p> For instance, even parameters ff could be estimated via the MCMC method (it is the way preferred by some other authors, e.g. <ref> [17] </ref>). However, innovation of ff by direct computation is rather easy (even in a general likelihood case, several steps of the Newton-Raphson algorithm suffice to stabilization of values of ^ ff). <p> The problem of moves between different models (including the case of models of different dimensions) is solved and illustrated in Green, <ref> [17] </ref>. We have experimented with a similar procedure. <p> They also proved consistency and asymptotic normality of the estimator. In the framework of MCMC approach, it is quite natural to generate the candidates of change-points randomly and to accept or reject them in accordance with the rules of MCMC algorithms. For instance, in <ref> [17] </ref> a problem of detection of multiple jumps of regression function (and of hazard function, in another illustrative example) is considered. Each member of the Markov chain is constructed as a histogram, i.e. a piecewise constant function. <p> In such a way, the points of jumps are easily detected. In [2] a similar method is used, also for estimation of a hazard rate. Authors preferred the Gibbs sampler (combined with the sampling-rejection method) and used the conditional priors as proposal distributions. As in <ref> [17] </ref>, the knots are proposed from Poisson process and also the updating of 'linear' parameters (e.g. of the heights of elements of histogram) is performed in the framework of the MCMC procedure.
Reference: [18] <author> Hastie T., Tibshirani R. </author> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: [19] <author> Hastings W.K. </author> <year> (1970). </year> <title> Monte Carlo sampling methods using Markov chains and their applications. </title> <journal> Biometrika 57, </journal> <pages> 97-109. </pages>
Reference-contexts: The limitation of direct application consists in that the exact knowledge of p j is required. That is why the Gibbs algorithm is often combined with the sampling-rejection method (applied subsequently to one component j after another). Metropolis-Hastings algorithm, <ref> [19] </ref>, also yields the Markov chain of -s with invariant distribution p (jx): Let m be the actual state (i. e. the last member) of the chain.
Reference: [20] <author> Janzura M. </author> <year> (1996). </year> <title> Image processing, Markov chain approach. </title> <journal> In:Proceedings of Compstat'96, Physica Verlag, </journal> <pages> 89-100. </pages>
Reference-contexts: In the present context it can be used in looking for the maximum of the posterior density function, or, more generally, the value minimizing a chosen loss function. The method has been rather successfully applied to Bayesian reconstruction of images ,[15], <ref> [20] </ref>, and to solution of other complex problems. 2.2 Example 1: The example is solved in [26] with the help of the sampling-rejection method, and with a variant of the sampling-importance resampling. <p> Quite naturally, there do not exist universal and exact rules how to use the annealing, it is more or less an adaptive procedure, "it is an art of its own" <ref> [20] </ref>. In the context of Bayesian estimation of parameters, simulated annealing applies to the search of maximum of (density of) posterior distribution, which is the most frequently used form of Bayes 'point' estimate of a parameter. <p> We give here only a brief introduction to applications of the MCMC approach in the field of image processing. There exists a number of papers, [15, 5], overviews, <ref> [20] </ref>, monographs, [32] on the subject. Let us consider a very simple example of a "two-valued" image, i. e. a black and white one. We assume that x i ; i 2 I, can be either zero or one.
Reference: [21] <author> Linka A., Picek J., Volf P. </author> <year> (1996). </year> <title> Monte Carlo method for likelihood regression analysis. </title> <booktitle> In: Proceedings of Comp-stat'96, </booktitle> <publisher> Physica Verlag, </publisher> <pages> 343-348. </pages>
Reference: [22] <author> Neal R.M. </author> <year> (1993). </year> <title> Probabilistic Inference Using Markov Chain Monte Carlo. </title> <type> Techn. Rep. </type> <institution> CGR-TR-93-1, Univ. of Toronto. </institution> <note> http://www.cs.utoronto.ca/~radford. </note>
Reference-contexts: As examples, let us mention Bayesian belief networks <ref> [22] </ref>, probabilistic expert systems, influence diagram as a tool of statistical decision [7]. In the framework of Bayesian approach, all unknown quantities (parameters of the model) are regarded as random variables. The analyst specifies (sometimes rather subjectively, using prior information about them) their prior distribution. <p> It is the case of onedimensional model, but this assumption is typically not met in multidimensional cases. That is why some authors recommend the hybrid procedures which combine random search with a gradient method <ref> [22] </ref>. Additive model: In the simplest scenario the multidimensional model has an additive form. The response function r (x) is then a sum of p component functions of one variable. <p> Simultaneously, the procedure should allow for a random jump away from a region of local optimum. One possibility how to organize such a procedure is described in <ref> [22] </ref>. An analogy with physical systems is employed, the process of moves leading to optimization is described in terms of Hamiltonian dynamics. We shall not describe this approach in detail here, but we shall recall the idea of the approach and the essence of its practical application. <p> Such a step has to generate a new candidate ( fl ; w fl ) on a different level of H, and accept it randomly in accordance with Metropolis (-Hastings) algorithm. A simplification (cf. again <ref> [22] </ref>) uses the fact that the stationary distribution for w is known. <p> Another method uses c (s), with value of c (s) decreasing when s grows. 18 In our case, it is recommended to follow a discretized version of Hamiltonian dynamics (9), namely to perform several small `leapfrog' moves, from "time" t to t + ", <ref> [22] </ref>. As it has been said, the signum of w is randomly changed before each move. This is repeated L times (say), and this loop yields a new value of variables (t + 1); w (t + 1). Remark.
Reference: [23] <author> Neal R.M. </author> <year> (1996). </year> <title> Bayesian Learning for Neural Networks. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference: [24] <author> Roberts G.O. and Smith A.F.M. </author> <year> (1994). </year> <title> Simple conditions for the convergence of the Gibbs sampler and Metropolis-Hastings algorithms, </title> <journal> Stoch. Processes and Applic. </journal> <volume> 49, </volume> <pages> 207-216. 22 </pages>
Reference-contexts: It is proved, under quite mild and natural conditions, that p (jx) is an invariant distribution of such a Markov chain and that m tends, when m ! 1, in distribution to a (p-dimensional) random variable whose distribution has density p (jx) ([15], <ref> [24] </ref>). Moreover, the ergodic property is guaranteed. Its meaning is that the sample averages converge to the mean of the posterior distribution. The limitation of direct application consists in that the exact knowledge of p j is required. <p> The conditions for convergence of a homogeneous Markov chain with state space E 2 IR m are formulated e. g. in <ref> [24] </ref>, paper [30] deals with the problem of detailed balance for various versions of Metropolis-Hastings algorithm. For the sake of brevity, let us consider here the case of an uncountable state space E IR 1 (measurable, naturally). <p> The convergence properties of the chain follows from the corresponding properties of the generating probability, <ref> [24] </ref>: Proposition 2. i) If Q is aperiodic, then K is aperiodic ii) If Q is irreducible (on E), and Q (x; y) = 0 if and only if Q (y; x) = 0, then K is irreducible. 3.2 Modifications of MH Algorithm There exist many modifications and generalizations of the
Reference: [25] <author> Smid J., Volf P. and Rao G. </author> <year> (1997). </year> <title> Monte Carlo approach to Bayesian regression modeling. In: Computer Intensive Methods in Control and Signal Processing. </title> <publisher> Birkhauser, Boston, </publisher> <pages> 169-180. </pages>
Reference: [26] <author> Smith A.F.M. and Gelfand A.E. </author> <year> (1992). </year> <title> Bayesian statistics without tears: A sampling-resampling perspective. </title> <journal> The Amer. </journal> <volume> Statistician 46, </volume> <pages> 84-88. </pages>
Reference-contexts: The method has been rather successfully applied to Bayesian reconstruction of images ,[15], [20], and to solution of other complex problems. 2.2 Example 1: The example is solved in <ref> [26] </ref> with the help of the sampling-rejection method, and with a variant of the sampling-importance resampling.
Reference: [27] <author> Stone C.J. </author> <year> (1994). </year> <title> The use of polynomial splines and their tensor products in multivariate function estimation. </title> <journal> Annals of Statist. </journal> <volume> 22, </volume> <pages> 118-194. </pages>
Reference: [28] <author> Tanner M.A. </author> <year> (1993). </year> <title> Tools for Statistical Inference: Methods for the Exploration of Posterior Distributions and Likelihood Functions. </title> <publisher> Springer Verlag. </publisher>
Reference: [29] <author> Tierney L. </author> <year> (1994). </year> <title> Markov chains for exploringposterior distributions. </title> <journal> Annals of Statist. </journal> <volume> 22, </volume> <pages> 1701-1728. </pages>
Reference: [30] <author> Tierney L. </author> <year> (1995). </year> <title> A Note on Metropolis-Hastings Kernels for General State Spaces. </title> <type> Techn. Report No 606, </type> <institution> School of Statistics, Univ. of Minnesota. </institution>
Reference-contexts: The conditions for convergence of a homogeneous Markov chain with state space E 2 IR m are formulated e. g. in [24], paper <ref> [30] </ref> deals with the problem of detailed balance for various versions of Metropolis-Hastings algorithm. For the sake of brevity, let us consider here the case of an uncountable state space E IR 1 (measurable, naturally). <p> For instance, in the part dealing with the hybrid MC approach we consider a mixing of single steps. These are alternated (or mixed) either in a deterministic or in a random way. For several other examples cf. <ref> [30] </ref>. The most common situation occurs when we deal with a multidimensional states x and the procedure updates them component after component. Then, we regularly (and mostly in the same sequence) alternate a finite number of single updating steps.
Reference: [31] <author> Volf P. </author> <year> (1993). </year> <title> Moving window estimation procedures for additive regression function. </title> <type> Kybernetika 29, </type> <pages> 389-400. </pages>
Reference: [32] <author> Winkler G. </author> <year> (1995). </year> <title> Image Analysis, Random Fields and Dynamic Monte Carlo Methods. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: It is desirable to join these two convergences to one process. It is proved, <ref> [32] </ref>, at least for the case of a finite state space fi, that the result depends on a proper rate of convergence of T (s) to zero. For instance, T (s) ~ 1= log (s) suffices theoretically. <p> We give here only a brief introduction to applications of the MCMC approach in the field of image processing. There exists a number of papers, [15, 5], overviews, [20], monographs, <ref> [32] </ref> on the subject. Let us consider a very simple example of a "two-valued" image, i. e. a black and white one. We assume that x i ; i 2 I, can be either zero or one.
Reference: [33] <author> Wold S. </author> <year> (1974). </year> <title> Spline functions in data analysis. </title> <type> Technometrics 16, 1-11. </type> <institution> Petr Volf, Institute of Information Theory and Automation, Czech Academy of Sciences, </institution> <address> Pod vodarenskou vez 4, 18208 Prague 8, e-mail: volf@utia.cas.cz Jan Smid, </address> <publisher> Morgan State University, </publisher> <address> Baltimore, MA, e-mail: smid@gsfc.nasa.gov Ales Linka, </address> <institution> Dept. of Mathematical Statistics, Technical University of Liberec, </institution> <note> Halkova 6, 46000 Liberec, e-mail: ales.linka@vslib.cz 23 </note>
Reference-contexts: fi M+1 = b, let us add six other 'dummy' knots fi j = a j (fi 1 a); fi M+1+j = b + j (b fi m ); j = 1; 2; 3: One way how to define the B-spline function (the way we use, following for instance Wold <ref> [33] </ref>) employs divided differences: B j (x; fi) = k=j2 &lt; (x fi k ) 3 j+2 Y (fi k fi s ) = ; for j = 1; 0; 1; ::: M; M + 1; M + 2: Here, function (u) + means u 1 [u &gt; 0].
References-found: 33

