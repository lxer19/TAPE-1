URL: ftp://ftphost.comp.vuw.ac.nz/doc/vuw-publications/Staff/Eric-Jones/cognitive-science-91.ps.gz
Refering-URL: http://www.comp.vuw.ac.nz/~jones/publications/publications_1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: jones@ils.nwu.edu  
Title: Adapting Abstract Knowledge  
Author: Eric K. Jones 
Address: Evanston, IL 60201  
Affiliation: The Institute for the Learning Sciences Northwestern University  
Abstract: For a case-based reasoner to use its knowledge flexibly, it must be equipped with a powerful case adapter. A case-based reasoner can only cope with variation in the form of the problems it is given to the extent that its cases in memory can be efficiently adapted to fit a wide range of new situations. In this paper, we address the task of adapting abstract knowledge about planning to fit specific planning situations. First we show that adapting abstract cases requires reconciling incommensurate representations of planning situations. Next, we describe a representation system, a memory organization, and an adaptation process tailored to this requirement. Our approach is implemented in brainstormer, a planner that takes abstract advice. 
Abstract-found: 1
Intro-found: 1
Reference: [ Bobrow and Winograd, 1977 ] <author> D. G. Bobrow and T. Wino-grad. </author> <title> An overview of krl, a knowledge representation language. </title> <journal> Cognitive Science, </journal> <volume> 1(1) </volume> <pages> 3-46. </pages>
Reference-contexts: Brainstormer's redescription mechanism relates to three areas of past work in cognitive science and ar tificial intelligence. First are investigations into the idea of viewing or redescription inference, starting with merlin [ Moore and Newell, 1973 ] and krl <ref> [ Bobrow and Winograd, 1977 ] </ref> in the 1970's. More recently, Ja-cobs has resurrected these ideas in the context of natural language generation [ Jacobs, 1987 ] . His ace system redescribes concepts with no associated generation method in terms of other concepts that have one.
Reference: [ Cullingford, 1977 ] <author> R. Cullingford. </author> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> PhD diss., </type> <institution> Yale Univ. </institution>
Reference-contexts: Even if a system contains the knowledge it needs to solve a problem, it may not be able to bring this knowledge to bear. At one extreme are systems that use radically incomplete inference procedures such as schema or script application <ref> [ Cullingford, 1977 ] </ref> , which although efficient, can only solve a limited class of problems. At the other extreme are systems that countenance multistep inference chaining [ Rieger, 1976; Wilensky, 1978 ] .
Reference: [ Falkenhainer et al., 1986 ] <author> B. Falkenhainer, D. K. Forbus, and D. Gentner. </author> <title> The structure-mapping engine. </title> <booktitle> In Proceedings AAAI-86, </booktitle> <pages> 272-277, </pages> <address> Philadelphia, PA. </address> <publisher> AAAI. </publisher>
Reference-contexts: This is a kind of analogical reasoning. Unlike many existing systems, however (e.g., Gentner's structure mapping engine <ref> [ Falkenhainer et al., 1986 ] </ref> ), brainstormer does not depend on preexisting structural or predicate correspondences between base and target domains. All that is required is that the source can be extended to a set of plausible recognition conditions for the target.
Reference: [ Feigenbaum et al., 1971 ] <author> E. A. Feigenbaum, B. Buchanan, and J. Lederberg. </author> <title> On generality and problem solving: a case study using the dendral program. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence 6, </booktitle> <pages> 165-190. </pages> <publisher> Edinburgh University Press, Edinburgh. </publisher>
Reference-contexts: Introduction Most knowledge-based systems are unable to use their knowledge flexibly: they can only solve problems that are stated in exactly the right way. This is one aspect of the brittleness problem <ref> [ Feigenbaum et al., 1971 ] </ref> , which limits the scope of many existing artificial intelligence systems both as practical tools and as cognitive models. Even if a system contains the knowledge it needs to solve a problem, it may not be able to bring this knowledge to bear.
Reference: [ Fikes and Nilsson, 1971 ] <author> R. E. Fikes and N. J. Nilsson. </author> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208. </pages>
Reference: [ Hammond, 1986 ] <author> K. J. Hammond. </author> <title> Case-based Planning: An Integrated Theory of Planning, Learning and Memory. </title> <type> PhD diss., </type> <institution> Yale Univ. </institution>
Reference-contexts: In principle, these systems are capable of solving a large number of problems, but in practice they make so many useless inferences that the number of problems they can actually solve under reasonable resource constraints remains small. Case-based reasoning has been proposed as a promising middle ground <ref> [ Hammond, 1986; Kolodner et al., 1985 ] </ref> . Like schema appliers, case-based reasoners trade completeness for efficiency. Unlike schema appliers, fl This research was supported in part by DARPA contract F49620-88-C-0058 and AFOSR contract 89-0493.
Reference: [ Jacobs, 1987 ] <author> P. S. Jacobs. </author> <title> Knowledge-intensive natural language generation. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 325-378. </pages>
Reference-contexts: Whole-whole views, in contrast, reference only preexisting concepts. For example, a terrorist attack can also be redescribed in terms of the preexisting concept of illegal action using a whole-whole view. Whole-whole views are similar in spirit to the views in Jacob's ace system <ref> [ Jacobs, 1987 ] </ref> . causal-explanation explained -- part-whole-view =view source terrorist-attack =attack actor group =terrorists typical-elt terrorist recog-conds (cause cause achieve-goal =g1 actor =terrorists state =attack caused =attack violates-goal state =attack goal prevent-goal =g2 actor brainstormer state terrorist-attack) target goal-conflict actor1 =terrorists actor2 brainstormer goal1 =g1 goal2 =g2 concept (gc-schema2 <p> First are investigations into the idea of viewing or redescription inference, starting with merlin [ Moore and Newell, 1973 ] and krl [ Bobrow and Winograd, 1977 ] in the 1970's. More recently, Ja-cobs has resurrected these ideas in the context of natural language generation <ref> [ Jacobs, 1987 ] </ref> . His ace system redescribes concepts with no associated generation method in terms of other concepts that have one.
Reference: [ Jones, 1990 ] <author> E. K. Jones. Brainstormer: </author> <title> A model of advice taking. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 269-276, </pages> <address> Cam-bridge, MA. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: The person taking the advice is therefore faced with the task of adapting abstract knowledge to fit her current problem solving needs. We have built the brainstormer system to investigate the problem of adapting abstract knowledge <ref> [ Jones, 1990; Jones, 1991 ] </ref> . Brainstormer is a planner that takes advice about problems in the domain of terrorist crisis management. Whenever it encounters a difficulty, it asks for advice.
Reference: [ Jones, 1991 ] <author> E. K. Jones. </author> <title> Knowledge refinement using a high-level, non-technical vocabulary. </title> <booktitle> In Machine Learning: Proceedings of the Eighth International Workshop, </booktitle> <address> Chicago, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The person taking the advice is therefore faced with the task of adapting abstract knowledge to fit her current problem solving needs. We have built the brainstormer system to investigate the problem of adapting abstract knowledge <ref> [ Jones, 1990; Jones, 1991 ] </ref> . Brainstormer is a planner that takes advice about problems in the domain of terrorist crisis management. Whenever it encounters a difficulty, it asks for advice.
Reference: [ Kass, 1989 ] <author> A. Kass. </author> <title> Adaptation-based explanation: Extending script/frame theory to handle novel input. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 143-147, </pages> <address> Detroit, MI. IJCAI. </address>
Reference-contexts: All that is required is that the source can be extended to a set of plausible recognition conditions for the target. Finally, our work extends earlier research on case adaptation in the field of case-based reasoning. In particular, Kass describes his research on abe <ref> [ Kass, 1989 ] </ref> as extending script/frame theory to handle a wider range of input situations; similarly, we are interested in flexibly relating past cases to new situations. Brain-stormer's cases, however, are much more abstract than abe's.
Reference: [ Kolodner et al., 1985 ] <author> J. L. Kolodner, R. L. Simpson, and K. Sycara-Cyranski. </author> <title> A process model of case-based reasoning in problem-solving. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <address> Los Angeles, CA. IJCAI. </address>
Reference-contexts: In principle, these systems are capable of solving a large number of problems, but in practice they make so many useless inferences that the number of problems they can actually solve under reasonable resource constraints remains small. Case-based reasoning has been proposed as a promising middle ground <ref> [ Hammond, 1986; Kolodner et al., 1985 ] </ref> . Like schema appliers, case-based reasoners trade completeness for efficiency. Unlike schema appliers, fl This research was supported in part by DARPA contract F49620-88-C-0058 and AFOSR contract 89-0493.
Reference: [ Moore and Newell, 1973 ] <author> J. Moore and A. Newell. </author> <title> How can merlin understand? In L. </title> <editor> W. Gregg, editor, </editor> <title> Knowledge and Cognition. </title> <publisher> Lawrence Erlbaum, </publisher> <address> NJ. </address>
Reference-contexts: Brainstormer's redescription mechanism relates to three areas of past work in cognitive science and ar tificial intelligence. First are investigations into the idea of viewing or redescription inference, starting with merlin <ref> [ Moore and Newell, 1973 ] </ref> and krl [ Bobrow and Winograd, 1977 ] in the 1970's. More recently, Ja-cobs has resurrected these ideas in the context of natural language generation [ Jacobs, 1987 ] .
Reference: [ Owens, 1988 ] <author> C. Owens. </author> <title> Domain-independent prototype cases for planning. </title> <booktitle> In Proceedings of a Workshop on Case-Based Reasoning, </booktitle> <pages> 302-311, </pages> <address> Clearwater, FL. </address> <publisher> Defense Advanced Research Projects Agency, Morgan Kaufmann. </publisher>
Reference-contexts: We have focused on proverbs for two reasons. First, proverbs are a well-defined class of abstract cases that encode useful culturally-shared knowledge about the idiosyncrasies of human planning and social interaction <ref> [ Owens, 1988; Schank, 1986; White, 1987 ] </ref> . The grass is always greener on the other side of the fence, for example, expresses a peculiarity of the human planning process: when comparing options, people tend to be biased in favor of the unfamiliar or the unpossessed.
Reference: [ Rieger, 1976 ] <author> C. Rieger. </author> <title> An organization of knowledge for problem solving and language comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 89-127. </pages>
Reference-contexts: At one extreme are systems that use radically incomplete inference procedures such as schema or script application [ Cullingford, 1977 ] , which although efficient, can only solve a limited class of problems. At the other extreme are systems that countenance multistep inference chaining <ref> [ Rieger, 1976; Wilensky, 1978 ] </ref> . In principle, these systems are capable of solving a large number of problems, but in practice they make so many useless inferences that the number of problems they can actually solve under reasonable resource constraints remains small.
Reference: [ Schank, 1986 ] <author> R. C. Schank. </author> <title> Explanation Patterns: Understanding Mechanically and Creatively. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: We have focused on proverbs for two reasons. First, proverbs are a well-defined class of abstract cases that encode useful culturally-shared knowledge about the idiosyncrasies of human planning and social interaction <ref> [ Owens, 1988; Schank, 1986; White, 1987 ] </ref> . The grass is always greener on the other side of the fence, for example, expresses a peculiarity of the human planning process: when comparing options, people tend to be biased in favor of the unfamiliar or the unpossessed.
Reference: [ Sowa, 1984 ] <author> J. F. Sowa. </author> <title> Conceptual Structures: Information Processing in Mind and Machines. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Usually, all possible concepts are hard-wired into a type hierarchy in advance, and the system can only categorize inputs as being instances of these fixed concepts. Brainstormer, in contrast, can dynamically extend its base set of concepts by the mechanism of -abstraction <ref> [ Sowa, 1984 ] </ref> : (a)(representations mentioning a) defines a new concept, whose instances are all of the a's that satisfy the conditions in the body of the lambda. In brainstormer, we represent - abstractions using the notation of views, which encode relationships between incommensurate representations of a single situation.
Reference: [ White, 1987 ] <author> G. M. White. </author> <title> Proverbs and cultural models. </title> <editor> In Dorothy Holland and N. Quinn, editors, </editor> <booktitle> Cultural Models in Language and Thought, </booktitle> <pages> 151-172. </pages> <publisher> Cambridge University Press, </publisher> <address> New York. </address>
Reference-contexts: In this paper, we consider a particular adaptation task: adapting abstract knowledge about planning to fit specific situations. A real-world activity in which this task arises is taking advice. People often communicate advice about planning in terms of high-level culturally-shared models of the planning process <ref> [ White, 1987 ] </ref> . The vocabulary of these models may be very different from the vocabulary of the representations the person actually uses to solve the problem. In other words, the initial form of the advice is not necessarily operational for problem solving. <p> We have focused on proverbs for two reasons. First, proverbs are a well-defined class of abstract cases that encode useful culturally-shared knowledge about the idiosyncrasies of human planning and social interaction <ref> [ Owens, 1988; Schank, 1986; White, 1987 ] </ref> . The grass is always greener on the other side of the fence, for example, expresses a peculiarity of the human planning process: when comparing options, people tend to be biased in favor of the unfamiliar or the unpossessed.
Reference: [ Wilensky, 1978 ] <author> R. Wilensky. </author> <title> Understanding Goal-Based Stories. </title> <type> PhD diss., </type> <institution> Yale Univ. </institution>
Reference-contexts: At one extreme are systems that use radically incomplete inference procedures such as schema or script application [ Cullingford, 1977 ] , which although efficient, can only solve a limited class of problems. At the other extreme are systems that countenance multistep inference chaining <ref> [ Rieger, 1976; Wilensky, 1978 ] </ref> . In principle, these systems are capable of solving a large number of problems, but in practice they make so many useless inferences that the number of problems they can actually solve under reasonable resource constraints remains small.
References-found: 18

