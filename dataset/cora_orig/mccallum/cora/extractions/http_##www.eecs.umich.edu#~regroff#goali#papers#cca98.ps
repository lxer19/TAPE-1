URL: http://www.eecs.umich.edu/~regroff/goali/papers/cca98.ps
Refering-URL: http://www.eecs.umich.edu/~regroff/goali/
Root-URL: http://www.eecs.umich.edu
Email: regroff@eecs.umich.edu kod@eecs.umich.edu pramod@eecs.umich.edu  
Title: Invertible Piecewise Linear Approximations for Color Reproduction  
Author: R. E. Groff D. E. Koditschek P. P. Khargonekar 
Address: 1301 Beal Ave., Ann Arbor, MI 48109-2122, USA  
Affiliation: EECS Department, University of Michigan  
Abstract: We consider the use of linear splines with variable knots for the approximation of unknown functions from data, motivated by control and estimation problems arising in color systems management. Unlike most popular nonlinear-in-parameters representations, piecewise linear (PL) functions can be simply inverted in closed form. For the one-dimensional case, we present a study comparing PL and neural network (NN) approximations for several function families. Preliminary results suggest that PL, in addition to their analytical benefits, are at least competitive with NN in terms of sum square error, computational effort, and training time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. R. Kang, </author> <title> Color Technology for Electronic Imagining Devices, </title> <publisher> SPIE Optical Engineering Press, </publisher> <year> 1997. </year>
Reference-contexts: The printer transforms the vector into a device-dependent space, CMY, which specifies the quantity of each pigment the printer must lay down in order to reproduce the desired color. This transformation is nonlinear and, approximate first principles models notwithstanding, must be computed in practice from empirical data <ref> [1] </ref>. When collecting data, each experiment consists of specifying a CMY vector to the printer and measuring the output in Lab coordinates. Thus, the empirical data is gathered using the inverse of the desired transformation. <p> In one dimension, the characterization of the piece 0 0.2 0.4 0.6 0.8 1 0.2 0.6 1 domain codomain (d 0 ,c 0 ) (d 2 ,c 2 ) (d 3 ,c 3 ) wise linear approximation is straightforward. Consider a PL with n line segments on the domain <ref> [0; 1] </ref> The PL is characterized by two vectors: d 2 R n+1 the vector of domain values, or knots, where 0 = d 0 &lt; d 1 &lt; ::: &lt; d n1 &lt; d n = 1, and c 2 R n+1 the vector of codomain values. <p> The PL and NN were given the same number of free parameters in order to study the relative performance per parameter. 3.1. Choice of function classes studied Five different function families from the class of home-omorphisms on <ref> [0; 1] </ref> were explored. That is, all functions mapped [0; 1] to [0; 1] and were continuous and invertible (i.e. monotonic). The five families fall into three groups: sigmoidal, piecewise linear and polynomial, chosen to "favor," respectively, NN, PL or neither. <p> The PL and NN were given the same number of free parameters in order to study the relative performance per parameter. 3.1. Choice of function classes studied Five different function families from the class of home-omorphisms on <ref> [0; 1] </ref> were explored. That is, all functions mapped [0; 1] to [0; 1] and were continuous and invertible (i.e. monotonic). The five families fall into three groups: sigmoidal, piecewise linear and polynomial, chosen to "favor," respectively, NN, PL or neither. Examples of typical functions from these families are presented in Figure 2. <p> The PL and NN were given the same number of free parameters in order to study the relative performance per parameter. 3.1. Choice of function classes studied Five different function families from the class of home-omorphisms on <ref> [0; 1] </ref> were explored. That is, all functions mapped [0; 1] to [0; 1] and were continuous and invertible (i.e. monotonic). The five families fall into three groups: sigmoidal, piecewise linear and polynomial, chosen to "favor," respectively, NN, PL or neither. Examples of typical functions from these families are presented in Figure 2. <p> The sigmoidal group contains superpositions of hyperbolic tangents, of the form f (x) = k 1 i=1 a i tanh (b i (x c i )) + k 2 (1) where a i and c i are distributed uniformly on <ref> [0; 1] </ref> and b i is exponentially distributed with mean 30. Then k 1 and k 2 are chosen such that f (0) = 0 and f (1) = 1. <p> The points x i and y i are chosen uniformly from <ref> [0; 1] </ref> for i = 1; :::; n1. The first family in the piecewise linear group has 10 line segments (m = 10), so, again all functions in this family lie within the parameter space of the PL approximation. <p> The second family has 30 line segments (m = 30), so the PL is underparameterized. The polynomial group consists of compositions of quadratic polynomials which satisfy f (0) = 0 and f (1) = 1 and are monotonically increasing. i.e. f 0 (x) 0 for x 2 <ref> [0; 1] </ref>. Quadratic polynomials satisfying these constraints can be parameterized as f ff (x) = (1 ff)x 2 + ffx (2) for ff 2 [0; 2]. Then the polynomial f = f ff 1 ffi f ff 2 ffi ::: ffi f ff m is indeed a homeomorphism of [0; 1], <p> 2 <ref> [0; 1] </ref>. Quadratic polynomials satisfying these constraints can be parameterized as f ff (x) = (1 ff)x 2 + ffx (2) for ff 2 [0; 2]. Then the polynomial f = f ff 1 ffi f ff 2 ffi ::: ffi f ff m is indeed a homeomorphism of [0; 1], since it is the composition of homeomorphic functions, and it has degree 2 m . The polynomial family presented here used m = 7 and the parameters ff i were distributed uniformly over [0; 1]. 3.2. <p> ffi f ff 2 ffi ::: ffi f ff m is indeed a homeomorphism of <ref> [0; 1] </ref>, since it is the composition of homeomorphic functions, and it has degree 2 m . The polynomial family presented here used m = 7 and the parameters ff i were distributed uniformly over [0; 1]. 3.2. Training methods The PL algorithm employed here minimizes square error via gradient descent. <p> The update law is then d k+1 i @d i c k+1 i @c i where the superscript is the iteration number and is the step size or "learning rate." The PL algorithm takes advantage of the fact that it is approximating a function from the class of homeomor-phisms on <ref> [0; 1] </ref> by fixing c 0 = 0 and c n = 1, in addition to d 0 = 0 and d n = 1. This reduces the number of free parameters for a PL with n line segments from 2n to 2 (n1).
Reference: [2] <author> J. Z. Chang, J. P. Allebach, and C. A. Bouman, </author> <title> "Sequential linear interpolation of multidimensional functions," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 92, no. 9, </volume> <pages> pp. 100, </pages> <year> 1997. </year>
Reference-contexts: Beyond interpolation on a uniform grid, one of the most sophisticated techniques for approximating color space transformations currently in use in the color industry is sequential linear interpolation <ref> [2] </ref>. This approach applies asymptotic analysis from information theory to find the optimal (nonuniform) grid point placement. Much attention has been given to various parameterizations of the space of approximations, especially nonlinear parameterizations such as Neural Networks (NN) and Radial Basis Function Networks (RBFN). <p> Quadratic polynomials satisfying these constraints can be parameterized as f ff (x) = (1 ff)x 2 + ffx (2) for ff 2 <ref> [0; 2] </ref>. Then the polynomial f = f ff 1 ffi f ff 2 ffi ::: ffi f ff m is indeed a homeomorphism of [0; 1], since it is the composition of homeomorphic functions, and it has degree 2 m .
Reference: [3] <author> A. R. Barron, </author> <title> "Universal approximation bounds for superpositions of a sigmoidal function," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 39, no. 3, </volume> <pages> pp. 930-945, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Much attention has been given to various parameterizations of the space of approximations, especially nonlinear parameterizations such as Neural Networks (NN) and Radial Basis Function Networks (RBFN). In higher dimensions, the convergence per parameter rates for such nonlinear families can potentially be better than linear-in-parameter function families <ref> [3] </ref>. Unfortunately, popular nonlinear families like NN and RBFN generally do not admit the "leveraging" of additional domain knowledge about the function, such as invertibility.
Reference: [4] <author> C. G. Atkeson, A. W. Moore, and S. Schaal, </author> <title> "Locally weighted learning," </title> <journal> Artificial Intelligence Review, </journal> <volume> vol. 11, no. </volume> <pages> 1-5, pp. 11-73, </pages> <month> Feb </month> <year> 1997. </year>
Reference-contexts: The printer physically realizes the inverse map and the application of control methodology seems most reasonable when working with the function most closely related to the physical system. A novel approximation method suggested by Atkeson and Schaal <ref> [4] </ref> uses a population of local "experts." Each "expert" is associated with an affine map and a Gaussian confidence. The "experts" vote on an output computed as the confidence weighted average of their affine components. Since the Gaussian bump has unbounded support, each expert has global influence.
Reference: [5] <author> D. L. Barrow, C. K. Chui, P. W. Smith, and J. D. Ward, </author> <title> "Unicity of best mean approximation by second order splines with variable knots," </title> <journal> Mathematics of Computation, </journal> <volume> vol. 32, no. 144, </volume> <pages> pp. 1131-1143, </pages> <month> October </month> <year> 1978. </year>
Reference-contexts: Piecewise polynomial representations (splines) can also be applied to function approximation problems. Qualitatively, these may be thought of as local "experts" which have partitioned the domain. Analytical results are available for the one dimensional case for certain function families <ref> [5] </ref>. Algorithms exist for one dimension [6] as well as for higher dimensions [7], although analytical results hold only for the one-dimensional case. Stone et al. present a statistical theory for the rate of L 2 convergence [8]. 2. <p> Figure 1 shows a PL with four line segments. Analytical results exist for the one-dimensional function approximation problem. In approximation, a completely known function is given and the objective is to find a PL which minimizes some norm, typically L 2 , of the error. Barrow et al. <ref> [5] </ref> provide some generalized convexity conditions which imply the existence of a unique best L 2 fit from the class of piecewise linear approximations. Gayle and Wolfe [6] provide similar results for approximation using higher order splines.
Reference: [6] <author> R. C. Gayle and J. M. Wolfe, </author> <title> "Unicity in piecewise polynomial L 1 -approximation via an algorithm," </title> <journal> Mathematics of Computation, </journal> <volume> vol. 65, no. 214, </volume> <pages> pp. 647-660, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Piecewise polynomial representations (splines) can also be applied to function approximation problems. Qualitatively, these may be thought of as local "experts" which have partitioned the domain. Analytical results are available for the one dimensional case for certain function families [5]. Algorithms exist for one dimension <ref> [6] </ref> as well as for higher dimensions [7], although analytical results hold only for the one-dimensional case. Stone et al. present a statistical theory for the rate of L 2 convergence [8]. 2. <p> Barrow et al. [5] provide some generalized convexity conditions which imply the existence of a unique best L 2 fit from the class of piecewise linear approximations. Gayle and Wolfe <ref> [6] </ref> provide similar results for approximation using higher order splines. Their proof uses an algorithm to calculate the best approximation over the domain of all knot vectors, for which global, unique convergence is shown via application of the contraction mapping theorem.
Reference: [7] <author> Y. Tourigny and M. J. Baines, </author> <title> "Analysis of an algorithm for generating locally optimal meshes for L 2 approximation by discontinuous piecewise polynomials," </title> <journal> Mathematics of Computation, </journal> <volume> vol. 66, no. 218, </volume> <pages> pp. 623-650, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Qualitatively, these may be thought of as local "experts" which have partitioned the domain. Analytical results are available for the one dimensional case for certain function families [5]. Algorithms exist for one dimension [6] as well as for higher dimensions <ref> [7] </ref>, although analytical results hold only for the one-dimensional case. Stone et al. present a statistical theory for the rate of L 2 convergence [8]. 2. <p> In higher dimensions the domain is partitioned into simplices: triangles in two dimensions, tetrahedra in three dimensions, and so on. Tourigny and Baines <ref> [7] </ref> present an algorithm for the two-dimensional function approximation problem, which can be generalized to higher dimensions. There are no corresponding analytical results. In order to produce an output for a given domain point in higher dimensions, the partition in which the domain point lies must first be identified.

References-found: 7

