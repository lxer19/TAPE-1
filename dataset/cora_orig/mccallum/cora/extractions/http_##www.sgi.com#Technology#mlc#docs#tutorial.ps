URL: http://www.sgi.com/Technology/mlc/docs/tutorial.ps
Refering-URL: http://www.sgi.com/Technology/mlc/docs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mlc@CS.Stanford.EDU  
Title: MLC Tutorial A Machine Learning library of C classes.  
Author: Ronny Kohavi 
Note: When to use MLC  
Date: November 24, 1995  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992), </year> <title> "Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms", </title> <journal> International Journal of Man-Machine Studies 36(1), </journal> <pages> 267-287. </pages>
Reference-contexts: Bottom up induction of OODGs Induce Oblivious read-Once Decision Graphs bottom up using the HOODG algorithm (Kohavi 1994). Nearest neighbor An implementation of Aha's IB1 and IB2 <ref> (Aha 1992) </ref> with many extensions and variations. Decision table An implementation of decision table induction algorithm, which is useful when couple with feature subset selection (Kohavi 1995). 1.2 Intended Audience MLC ++ is first of all a C ++ class library.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference: <author> Kohavi, R. </author> <year> (1994), </year> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations, </title> <booktitle> in "Twelfth National Conference on Artificial Intelligence", </booktitle> <pages> pp. 613-618. </pages>
Reference-contexts: Top down induction of decision trees Induce decision trees top down using the ID3 algo rithm (Quinlan 1986), or interface the commercially available C4.5 (Quinlan 1993). Bottom up induction of OODGs Induce Oblivious read-Once Decision Graphs bottom up using the HOODG algorithm <ref> (Kohavi 1994) </ref>. Nearest neighbor An implementation of Aha's IB1 and IB2 (Aha 1992) with many extensions and variations.
Reference: <author> Kohavi, R. </author> <year> (1995), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> "Proceedings of the European Conference on Machine Learning", Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 174-189. </pages>
Reference-contexts: Nearest neighbor An implementation of Aha's IB1 and IB2 (Aha 1992) with many extensions and variations. Decision table An implementation of decision table induction algorithm, which is useful when couple with feature subset selection <ref> (Kohavi 1995) </ref>. 1.2 Intended Audience MLC ++ is first of all a C ++ class library.
Reference: <author> Koutsofios, E. & North, S. C. </author> <year> (1994), </year> <title> Drawing graphs with dot, </title> <note> Available by anonymous ftp from research.att.com:dist/drawdag/dotdoc.ps.Z. </note>
Reference-contexts: MLC ++ allows generating X windows output by interfacing the dot and dotty program to display trees and graphs <ref> (Koutsofios & North 1994) </ref>. <p> There are no options for this preference. DotGraphPref Generate dot <ref> (Koutsofios & North 1994) </ref> format. The output can then be given to dot or dotty which can generate postscript, or display the graph on an X terminal. There are no options for this preference. DotPostscriptPref Generate postscript output directly using dot.
Reference: <author> Michalski, R. S. </author> <year> (1978), </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions, </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1995), </year> <note> UCI repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: 1 Introduction The MLC ++ library provides tools that can help students and researchers experiment with supervised learning algorithms. 1.1 Available Tools The library currently includes the following tools: Input/Output Read and write Quinlan's format (Quinlan 1993), which is a superset of Irvine's format <ref> (Murphy & Aha 1995) </ref>. Quinlan's format defines a machine readable file that allows automatic parsing of the data file. Accuracy estimation Besides holdout (train/test), which is the most common accuracy estimation method, MLC ++ supports cross-validation, stratified cross-validation, bootstrap .632, and generation of learning curves. <p> The file formats are based on Quinlan's C4.5 format (Quinlan 1993), which share the same data file format with the Irvine database <ref> (Murphy & Aha 1995) </ref>. Note that most Irvine datasets do not include a names file that defines the dataset schema (i.e., the attribute names and domains). The data file and test files contain labelled instances, one instance per line.
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> "Induction of decision trees", </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: They are similar to Karnaugh maps, but are generalized to non Boolean input/output. GLDs are described in Michalski (1978), Wnek, Sarma, Wahab & Michalski (1990), and Thrun et al. (1991). Top down induction of decision trees Induce decision trees top down using the ID3 algo rithm <ref> (Quinlan 1986) </ref>, or interface the commercially available C4.5 (Quinlan 1993). Bottom up induction of OODGs Induce Oblivious read-Once Decision Graphs bottom up using the HOODG algorithm (Kohavi 1994). Nearest neighbor An implementation of Aha's IB1 and IB2 (Aha 1992) with many extensions and variations. <p> If everything compiles correctly, you should see the following when executing a.out setupTest executing... setupTest OK If something does not work properly, see the troubleshooting guide (Appendix A on page 25). 4 Building an Induction Algorithm In this section, you will write a program that runs the ID3 induction algorithm <ref> (Quinlan 1986) </ref> on a training set. You will then display the induced tree, and finally, you will test the tree on a test set. No specific knowledge of the algorithm is needed to follow the tutorial. ID3 is an algorithm for top-down induction of decision trees (TDIDT).
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California. </address>
Reference-contexts: 1 Introduction The MLC ++ library provides tools that can help students and researchers experiment with supervised learning algorithms. 1.1 Available Tools The library currently includes the following tools: Input/Output Read and write Quinlan's format <ref> (Quinlan 1993) </ref>, which is a superset of Irvine's format (Murphy & Aha 1995). Quinlan's format defines a machine readable file that allows automatic parsing of the data file. <p> GLDs are described in Michalski (1978), Wnek, Sarma, Wahab & Michalski (1990), and Thrun et al. (1991). Top down induction of decision trees Induce decision trees top down using the ID3 algo rithm (Quinlan 1986), or interface the commercially available C4.5 <ref> (Quinlan 1993) </ref>. Bottom up induction of OODGs Induce Oblivious read-Once Decision Graphs bottom up using the HOODG algorithm (Kohavi 1994). Nearest neighbor An implementation of Aha's IB1 and IB2 (Aha 1992) with many extensions and variations. <p> The file formats are based on Quinlan's C4.5 format <ref> (Quinlan 1993) </ref>, which share the same data file format with the Irvine database (Murphy & Aha 1995). Note that most Irvine datasets do not include a names file that defines the dataset schema (i.e., the attribute names and domains).
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> "Selecting a classification method by cross-validation", </title> <booktitle> Machine Learning 13(1), </booktitle> <pages> 135-143. </pages>
Reference: <author> Thrun et al. </author> <year> (1991), </year> <title> The Monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Ullman, J. D. </author> <year> (1988), </year> <booktitle> Principles of database and knowledge-base systems, </booktitle> <volume> Vol. 1, </volume> <publisher> Computer Science Press. </publisher>

References-found: 12

