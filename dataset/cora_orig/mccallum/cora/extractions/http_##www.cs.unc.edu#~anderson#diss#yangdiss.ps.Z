URL: http://www.cs.unc.edu/~anderson/diss/yangdiss.ps.Z
Refering-URL: http://www.cs.unc.edu/~anderson/
Root-URL: http://www.cs.unc.edu
Title: Scalable Synchronization in Shared Memory Multiprocessing Systems  
Author: Anderson 
Degree: Jae-Heon Yang, Doctor of Philosophy, 1994 Dissertation directed by: Assistant Professor James  
Affiliation: Department of Computer Science  
Note: Abstract Title of Dissertation:  H.  
Abstract: It is our thesis that scalable synchronization can be achieved with only minimal hardware support, specifically read/write atomicity. This is contrary to the conventional viewpoint that stronger hardware support is required for scalable synchronization; such support not only requires additional cost, but also leads to portability problems. As evidence in support of our thesis, we present a new scalable mutual exclusion algorithm based on read and write instructions. The performance of this algorithm is better than prior mutual exclusion algorithms based on read/write atomicity, and even rivals that of the fastest mutual exclusion algorithms that require stronger primitives. Our algorithm is based on the technique of local spinning, i.e., busy-waiting on variables that are locally-accessible to the waiting process. Local-spinning minimizes remote accesses of shared memory, which tend to dominate performance under heavy contention. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Afek, H. Attiya, D. Dolev, E. Gafni, M. Merritt, and N. Shavit, </author> <title> "Atomic Snapshots of Shared Memory", </title> <booktitle> Proceedings of the Ninth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1990, </year> <pages> pp. 1-14. </pages>
Reference-contexts: In the arbitration tree, process 0 first competes with process 1 at level 0, and then with one of processes 2 and 3 at level 1. At level 1 in the tree, both processes 2 and 3 access C <ref> [1; 1] </ref> and T [1; 0]. However, at this level, each still has a unique spin location, namely P [1; 2] and P [1; 3], respectively. <p> In the arbitration tree, process 0 first competes with process 1 at level 0, and then with one of processes 2 and 3 at level 1. At level 1 in the tree, both processes 2 and 3 access C [1; 1] and T <ref> [1; 0] </ref>. However, at this level, each still has a unique spin location, namely P [1; 2] and P [1; 3], respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. <p> At level 1 in the tree, both processes 2 and 3 access C [1; 1] and T [1; 0]. However, at this level, each still has a unique spin location, namely P <ref> [1; 2] </ref> and P [1; 3], respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. Otherwise, for example, process 0 might update the spin location for process 2, P [1; 2], <p> At level 1 in the tree, both processes 2 and 3 access C [1; 1] and T [1; 0]. However, at this level, each still has a unique spin location, namely P [1; 2] and P <ref> [1; 3] </ref>, respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. <p> P <ref> [1; 2] </ref> and P [1; 3], respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. Otherwise, for example, process 0 might update the spin location for process 2, P [1; 2], when in fact it is competing against process 3. This could result in a violation of either mutual exclusion or starvation-freedom. As seen in Figures 2.2 and 2.3, this "identity problem" is handled by means of the rival variable. <p> Consider the following computation. H 0 = H Y 3 ffi L (1) ffi L (2) ffi ffi L (jY 3j) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R jY 3j ; W jY 3j ; jY 3j]i We will use H 0 to construct the computation G mentioned at the beginning of the proof. <p> We show this below. Without loss of generality, assume the processes are numbered so that Z = f1; 2; : : : ; jZjg. The computation G we seek is defined as follows. G = H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R jZj ; W jZj ; jZj]i Observe that, because H 0 satisfies (C2) through (C4), G also satisfies (C2) through (C4). We now show that G satisfies (C1). 102 Condition (C1). <p> Hence, we conclude that G satisfies (C1). 2 To complete the proof of the lemma, we need to show that G is actually a computation in C. This is established in the following claim. Claim 3.1. G 2 C. Proof: The proof is by induction on the subsequence h <ref> [R 1 ; W 1 ; 1] </ref>; : : : ; [R jZj ; W jZj ; jZj]i. Induction Base. We use Lemmas 3.2, 3.3, and 3.4 to establish the base case. Because H satisfies (C1), by Lemma 3.2, H Z 2 C. Consider j 2 Z. <p> By Lemma 3.4, it follows that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) 2 C. Induction Hypothesis. Assume that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; Induction Step. We use (P2) to prove that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W <p> (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; Induction Step. We use (P2) to prove that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : : ; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. <p> ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : : ; Consider x in R j :var. <p> Because G satisfies (C1), x is not written by a process other than j in H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R j1 ; W j1 ; j 1]i. <p> Hence, we have the following. (8x : x 2 R j :var :: value (x; H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R j1 ; W j1 ; j 1]i) = value (x; H ffi L (j))) (3.19) By (3.13), (3.17), (3.18), (3.19), and (P2), we conclude that H Z ffi L (1)ffiL (2)ffi ffiL <p> ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : : ; [R j1 ; W j1 ; j 1]i) = value (x; H ffi L (j))) (3.19) By (3.13), (3.17), (3.18), (3.19), and (P2), we conclude that H Z ffi L (1)ffiL (2)ffi ffiL (jZj)ffih <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R j ; W j ; j]i 2 C. 2 By construction, each process in Z executes r + 1 remote events in G. <p> By (3.20), we have jZj d (n 1)=(2v + 1)vce. The computation G we seek is defined as follows. G = H Z ffi F (1) ffi F (2) ffi ffi F (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : : ; [R jZj ; W jZj ; jZj]i Because H satisfies (C5), H also satisfies (C1). Thus, by Lemma 3.2, H Z 2 C.
Reference: [2] <author> A. Agarwal and M. Cherian, </author> <title> "Adaptive Backoff Synchronization Techniques", </title> <booktitle> Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <month> May, </month> <year> 1989, </year> <pages> pp. 396-406. </pages>
Reference-contexts: At level 1 in the tree, both processes 2 and 3 access C [1; 1] and T [1; 0]. However, at this level, each still has a unique spin location, namely P <ref> [1; 2] </ref> and P [1; 3], respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. Otherwise, for example, process 0 might update the spin location for process 2, P [1; 2], <p> P <ref> [1; 2] </ref> and P [1; 3], respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. Otherwise, for example, process 0 might update the spin location for process 2, P [1; 2], when in fact it is competing against process 3. This could result in a violation of either mutual exclusion or starvation-freedom. As seen in Figures 2.2 and 2.3, this "identity problem" is handled by means of the rival variable. <p> The average execution time for the 64 processor case, which is not depicted in Figure 2.6, is about 330 microseconds. Where there is a possibility of contention among a large number of processors, it should be avoided, or used with good backoff scheme <ref> [2] </ref>. Three algorithms based on atomic reads and writes | Lamport's, Peterson and Fischer's, and Styer's | also showed poor scalability. In particular, the performance of Lamport's algorithm degrades dramatically as the number of contenders increases. <p> Consider the following computation. H 0 = H Y 3 ffi L (1) ffi L (2) ffi ffi L (jY 3j) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R jY 3j ; W jY 3j ; jY 3j]i We will use H 0 to construct the computation G mentioned at the beginning of the proof. <p> The computation G we seek is defined as follows. G = H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R jZj ; W jZj ; jZj]i Observe that, because H 0 satisfies (C2) through (C4), G also satisfies (C2) through (C4). We now show that G satisfies (C1). 102 Condition (C1). <p> By Lemma 3.4, it follows that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) 2 C. Induction Hypothesis. Assume that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; Induction Step. We use (P2) to prove that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : : ; <p> (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; Induction Step. We use (P2) to prove that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : : ; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. <p> ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : : ; Consider x in R j :var. <p> Because G satisfies (C1), x is not written by a process other than j in H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R j1 ; W j1 ; j 1]i. <p> Hence, we have the following. (8x : x 2 R j :var :: value (x; H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R j1 ; W j1 ; j 1]i) = value (x; H ffi L (j))) (3.19) By (3.13), (3.17), (3.18), (3.19), and (P2), we conclude that H Z ffi L (1)ffiL (2)ffi ffiL (jZj)ffih [R 1 ; W 1 ; <p> ; W 2 ; 2]; : : : ; [R j1 ; W j1 ; j 1]i) = value (x; H ffi L (j))) (3.19) By (3.13), (3.17), (3.18), (3.19), and (P2), we conclude that H Z ffi L (1)ffiL (2)ffi ffiL (jZj)ffih [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R j ; W j ; j]i 2 C. 2 By construction, each process in Z executes r + 1 remote events in G. As shown above, G satisfies conditions (C1) through (C4). <p> By (3.20), we have jZj d (n 1)=(2v + 1)vce. The computation G we seek is defined as follows. G = H Z ffi F (1) ffi F (2) ffi ffi F (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : : ; [R jZj ; W jZj ; jZj]i Because H satisfies (C5), H also satisfies (C1). Thus, by Lemma 3.2, H Z 2 C. It is straightforward to use this fact to prove that G 2 C.
Reference: [3] <author> R. Alur and G. Taubenfeld, </author> <title> "Results about Fast Mutual Exclusion", </title> <booktitle> Proceedings of the Thirteenth IEEE Real-Time Systems Symposium, </booktitle> <month> December, </month> <year> 1992, </year> <pages> pp. 12-21. </pages>
Reference-contexts: At level 1 in the tree, both processes 2 and 3 access C [1; 1] and T [1; 0]. However, at this level, each still has a unique spin location, namely P [1; 2] and P <ref> [1; 3] </ref>, respectively. If process 0 encounters contention when competing at level 1, then it is important that process 0 knows precisely which of 2 or 3 it is competing against. <p> It is interesting to note that there exist read/write mutual exclusion algorithms with write-contention N that have O (1) time complexity in the absence of competition <ref> [3, 37, 60] </ref>. Thus, establishing the above-mentioned lower bound for read/write algorithms will require proof techniques that differ from those given in Chapter 2. We do not know whether the bound of Theorem 3.5 is tight.
Reference: [4] <author> J. Anderson, </author> <title> "Multi-Writer Composite Registers", </title> <journal> Distributed Computing, </journal> <volume> Vol. 7, </volume> <year> 1994, </year> <pages> pp. 175-195. </pages>
Reference: [5] <author> J. Anderson, </author> <title> "Composite Registers", </title> <journal> Distributed Computing, </journal> <volume> Vol. 6, </volume> <year> 1993, </year> <pages> pp. 141-154. </pages>
Reference: [6] <author> J. Anderson, </author> <title> "A Fine-Grained Solution to the Mutual Exclusion Problem", </title> <journal> Acta Informatica, </journal> <volume> Vol. 30, No. 3, </volume> <year> 1993, </year> <pages> pp. 249-265. 147 </pages>
Reference-contexts: This des-ignation is somewhat of a misnomer, as such algorithms are not necessarily fast in the presence of contention. In fact, the problem of designing a scalable algorithm requiring only read/write atomicity has remained open. In this chapter, we present such an algorithm. In a recent paper <ref> [6] </ref>, Anderson presented a mutual exclusion algorithm that uses only local spins and that requires only atomic read and write operations. In his algorithm, each of N processes executes O (N ) remote operations to enter its critical section whether there is contention or not. <p> Our algorithm induces O (log N ) remote operations under any amount of contention, and thus is an improvement over the algorithm given by Anderson in <ref> [6] </ref>. We also present a modified version of this algorithm that requires only O (1) remote operations in the absence of contention. Unfortunately, in this modified algorithm, worst-case complexity rises to O (N ). <p> This could result in a violation of either mutual exclusion or starvation-freedom. As seen in Figures 2.2 and 2.3, this "identity problem" is handled by means of the rival variable. Because of this problem, the two-process local-spin algorithm given by Anderson in <ref> [6] </ref> cannot be readily applied within an arbitration tree to get an N -process local-spin algorithm. 29 With regard to complexity, note that if variable P [i] is local to process i in the two process algorithm, then process i executes a constant number of remote operations in its two-process entry <p> Thus, we have the following corollary. 89 Corollary 3.1: For any system S satisfying the conditions of Theorem 3.1, there exist (N ) processes i in P for which the conclusion of the theorem holds. 2 Similar corollaries apply to the theorems in the following sections. In <ref> [6] </ref>, a mutual exclusion algorithm requiring O (N ) remote memory references per critical section acquisition is given that employs only single-reader, single-writer variables. Thus, if v and k are taken to be positive constants, then the bound of Theorem 3.1 is asymptotically tight. <p> Because waiting in any form is precluded in such implementations, both classes of objects are clearly restricted to allow only operations that may read or write shared variables. In a recent paper <ref> [6] </ref>, Anderson showed that any object that allow only operations of the form "S" can be implemented from single-reader, single-writer variables, without busy-waiting on remote variables. In this chapter, we extend past work on object implementations by considering operations with enabling conditions. <p> Anderson has shown in <ref> [6] </ref> that the mutual exclusion problem can be solved without global busy-waiting using only single-reader, single-writer, boolean variables. <p> Because such an order is statically kept in the program, it is straightforward to show that the following assertion holds, which implies that the Progress requirement holds. 140 (8i :: i@f1::7g 7! i@f8g _ :B [i]) 4.5 Discussion Anderson's results <ref> [6] </ref> imply that objects with unconditional operations are implementable by single-reader, single-writer variables, in distributed shared memory machines and cache-coherent machines. Our results show that objects with conditional operations are also implementable by single-reader, single-writer variables, in distributed shared memory machines.
Reference: [7] <author> J. Anderson and M. Gouda, </author> <title> "A Criterion for Atomicity", </title> <journal> Formal Aspects of Computing: The International Journal of Formal Methods, </journal> <volume> Vol. 4, No. 3, </volume> <month> May, </month> <year> 1992, </year> <pages> pp. 273-298. </pages>
Reference: [8] <author> J. Anderson and B. Groselj, </author> <title> "Beyond Atomic Registers: Bounded Wait-Free Implementations of Nontrivial Objects", </title> <booktitle> Science of Computer Programming, </booktitle> <volume> Vol. 19, No. 3, </volume> <month> December, </month> <year> 1992, </year> <pages> pp. 197-237. </pages>
Reference: [9] <author> T. Anderson, </author> <title> "The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 1, </volume> <month> January, </month> <year> 1990, </year> <pages> pp. 6-16. </pages>
Reference-contexts: Examples of such mechanisms | hereafter called strong primitives | include the fetch-and-store, compare-and-swap, and fetch-and-add instructions. Early solutions based on strong primitives, such as the familiar test-and-set lock, were conceptually simple, but resulted in somewhat poor performance. More recently, queue-based spin locks have been proposed by Anderson <ref> [9] </ref>, by Graunke and Thakkar [26], and by Mellor-Crummey and Scott [45] that exhibit better performance; these locks are implemented using fetch-and-add, fetch-and-store, or compare-and-swap instructions, respectively. <p> We tested seven mutual exclusion algorithms on the TC2000: a simple test-and-set algorithm; the queue-based algorithm using compare-and-swap given by Mellor-Crummey and Scott in [45]; the queue-based algorithm using fetch-and-add given by T. Anderson in <ref> [9] </ref>; the fast mutual exclusion algorithm given by Lamport in [37]; the tree-based algorithm given by Styer in [56]; the tree-based 72 73 algorithm given by Peterson and Fischer in [52]; and the mutual exclusion algo-rithm described in Section 2.3. <p> Because other strong primitives are not provided, we used a version of Mellor-Crummey and Scott's algorithm that is implemented with fetch-and-store and that does not ensure starvation-freedom [45]. Fetch-and-add, which is used in T. Anderson's algorithm, was simulated by a test-and-set algorithm with randomized backoff, as Anderson did in <ref> [9] </ref>. The experiments on the Symmetry show similar results to that for the TC2000. However, on the Symmetry, T. Anderson's algorithm has the best overall performance, mainly because the availability of coherent caches makes all spins in his algorithm local. <p> In one of our tests for the two-process case, one process executed 50,000 critical sections during a period of time in which the other process executed only 120 critical sections. 76 77 Dependence on coherent caching for efficient synchronization <ref> [9, 26] </ref> is ques-tionable, as many caching schemes do not cache shared writable data. Our solution neither requires a coherent cache for efficient implementation nor any strong primitives. <p> Limiting access-contention is an important consideration when designing algorithms for problems, such as mutual exclusion and shared counting, that must cope well with high competition among processes <ref> [9, 28, 29, 53] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [10] <author> G. Andrews, </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice, </booktitle> <publisher> The Ben-jamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: The execution of such a program may be considered as an interleaving of the executions of its component processes. When processes interact, not all possible interleavings are desirable. In such instances, processes must 1 be synchronized to prevent unacceptable interleavings <ref> [10] </ref>. Synchronization is not without its cost; it almost always decreases the level of concurrency, and hence degrades performance. A synchronization algorithm is said to be scalable if increasing the number of processes to be synchronized does not degrade performance dramatically. <p> In particular, consider the program given in Figure 4.2, which is taken from <ref> [10] </ref>; in this program, ENTRY and EXIT denote entry and exit sections from an N -process solution to the mutual exclusion problem. In order to execute its critical section, process i repeatedly executes ENTRY and EXIT, checking B [i] in between.
Reference: [11] <author> J. Archibald and J. Baer, </author> <title> "An Economical Solution to the Cache Coherence Problem", </title> <booktitle> Proceedings of the 12th International Symposium on Computer Architecture, </booktitle> <month> June, </month> <year> 1985, </year> <pages> pp. 355-362. </pages>
Reference-contexts: However, caching shared variables may introduce inconsistent copies of a shared variable. In order to maintain consistency among multiple copies of a variable (in a memory module and possibly multiple caches), a cache-coherence protocol must be provided by hardware, or by software, or by a combination of both <ref> [11, 15, 19] </ref>. When a new value is written to a cached variable, a cache-coherence protocol either invalidates other copies of the variable, or updates those copies to the new value. The Sequent Symmetry is an example of a cache-coherent shared-memory multiprocessor [55].
Reference: [12] <author> J. Aspens and M. Herlihy, </author> <title> "Wait-Free Data Structures in the Asynchronous PRAM Model", </title> <booktitle> Proceedings of the Second Annual ACM Symposium on Parallel Architectures and Algorithms, </booktitle> <month> July, </month> <year> 1990. </year>
Reference: [13] <institution> BBN Advanced Computers, Inside the TC2000 Computer, </institution> <month> February, </month> <year> 1990. </year>
Reference-contexts: These references are served without the longer latency of remote references, resulting in reduced bandwidth demands on the global interconnect [41]. The BBN TC2000 is an example of a distributed shared-memory multiprocessor <ref> [13] </ref>. The TC2000 consists of a number of nodes, each of which contains a processor and a memory unit. The nodes are connected via a multi-stage interconnection network, known as the Butterfly switch. <p> Each node's processor, a Mo-torola 88100, provides an atomic fetch-and-store instruction called xmem. Other strong primitives such as compare-and-swap and fetch-and-add are provided using the TC2000 hardware locking protocol <ref> [13] </ref>. We tested seven mutual exclusion algorithms on the TC2000: a simple test-and-set algorithm; the queue-based algorithm using compare-and-swap given by Mellor-Crummey and Scott in [45]; the queue-based algorithm using fetch-and-add given by T.
Reference: [14] <author> G. Bell, </author> <title> "Ultracomputers: A Teraflop Before Its Time", </title> <journal> Communications of the ACM , Vol. </journal> <volume> 35, No. 8, </volume> <year> 1992, </year> <pages> pp. 26-47. 148 </pages>
Reference-contexts: The interconnection network of the KSR1 is formed of hierarchical rings. Scalability is achieved by connecting 32 processors to a ring that operates at one GB/sec. Interconnection bandwidth within a ring scales linearly, because every ring slot may contain a transaction <ref> [14] </ref>. The current KSR1 machine uses a two level hierarchy to interconnect 34 rings, and scales up to 1088 processors. As the hierarchical ring may have an arbitrary number of levels, more processors could be added. A unique feature of the KSR1 multiprocessor is its ALLCACHE 1 mechanism.
Reference: [15] <author> P. Bitar and A. Despain, </author> <title> "Multiprocessor Cache Synchronization Issues, Innovations, Evolution", </title> <booktitle> Proceedings of the 13th International Symposium on Computer Architecture, </booktitle> <month> June, </month> <year> 1986, </year> <pages> pp. 424-433. </pages>
Reference-contexts: However, caching shared variables may introduce inconsistent copies of a shared variable. In order to maintain consistency among multiple copies of a variable (in a memory module and possibly multiple caches), a cache-coherence protocol must be provided by hardware, or by software, or by a combination of both <ref> [11, 15, 19] </ref>. When a new value is written to a cached variable, a cache-coherence protocol either invalidates other copies of the variable, or updates those copies to the new value. The Sequent Symmetry is an example of a cache-coherent shared-memory multiprocessor [55].
Reference: [16] <author> B. Bloom, </author> <title> "Constructing Two-Writer Atomic Registers", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 37, No. 12, </volume> <month> December, </month> <year> 1988, </year> <pages> pp. 1506-1514. </pages>
Reference: [17] <author> J. Burns and N. Lynch, </author> <title> "Bounds on Shared Memory for Mutual Exclusion", </title> <journal> Information and Computation, </journal> <volume> Vol. 107, </volume> <year> 1993, </year> <pages> pp. 171-184. </pages>
Reference-contexts: Past work on the complexity of mutual exclusion has almost exclusively focused on space requirements <ref> [17] </ref>; the limited work on time bounds that has been done has focused on partially synchronous models [43]. The lack of prior work on time bounds for mutual exclusion within asynchronous models is probably due to difficulties associated with measuring the time spent within busy-waiting constructs.
Reference: [18] <author> J. Burns and G. Peterson, </author> <title> "Constructing Multi-Reader Atomic Values from Non-Atomic Values", </title> <booktitle> Proceedings of the Sixth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1987, </year> <pages> pp. 222-231. </pages>
Reference: [19] <author> M. Censier and P. Feautier, </author> <title> "A New Solution to Coherence Problems in Multicache Systems", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 27, No. 12, </volume> <month> December, </month> <year> 1978, </year> <pages> pp. 1112-1118. </pages>
Reference-contexts: However, caching shared variables may introduce inconsistent copies of a shared variable. In order to maintain consistency among multiple copies of a variable (in a memory module and possibly multiple caches), a cache-coherence protocol must be provided by hardware, or by software, or by a combination of both <ref> [11, 15, 19] </ref>. When a new value is written to a cached variable, a cache-coherence protocol either invalidates other copies of the variable, or updates those copies to the new value. The Sequent Symmetry is an example of a cache-coherent shared-memory multiprocessor [55].
Reference: [20] <author> K. Chandy and J. Misra, </author> <title> Parallel Program Design: A Foundation, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: This assumption is reflective of a distributed shared memory model. We refer to a statement execution as an operation. An operation is remote if it accesses remote variables, and is local otherwise. Following <ref> [20] </ref>, we define safety properties using unless assertions and progress properties using leads-to assertions. Consider two predicates P and Q over the variables of a program.
Reference: [21] <author> E. Dijkstra, </author> <title> "Solution of a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM , Vol. </journal> <volume> 8, No. 9, </volume> <year> 1965, </year> <pages> pp. 569. </pages>
Reference-contexts: Algorithms that provide mutual exclusion by busy-waiting are commonly called spin locks. The mutual exclusion problem was first formally stated and solved in a seminal paper by Dijkstra <ref> [21] </ref>. In this problem, each of a set of processes repeatedly executes a program fragment known as its critical section. Before executing its critical section, a process must first execute another program fragment, its "entry section", and upon termination of its critical section, a third program fragment, its "exit section".
Reference: [22] <author> E. Dijkstra, </author> <title> A Discipline of Programming, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference: [23] <author> C. Dwork, M. Herlihy, and O. Waarts, </author> <title> "Contention in Shared Memory Algorithms", </title> <booktitle> Proceedings of the 25th ACM Symposium on Theory of Computing, </booktitle> <month> May, </month> <year> 1993, </year> <pages> pp. 174-183. 149 </pages>
Reference-contexts: In Section 3.4, we present an algorithm that provides us upper bounds matching these lower bounds for arbitrary contention. Related work includes previous research by Dwork et al. given in <ref> [23] </ref>, where 82 it is shown that solving mutual exclusion with access-contention c requires ((log 2 N )=c) memory references. Our work extends that of Dwork et al. in several directions. First, the implications concerning fast mutual exclusion and cache coherence noted above do not follow from their work.
Reference: [24] <author> A. Glew and W. Hwu, </author> <title> "A Feature Taxonomy and Survey of Synchronization Primitive Implementations", </title> <type> Technical Report UILU-ENG-91-2211, </type> <institution> Center for Reliable and High-Performance Computing, University of Illinois at Urbana-Champaign, </institution> <month> February, </month> <year> 1991. </year>
Reference-contexts: Such mechanisms, however, require additional hardware support. For example, Silicon Graphics' PowerSeries machines are built using a multiple number of R2000/R3000 microprocessors, which do not have any strong primitives. In order to implement test-and-set, a synchronization memory with a special interconnect was inevitably developed <ref> [24] </ref>. In addition, algorithms based on strong primitives may be inefficient on machines that do not support such primitives in hardware. Because different architectures are likely to provide different sets of strong primitives, algorithms based on such primitives are often of limited portability.
Reference: [25] <author> J. Goodman, M. Vernon, and P. Woest, </author> <title> "Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors", </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1989, </year> <pages> pp. 64-75. </pages>
Reference-contexts: In addition to the software-based solutions to the mutual exclusion problem described above, a number of hardware-based implementations have been proposed. Of particular interest are the solutions given by Goodman, Vernon, and Woest in <ref> [25] </ref>, and by Lee and Ramachandran in [39], which exploit underlying cache coherence mechanisms. These implementations involve the construction of a distributed queue in hardware.
Reference: [26] <author> G. Graunke and S. Thakkar, </author> <title> "Synchronization algorithms for shared-memory multiprocessors", </title> <journal> IEEE Computer , Vol. </journal> <volume> 23, </volume> <month> June, </month> <year> 1990, </year> <pages> pp. 60-69. </pages>
Reference-contexts: Early solutions based on strong primitives, such as the familiar test-and-set lock, were conceptually simple, but resulted in somewhat poor performance. More recently, queue-based spin locks have been proposed by Anderson [9], by Graunke and Thakkar <ref> [26] </ref>, and by Mellor-Crummey and Scott [45] that exhibit better performance; these locks are implemented using fetch-and-add, fetch-and-store, or compare-and-swap instructions, respectively. <p> In one of our tests for the two-process case, one process executed 50,000 critical sections during a period of time in which the other process executed only 120 critical sections. 76 77 Dependence on coherent caching for efficient synchronization <ref> [9, 26] </ref> is ques-tionable, as many caching schemes do not cache shared writable data. Our solution neither requires a coherent cache for efficient implementation nor any strong primitives.
Reference: [27] <author> M. Herlihy, </author> <title> "Wait-Free Synchronization", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 13, No. 1, </volume> <year> 1991, </year> <pages> pp. 124-149. </pages>
Reference-contexts: In this section, we define the notion of an implementation formally. Except for modifications to handle liveness conditions, our notion of implementation is similar to that given by Herlihy in <ref> [27] </ref>, which is based on the I/O automata of Lynch and Tuttle [44]. The following description is adopted from Herlihy [27]. 4.2.1 I/O automata We model concurrent programs using a simplified form of I/O automata [44]. <p> Except for modifications to handle liveness conditions, our notion of implementation is similar to that given by Herlihy in <ref> [27] </ref>, which is based on the I/O automata of Lynch and Tuttle [44]. The following description is adopted from Herlihy [27]. 4.2.1 I/O automata We model concurrent programs using a simplified form of I/O automata [44]. I/O automata provide a convenient way for describing what it means for one object to implement another. <p> Note this definition corresponds to weak fairness of every action. 2 Unless otherwise noted, we henceforth assume that all histories are fair. Let I j be an implementation of A j . Following Lynch and Tuttle [44] and Herlihy <ref> [27] </ref>, we say that I j is correct, if for every fair history H of every system fP 1 ; : : : ; P n ; A 1 ; : : : ; I j ; : : : ; A m g, there exists a fair history H 0 <p> In particular, we showed that any shared object, no matter how complicated, can be implemented from single-reader, single-writer variables without global busy-waiting on distributed shared memory multiprocessors. 144 5.2 Future Research For wait-free algorithms, Herlihy has characterized synchronization primitives by consensus number <ref> [27] </ref>. Such a characterization is not applicable when waiting is introduced. One way of determining the power of synchronization primitives in this case is to compare the time complexity of mutual exclusion using such primitives.
Reference: [28] <author> M. Herlihy, B. Lim, and N. Shavit, </author> <title> "Low Contention Load Balancing on Large-Scale Multiprocessors", </title> <booktitle> Proceedings of Symposium on Parallel Algorithms and Architectures '92 , 1992, </booktitle> <pages> pp. 219-227. </pages>
Reference-contexts: Limiting access-contention is an important consideration when designing algorithms for problems, such as mutual exclusion and shared counting, that must cope well with high competition among processes <ref> [9, 28, 29, 53] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [29] <author> M. Herlihy, N. Shavit, and O. Waarts, </author> <title> "Low Contention Linearizable Counting", </title> <booktitle> Proceedings of the 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October, </month> <year> 1991, </year> <pages> pp. 526-535. </pages>
Reference-contexts: Limiting access-contention is an important consideration when designing algorithms for problems, such as mutual exclusion and shared counting, that must cope well with high competition among processes <ref> [9, 28, 29, 53] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [30] <author> M. Herlihy and J. Wing, </author> <title> "Linearizability: A Correctness Condition for Concurrent Objects", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 12, No. 3, </volume> <year> 1990, </year> <pages> pp. 463-492. </pages>
Reference: [31] <author> A. Israeli and M. Li, </author> <title> "Bounded time-stamps", </title> <booktitle> Proceedings of the 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1987, </year> <pages> pp. 371-382. 150 </pages>
Reference: [32] <author> J. Kessels, </author> <title> "Arbitration Without Common Modifiable Variables", </title> <journal> Acta In--formatica, </journal> <volume> Vol. 17, </volume> <year> 1982, </year> <pages> pp. 135-141. </pages>
Reference-contexts: The first starvation-free solution was presented by Knuth in [34]. Of the many early solutions to the mutual exclusion problem, most are quite complicated and difficult to understand. A notable exception is an especially simple solution first presented by Peterson in [50] and later refined by Kessels in <ref> [32] </ref>. The approach taken by Kessels was to first solve the mutual exclusion problem for two processes, and to then use the two-process solution in a binary arbitration tree to solve the N - process case. <p> Most such algorithms also require O (N ) remote operations in the absence of contention. Some exceptions to the latter include the algorithm given by Kessels in <ref> [32] </ref> and the previously mentioned one given by Lamport in [37]. Kessels' algorithm generates O (log N ) remote operations in the absence of contention, while Lamport's generates O (1). A variant of Lamport's algorithm has recently been presented by Styer in [56]. <p> We also require that each process in its exit section eventually enters its noncritical section; this requirement is trivially satisfied by our solution (and most others), so we will not consider it further. As in <ref> [32] </ref>, we first solve the mutual exclusion problem for two processes, and then apply our two-process solution in a binary arbitration tree to get an N -process solution. The two-process algorithm is depicted in Figure 2.1.
Reference: [33] <author> L. Kirousis, E. Kranakis, and P. Vitanyi, </author> <title> "Atomic Multireader Register", </title> <booktitle> Proceedings of the Second International Workshop on Distributed Computing, Lecture Notes in Computer Science 312, </booktitle> <pages> pp. 278-296, </pages> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference: [34] <author> D. Knuth, </author> <title> "Additional Comments on a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM , Vol. </journal> <volume> 9, No. 5, </volume> <year> 1966, </year> <pages> pp. 321-322. </pages>
Reference-contexts: Dijkstra's original solution is depicted in Figure 1.3. His algorithm satisfied the livelock-freedom property, but not the starvation-freedom property. The first starvation-free solution was presented by Knuth in <ref> [34] </ref>. Of the many early solutions to the mutual exclusion problem, most are quite complicated and difficult to understand. A notable exception is an especially simple solution first presented by Peterson in [50] and later refined by Kessels in [32].
Reference: [35] <institution> Kendall Square Research, Technical Summary, Walthan, Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: If a variable is written, every processor that has a copy of that variable in its local cache either invalidates or updates its copy. The Kendall Square Research KSR1 multiprocessor is another example of a cache-coherent shared-memory multiprocessor <ref> [35] </ref>. The interconnection network of the KSR1 is formed of hierarchical rings. Scalability is achieved by connecting 32 processors to a ring that operates at one GB/sec. Interconnection bandwidth within a ring scales linearly, because every ring slot may contain a transaction [14].
Reference: [36] <author> L. Lamport, </author> <title> "On Interprocess Communication, Parts I and II", </title> <journal> Distributed Computing, </journal> <volume> Vol. 1, </volume> <year> 1986, </year> <pages> pp. 77-101. </pages>
Reference-contexts: We show that very simple fine-grained objects suffice, particularly single-reader, single-writer variables. 123 Recent work on wait-free synchronization has dealt with the implementation of objects that are only read or written. The seminal work on this subject is Lamport's paper on interprocess communication <ref> [36] </ref>; other representative papers include [1, 4, 5, 8, 12, 16, 18, 31, 33, 42, 48, 51, 54, 57, 59]. In work on wait-free synchronization, the central problem is that of implementing one class of objects from another class of objects without any waiting.
Reference: [37] <author> L. Lamport, </author> <title> "A Fast Mutual Exclusion Algorithm", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 5, No. 1, </volume> <month> February, </month> <year> 1987, </year> <pages> pp. 1-11. </pages>
Reference-contexts: Because the competition for critical section is likely to be low in many well-designed systems, achieving fast mutual exclusion in the absence of competition is of great practical interest <ref> [37] </ref>. In most shared-memory multiprocessors, an atomic operation may access only a constant number of remote variables. In fact, most commonly-available synchronization primitives (e.g., read, write, test-and-set, fetch-and-store, compare-and-swap, and fetch-and-add) access only one remote variable. In this case, the first and the last of our bounds are asymptotically tight. <p> Second, even in the absence of contention, such algorithms require a process contending for its critical section to execute many operations. The second of these two problems has subsequently been addressed, specifically by Lamport in <ref> [37] </ref>, where a read/write algorithm is given that requires only a constant number of operations per critical section acquisition in the absence of contention. Following the title of Lamport's paper, such algorithms 19 have come to be known simply as "fast mutual exclusion algorithms". <p> Most such algorithms also require O (N ) remote operations in the absence of contention. Some exceptions to the latter include the algorithm given by Kessels in [32] and the previously mentioned one given by Lamport in <ref> [37] </ref>. Kessels' algorithm generates O (log N ) remote operations in the absence of contention, while Lamport's generates O (1). A variant of Lamport's algorithm has recently been presented by Styer in [56]. <p> We tested seven mutual exclusion algorithms on the TC2000: a simple test-and-set algorithm; the queue-based algorithm using compare-and-swap given by Mellor-Crummey and Scott in [45]; the queue-based algorithm using fetch-and-add given by T. Anderson in [9]; the fast mutual exclusion algorithm given by Lamport in <ref> [37] </ref>; the tree-based algorithm given by Styer in [56]; the tree-based 72 73 algorithm given by Peterson and Fischer in [52]; and the mutual exclusion algo-rithm described in Section 2.3. Performance results obtained by running these seven algorithms on the TC2000 are summarized in Figure 2.6. <p> It is interesting to note that there exist read/write mutual exclusion algorithms with write-contention N that have O (1) time complexity in the absence of competition <ref> [3, 37, 60] </ref>. Thus, establishing the above-mentioned lower bound for read/write algorithms will require proof techniques that differ from those given in Chapter 2. We do not know whether the bound of Theorem 3.5 is tight.
Reference: [38] <author> L. Lamport, </author> <title> "win and sin: Predicate Transformers for Concurrency", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 12, No. 3, </volume> <year> 1990, </year> <pages> pp. 396-428. </pages>
Reference: [39] <author> C. Lee and U. Ramachandran, </author> <title> "Synchronization with Multiprocessor Caches", </title> <booktitle> Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> May, </month> <year> 1990, </year> <pages> pp. 27-37. 151 </pages>
Reference-contexts: In addition to the software-based solutions to the mutual exclusion problem described above, a number of hardware-based implementations have been proposed. Of particular interest are the solutions given by Goodman, Vernon, and Woest in [25], and by Lee and Ramachandran in <ref> [39] </ref>, which exploit underlying cache coherence mechanisms. These implementations involve the construction of a distributed queue in hardware.
Reference: [40] <author> D. Lenowski et. al., </author> <title> "The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor", </title> <booktitle> Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <month> May, </month> <year> 1990, </year> <pages> pp. 148-159. </pages>
Reference-contexts: This results in limited scalability because the common bus and the individual processor caches easily saturate. A directory-based cache coherence scheme is adopted in DASH to avoid the scalability problem of snoopy schemes. This scheme eliminates the need to broadcast every memory request to all processor caches <ref> [40] </ref>. The directory keeps a record of the processor caches that hold a copy of each memory block. Because only the caches with copies may be affected by accesses to the memory block, only those caches need be notified of such accesses.
Reference: [41] <author> D. Lenowski et. al., </author> <title> "The Stanford DASH Multiprocessor", </title> <journal> IEEE Computer , Vol. </journal> <volume> 25, No. 3, </volume> <year> 1992, </year> <pages> pp. 63-79. </pages>
Reference-contexts: The shared address space also provides better support for parallelizing 2 compilers, multiprogramming, and standard operating systems. These features make a shared-memory multiprocessor easier to program than a message-passing machine <ref> [41] </ref>. In the rest of this section, we briefly discuss two architectural paradigms for shared-memory multiprocessors, namely distributed shared memory and cache-coherent memory, and examine some multiprocessors that adopt such paradigms. Distributed Shared Memory Machines 3 A distributed shared-memory multiprocessor is depicted in Figure 1.1. <p> Some references to shared variables and all references to private variables and codes can be made local to each processor. These references are served without the longer latency of remote references, resulting in reduced bandwidth demands on the global interconnect <ref> [41] </ref>. The BBN TC2000 is an example of a distributed shared-memory multiprocessor [13]. The TC2000 consists of a number of nodes, each of which contains a processor and a memory unit. The nodes are connected via a multi-stage interconnection network, known as the Butterfly switch. <p> In fact, the above two paradigms distributed shared memory and cache-coherent memory provide a means to exploit locality in programs, and both may be adopted in the same system. The DASH multiprocessor is an example of such a system <ref> [41] </ref>. The DASH machine is a shared-memory multiprocessor that provides a single address space with a distributed memory and coherent caches. Most commercial multiprocessors with coherent caches, including the Sequent Symmetry, rely on snooping to maintain coherence.
Reference: [42] <author> M. Li, J. Tromp, and P. Vitanyi, </author> <title> "How to Construct Wait-Free Variables", </title> <booktitle> Proceedings of International Colloquium on Automata, Languages, and Programming, Lecture Notes in Computer Science 372, </booktitle> <pages> pp. 488-505, </pages> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference: [43] <author> N. Lynch and N. Shavit, </author> <title> "Timing-Based Mutual Exclusion", </title> <booktitle> Proceedings of the Thirteenth IEEE Real-Time Systems Symposium, </booktitle> <month> December, </month> <year> 1992, </year> <pages> pp. 2-11. </pages>
Reference-contexts: Past work on the complexity of mutual exclusion has almost exclusively focused on space requirements [17]; the limited work on time bounds that has been done has focused on partially synchronous models <ref> [43] </ref>. The lack of prior work on time bounds for mutual exclusion within asynchronous models is probably due to difficulties associated with measuring the time spent within busy-waiting constructs.
Reference: [44] <author> N. Lynch and M. Tuttle, </author> <title> "An Introduction to Input/Output Automata", </title> <type> Technical Report MIT/LCS/TM-373, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> November, </month> <year> 1988. </year>
Reference-contexts: In this section, we define the notion of an implementation formally. Except for modifications to handle liveness conditions, our notion of implementation is similar to that given by Herlihy in [27], which is based on the I/O automata of Lynch and Tuttle <ref> [44] </ref>. The following description is adopted from Herlihy [27]. 4.2.1 I/O automata We model concurrent programs using a simplified form of I/O automata [44]. I/O automata provide a convenient way for describing what it means for one object to implement another. <p> handle liveness conditions, our notion of implementation is similar to that given by Herlihy in [27], which is based on the I/O automata of Lynch and Tuttle <ref> [44] </ref>. The following description is adopted from Herlihy [27]. 4.2.1 I/O automata We model concurrent programs using a simplified form of I/O automata [44]. I/O automata provide a convenient way for describing what it means for one object to implement another. <p> Note this definition corresponds to weak fairness of every action. 2 Unless otherwise noted, we henceforth assume that all histories are fair. Let I j be an implementation of A j . Following Lynch and Tuttle <ref> [44] </ref> and Herlihy [27], we say that I j is correct, if for every fair history H of every system fP 1 ; : : : ; P n ; A 1 ; : : : ; I j ; : : : ; A m g, there exists a fair
Reference: [45] <author> J. Mellor-Crummey and M. Scott, </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 9, No. 1, </volume> <month> February, </month> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: Early solutions based on strong primitives, such as the familiar test-and-set lock, were conceptually simple, but resulted in somewhat poor performance. More recently, queue-based spin locks have been proposed by Anderson [9], by Graunke and Thakkar [26], and by Mellor-Crummey and Scott <ref> [45] </ref> that exhibit better performance; these locks are implemented using fetch-and-add, fetch-and-store, or compare-and-swap instructions, respectively. <p> Other strong primitives such as compare-and-swap and fetch-and-add are provided using the TC2000 hardware locking protocol [13]. We tested seven mutual exclusion algorithms on the TC2000: a simple test-and-set algorithm; the queue-based algorithm using compare-and-swap given by Mellor-Crummey and Scott in <ref> [45] </ref>; the queue-based algorithm using fetch-and-add given by T. <p> The graph depicted for the MCS algorithm is mostly flat, except at the point for two processors. This anomaly at two processors coincides with results reported by Mellor-Crummey and Scott on the Sequent Symmetry, and was attributed by them to the lack of a compare-and-swap instruction on the Symmetry <ref> [45] </ref>. As our implementation of their algorithm did employ compare-and-swap, we have not found a satisfying explanation for this behavior on the TC2000. T. Anderson's algorithm requires only local spinning when implemented on a machine with coherent caches. <p> Anderson's algorithm is far better than that of the simple test-and-set algorithm. Because the processes 75 in Anderson's algorithm spin globally on the TC2000, this might be interpreted as a counterexample to our belief that minimizing remote operations is important for good scalability. However, Mellor-Crummey and Scott reported in <ref> [45] </ref> that Anderson's algorithm produced far fewer remote operations than the test-and-set algorithm. Sequent Symmetry Performance results of experiments on the Sequent Symmetry are summarized in provides an atomic fetch-and-store instruction. <p> Sequent Symmetry Performance results of experiments on the Sequent Symmetry are summarized in provides an atomic fetch-and-store instruction. Because other strong primitives are not provided, we used a version of Mellor-Crummey and Scott's algorithm that is implemented with fetch-and-store and that does not ensure starvation-freedom <ref> [45] </ref>. Fetch-and-add, which is used in T. Anderson's algorithm, was simulated by a test-and-set algorithm with randomized backoff, as Anderson did in [9]. The experiments on the Symmetry show similar results to that for the TC2000. However, on the Symmetry, T. <p> In particular, an algorithm by Mellor-Crummey and Scott given in <ref> [45] </ref> solves the mutual exclusion problem for w processes, in O (1) time, with access-contention (and hence write-contention) w. By applying this solution within a balanced w-ary tree with N leaves, it is possible to obtain an N -process fi (log w N ) mutual exclusion algorithm with access-contention w.
Reference: [46] <author> M. Merritt and G. Taubenfeld, </author> <title> "Knowledge in Shared Memory Systems", </title> <booktitle> Proceedings of the Tenth ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August, </month> <year> 1991, </year> <pages> pp. 189-200. </pages>
Reference-contexts: The above-mentioned time bounds are then established in Sections 3.4 and 3.5. We end the chapter with some discussion in Section 3.6. 3.2 Shared-Memory Systems Our model of a shared-memory system is similar to that given by Merritt and Taubenfeld in <ref> [46] </ref>. A system S = (C; P; V ) consists of a set of computations C, 83 a set of processes P = f1; 2; : : : ; N g, and a set of variables V . A computation is a finite sequence of events.
Reference: [47] <author> M. Michael and M. Scott, </author> <title> "Fast Mutual Exclusion, Even With Contention", </title> <type> Technical Report, </type> <institution> University of Rochester, </institution> <month> June, </month> <year> 1993. </year> <month> 152 </month>
Reference-contexts: Because the processes in his algorithm busy-wait on remote variables, such complexity calculations do not give a true indication of scalability. Another recent algorithm of interest is one given by Michael and Scott in <ref> [47] </ref>. Although this algorithm generates O (N ) 20 remote memory references in the presence of contention and O (1) in the absence of contention, it requires both full- and half-word reads and writes to memory, which is a level of hardware support more powerful than ordinary read/write atomicity.
Reference: [48] <author> R. Newman-Wolfe, </author> <title> "A Protocol for Wait-Free, Atomic, Multi-Reader Shared Variables, </title> <booktitle> Proceedings of the Sixth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1987, </year> <pages> pp. 232-248. </pages>
Reference: [49] <author> S. Owicki and D. Gries, </author> <title> "An Axiomatic Proof Technique for Parallel Programs I", </title> <journal> Acta Informatica, </journal> <volume> Vol. 6, </volume> <year> 1976, </year> <pages> pp. 319-340. </pages>
Reference: [50] <author> G. Peterson, </author> <title> "Myths About the Mutual Exclusion Problem", </title> <journal> Information Processing Letters, </journal> <volume> Vol. 12, No. 3, </volume> <month> June, </month> <year> 1981, </year> <pages> pp. 115-116. </pages>
Reference-contexts: The first starvation-free solution was presented by Knuth in [34]. Of the many early solutions to the mutual exclusion problem, most are quite complicated and difficult to understand. A notable exception is an especially simple solution first presented by Peterson in <ref> [50] </ref> and later refined by Kessels in [32]. The approach taken by Kessels was to first solve the mutual exclusion problem for two processes, and to then use the two-process solution in a binary arbitration tree to solve the N - process case.
Reference: [51] <author> G. Peterson and J. Burns, </author> <title> "Concurrent Reading While Writing II: The Multi-Writer Case", </title> <booktitle> Proceedings of the 28th Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference: [52] <author> G. Peterson and M. Fischer, </author> <title> "Economical Solutions for the Critical Section Problem in a Distributed System", </title> <booktitle> Proceedings of the 9th ACM Symposium on Theory of Computing, </booktitle> <month> May, </month> <year> 1977, </year> <pages> pp. 91-97. </pages>
Reference-contexts: Anderson in [9]; the fast mutual exclusion algorithm given by Lamport in [37]; the tree-based algorithm given by Styer in [56]; the tree-based 72 73 algorithm given by Peterson and Fischer in <ref> [52] </ref>; and the mutual exclusion algo-rithm described in Section 2.3. Performance results obtained by running these seven algorithms on the TC2000 are summarized in Figure 2.6. Each point (x; y) in each graph represents the average time y for one critical section execution with x competing processors.
Reference: [53] <author> G. Pfister and A. Norton, </author> <title> "Hot Spot Contention and Combining in Multistage Interconnection Networks", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-34, No. 11, </volume> <month> November, </month> <year> 1985, </year> <pages> pp. 943-948. </pages>
Reference-contexts: On many large scale shared-memory multiprocessors, multistage interconnection networks are employed to get a high bandwidth connection between processors and memory modules. Pfister and Norton <ref> [53] </ref> have shown that when many processors request access to the same memory location, making it a highly contended variable called hot spot, a tree-shaped saturation builds up in the interconnection network, resulting in performance degradation not only for those processors accessing the hot spot, but other processors as well. <p> Limiting access-contention is an important consideration when designing algorithms for problems, such as mutual exclusion and shared counting, that must cope well with high competition among processes <ref> [9, 28, 29, 53] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [54] <author> A. Singh, J. Anderson, and M. Gouda, </author> <title> "The Elusive Atomic Register, Revisited", </title> <booktitle> Proceedings of the Sixth Annual Symposium on Principles of Distributed Computing, </booktitle> <year> 1987, </year> <pages> pp. 206-221. </pages> <note> To appear in Journal of the ACM </note> . 
Reference: [55] <institution> Sequent Computer Systems, </institution> <type> Sequent Technical Summary, </type> <year> 1987. </year>
Reference-contexts: When a new value is written to a cached variable, a cache-coherence protocol either invalidates other copies of the variable, or updates those copies to the new value. The Sequent Symmetry is an example of a cache-coherent shared-memory multiprocessor <ref> [55] </ref>. On the Symmetry, the processors and memory nodes are interconnected via a shared bus. In other words, all processors, memory modules, and I/O controllers plug into a single bus. A processor node consists of an Intel 5 80386 or Intel 80486 and a 64 Kbyte, two-way set-associative cache.
Reference: [56] <author> E. Styer, </author> <title> "Improving Fast Mutual Exclusion", </title> <booktitle> Proceedings of the Eleventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1992, </year> <pages> pp. 159-168. </pages>
Reference-contexts: Kessels' algorithm generates O (log N ) remote operations in the absence of contention, while Lamport's generates O (1). A variant of Lamport's algorithm has recently been presented by Styer in <ref> [56] </ref>. Although Styer claims that his algorithm is more scalable than Lamport's, in terms of time complexity, they are actually very similar: both generate unbounded remote operations under heavy contention and O (1) operations in the absence of contention. <p> Anderson in [9]; the fast mutual exclusion algorithm given by Lamport in [37]; the tree-based algorithm given by Styer in <ref> [56] </ref>; the tree-based 72 73 algorithm given by Peterson and Fischer in [52]; and the mutual exclusion algo-rithm described in Section 2.3. Performance results obtained by running these seven algorithms on the TC2000 are summarized in Figure 2.6.
Reference: [57] <author> J. Tromp, </author> <title> "How to Construct an Atomic Variable", </title> <booktitle> Proceedings of the Third International Workshop on Distributed Algorithms, Lecture Notes in Computer Science 392, </booktitle> <pages> pp. 292-302, </pages> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference: [58] <author> P. Turan, </author> <title> "On an extremal problem in graph theory"(in Hungarian), </title> <journal> Mat. Fiz. </journal> <volume> Lapok , Vol. 48, </volume> <year> 1941, </year> <pages> pp. 436-452. </pages>
Reference-contexts: G ffi hEat i i and let L (2) = G 0 ffi hEat j i.) Note that value (i:dine; F ) = eat ^ value (j:dine; F ) = eat holds, which implies that S does not solve the minimal mutual exclusion problem. 2 The next theorem by Turan <ref> [58] </ref> will be used in subsequent lemmas. Theorem 3.2 (Turan): Let G = hV; Ei be an undirected multigraph, 3 where V is a set of vertices and E is a set of edges.
Reference: [59] <author> P. Vitanyi and B. Awerbuch, </author> <title> "Atomic Shared Register Access by Asynchronous Hardware", </title> <booktitle> Proceedings of the 27th IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1986, </year> <pages> pp. 233-243. </pages>
Reference: [60] <author> J.-H. Yang and J. Anderson, </author> <title> "Fast, Scalable Synchronization with Minimal Hardware Support", </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August, </month> <year> 1993, </year> <pages> pp. 171-182. </pages>
Reference-contexts: A correctness proof for the simpler two-process algorithm is presented in <ref> [60] </ref>. We begin by presenting definitions and notational conventions that will be used in the remainder of the chapter. As in Section 2.3, we assume that the number of processes N is a power of 2. <p> One of the primary contributions of this chapter is to show that it is possible to establish meaningful time bounds for such problems. In Chapter 2, we proposed a time measure for concurrent programs that dis 80 tinguishes between local and remote accesses of shared memory <ref> [60] </ref>. Under our measure, the time complexity of a concurrent program is measured by counting only remote accesses of shared variables; local accesses are ignored. We present several lower-bound results for mutual exclusion that are based on the time complexity measure proposed in Chapter 2. <p> It is interesting to note that there exist read/write mutual exclusion algorithms with write-contention N that have O (1) time complexity in the absence of competition <ref> [3, 37, 60] </ref>. Thus, establishing the above-mentioned lower bound for read/write algorithms will require proof techniques that differ from those given in Chapter 2. We do not know whether the bound of Theorem 3.5 is tight. <p> We do not know whether the bound of Theorem 3.5 is tight. We conjecture that this bound can be improved to (log vw N ), which has a matching algorithm when v is taken to be a constant <ref> [60] </ref>. It is our belief that the most important contribution of Chapter 3 is to show that meaningful time bounds can be established for concurrent programming problems for which busy-waiting is inherent. We hope that our work will spark new work on time complexity results for such problems.
Reference: [61] <author> J.-H. Yang and J. Anderson, </author> <title> "Time Bounds for Mutual Exclusion and Related Problems", </title> <booktitle> Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <month> May, </month> <year> 1994, </year> <pages> pp. 224-233. 154 </pages>
References-found: 61

