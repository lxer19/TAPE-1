URL: http://www-cse.ucsd.edu/users/gary/pubs/dair94.ps
Refering-URL: http://www.cse.ucsd.edu/users/gary/
Root-URL: 
Title: Optimizing Parameters in a Ranked Retrieval System Using Multi-Query Relevance Feedback  
Author: Brian T. Bartell and Garrison W. Cottrell and Richard K. Belew 
Address: La Jolla, California 92037  La Jolla, California 92093-0114  
Affiliation: Advanced Technology Group Encylopdia Britannica, Inc.  Department of Computer Science Engineering-0114 University of California, San Diego  
Abstract: A method is proposed by which parameters in ranked-output text retrieval systems can be automatically optimized to improve retrieval performance. A ranked-output text retrieval system implements a ranking function which orders documents, placing documents estimated to be more relevant to the user's query before less relevant ones. The proposed method is to adjust system parameters to maximize the match between the system's document ordering and the user's desired ordering, given by relevance feedback. The utility of the approach is demonstrated by estimating the similarity measure in a vector space model of information retrieval. The approach automatically finds a similarity measure which performs equivalent to or better than all "classic" similarity measures studied. It also performs within 1% of an estimated theoretically optimal measure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brian T. Bartell. </author> <title> Optimizing Ranking Functions: A Connectionist Approach to Adaptive Information Retrieval. </title> <type> PhD thesis, </type> <institution> Department of Computer Science & Engineering, The University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: An overview of the approach is provided here; further details can be found in other work <ref> [1] </ref>.
Reference: [2] <author> R.K. Belew. </author> <title> Adaptive information retrieval: Using a connectionist representation to retrieve and learn about documents. </title> <booktitle> In Proc. SIGIR 1989, </booktitle> <pages> pages 11-20, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: This formulation of the ranked retrieval problem is sufficiently general to include many popular methods for ranked retrieval, such as the vector space model [12] the probabilistic ranking model [3], and spreading activation models <ref> [2] </ref>. The goal of the method is to find values of the parameters fi which result in the best ordering of documents. The target ordering is typically given by user feedback to the system.

Reference: [4] <author> I. Borg and J. Lingoes. </author> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Optimizing J will therefore likely improve average precision. We have performed this same comparison using a number of other criteria from the statistical, psychometrics, and text retrieval literatures, including Kruskal's stress function [9], a variation of Pearson's correlation coefficient <ref> [4] </ref>, Wong's Perceptron criterion [13] and Fuhr & Buckley's Least Squared Error criterion [5]. Of all the alternative criteria studied, none matched average precision as closely as the criterion we have selected.
Reference: [5] <author> Norbert Fuhr and Chris Buckley. </author> <title> A probabilistic learning approach for document indexing. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 223-248, </pages> <year> 1991. </year>
Reference-contexts: We have performed this same comparison using a number of other criteria from the statistical, psychometrics, and text retrieval literatures, including Kruskal's stress function [9], a variation of Pearson's correlation coefficient [4], Wong's Perceptron criterion [13] and Fuhr & Buckley's Least Squared Error criterion <ref> [5] </ref>. Of all the alternative criteria studied, none matched average precision as closely as the criterion we have selected.
Reference: [6] <author> L. Guttman. </author> <title> What is not what in statistics. </title> <journal> The Statistician, </journal> <volume> 26 </volume> <pages> 81-107, </pages> <year> 1978. </year>
Reference-contexts: When no fi exists which can provide the perfect ranking, we desire a fi which matches the target ordering q as well as possible. 3 Method Our approach is to optimize a criterion derived from Guttman's Point Alienation <ref> [6] </ref>, a measure of rank correlation between two variables. An overview of the approach is provided here; further details can be found in other work [1].
Reference: [7] <author> Donna Harman. </author> <title> An experimental study of factors important in document ranking. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 186-193, </pages> <address> Pisa, Italy, </address> <year> 1986. </year>
Reference-contexts: Pragmatically, the selection of similarity measure can have a significant effect on the performance of the system; for example, Harman <ref> [7] </ref> demonstrates a 12% improvement in average precision by switching between two different normalized inner product measures. We address this problem by optimizing the parameters of a general class of similarity measures.
Reference: [8] <author> William P. Jones and George W. Fur-nas. </author> <title> Pictures of relevance: A geometric analysis of similarity measures. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 38(6) </volume> <pages> 420-442, </pages> <year> 1987. </year>
Reference-contexts: The problem is interesting for both historical and pragmatic reasons. Historically, a large number of similarity measures have been proposed (McGill, et. al., [10] and Jones & Furnas <ref> [8] </ref> contain lengthy enumerations), and no single similarity measure has gained universal acceptance. Pragmatically, the selection of similarity measure can have a significant effect on the performance of the system; for example, Harman [7] demonstrates a 12% improvement in average precision by switching between two different normalized inner product measures. <p> As Table 1 illustrates, various values of fi correspond to common similarity measures in the text retrieval literature <ref> [8] </ref>. We have selected this model and parameterization because of its correspondence with well known measures and the vector L-norms. In addition, the model has only two parameters, which facilitates visualization and analysis of the optimization problem.
Reference: [9] <author> J. B. Kruskal. </author> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27, </pages> <month> March </month> <year> 1964. </year>
Reference-contexts: Optimizing J will therefore likely improve average precision. We have performed this same comparison using a number of other criteria from the statistical, psychometrics, and text retrieval literatures, including Kruskal's stress function <ref> [9] </ref>, a variation of Pearson's correlation coefficient [4], Wong's Perceptron criterion [13] and Fuhr & Buckley's Least Squared Error criterion [5]. Of all the alternative criteria studied, none matched average precision as closely as the criterion we have selected.
Reference: [10] <author> M. McGill, M. Koll, and T. Noreault. </author> <title> An evaluation of factors affecting document ranking by information retrieval systems. </title> <type> Technical report, </type> <institution> School of Information Studies, Syracuse University, Syracuse, </institution> <address> New York 13210, </address> <month> Octo-ber </month> <year> 1979. </year>
Reference-contexts: The problem is interesting for both historical and pragmatic reasons. Historically, a large number of similarity measures have been proposed (McGill, et. al., <ref> [10] </ref> and Jones & Furnas [8] contain lengthy enumerations), and no single similarity measure has gained universal acceptance.
Reference: [11] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vet-terling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cam-bridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: When R is completely mis--ordered, the numerator is the negative of the denominator, and the ratio is -1.0. We minimize J (R fi ) using Conjugate Gradient, a gradient-based optimization method <ref> [11] </ref>. J is mostly differentiable and is therefore suitable for gradient-based optimization. There are pathological cases in which the gradient of J is not defined. One such critical point occurs when all R fi;q (d) are equivalent for d 2 D.
Reference: [12] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Typically the retrieval system will display the retrieved documents to the user in the order given by R fi;q (d). This formulation of the ranked retrieval problem is sufficiently general to include many popular methods for ranked retrieval, such as the vector space model <ref> [12] </ref> the probabilistic ranking model [3], and spreading activation models [2]. The goal of the method is to find values of the parameters fi which result in the best ordering of documents. The target ordering is typically given by user feedback to the system.
Reference: [13] <author> S. K. M. Wong, Y. J. Cai, and Y. Y. Yao. </author> <title> Computation of term associations by a neural network. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Optimizing J will therefore likely improve average precision. We have performed this same comparison using a number of other criteria from the statistical, psychometrics, and text retrieval literatures, including Kruskal's stress function [9], a variation of Pearson's correlation coefficient [4], Wong's Perceptron criterion <ref> [13] </ref> and Fuhr & Buckley's Least Squared Error criterion [5]. Of all the alternative criteria studied, none matched average precision as closely as the criterion we have selected.
Reference: [14] <author> S. K. M. Wong and Y. Y. Yao. </author> <title> Query formulation in linear retrieval models. </title> <journal> Journal of the American Society for Information Retrieval, </journal> <volume> 41(5) </volume> <pages> 334-341, </pages> <year> 1990. </year> <note> Bartell, Cottrell, and Belew </note>
Reference-contexts: This provides a target ordering over the documents for the set of past user queries. We formalize this target ordering with the preference notation of Wong, et. al.: the binary relation q denotes the preference ordering over documents for the query q <ref> [14] </ref>. For two documents d and d 0 , d q d 0 () the user prefers d to d 0 (1) We let Q denote the set of past user queries.
References-found: 13

