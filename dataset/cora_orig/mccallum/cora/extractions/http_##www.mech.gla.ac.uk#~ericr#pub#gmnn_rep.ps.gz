URL: http://www.mech.gla.ac.uk/~ericr/pub/gmnn_rep.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00366.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: ericr@mech.gla.ac.uk peterg@mech.gla.ac.uk  
Title: Neural networks for modelling and control  
Author: Eric Ronco and Peter J. Gawthrop 
Date: November 10, 1997  
Address: Glasgow  
Affiliation: Centre for System and Control Department of Mechanical Engineering University of  
Pubnum: Technical Report: csc97008  
Abstract-found: 0
Intro-found: 0
Reference: <author> Agarwal, </author> <month> Mukul </month> <year> (1997). </year> <title> A systematic classification of neural-network-based control. </title> <journal> IEEE Control Systems Magazine 17(2), </journal> <pages> 75-93. </pages>
Reference-contexts: Therefore, the weights (0.7) of any perceptron can be updated by determining the error gradient (0.8) from the last layer to the first one. 0.2.2 Neuro-control approaches Neural networks have been used for different purposes in the context of control (see for details <ref> (Agarwal, 1997) </ref>). We are only interested in the different existing methods used to develop a neuro-controller. Most of these methods are based on inverse control. <p> The neural training consists simply of learning the mapping between the sensory information received by the human controller and the control input (see figure 0.4). The problem with this method is that it is sometimes difficult to determine the information used by the expert to control a system <ref> (Agarwal, 1997) </ref>. A similar approach entails training a neuro-controller to mimic the behaviour of an other controller designed with a conventional method (see figure 0.5). This can be useful when the 5 Note that X is the vector of the network inputs. conventional control approach requires too many computations.
Reference: <author> Arbib, M.A. </author> <year> (1987). </year> <title> Brain, Machine and Mathematics. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Berthier, N. E., Singh S. P., Barto A. G. and Houk J. C. </author> <year> (1993). </year> <title> Distributed representation of limb motor programs in array of adjustable pattern generators. </title> <journal> Journal of Cognitive Neuroscience 5(1), </journal> <pages> 56-78. </pages>
Reference: <author> Bottou, L. and P. </author> <month> Galliinari </month> <year> (1991). </year> <title> A framework for the cooperation of learning algorithms. In: Neural Information Processing Systems 3 (Lippmann and al., </title> <editor> Eds.). </editor> <volume> Vol. 3. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Brashers-Krug, T., R. Shadmehr and E. </author> <month> Todorov </month> <year> (1995). </year> <title> Catastrophic interference in human motor learning. </title> <booktitle> Advances in Neural Information Processing Systems. </booktitle>
Reference: <author> Carmon, A. </author> <year> (1986). </year> <title> Applying self-tuning control to plant. </title> <booktitle> Control and Instumentation 18, </booktitle> <pages> 81-83. </pages>
Reference: <author> Carpenter, G. A. and S. </author> <title> Grossberg (1988). The art of adaptive pattern recognition by a self-organising neural network. </title> <booktitle> IEEE Computer 21(3), </booktitle> <pages> 77-88. </pages>
Reference-contexts: This is due to the existence of flat areas and/or local minima in the error surface (Rumulhart and McClelland, 1986; LeCun, 1987; Choi and Choi, 1992; Hecht-Nielsen, 1991). Another cause for the learning failure is the "stability-plasticity dilemma" <ref> (Carpenter and Grossberg, 1988) </ref>. This is typical of problems where more than one task has to be learned by the same network (two different tasks can be simply two different non-linear regions of the system).
Reference: <author> Carpenter, R.H.S. </author> <year> (1984). </year> <title> Neurophysiology. </title> <publisher> London. </publisher>
Reference: <author> Carson, E. R. </author> <year> (1990). </year> <booktitle> Measurement and control in medicine. Measurement and Control 23, </booktitle> <pages> 260-262. </pages>
Reference-contexts: It sounds like "the brain plays on spinal cord not as one plays piano, but rather as one selects a disc from a juke box" <ref> (Carson, 1990) </ref>. Another very important statement is that, with practice, biological control systems can develop an accurate internal model of the plant under control (Shadmehr and Mussa-Ivaldi, 1994; Brashers-Krug et al., 1995; Keele et al., 1995; Kawato, 1989). This has been nicely demonstrated by (Shadmehr and Mussa-Ivaldi, 1994).
Reference: <author> Chen, S., S. A. Billings and P. M. </author> <title> Grant (1990). Non-linear system identification using neural networks. </title> <journal> Int. J. Control 51, </journal> <pages> 1191-1214. </pages>
Reference-contexts: This way the MLP can be interpreted as a NARMAX model (Non-linear Auto-Regressive model, with Moving Average part and eXogenous input) of the system <ref> (Chen et al., 1990) </ref>. The NARMAX model is the lagged version of the NARX model. <p> The various local models are activated by an input vector X varying with time that usually correspond to the vector of parameters of a NARMAX model (Non-linear Auto-Regressive model, with Moving Average part and eXogenous input) of the system <ref> (Chen et al., 1990) </ref>. The NARMAX model is the lagged version of the NARX model.
Reference: <author> Choi, Chong-Ho and Jin Young Choi (1992). </author> <title> Partially trained neural networks based on partition of unity. </title> <booktitle> In: Proceeding of the International Joint Conference on Neural Networks (IJCN92). </booktitle>
Reference-contexts: It appears that the number of parameters seems excessive and the unclear correlation between architecture and learning abilities make it difficult to set up optimal multi-CALM model (Francesco, 1994). Another example of such a self-clustering approach was given by <ref> (Choi and Choi, 1992) </ref>. The learning procedure implies two stages. First, the centre of each partition is found by unsupervised learning. Following this stage, a supervised learning is used to train the models connected to each partitioning unit.
Reference: <author> Clarke, D. W., C. Mohtadi and P. S. </author> <month> Tuffs </month> <year> (1987). </year> <title> Generalized predictive control-i. the basic algorithm. </title> <type> Automatica 23, </type> <pages> 137-148. </pages>
Reference-contexts: This is the basis of predictive control: the control is achieved according to the predicted output of the plant given by its forward model. This idea has been generalised by <ref> (Clarke et al., 1987) </ref> to deal with plant with complex dynamics (e.g. unstable inverse systems, time-varying time delay, etc), and plant model mismatch (Rusnak et al., 1996). (Clarke et al., 1987) coined this controller as the "generalised predictive control" (GPC). <p> This idea has been generalised by <ref> (Clarke et al., 1987) </ref> to deal with plant with complex dynamics (e.g. unstable inverse systems, time-varying time delay, etc), and plant model mismatch (Rusnak et al., 1996). (Clarke et al., 1987) coined this controller as the "generalised predictive control" (GPC). The GPC is the main design method of a model based predictive controller (MBPC). A MBPC is composed of three main components: the system, its model and a function optimiser (see figure 0.10).
Reference: <author> Cybenko, G. </author> <year> (1988). </year> <title> Continuous valued neural networks with two hidden layers are sufficient. </title> <type> Technical report. </type> <institution> Departement of computer science, Tufts University. </institution> <address> Medford, MA. </address>
Reference: <author> Cybenko, G. </author> <year> (1989). </year> <title> Approximation by superposition of a sigmoidal function. Mathematics of Control, Signal and Systems 2, </title> <type> 303-314. 29 Feldman, J.A. </type> <year> (1989). </year> <title> Neural representation of conceptual knowledge. In: Neural connections, mental computation (Nadel and al., </title> <editor> Eds.). </editor> <publisher> MIT Press. </publisher> <address> Cambridge, MA. </address>
Reference: <author> Fogelman-Soulie, F. </author> <year> (1993). </year> <title> Multi-modular neural network-hybrid architectures: a review. </title> <booktitle> In: Proceedings of 1993 International Joint Conference on Neural Networks. </booktitle>
Reference: <author> Francesco, M. </author> <year> (1994). </year> <title> Functional Networks. </title> <type> PhD thesis. </type> <institution> Faculte des Sciences de l'Universite de Geneve. </institution>
Reference-contexts: This model is named "CALM" for Categorising And Learning Module. A CALM module tends to autonomously cluster stimuli belonging to the same region by way of inhibitory connections between neurons involving competition in the CALM module. Note that <ref> (Francesco, 1994) </ref> highlights the fact that this model was developed with psychological and neurological considerations. Thus this model is very interesting from a life science point of view but seems to have some limitation considering its usefulness. <p> Thus this model is very interesting from a life science point of view but seems to have some limitation considering its usefulness. It appears that the number of parameters seems excessive and the unclear correlation between architecture and learning abilities make it difficult to set up optimal multi-CALM model <ref> (Francesco, 1994) </ref>. Another example of such a self-clustering approach was given by (Choi and Choi, 1992). The learning procedure implies two stages. First, the centre of each partition is found by unsupervised learning. Following this stage, a supervised learning is used to train the models connected to each partitioning unit.
Reference: <author> Gawthrop, Peter J. </author> <year> (1995). </year> <title> Continuous-time local state local model networks. In: </title> <journal> IEEE Systems, man and cybernetics. Vancouver. </journal>
Reference-contexts: Embedding modularity into Neural Networks (NN) gives many advantages over the use of single NN (see for further details <ref> (Ronco and Gawthrop, 1995) </ref>). "Modular Neural Network" (MNN) refers here to a network performing a local computation of the information. That is to say that only one part of the network is involved in computing each input vector. <p> That is to say that only one part of the network is involved in computing each input vector. This definition is certainly loose but because currently there is no real definition of a MNN, this is the one adopted in this study (for a discussion about this issue see <ref> (Ronco and Gawthrop, 1995) </ref>). Modularity is a natural way to ease the learning of complex behaviour. <p> The main issues to develop a MNN are the modular organisation in the network architecture and the decomposition of a task (problem, environment) into sub-tasks <ref> (Ronco and Gawthrop, 1995) </ref>. A sensible way to decompose a task is to do so according to the computation capability of the modules. In fact, the modules should also be built according to the task they would have to perform. <p> Two different neuro-control approaches have been distinguished. One entails developing a single controller from a neural network and the other one intends to embed a number of controllers inside a neural network <ref> (Gawthrop, 1995) </ref>. The single controller approach is more conventional in the sense that in control engineering first a model of the plant is conceived and then a controller is designed. The main single neuro-control approaches find in the literature have been described.
Reference: <author> Gawthrop, Peter J. </author> <year> (1996). </year> <title> Self-tuning pid control structure. </title> <booktitle> In: IEE Colloqium. </booktitle> <address> London. </address>
Reference: <author> Gawthrop, Peter J. and Eric Ronco (1996). </author> <title> Local model networks and self-tuning predictive control. </title> <booktitle> In: IEEE Mediterranean Symposium on New Directions in Control and Automation. </booktitle>
Reference: <author> Gollee, H. and D. J. </author> <title> Murray-Smith (1997). Validation of an empirical model structure by analysing model properties. </title> <booktitle> In: International Conference on Engineering Applications of Neural Networks. </booktitle> <address> Stockholm, Sweden. </address>
Reference-contexts: Each local model being linear the "learning" is straightforward using least squares like methods. The interaction between local models introduces non-linearity into the structure. This tends to prohibit the use of least square like method. A gradient descent would have to be used in the global case as in <ref> (Gollee and Murray-Smith, 1997) </ref>. In this case the convergence to a global minima is no more ensured and the training time is much more longer. * A local learning can enable model switching i.e. selection at each instant of one local model only.
Reference: <author> Gollee, H. and K. J. </author> <title> Hunt (1997). Nonlinear modelling and control of electrically stimulated muscle: a local model network approach. </title> <journal> Int J Control. </journal> <note> To appear. </note>
Reference-contexts: Each local model being linear the "learning" is straightforward using least squares like methods. The interaction between local models introduces non-linearity into the structure. This tends to prohibit the use of least square like method. A gradient descent would have to be used in the global case as in <ref> (Gollee and Murray-Smith, 1997) </ref>. In this case the convergence to a global minima is no more ensured and the training time is much more longer. * A local learning can enable model switching i.e. selection at each instant of one local model only.
Reference: <author> Gomi, </author> <title> Hiroaki and Mitsuo Kawato (1993). Neural network for a closed-loop system using feedback-error-control. </title> <booktitle> Neural Networks pp. </booktitle> <pages> 933-946. </pages>
Reference-contexts: For instance the use of the system output can produce an exact inverse even when the forward model is inexact; this is not the case when the forward model is used (Jordan and Rumelhart, 1992). The approach studied by (Narendra and Parthasarathy, 1990) and latter by <ref> (Gomi and Kawato, 1993) </ref> was slightly different from the one just discussed. A reference model was injected into the structure to give a desired transient system output y rather than a simple desired output r not varying with time (see figure 0.9).
Reference: <author> Gomm, J.B., J.T. Evans and D. </author> <title> Williams (1997). Development and performance of a neural-network predictive controller. </title> <journal> Control Engineering Practice 5(1), </journal> <pages> 49-59. </pages>
Reference-contexts: This way of introducing dynamics into a static network has the advantage of being simple to implement but it has restricted dynamics properties compared to a recurrent neural network <ref> (Gomm et al., 1997) </ref>. In the other hand a recurrent neural network tends to further complicate the learning (Venugopal and al., 1994). Note that X is the vector of the network inputs.
Reference: <author> Gorinevsky, Dimitry M. </author> <year> (1993). </year> <title> Modelling of direct motor program learning in fast human arm motions. </title> <booktitle> Biological Cybernetics 69, </booktitle> <pages> 219-228. </pages>
Reference-contexts: If the neuro-controller is not a true inverse model of the system this can lead to control inconsistency which can not be overcome without feedback. It is not trivial at all to learn the inverse dynamics of highly delayed and noisy systems that are widespread in the real world <ref> (Gorinevsky, 1993) </ref>. However, even if one can get a true inverse model of the system, this controller is still going to be very sensitive to system disturbances and delays.
Reference: <author> Grossberg, S. </author> <year> (1973). </year> <title> Contour enhancement, short term memory and constancies in reverberating neural networks. </title> <booktitle> Studies in Applied Mathematics 52, </booktitle> <pages> 217-57. </pages>
Reference: <author> Grossberg, S. </author> <year> (1978). </year> <title> A theory of visual coding, memory and development. In: Formal theories of visual perception. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Happel, B.L.M. </author> <title> and J.M.J Murre (1994). Design and evolution of modular neural network architectures. </title> <booktitle> Neural Networks 7, </booktitle> <pages> 985-1004. </pages>
Reference-contexts: Each of these elements itself only has a simple function. Complex functions are obtained as emergent behaviour of the dynamic interactions of the simple elements" <ref> (Happel and Murre, 1994) </ref>. The first attempt to build such a computing system was made by (McCulloch and Pitts, 1943) in 1943. They developed an analogous system composed of interconnected simple computing units based on the model of a neuron.
Reference: <author> Hecht-Nielsen, R. </author> <year> (1991). </year> <title> Neurocomputing. </title> <publisher> Addison Wesley. </publisher>
Reference: <author> Hopfield, J.J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> In: Proceeding of the National Acadamy of Sciences of the U.S.A.. </booktitle> <volume> Vol. 79. </volume> <pages> pp. 2554-2558. </pages>
Reference-contexts: Early work of Grossberg dealt with the problem of cooperation and competition among cells (Grossberg, 1973; Grossberg, 1978). However a real resurgence of this field started in the eighties with the work on recurrent neural network done by Hopfield <ref> (Hopfield, 1982) </ref> and the model of self-organising map developed by Kohonen (Kohonen, 1982). A strong impulse to the connexionnist models was given in 1986 when (Rumulhart and McClelland, 1986; LeCun, 1987) rediscovered the back-propagation learning algorithm developed in first place by (Werbos, 1974).
Reference: <author> Hornik, K. </author> <year> (1993). </year> <title> Some new results on neural network approximation. </title> <booktitle> Neural Networks 6, </booktitle> <pages> 1069-1072. </pages>
Reference: <author> Houk, James C. and Steven P. </author> <title> Wise (1995). Distributed modular architectures linking basal ganglia, cerebellum, and cerebral cortex: Their role in planning and controlling action. </title> <type> Cerebral Cortex 2, </type> <pages> 95-110. </pages>
Reference: <author> Hunt, K. J. and T. A. </author> <title> Johansen (1996). Adaptive local controller networks. </title> <note> In preparation. </note>
Reference: <author> Hunt, K.J., D. Sbarbaro, R. Zbikowski and P.J. </author> <month> Gawthrop </month> <year> (1992). </year> <title> Neural networks for control systems-a survey. </title> <type> Automatica 28(6), </type> <pages> 1083-1112. </pages>
Reference-contexts: The important non-linear diversity is the primary reason why no systematic and generally applicable theory for non-linear control design has yet evolved. It is the ability of neural networks to model non-linear systems which is the feature to be most readily exploited in the synthesis of non-linear controllers <ref> (Hunt et al., 1992) </ref>. "A connectionnist model (i.e. artificial neural network) consists of many interconnected, autonomous basic elements. Each of these elements itself only has a simple function. Complex functions are obtained as emergent behaviour of the dynamic interactions of the simple elements" (Happel and Murre, 1994). <p> The first method to develop a neuro-controller involved trying to replicate a human controller. (Widrow and Smith, 1964) applied this technic to control the inverted pendulum. This can be useful for plants controlled by humans for which it is difficult to design a standard controller <ref> (Hunt et al., 1992) </ref>. The neural training consists simply of learning the mapping between the sensory information received by the human controller and the control input (see figure 0.4).
Reference: <author> Jacobs, R.A. and M.I. </author> <title> Jordan (1991). A competitive modular connectionist architecture. In: Neural Information Processing Systems 3 (Lippman and al., </title> <editor> Eds.). </editor> <volume> Vol. </volume> <pages> 3. </pages>
Reference: <author> Jacobs, R.A. and M.I. </author> <title> Jordan (1993). Learning piecewise control strategies in a modular neural network architecture. </title> <journal> IEEE Transaction on Systems, Man, and Cybernetics 23(2), </journal> <pages> 337-345. </pages>
Reference-contexts: These are important restrictions of the usefulness of this algorithm. It is difficult to use this scheme for control purposes as we can not easily design controllers out of the unclear modular representations. Note however that this algorithm has been used in <ref> (Jacobs and Jordan, 1993) </ref> to develop an inverse controller of a two joint robot's arm using the method of (Kawato et al., 1987) described earlier.
Reference: <author> Jacobs, R.A., M.I. Jordan and A.G. </author> <title> Barto (1991a). Task decomposition competition in a modular connectionist architecture: The what and where vision tasks. </title> <booktitle> Cognitive Science 15, </booktitle> <pages> 219-250. </pages>
Reference: <author> Jacobs, R.A., M.I. Jordan, S.J. Nowlan and G.E. </author> <title> Hinton (1991b). Adaptive mixture of local experts. </title> <booktitle> Neural Computation pp. </booktitle> <pages> 79-87. </pages>
Reference: <author> Johansen, T. A. and B. A. </author> <title> Foss (1992). A narmax model representation for adaptive control based on local model. Modeling, Identification, </title> <booktitle> and control 13(1), </booktitle> <pages> 25-39. </pages>
Reference: <author> Johansen, T. A. and B. A. </author> <title> Foss (1993). Constructing NARMAX models using ARMAX models. </title>
Reference-contexts: Simple local models are used to describe the system within each region. A global model is formed by interpolating the local models using interpolation functions w, depending on the operating condition <ref> (Johansen and Foss, 1993) </ref>. An important advantage of this method is the facility to transform the LMN into a local controller network.
Reference: <editor> Int. J. </editor> <booktitle> Control 58, </booktitle> <pages> 1125-1153. </pages>
Reference: <author> Johansen, T. A. and B. A. </author> <title> Foss (1995). Identification of non-linear system structure and parameters using regime decomposition. </title> <type> Automatica 31(2), </type> <pages> 321-326. </pages>
Reference: <author> Jones, R. D. and al. </author> <year> (1991). </year> <title> Nonlinear adaptive networks: A little theory, a few applications. </title> <type> Technical Report 91-273. </type> <institution> Los Alamos National Lab, NM. </institution>
Reference: <author> Jordan, M.I. and R.A. </author> <title> Jacobs (1994). Hierarchical mixtures of experts and the em algorithm. </title> <booktitle> Neural Computation 6, </booktitle> <pages> 181-214. </pages>
Reference-contexts: Rather than a spatial clustering approach a very interesting criteria has been used by Jacobs and Jordan when they conceived the "Adaptive Mixture of Experts" (AME) (Jacobs et al., 1991b; Jacobs and Jordan, 1991; Jacobs et al., 1991a) (see figure 0.18) and further improved it in <ref> (Jordan and Jacobs, 1994) </ref> using the EM algorithm to enable a self-hierarchical decomposition of the problem. The idea involves gating the input vectors according to the approximation (learning) capability of each module. 21 Two types of systems are used that are both multi-layer perceptron neural networks.
Reference: <author> Jordan, Michael I. and David E. </author> <title> Rumelhart (1992). Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science pp. </journal> <pages> 307-354. </pages>
Reference-contexts: Indeed this method can not be generalised to many systems. Back-propagation through time neuro-control learning To avoid the problem of the specialised inverse control learning a number of researchers (Nguyen and Widrow, 1990; Narendra and Parthasarathy, 1990; Jordan and Rumelhart, 1992) have independently developed the "back-propagation through time" <ref> (Jordan and Rumelhart, 1992) </ref> neuro-control leaning method. The problem of the specialised inverse control learning is that the performance error e y = r y is not reliable because it is not directly related to the neuro-controller output u. <p> However the use of the system output should give better results. For instance the use of the system output can produce an exact inverse even when the forward model is inexact; this is not the case when the forward model is used <ref> (Jordan and Rumelhart, 1992) </ref>. The approach studied by (Narendra and Parthasarathy, 1990) and latter by (Gomi and Kawato, 1993) was slightly different from the one just discussed.
Reference: <author> Karmiloff-Smith, </author> <title> Annette (1994). Precis of beyond modularity: A developmental perspective on cognitive science. </title> <booktitle> Behavioral and Brain Sciences 17, </booktitle> <pages> 693-745. </pages>
Reference: <author> Kawato, M., K. Furukawa and R. </author> <title> Suzuki (1987). Forward models: Supervised learning with a distal teacher. </title> <journal> CBiological Cybernetics pp. </journal> <pages> 169-185. </pages> <month> Kawato, </month> <title> Mitsuo (1989). Adaptation and learning in control of voluntary movement by the central nervous system. </title> <booktitle> Advanced Robotics 3(3), </booktitle> <pages> 229-249. </pages>
Reference-contexts: Note however that this algorithm has been used in (Jacobs and Jordan, 1993) to develop an inverse controller of a two joint robot's arm using the method of <ref> (Kawato et al., 1987) </ref> described earlier. The idea of gating the input vectors according to the approximation capability of the local models has extensively been studied in the frame of spatial clustering as well as in the context 22 of "clustering free gating systems" (see (Ronco, 1997)).
Reference: <author> Keele, Steven W., Peggy Jennings, Steven Jones, David Caulton and Asher Cohen (1995). </author> <title> On the modularity of sequence representation. Journal of Motor Behavior 27(1), </title> <type> 17-30. 31 Kohonen, </type> <institution> T. </institution> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <booktitle> Biological Cybernetics 43, </booktitle> <pages> 59-69. </pages>
Reference: <author> Lapedes, A. and R. </author> <title> Farber (1987). How neural nets work. </title> <booktitle> In: Neural Information Processing Systems (Anderson, Ed.). </booktitle> <pages> pp. 442-456. </pages> <institution> New-York: American Institute of Physics. </institution>
Reference: <author> LeCun, I. </author> <year> (1987). </year> <title> Modeles connexionnistes de l'apprentissage. </title> <type> PhD thesis. </type> <institution> Universite Paris VI. Paris, France. </institution>
Reference: <author> Martinez, D. </author> <year> (1992). </year> <title> OFFSET: une methode de construction incrementale de reseaux de neur-ones multicouche et son application a la conception d'un autopilote automobile.. </title> <type> PhD thesis. </type> <institution> Universite Paul Sabatier. Toulouse, France. </institution>
Reference-contexts: is reported in the following section. 0.3 Gated modular neural networks for control "The challenge of the next generation of neural networks is not learning by individual weight updating, but the composing of network modules and how to resolve the competition" and the cooperation "of the different modules" (Muhlenbein in <ref> (Martinez, 1992) </ref>). Embedding modularity into Neural Networks (NN) gives many advantages over the use of single NN (see for further details (Ronco and Gawthrop, 1995)). "Modular Neural Network" (MNN) refers here to a network performing a local computation of the information.
Reference: <author> McCulloch, W.S. and W. </author> <title> Pitts (1943). A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics 5, </journal> <pages> 115-133. </pages>
Reference-contexts: Each of these elements itself only has a simple function. Complex functions are obtained as emergent behaviour of the dynamic interactions of the simple elements" (Happel and Murre, 1994). The first attempt to build such a computing system was made by <ref> (McCulloch and Pitts, 1943) </ref> in 1943. They developed an analogous system composed of interconnected simple computing units based on the model of a neuron. This algorithm was revised some twenty years later by (Rosenblatt, 1960; Widrow and Hoff, 1960). They respectively called their algorithm "perceptron" and "adaline".
Reference: <author> Middleton, R. H., G. C. Goodwin, D. J. Hill and D. Q. </author> <title> Mayne (1988). Design issues in adaptive control. </title> <booktitle> IEEE Transaction on Automatic Control 33(1), </booktitle> <pages> 50-58. </pages>
Reference-contexts: Thus the selection of the controller according to the modelling error is feasible. This idea has been used by (Narendra et al., 1995; Narendra and Balakrishan, 1997) to develop the "multiple switched models" (MSM) scheme. However, such a multiple controllers scheme was prior introduced by <ref> (Middleton et al., 1988) </ref> and further extended in (Morse, 1990; Morse et al., 1992; Weller and Goodwin, 1994) and coined as the "hysteresis switching algorithm".
Reference: <author> Miikkulainen, R. and M.G. </author> <title> Dyer (1991). Natural language processing with modular pdp networks and distributed lexicon. </title> <booktitle> Cognitive Science 15, </booktitle> <pages> 343-399. </pages>
Reference-contexts: The learning procedure implies two stages. First, the centre of each partition is found by unsupervised learning. Following this stage, a supervised learning is used to train the models connected to each partitioning unit. A similar method to automaticly cluster the input space but using more sophisticated models (see <ref> (Miikkulainen and Dyer, 1991) </ref>) consists of using a Kohonen algorithm to cluster the input space and then connect to each local region a NN.
Reference: <author> Miller, W. T., R. S. Sutton and P. J. </author> <title> Werbos (1990). Neural Networks for Control. </title> <publisher> MIT Press. </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: However this leads as well to a system inverse based neuro-controller since this is the desired output y that drives the neuro-controller. Biological control and neuro-control based on system inverse Neuro-control methods based on system inverse, although applied, mostly in robotics (see for some examples <ref> (Miller et al., 1990) </ref>), have serious drawbacks. The most important concerns the lack of feedback. If the neuro-controller is not a true inverse model of the system this can lead to control inconsistency which can not be overcome without feedback.
Reference: <author> Mills, P. M., A. Y. Zomaya and M. O. </author> <month> Tade </month> <year> (1994). </year> <title> Adaptive model-based control using neural networks. </title> <journal> International J. Control 60, </journal> <pages> 1163-1192. </pages>
Reference: <author> Minski, </author> <title> M.L. and S.A. Papert (1969). Perceptron. </title> <publisher> MIT press. </publisher> <address> Cambridge, MA. </address>
Reference-contexts: They respectively called their algorithm "perceptron" and "adaline". Their major contribution concerns the implementation of a learning rule based on the minimisation of the quadratic approximation error. A total blackout occurred several years later as <ref> (Minski and Papert, 1969) </ref> demonstrated the very poor mapping potential of these algorithms. Note that at this period the machine of Turing (i.e. the sequential computer with located memory and so on) was seen as the model of the brain.
Reference: <author> Monrocq, C. </author> <year> (1993). </year> <title> A probabilistic approach which provides a modular and adaptive neural network architecture for discrimination. </title> <booktitle> In: Third International Conference on Artificial Neural Networks. </booktitle> <volume> Vol. 372. </volume> <pages> pp. 252-256. </pages>
Reference: <author> Moody, J. and C. </author> <month> Darken </month> <year> (1989). </year> <title> Fast-learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation 1, </booktitle> <pages> 281-294. </pages>
Reference-contexts: A similar method to automaticly cluster the input space but using more sophisticated models (see (Miikkulainen and Dyer, 1991)) consists of using a Kohonen algorithm to cluster the input space and then connect to each local region a NN. In <ref> (Moody and Darken, 1989) </ref> the centres of the RBFs are also placed using a self-organising method. (Roberts and Tarassenko, 1994) proposed a constructive method to develop the LMN structure.
Reference: <author> Morse, A. S. </author> <year> (1990). </year> <title> Toward a unified theory of parameter adaptive control-tunability. </title> <booktitle> IEEE Transaction on Automatic Control 35(9), </booktitle> <pages> 1002-1012. </pages>
Reference: <author> Morse, A. S., D. Q. Mayne and G. C. </author> <title> Goodwin (1992). Applications of hysteresis switching in parameter adaptive control. </title> <booktitle> IEEE Transaction on Automatic Control 37(9), </booktitle> <pages> 1343-1354. </pages>
Reference: <author> Mountcastle, Vernon B. </author> <year> (1978). </year> <title> An organizing principle for cerebral function: The unit module and the distributed system. In: The Mindfull Brain (Edelman and Mountcastle, </title> <editor> Eds.). </editor> <publisher> MIT press. </publisher>
Reference-contexts: This definition is certainly loose but because currently there is no real definition of a MNN, this is the one adopted in this study (for a discussion about this issue see (Ronco and Gawthrop, 1995)). Modularity is a natural way to ease the learning of complex behaviour. Mountcastle <ref> (Mountcastle, 1978) </ref> and many others (Karmiloff-Smith, 1994; Houk and Wise, 1995; Carpenter, 1984) argue that modularity is a brain organising principle. (Feldman, 1989) and (Simon, 1981) highlight the fact that a complex behaviour requires the bringing together of several different kinds of knowledge and processing, which is not possible without structure
Reference: <author> Mure, J.M.J., R.H. Phaf and G. </author> <title> Wolters (1992). Calm: Categorizing and learning module. Neural Networks 5, 55-82. Murray-Smith, Roderick (1994). A Local Model Network Approach to Nonlinear Modelling. </title> <type> PhD thesis. </type> <institution> Dept. of Computer Science: U. of Strathclyde. </institution> <address> Glasgow, Scotland, UK. 32 Murray-Smith, </address> <month> Roderik. </month> <title> and Henrik Gollee (1994). A constructive learning algorithm for local model networks. </title> <booktitle> In: IEEE Worshop on Computer-intensive methods in control and signal processing. </booktitle> <pages> pp. 21-29. </pages>
Reference-contexts: Some self-organising methods to cluster the input space have been developed. The operating space is partitioned using a binary tree approach in (Stoelting, 1990). A more sophisticated approach can be found in the model developed by <ref> (Mure et al., 1992) </ref>. This model is named "CALM" for Categorising And Learning Module. A CALM module tends to autonomously cluster stimuli belonging to the same region by way of inhibitory connections between neurons involving competition in the CALM module.
Reference: <author> Narendra, K. S. and J. </author> <month> Balakrishan </month> <year> (1997). </year> <title> Adaptive control using multiple models. </title> <journal> IEEE transactions on Automatic Control 42(2), </journal> <pages> 171-187. </pages>
Reference-contexts: The 23 use of instantaneous and long term measures of the modelling error makes this index a reliable estimate modelling performance <ref> (Narendra and Balakrishan, 1997) </ref>. <p> This small "parametric distance" ensures a fast adaptation. Hence, this scheme enables the overall controller to handle fast changes in the plant whilst ensuring a small steady state error. This scheme has another fundamental advantage. The authors have studied and proved the stability properties of the overall network <ref> (Narendra and Balakrishan, 1997) </ref>. If all the models are adaptive the overall system is stable provided a small and fixed interval is allowed between switches. If all the models are fixed the stability can be achieved if at least one model is sufficiently close to the plant. <p> This stability is therefore highly dependent on the modelling accuracy that cannot be ensured. However, the use of fixed models and one adaptive 24 model (reinitialisable or not) ensures overall stability. Hence, this compromise yields again very desirable properties. In <ref> (Narendra and Balakrishan, 1997) </ref> it is shown also that the number and locations of the models in the operating space must be related to the "sensitivity region" (i.e. the region where the models and plant parameters differ the most) and not be uniformly distributed.
Reference: <author> Narendra, K. S. and K. </author> <title> Parthasarathy (1990). Identification and control of dynamic systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks 1, </journal> <pages> 4-27. </pages>
Reference-contexts: For instance the use of the system output can produce an exact inverse even when the forward model is inexact; this is not the case when the forward model is used (Jordan and Rumelhart, 1992). The approach studied by <ref> (Narendra and Parthasarathy, 1990) </ref> and latter by (Gomi and Kawato, 1993) was slightly different from the one just discussed. A reference model was injected into the structure to give a desired transient system output y rather than a simple desired output r not varying with time (see figure 0.9).
Reference: <author> Narendra, Kumpati S., Jeyendran Balakrishnan and Kemal M. </author> <month> Ciliz </month> <year> (1995). </year> <title> Adaptation and learning using multiple models, switching, and tuning. </title> <journal> IEEE Control Systems 3, </journal> <pages> 37-51. </pages>
Reference-contexts: Hence, this scheme implies the selection of only one controller at each instant The authors show, according to control simulations of linear plants having their parameters changing abruptly over time, that the model-controller pairs should not all be fixed or adaptive <ref> (Narendra et al., 1995) </ref>. On one hand fixed controllers are computationally more efficient than adaptive ones since adaptation implies intensive computation at each instant. Moreover an adaptive controller has difficulty to cope with abrupt changes in the plant (e.g. external disturbances, variation of the system parameters). <p> On the other hand a very large number of fixed models could be required to achieve desired control performances (e.g. small steady state error) where only one adaptive controller could suffice. Hence, the best compromise according to the authors <ref> (Narendra et al., 1995) </ref> is the use of fixed models and one or few adaptive models that can be reinitialised to fit the currently selected controller. <p> This algorithm is constructive and shares very similar features with the one developed in (Ronco, 1997; Ronco and Gawthrop, submittedc). The idea is to add a new adaptive model-controller pair as soon the control error exceeds a certain threshold. During a simulation reported in <ref> (Narendra et al., 1995) </ref> this method leaded to an optimum placement and determination of the required number of model-controller pairs. Optimal construction of the network are also reported in (Ronco, 1997; Ronco and Gawthrop, submittedc). <p> Optimal construction of the network are also reported in (Ronco, 1997; Ronco and Gawthrop, submittedc). So far, the multiple switched model control capability has been discussed in the context of linear systems. Surprisingly <ref> (Narendra et al., 1995) </ref> are not using linear models to deal with non-linear systems. Instead they use Multi-Layer Perceptron (MLP) as model-controller pairs. <p> <ref> (Narendra et al., 1995) </ref> are not using linear models to deal with non-linear systems. Instead they use Multi-Layer Perceptron (MLP) as model-controller pairs. Although they highlighted positive features of the MLP this network should not be suitable in the context of control due at least to its unclear function representation. (Narendra et al., 1995) themselves highlight a serious weakness of the MLP as an adaptive algorithm. <p> Otherwise a linear controller is required to stabilise the system and therefore the MLP can only be used to improve performance <ref> (Narendra et al., 1995) </ref>. In (Ronco, 1997; Ronco and Gawthrop, submittedc) the MSM is used to control non-linear systems. To reduce the number of model-controller pairs and increase the control performance smooth functions are used as building block of the network. These function are low order polynomials. <p> Furthermore the connection of the modules and their region of validity is going to be difficult to establish due to the unclear representation achieved by a MLP. 26 The idea of using the module performances for their own gating has been further extended in the "multiple switched models" <ref> (Narendra et al., 1995) </ref>. The idea is to use the performance of various models to select their connected controllers.
Reference: <author> Nguyen, D. H. and B. </author> <title> Widrow (1990). Neural Networks for Self-learning Control Systems. </title> <journal> IEEE Control Systems Magazine 10, </journal> <pages> 18-23. </pages>
Reference: <author> Nowlan, S.J. Hinton, G.E. </author> <year> (1991). </year> <title> Evaluation of adaptive mixtures of competing experts. </title> <booktitle> In: Neural Information Processing Systems 3 (Lippmann, Ed.). </booktitle> <volume> Vol. 3. </volume> <booktitle> Omatu, Sigeru, Marzuki Khalid and Rubiyah Yussof (1990). Neuro-Control and its Applications. </booktitle> <publisher> Springer. </publisher>
Reference: <author> Ortega, J. Gomez and Camacho E.F. </author> <year> (1996). </year> <title> Mobile robot navigation in a partially structured static environment, using neural predictive control. </title> <journal> Control Engineering Practice 4(12), </journal> <pages> 1669-1679. </pages>
Reference-contexts: This gives a neuro-model based predictive controller corresponding to figure 0.11. Neural network could be used as well as a controller. For instance, in <ref> (Ortega and E.F., 1996) </ref>, a neural network is used to implement the cost function and hence increase the computing speed for a robotics path tracking problem. 0.2.3 Concluding remarks regarding neuro-control techniques As discussed during the introduction, the neural networks and especially the widely used multilayer perceptron (MLP) have got important
Reference: <author> Park, M. G. and N. Z. </author> <title> Cho (1995). Self-tuning control of a nuclear-reactor using a gaussian function neural-network. </title> <booktitle> Nuclear Technology 110, </booktitle> <pages> 285-293. </pages>
Reference: <author> Platt, J. </author> <year> (1991). </year> <title> A resource-allocating network for function interpolation. </title> <booktitle> Neural Computation 3, </booktitle> <pages> 213-225. </pages>
Reference-contexts: However, to achieve the best clustering implies taking into account the approximation capability of the local models. Thus, clustering and local model identification should not be separated but rather be an interactive process. An early attempt at doing this was carried out by <ref> (Platt, 1991) </ref>. He proposed to add a new model when a pattern presented to the LMN caused an error larger than a given threshold. This does not resolve the problem of determining 17 the region of activity of each local model which is vital especially whilst applying local learning.
Reference: <author> Poggio, T. and F. </author> <title> Girosi (1990). Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE 78, </booktitle> <pages> 1481-1497. </pages>
Reference: <author> Powell, M. J. D. </author> <year> (1987). </year> <title> Radial basis functions for multivariable interpolation: a review. </title> <booktitle> In: Algorithms for approximation. </booktitle> <pages> pp. 143-167. </pages> <publisher> Clarendon, press. </publisher>
Reference-contexts: Note that the LMN is a generalisation of the basis function neural network <ref> (Powell, 1987) </ref> that was developed in first place for classification purposes. Moreover it has very close connections to the fuzzy model developed by (Takagi and Sugeno, 1985; Sugeno and Kang, 1986).
Reference: <author> Psaltis, D., A. Sideris and A. A. </author> <title> Yamamura (1988). A multilayered neural network controller. </title> <journal> IEEE Control Systems Magazine 8, </journal> <pages> 17-21. </pages>
Reference-contexts: The system must be brought into the desired operating region where the controller will have to operate. This is difficult to achieve without strong a priori knowledge about the system. 6 Specialised inverse neuro-control learning <ref> (Psaltis et al., 1988) </ref> proposed a "specialised inverse learning". This is a goal directed neuro-control approach. This feature is the fundamental difference between specialised inverse learning and direct inverse learning. The network is trained on-line in order to minimise the control performance e y = r y (see figure 0.7). <p> It is so because e y is not directly related to the model output as it should be. Hence, e y can be totally uncorrelated to the relevant neuro-controller learning performance e u = u u. The error can be of inverse sign or of a completely different magnitude. <ref> (Psaltis et al., 1988) </ref> argue that to apply this method the Jacobian of the process is necessary. However it seems that even a crude approximation of the error e u can allow convergence of the neuro-controller learning.
Reference: <author> Reilly, D.L., L.N. Cooper and Elbaum C. </author> <year> (1982). </year> <title> A neural model for category learning. </title> <booktitle> Biological Cybernetics 45, </booktitle> <pages> 35-41. </pages>
Reference-contexts: Although some euristics are used to speed this decomposition process it is clear that it is highly computation demanding. It is therefore not suitable for online modelling/control purposes. The algorithm conceived by Relly, Cooper and Elbaum, the RCE <ref> (Reilly et al., 1982) </ref>, is interesting for on-line decompositions purposes. It was developed some 15 years ago for binary classification purposes. This algorithm is somehow only a gating system but capable of non-convex clustering. This gating system is a feed-forward network composed of two different unit layers (see figure 0.16).
Reference: <author> Roberts, S. and L. </author> <month> Tarassenko </month> <year> (1994). </year> <title> A probalistic resource allocating network for novelty detection. </title> <booktitle> Neural Computation 6, </booktitle> <pages> 270-284. </pages>
Reference-contexts: In (Moody and Darken, 1989) the centres of the RBFs are also placed using a self-organising method. <ref> (Roberts and Tarassenko, 1994) </ref> proposed a constructive method to develop the LMN structure. They add a new model whenever an input occurs which is not near the centre of any of the RBFs' receptive fields.
Reference: <author> Ronco, </author> <title> Eric (1994). Apprentissage a complexite progressive dans les systemes connexionnistes. </title> <type> Master's thesis. </type> <institution> Institut National Polytechnique de Grenoble. Grenoble, France. </institution>
Reference: <author> Ronco, </author> <title> Eric (1997). Incremental Polynomial Controller Networks: two self-organising non-linear controllers. </title> <type> PhD thesis. </type> <institution> Faculty of Mechanical Engineering, University of Glasgow. </institution> <note> 33 Ronco, </note> <author> Eric and Peter J. </author> <month> Gawthrop </month> <year> (1995). </year> <title> Modular neural networks: a state of the art. </title> <type> Technical Report CSC-95026. </type> <institution> Centre for System and Control. Faculty of mechanical Engineering, University of Glasgow, Uk. </institution> <note> Available at www.mech.gla.ac.uk/~ericr/pub/surveyMNN.ps. </note>
Reference-contexts: The units g performs a radial clustering of the input space and the units h affine the clustering. This case corresponds to the situation where the first unit g 1 did not suffice to achieve an accurate clustering and therefore a second unit g 2 was added. 18 In <ref> (Ronco, 1997) </ref> the constructive clustering method of the RCE has been reused in the context of controller networks to sort out the problem of automatically clustering an input space and therefore automatically construct the network. Note that this network construction algorithm is equally applicable to a model network. <p> Note that this network construction algorithm is equally applicable to a model network. The "Clustered Controller Network" (CCN) used in <ref> (Ronco, 1997) </ref> is a simplified version of the LCN since the clustering is achieved on a single quantity only. There are several important advantages arising from this simple clustering. Among them is the facility to determine the neighbourhood of an operating condition. <p> A linear controller is then designed from this model. Any conventional control design methods can be used (e.g. pole placement, Model Reference Adaptive Controller. The latter is used in this study. Details about this control design approach can be found in the <ref> (Ronco, 1997) </ref> (chapter 1). A rbf is centred on the current operating condition and its width is set to almost zero. The new controller-rbf pair is finally added to the CCN. These features are the basis of the "Incremental Network Construction" (INC). <p> This is indeed arbitrary but sufficient in most cases to remove the undesirable controllers. A better criteria could be found in the singularity vector expressed by the Singular Value Decomposition (SVD) method used for the models approximation (see <ref> (Ronco, 1997) </ref> (chapter 2) for details). However this possibility has not been sufficiently investigate to be exposed here. <p> It is even simpler since no control design will be required. The only difficulty without a controller is to have a method to efficiently sample the plant, but this is a known problem in system identification. Although the method developed in <ref> (Ronco, 1997) </ref> can lead to an optimum clustering of an input space using RBF, in many cases, when the input space implies the clustering of more than a single dimension, this clustering is unlikely to be optimal. <p> The idea of gating the input vectors according to the approximation capability of the local models has extensively been studied in the frame of spatial clustering as well as in the context 22 of "clustering free gating systems" (see <ref> (Ronco, 1997) </ref>). In the latter it is the modelling error of a local model that is used to decide if its connected controller should be selected or not.
Reference: <author> Ronco, Eric and Peter J. </author> <month> Gawthrop </month> <year> (1996). </year> <title> Incremental linear controllers network. </title> <type> Technical Report CSC-96008. </type> <institution> Centre for system and control, U. of glasgow, UK. </institution> <note> (Available at www.mech.gla.ac.uk/~ericr/pub/ilcn pcd.ps.gz). </note>
Reference: <author> Ronco, Eric and Peter J. </author> <month> Gawthrop </month> <year> (1997a). </year> <title> Gated modular neural networks for modelling and control. </title> <type> Technical Report CSC-97008. </type> <institution> Centre for system and control, U. of glasgow, UK. </institution> <note> (Available at www.mech.gla.ac.uk/~ericr/pub/gmnn.ps.gz). </note>
Reference-contexts: There are few modular networks that apply systematic methods to do so. Those modular neural networks use a gating system to dispatch the information to the adequate modules. <ref> (Ronco and Gawthrop, 1997a) </ref> refer to them as the "gated modular neural networks". In brief, there are two main types of neural networks. In one hand there are the single neural networks best represented by the MLP.
Reference: <author> Ronco, Eric and Peter J. </author> <month> Gawthrop </month> <year> (1997b). </year> <title> Incremental model reference adaptive polynomial controllers network. </title> <type> Technical Report CSC-96008. </type> <institution> Centre for system and control, U. of glasgow, UK. </institution> <note> (Available at www.mech.gla.ac.uk/~ericr/pub/ipmracn.ps.gz). </note>
Reference: <author> Ronco, Eric and Peter J. </author> <month> Gawthrop </month> <year> (1997c). </year> <title> Incremental model reference adaptive polynomial controllers network. </title> <booktitle> In: Proceeding of the IEEE Conference on Decision and Control (CDC'97). </booktitle> <publisher> (In Press). </publisher>
Reference: <author> Ronco, Eric and Peter J. </author> <month> Gawthrop </month> <year> (1997d). </year> <title> Incremental model reference adaptive polynomial controllers network. </title> <type> Technical Report CSC-97003. </type> <institution> Centre for System and Control. University of Glasgow, UK. </institution> <note> (Available at www.mech.gla.ac.uk/~ericr/pub/ieee smc.ps.gz). </note>
Reference: <author> Ronco, Eric and Peter J. Gawthrop (submitteda). </author> <title> Incremental controller networks: A comparative study between two self-organising non-linear controllers. Journal of Modeling, Identification and Control. </title>
Reference: <author> Ronco, Eric and Peter J. Gawthrop (submittedb). </author> <title> Incremental model reference adaptive polynomial controllers network. </title> <journal> International Journal of Systems Science. </journal>
Reference: <author> Ronco, Eric and Peter J. Gawthrop (submittedc). </author> <title> Incremental polynomial model-controller network: a self organising non-linear controller. International Journal of Control, special issue on Multiple Model Approaches to Modelling and Control. </title>
Reference: <author> Ronco, Eric and Peter J. Gawthrop (submittedd). </author> <title> Polynomial models network for system mod-elling and control. Neural Computing Survey. </title>
Reference: <author> Ronco, Eric, Henrik Gollee and Peter J. </author> <month> Gawthrop </month> <year> (1996a). </year> <title> Modular neural network and self decomposition. </title> <type> Technical Report CSC-96012. </type> <institution> Centre for system and control, U. of glasgow, UK. </institution> <note> Available at www.mech.gla.ac.uk/~ericr/pub/mnnsd.ps.Z. </note>
Reference: <author> Ronco, Eric, Peter J. Gawthrop and Mohamed Abderrahim (1996b). </author> <title> Progressive local control. </title> <booktitle> In: World Automatic Conference. </booktitle> <pages> pp. 637-642. </pages>
Reference: <author> Ronco, Eric, Peter J. Gawthrop and Yasmine Mather (1996c). </author> <title> Incremental modular controllers network. </title> <booktitle> In: Proceeding of the International Conference on Intelligent and Cognitive Systems (ICICS'96). </booktitle>
Reference: <author> Rosenblatt, F. </author> <year> (1960). </year> <title> Perceptron simulation experiments. </title> <booktitle> In: Proceeding of the IRE. 34 Rueckl, J.G., K.R. Cave and Kosslyn S.M. </booktitle> <year> (1989). </year> <title> Why are 'what' and 'where' processed by separate cortical visual systems? a computational investigation. </title> <journal> Journal of Cognitive Neuroscience 1, </journal> <pages> 171-186. </pages>
Reference: <author> Rumulhart, D.E. and J.L. </author> <title> McClelland (1986). </title> <booktitle> Parallel Distributed Processing. </booktitle> <volume> Vol. 1. </volume> <publisher> MIT press. </publisher> <address> Cambridge. </address>
Reference: <author> Rusnak, A., M. Fikar, K. Najim and A. </author> <month> Meszaros </month> <year> (1996). </year> <title> Generalized predictive control based on neural networks. </title> <booktitle> Neural Processing Letters 4, </booktitle> <pages> 107-112. </pages>
Reference-contexts: This idea has been generalised by (Clarke et al., 1987) to deal with plant with complex dynamics (e.g. unstable inverse systems, time-varying time delay, etc), and plant model mismatch <ref> (Rusnak et al., 1996) </ref>. (Clarke et al., 1987) coined this controller as the "generalised predictive control" (GPC). The GPC is the main design method of a model based predictive controller (MBPC). A MBPC is composed of three main components: the system, its model and a function optimiser (see figure 0.10).
Reference: <author> Sanger, Terence D. </author> <year> (1991). </year> <title> A tree-structured adaptive network for function approximation in high-dimensional spaces. </title> <booktitle> IEEE transaction on Neural Networks 2(2), </booktitle> <pages> 285-293. </pages>
Reference-contexts: More sophisticated gating system would be preferable to implement the model/controller networks. A GMNN capable of a non-convex clustering is considered in the next section. Note that <ref> (Sanger, 1991) </ref> reports a technique to reduce a n dimentional input space into separate lower dimensional spaces. The problem of this method is that it assumes that many quantities are uncoupled and redundant.
Reference: <author> Sanner, R. M. and D. L. </author> <title> Akin (1990). Neuromorphic pitch attitude regulation of an underwater telerobot. </title> <journal> IEEE Control Systems Magazine 10, </journal> <pages> 62-67. </pages>
Reference: <author> Schmidt, Richard A. </author> <year> (1982). </year> <title> Motor Control and Learning. Human Kinetics Publishers. </title> <publisher> Cham-paign, Illinois. </publisher>
Reference-contexts: For example the bat swing in hitting a baseball requires 100msec although 150-200msec seem to be required when information processing is involved. Other examples of insufficient time to perform a motor problem by high level system are given in <ref> (Schmidt, 1982) </ref>. In addition, (Schmidt, 1982) has shown, according to a review of experiences carried out on deafferented animals, that sensory feedback is not required to perform complex control tasks. <p> For example the bat swing in hitting a baseball requires 100msec although 150-200msec seem to be required when information processing is involved. Other examples of insufficient time to perform a motor problem by high level system are given in <ref> (Schmidt, 1982) </ref>. In addition, (Schmidt, 1982) has shown, according to a review of experiences carried out on deafferented animals, that sensory feedback is not required to perform complex control tasks. For instance (Taub, 1976) describes cases of deafferented monkeys that do not seem to be affected when climbing, swinging, eating, etc. <p> For instance (Taub, 1976) describes cases of deafferented monkeys that do not seem to be affected when climbing, swinging, eating, etc. In brief, closed loop system would be used to make action more precise but would not be the primary cause of it <ref> (Schmidt, 1982) </ref>. Closed loop system will be important when process is slow and regulation is important as for the homeostasis behaviour.
Reference: <author> Schwarzenbach, J. and K.F. </author> <title> Gill (1992). System Modelling and Control. </title> <publisher> Edward Arnold. </publisher>
Reference-contexts: Thus, assumption of system linearity has been made to develop a control theory on a solid basis. In reality most of the systems are non-linear. However many systems can be represented without significant loss of accuracy by an equivalent linear representation <ref> (Schwarzenbach and Gill, 1992) </ref>. Control design from system linearisation is a widely applied technique in the industry. The other systems, having their numbers drastically increasing, are characterised by complex non-linear dynamics (e.g. high non-linearity, fast parameter variations, external disturbances).
Reference: <author> Shadmehr, Reza and Ferdinando A. </author> <month> Mussa-Ivaldi </month> <year> (1994). </year> <title> Adaptive representation of dynamics during learning of a motor task. </title> <journal> The Journal Of Neuroscience 14(5), </journal> <pages> 3208-3224. </pages>
Reference-contexts: Another very important statement is that, with practice, biological control systems can develop an accurate internal model of the plant under control (Shadmehr and Mussa-Ivaldi, 1994; Brashers-Krug et al., 1995; Keele et al., 1995; Kawato, 1989). This has been nicely demonstrated by <ref> (Shadmehr and Mussa-Ivaldi, 1994) </ref>. Subjects have to practice a reaching movement in a force field. At the beginning of the learning the movement of the hand is highly affected by the force field.
Reference: <author> Shadmehr, Reza, Tom Brashers-Krug and Ferdinando Mussa-Ivaldi (1995). </author> <title> Interference in learning internal models of inverse dynamics in humans. </title> <booktitle> Advances in Neural Information Processing Systems. </booktitle>
Reference: <author> Shamma, J. S. and M. </author> <month> Athans </month> <year> (1992). </year> <title> Gain scheduling: Potential hazards and possible remedies. </title> <journal> IEEE Control Systems Magazine 12(3), </journal> <pages> 101-107. </pages>
Reference: <author> Shamma, J.S. and M. </author> <month> Athans </month> <year> (1990). </year> <title> Analysis of gain scheduled control for nonlinear plants. </title> <booktitle> IEEE Transaction on Automatic Control 35, </booktitle> <pages> 898-907. </pages>
Reference: <author> Shorten, Robert (1997). </author> <title> Issues in the control of hybrid dynamical systems. In: Centre for Systems and Control Seminar. </title> <institution> Faculty of Engineering, University of Glasgow, </institution> <address> Scotland. </address>
Reference-contexts: If the models are only interpretable globally, due to interaction between each local models, the analysis is difficult and the risk of instability is high <ref> (Shorten, 1997) </ref>. * A local learning enables the use of different function structure for each of the local models. This could be important if one wishes to introduce a priori knowledge in the LMN through the use of different local models more suited to different operating regions.
Reference: <author> Simon, H. </author> <year> (1981). </year> <booktitle> The sciences of the artificial. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT press. </publisher>
Reference-contexts: Modularity is a natural way to ease the learning of complex behaviour. Mountcastle (Mountcastle, 1978) and many others (Karmiloff-Smith, 1994; Houk and Wise, 1995; Carpenter, 1984) argue that modularity is a brain organising principle. (Feldman, 1989) and <ref> (Simon, 1981) </ref> highlight the fact that a complex behaviour requires the bringing together of several different kinds of knowledge and processing, which is not possible without structure (i.e. modularity). Moreover, and according to Marr such a modularity can enable a learning economy.
Reference: <author> Slotine, Jean-Jacques E. </author> <year> (1985). </year> <title> The robust control of robot manipulators. </title> <journal> The International Journal of Robotics Research 4, </journal> <pages> 49-64. </pages>
Reference-contexts: However, even if one can get a true inverse model of the system, this controller is still going to be very sensitive to system disturbances and delays. Actually 8 <ref> (Slotine, 1985) </ref> showed that with an accurate model of a robot manipulator, the average tracking error of a desired target quickly degrades as uncertainty (e.g. disturbances) increases, with the system eventually becoming unstable. <p> This is actually very unlikely, especially at high speed, since strong coupling effects between joints occur that are sources of high disturbances <ref> (Slotine, 1985) </ref>. The high sensitivity of inverse controller to disturbances makes them unsuitable for fast control action. In summary we have a biological control system that mainly acts in an open-loop manner in order to ensure fast control actions.
Reference: <author> Steck, J.E., K Rokhsaz and S.P. </author> <month> Shue </month> <year> (1996). </year> <title> Linear and neural netwok feedback for flight control decoupling. </title> <journal> IEEE Control Systems Magazine 16(4), </journal> <pages> 22-30. </pages>
Reference: <author> Stoelting, R. K. </author> <year> (1990). </year> <note> Pharmacology and Physiology in Anesthetic Practice. Lippincott Press. </note>
Reference-contexts: Hence, more advance methods should be used to tackle the problem of determining the optimum size of the RBFs. Some self-organising methods to cluster the input space have been developed. The operating space is partitioned using a binary tree approach in <ref> (Stoelting, 1990) </ref>. A more sophisticated approach can be found in the model developed by (Mure et al., 1992). This model is named "CALM" for Categorising And Learning Module.
Reference: <author> Stokbro, K., J. A. Hertz and D. K. </author> <month> Umberger </month> <year> (1990). </year> <title> Exploiting neurons with localized receptive fields to learn chaos. </title> <journal> Journal of Complex Systems 4, 603-. </journal>
Reference: <author> Sugeno, M. and G. T. </author> <title> Kang (1986). Fuzzy modelling and control of multilayer incinerator. Fuzzy Sets and Systems 18, </title> <type> 329-346. </type> <note> 35 Szilas, </note> <author> N. and E. </author> <month> Ronco </month> <year> (1995). </year> <title> Action for learning in non-symbolic systems. </title> <booktitle> In: Europeen Conference on Cognitive Science. </booktitle>
Reference: <author> Takagi, T. and M. </author> <title> Sugeno (1985). Fuzzy identification of systems and its application to modeling and control. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics 15(1), </journal> <pages> 116-132. </pages> <month> Taub, </month> <title> E (1976). Movements in nonhuman primates deprived of somatosensory feedback. </title> <journal> Exercise and Sport Sciences Reviews 4, </journal> <pages> 335-374. </pages>
Reference: <author> Temeng, K. O., P. D. Schnelle and T. J. </author> <month> McAvoy </month> <year> (1995). </year> <title> Model-predictive control of an industrial packed-bed reactor using neural networks. </title> <journal> J. Process Control 5, </journal> <pages> 19-27. </pages>
Reference: <author> Toates, F.M. </author> <year> (1975). </year> <title> Control theory in biology and experimental psychology. </title> <publisher> London. </publisher>
Reference: <author> Venugopal, K.P and al. </author> <year> (1994). </year> <title> A recurrent neural network controller and learning algorithm for the on-line learning control of autonomous underwater vehicles. </title> <booktitle> Neural Networks 7(5), </booktitle> <pages> 833-846. </pages>
Reference-contexts: This way of introducing dynamics into a static network has the advantage of being simple to implement but it has restricted dynamics properties compared to a recurrent neural network (Gomm et al., 1997). In the other hand a recurrent neural network tends to further complicate the learning <ref> (Venugopal and al., 1994) </ref>. Note that X is the vector of the network inputs.
Reference: <author> Weller, S. R. and G. C. </author> <title> Goodwin (1994). Hysteresis switching adaptive control of linear mul-tivariable systems. </title> <booktitle> IEEE Transaction on Automatic Control 39(7), </booktitle> <pages> 1360-1375. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavior Sciences. </title> <type> Ph.D. Thesis. </type> <institution> Harvard University, Committee on Applied Mathematics. </institution>
Reference-contexts: A strong impulse to the connexionnist models was given in 1986 when (Rumulhart and McClelland, 1986; LeCun, 1987) rediscovered the back-propagation learning algorithm developed in first place by <ref> (Werbos, 1974) </ref>. It enabled the use of the multi-layer perceptron which superseded the poor modelling capability of its single layer ancestor, the perceptron.
Reference: <author> Widrow, B. and F.W. </author> <title> Smith (1964). </title> <booktitle> Pattern-recognizing control systems. In: Proceeding of Computer and Information Sciences. </booktitle>
Reference-contexts: As seen with the MLP a supervised neural network is naturally fitted to mimic the behaviour of another system. The first method to develop a neuro-controller involved trying to replicate a human controller. <ref> (Widrow and Smith, 1964) </ref> applied this technic to control the inverted pendulum. This can be useful for plants controlled by humans for which it is difficult to design a standard controller (Hunt et al., 1992).
Reference: <author> Widrow, B. </author> <title> and M.E. Hoff (1960). Adaptive switching circuits. </title> <booktitle> WESCON Conv. Rec. </booktitle> <pages> pp. 96-140. </pages>
Reference: <author> Widrow, B. and S. D. </author> <title> Stearns (1985). Adaptive Signal Processing. </title> <publisher> Prentice-Hall. </publisher> <address> Englewood Cliffs. </address>
Reference: <author> Zhang, Y., P. Sen and G.E. </author> <title> Hearn (1995). An on-line trained adaptive neural controller. </title> <journal> IEEE Control Systems Magazine 15, </journal> <pages> 67-75. 36 </pages>
Reference-contexts: However it seems that even a crude approximation of the error e u can allow convergence of the neuro-controller learning. For instance, from the sign of the plant derivative, efficient neuro-control learning has been obtained <ref> (Zhang et al., 1995) </ref>. Indeed this method can not be generalised to many systems.
References-found: 117

