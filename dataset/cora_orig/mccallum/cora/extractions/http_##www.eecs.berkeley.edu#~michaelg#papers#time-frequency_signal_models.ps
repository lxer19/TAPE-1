URL: http://www.eecs.berkeley.edu/~michaelg/papers/time-frequency_signal_models.ps
Refering-URL: http://www.eecs.berkeley.edu/~michaelg/publications.html
Root-URL: 
Title: TIME-FREQUENCY SIGNAL MODELS FOR MUSIC ANALYSIS, TRANSFORMATION, AND SYNTHESIS  
Author: Michael Goodwin Martin Vetterli 
Address: Berkeley  
Affiliation: Department of Electrical Engineering and Computer Science Center for New Music and Audio Technologies University of California at  
Abstract: In signal analysis-synthesis, the analysis derives a set of parameters that the synthesis uses to reconstruct the original signal. In musical applications, this reconstruction should be perceptually accurate, and the parameterization should allow for such desirable signal modifications as time-scaling, pitch-shifting, and cross-synthesis; the analysis parameters should correspond to a signal model that is flexible enough to allow these transformations. Sinusoidal modeling meets this flexibility requirement, but has difficulty representing some salient features of musical signals such as attack transients and noiselike processes. In this paper, sinusoidal modeling is reviewed and some variations are proposed to account for its shortcomings; also, wavelet-based representations of musical signals are considered. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> X. Serra and J. Smith. </author> <title> Spectral modeling synthesis: A sound analysis / synthesis system based on a deterministic plus stochastic decomposition. </title> <journal> Computer Music Journal, </journal> <volume> 14(4) </volume> <pages> 12-24, </pages> <month> Winter </month> <year> 1990. </year>
Reference-contexts: Since this sum-of-partials model is not well-suited for representing broadband noise processes, an additive noise component s (t) is often included in the signal model, resulting in a deterministic plus stochastic decomposition <ref> [1] </ref>: x (t) = d (t) + s (t). For musical signals, the additive noise accounts for such inherently stochastic musical features as breath noise. <p> For musical signals, the additive noise accounts for such inherently stochastic musical features as breath noise. Because these processes are an integral part of music, the noise component must be modeled independently of the partials and reinjected at the synthesis stage to insure the realism of the output music <ref> [1, 2] </ref>; this inclusion of noise is a prerequisite for perceptual losslessness. Analysis methods for the sinusoidal model are generally frame-by-frame approaches; the analysis parameters are then frame-rate representations of the time-varying am-plitude and frequency tracks of each partial. <p> Analysis methods for the sinusoidal model are generally frame-by-frame approaches; the analysis parameters are then frame-rate representations of the time-varying am-plitude and frequency tracks of each partial. In <ref> [1, 3, 4] </ref>, the analysis uses the short-time Fourier transform (STFT); the parameters of the partials in a given frame are found by estimating the amplitude, frequency, and phase of the peaks in the magnitude spectrum of the oversampled discrete Fourier transform (DFT) of that frame. <p> For signals such as music with dynamic spectral content, line tracking is a difficult problem, and a variety of algorithms have been proposed <ref> [1, 3] </ref>. After partial continuity is established by line tracking, it is necessary to interpolate the frame-rate partial parameters to determine the sample-rate oscillator control functions. Typically, interpolation is done using low-order polynomial models such as linear amplitude and cubic phase.
Reference: [2] <author> M. Goodwin. </author> <title> Residual modeling in music analysis-synthesis. </title> <publisher> ICASSP-1996. </publisher>
Reference-contexts: For musical signals, the additive noise accounts for such inherently stochastic musical features as breath noise. Because these processes are an integral part of music, the noise component must be modeled independently of the partials and reinjected at the synthesis stage to insure the realism of the output music <ref> [1, 2] </ref>; this inclusion of noise is a prerequisite for perceptual losslessness. Analysis methods for the sinusoidal model are generally frame-by-frame approaches; the analysis parameters are then frame-rate representations of the time-varying am-plitude and frequency tracks of each partial. <p> should only contain the broadband stochastic component of the signal model; models of the residual are generally simple and can not specifically account for these features, so they will not be preserved by analysis-synthesis of the residual and thus will not appear accurately in the final deterministic plus stochastic reconstruction <ref> [2] </ref>, which will then be perceptually lossy. Note that narrowband noise is represented by the partials since it can be expressed as a sinusoid modulated by a lowpass random process, namely a sinusoid varying at the frame-rate time scale.
Reference: [3] <author> R. McAulay and T. Quatieri. </author> <title> Speech analysis / synthesis based on a sinusoidal representation. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Analysis methods for the sinusoidal model are generally frame-by-frame approaches; the analysis parameters are then frame-rate representations of the time-varying am-plitude and frequency tracks of each partial. In <ref> [1, 3, 4] </ref>, the analysis uses the short-time Fourier transform (STFT); the parameters of the partials in a given frame are found by estimating the amplitude, frequency, and phase of the peaks in the magnitude spectrum of the oversampled discrete Fourier transform (DFT) of that frame. <p> For signals such as music with dynamic spectral content, line tracking is a difficult problem, and a variety of algorithms have been proposed <ref> [1, 3] </ref>. After partial continuity is established by line tracking, it is necessary to interpolate the frame-rate partial parameters to determine the sample-rate oscillator control functions. Typically, interpolation is done using low-order polynomial models such as linear amplitude and cubic phase. <p> given by ^ fi q;i [n] = fi q;i + ! q;i n + ff q;i n + fi q;i n (4) where fi and ! enforce phase and frequency matching constraints at the frame boundaries, and ff and fi are chosen the make the total phase progression maximally smooth <ref> [3] </ref>. Sinusoidal modeling has found many applications in speech and audio coding and analysis-synthesis. <p> Sinusoidal modeling has found many applications in speech and audio coding and analysis-synthesis. This is primarily because the representation in terms of sinusoidal parameters is efficient and readily allows for such desirable transformations as time-scaling, pitch-shifting, and cross-synthesis <ref> [3, 4, 6] </ref>; it also provides for novel modifications based on a musical timbre space, such as arbitrary interpolation between disparate sounds. However, sinusoidal modeling only derives an accurate reconstruction for signals that vary smoothly on a time scale comparable to the frame rate.
Reference: [4] <author> T. Quatieri and R. McAulay. </author> <title> Speech transformations based on a sinusoidal representation. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Analysis methods for the sinusoidal model are generally frame-by-frame approaches; the analysis parameters are then frame-rate representations of the time-varying am-plitude and frequency tracks of each partial. In <ref> [1, 3, 4] </ref>, the analysis uses the short-time Fourier transform (STFT); the parameters of the partials in a given frame are found by estimating the amplitude, frequency, and phase of the peaks in the magnitude spectrum of the oversampled discrete Fourier transform (DFT) of that frame. <p> Sinusoidal modeling has found many applications in speech and audio coding and analysis-synthesis. This is primarily because the representation in terms of sinusoidal parameters is efficient and readily allows for such desirable transformations as time-scaling, pitch-shifting, and cross-synthesis <ref> [3, 4, 6] </ref>; it also provides for novel modifications based on a musical timbre space, such as arbitrary interpolation between disparate sounds. However, sinusoidal modeling only derives an accurate reconstruction for signals that vary smoothly on a time scale comparable to the frame rate.
Reference: [5] <author> E. George and M. Smith. </author> <title> Analysis-by-synthesis / overlap-add sinusoidal modeling applied to the analysis and synthesis of musical tones. </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> 40(6) </volume> <pages> 497-516, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This contrasts with the time-domain analysis-by-synthesis proposed in <ref> [5] </ref>, where the partials are estimated by exhaustively searching for sinusoids that correlate strongly with the signal frame; when the maximally correlated sinusoid is found, its contribution is subtracted from the frame and the process is iterated as in a matching pursuit.
Reference: [6] <author> X. Rodet and P. Depalle. </author> <title> Spectral envelopes and inverse FFT synthesis. </title> <booktitle> Proceedings of the 93rd Audio Engineering Society Convention, </booktitle> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Synthesis can also be done in the frequency domain by accumulating the spectral contributions of the partials for each frame and then using an inverse DFT and overlap-add to construct the output from the frame-by-frame spectra <ref> [6] </ref>. This approach will not be discussed further, however, since time-domain synthesis more directly illustrates the signal representation issues this paper is concerned with. <p> Sinusoidal modeling has found many applications in speech and audio coding and analysis-synthesis. This is primarily because the representation in terms of sinusoidal parameters is efficient and readily allows for such desirable transformations as time-scaling, pitch-shifting, and cross-synthesis <ref> [3, 4, 6] </ref>; it also provides for novel modifications based on a musical timbre space, such as arbitrary interpolation between disparate sounds. However, sinusoidal modeling only derives an accurate reconstruction for signals that vary smoothly on a time scale comparable to the frame rate.
Reference: [7] <author> R. Kronland-Martinet. </author> <title> The wavelet transform for analysis, synthesis, and processing of speech and music sounds. </title> <journal> Computer Music Journal, </journal> <volume> 12(4) </volume> <pages> 11-20, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: WAVELETS The above consideration of time-frequency resolution naturally leads to a discussion of wavelets and their underlying signal model. Though some musical applications of the continuous wavelet transform have been developed <ref> [7] </ref>, the focus here will be limited to the discrete wavelet transform and its implementation using a tree-structured filterbank. The discrete wavelet transform is based on critically sampled two-channel perfect reconstruction filterbanks with halfband highpass and lowpass branches.
Reference: [8] <author> M. Rodriguez-Hernandez and F. Casajus-Quiros. </author> <title> Improving time-scale modification of audio signals using wavelets. </title> <journal> ICSPAT-1994, </journal> <volume> 2 </volume> <pages> 1573-1577. </pages>
Reference-contexts: To get both the desirable time-frequency tradeoff of the wavelet transform and the musically meaningful modifications of the sinusoidal model, the two approaches can be merged by applying a sinusoidal analysis in each of the wavelet transform channels <ref> [8] </ref>. This amounts to using a different frame rate for sinusoids in different fre quency bands, and supplements the wavelet transform with the ability to separate and estimate multiple sinusoids in a single band.
Reference: [9] <author> M. Dolson. </author> <title> The phase vocoder: A tutorial. </title> <journal> Computer Music Journal, </journal> <volume> 10(4) </volume> <pages> 14-27, </pages> <month> Winter </month> <year> 1986. </year>
Reference-contexts: Synthesis for this multi-rate sinusoidal model can be done using oscillators in the channels followed by the synthesis filterbank, or by using a bank of oscillators with multi-rate interpolation models. This approach is basically an extension of the classic phase vocoder <ref> [9] </ref> that allows more than one partial in each frequency band and achieves a time-frequency tradeoff that represents partial behavior more accurately than fixed resolution methods. 4.
Reference: [10] <author> G. Evangelista. </author> <title> Pitch-synchronous wavelet representations of speech and music signals. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 41(12) </volume> <pages> 3313-3330, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: PITCH-SYNCHRONOUS WAVELETS For pseudo-periodic signals such as music, an appealing alternative to the lowpass-plus-details wavelet model is the approach presented by Evangelista in <ref> [10] </ref>, in which a pseudo-periodic signal is modeled as a sum of a periodic signal plus deviations from periodicity. <p> Time-scale and pitch modifications can be achieved by interpolating or decimating the channel signals in time or across channels, respectively, and interesting cross-synthesis results can be obtained by applying the modulation of one musical signal to the periodic estimate of another <ref> [10] </ref>; these modifications are facilitated by the pseudo-sinusoidal nature of the representation. In this model, transients and noise are not part of the fundamental signal estimate, and are primarily represented by the wide bands away from the harmonics. <p> In some cases, robust pitch detection may improve this representation; if no pitch is detected, the PSWT can revert to the standard wavelet transform, which is more suitable for non-periodic signal components <ref> [10] </ref>. 5. WAVELET PACKETS Wavelet packets, like wavelets, are based on iterating two-channel perfect reconstruction filterbanks. In this case, however, the filterbank can be iterated on either branch.
Reference: [11] <author> C. Herley et al. </author> <title> Time-varying orthonormal tilings of the time-frequency plane. </title> <journal> ICASSP-1993, </journal> <volume> 3 </volume> <pages> 205-208. </pages>
Reference-contexts: division of the frequency domain that represents the signal optimally with respect to the applied metric; this frequency division can be time estimate by the PSWT, and (c,d,e) three levels of details. varied to create a wavelet packet that adapts to optimally represent a signal whose characteristics change in time <ref> [11] </ref>. Pruning the filterbank tree corresponds to choosing a basis for expanding the input signal. One proposed metric for pruning is the entropy of the expansion coefficients for the basis that corresponds to the particular pruning.
Reference: [12] <author> J. Berger, R. Coifman, and M. Goldberg. </author> <title> Removing noise from music using local trigonometric bases and wavelet packets. </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> 42(10) </volume> <pages> 808-818, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Small-valued coefficients can be considered noise, and a thresholding scheme can be applied so that these noise contributions are excluded from the synthesis. This approach was proposed as a technique for removing unwanted noise from music recordings <ref> [12] </ref>, but can also serve to derive the deterministic-plus-stochastic signal decomposition discussed earlier; because this method does not involve interpolation, it results in a possibly cleaner stochastic component than the sinusoidal analysis-synthesis residual, and allows for separate processing of noiselike components as in the sinusoidal model.
Reference: [13] <author> C. van den Branden Lambrecht and M. Karrakchou. </author> <title> Wavelet packets-based high-resolution spectral estimation. </title> <booktitle> Signal Processing, </booktitle> <volume> 47(2) </volume> <pages> 135-144, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: As in the wavelet transform, modifications based strictly on wavelet packet coefficients are not easily tractable. This difficulty can be circumvented by using the metric proposed in <ref> [13] </ref>, which measures the number of partials in the frequency band corresponding to a node. The two-channel filterbank is iterated at a given node if the number of partials found in the prospective children nodes is greater than or equal to the number of partials at the given parent node. <p> This process continues until each frequency band contains at most one partial; the partial parameters are then estimated based on the Tufts-Kumaresan method, which improves in accuracy when applied in subbands as in this approach <ref> [13] </ref>. This analysis, which can be viewed as a phase vocoder based on a wavelet packet filterbank, provides an optimal sinusoidal decomposition of the signal.
References-found: 13

