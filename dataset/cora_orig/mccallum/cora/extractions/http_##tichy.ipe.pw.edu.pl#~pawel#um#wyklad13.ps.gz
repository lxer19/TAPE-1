URL: http://tichy.ipe.pw.edu.pl/~pawel/um/wyklad13.ps.gz
Refering-URL: http://tichy.ipe.pw.edu.pl/~pawel/um/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Uczenie si maszyn: wykad 13  
Author: Uczenie si ze wzmocnieniem (cz- ) Pawe Cichosz Semestr zimowy / Uczenie si strategii 
Note: 1.1 Algorytm AHC  
Abstract: Celem uczenia si ze wzmocnieniem jest nauczenie si strategii optymalnej (lub strategii dobrze przybliajcej strategi optymaln), za- uczenie si funkcji warto-ci moe by jedynie -rodkiem do tego celu. Przedstawimy dwa algorytmy, z ktrych pierwszy uczy si w podany wcze-niej sposb funkcji warto-ci, lecz wzgldem modytkowanej strategii, reprezentowanej za pomoc dodatkowej funkcji, a drugi uczy si (optymalnej) funkcji warto-ci akcji jako zwartej reprezentacji zarwno funkcji warto-ci jak i strategii (zachannej). Oba s najlepiej znanymi przykadami algorytmw uczenia si ze wzmocnieniem opartych na metodach rnic czasowych i stanowi w istocie pewne modytkacje podstawowego algorytmu TD dla warto-ciowania strategii. Algorytm AHC (Adaptive Heuristic Critic) jest przykadem architektury agenta typu aktor-krytyk, w ktrym oddzielone s moduy odpowiedzialne za podejmowanie decyzji (aktor) i przy-pisanie zasugi (krytyk). Rol drugiego peni oczywi-cie funkcja warto-ci. Rol pierwszego peni funkcja strategii, ktra kadej parze stan-akcja hx; ai przyporzdkowuje pewn liczb (x; a), od ktrej zaley szansa wykonania tej akcji. Odrniamy tu funkcj strategii od strategii : pierw-sza jest tylko pewnym sposobem reprezentowania drugiej. W najprostszym przypadku strategi, ktr posuguje si algorytm AHC, mona okre-li jako zachann wzgldem funkcji strategii (w stanie x wybr akcji a o najwyszej warto-ci (x; a)), chocia dalej zobaczymy, e w praktyce stosowane s strategie stochastyczne, ktre akcjom o wyszych warto-ciach funkcji strategii daj tylko pewne preferencje. W kadym kroku czasu modytkowana jest zarwno funkcja strategii dla aktualnego stanu i akcji, jak i funkcja warto-ci dla aktualnego stanu. W rezultacie algorytm uczy si funkcji warto-ci nie wzgldem ustalonej strategii, lecz wzgldem strategii zmieniajcej si w czasie. Wane jest, aby akcje wykonywane w trakcie uczenia si byy (przynajmniej w sensie probabilistycznym) zgodne z aktualn funkcj strategii, gdy w przeciwnym przypadku funkcja warto-ci nie bdzie reprezentowa warto-ci stanw dla tej strategii. To wymaganie oznacza, e AHC jest algorytmem typu on-policy (posuguje si strategi, ktrej si uczy). Szczegowy algorytm przedstawia tablica 1. Do aktualizacji obu funkcji uyta jest ta sama warto- bdu TD. W przypadku funkcji war-to-ci wiemy ju, e celem jest przyblienie warto-ci stanu x t do r t + flV t (x t+1 ). W przypadku funkcji strategii moemy to wyja-ni nieformalnie nastpujco. Warto- V t (x t ) reprezentuje ocze-kiwane (w czasie t) efekty (w sensie dugoterminowej zdyskontowanej sumy nagrd) wykonania w stanie x t akcji zgodnej z aktualn strategi. Z kolei suma r t + flV t (x t+1 ) daje poprawione oszaco-wanie tych efektw: uwzgldnia faktyczne natychmiastowe skutki wykonania w stanie x t akcji a t , wybranej na podstawie t . Je-li bd TD, jako rnica tych dwch oszacowa, jest dodatni, to 
Abstract-found: 1
Intro-found: 1
References-found: 0

