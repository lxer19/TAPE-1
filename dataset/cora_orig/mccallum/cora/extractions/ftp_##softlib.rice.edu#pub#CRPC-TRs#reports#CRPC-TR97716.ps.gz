URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97716.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Parallel Implementation of the Nonsymmetric QR Algorithm for Distributed Memory Architectures  
Author: Greg Henry David Watkins Jack Dongarra 
Keyword: Key Words: Parallel computing, eigenvalue, Schur decomposition, QR algorithm  
Note: AMS (MOS) Subject Classification: 65F15, 15A18  
Date: March 11, 1997  
Abstract: One approach to solving the nonsymmetric eigenvalue problem in parallel is to parallelize the QR algorithm. Not long ago, this was widely considered to be a hopeless task. Recent efforts have made significant advances, although the methods proposed up to now have suffered from scalability problems. This paper discusses an approach to parallelizing the QR algorithm that greatly improves scalability. A theoretical analysis indicates that the algorithm is ultimately not scalable, but the nonscalability does not become evident until the matrix dimension is enormous. Experiments on the Intel Paragon TM system, the IBM SP2 supercomputer, and the Intel ASCI Option Red Supercomputer are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anderson, E., Bai, Z., Bischof, C., Demmel, J., Dongarra, J., Du Croz, J., Greenbaum, A., Hammarling, S., McKenney, A., Sorenson, D., </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1992 </year>
Reference-contexts: This tends to suffer from stability problems since the two smaller problems might have arbitrarily worse condition than the parent problem [33]. In situations where more than just a few of the eigenvalues (and perhaps eigenvectors as well) are needed, the most competitive serial algorithm is the QR algorithm <ref> [21, 1] </ref>. Matrix multiply methods tend to require many more flops, as well as sometimes encountering accuracy problems [4]. Although matrix tearing methods may have lower flops counts, they require finding all the eigenvectors and hence are only useful when all the eigenvectors are required. <p> Jacobi methods [24] have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form [14, 48]. These might require fewer flops but they are plagued by instability. Against this competition, blocked versions of the implicit double shift QR algorithm <ref> [28, 31, 1] </ref> appear promising. One serious drawback of the double implicit shift QR algorithm is that its core computation is based on Householder reflections of size 3. <p> Unfortunately, this requires too many more flops and the GEMM itself has two of the three required dimensions very small [31]. However, even if a multishift QR algorithm is used without the additional matrix multiply (as was implemented in LAPACK <ref> [1] </ref>), the algorithm has convergence problems caused by roundoff errors if the value of M is too large. This was 2 discussed by Dubrulle [19] and Watkins [45, 46]. Because of this, a multishift size of M = 6 was implemented in LAPACK. <p> Tricks to delay the application of the rows and/or column reflections appear to 1 We use the term "HQR" to mean a practical Hessenberg QR iteration, for example, EISPACK's HQR code [37] or LAPACK's HSEQR or LAHQR code <ref> [1] </ref>. 5 only delay the inevitable load imbalance. The same argument holds for using a one dimensional row wrapped mapping of the data, in which the row transforms are unevenly distributed. For scalability reasons, distributed memory linear algebra computations often require a two dimensional block wrap torus mapping [27, 43]. <p> In all cases, the same amount of memory per node was used (including the temporary scratch space). We provide the problem size, and the efficiency as compared to LAHQR from LAPACK <ref> [1] </ref>. In Table 2, we see the results for doing the first superiteration of a complete Schur decomposition on an Intel Paragon TM Supercomputer running OSF R1.4.
Reference: [2] <author> Auslander, L., Tsao, A., </author> <title> On Parallelizable Eigensolvers, </title> <journal> Advanced Appl. Math., </journal> <volume> Vol. 13, </volume> <pages> pp. 253-261, </pages> <year> 1992 </year>
Reference-contexts: Of the various parallel algorithms that have been proposed, the ones that have received most attention recently have been based on matrix multiplication. The reason is clear: large matrix multiplication is highly parallel. Auslander and Tsao <ref> [2] </ref> and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum.
Reference: [3] <author> Bai, Z., Demmel, D., </author> <title> On a Block Implementation of Hessenberg Multishift QR Iteration Argonne National Laboratory Technical Report ANL-MCS-TM-127, 1989, </title> <journal> and International Journal of High Speed Computing, </journal> <volume> Vol. 1, </volume> <year> 1989, </year> <pages> p. 97-112 </pages>
Reference-contexts: One attempt to rectify this problem was the multishift QR algorithm of Bai and Demmel <ref> [3] </ref>, which we mentioned earlier. The idea was to generate a large number M of shifts and use them to chase a large bulge. This allowed for a GEMM-based (level-3 BLAS) algorithm to be used [3]. <p> One attempt to rectify this problem was the multishift QR algorithm of Bai and Demmel <ref> [3] </ref>, which we mentioned earlier. The idea was to generate a large number M of shifts and use them to chase a large bulge. This allowed for a GEMM-based (level-3 BLAS) algorithm to be used [3]. Unfortunately, this requires too many more flops and the GEMM itself has two of the three required dimensions very small [31]. <p> This practice results in subquadratic (although still superlinear) convergence [39, 41]. If we wish to chase many bulges at once without sacrificing quadratic convergence, we must change the shifting strategy. One of the strategies proposed in <ref> [3] </ref> was a generalization of the Wilkinson shift. Instead of choosing the two shifts to be the eigenvalues of the lower 2fi 2 matrix, one calculates the eigenvalues of the lower M fi M matrix, where M is an even number that is significantly greater than two (e.g.
Reference: [4] <author> Bai, Z., Demmel, J., </author> <title> Design of a Parallel Nonsymmetric Eigenroutine Toolbox, Part I, Parallel Processing for Scientific Computing, </title> <editor> Editors R. Sincovec, D. Keyes, M. Leuze, L. Petzold, and D. </editor> <booktitle> Reed, </booktitle> <pages> pp. 391-398, </pages> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1993 </year>
Reference-contexts: The reason is clear: large matrix multiplication is highly parallel. Auslander and Tsao [2] and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel <ref> [4] </ref> use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also [6, 10, 5, 7].) Dongarra and Sidani [17] introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems, which are solved independently and then glued <p> In situations where more than just a few of the eigenvalues (and perhaps eigenvectors as well) are needed, the most competitive serial algorithm is the QR algorithm [21, 1]. Matrix multiply methods tend to require many more flops, as well as sometimes encountering accuracy problems <ref> [4] </ref>. Although matrix tearing methods may have lower flops counts, they require finding all the eigenvectors and hence are only useful when all the eigenvectors are required. Furthermore, there are instances where they simply fail [33]. Jacobi methods [24] have notoriously high flop counts.
Reference: [5] <author> Bai, Z., Demmel J., </author> <title> Design of a Parallel Nonsymmetric Eigenroutine Toolbox, Part II, </title> <institution> University of Cali-fornia at Berkeley Technical Report in Progress: </institution> <month> 1/96 </month>
Reference-contexts: Auslander and Tsao [2] and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel [4] use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also <ref> [6, 10, 5, 7] </ref>.) Dongarra and Sidani [17] introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems, which are solved independently and then glued back together with a Newton iteration.
Reference: [6] <author> Bai, Z., Demmel, J., Dongarra, J., Petitet, A., Robinson, H., Stanley, K., </author> <title> The Spectral Decomposition of Nonsymmetric Matrices on Distributed Memory Parallel Computers, </title> <note> LAPACK working note 91, </note> <institution> University of Tennessee at Knoxville, </institution> <month> Jan. </month> <note> 1995 To appear in SIAM Scientific Computing. </note>
Reference-contexts: Auslander and Tsao [2] and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel [4] use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also <ref> [6, 10, 5, 7] </ref>.) Dongarra and Sidani [17] introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems, which are solved independently and then glued back together with a Newton iteration.
Reference: [7] <author> Bai, Z., Demmel, J., Gu, M., </author> <title> An Inverse Free Parallel Spectral Divide and Conquer Algorithm for Nonsymmetric Eigenproblems, </title> <type> Draft appearing somewhere. </type>
Reference-contexts: Auslander and Tsao [2] and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel [4] use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also <ref> [6, 10, 5, 7] </ref>.) Dongarra and Sidani [17] introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems, which are solved independently and then glued back together with a Newton iteration.
Reference: [8] <author> Berry, M. W., Dongarra, J.J., Kim, Y., </author> <title> A Highly Parallel Algorithm for the Reduction of a Nonsymmetric Matrix to Block Upper-Hessenberg Form, </title> <journal> Parallel Computing, </journal> <volume> Vol 21, No.8, </volume> <month> August, </month> <year> 1995, </year> <pages> pp 1189-1212. </pages>
Reference-contexts: Here, Q is the orthogonal matrix of Schur vectors, and T is an upper quasi-triangular matrix (1 fi 1 and 2 fi 2 blocks along the main diagonal). We can assume that H is upper Hessenberg because the reduction to Hessenberg form is well understood and can be parallelized <ref> [8, 15, 18] </ref>. The implicit Francis iteration assumes that H is unreduced (subdiagonals nonzero). As the iterates progress, wise choices of shifts allow the subdiagonals to converge to zero.
Reference: [9] <author> Blackford, S., Choi, J., Cleary, A., Demmel, J., Dhillon, I., Dongarra, J., Henry, G., Hammarling, S., Petitet, A., Stanley, K., Walker, D., Whaley, </author> <title> R.C., ScaLAPACK: A Portable Linear Algebra Library for Distributed Memory Computers Design Issues and Performance, </title> <institution> University of Tennessee at Knoxville Technical Report CS-95-283, </institution> <note> LAPACK Working Note 95, 1995, Also: Proceedings of Supercomputing '96, 1996, ISBN 0-89791-851-1 </note>
Reference-contexts: Similarly, at the end of the iteration, there will be wind down costs. To one familiar with parallel linear algebra, the first instinct is to dismiss this overhead, or to include a small "fudge factor" in some modeling. In cases like doing a parallel LU decomposition <ref> [9] </ref>, for example, there is a pipeline start-up when doing the horizontal broadcast of the multipliers around a ring. The key difference, however, is that in that case all the nodes are busy while the pipe is beginning to grow.
Reference: [10] <author> Byers, R., </author> <title> Numerical Stability and Instability in Matrix Sign Function Based Algorithms, Computational and Combinatorial Methods in Systems Theory, </title> <editor> C. Byrnes and A. Lindquist, </editor> <booktitle> editors, </booktitle> <pages> pp. 185-200, </pages> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: Auslander and Tsao [2] and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel [4] use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also <ref> [6, 10, 5, 7] </ref>.) Dongarra and Sidani [17] introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems, which are solved independently and then glued back together with a Newton iteration.
Reference: [11] <author> Boley., D., Maier, R., </author> <title> A Parallel QR Algorithm for the Nonsymmetric Eigenvalue Problem, </title> <institution> Univ. of Minn. at Minneapolis, Dept. of Computer Science, </institution> <type> Technical Report TR-88-12, </type> <year> 1988 </year>
Reference-contexts: Most of the results we present, with a few minor modifications to the modeling, would also hold true for slightly larger bulges (e.g. degree six). The first attempts at parallelizing the implicit double shift QR algorithm were unsuccessful. See Boley et. al. <ref> [11] </ref>, Geist et. al. [22, 23], Eberlein [20], and Stewart [38]. More successful methods came from vector implementations [16]. Usually, the key problem is to distribute the work evenly given its sequential nature. A major step forward in work distribution was made by van de Geijn [40] in 1988.
Reference: [12] <author> Chakrabarti, S., Demmel, J., Yelick, K., </author> <title> Modeling the Benefits of Mixed Data and Task Parallelism, </title> <booktitle> Seventh Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July 17-19, </month> <year> 1995, </year> <institution> UC Santa Barbara, </institution> <address> CA. </address>
Reference: [13] <author> Dongarra, J. J., Du Croz, J., Hammarling, and Duff, I. S., </author> <year> 1988, </year> <title> A set of Level 3 basic linear algebra subprograms, </title> <journal> ACM TOMS, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: One serious drawback of the double implicit shift QR algorithm is that its core computation is based on Householder reflections of size 3. This is a drawback for several reasons: it lacks the vendor supported performance tuning of the BLAS (basic linear algebra subroutines <ref> [13, 34] </ref>), and it has data re-use similar to level-1 operations (it does O (n) flops on O (n) data [24].) This imposes an upper limit to how fast it can run on the high performance computers with a memory hierarchy.
Reference: [14] <author> Dongarra, J. J., Geist, G.A., Romine, C.H., </author> <title> Fortran Subroutines for Computing the Eigenvalues and Eigen-vectors of a General Matrix By Reduction to General Tridiagonal Form, </title> <journal> ACM Trans. Math. Software, </journal> <volume> Vol. 18, </volume> <year> 1992, </year> <pages> p. 392-400 </pages>
Reference-contexts: Furthermore, there are instances where they simply fail [33]. Jacobi methods [24] have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form <ref> [14, 48] </ref>. These might require fewer flops but they are plagued by instability. Against this competition, blocked versions of the implicit double shift QR algorithm [28, 31, 1] appear promising.
Reference: [15] <author> Dongarra, J. J., Hammarling, S., J., Sorensen, D. C., </author> <title> Block Reduction of Matrices to Condensed Forms for Eigenvalue Computations, </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 215-227, </pages> <year> 1989. </year> <month> 26 </month>
Reference-contexts: Here, Q is the orthogonal matrix of Schur vectors, and T is an upper quasi-triangular matrix (1 fi 1 and 2 fi 2 blocks along the main diagonal). We can assume that H is upper Hessenberg because the reduction to Hessenberg form is well understood and can be parallelized <ref> [8, 15, 18] </ref>. The implicit Francis iteration assumes that H is unreduced (subdiagonals nonzero). As the iterates progress, wise choices of shifts allow the subdiagonals to converge to zero.
Reference: [16] <author> Dongarra, J. J., Kaufman, L., Hammarling, S., </author> <title> Squeezing the Most out of Eigenvalue Solvers on High--Performance Computers Linear Algebra and Its Applications, </title> <journal> Vol. </journal> <volume> 77, </volume> <year> 1986, </year> <pages> pp. 113-136 </pages>
Reference-contexts: The first attempts at parallelizing the implicit double shift QR algorithm were unsuccessful. See Boley et. al. [11], Geist et. al. [22, 23], Eberlein [20], and Stewart [38]. More successful methods came from vector implementations <ref> [16] </ref>. Usually, the key problem is to distribute the work evenly given its sequential nature. A major step forward in work distribution was made by van de Geijn [40] in 1988.
Reference: [17] <author> Dongarra, J. J., and Sidani, M., </author> <title> A Parallel Algorithm for the Nonsymmetric Eigenvalue Problem, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> Vol. 14, No. 3, </volume> <pages> pp. 542-569, </pages> <month> May </month> <year> 1993. </year> <note> Also: Technical Report Number ORNL/TM-12003, ORNL, </note> <institution> Oak Ridge Tennessee, </institution> <year> 1991 </year>
Reference-contexts: Auslander and Tsao [2] and Lederman, Tsao, and Turnbull [36] use multiply-based parallel algorithms based on matrix polynomials to split the spectrum. Bai and Demmel [4] use similar matrix multiply techniques using the matrix sign function to split the spectrum (see also [6, 10, 5, 7].) Dongarra and Sidani <ref> [17] </ref> introduced tearing methods based on doing rank one updates to an unsymmetric Hessenberg matrix, resulting in two smaller problems, which are solved independently and then glued back together with a Newton iteration.
Reference: [18] <author> Dongarra, J. J., van de Geijn, R. A., </author> <title> Reduction to Condensed Form on Distributed Memory Architectures, </title> <journal> Parallel Computing, </journal> <volume> 18, </volume> <pages> pp. 973-982, </pages> <year> 1992. </year>
Reference-contexts: Here, Q is the orthogonal matrix of Schur vectors, and T is an upper quasi-triangular matrix (1 fi 1 and 2 fi 2 blocks along the main diagonal). We can assume that H is upper Hessenberg because the reduction to Hessenberg form is well understood and can be parallelized <ref> [8, 15, 18] </ref>. The implicit Francis iteration assumes that H is unreduced (subdiagonals nonzero). As the iterates progress, wise choices of shifts allow the subdiagonals to converge to zero.
Reference: [19] <author> Dubrulle, A., </author> <title> The Multishift QR Algorithm- Is it Worth the Trouble?, </title> <type> IBM Scientific Center Technical Report Draft, </type> <address> 1992, Palo Alto, CA </address>
Reference-contexts: However, even if a multishift QR algorithm is used without the additional matrix multiply (as was implemented in LAPACK [1]), the algorithm has convergence problems caused by roundoff errors if the value of M is too large. This was 2 discussed by Dubrulle <ref> [19] </ref> and Watkins [45, 46]. Because of this, a multishift size of M = 6 was implemented in LAPACK. It is not clear that this is faster than the double implicit shift QR when blocked [31]. <p> The idea of chasing multiple bulges in parallel is not new [26, 38, 39, 41, 35]. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence [41]. What is new [45] (having seen <ref> [19] </ref>) is the idea of generating many shifts at once rather than two at a time, thereby allowing all bulges to carry up-to-date shifts.
Reference: [20] <author> Eberlein, P. J., </author> <title> On the Schur Decomposition of a Matrix for Parallel Computation, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-36, </volume> <pages> pp. 167-174, </pages> <year> 1987 </year>
Reference-contexts: Most of the results we present, with a few minor modifications to the modeling, would also hold true for slightly larger bulges (e.g. degree six). The first attempts at parallelizing the implicit double shift QR algorithm were unsuccessful. See Boley et. al. [11], Geist et. al. [22, 23], Eberlein <ref> [20] </ref>, and Stewart [38]. More successful methods came from vector implementations [16]. Usually, the key problem is to distribute the work evenly given its sequential nature. A major step forward in work distribution was made by van de Geijn [40] in 1988.
Reference: [21] <author> Francis, J. G. F., </author> <title> The QR Transformation: A unitary analogue to the LR Transformation, Parts 1 and 2, </title> <journal> Comp. J. </journal> <volume> 4, </volume> <year> 1961, </year> <pages> pp. 265-272, 332-45 </pages>
Reference-contexts: 1 Introduction Over the years many methods for solving the parallel unsymmetric eigenvalue problem have been suggested. Most of these methods have serious drawbacks, either in terms of stability, accuracy, scalability, or requiring extra work. This paper describes a version of the QR algorithm <ref> [21] </ref> that has significantly better scaling properties than earlier versions, as well as being stable, accurate, and efficient in terms of flop count or iteration count. Most implementations of the QR algorithm perform QR iterations implicitly by chasing bulges down the subdiagonal of an upper Hessenberg matrix [24, 44]. <p> Most implementations of the QR algorithm perform QR iterations implicitly by chasing bulges down the subdiagonal of an upper Hessenberg matrix [24, 44]. The original version due to J. G. F. Francis <ref> [21] </ref>, which has long been the standard serial algorithm, is of this type. It begins each iteration by choosing two shifts (for convergence acceleration) and using them to form a bulge of degree 2. <p> This tends to suffer from stability problems since the two smaller problems might have arbitrarily worse condition than the parent problem [33]. In situations where more than just a few of the eigenvalues (and perhaps eigenvectors as well) are needed, the most competitive serial algorithm is the QR algorithm <ref> [21, 1] </ref>. Matrix multiply methods tend to require many more flops, as well as sometimes encountering accuracy problems [4]. Although matrix tearing methods may have lower flops counts, they require finding all the eigenvectors and hence are only useful when all the eigenvectors are required. <p> H P 0 HP 0 Compute P i so that P i H has zero (i + 2; i) and (i + 3; i) entries. Update H P i HP i Update Q QP i endfor changes. 2.1 Single Bulge The double implicit Francis step <ref> [21] </ref> enables an iterative algorithm that goes from H (upper Hessenberg) to H = QT Q T (Schur decomposition [24, 48]). Here, Q is the orthogonal matrix of Schur vectors, and T is an upper quasi-triangular matrix (1 fi 1 and 2 fi 2 blocks along the main diagonal).
Reference: [22] <author> Geist, G.A., Davis, G.J., </author> <title> Finding Eigenvalues and Eigenvectors of Unsymmetric matrices using a Distributed Memory Multiprocessor, </title> <journal> Parallel Computing, </journal> <volume> Vol 13, No. 2, </volume> <pages> pp. 199-209, </pages> <year> 1990. </year>
Reference-contexts: Most of the results we present, with a few minor modifications to the modeling, would also hold true for slightly larger bulges (e.g. degree six). The first attempts at parallelizing the implicit double shift QR algorithm were unsuccessful. See Boley et. al. [11], Geist et. al. <ref> [22, 23] </ref>, Eberlein [20], and Stewart [38]. More successful methods came from vector implementations [16]. Usually, the key problem is to distribute the work evenly given its sequential nature. A major step forward in work distribution was made by van de Geijn [40] in 1988.
Reference: [23] <author> Geist, G.A., Ward, R.C., Davis, G.J., Funderlic, R.E., </author> <title> Finding Eigenvalues and Eigenvectors of Unsym-metric Matrices using a Hypercube Multiprocessor, </title> <address> Monterey, CA, </address> <booktitle> Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <editor> Editor G. </editor> <booktitle> Fox, </booktitle> <pages> pp. 1577-1582, </pages> <year> 1988 </year>
Reference-contexts: Most of the results we present, with a few minor modifications to the modeling, would also hold true for slightly larger bulges (e.g. degree six). The first attempts at parallelizing the implicit double shift QR algorithm were unsuccessful. See Boley et. al. [11], Geist et. al. <ref> [22, 23] </ref>, Eberlein [20], and Stewart [38]. More successful methods came from vector implementations [16]. Usually, the key problem is to distribute the work evenly given its sequential nature. A major step forward in work distribution was made by van de Geijn [40] in 1988. <p> Unfortunately, this would be unacceptable because applying all the column reflections, half the total work, would involve at most 3 processors, thus implying the maximum speed-up obtainable would be 6 <ref> [23] </ref>. Tricks to delay the application of the rows and/or column reflections appear to 1 We use the term "HQR" to mean a practical Hessenberg QR iteration, for example, EISPACK's HQR code [37] or LAPACK's HSEQR or LAHQR code [1]. 5 only delay the inevitable load imbalance.
Reference: [24] <author> Golub, G., Van Loan, C., </author> <title> Matrix Computations, 2nd Ed., 1989, </title> <publisher> The John Hopkins University Press. </publisher>
Reference-contexts: Most implementations of the QR algorithm perform QR iterations implicitly by chasing bulges down the subdiagonal of an upper Hessenberg matrix <ref> [24, 44] </ref>. The original version due to J. G. F. Francis [21], which has long been the standard serial algorithm, is of this type. It begins each iteration by choosing two shifts (for convergence acceleration) and using them to form a bulge of degree 2. <p> Although matrix tearing methods may have lower flops counts, they require finding all the eigenvectors and hence are only useful when all the eigenvectors are required. Furthermore, there are instances where they simply fail [33]. Jacobi methods <ref> [24] </ref> have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form [14, 48]. These might require fewer flops but they are plagued by instability. <p> This is a drawback for several reasons: it lacks the vendor supported performance tuning of the BLAS (basic linear algebra subroutines [13, 34]), and it has data re-use similar to level-1 operations (it does O (n) flops on O (n) data <ref> [24] </ref>.) This imposes an upper limit to how fast it can run on the high performance computers with a memory hierarchy. One attempt to rectify this problem was the multishift QR algorithm of Bai and Demmel [3], which we mentioned earlier. <p> Update H P i HP i Update Q QP i endfor changes. 2.1 Single Bulge The double implicit Francis step [21] enables an iterative algorithm that goes from H (upper Hessenberg) to H = QT Q T (Schur decomposition <ref> [24, 48] </ref>). Here, Q is the orthogonal matrix of Schur vectors, and T is an upper quasi-triangular matrix (1 fi 1 and 2 fi 2 blocks along the main diagonal). <p> The implicit Francis iteration assumes that H is unreduced (subdiagonals nonzero). As the iterates progress, wise choices of shifts allow the subdiagonals to converge to zero. As in <ref> [24] </ref>, at some stage of the algorithm H might be in the following form: H = 6 H 11 H 12 H 13 H 33 7 We assume that H 22 is the largest unreduced Hessenberg matrix above H 33 (which has converged) in the current iteration. <p> Golub and Van Loan <ref> [24] </ref> report the figure 25N 3 . All of the numbers in Figure 4 are less than 25N 3 . More importantly there is a clear tendency, especially in the 1 fi 1 case, for the multiple of N 3 to decrease as N increases.
Reference: [25] <author> Gupta, A., Kumar, V., </author> <title> On the Scalability of FFT on Parallel Computers, </title> <booktitle> Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1990 </year>
Reference-contexts: In addition to this problem, the algorithms resulting from all these works were only iso-efficient <ref> [25] </ref>. That is, you could get 100 percent efficiency, but only if the problem size was allowed to scale faster than memory does. Nevertheless, these were the first algorithms ever to achieve theoretically perfect speed-up.
Reference: [26] <author> D. E. Heller and I. C. F. Ipsen, </author> <title> Systolic networks for orthogonal equivalence transformations and their applications, </title> <booktitle> Conference on Advanced Research in VLSI, </booktitle> <publisher> M.I.T., </publisher> <year> 1982. </year>
Reference-contexts: Here we pursue the idea of using M shifts to form and chase M /2 bulges in parallel. The idea of chasing multiple bulges in parallel is not new <ref> [26, 38, 39, 41, 35] </ref>. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence [41].
Reference: [27] <author> Hendrickson, B.A., Womble, D.E., </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers., </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> Vol. 15, No. 5, pp.1201-1226, </volume> <month> Sept. </month> <year> 1994 </year>
Reference-contexts: The same argument holds for using a one dimensional row wrapped mapping of the data, in which the row transforms are unevenly distributed. For scalability reasons, distributed memory linear algebra computations often require a two dimensional block wrap torus mapping <ref> [27, 43] </ref>. To maximize the distribution of this computation, we could wrap our 2D block wrap mapping as tightly as possible with a block size of one (this would create other problems which we will ignore for now).
Reference: [28] <author> Henry, G., </author> <title> Improving the Unsymmetric Parallel QR Algorithm on Vector Machines, </title> <booktitle> 6th SIAM Parallel Conference Proceedings, </booktitle> <month> March </month> <year> 1993 </year>
Reference-contexts: Jacobi methods [24] have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form [14, 48]. These might require fewer flops but they are plagued by instability. Against this competition, blocked versions of the implicit double shift QR algorithm <ref> [28, 31, 1] </ref> appear promising. One serious drawback of the double implicit shift QR algorithm is that its core computation is based on Householder reflections of size 3.
Reference: [29] <author> Henry, G., </author> <title> The Shifted Hessenberg System Solve Computation, </title> <note> Cornell Theory Center Technical Report, CTC94TR163, 1/94 </note>
Reference: [30] <author> Henry, G., </author> <title> A Parallel Unsymmetric Inverse Iteration Solver, </title> <booktitle> 7th SIAM Parallel Conference Proceedings, </booktitle> <year> 1994 </year>
Reference-contexts: In this case, serial performance on the Intel Paragon system was around 8.2 Mflops (for roughly half the flops.) These positive results may lead to even better methods in future, since combining using HQR for finding eigenvalues and new GEMM-based inverse iteration methods for finding eigenvectors <ref> [30] </ref> might lead to completing the spectrum significantly faster than results in Table 2. Furthermore, there are better load balancing properties to the eigenvalue only code on a Cartesian mapping.
Reference: [31] <author> Henry, G., </author> <title> Improving Data Re-Use in Eigenvalue-Related Computations, </title> <type> Ph.D. Thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1994 </year>
Reference-contexts: Jacobi methods [24] have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form [14, 48]. These might require fewer flops but they are plagued by instability. Against this competition, blocked versions of the implicit double shift QR algorithm <ref> [28, 31, 1] </ref> appear promising. One serious drawback of the double implicit shift QR algorithm is that its core computation is based on Householder reflections of size 3. <p> This allowed for a GEMM-based (level-3 BLAS) algorithm to be used [3]. Unfortunately, this requires too many more flops and the GEMM itself has two of the three required dimensions very small <ref> [31] </ref>. However, even if a multishift QR algorithm is used without the additional matrix multiply (as was implemented in LAPACK [1]), the algorithm has convergence problems caused by roundoff errors if the value of M is too large. This was 2 discussed by Dubrulle [19] and Watkins [45, 46]. <p> This was 2 discussed by Dubrulle [19] and Watkins [45, 46]. Because of this, a multishift size of M = 6 was implemented in LAPACK. It is not clear that this is faster than the double implicit shift QR when blocked <ref> [31] </ref>. Because of the difficulties in chasing large bulges, we restrict our analysis in this paper to bulges of degree two. Most of the results we present, with a few minor modifications to the modeling, would also hold true for slightly larger bulges (e.g. degree six). <p> If we apply them to B + 2 columns of size N, we perform 10N B flops, and we must access (B + 2)N 13 data. The data re-use fraction <ref> [31] </ref> is at best 10 flops per data element accessed in the limit. If B is one, which is the worst case, then only 3 flops are done per data element accessed. So, roughly one can improve the data re-use by a factor of 3 by applying these transforms simultaneously.
Reference: [32] <author> Henry, G., van de Geijn, R., </author> <title> Parallelizing the QR Algorithm for the Unsymmetric Algebraic Eigenvalue Problem: </title> <note> Myths and Reality Accepted for publication in SIAM Journal of Scientific Computing, Date unknown. </note>
Reference-contexts: There, and in van de Geijn and Hudson [42], a wrap Hankel mapping was used to distribute the work evenly. A simple case of this, anti-diagonal mappings, was exploited in the paper by Henry and van de Geijn <ref> [32] </ref>. One difficulty these algorithms faced is that they all used non-Cartesian mappings. In these mappings, it is impossible to go across both a row and a column with a single fixed offset. <p> That is, you could get 100 percent efficiency, but only if the problem size was allowed to scale faster than memory does. Nevertheless, these were the first algorithms ever to achieve theoretically perfect speed-up. In <ref> [32] </ref> it was also proved that the standard double implicit shift QR algorithm (not just the one with anti-diagonal mappings) cannot be scalable. <p> Similarly, column reflections can use at most 3R processors. The maximum speed-up obtainable is then 3 (R + C), where in practice one might expect no more than two times the minimum of R and C. If one used an anti-diagonal mapping of the data <ref> [32] </ref>, then element H (i; j) or (if one uses a block mapping as one should) submatrix H ij is assigned to processor (i + j 2)mod P; where P is the number of processors. <p> We consider the results in Table 2 encouraging. Efficiencies remained basically the same throughout all the runs, and the overall performance was in excess of the serial code it was modeled after. We briefly compare this algorithm to the first successful parallel QR algorithm in <ref> [32] </ref>. That algorithm achieved maximum performance when using 96 nodes, after which the nonscalability caused performance degradation. The new algorithm achieves faster performance on 49 nodes and appears to scale on the Intel Paragon supercomputer.
Reference: [33] <author> Jessup, </author> <title> E.R., A Case Against a Divide and Conquer Approach to the Nonsymmetric Eigenvalue Problem, </title> <type> ORNL Technical Report ORNL/TM-11903, </type> <institution> Oak Ridge Tennessee, </institution> <year> 1991 </year> <month> 27 </month>
Reference-contexts: This tends to suffer from stability problems since the two smaller problems might have arbitrarily worse condition than the parent problem <ref> [33] </ref>. In situations where more than just a few of the eigenvalues (and perhaps eigenvectors as well) are needed, the most competitive serial algorithm is the QR algorithm [21, 1]. Matrix multiply methods tend to require many more flops, as well as sometimes encountering accuracy problems [4]. <p> Although matrix tearing methods may have lower flops counts, they require finding all the eigenvectors and hence are only useful when all the eigenvectors are required. Furthermore, there are instances where they simply fail <ref> [33] </ref>. Jacobi methods [24] have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form [14, 48]. These might require fewer flops but they are plagued by instability.
Reference: [34] <author> K-agstrom, B., Van Loan, </author> <note> C.F., GEMM-Based Level-3 BLAS, Cornell Theory Center Technical Report, CTC91TR47, 1/91. </note>
Reference-contexts: One serious drawback of the double implicit shift QR algorithm is that its core computation is based on Householder reflections of size 3. This is a drawback for several reasons: it lacks the vendor supported performance tuning of the BLAS (basic linear algebra subroutines <ref> [13, 34] </ref>), and it has data re-use similar to level-1 operations (it does O (n) flops on O (n) data [24].) This imposes an upper limit to how fast it can run on the high performance computers with a memory hierarchy.
Reference: [35] <author> L. Kaufman, </author> <title> A parallel QR algorithm for the symmetric eigenvalue problem, </title> <journal> J. Parallel and Distributed Computing, </journal> <note> to appear. </note>
Reference-contexts: Here we pursue the idea of using M shifts to form and chase M /2 bulges in parallel. The idea of chasing multiple bulges in parallel is not new <ref> [26, 38, 39, 41, 35] </ref>. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence [41].
Reference: [36] <author> Lederman, S., Tsao, A., Turnbull, T., </author> <title> A parallelizable eigensolver for real diagonalizable matrices with real eig envalues, </title> <type> Technical Report TR-91-042, </type> <institution> Supercomputing Research Center, </institution> <year> 1991 </year>
Reference-contexts: Of the various parallel algorithms that have been proposed, the ones that have received most attention recently have been based on matrix multiplication. The reason is clear: large matrix multiplication is highly parallel. Auslander and Tsao [2] and Lederman, Tsao, and Turnbull <ref> [36] </ref> use multiply-based parallel algorithms based on matrix polynomials to split the spectrum.
Reference: [37] <author> Smith, B., Boyle, J., Dongarra, J., Garbow, B., Ikebe, Y., Klema, V., Moler, C. </author> <title> Matrix Eigensystem Routines - EISPACK guide, </title> <booktitle> 2nd Ed. Lecture Notes in Computer Science 6 Springer-Verlag, </booktitle> <year> 1976 </year>
Reference-contexts: Tricks to delay the application of the rows and/or column reflections appear to 1 We use the term "HQR" to mean a practical Hessenberg QR iteration, for example, EISPACK's HQR code <ref> [37] </ref> or LAPACK's HSEQR or LAHQR code [1]. 5 only delay the inevitable load imbalance. The same argument holds for using a one dimensional row wrapped mapping of the data, in which the row transforms are unevenly distributed.
Reference: [38] <author> Stewart, G. W., </author> <title> A Parallel Implementation of the QR Algorithm, </title> <booktitle> Parallel Computing 5, </booktitle> <pages> pp. 187-196, </pages> <note> 1987 [39] van de Geijn, </note> <author> R., A., </author> <title> Implementing the QR-Algorithm on an Array of Processors, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Univ. of MD, TR-1897, </institution> <note> 1987 [40] van de Geijn, </note> <author> R., A., </author> <title> Storage Schemes for Parallel Eigenvalue Algorithms, Numerical Linear Algebra, </title> <booktitle> Digital Signal Processing and Parallel Algorithms, </booktitle> <pages> pp. 639-648, </pages> <editor> Editors G. Golub and P. Van Dooren, </editor> <publisher> Springer Verlag, </publisher> <year> 1988 </year>
Reference-contexts: The first attempts at parallelizing the implicit double shift QR algorithm were unsuccessful. See Boley et. al. [11], Geist et. al. [22, 23], Eberlein [20], and Stewart <ref> [38] </ref>. More successful methods came from vector implementations [16]. Usually, the key problem is to distribute the work evenly given its sequential nature. A major step forward in work distribution was made by van de Geijn [40] in 1988. <p> Here we pursue the idea of using M shifts to form and chase M /2 bulges in parallel. The idea of chasing multiple bulges in parallel is not new <ref> [26, 38, 39, 41, 35] </ref>. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence [41].
Reference: [41] <author> R. A. van de Geijn, </author> <title> Deferred shifting schemes for parallel QR methods, </title> <note> SIAM J. Matrix Anal. Appl., 14 (1993) pp. 180-194. </note> <author> [42] van de Geijn, R.,A., Hudson, D.G., </author> <title> An Efficient Parallel Implementation of the Nonsymmetric QR Algorithm, </title> <booktitle> Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pp. 697-700, </pages> <note> 1989 [43] van de Geijn, </note> <author> R. A., Walker, D., </author> <title> Look at Scalable Dense Linear Algebra Libraries, </title> <booktitle> Scalable High-Performance Computing Conference, </booktitle> <address> April 1992, </address> <publisher> IEEE Press. </publisher>
Reference-contexts: Here we pursue the idea of using M shifts to form and chase M /2 bulges in parallel. The idea of chasing multiple bulges in parallel is not new <ref> [26, 38, 39, 41, 35] </ref>. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence [41]. <p> The idea of chasing multiple bulges in parallel is not new [26, 38, 39, 41, 35]. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence <ref> [41] </ref>. What is new [45] (having seen [19]) is the idea of generating many shifts at once rather than two at a time, thereby allowing all bulges to carry up-to-date shifts. <p> This means that if we want to chase several bulges at once and use the Wilkinson strategy, we must use out-of-date shifts. This practice results in subquadratic (although still superlinear) convergence <ref> [39, 41] </ref>. If we wish to chase many bulges at once without sacrificing quadratic convergence, we must change the shifting strategy. One of the strategies proposed in [3] was a generalization of the Wilkinson shift.
Reference: [44] <author> Watkins, </author> <title> D.S., Fundamentals of Matrix Computations, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1991 </year>
Reference-contexts: Most implementations of the QR algorithm perform QR iterations implicitly by chasing bulges down the subdiagonal of an upper Hessenberg matrix <ref> [24, 44] </ref>. The original version due to J. G. F. Francis [21], which has long been the standard serial algorithm, is of this type. It begins each iteration by choosing two shifts (for convergence acceleration) and using them to form a bulge of degree 2.
Reference: [45] <author> Watkins, </author> <title> D.S., Shifting Strategies for the Parallel QR Algorithm, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15 (1994), </volume> <pages> pp. 953-958. </pages>
Reference-contexts: In principle this procedure should give the same result as a multishift iteration, but in practice (in the face of roundoff errors), our procedure performs much better <ref> [45] </ref>. Of the various parallel algorithms that have been proposed, the ones that have received most attention recently have been based on matrix multiplication. The reason is clear: large matrix multiplication is highly parallel. <p> However, even if a multishift QR algorithm is used without the additional matrix multiply (as was implemented in LAPACK [1]), the algorithm has convergence problems caused by roundoff errors if the value of M is too large. This was 2 discussed by Dubrulle [19] and Watkins <ref> [45, 46] </ref>. Because of this, a multishift size of M = 6 was implemented in LAPACK. It is not clear that this is faster than the double implicit shift QR when blocked [31]. <p> The idea of chasing multiple bulges in parallel is not new [26, 38, 39, 41, 35]. However, it was long thought that this practice would require the use of out-of-date shifts, resulting in degradation of convergence [41]. What is new <ref> [45] </ref> (having seen [19]) is the idea of generating many shifts at once rather than two at a time, thereby allowing all bulges to carry up-to-date shifts.
Reference: [46] <author> Watkins, </author> <title> D.S., The Transmission of Shifts and Shift Blurring in the QR Algorithm, </title> <journal> Linear Algebra Appl., </journal> <month> 241-243 </month> <year> (1996), </year> <pages> pp. 877-896. </pages>
Reference-contexts: However, even if a multishift QR algorithm is used without the additional matrix multiply (as was implemented in LAPACK [1]), the algorithm has convergence problems caused by roundoff errors if the value of M is too large. This was 2 discussed by Dubrulle [19] and Watkins <ref> [45, 46] </ref>. Because of this, a multishift size of M = 6 was implemented in LAPACK. It is not clear that this is faster than the double implicit shift QR when blocked [31].
Reference: [47] <author> Watkins, D.S., Elsner L., </author> <title> Convergence of Algorithms of Decomposition Type for the Eigenvalue Problem, </title> <journal> Linear Algebra Appl. </journal> <volume> 143, </volume> <year> 1991, </year> <pages> p. 19-47. </pages>
Reference-contexts: This is inexpensive and works well as long as only one bulge at a time is being chased. The convergence rate is usually quadratic <ref> [48, 47] </ref>. However, this strategy has the following shortcoming for parallel computing. The correct shifts for the next iteration cannot be calculated until the bulge for the current iteration has been 6 chased all the way to the bottom of the matrix. <p> M = 32). Then one has enough shifts to chase M=2 bulges in either serial or parallel fashion before having to go back for more shifts. This strategy also results (usually) in quadratic convergence, as was proved in <ref> [47] </ref> and has been observed in practice. We refer to each cycle of computing M shifts and chasing M=2 bulges as a super-iteration. The question of how to determine the number of bulges S = M=2 per super-iteration is important.
Reference: [48] <author> Wilkinson, J.H., </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965 </year>
Reference-contexts: Furthermore, there are instances where they simply fail [33]. Jacobi methods [24] have notoriously high flop counts. There are also methods by Dongarra, Geist, and Romine based on initial reductions to tridiagonal form <ref> [14, 48] </ref>. These might require fewer flops but they are plagued by instability. Against this competition, blocked versions of the implicit double shift QR algorithm [28, 31, 1] appear promising. <p> Update H P i HP i Update Q QP i endfor changes. 2.1 Single Bulge The double implicit Francis step [21] enables an iterative algorithm that goes from H (upper Hessenberg) to H = QT Q T (Schur decomposition <ref> [24, 48] </ref>). Here, Q is the orthogonal matrix of Schur vectors, and T is an upper quasi-triangular matrix (1 fi 1 and 2 fi 2 blocks along the main diagonal). <p> This allows the work to be distributed evenly. Furthermore, we maintain this even distribution when we use any multiple of R bulges- a mechanism useful for decreasing the significance of pipeline start-up and wind-down. 2.3 Multiple Bulges The usual strategy for computing shifts is the Wilkinson strategy <ref> [48] </ref>, in which the shifts are taken to be the eigenvalues of the lower 2 fi 2 submatrix. This is inexpensive and works well as long as only one bulge at a time is being chased. The convergence rate is usually quadratic [48, 47]. <p> This is inexpensive and works well as long as only one bulge at a time is being chased. The convergence rate is usually quadratic <ref> [48, 47] </ref>. However, this strategy has the following shortcoming for parallel computing. The correct shifts for the next iteration cannot be calculated until the bulge for the current iteration has been 6 chased all the way to the bottom of the matrix. <p> Our flop count takes deflations into account, but it ignores the fact that the matrix can split apart in the middle due to some H i+1;i (1 t i t N ) becoming effectively zero. Many of the subdiagonal entries H i+1;i drift linearly toward zero <ref> [48] </ref> in the course of the computation, so it is to be expected that such splittings will sometimes occur. It is reasonable to expect splittings to occur more frequently in large problems than in small ones. Whenever such a split occurs, the flop count is decreased. <p> If a pair of eigenvalues is deflated every four (or whatever number) of iterations, as in our model, the average submatrix size will be :5N . In fact one might expect a somewhat larger average, based on the observation <ref> [48] </ref> that more iterations per eigenvalue are required in the earlier iterations (large matrices) than in the later iterations (small matrices). On the other hand, splittings will have the effect of decreasing the average.
Reference: [49] <author> Wu, L., Chu, E., </author> <title> New Distributed-memory Parallel Algorithms for Solving Nonsymmetric Eigenvalue Problems, </title> <booktitle> Proceedings of the 7th SIAM Parallel Conference on Parallel Processing for Scientific Computing, </booktitle> <editor> Ed. Bailey, et. al., </editor> <publisher> SIAM Publications, </publisher> <month> Feb. </month> <year> 1995 </year> <month> 28 </month>
Reference-contexts: Clearly, any mapping where (if the matrix is large enough) any row and any column is distributed roughly evenly among all the processors would suffice. Unfortunately, no Cartesian mappings satisfy this criterion. There are reasons to believe that the anti-diagonal distribution is ideal. Block diagonal mappings have been suggested <ref> [49] </ref>, but these suffer from load imbalances that are avoided in the anti-diagonal case. By making a slight modification to the algorithm, one could chase several bulges at once and continue to use a two dimensional Cartesian mapping.
References-found: 45

