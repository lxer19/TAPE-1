URL: http://www.cs.cornell.edu/Info/People/ronitt/PAP/grs.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ronitt/papers.html
Root-URL: 
Title: Learning polynomials with queries: The highly noisy case  task for the case when F  
Author: ODED GOLDREICH RONITT RUBINFELD MADHU SUDAN 
Note: ffi n and exponential in d, provided ffi is p  GF(2) (and d 1).  
Abstract: Given a function f mapping n-variate inputs from a finite field F into F , we consider the task of reconstructing a list of all n-variate degree d polynomials which agree with f on a tiny but non-negligible fraction, ffi, of the input space. We give a randomized algorithm for solving this task which accesses f as a black box and runs in time polynomial in 1 d=jF j). For the special case when d = 1, we solve this problem for all * def jF j &gt; 0. In this case the running time of our algorithm is bounded by a polynomial in 1 * ; n and exponential in d. Our algorithm generalizes a previously known algorithm, due to Goldreich and Levin, that solves this
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise.
Reference: [2] <author> D. Angluin, M. Krikis. </author> <title> Learning with Malicious Membership Queries and Exceptions. </title> <address> COLT, </address> <year> 1994. </year>
Reference-contexts: First, it falls into the paradigm of learning with persistent noise. Here one assumes that the function f is derived from some function in the class C by adding noise to it. Typical works in this direction either tolerate only small amounts of noise <ref> [2, 38, 19, 37] </ref> (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random [1, 24, 18, 23, 31, 13, 34] (i.e., that the decision of whether or not to modify the function at any given input is made
Reference: [3] <author> S. Ar, R. Lipton, R. Rubinfeld, M. Sudan. </author> <title> Reconstructing Algebraic Functions from Mixed Data. </title> <booktitle> FOCS, </booktitle> <year> 1992. </year>
Reference-contexts: This setting is very related to the setting investigated by Ar et. al. <ref> [3] </ref>, except that their techniques require that the fraction of inputs left unexplained by any g i be smaller than the fraction of inputs on which each g i agrees with f . We believe that our relaxation makes the setting more appealing and closer in spirit to agnostic learning. <p> Linear Polynomials Goldreich and Levin [17] have solved the reconstruction problem in the case where d = 1 and F = GF (2). Similar ideas are used by Kushilevitz and Mansour [23] to learn boolean decision trees. Reconstruction of polynomials under structured error models Ar et. al. <ref> [3] </ref> have considered the problem of reconstructing a list of polynomials which together explain the input-output relation of a given black-box. However, they have required that the fraction of inputs left uncovered by any of the polynomials be smaller than the fraction of inputs covered by any single polynomial. <p> However, they have required that the fraction of inputs left uncovered by any of the polynomials be smaller than the fraction of inputs covered by any single polynomial. An alternative way of viewing the work of Ar et. al. <ref> [3] </ref> is as reconstructing the list of polynomials that agree with the f on ffi fraction of the inputs, provided that the input-output relation satisfies some (unknown) algebraic identities.
Reference: [4] <author> D. Beaver and J. Feigenbaum. </author> <title> Hiding Instance in Multioracle Queries. </title> <booktitle> STACS, </booktitle> <year> 1990. </year>
Reference-contexts: Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions [8, 9, 26, 32]. Self-correctors for f that are polynomial functions over a finite field were found by <ref> [4; 26] </ref>. The fraction of errors they could correct was improved to almost 1=4 independently by [14] and [10] and then to almost 1=2 by [15] (using a solution for the univariate case implicit in [5]).
Reference: [5] <author> E. Berlekamp and L. Welch. </author> <title> Error Correction of Algebraic Block Codes. </title> <type> US Patent, </type> <institution> Number 4,633,470, </institution> <year> 1986. </year>
Reference-contexts: Self-correctors for f that are polynomial functions over a finite field were found by [4; 26]. The fraction of errors they could correct was improved to almost 1=4 independently by [14] and [10] and then to almost 1=2 by [15] (using a solution for the univariate case implicit in <ref> [5] </ref>). However, when the error is larger than 1 2 (or, alternatively ffi &lt; 1=2), the utility of the standard self-correction approach seems to disappear, since there could be more than one polynomial that agrees with the program on an ffi &lt; 1=2 fraction of the inputs.
Reference: [6] <author> A. Blum and P. Chalasani. </author> <title> Learning Switching Concepts. </title> <address> COLT, </address> <year> 1992. </year>
Reference: [7] <author> A. Blum, M. Furst, M. Kearns, R. Lipton. </author> <title> Cryptographic Primitives Based on Hard Learning Problems. </title> <booktitle> CRYPTO, </booktitle> <year> 1993. </year>
Reference-contexts: Finally, queries seem essential to our learning algorithm since for the case F = GF (2) and d = 1 the problem reduces to the well-known problem of learning parity with noise [18] which is commonly believed to be hard when one is only allowed uniformly and independently chosen examples <ref> [18, 7, 20] </ref>. (Actually, learning parity with noise is considered hard even for random noise, whereas here the noise is adversarial.) In the full version, we give evidence that the problem may be hard with respect to d even in the case where n = 1.
Reference: [8] <author> M. Blum, M. Luby and R. Rubinfeld. </author> <title> Self-Testing/Correcting with Applications to Numerical Problems. </title> <booktitle> STOC, </booktitle> <year> 1990. </year>
Reference-contexts: Self-Correction In the case when the noise rate is positive but small, one approach used to solving the reconstruction problem is to use self-correctors, introduced independently in <ref> [8] </ref> and [26]. Self-correctors convert programs that are known to be correct on a fraction ffi of inputs into programs that are correct on each input. Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions [8, 9, 26, 32]. <p> Self-correctors convert programs that are known to be correct on a fraction ffi of inputs into programs that are correct on each input. Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions <ref> [8, 9, 26, 32] </ref>. Self-correctors for f that are polynomial functions over a finite field were found by [4; 26].
Reference: [9] <author> R. Cleve, M. Luby. </author> <title> A Note on Self-Testing/Correcting Methods for Trigonometric Functions. </title> <institution> International Computer Science Institute Technical Report, TR-90-032, </institution> <month> July, </month> <year> 1990. </year>
Reference-contexts: Self-correctors convert programs that are known to be correct on a fraction ffi of inputs into programs that are correct on each input. Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions <ref> [8, 9, 26, 32] </ref>. Self-correctors for f that are polynomial functions over a finite field were found by [4; 26].
Reference: [10] <author> D. Coppersmith, </author> <title> private communication. </title> <month> September </month> <year> 1990. </year>
Reference-contexts: Self-correctors for f that are polynomial functions over a finite field were found by [4; 26]. The fraction of errors they could correct was improved to almost 1=4 independently by [14] and <ref> [10] </ref> and then to almost 1=2 by [15] (using a solution for the univariate case implicit in [5]).
Reference: [11] <author> R. DeMillo and R. Lipton. </author> <title> A probabilistic remark on algebraic program testing. </title> <journal> Information Processing Letters, </journal> <volume> 7(4) </volume> <pages> 193-195, </pages> <month> June </month> <year> 1978. </year>
Reference-contexts: Q jfx : 9i i (x) = 1gj m 0 i=1 x X X i (x) j (x) m 0 ffiQ 2 jfx : f 1 (x) = f 2 (x)gj Since two degree d polynomials f 1 and f 2 can agree on at most d q Q points <ref> [11, 36, 39] </ref>, we get: m 0 ffiQ m 0 (m 0 1) q Consider the function g (y) def Then the above inequality says that g (m 0 ) 0, for every integer m 0 m. Let ff 1 and ff 2 be the roots of g.
Reference: [12] <author> Y. Freund, D. Ron. </author> <title> Learning to Model Sequences Generated by Switching Distributions. </title> <address> COLT, </address> <year> 1995. </year>
Reference: [13] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> Exact identification of read-once formulas using fixed points of amplification functions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22 (1993), </volume> <pages> 705-726. </pages>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise.
Reference: [14] <author> P. Gemmell, R. Lipton, R. Rubinfeld, M. Sudan and A. Wigderson. </author> <title> Self-Testing/Correcting for Polynomials and for Approximate Functions. </title> <booktitle> STOC, </booktitle> <year> 1991. </year>
Reference-contexts: Self-correctors for f that are polynomial functions over a finite field were found by [4; 26]. The fraction of errors they could correct was improved to almost 1=4 independently by <ref> [14] </ref> and [10] and then to almost 1=2 by [15] (using a solution for the univariate case implicit in [5]).
Reference: [15] <author> P. Gemmell and M. Sudan. </author> <title> Highly resilient correctors for polynomials. </title> <journal> Info. Proc. Letters, </journal> <volume> 43 (1992), </volume> <pages> 169-174. </pages>
Reference-contexts: Self-correctors for f that are polynomial functions over a finite field were found by [4; 26]. The fraction of errors they could correct was improved to almost 1=4 independently by [14] and [10] and then to almost 1=2 by <ref> [15] </ref> (using a solution for the univariate case implicit in [5]).
Reference: [16] <author> O. Goldreich. </author> <title> Foundations of Cryptography Fragments of a Book. </title> <institution> Weiz-mann Institute of Science, </institution> <month> February </month> <year> 1995. </year> <title> Available from the ECCC, email ftpmail@ftp.eccc.uni-trier.de (use subject 'help eccc' for initial instructions). </title>
Reference-contexts: We may assume that 1=q &lt; *=4 (since otherwise q &lt; 4=* 2 We refer to the original algorithm as in [17], not to a simpler algorithm which appears in later versions (cf., <ref> [25, 16] </ref>). and we can afford to perform the simpler procedure above).
Reference: [17] <author> O. Goldreich and L.A. Levin. </author> <title> A Hard-Core Predicate for any One-Way Function. </title> <booktitle> STOC, </booktitle> <year> 1989. </year>
Reference-contexts: Linear Polynomials Goldreich and Levin <ref> [17] </ref> have solved the reconstruction problem in the case where d = 1 and F = GF (2). Similar ideas are used by Kushilevitz and Mansour [23] to learn boolean decision trees. <p> In this case our algorithm is a generalization of an algorithm due to Goldreich and Levin <ref> [17] </ref> 2 . (The original algorithm is regained by setting q = 2.) To proceed, we need the following definition: the i-prefix of a linear polynomial p (x 1 ; :::; x n ) is the polynomial which results by summing up all of the (degree 1) monomials in which only <p> As shown in <ref> [17] </ref>, the correct candidate passes the test with overwhelming probability. On the other hand, as shown in Section 5, at most O (1=* 2 ) candidates (of a certain length) may pass the test. The above yields a poly (nq=*)-time algorithm. <p> We may assume that 1=q &lt; *=4 (since otherwise q &lt; 4=* 2 We refer to the original algorithm as in <ref> [17] </ref>, not to a simpler algorithm which appears in later versions (cf., [25, 16]). and we can afford to perform the simpler procedure above).
Reference: [18] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> STOC, </booktitle> <year> 1993. </year>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise. <p> Finally, queries seem essential to our learning algorithm since for the case F = GF (2) and d = 1 the problem reduces to the well-known problem of learning parity with noise <ref> [18] </ref> which is commonly believed to be hard when one is only allowed uniformly and independently chosen examples [18, 7, 20]. (Actually, learning parity with noise is considered hard even for random noise, whereas here the noise is adversarial.) In the full version, we give evidence that the problem may be <p> Finally, queries seem essential to our learning algorithm since for the case F = GF (2) and d = 1 the problem reduces to the well-known problem of learning parity with noise [18] which is commonly believed to be hard when one is only allowed uniformly and independently chosen examples <ref> [18, 7, 20] </ref>. (Actually, learning parity with noise is considered hard even for random noise, whereas here the noise is adversarial.) In the full version, we give evidence that the problem may be hard with respect to d even in the case where n = 1.
Reference: [19] <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22 (1993), </volume> <pages> 807-837. </pages>
Reference-contexts: First, it falls into the paradigm of learning with persistent noise. Here one assumes that the function f is derived from some function in the class C by adding noise to it. Typical works in this direction either tolerate only small amounts of noise <ref> [2, 38, 19, 37] </ref> (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random [1, 24, 18, 23, 31, 13, 34] (i.e., that the decision of whether or not to modify the function at any given input is made
Reference: [20] <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire and L. Sellie. </author> <title> On the Learnability of Discrete Distributions, </title> <booktitle> STOC, </booktitle> <year> 1994. </year>
Reference-contexts: Finally, queries seem essential to our learning algorithm since for the case F = GF (2) and d = 1 the problem reduces to the well-known problem of learning parity with noise [18] which is commonly believed to be hard when one is only allowed uniformly and independently chosen examples <ref> [18, 7, 20] </ref>. (Actually, learning parity with noise is considered hard even for random noise, whereas here the noise is adversarial.) In the full version, we give evidence that the problem may be hard with respect to d even in the case where n = 1.
Reference: [21] <author> M. Kearns, R. Schapire and L. Sellie, </author> <title> Towards efficient agnostic learning. </title> <address> COLT, </address> <year> 1992. </year>
Reference-contexts: Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598 (madhu@watson.ibm.com). A second interpretation of the reconstruction problem is within the paradigm of agnostic learning introduced by Kearns et. al. <ref> [21] </ref> (see also [27, 28, 22]). In the setting of agnostic learning, the learner is to make no assumptions regarding the natural phenomena underlying the input/output relationship of the function, and the goal of the learner is to come up with a simple explanation which best fits the examples.
Reference: [22] <author> P. Koiran. </author> <title> Efficient Learning of Continuous Neural Nets. </title> <address> COLT, </address> <year> 1994. </year>
Reference-contexts: Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598 (madhu@watson.ibm.com). A second interpretation of the reconstruction problem is within the paradigm of agnostic learning introduced by Kearns et. al. [21] (see also <ref> [27, 28, 22] </ref>). In the setting of agnostic learning, the learner is to make no assumptions regarding the natural phenomena underlying the input/output relationship of the function, and the goal of the learner is to come up with a simple explanation which best fits the examples.
Reference: [23] <author> E. Kushilevitz, Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <booktitle> STOC, </booktitle> <year> 1991. </year>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise. <p> Linear Polynomials Goldreich and Levin [17] have solved the reconstruction problem in the case where d = 1 and F = GF (2). Similar ideas are used by Kushilevitz and Mansour <ref> [23] </ref> to learn boolean decision trees. Reconstruction of polynomials under structured error models Ar et. al. [3] have considered the problem of reconstructing a list of polynomials which together explain the input-output relation of a given black-box.
Reference: [24] <author> P. Laird. </author> <title> Learning From Good Data and Bad. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise.
Reference: [25] <author> L.A. Levin, </author> <title> Randomness and Non-determinism. </title> <journal> J. of Symb. Logic, </journal> <volume> 58(3) </volume> <pages> 1102-1103, </pages> <year> 1993. </year> <booktitle> Also in International Congress of Mathematicians: Abstracts of Invited Lectures. </booktitle> <address> p.155, Zurich, </address> <month> August 4, </month> <year> 1994. </year>
Reference-contexts: We may assume that 1=q &lt; *=4 (since otherwise q &lt; 4=* 2 We refer to the original algorithm as in [17], not to a simpler algorithm which appears in later versions (cf., <ref> [25, 16] </ref>). and we can afford to perform the simpler procedure above).
Reference: [26] <author> R. Lipton. </author> <title> New directions in testing. </title> <booktitle> In Distributed Computing and Cryptography, DIMACS Series on Discrete Mathematics and Theoretical Computer Science, </booktitle> <pages> pages 191-202, </pages> <editor> v. </editor> <volume> 2, </volume> <year> 1991. </year>
Reference-contexts: Self-Correction In the case when the noise rate is positive but small, one approach used to solving the reconstruction problem is to use self-correctors, introduced independently in [8] and <ref> [26] </ref>. Self-correctors convert programs that are known to be correct on a fraction ffi of inputs into programs that are correct on each input. Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions [8, 9, 26, 32]. <p> Self-correctors convert programs that are known to be correct on a fraction ffi of inputs into programs that are correct on each input. Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions <ref> [8, 9, 26, 32] </ref>. Self-correctors for f that are polynomial functions over a finite field were found by [4; 26]. <p> Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions [8, 9, 26, 32]. Self-correctors for f that are polynomial functions over a finite field were found by <ref> [4; 26] </ref>. The fraction of errors they could correct was improved to almost 1=4 independently by [14] and [10] and then to almost 1=2 by [15] (using a solution for the univariate case implicit in [5]).
Reference: [27] <author> Wolfgang Maass. </author> <title> Efficient Agnostic PAC-Learning with Simple Hypotheses. </title> <address> COLT, </address> <year> 1994. </year>
Reference-contexts: Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598 (madhu@watson.ibm.com). A second interpretation of the reconstruction problem is within the paradigm of agnostic learning introduced by Kearns et. al. [21] (see also <ref> [27, 28, 22] </ref>). In the setting of agnostic learning, the learner is to make no assumptions regarding the natural phenomena underlying the input/output relationship of the function, and the goal of the learner is to come up with a simple explanation which best fits the examples.
Reference: [28] <author> Wolfgang Maass. </author> <title> Agnostic PAC-Learning of Functions on Analog Neural Nets. </title> <booktitle> Proc. 7th IEEE Conf. on Neural Information Processing Systems. </booktitle>
Reference-contexts: Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598 (madhu@watson.ibm.com). A second interpretation of the reconstruction problem is within the paradigm of agnostic learning introduced by Kearns et. al. [21] (see also <ref> [27, 28, 22] </ref>). In the setting of agnostic learning, the learner is to make no assumptions regarding the natural phenomena underlying the input/output relationship of the function, and the goal of the learner is to come up with a simple explanation which best fits the examples.
Reference: [29] <author> F. MacWilliams and N. Sloane. </author> <title> The Theory of Error-Correcting Codes. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: Thus, in this case, the above algorithm will output only the polynomial p. A different perspective: Maximum-likelihood decoding of error-correcting codes Maximum likelihood decoding is the term applied to the task of computing the nearest codeword from a specified error-correcting code to a given word (cf., <ref> [29] </ref>). Consider the error-correcting code which encodes n+d ments of F by first computing the polynomial obtained by using these elements as the list of coefficients and then evaluating the polynomial at all points in the field. <p> Johnson (c.f., MacWilliams and Sloane <ref> [29] </ref>) for the case q = 2.
Reference: [30] <author> Y. Mansour. </author> <title> Randomized approximation and interpolation of sparse polynomials. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24 (1995), </volume> <pages> 357-368. </pages>
Reference: [31] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Learning fallible finite state automata. </title> <address> COLT, </address> <year> 1993. </year> <note> To appear in Machine Learning, COLT '93 special issue. </note>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise.
Reference: [32] <author> R. Rubinfeld. </author> <title> Robust Functional Equations and their Applications to Program Testing. </title> <booktitle> FOCS, </booktitle> <year> 1994. </year>
Reference-contexts: Self-correctors convert programs that are known to be correct on a fraction ffi of inputs into programs that are correct on each input. Self-correctors for values of ffi that are larger than 3=4 have been constructed for several functions <ref> [8, 9, 26, 32] </ref>. Self-correctors for f that are polynomial functions over a finite field were found by [4; 26].
Reference: [33] <author> R. Rubinfeld and M. Sudan. </author> <title> Robust Characterizations of Polynomials and their Applications to Program Testing. </title> <note> To appear in SIAM J. of Computing. Also available as IBM Research Report RC 19156 (83446) 9/9/93 and Cornell CS Tech. Report 93-1387, </note> <month> September </month> <year> 1993. </year>
Reference: [34] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Information Processing Letters, </journal> <volume> 37 (1991), </volume> <pages> 279-284. </pages>
Reference-contexts: Typical works in this direction either tolerate only small amounts of noise [2, 38, 19, 37] (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random <ref> [1, 24, 18, 23, 31, 13, 34] </ref> (i.e., that the decision of whether or not to modify the function at any given input is made by a random process). In contrast, we take the setting to an extreme, by considering a very large amount of (possibly adversarially chosen) noise.
Reference: [35] <author> Y. Sakakibara and Rani Siromoney. </author> <title> A noise model on learning sets of strings. </title> <address> COLT, </address> <year> 1992. </year>
Reference: [36] <author> J. T. Schwartz. </author> <title> Fast probabilistic algorithms for verification of polynomial identities. </title> <journal> Journal of the ACM, </journal> <volume> 27 </volume> <pages> 701-717, </pages> <year> 1980. </year>
Reference-contexts: Q jfx : 9i i (x) = 1gj m 0 i=1 x X X i (x) j (x) m 0 ffiQ 2 jfx : f 1 (x) = f 2 (x)gj Since two degree d polynomials f 1 and f 2 can agree on at most d q Q points <ref> [11, 36, 39] </ref>, we get: m 0 ffiQ m 0 (m 0 1) q Consider the function g (y) def Then the above inequality says that g (m 0 ) 0, for every integer m 0 m. Let ff 1 and ff 2 be the roots of g.
Reference: [37] <author> R. H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <address> COLT, </address> <year> 1988. </year>
Reference-contexts: First, it falls into the paradigm of learning with persistent noise. Here one assumes that the function f is derived from some function in the class C by adding noise to it. Typical works in this direction either tolerate only small amounts of noise <ref> [2, 38, 19, 37] </ref> (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random [1, 24, 18, 23, 31, 13, 34] (i.e., that the decision of whether or not to modify the function at any given input is made
Reference: [38] <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the 9th International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 560-566), </pages> <year> 1985. </year>
Reference-contexts: First, it falls into the paradigm of learning with persistent noise. Here one assumes that the function f is derived from some function in the class C by adding noise to it. Typical works in this direction either tolerate only small amounts of noise <ref> [2, 38, 19, 37] </ref> (i.e., that the function is modified only at a small fraction of all possible inputs) or assume that the noise is random [1, 24, 18, 23, 31, 13, 34] (i.e., that the decision of whether or not to modify the function at any given input is made
Reference: [39] <author> R. Zippel. </author> <title> Probabilistic algorithms for sparse polynomials. </title> <booktitle> EUROSAM '79, Lecture Notes in Computer Science, </booktitle> <volume> 72 </volume> <pages> 216-226, </pages> <year> 1979. </year>
Reference-contexts: Q jfx : 9i i (x) = 1gj m 0 i=1 x X X i (x) j (x) m 0 ffiQ 2 jfx : f 1 (x) = f 2 (x)gj Since two degree d polynomials f 1 and f 2 can agree on at most d q Q points <ref> [11, 36, 39] </ref>, we get: m 0 ffiQ m 0 (m 0 1) q Consider the function g (y) def Then the above inequality says that g (m 0 ) 0, for every integer m 0 m. Let ff 1 and ff 2 be the roots of g.
Reference: [40] <author> R.E. Zippel. </author> <title> Interpolating Polynomials from their Values. </title> <journal> J. Symbolic Computation 9, </journal> <pages> pages 375-403, </pages> <year> 1990. </year>
Reference: [41] <author> R.E. Zippel. </author> <title> Effective Polynomial Computation. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: In this case the 1 This is different from random noise as the set of corrupted inputs is selected adversarially only the values at these inputs are random. problem is well analyzed and the reader is referred to <ref> [41] </ref>, for instance, for a history of the polynomial interpolation problem. Self-Correction In the case when the noise rate is positive but small, one approach used to solving the reconstruction problem is to use self-correctors, introduced independently in [8] and [26].
References-found: 41

