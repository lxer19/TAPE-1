URL: http://cs.nyu.edu/cs/projects/ultra/nsf/proposal.ps
Refering-URL: http://cs.nyu.edu/cs/projects/ultra/
Root-URL: http://www.cs.nyu.edu
Title: proposal embarks on a program of characterizing and modeling the performance and behavior of scalable
Note: 1. Project Summary The present  These  Evaluating the NYU Ultracomputer Page 1  
Abstract: The NYU Ultracomputer project has studied the scalability of computing systems and algorithms, specifically the shared-memory computation model. Currently under construction is a 16-processor Ultracomputer using NYU designed, custom VLSI switches to interconnect processors and memories. This system supports fetch-and-phi coordination primitives and is the first to support asynchronous combining of simultaneous requests to the same memory location. The use of XILINX chips in the PE provides two capabilities that will enhance experimentation. First, the PEs have the ability to count a large number of arbitrary events within the PE and to produce summaries such as histograms of network delays. Second, various parameters of the network interface can be adjusted to determine the effects of these choices on system performance, on both a micro and macroscopic level. The presence of XILINX chips on the switch board enables monitoring of the behavior of the network and recording statistics such as the number of combined operations at each switch and maximum and typical queue lengths within the switches. will be studied in detail. Examine issues in the design of parallel operating systems, especially synchronization algorithms. Explore alternatives in processing element design. Study the behavior of the multi-stage interconnection network. Construct mathematical models of parallel system behavior based on our experiments and measure ments. Such models can be used to predict the behavior of large numbers of parallel processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> William Y. Chen, Scott A. Mahlke, and Wen-mei W. Hwu, </author> <title> ``Tolerating First Level Memory Access Latency in High-Performance Systems'', </title> <booktitle> Proc. 21st Annual ICPP I, </booktitle> <pages> pp. </pages> <month> 36-43 (August </month> <year> 1992). </year>
Reference-contexts: Subsequent research in this area may involve further increasing the number of outstanding loads by providing a mechanism for the processor to fetch data into registers internal to the XILINX and only later to request such data. This is similar to the mechanism studied under simulations in <ref> [1] </ref>. The information we obtain from these investigations will assist in the design of algorithms and compiler optimization, as well as the design of future PEs for larger systems. 2.4.5.
Reference: [2] <author> Jerome Chiabaut, Vladimir Fleyshgakker, and Anne Greenbaum, </author> <title> ``Porting Scientific Applica tions to the NYU Ultracomputer'', Ultracomputer Note 144, </title> <institution> Courant Institute, NYU (1988). </institution>
Reference-contexts: In addition, various program libraries are available, which contain coordination routines such as barrier synchronization and counting semaphores, routines for concurrent access to important data structures such as queues, and other items as well. Previous work on the bus-based 8-PE Ultra II <ref> [2] </ref> showed a speedup limited by bus contention to around seven on many highly-parallel applications; the interconnection network on Ultra III should Evaluating the NYU Ultracomputer Page 8 overcome this limitation (naturally algorithms with significant serial portions do not perform this well).
Reference: [3] <author> W. Crowther, J. Goodhue, R. Gurwitz, R. Rettberg, and R. Thomas, </author> <title> ``The Butterfly (TM) Parallel Processor'', </title> <journal> IEEE Computer Architecture Technical Committee Newsletter, </journal> <pages> pp. </pages> <month> 18-45 (Sep tember </month> <year> 1985). </year>
Reference-contexts: Although these algorithms are bottleneck-free on an idealized PRAM, their actual performance is highly dependent on the target architecture and the run-time environment provided by the operating system. For example, although the BBN Butterfly <ref> [3] </ref> supports atomic fetch-and-add operations, they are slow and serialized on that architecture. Such systems impose an architectural bottleneck on bottleneck-free coordination algorithms based on fetch-and-add. <p> The Behavior of Multistage Interconnection Networks Other experimental systems with good performance monitoring capabilities have mesh or hypercube connections (e.g., Alewife [27], the Message-Driven Processor [30], Hnet [25], and the Stanford Dash [28]), or use an unbuffered multi-stage interconnection network (e.g., the Transit Network [14] or the BBN Butterfly <ref> [3] </ref>). Buffered multistage interconnection networks have been the subject of many analytical and simulation studies, but the only opportunity for measurement has been the RP3 [29]. The RP3 implementation did not include combining, so Ultra III will offer the first opportunity to measure its effect.
Reference: [4] <author> Susan R. Dickey and Ora E. Percus, </author> <title> ``Performance Differences among Combining Switch Archi tectures'', </title> <booktitle> Proc. 21st Annual ICPP I, </booktitle> <pages> pp. </pages> <month> 110-117 (August </month> <year> 1992). </year>
Reference-contexts: Evaluating the NYU Ultracomputer Page 11 2.4.6. Mathematical Models of System Behavior We are interested in developing models of parallel system behavior that can be extrapolated to larger systems. Our previous work in system modeling has concentrated on network behavior [22, 19] and on the behavior of combining switches <ref> [4] </ref> under stochastic loads, sometimes with a fixed percentage of hot spot traffic. Memory traffic generated from real applications may be significantly different from that assumed in these stochastic models.
Reference: [5] <author> Jan Edler, </author> <title> ``Practical Structures for Parallel Operating Systems'', </title> <type> Ph.D. Thesis, </type> <institution> Courant Insti tute, NYU (1993). </institution>
Reference-contexts: This class of algorithms may generate substantially less network traffic than those that utilize conventional shared variables. The communication algorithms that will be investigated include implementations of several families of queues, multi-queues and an asynchronous signal delivery system <ref> [5] </ref>. The queue algorithms primarily differ in the relative ordering of inserts and deletes, their ability to support internal deletions, and the mechanisms utilized to store and access individual items. <p> Some Symunix experiments were performed on our old Ultra II prototype to compare the performance effects of fetch-and-add algorithms to semaphores <ref> [5] </ref>. In one experiment, we measured the number of open and close operations executed per second by up to eight (the number of PEs in our system) processes using three different Unix kernels: a uniprocessor kernel, Symunix with the fetch-and-add algorithms replaced by semaphores, and normal Symunix.
Reference: [6] <author> Jan Edler, Jim Lipkis, and Edith Schonberg, </author> <title> ``Memory Management in Symunix II: A Design for Large-Scale Shared Memory Multiprocessors'', Proc. USENIX Workshop on UNIX and Supercomputers (September 1988). </title> <note> Also available as Ultracomputer Note 135, </note> <institution> Courant Institute, NYU. </institution>
Reference-contexts: Results from experiments on queues will be useful not only for the selection and tuning of these queue algorithms but also for the choice of data storage mechanisms for other applications. Several algorithms have been proposed for parallel memory allocation on Ultracomputers <ref> [26, 6] </ref>. These algorithms include parallel buddy and first-fit systems. Our evaluation of these algorithms will consider fragmentation as an additional performance characteristic.
Reference: [7] <author> Eric Freudenthal, </author> <title> ``Evaluation of Interprocessor Triggers as a Coordination Primitive for Shared-Memory MIMD Computers'', 1992 Massively Parallel Computing Initiative Yearly Report, </title> <institution> Lawrence Livermore National Labs </institution> (). 
Reference-contexts: Finally, we will use the Ultra III prototype to explore a new class of coordination algorithms that use each processor's reflect register as a single bit of processor-local shared memory for spin-waiting <ref> [7] </ref>. This class of algorithms may generate substantially less network traffic than those that utilize conventional shared variables. The communication algorithms that will be investigated include implementations of several families of queues, multi-queues and an asynchronous signal delivery system [5].
Reference: [8] <author> Eric Freudenthal and Allan Gottlieb, </author> <title> ``Process Coordination with Fetch-and-Increment'', </title> <booktitle> Proc. ASPLOS-IV, </booktitle> <pages> pp. </pages> <month> 260-268 (April </month> <year> 1991). </year>
Reference-contexts: The synchronization algorithms to be investigated include numerous competing implementations of binary and counting semaphores, readers/writers, and barriers for groups with both fixed and variable Evaluating the NYU Ultracomputer Page 9 sizes <ref> [8] </ref>. Differing implementations of the same primitive vary in memory requirements, access patterns, and fairness properties. We are interested in exploring the effectiveness of these different schemes.
Reference: [9] <author> G. R. Goke and G. J. Lipovski, </author> <title> ``Banyan Networks for Partitioning Multiprocessor Systems'', </title> <booktitle> Proc. 1st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. </pages> <month> 21-28 </month> <year> (1973). </year>
Reference-contexts: All operations are combinable by the network except for Partial-Word-Store, Reflect, and Broadcast. 2.2.3. Interconnection Network The Ultracomputer network is built from K K switches and has the topology of the omega network [16] and the SW Banyan <ref> [9] </ref>. It consists of log K N stages, where N is the number of PEs in the system.
Reference: [10] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larry Rudolph, and Mark Snir, </author> <title> ``The NYU UltracomputerDesigning a MIMD, Shared-Memory Parallel Machine'', </title> <journal> IEEE Transactions on Computers 32 (2), </journal> <pages> pp. </pages> <month> 175-189 (February </month> <year> 1983). </year>
Reference-contexts: Our current NSF grant is funding the construction of a 16-processor Ultracomputer using our custom VLSI switches to interconnect processors and memories. This system supports fetch-and-phi coordination primitives and is the first to support asynchronous combining of simultaneous requests to the same memory location <ref> [10] </ref>. The present proposal embarks on a program of characterizing and modeling the performance and behavior of scalable shared-memory systems and their algorithms. Numerous measurements and architectural experiments will exploit the configurability of our 16-processor system and a 256-processor system that may become available during the proposal period.
Reference: [11] <author> Anne Greenbaum, </author> <title> ``Solving Sparse Triangular Linear Systems Using FORTRAN with Parallel Extensions on the NYU Ultracomputer Prototype'', Ultracomputer Note 99, </title> <institution> Courant Institute, NYU (1986). </institution>
Reference-contexts: A first step will be to duplicate this research and see what speedups are obtained. Previous research on scientific codes <ref> [11, 12] </ref> has included analyses of the types of operations that can be executed efficiently on various parallel architectures, identification of the operations used in current codes, and modification or redesign of the algorithms used in these programs to take better advantage of the architectures.
Reference: [12] <author> Anne Greenbaum, Congming Li, and Han Zheng Chao, </author> <title> ``Comparison of Linear System Solvers Applied to Diffusion-Type Finite Element Equations'', Ultracomputer Note 126, </title> <institution> Courant Insti tute, NYU (1987). </institution>
Reference-contexts: A first step will be to duplicate this research and see what speedups are obtained. Previous research on scientific codes <ref> [11, 12] </ref> has included analyses of the types of operations that can be executed efficiently on various parallel architectures, identification of the operations used in current codes, and modification or redesign of the algorithms used in these programs to take better advantage of the architectures.
Reference: [13] <author> Hong Jiang and Laxmi N. Bhuyan, ``MVAMIN: </author> <title> Mean Value Analysis Algorithms for Multistage Interconnection Networks'', </title> <journal> Journal of Parallel and Distributed Computing 12, </journal> <pages> pp. </pages> <month> 189-201 </month> <year> (1991). </year>
Reference-contexts: This effect cannot be modeled exactly with either open network models, such as those used in [18, 15], which allow a processor to issue an indefinite number of requests, or with simple closed network models, such as <ref> [13] </ref>, in which a processor has a fixed number of messages in the system independent of the round-trip latency of a single message.
Reference: [14] <author> Thomas F. Knight, Jr., </author> <title> ``Technologies for Low-Latency Interconnection Switches'', </title> <booktitle> Symposium on Parallel Algorithms and Architectures, </booktitle> <month> (June </month> <year> 1989). </year>
Reference-contexts: The Behavior of Multistage Interconnection Networks Other experimental systems with good performance monitoring capabilities have mesh or hypercube connections (e.g., Alewife [27], the Message-Driven Processor [30], Hnet [25], and the Stanford Dash [28]), or use an unbuffered multi-stage interconnection network (e.g., the Transit Network <ref> [14] </ref> or the BBN Butterfly [3]). Buffered multistage interconnection networks have been the subject of many analytical and simulation studies, but the only opportunity for measurement has been the RP3 [29]. The RP3 implementation did not include combining, so Ultra III will offer the first opportunity to measure its effect.
Reference: [15] <author> Clyde P. Kruskal, Marc Snir, and Allan J. Weiss, </author> <title> ``On the Distribution of Delays in Buffered Multistage Interconnection Networks for Uniform and Nonuniform Traffic'', </title> <booktitle> Proc. 1984 Interna tional Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <month> 219-220 </month> <year> (1984). </year>
Reference-contexts: This effect cannot be modeled exactly with either open network models, such as those used in <ref> [18, 15] </ref>, which allow a processor to issue an indefinite number of requests, or with simple closed network models, such as [13], in which a processor has a fixed number of messages in the system independent of the round-trip latency of a single message.
Reference: [16] <author> Duncan H. Lawrie, </author> <title> ``Access and Alignment of Data in an Array Processor'', </title> <journal> IEEE Transactions on Computers 24 (12), </journal> <pages> pp. </pages> <month> 1145-1155 (December </month> <year> 1975). </year>
Reference-contexts: All operations are combinable by the network except for Partial-Word-Store, Reflect, and Broadcast. 2.2.3. Interconnection Network The Ultracomputer network is built from K K switches and has the topology of the omega network <ref> [16] </ref> and the SW Banyan [9]. It consists of log K N stages, where N is the number of PEs in the system.
Reference: [17] <author> Gyung Ho Lee, Clyde P. Kruskal, and David J. Kuck, </author> <title> ``The Effectiveness of Combining in Shared-Memory Parallel Computers in the Presence of Hot-Spots'', </title> <booktitle> Proc. 1986 International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <month> 35-41 (August </month> <year> 1986). </year>
Reference-contexts: Characterize traffic levels and hotspot concentrations at which combining is useful and effective. More than two-way combining may be required for large networks <ref> [17] </ref>; measuring the amount of increased combining in the earlier stages due to blocking at the later stages will give us better estimates as to how large a network must be before greater than two-way combining is needed. Evaluating the NYU Ultracomputer Page 11 2.4.6.
Reference: [18] <author> T. Lin and L. Kleinrock, </author> <title> ``Performance Anlysis of Finite-Buffered Multistage Interconnection Networks with a General Traffic Pattern'', </title> <booktitle> Proc. ACM Sigmetrics Performance 91, </booktitle> <pages> pp. </pages> <note> 68-78 Evaluating the NYU Ultracomputer Page 16 (1991). </note>
Reference-contexts: This effect cannot be modeled exactly with either open network models, such as those used in <ref> [18, 15] </ref>, which allow a processor to issue an indefinite number of requests, or with simple closed network models, such as [13], in which a processor has a fixed number of messages in the system independent of the round-trip latency of a single message.
Reference: [19] <author> Yue-Sheng Liu, </author> <title> ``Architecture and Performance of Processor-Memory Interconnection Networks for MIMD Shared Memory Parallel Processing Systems'', </title> <publisher> Ph. </publisher> <address> D. Dissertation, New York University, </address> <note> Department of Computer Science (September 1990). </note>
Reference-contexts: Such stimuli will include synthetic applications similar to the MAD kernels [21] as well as special XILINX firmware designed to stress the network. Previous simulation studies of network behavior <ref> [23, 19] </ref> have shown combining to be effective in alleviating tree saturation due to hot spots at memory. Using statistics obtained with both real and artificial loading of the network, we plan to: Verify simulation and analytical models of single hotspot behavior. <p> Evaluating the NYU Ultracomputer Page 11 2.4.6. Mathematical Models of System Behavior We are interested in developing models of parallel system behavior that can be extrapolated to larger systems. Our previous work in system modeling has concentrated on network behavior <ref> [22, 19] </ref> and on the behavior of combining switches [4] under stochastic loads, sometimes with a fixed percentage of hot spot traffic. Memory traffic generated from real applications may be significantly different from that assumed in these stochastic models.
Reference: [20] <author> John M. Mellor-Crummey and Michael L. Scott, </author> <title> ``Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors'', </title> <note> ACM TOCS 9 (1) (February 1991). </note>
Reference-contexts: In contrast, we anticipate that combining in the Ultracomputer network will eliminate this bottleneck, though we may discover other classes of network contention that affect the performance of coordination algorithms. We experienced a related effect while experimenting with a BBN TC-2000 Butterfly at LLNL. The coordination algorithms in <ref> [20] </ref> make extensive use of the processor-local shared memory provided on the Butterfly to reduce network load. Their idea is to partition data structures so that waiting processors spin on local variables, which are updated by remote processors when the waited-for event has occurred. <p> We investigated ``triggers'', which are a single bit of processor-local shared memory per processor. A processor spins on its local trigger; when it is set, the processor examines (remote) shared memory to determine the necessary actions. We modified the algorithms in <ref> [20] </ref> to use these triggers and timed the original and modified versions. The result was that both versions performed much worse than indicated in [20]. Clearly, these algorithms are very sensitive to the execution environment (the environments at LLNL and Argonne, where the referenced work was done, are different). <p> We modified the algorithms in <ref> [20] </ref> to use these triggers and timed the original and modified versions. The result was that both versions performed much worse than indicated in [20]. Clearly, these algorithms are very sensitive to the execution environment (the environments at LLNL and Argonne, where the referenced work was done, are different). We plan to repeat the above experment on our prototype, using the ``reflect'' network operation to provide the triggers.
Reference: [21] <author> Arun Nanda and Lionel M. Ni, </author> <title> ``MAD Kernels: An Experimental Testbed to Study Multiproces sor Memory System Behavior'', </title> <booktitle> Proc. 21st Annual ICPP I, </booktitle> <pages> pp. </pages> <month> 28-35 (August </month> <year> 1992). </year>
Reference-contexts: Statistics can be gathered during the course of other work on applications and operating systems as well as by applying artificial stimuli to the network. Such stimuli will include synthetic applications similar to the MAD kernels <ref> [21] </ref> as well as special XILINX firmware designed to stress the network. Previous simulation studies of network behavior [23, 19] have shown combining to be effective in alleviating tree saturation due to hot spots at memory.
Reference: [22] <author> Ora E. Percus and Jerome K. Percus, </author> <title> ``Elementary Properties of Clock-regulated Queues'', </title> <journal> SIAM Journal on Applied Mathematics 50, </journal> <pages> pp. </pages> <month> 1166-1175 </month> <year> (1990). </year>
Reference-contexts: Evaluating the NYU Ultracomputer Page 11 2.4.6. Mathematical Models of System Behavior We are interested in developing models of parallel system behavior that can be extrapolated to larger systems. Our previous work in system modeling has concentrated on network behavior <ref> [22, 19] </ref> and on the behavior of combining switches [4] under stochastic loads, sometimes with a fixed percentage of hot spot traffic. Memory traffic generated from real applications may be significantly different from that assumed in these stochastic models.
Reference: [23] <author> Gregory F. Pfister and V. Alan Norton, </author> <title> ```Hot Spot' Contention and Combining in Multistage Interconnection Networks'', </title> <journal> IEEE Transactions on Computers 34 (10), </journal> <pages> pp. </pages> <month> 943-948 (October </month> <year> 1985). </year>
Reference-contexts: Such stimuli will include synthetic applications similar to the MAD kernels [21] as well as special XILINX firmware designed to stress the network. Previous simulation studies of network behavior <ref> [23, 19] </ref> have shown combining to be effective in alleviating tree saturation due to hot spots at memory. Using statistics obtained with both real and artificial loading of the network, we plan to: Verify simulation and analytical models of single hotspot behavior.
Reference: [24] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta, </author> <title> ``SPLASH: Stanford Parallel Applications for Shared-Memory'', </title> <booktitle> Proc. 21st Annual ICPP I, </booktitle> <pages> pp. </pages> <month> 36-43 (August </month> <year> 1992). </year>
Reference-contexts: Numerical codes have been written that serve as good test programs for determining optimal synchronization strategies. These codes can be used in the study of static versus dynamic synchronization techniques under different operating conditions (e.g., timesharing vs. single user mode). Benchmarks like SPLASH <ref> [24] </ref> are also of interest, especially since they allow the comparison of our work with measurements on other architectures. We are currently in the process of porting the SPLASH benchmarks to the Ultra III prototype. 2.4.3.
Reference: [25] <author> David Smitley, Frank Hady, and Dan Burris, </author> <title> ``Hnet: A High-performance Network Evaluation Testbed'', </title> <booktitle> Proc. 21st Annual ICPP I, </booktitle> <pages> pp. </pages> <month> 276-279 (August </month> <year> 1992). </year>
Reference-contexts: The Behavior of Multistage Interconnection Networks Other experimental systems with good performance monitoring capabilities have mesh or hypercube connections (e.g., Alewife [27], the Message-Driven Processor [30], Hnet <ref> [25] </ref>, and the Stanford Dash [28]), or use an unbuffered multi-stage interconnection network (e.g., the Transit Network [14] or the BBN Butterfly [3]). Buffered multistage interconnection networks have been the subject of many analytical and simulation studies, but the only opportunity for measurement has been the RP3 [29].
Reference: [26] <author> James Wilson, </author> <title> ``Operating System Data Structures for Shared-Memory MIMD Machines with Fetch-and-Add'', </title> <type> Ph.D. thesis, </type> <institution> Courant Institute, NYU (1988). </institution>
Reference-contexts: Results from experiments on queues will be useful not only for the selection and tuning of these queue algorithms but also for the choice of data storage mechanisms for other applications. Several algorithms have been proposed for parallel memory allocation on Ultracomputers <ref> [26, 6] </ref>. These algorithms include parallel buddy and first-fit systems. Our evaluation of these algorithms will consider fragmentation as an additional performance characteristic.
Reference: [27] <author> Anant Agarwal, et. al, </author> <title> ``The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor'', </title> <type> Technical Report MIT/LCS/TM-454, </type> <institution> MIT Laboratory for Computer Science (June 1991). </institution>
Reference-contexts: The Behavior of Multistage Interconnection Networks Other experimental systems with good performance monitoring capabilities have mesh or hypercube connections (e.g., Alewife <ref> [27] </ref>, the Message-Driven Processor [30], Hnet [25], and the Stanford Dash [28]), or use an unbuffered multi-stage interconnection network (e.g., the Transit Network [14] or the BBN Butterfly [3]).
Reference: [28] <author> Daniel Lenoski et. al, </author> <title> ``The Stanford Dash Multiprocessor'', </title> <booktitle> IEEE Computer 25 (3) (March 1992). </booktitle>
Reference-contexts: The Behavior of Multistage Interconnection Networks Other experimental systems with good performance monitoring capabilities have mesh or hypercube connections (e.g., Alewife [27], the Message-Driven Processor [30], Hnet [25], and the Stanford Dash <ref> [28] </ref>), or use an unbuffered multi-stage interconnection network (e.g., the Transit Network [14] or the BBN Butterfly [3]). Buffered multistage interconnection networks have been the subject of many analytical and simulation studies, but the only opportunity for measurement has been the RP3 [29].
Reference: [29] <author> G. Pfister, et. al, </author> <title> ``The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture'', </title> <booktitle> Proc. 1985 International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <month> 764-771 (August, </month> <year> 1985). </year>
Reference-contexts: Buffered multistage interconnection networks have been the subject of many analytical and simulation studies, but the only opportunity for measurement has been the RP3 <ref> [29] </ref>. The RP3 implementation did not include combining, so Ultra III will offer the first opportunity to measure its effect.

References-found: 29

