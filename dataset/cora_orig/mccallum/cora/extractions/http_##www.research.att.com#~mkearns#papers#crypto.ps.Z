URL: http://www.research.att.com/~mkearns/papers/crypto.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: Cryptographic Limitations on Learning Boolean Formulae and Finite Automata  
Author: Michael Kearns Leslie Valiant 
Address: 600 Mountain Avenue, Room 2A-423 Murray Hill, New Jersey 07974  Cambridge, Massachussetts 02138  
Affiliation: AT&T Bell Laboratories  Aiken Computation Laboratory Harvard University  
Abstract: In this paper we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses. Our methods reduce the problems of cracking a number of well-known public-key cryptosys- tems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory: in particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography. We also apply our results to obtain strong intractability results for approximating a gener - alization of graph coloring. fl This research was conducted while the author was at Harvard University and supported by an A.T.& T. Bell Laboratories scholarship. y Supported by grants ONR-N00014-85-K-0445, NSF-DCR-8606366 and NSF-CCR-89-02500, DAAL03-86-K-0171, DARPA AFOSR 89-0506, and by SERC. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Adleman, K. Manders, G. Miller. </author> <title> On taking roots in finite fields. </title> <booktitle> Proceedings of the 18th I.E.E.E. Symposium on Foundations of Computer Science, </booktitle> <year> 1977, </year> <pages> pp. 175-178. </pages>
Reference-contexts: This is accomplished by giving an NC 1 implementation of the first three steps of the root-finding algorithm of Adleman, Manders and Miller <ref> [1] </ref> as it is described by Angluin [5]. Note that if we let a = x 2 mod N , then either r = a or r = (N a) mod N according to the definition of the modified Rabin function. The circuit has four phases. Phase I. <p> Since p and q are both congruent to 3 mod 4, u and p u are square roots of a mod q, and v and q v are square roots of a mod q by the results of Adleman et al. <ref> [1] </ref> (see also Angluin [5]). Phase III. Using Chinese remaindering, combine u; p u; v and q v to compute the four square roots of a mod N (see e.g. Kranakis [28]).
Reference: [2] <author> A. Aho, J. Hopcroft, J. Ullman. </author> <title> The design and analysis of computer algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: the standard schemes (see Garey and Johnson [20]), and denote by jxj and jcj the length of these encodings measured in bits (or in the case of real-valued domains, some other reasonable measure of length that may depend on the model of arithmetic computation used; see Aho, Hopcroft and Ullman <ref> [2] </ref>). Parameterized representation classes. In this paper we study parameterized classes of representations. Here we have a stratified domain X = [ n1 X n and representation class C = [ n1 C n .
Reference: [3] <author> D. Aldous. </author> <title> On the Markov chain simulation method for uniform combinatorial distributions and simu-lated annealing. </title> <institution> University of California at Berkeley Statistics Department, </institution> <type> technical report number 60, </type> <year> 1986. </year>
Reference: [4] <author> W. Alexi, B. Chor, O. Goldreich, </author> <title> C.P. Schnorr. RSA and Rabin functions: certain parts are as hard as the whole. </title> <journal> S.I.A.M. Journal on Computing, </journal> <volume> 17(2), </volume> <year> 1988, </year> <pages> pp. 194-209. </pages>
Reference-contexts: Further- more, the following result from Alexi et al. <ref> [4] </ref> indicates that determining the least significant bit of x is as hard as inverting RSA (which amounts to determining all the bits of x). Theorem 1 (Alexi et al. [4]) Let x; N and e be as above. <p> Further- more, the following result from Alexi et al. <ref> [4] </ref> indicates that determining the least significant bit of x is as hard as inverting RSA (which amounts to determining all the bits of x). Theorem 1 (Alexi et al. [4]) Let x; N and e be as above. <p> Furthermore, this reduction still holds when N is restricted to be a Blum integer in both problems. The modified Rabin encryption function <ref> [4] </ref> is specified by two primes p and q of length l, both congruent to 3 modulo 4. Let N = p q (thus N is a Blum integer). <p> Theorem 3 (Alexi et al. <ref> [4] </ref>) Let x and N be as above. <p> Thus we have a polynomial advantage for inverting the least significant bit of RSA. This allows us to invert RSA by the results of Alexi et al. <ref> [4] </ref> given as Theorem 1. 20 Ease of evaluating r (p;q;e) 2 C n : For each r (p;q;e) 2 C n , we show that r (p;q;e) has an equivalent NC 1 circuit. <p> This implies that we can factor Blum integers by the results of Rabin [35] and Alexi et al. <ref> [4] </ref> given in Theorems 2 and 3.
Reference: [5] <author> D. Angluin. </author> <title> Lecture notes on the complexity of some problems in number theory. </title> <institution> Yale University Computer Science Department, </institution> <type> technical report number TR-243, </type> <year> 1982. </year> <month> 38 </month>
Reference-contexts: For an introduction to number theory that is relevant to cryptography, we refer the reader to the work of Angluin <ref> [5] </ref> and Kranakis [28]. For N a natural number, Z N will denote the ring of integers modulo N , and Z fl N will denote the multiplicative group modulo N . <p> This is accomplished by giving an NC 1 implementation of the first three steps of the root-finding algorithm of Adleman, Manders and Miller [1] as it is described by Angluin <ref> [5] </ref>. Note that if we let a = x 2 mod N , then either r = a or r = (N a) mod N according to the definition of the modified Rabin function. The circuit has four phases. Phase I. <p> Note that since p and q are both congruent to 3 mod 4, (N a) mod N is never a quadratic residue mod N (see Angluin <ref> [5] </ref>). If it is decided that r = (N a) mod N , generate the intermediate output a mod N . This can clearly be done in NC 1 . Also, notice that for any z, z 2i = (N z) 2i mod N for i 1. <p> Since p and q are both congruent to 3 mod 4, u and p u are square roots of a mod q, and v and q v are square roots of a mod q by the results of Adleman et al. [1] (see also Angluin <ref> [5] </ref>). Phase III. Using Chinese remaindering, combine u; p u; v and q v to compute the four square roots of a mod N (see e.g. Kranakis [28]).
Reference: [6] <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75, </volume> <year> 1987, </year> <pages> pp. 87-106. </pages>
Reference: [7] <author> D. Angluin, M. Kharitonov. </author> <title> When won't membership queries help? Proceedings of the 23rd A.C.M. </title> <booktitle> Symposium on the Theory of Computing, </booktitle> <year> 1991, </year> <pages> pp. 444-454. </pages>
Reference-contexts: It would also be interesting to demonstrate a partial converse to our results: for instance, if we assume there is a representation class that is hard to learn, can it be used to construct any interesting cryptographic primitives? We encourage the reader to see the paper of Angluin and Kharitonov <ref> [7] </ref>, where the methods here are extended to prove hardness results for learning with queries.
Reference: [8] <author> D. Angluin, P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1988, </year> <pages> pp. 319-342. </pages>
Reference-contexts: If we also allow m to depend polynomially on 1=fi, we can obtain an estimate ^p such that p fi &lt; ^p &lt; p + fi with probability at least 1 ff (see for instance the paper of Angluin and Laird <ref> [8] </ref>). Notational conventions. Let E (x) be an event and (x) a random variable that depend on a parameter x that takes on values in a set X.
Reference: [9] <author> D. Angluin, L.G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 18, </volume> <year> 1979, </year> <pages> pp. 155-193. </pages>
Reference-contexts: Then for 0 ff 1, 11 Fact CB1. LE (p; m; (1 ff)mp) e ff 2 mp=2 and Fact CB2. GE (p; m; (1 + ff)mp) e ff 2 mp=3 These bounds in the form they are stated are from the paper of Angluin and Valiant <ref> [9] </ref> and follow from Chernoff [17]. Although we will make frequent use of Fact CB1 and Fact CB2, we will do so in varying levels of detail, depending on the complexity of the calculation involved.
Reference: [10] <author> P.W. Beame, S.A. Cook, H.J. </author> <title> Hoover. Log depth circuits for division and related problems. </title> <journal> S.I.A.M. Journal on Computing, </journal> <volume> 15(4), </volume> <year> 1986, </year> <pages> pp. 994-1003. </pages>
Reference-contexts: This is an NC 1 step by the iterated product circuits of Beame, Cook and Hoover <ref> [10] </ref>. 5.2 A learning problem based on quadratic residues The representation class C n : Let l be the largest natural number satisfying 4l 2 + 6l + 2 n. <p> Each such circuit involves only a division step followed by a multiplication and a subtraction. The results of Beame et al. <ref> [10] </ref> imply that these steps can be carried out by an NC 1 circuit. Phase II. Compute x (p1)=2 mod p and x (q1)=2 mod q. These can be computed by multiplying the appropriate powers mod p and mod q computed in Phase I. <p> These can be computed by multiplying the appropriate powers mod p and mod q computed in Phase I. Since the iterated product of l numbers each of length l bits can be computed in NC 1 by the results of Beame et al. <ref> [10] </ref>, this is also an NC 1 step. Phase III. Determine if x (p1)=2 = 1 mod p or x (p1)=2 = 1 mod p, and if x (q1)=2 = 1 mod q or x (q1)=2 = 1 mod q. <p> Then for some polynomial p (n), the problems of inverting the RSA encryption function, recognizing quadratic residues and factoring Blum integers are probabilistic polynomial-time reducible to weakly learning ADFA p (n) . 26 Using results of Chandra, Stockmeyer and Vishkin [16], Beame et al. <ref> [10] </ref> and Reif [36], it can be shown that the representations described in Section 5 can each be computed by a polynomial-size, constant-depth threshold circuit.
Reference: [11] <author> A. Blum. </author> <title> An ~ O(n 0:4 )-approximation algorithm for 3-coloring. </title> <booktitle> Proceedings of the 21st A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1989, </year> <pages> pp. 535-542. </pages>
Reference-contexts: Several well-studied problems apparently have this property, but little has been proven in this direction. Perhaps the best example is graph coloring, where the best polynomial-time algorithms require approximately n 11=(k1) colors on k-colorable n-vertex graphs (see Wigderson [40] and Blum <ref> [11] </ref>) but coloring has been proven NP-hard only for (2*)k colors for any * &gt; 0 (see Garey and Johnson [20]).
Reference: [12] <author> A. Blum, R.L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 9-18. </pages>
Reference-contexts: Representation-based hardness results for learning various classes of neural networks can also be derived from the results of Judd [25] and Blum and Rivest <ref> [12] </ref>. The first representation-independent hardness results for the distribution-free model follow from the work of Goldreich, Goldwasser and Micali [22], whose true motivation was to find easy-to- compute functions whose output on random inputs appears random to all polynomial-time algorithms. <p> Theorem 11 states that even if we allow a much larger net than is actually required, finding these weights is computationally intractable, even for only a constant number of "hidden layers". This result should be contrasted with those of Judd [25] and Blum and Rivest <ref> [12] </ref>, which rely on the weaker assumption P 6= NP but do not prove hardness for relaxed consistency and do not allow the hypothesis network to be substantially larger than the smallest consistent network. We also make no assumptions on the topology of the output circuit.
Reference: [13] <author> M. Blum, S. Micali. </author> <title> How to generate cryptographically strong sequences of pseudo-random bits. </title> <journal> S.I.A.M. Journal on Computing, </journal> <volume> 13(4), </volume> <year> 1984, </year> <pages> pp. 850-864. 39 </pages>
Reference-contexts: Pitt and Warmuth [33] then used this result to construct other hard-to-learn representation classes. For definitions and a discussion of one-way functions we refer the reader to Yao [41], Blum and Micali <ref> [13] </ref>, Levin [29], and Goldreich et al. [22].
Reference: [14] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24, </volume> <year> 1987, </year> <pages> pp. 377-380. </pages>
Reference-contexts: Such results are infrequent in complexity theory, and seem difficult to obtain for natural problems using presumably weaker assumptions such as P 6= NP . We begin by stating a needed theorem of Blumer et al. known as Occam's Razor <ref> [14] </ref>. Their result essentially gives an upper bound on the sample size required for learning C by H, and shows that the general technique of finding an hypothesis that is both consistent with the sample drawn and significantly shorter than this sample is sufficient for distribution-free learning. <p> Thus, if one can efficiently perform data compression on a random sample, then one can learn efficiently. Theorem 8 (Blumer et al. <ref> [14] </ref>) Let C and H be polynomially evaluatable parameterized Boolean representation classes.
Reference: [15] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the A.C.M., </journal> <volume> 36(4), </volume> <year> 1989, </year> <pages> pp. 929-965. </pages>
Reference-contexts: These theorems demonstrate that the results of Section 6 are in some sense not dependent upon the particular models of learnability that we study, since we are able to restate the hardness of learning in terms of standard combinatorial optimization problems. Using a generalization of Theorem 8 <ref> [15] </ref>, we can in fact prove Theorems 9, 10 and 11 for the Relaxed Consistency Problem, where the hypothesis found must agree with only a fraction 1=2 + 1=p (opt Con (S); n) for any fixed polynomial p. <p> The central idea of the proof is the same: since the results of Blumer et al. <ref> [15] </ref> demonstrate that for sufficient sample size, solution of the relaxed consistency problem implies weak learning, and we have shown weak learning to be as hard as the cryptographic problems for the various representation classes, the relaxed consistency problem is as hard as the cryptographic problems.
Reference: [16] <author> A.K. Chandra, L.J. Stockmeyer, U. Vishkin. </author> <title> Constant depth reducibility. </title> <journal> S.I.A.M. Journal on Computing, </journal> <volume> 13(2), </volume> <year> 1984, </year> <pages> pp. 423-432. </pages>
Reference-contexts: Then for some polynomial p (n), the problems of inverting the RSA encryption function, recognizing quadratic residues and factoring Blum integers are probabilistic polynomial-time reducible to weakly learning ADFA p (n) . 26 Using results of Chandra, Stockmeyer and Vishkin <ref> [16] </ref>, Beame et al. [10] and Reif [36], it can be shown that the representations described in Section 5 can each be computed by a polynomial-size, constant-depth threshold circuit.
Reference: [17] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23, </volume> <year> 1952, </year> <pages> pp. 493-509. </pages>
Reference-contexts: LE (p; m; (1 ff)mp) e ff 2 mp=2 and Fact CB2. GE (p; m; (1 + ff)mp) e ff 2 mp=3 These bounds in the form they are stated are from the paper of Angluin and Valiant [9] and follow from Chernoff <ref> [17] </ref>. Although we will make frequent use of Fact CB1 and Fact CB2, we will do so in varying levels of detail, depending on the complexity of the calculation involved.
Reference: [18] <author> W. Diffie, M. Hellman. </author> <title> New directions in cryptography. </title> <journal> I.E.E.E. Transactions on Information Theory, </journal> <volume> 22, </volume> <year> 1976, </year> <pages> pp. 644-654. </pages>
Reference-contexts: In response to complaints such as these and also more subtle security concerns, the field of public-key cryptography was created by Diffie and Hellman <ref> [18] </ref>. Public-key cryptography solves the problem of Alice and Bob via the use of trapdoor functions.
Reference: [19] <author> A. Ehrenfeucht, D. Haussler, M. Kearns. L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3), </volume> <year> 1989, </year> <pages> pp. 247-261. </pages>
Reference: [20] <author> M. Garey, D. Johnson. </author> <title> Computers and intractability: a guide to the theory of NP-completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: We assume that domain points x 2 X and representations c 2 C are efficiently encoded using any of the standard schemes (see Garey and Johnson <ref> [20] </ref>), and denote by jxj and jcj the length of these encodings measured in bits (or in the case of real-valued domains, some other reasonable measure of length that may depend on the model of arithmetic computation used; see Aho, Hopcroft and Ullman [2]). Parameterized representation classes. <p> Perhaps the best example is graph coloring, where the best polynomial-time algorithms require approximately n 11=(k1) colors on k-colorable n-vertex graphs (see Wigderson [40] and Blum [11]) but coloring has been proven NP-hard only for (2*)k colors for any * &gt; 0 (see Garey and Johnson <ref> [20] </ref>). Thus for 3-colorable graphs we only know that 5-coloring is hard, but the best algorithm requires roughly O (n 0:4 ) colors on n-vertex graphs! This leads us to look for approximation-preserving reductions from our provably hard optimization problems to other natural problems. <p> The NP-hardness results follow from Garey and Johnson <ref> [20] </ref> and Pitt and Warmuth [34]. 37 9 Open Problems A technical open problem is to improve the constructions given here to prove representation- independent hardness results for even simpler classes of formulae and circuits.
Reference: [21] <author> E.M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37, </volume> <year> 1978, </year> <pages> pp. 302-320. </pages>
Reference-contexts: Gold <ref> [21] </ref> gave the first representation-based hardness results that apply to the distribution- free model of learning.
Reference: [22] <author> O. Goldreich, S. Goldwasser, S. Micali. </author> <title> How to construct random functions. </title> <journal> Journal of the A.C.M., </journal> <volume> 33(4), </volume> <year> 1986, </year> <pages> pp. 792-807. 40 </pages>
Reference-contexts: A similar result holds for Boolean threshold functions [32]. The only previous representation-independent hardness results for distribution-free learning follow from the elegant work of Goldreich, Goldwasser and Micali <ref> [22] </ref> on constructing random functions. <p> Then it is shown by Goldreich et al. <ref> [22] </ref> that if there exists a one-way function, then for some polynomial p (n), CKT p (n) is not learnable in polynomial time (by any polynomial-time evaluatable representation class). Pitt and Warmuth [33] then used this result to construct other hard-to-learn representation classes. <p> Pitt and Warmuth [33] then used this result to construct other hard-to-learn representation classes. For definitions and a discussion of one-way functions we refer the reader to Yao [41], Blum and Micali [13], Levin [29], and Goldreich et al. <ref> [22] </ref>. Note that in any reasonable model of learning, we intuitively do not expect to find polynomial- time learning algorithms for classes of representations that are not polynomial-time evaluatable, since a learning algorithm may not even have enough time to write down a good hypothesis. <p> Representation-based hardness results for learning various classes of neural networks can also be derived from the results of Judd [25] and Blum and Rivest [12]. The first representation-independent hardness results for the distribution-free model follow from the work of Goldreich, Goldwasser and Micali <ref> [22] </ref>, whose true motivation was to find easy-to- compute functions whose output on random inputs appears random to all polynomial-time algorithms. <p> One of the main contributions of the research presented here is representation-independent hardness results for much simpler classes than those addressed by Gol- dreich et al. <ref> [22] </ref> or Pitt and Warmuth [33], among them the classes of Boolean formulae, acyclic deterministic finite automata and constant-depth threshold circuits. 4 Background and Definitions from Cryptography Some basic number theory. <p> Using the results of Goldreich et al. <ref> [22] </ref>, it is also possible to show similar hardness results for the Boolean circuit consistency problem Con (CKT; CKT) using the weaker assumption that there exists a one-way function. It is interesting to contrast Theorem 10 with similar results obtained by Pitt and Warmuth [34].
Reference: [23] <author> T. Hancock. </author> <title> On the difficulty of finding small consistent decision trees. </title> <institution> Harvard University, </institution> <type> unpublished manuscript, </type> <year> 1989. </year>
Reference-contexts: This possibility is addressed and dismissed (modulo cryptographic assumptions) by the results in of this paper. Hancock <ref> [23] </ref> has shown that learning decision trees of size n by decision trees of size n cannot be done in polynomial time unless RP = NP .
Reference: [24] <author> D. Haussler, M. Kearns, N. Littlestone, M. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 42-55, </pages> <institution> and University of California at Santa Cruz Information Sciences Department, </institution> <type> technical report number UCSC-CRL-88-06, </type> <year> 1988. </year>
Reference-contexts: These models, however, are equivalent with respect to polynomial-time computation, in the sense that any class learnable in polynomial time in one model is learnable in polynomial time in the other model, as shown by Haussler et al. <ref> [24] </ref>. Given a fixed target representation c 2 C, and given fixed target distributions D + c and D there is a natural measure of the error (with respect to c, D + c and D c ) of a representation h from a representation class H. <p> We assume that A is given the values of n and jcj as input; the latter assumption is without loss of generality <ref> [24] </ref>. We will drop the phrase "from examples" and simply say that C is learnable by H, and C is polynomially learnable by H. We say C is polynomially learnable to mean that C is polynomially learnable by H for some polynomially evaluatable H. <p> Gold [21] gave the first representation-based hardness results that apply to the distribution- free model of learning. He proves that the problem of finding the smallest deterministic finite automaton consistent with a given sample is NP-complete; the results of Haussler et al. <ref> [24] </ref> can be easily applied to Gold's result to prove that learning deterministic finite automata of size n by deterministic finite automata of size n cannot be accomplished in polynomial time unless RP = NP .
Reference: [25] <author> S. Judd. </author> <title> Learning in neural networks. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 2-8. </pages>
Reference-contexts: Hancock [23] has shown that learning decision trees of size n by decision trees of size n cannot be done in polynomial time unless RP = NP . Representation-based hardness results for learning various classes of neural networks can also be derived from the results of Judd <ref> [25] </ref> and Blum and Rivest [12]. The first representation-independent hardness results for the distribution-free model follow from the work of Goldreich, Goldwasser and Micali [22], whose true motivation was to find easy-to- compute functions whose output on random inputs appears random to all polynomial-time algorithms. <p> Theorem 11 states that even if we allow a much larger net than is actually required, finding these weights is computationally intractable, even for only a constant number of "hidden layers". This result should be contrasted with those of Judd <ref> [25] </ref> and Blum and Rivest [12], which rely on the weaker assumption P 6= NP but do not prove hardness for relaxed consistency and do not allow the hypothesis network to be substantially larger than the smallest consistent network.
Reference: [26] <author> M. Kearns, M. Li, L. Pitt, L.G. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> Proceedings of the 19th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 285-295. </pages>
Reference-contexts: Then for some polynomial p (n), the problems of inverting the RSA encryption function, recognizing quadratic residues and factoring Blum integers are probabilistic polynomial-time reducible to weakly learning BF p (n) . In fact, we can apply the substitution arguments of Kearns et al. <ref> [26] </ref> to show that Theorem 4 holds even for the class of monotone Boolean formulae in which each variable appears at most once. Pitt and Warmuth [33] show that if the class ADFA is polynomially weakly learnable, then the class BF is polynomially weakly learnable.
Reference: [27] <author> M. Kearns, L. Pitt. </author> <title> A polynomial-time algorithm for learning k-variable pattern languages from examples. </title> <booktitle> Proceedings of the 1989 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989, </year> <pages> pp. 57-71. </pages>
Reference: [28] <author> E. Kranakis. </author> <title> Primality and cryptography. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: For an introduction to number theory that is relevant to cryptography, we refer the reader to the work of Angluin [5] and Kranakis <ref> [28] </ref>. For N a natural number, Z N will denote the ring of integers modulo N , and Z fl N will denote the multiplicative group modulo N . <p> Phase III. Using Chinese remaindering, combine u; p u; v and q v to compute the four square roots of a mod N (see e.g. Kranakis <ref> [28] </ref>). Given p and q, this requires only a constant number of multiplication and addition steps, and so is computed in NC 1 . Phase IV.
Reference: [29] <author> L. Levin. </author> <title> One-way functions and pseudorandom generators. </title> <booktitle> Proceedings of the 17th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1985, </year> <pages> pp. 363-365. </pages>
Reference-contexts: Pitt and Warmuth [33] then used this result to construct other hard-to-learn representation classes. For definitions and a discussion of one-way functions we refer the reader to Yao [41], Blum and Micali [13], Levin <ref> [29] </ref>, and Goldreich et al. [22]. Note that in any reasonable model of learning, we intuitively do not expect to find polynomial- time learning algorithms for classes of representations that are not polynomial-time evaluatable, since a learning algorithm may not even have enough time to write down a good hypothesis.
Reference: [30] <author> M. Li, U. Vazirani. </author> <title> On the learnability of finite automata. </title> <booktitle> 41 Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 359-370. </pages>
Reference-contexts: There are some technical issues involved in properly defining the problem of learning finite automata in the distribution-free model; see Pitt and Warmuth [33] for details. Gold's results were improved by Li and Vazirani <ref> [30] </ref>, who show that finding an automaton 9=8 larger than the smallest consistent automaton is still NP -complete.
Reference: [31] <author> N. Linial, Y. Mansour, N. Nisan. </author> <title> Constant depth circuits, Fourier transform and learnability. </title> <booktitle> Proceedings of the 30th I.E.E.E. Symposium on the Foundations of Computer Science, </booktitle> <year> 1989, </year> <pages> pp. 574-579. </pages>
Reference: [32] <author> L. Pitt, L.G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the A.C.M., </journal> <volume> 35(4), </volume> <year> 1988, </year> <pages> pp. 965-984. </pages>
Reference-contexts: restricted to represent its hypothesis in 2-term DNF form, but there is 1 The distribution-free model of learning that we use and will define shortly is often also referred to as the probably approximately correct or PAC model of learning. 2 a polynomial-time learning algorithm if we relax this restriction <ref> [32] </ref>. A similar result holds for Boolean threshold functions [32]. The only previous representation-independent hardness results for distribution-free learning follow from the elegant work of Goldreich, Goldwasser and Micali [22] on constructing random functions. <p> but there is 1 The distribution-free model of learning that we use and will define shortly is often also referred to as the probably approximately correct or PAC model of learning. 2 a polynomial-time learning algorithm if we relax this restriction <ref> [32] </ref>. A similar result holds for Boolean threshold functions [32]. The only previous representation-independent hardness results for distribution-free learning follow from the elegant work of Goldreich, Goldwasser and Micali [22] on constructing random functions. <p> For each of these algorithms, the 12 hypothesis class is the same as the target class; that is, in each case C is polynomially learnable by C. Pitt and Valiant <ref> [32] </ref> subsequently observed that the classes k-term-DNF and k-clause-CNF, when viewed as functions, are properly contained within the classes kCNF and kDNF, respectively. <p> Gold's results were improved by Li and Vazirani [30], who show that finding an automaton 9=8 larger than the smallest consistent automaton is still NP -complete. As we have already discussed, Pitt and Valiant <ref> [32] </ref> prove that for k 2, learning k-term-DNF by k-term-DNF is NP-hard by giving a randomized reduction from a generalization of the graph coloring problem.
Reference: [33] <author> L. Pitt, </author> <title> M.K. Warmuth. Reductions among prediction problems: on the difficulty of predicting automata. </title> <booktitle> Proceedings of the 3rd I.E.E.E. Conference on Structure in Complexity Theory, </booktitle> <year> 1988, </year> <pages> pp. 60-69. </pages>
Reference-contexts: Then it is shown by Goldreich et al. [22] that if there exists a one-way function, then for some polynomial p (n), CKT p (n) is not learnable in polynomial time (by any polynomial-time evaluatable representation class). Pitt and Warmuth <ref> [33] </ref> then used this result to construct other hard-to-learn representation classes. For definitions and a discussion of one-way functions we refer the reader to Yao [41], Blum and Micali [13], Levin [29], and Goldreich et al. [22]. <p> There are some technical issues involved in properly defining the problem of learning finite automata in the distribution-free model; see Pitt and Warmuth <ref> [33] </ref> for details. Gold's results were improved by Li and Vazirani [30], who show that finding an automaton 9=8 larger than the smallest consistent automaton is still NP -complete. <p> A simplified and weakened statement of their result is that the class of polynomial-size Boolean circuits is not polynomially learnable by any polynomially evaluatable H, provided that there exists a one-way function (see Yao [41]). Pitt and Warmuth <ref> [33] </ref> defined a general notion of reducibility for learning and gave a number of other representation classes that are not polynomially learnable under the same assumption by giving reductions from the learning problem for polynomial-size circuits. <p> One of the main contributions of the research presented here is representation-independent hardness results for much simpler classes than those addressed by Gol- dreich et al. [22] or Pitt and Warmuth <ref> [33] </ref>, among them the classes of Boolean formulae, acyclic deterministic finite automata and constant-depth threshold circuits. 4 Background and Definitions from Cryptography Some basic number theory. For an introduction to number theory that is relevant to cryptography, we refer the reader to the work of Angluin [5] and Kranakis [28]. <p> In fact, we can apply the substitution arguments of Kearns et al. [26] to show that Theorem 4 holds even for the class of monotone Boolean formulae in which each variable appears at most once. Pitt and Warmuth <ref> [33] </ref> show that if the class ADFA is polynomially weakly learnable, then the class BF is polynomially weakly learnable. <p> In general, any representation class whose computational power subsumes that of NC 1 is not weakly learnable; however, more subtle reductions are also possible. In particular, our results resolve a problem posed by Pitt and Warmuth <ref> [33] </ref> by showing that under cryptographic assumptions, the class of all languages accepted by logspace Turing machines is not weakly learnable. Pitt and Warmuth [33] introduce a general notion of reduction between learning problems, and a number of learning problems are shown to have equivalent computational difficulty (with respect to probabilistic <p> In particular, our results resolve a problem posed by Pitt and Warmuth <ref> [33] </ref> by showing that under cryptographic assumptions, the class of all languages accepted by logspace Turing machines is not weakly learnable. Pitt and Warmuth [33] introduce a general notion of reduction between learning problems, and a number of learning problems are shown to have equivalent computational difficulty (with respect to probabilistic polynomial-time reducibility); thus, if the learning problem for a representation class C 1 reduces to the learning problem for a representation class C 2 <p> Learning problems are then classified according to the complexity of their evaluation problem, the problem of evaluating a representation on an input example. In Pitt and Warmuth <ref> [33] </ref> the evaluation problem is treated as a uniform problem (i.e., one algorithm for evaluating all representations in the class); by treating the evaluation problem nonuniformly (e.g., a separate circuit for 27 each representation) we were able to show that NC 1 contains a number of presumably hard-to-learn classes of Boolean
Reference: [34] <author> L. Pitt, </author> <title> M.K. Warmuth. The minimum consistent DFA problem cannot be approximated within any polynomial. </title> <booktitle> Proceedings of the 21st A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1989, </year> <pages> pp. 421-432. </pages>
Reference-contexts: Pitt and Warmuth <ref> [34] </ref> dramatically improved the results of Gold by proving that deterministic finite automata of size n cannot be learned in polynomial time by deterministic finite automata of 14 size n ff for any fixed value ff 1 unless RP = NP. <p> It is interesting to contrast Theorem 10 with similar results obtained by Pitt and Warmuth <ref> [34] </ref>. They also prove hardness results for the problem of finding small deterministic finite automata consistent with a labeled sample, but based on the weaker assumption - 6= NP. <p> The NP-hardness results follow from Garey and Johnson [20] and Pitt and Warmuth <ref> [34] </ref>. 37 9 Open Problems A technical open problem is to improve the constructions given here to prove representation- independent hardness results for even simpler classes of formulae and circuits.
Reference: [35] <author> M.O. Rabin. </author> <title> Digital signatures and public key functions as intractable as factoring. </title> <institution> M.I.T. Laboratory for Computer Science, </institution> <type> technical report number TM-212, </type> <year> 1979. </year>
Reference-contexts: The probability is taken over x chosen uniformly from Z N and any coin tosses of A. The Rabin and modified Rabin encryption functions. The Rabin encryption function <ref> [35] </ref> is specified by two primes p and q of length l. <p> Known results regarding the security of the Rabin function include the following: Theorem 2 (Rabin <ref> [35] </ref>) Let x and N be as above. <p> This implies that we can factor Blum integers by the results of Rabin <ref> [35] </ref> and Alexi et al. [4] given in Theorems 2 and 3.
Reference: [36] <author> J. Reif. </author> <title> On threshold circuits and polynomial computations. </title> <booktitle> Proceedings of the 2nd Structure in Complexity Theory Conference, </booktitle> <year> 1987, </year> <pages> pp. 118-125. </pages>
Reference-contexts: Then for some polynomial p (n), the problems of inverting the RSA encryption function, recognizing quadratic residues and factoring Blum integers are probabilistic polynomial-time reducible to weakly learning ADFA p (n) . 26 Using results of Chandra, Stockmeyer and Vishkin [16], Beame et al. [10] and Reif <ref> [36] </ref>, it can be shown that the representations described in Section 5 can each be computed by a polynomial-size, constant-depth threshold circuit.
Reference: [37] <author> R. Rivest, A. Shamir, L. Adleman. </author> <title> A method for obtaining digital signatures and public key cryptosystems. </title> <journal> Communications of the A.C.M., </journal> <volume> 21(2), </volume> <year> 1978, </year> <pages> pp. 120-126. </pages>
Reference-contexts: The existence of such a d is guaranteed for all elements e for which gcd (e; '(N )) = 1. The RSA encryption function <ref> [37] </ref> is then defined for all x 2 ZN by RSA (x; N; e) = x e mod N: Note that decryption can be accomplished by exponentiation mod N : (x e ) d = x ed mod N = x 1+i'(N) mod N = x mod N for some natural <p> Thus, decryption from only the public key and the ciphertext is hard. (v) There is a polynomial-time algorithm D that on input k; k 0 and f (k; x) outputs x. Thus, decryption given the private key (or trapdoor) is easy. As an example, consider the RSA cryptosystem <ref> [37] </ref>. Here the distribution P n is uniform over all (k; k 0 ) where k 0 = (p; q) for n-bit primes p and q and k = (p q; e) with e 2 Z fl '(pq) .
Reference: [38] <author> R. Schapire. </author> <title> On the strength of weak learnability. </title> <booktitle> 42 Proceedings of the 30th I.E.E.E. Symposium on the Foundations of Computer Science, </booktitle> <year> 1989, </year> <pages> pp. 28-33. </pages>
Reference-contexts: Note that in any reasonable model of learning, we intuitively do not expect to find polynomial- time learning algorithms for classes of representations that are not polynomial-time evaluatable, since a learning algorithm may not even have enough time to write down a good hypothesis. More formally, Schapire <ref> [38] </ref> has shown that any representation class that is not evaluatable in polynomial time cannot be learned in polynomial time in the distribution-free model. <p> All representation classes considered here are polynomi- ally evaluatable. It is worth mentioning at this point that Schapire <ref> [38] </ref> has shown that if a representation class is not polynomially evaluatable, then it is not efficiently learnable in our model. Thus, perhaps not surprisingly we see that classes that are not polynomially evaluatable are not only "unfair" as learning problems but also intractable. Samples.
Reference: [39] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the A.C.M., </journal> <volume> 27(11), </volume> <year> 1984, </year> <pages> pp. 1134-1142. </pages>
Reference-contexts: The outline of the paper is as follows: in Section 2, we provide definitions for the distribution-free model of learning, adapted from Valiant <ref> [39] </ref>. Then in Section 3, we discuss previous hardness results for learning, both of the representation-based and representation-independent type. Section 4 gives the needed definitions and background from cryptography. <p> 2 ; x 3 )] denotes the probability of event E when x 1 is drawn from distribution P 1 , x 2 from P 2 , and x 3 from P 3 (all draws being independent). 3 Previous Hardness Results for Learning The initial paper defining the distribution-free model <ref> [39] </ref> gave the first polynomial-time learning algorithms in this model. It showed that the class of monomials is polynomially learnable, as are the classes kCNF and kDNF (with time complexity O (n k )). <p> Pitt and Valiant [32] subsequently observed that the classes k-term-DNF and k-clause-CNF, when viewed as functions, are properly contained within the classes kCNF and kDNF, respectively. Combined with the results above <ref> [39] </ref>, this shows that for fixed k, the class k-term-DNF is poly- nomially learnable by kCNF, and the class k-clause-CNF is polynomially learnable by kDNF. More surprisingly, Pitt and Valiant prove that for any fixed k 2, learning k-term-DNF by k-term-DNF and learning k-clause-CNF by k-clause-CNF are NP-hard problems.
Reference: [40] <author> A. Wigderson. </author> <title> A new approximate graph coloring algorithm. </title> <booktitle> Proceedings of the 14th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1982, </year> <pages> pp. 325-329. </pages>
Reference-contexts: Several well-studied problems apparently have this property, but little has been proven in this direction. Perhaps the best example is graph coloring, where the best polynomial-time algorithms require approximately n 11=(k1) colors on k-colorable n-vertex graphs (see Wigderson <ref> [40] </ref> and Blum [11]) but coloring has been proven NP-hard only for (2*)k colors for any * &gt; 0 (see Garey and Johnson [20]).
Reference: [41] <author> A.C. Yao. </author> <title> Theory and application of trapdoor functions. </title> <booktitle> Proceedings of the 23rd I.E.E.E. Symposium on the Foundations of Computer Science, </booktitle> <year> 1982, </year> <pages> pp. </pages> <month> 80-91. </month> <title> Difficulty of coloring F using A = 1 A = jF j 1=29 A = jF j 0:499::: A = jF j </title>
Reference-contexts: Pitt and Warmuth [33] then used this result to construct other hard-to-learn representation classes. For definitions and a discussion of one-way functions we refer the reader to Yao <ref> [41] </ref>, Blum and Micali [13], Levin [29], and Goldreich et al. [22]. <p> A simplified and weakened statement of their result is that the class of polynomial-size Boolean circuits is not polynomially learnable by any polynomially evaluatable H, provided that there exists a one-way function (see Yao <ref> [41] </ref>). Pitt and Warmuth [33] defined a general notion of reducibility for learning and gave a number of other representation classes that are not polynomially learnable under the same assumption by giving reductions from the learning problem for polynomial-size circuits. <p> a family of functions each of whose members f is easy to compute (that is, given x, it is easy to compute f (x)), hard to invert (that is, given only f (x), it is difficult to compute x), but easy to invert given a secret "key" to the function <ref> [41] </ref> (the trapdoor). We then constructed a learning problem in which the complexity of inverting the function given the trapdoor key corresponds to the complexity of the representations being learned, and learning from random examples corresponds to inverting the function without the trapdoor key. <p> In this section we formalize these ideas and prove a theorem demonstrating that this is indeed a viable approach. We use the following definition for a family of trapdoor functions, which can be derived from Yao <ref> [41] </ref>: let P = fP n g be a family of probability distributions, where for n 1 the distribution P n is over pairs (k; k 0 ) 2 f0; 1g n fi f0; 1g n .
References-found: 41

