URL: http://euler.mcs.utulsa.edu/~sandip/help.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/DAI.html
Root-URL: 
Email: Email: mahend@euler.mcs.utulsa.edu, sandip@kolkata.mcs.utulsa.edu  
Phone: Telephone: (918) 631-2985 Fax: (918) 631-3077  
Title: To help or not to help  
Author: Mahendra Sekaran Sandip Sen 
Address: Tulsa  
Affiliation: Department of Mathematical Computer Sciences University of  
Abstract: Any designer of intelligent agents in a multiagent system is faced with the choice of encoding a strategy of interaction with other agents. If the nature of other agents are known in advance, a suitable strategy may be chosen from the continuum between completely selfish behavior on one extreme and a philanthropic behavior on the other. In an open and dynamic system, however, it is unrealistic to assume that the nature of all other agents, possibly designed and used by users with very different goals and motivations, are known precisely. In the presence of this uncertainty, is it possible to build agents that adapt their behavior to interact appropriately with the particular group of agents in the current scenario? We address this question by borrowing on the simple yet powerful concept of reciprocal behavior. We propose a stochastic decision making scheme which promotes reciprocity among agents. Using a package delivery problem we show that reciprocal behavior can lead to system-wide cooperation, and hence close to optimal global performance can be achieved even though each individual agent chooses actions to benefit itself. More interestingly, we show that agents who do not help others perform worse in the long run when compared with reciprocal agents. Thus it is to the best interest of every individual agent to help other agents. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bond, A.H. & Gasser, L. </author> <year> (1988). </year> <booktitle> Readings in Distributed AI, </booktitle> <address> San Mateo, </address> <publisher> CA:Morgan Kaufman Publishers. </publisher>
Reference-contexts: Our results show that close to optimal system performance can be obtained without sacrificing individual preferences or autonomy. Coordination of multiple agents Multiagent systems are a particular type of distributed artificial intelligence (DAI) system <ref> (Bond, 1988) </ref>, in which autonomous intelligent agents in habit a world with no global control or globally con-sistent knowledge. In contrast to cooperative problem solvers (Durfee,Lesser, Corkill, 1989), agents in multiagent systems are not pre-disposed to help each other out with the resources and capabilities that they possess.
Reference: <author> Durfee, E.H. </author> <year> (1988). </year> <title> Coordination of Distributed Problem Solvers.K;uwer Academic Publishers. </title>
Reference: <author> Durfee, E.H., Lesser, V.R., & Corkill, D.D. </author> <year> (1989). </year> <title> Trends in cooperative distributed problem solving. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(1) </volume> <pages> 63-83. </pages>
Reference-contexts: Coordination of multiple agents Multiagent systems are a particular type of distributed artificial intelligence (DAI) system (Bond, 1988), in which autonomous intelligent agents in habit a world with no global control or globally con-sistent knowledge. In contrast to cooperative problem solvers <ref> (Durfee,Lesser, Corkill, 1989) </ref>, agents in multiagent systems are not pre-disposed to help each other out with the resources and capabilities that they possess. These agents may still need to coordinate their activities with others to achieve their own local goals.
Reference: <author> Fox, M.S. </author> <year> (1981). </year> <title> An organizational view of distributed systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 70-80. </pages>
Reference: <author> Gasser, </author> <title> L (1991. Social conceptions of knowledge and action: DAI foundations and open systems semantics. </title> <journal> Artificial Intelligence, 47(1-3):107-138. </journal>
Reference: <author> Genesereth, M., Ginsberg, M., & Rosenschein, J. </author> <year> (1986). </year> <title> Cooperation without communications. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 51-57, </pages> <address> Philadelphia, Penn-sylvania. </address>
Reference: <author> Gmytrasiewicz91, P.J., Durfee, E.H., & Wehe, D.K. </author> <year> (1991). </year> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68. </pages>
Reference: <author> Goldman, C. & Rosenschein, J.S. </author> <year> (1994). </year> <title> Emergent coordination through the use of cooperative state-changing rules. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 408-413. </pages>
Reference-contexts: Similar analyses of the effects of fi and t can be made for any cooperation decision after agents have experienced a number of exchanges. In essence, fi and t can be used to choose a cooperation level <ref> (Goldman, 1994) </ref> for the agents at the onset of the experiments. The level of cooperation or the inclination to help another agent, however, dynamically changes with problem solving experience.
Reference: <author> Hewitt, C. </author> <year> (1991). </year> <title> Open information systems semantics for distributed artificial intelligence. </title> <journal> Artificial Intelligence, 47(1-3):79-106. </journal>
Reference: <author> Rapoport, A. </author> <year> (1989). </year> <title> Prisoner's dilemma. </title> <editor> In J. Eatwell, M. Milgate, and P. Newman, editors, </editor> <booktitle> The New Palgrave: Game Theory, </booktitle> <pages> pages 199-204. </pages> <publisher> Macmillan, London. </publisher>
Reference-contexts: The other dimension to consider is if we are analyzing a single instance of agent interaction or if we are considering an ensemble of agent interactions (e.g., in the prisoner's dilemma problem <ref> (Rapoport, 1989) </ref>, most of the formal analysis assume repeated interactions). In this paper, we assume agents have individual goals or tasks to complete. These individual goals, however, can be achieved more expediently if an agent receives assistance from other agents.
Reference: <author> Sekaran, M. & Sen, S. </author> <year> (1994). </year> <booktitle> Learning with friends and foes.In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 800-805. </pages>
Reference: <author> Sen, S. & Sekaran, M. </author> <year> (1995). </year> <title> Using reciprocity to adapt to others. </title> <booktitle> International Joint conference on Artificial Intelligence workshop on Adaptation and Learning in Multiagent Sytems. </booktitle>
Reference-contexts: The question here is the following: given that there are scopes for cooperation, how should self-motivated agents choose when to cooperate and when not to cooperate with another agent? In the following section we provide an on-line mechanism to answer this question. Reciprocal decision making In a companion paper <ref> (Sen, 1995) </ref> we have shown that reciprocal behavior can be used effectively by agents to balance their workloads. In that paper, each task could be carried out by any agent, and agents could exchange tasks to improve local performance.
Reference: <author> Smith, R.G. </author> <title> (1980).The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, C-29(12):1104-1113. </journal>
References-found: 13

