URL: http://www-lsi.upc.es/~talavera/papers/ecml98.ps.gz
Refering-URL: http://www-lsi.upc.es/~talavera/papers.html
Root-URL: 
Email: talavera@lsi.upc.es  roure@eupmt.upc.es  
Phone: 2  
Title: A buffering strategy to avoid ordering effects in clustering strategy may help the system to
Author: Luis Talavera and Josep Roure 
Note: Not-Yet  
Address: Campus Nord, Modul C6, Jordi Girona 1-3 08034 Barcelona, Catalonia, Spain  Avda. Puig i Cadafalch 101-111 08303 Mataro, Catalonia, Spain  
Affiliation: 1 Universitat Politecnica de Catalunya Departament de Llenguatges i Sistemes Informatics  Escola Universitaria Politecnica de Mataro Departament d'Informatica de Gestio  
Abstract: It is widely reported in the literature that incremental clustering systems suffer from instance ordering effects and that under some orderings, extremely poor clusterings may be obtained. In this paper we present a new strategy aimed to mitigate these effects, the Not-Yet strategy which has a general and open formulation and it is not coupled to any particular system. Results suggest that the strategy improves the clustering quality and also that performance is limited by its limited foresight. We also show that, when combined with other strategies, the 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. R. Anderson and M. Matessa. </author> <title> Explorations of an incremental, bayesian algorithm for categorization. </title> <journal> Machine Learning, </journal> (9):275-308, 1992. 
Reference-contexts: The measure of category utility used in this system is also used in the experiments as the CQF. We used a COBWEB-like clustering strategy because it is simple, well-known and it has been applied (or augmented) in several learning systems <ref> [1, 7] </ref>. As stated before, we embed the basic control procedure into another one implementing the Not-Yet strategy. The buffer is cleared at the end of the clustering process and, therefore, an unlimited size is assumed.
Reference: 2. <author> J. Bejar. Adquisicion automatica de conocimiento en dominios poco estructurados. </author> <type> PhD thesis, </type> <institution> Facultat d'Informatica de Barcelona, UPC, </institution> <year> 1995. </year>
Reference-contexts: We think that this formulation should help in applying the strategy to any existing algorithm without any major changes. The second related work is the application of this strategy to the LINNEO + clustering system <ref> [2, 10] </ref>. This work contains the basic ideas proposed here, but again the application is tuned for an specific system and the problems studied are deeply related to a particular clustering strategy. Also, we have to mention relevant Fisher's work on iterative optimization of clusterings [4].
Reference: 3. <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> (2):139-172, 1987. 
Reference-contexts: This is a typical model of incremental clustering using a hill climbing strategy which estimates the goodness of applying the available operators and chooses the best option, without reconsider any decision made. Particularly, this model corresponds to the one used in the COBWEB system <ref> [3] </ref>. The measure of category utility used in this system is also used in the experiments as the CQF. We used a COBWEB-like clustering strategy because it is simple, well-known and it has been applied (or augmented) in several learning systems [1, 7].
Reference: 4. <author> D. H. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <journal> Journal of Artificial Intelligence Research, </journal> (4):147-180, 1995. 
Reference-contexts: This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [4, 6, 7, 8, 9] </ref>. Let I be an instance and P be a partition Let E be the expected utility/confidence of adding I to P Let ff be the Not-Yet threshold value if E ff then add (P ,I) else add NY buffer (I) endif Table 1. <p> Clustering results. Averages and standard deviations over 50 trials sponds to the original algorithm without buffering any instance. Although we are using a hierarchical clustering method, the CQF is given only for the top level, which is expected to score highest <ref> [4] </ref> Results demonstrate that instance ordering has a critical effect in cluster quality. The quality of discovered clusterings consistently drops in a 35-40% when bad orderings are used, being far from the optimal values found in the literature [4]. <p> given only for the top level, which is expected to score highest <ref> [4] </ref> Results demonstrate that instance ordering has a critical effect in cluster quality. The quality of discovered clusterings consistently drops in a 35-40% when bad orderings are used, being far from the optimal values found in the literature [4]. The Not-Yet strategy modestly improves results in the random case, but note that the good performance of the original clustering procedure on random orderings lets little room for improvement. With bad orderings, the strategy improves the poor scores obtained with the basic algorithm up to a 20%. <p> This work contains the basic ideas proposed here, but again the application is tuned for an specific system and the problems studied are deeply related to a particular clustering strategy. Also, we have to mention relevant Fisher's work on iterative optimization of clusterings <ref> [4] </ref>. This work explores several methods for iteratively improving clustering quality, showing that among these methods some exhibit an optimum performance.
Reference: 5. <author> D. H. Fisher and P. Langley. </author> <title> Conceptual clustering and its relation to numerical taxonomy. </title> <editor> In W. A. Gale, editor, </editor> <booktitle> Artificial Intelligence and Statistics. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading,MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Ideally, intelligent agents should possess the ability of adapting their behavior to the environment over time through learning. Thus, learning methods should be able of updating a knowledge base in a continual basis as new experience is gained. Particularly, if an agent performing a clustering task <ref> [5] </ref> should be able of using its learned knowledge to carry out some performance task at any stage of learning, the conceptual scheme should evolve as every new instance is observed without simultaneously processing previous instances. This sort of clustering is often referred to as incremental clustering.
Reference: 6. <author> D. H. Fisher, L. Xu, and N. Zard. </author> <title> Ordering effects in clustering. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 163-168, </pages> <year> 1992. </year>
Reference-contexts: This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [4, 6, 7, 8, 9] </ref>. Let I be an instance and P be a partition Let E be the expected utility/confidence of adding I to P Let ff be the Not-Yet threshold value if E ff then add (P ,I) else add NY buffer (I) endif Table 1.
Reference: 7. <author> J. H. Gennari, P. Langley, and D. Fisher. </author> <title> Models of incremental concept formation. </title> <journal> Artificial Intelligence, </journal> (40):11-61, 1989. 
Reference-contexts: This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [4, 6, 7, 8, 9] </ref>. Let I be an instance and P be a partition Let E be the expected utility/confidence of adding I to P Let ff be the Not-Yet threshold value if E ff then add (P ,I) else add NY buffer (I) endif Table 1. <p> The measure of category utility used in this system is also used in the experiments as the CQF. We used a COBWEB-like clustering strategy because it is simple, well-known and it has been applied (or augmented) in several learning systems <ref> [1, 7] </ref>. As stated before, we embed the basic control procedure into another one implementing the Not-Yet strategy. The buffer is cleared at the end of the clustering process and, therefore, an unlimited size is assumed.
Reference: 8. <author> P. Langley. </author> <title> Order effects in incremental learning. </title> <editor> In P. Reimann and H. Spada, editors, </editor> <title> Learning in humans and machines: Towards an Interdisciplinary Learning Science. </title> <publisher> Pergamon, </publisher> <year> 1995. </year>
Reference-contexts: This sort of clustering is often referred to as incremental clustering. As noted by Langley <ref> [8] </ref>, there can be several interpretations of incremental learning. In the remainder of this paper, we will assume that a clustering method is incremental if inputs one instance at a time, does not reprocess previous instances and maintains only one conceptual structure in memory. <p> This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [4, 6, 7, 8, 9] </ref>. Let I be an instance and P be a partition Let E be the expected utility/confidence of adding I to P Let ff be the Not-Yet threshold value if E ff then add (P ,I) else add NY buffer (I) endif Table 1.
Reference: 9. <author> M. Lebowitz. </author> <title> Deferred commitment in unimem: waiting to learn. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 80-86, </pages> <year> 1988. </year>
Reference-contexts: This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [4, 6, 7, 8, 9] </ref>. Let I be an instance and P be a partition Let E be the expected utility/confidence of adding I to P Let ff be the Not-Yet threshold value if E ff then add (P ,I) else add NY buffer (I) endif Table 1. <p> Lebowitz first introduced the idea of deferred commitment within the framework of his UNIMEM conceptual clustering system <ref> [9] </ref>. Our proposal extends Lebowitz's work by decoupling the buffering strategy from any particular system. Also, we have introduced the ff parameter, that allows to see the original algorithm as a particular case of the new control strategy.
Reference: 10. <author> J. Roure. </author> <title> Study of methods and heuristics to improve the fuzzy classifications of LINNEO + . Master's thesis, </title> <institution> Facultat d'Informatica de Barcelona, UPC, </institution> <year> 1994. </year>
Reference-contexts: We think that this formulation should help in applying the strategy to any existing algorithm without any major changes. The second related work is the application of this strategy to the LINNEO + clustering system <ref> [2, 10] </ref>. This work contains the basic ideas proposed here, but again the application is tuned for an specific system and the problems studied are deeply related to a particular clustering strategy. Also, we have to mention relevant Fisher's work on iterative optimization of clusterings [4].
References-found: 10

