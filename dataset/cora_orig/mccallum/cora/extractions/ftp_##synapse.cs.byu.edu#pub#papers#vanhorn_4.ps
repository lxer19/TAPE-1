URL: ftp://synapse.cs.byu.edu/pub/papers/vanhorn_4.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: hoeffgen,simon@nereus.informatik.uni-dortmund.de  kevin@bert.cs.byu.edu  
Title: Robust Trainability of Single Neurons  
Author: Klaus-U. Hoffgen, Hans-U. Simon Kevin S. Van Horn 
Note: These authors gratefully acknowledge the support of Bundesministerium fur Forschung und Technologie grant 01IN102C/2. The authors take responsibility for the content.  
Date: June 3, 1994  
Address: D-4600 Dortmund 50  Provo, UT 84602  
Affiliation: Lehrstuhl Informatik II Universitat Dortmund  Computer Science Department Brigham Young University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Abe, J. Takeuchi, and M. K. Warmuth, </author> <title> Polynomial learnability of prob abilistic concepts with respect to the Kullback-Leibler divergence, </title> <booktitle> in "Pro-ceedings of the 4th Annual Workshop on Computational Learning Theory, </booktitle> <year> 1991," </year> <pages> pp. 277-289. </pages>
Reference-contexts: It is however not quite clear which additional tools are available for the purposes of neural learning. Our notions of learnability are not new, except for limited degradation and heuristical learning with constant reject rate. Robust learning has already been used by Abe, Takeuchi and Warmuth in <ref> [1, 2] </ref>. Their notion of robust learning includes also stochastic rules (without a deterministic distinction between positive and negative examples) and allows cost measures different from the prediction error. Notice however that negative results are stronger for the more specific situation, whereas positive results are stronger for the general case.
Reference: [2] <author> N. Abe and M. K. Warmuth, </author> <title> On the computational complexity of approxi mating distributions by probabilistic automata, </title> <booktitle> in "Proceedings of the 3rd Annual Workshop on Computational Learning Theory, </booktitle> <year> 1990," </year> <pages> pp. 52-66. </pages>
Reference-contexts: The following result has been already observed in <ref> [2] </ref> (although the authors state it in a technically slightly different setting.) Theorem 2.2 If RP 6= NP and MinDis (H) restricted to C-legal input samples is NP-hard, then C is not PAO learnable by H. Proof. <p> It is however not quite clear which additional tools are available for the purposes of neural learning. Our notions of learnability are not new, except for limited degradation and heuristical learning with constant reject rate. Robust learning has already been used by Abe, Takeuchi and Warmuth in <ref> [1, 2] </ref>. Their notion of robust learning includes also stochastic rules (without a deterministic distinction between positive and negative examples) and allows cost measures different from the prediction error. Notice however that negative results are stronger for the more specific situation, whereas positive results are stronger for the general case. <p> Therefore, we did not introduce the notion of robust learning in full generality. Heuristical learning with constant accept rate has been introduced by Pitt and Valiant in [23]. They also applied the combinatorial method to PAC learning and heuristical learning. In <ref> [2] </ref>, this method is used for showing that probabilistic automata are not robustly trainable. There exist some other nonlearnability results concerning neural nets, which we will mention briefly. Judd proved various nonlearnability results in this context (see [14]).
Reference: [3] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy, </author> <title> Proof veri fication and intractability of approximation problems, </title> <booktitle> in "Proceedings of the 33rd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1992," </year> <pages> pp. 14-23. </pages>
Reference: [4] <author> M. Bellare, S. Goldwasser, C. Lund, & A. Russell, </author> <title> Efficient probabilistically checkable proofs and applications to approximation, </title> <booktitle> in "Proceedings of the 25th Annual ACM Symposium on Theory of Computing," </booktitle> <pages> pp. 294-304. </pages>
Reference-contexts: Question: Is there a hitting set for C of cardinality at most k, i.e., a subset R of T s.t. R " M 6= ; for all M 2 C? Hitting Set is not only NP-complete, it is also difficult to approximate. Bel-lare, Goldwasser, Lund and Russel <ref> [4] </ref> have proven the following: Theorem 4.2 For all d, no PTAA approximates Hitting Set to within a constant factor d, unless P=NP. They actually prove the above for Minimum Set Cover, which is isomorphic to Hitting Set [20, 15].
Reference: [5] <author> A. Blum, </author> <type> personal communication, </type> <year> 1992. </year>
Reference-contexts: We omit the details and refer the reader to standard books on linear programming such as [22]. Theorem 3.1 MinDis (Halfspaces) is NP-complete. We give a proof of this theorem in the next section by proving a strengthened result. A. Blum <ref> [5] </ref> independently suggested another proof of Theorem 3.1 using a polynomial transformation from the NP-complete problem Open Hemisphere (problem MP6 in [12]) to MinDis (Halfspaces).
Reference: [6] <author> A. Blum and R. L. Rivest, </author> <title> Training a 3-node neural network is NP complete, </title> <booktitle> in "Proceedings of the 1988 Workshop on Computational Learning Theory," </booktitle> <pages> pp. 9-18. </pages>
Reference-contexts: His polynomial transformations from NP-hard problems to consistency problems suffer a little bit from using quite artificial architectures which are not likely to be used in any existing learning environment. Nevertheless, his results are among the first rigorously proven negative results concerning neural learning. Blum and Rivest in <ref> [6] </ref> showed nonlearnabilty for quite simple architectures: one hidden layer with k 2 hidden units and one output unit which computes the logical AND. This architecture represents polyhedrons with k facets, i.e., intersections of k halfspaces.
Reference: [7] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <title> Learnabil ity and the Vapnik-Chervonenkis dimension, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 36 (1989), </volume> <pages> 929-965. </pages>
Reference-contexts: A perfect adjustment with x 2 c () f (x) = 1 exists iff the concept is a linear halfspace. If we restrict the class of concepts to halfspaces, we may use linear programming for finding an adjustment which is consistent to the sample. This leads to the well-known <ref> [7] </ref> fact that halfspaces are PAC learnable (and neurons are trainable to PAC-learn them). This positive result is valid for variable dimension n. Throughout the paper, we will demand that our learning algorithms work properly for variable n. <p> In this section, we discuss the hypothesis class Halfspaces of linear halfspaces in R n with variable dimension n. Notice that the consistency problem for Halfspaces is just linear programming, which can be done in polynomial time [19, 16]. It is therefore not surprising that Halfspaces is learnable (see <ref> [7] </ref>). But we state that Halfspaces does not allow robust learning (assumed RP 6= NP). Notice that Halfspaces is the hypothesis class associated with the simplest neural architecture, consisting of a single McCulloch-Pitts neuron only. <p> Follows directly from Corollary 4.10 and Theorem 4.1. 2 The negative results of Corollary 4.11 are rather surprising in light of the fact that there exist polynomial algorithms for learning all of the hypothesis spaces mentioned therein, under the PAC model <ref> [7, 25, 24] </ref>.
Reference: [8] <author> S. A. Cook, </author> <title> The complexity of theorem-proving procedures, </title> <booktitle> in "Proceed ings, 3rd Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1971," </year> <pages> pp. 151-158. </pages>
Reference-contexts: The following result is therefore not surprising. Lemma 5.3 Min SAT is MIN PB-complete under cost-preserving polynomial transformations. Proof. The proof is an adaption of analogous reasonings in [21, 9], which in turn are adaptions of Cook's theorem <ref> [8] </ref>. Given an instance x of a MIN PB problem, we first compute a feasible solution y 0 for x.
Reference: [9] <author> P. Crescenzi and A. Panconesi, </author> <title> Completeness in approximation classes, </title> <journal> Inform. and Comput. </journal> <volume> 93 (1991), </volume> <pages> 241-262. </pages>
Reference-contexts: The following result is therefore not surprising. Lemma 5.3 Min SAT is MIN PB-complete under cost-preserving polynomial transformations. Proof. The proof is an adaption of analogous reasonings in <ref> [21, 9] </ref>, which in turn are adaptions of Cook's theorem [8]. Given an instance x of a MIN PB problem, we first compute a feasible solution y 0 for x.
Reference: [10] <author> Y. L. Cun, J. S. Denker, and S. A. Solla, </author> <title> Optimal brain damage, </title> <booktitle> in "Ad vances in Neural Information Processing Systems 2" (D. </booktitle> <editor> S. Touretzky, </editor> <publisher> Ed.), </publisher> <pages> pp. 598-605, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year> <month> 24 </month>
Reference-contexts: It should expand the architecture when overstraining occurs, or contract it if there is evidence for understraining. This coincides with the empirical observation of many researchers and their heuristical attempts to specify appropriate rules for expansion or contraction (see <ref> [10, 11] </ref>). We may consider our results as complexity-theoretical support for their approaches. It is also interesting to note that the concept class we have used for knocking out a single neuron (hyperplanes) is PAC learnable. Two hidden units in one hidden layer are sufficient to represent hyperplanes.
Reference: [11] <author> S. E. Fahlman AND C. Lebiere, </author> <booktitle> The cascade-correlation learning architec ture, in "Advances in Neural Information Processing Systems 2" (D. </booktitle> <editor> S. Touretzky, </editor> <publisher> Ed.), </publisher> <pages> pp. 524-532, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: It should expand the architecture when overstraining occurs, or contract it if there is evidence for understraining. This coincides with the empirical observation of many researchers and their heuristical attempts to specify appropriate rules for expansion or contraction (see <ref> [10, 11] </ref>). We may consider our results as complexity-theoretical support for their approaches. It is also interesting to note that the concept class we have used for knocking out a single neuron (hyperplanes) is PAC learnable. Two hidden units in one hidden layer are sufficient to represent hyperplanes.
Reference: [12] <author> M. R. Garey and D. S. Johnson, </author> <title> "Computers and Intractability: A Guide to the Theory of NP-Completeness," </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Theorem 3.1 MinDis (Halfspaces) is NP-complete. We give a proof of this theorem in the next section by proving a strengthened result. A. Blum [5] independently suggested another proof of Theorem 3.1 using a polynomial transformation from the NP-complete problem Open Hemisphere (problem MP6 in <ref> [12] </ref>) to MinDis (Halfspaces). While his proof needs multisets in the definition of MinDis (Halfspaces), or alternatively uses vectors with non-Boolean entries, our proof will only use sets of Boolean vectors.
Reference: [13] <author> D. S. Johnson and F. P. Preparata, </author> <title> The densest hemisphere problem, </title> <journal> Theor. Comp. Sci. </journal> <volume> 6 (1978), </volume> <pages> 93-107. </pages>
Reference-contexts: The approxi 23 mation problems are then solvable in polynomial time if we apply the algorithm of Johnson and Preparata for the Densest Hemisphere problem (see <ref> [13] </ref>). As for future work, we suspect that the phenomenon of unlimited degradation occurs also for heuristical learning. The analysis of learning algorithms on dynamical architectures seems to be one of the most striking problems for future research on neural learning.
Reference: [14] <author> J. S. Judd, </author> <title> "Neural Network Design and the Classification of Learning," </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: In [2], this method is used for showing that probabilistic automata are not robustly trainable. There exist some other nonlearnability results concerning neural nets, which we will mention briefly. Judd proved various nonlearnability results in this context (see <ref> [14] </ref>). His polynomial transformations from NP-hard problems to consistency problems suffer a little bit from using quite artificial architectures which are not likely to be used in any existing learning environment. Nevertheless, his results are among the first rigorously proven negative results concerning neural learning.
Reference: [15] <author> V. Kann, </author> <title> "On the Approximability of NP-complete Optimization Prob lems," </title> <type> Ph.D. thesis, </type> <institution> Royal Institute of Technology, Dept. of Numerical Analysis and Computing Science, Stockholm, Sweden, </institution> <year> 1992. </year>
Reference-contexts: Bel-lare, Goldwasser, Lund and Russel [4] have proven the following: Theorem 4.2 For all d, no PTAA approximates Hitting Set to within a constant factor d, unless P=NP. They actually prove the above for Minimum Set Cover, which is isomorphic to Hitting Set <ref> [20, 15] </ref>. We now give the transformation from Hitting Set to MinDis (Halfspaces). 10 Theorem 4.3 Hitting Set cp pol MinDis (Halfspaces). Proof. Let (T 0 ; C 0 ; k) be an instance of Hitting Set. <p> A more formal definition is contained in <ref> [15, 20] </ref>, with the only difference being that they do not require that it be possible to find a feasible solution in polynomial time. We impose this extra requirement in order to restrict ourselves to problems where any computational difficulty is in minimizing, and not in finding a feasible solution.
Reference: [16] <author> N. Karmarkar, </author> <title> A new polynomial time algorithm for linear programming, </title> <booktitle> Combinatorica 4 (1984), </booktitle> <pages> 373-395. </pages>
Reference-contexts: NNA is called trainable if H (NNA) is learnable. In this section, we discuss the hypothesis class Halfspaces of linear halfspaces in R n with variable dimension n. Notice that the consistency problem for Halfspaces is just linear programming, which can be done in polynomial time <ref> [19, 16] </ref>. It is therefore not surprising that Halfspaces is learnable (see [7]). But we state that Halfspaces does not allow robust learning (assumed RP 6= NP). Notice that Halfspaces is the hypothesis class associated with the simplest neural architecture, consisting of a single McCulloch-Pitts neuron only.
Reference: [17] <author> M. J. Kearns, M. Li, L. Pitt, and L. Valiant, </author> <title> On the learnability of boolean formula, </title> <booktitle> in "Proceedings, 19th ACM Symposium on Theory of Computing, </booktitle> <year> 1987," </year> <pages> pp. 285-295. </pages>
Reference-contexts: Input: A multiset S = S + [ S (disjoint union) of positive and negative 5 examples. Question: Does there exist a concept c 2 C consistent with S, i.e., 9c 2 C : c " S = S + ? The following result is shown in <ref> [17, 23] </ref>: Theorem 2.1 If RP 6= NP and the consistency problem for C is NP-hard, then C is not learnable. Similarly, it turns out that the question whether a hypothesis class H allows robust learning is related to the so-called Minimizing Disagreement problem: * MinDis (H).
Reference: [18] <author> M. J. Kearns, R. E. Schapire, and L. M. Sallie, </author> <title> Toward efficient agnostic learning, </title> <booktitle> in "Proceedings of the 5th Annual Workshop on Computational Learning Theory, </booktitle> <year> 1992," </year> <pages> pp. 341-353. </pages>
Reference-contexts: This means we use the class H 0 as a touchstone class to define the goal of the learning problem, but allow a hypotheses class H H 0 , which provides the hypotheses of our learning algorithm. This framework was introduced by Kearns et.al. in <ref> [18] </ref>. It is especially useful to overcome representational problems inherent to a given touchstone class.
Reference: [19] <author> L. G. </author> <title> Khachian, A polynomial algorithm for linear programming, </title> <journal> Soviet Math. </journal> <volume> Doklady 20 (1979), </volume> <pages> 191-194. </pages>
Reference-contexts: NNA is called trainable if H (NNA) is learnable. In this section, we discuss the hypothesis class Halfspaces of linear halfspaces in R n with variable dimension n. Notice that the consistency problem for Halfspaces is just linear programming, which can be done in polynomial time <ref> [19, 16] </ref>. It is therefore not surprising that Halfspaces is learnable (see [7]). But we state that Halfspaces does not allow robust learning (assumed RP 6= NP). Notice that Halfspaces is the hypothesis class associated with the simplest neural architecture, consisting of a single McCulloch-Pitts neuron only.
Reference: [20] <author> P. G. Kolaitis and M. N. Thakur, </author> <title> Approximation properties of NP mini mization classes, </title> <booktitle> in "Proceedings of the 6th Annual Conference on Structures in Complexity Theory, </booktitle> <year> 1991," </year> <pages> pp. 353-366. </pages>
Reference-contexts: Bel-lare, Goldwasser, Lund and Russel [4] have proven the following: Theorem 4.2 For all d, no PTAA approximates Hitting Set to within a constant factor d, unless P=NP. They actually prove the above for Minimum Set Cover, which is isomorphic to Hitting Set <ref> [20, 15] </ref>. We now give the transformation from Hitting Set to MinDis (Halfspaces). 10 Theorem 4.3 Hitting Set cp pol MinDis (Halfspaces). Proof. Let (T 0 ; C 0 ; k) be an instance of Hitting Set. <p> A more formal definition is contained in <ref> [15, 20] </ref>, with the only difference being that they do not require that it be possible to find a feasible solution in polynomial time. We impose this extra requirement in order to restrict ourselves to problems where any computational difficulty is in minimizing, and not in finding a feasible solution.
Reference: [21] <author> P. Orponen and H. Mannila, </author> <title> On approximation preserving reductions: Complete problems and robust measures, </title> <institution> Research Report C-1987-28, University of Helsinki, </institution> <year> 1987. </year>
Reference-contexts: The following result is therefore not surprising. Lemma 5.3 Min SAT is MIN PB-complete under cost-preserving polynomial transformations. Proof. The proof is an adaption of analogous reasonings in <ref> [21, 9] </ref>, which in turn are adaptions of Cook's theorem [8]. Given an instance x of a MIN PB problem, we first compute a feasible solution y 0 for x.
Reference: [22] <author> C. H. Papadimitriou and K. Steiglitz, </author> <title> "Combinatorial Optimization," </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: It is evident that MinDis (Halfspaces) belongs to NP because there is always a polynomial `guess' for the (rational) coefficients of an optimal separating hyperplane. We omit the details and refer the reader to standard books on linear programming such as <ref> [22] </ref>. Theorem 3.1 MinDis (Halfspaces) is NP-complete. We give a proof of this theorem in the next section by proving a strengthened result. A. Blum [5] independently suggested another proof of Theorem 3.1 using a polynomial transformation from the NP-complete problem Open Hemisphere (problem MP6 in [12]) to MinDis (Halfspaces).
Reference: [23] <author> L. Pitt and L. G. Valiant, </author> <title> Computational limitations on learning from examples, </title> <journal> J. Assoc. Comput. Mach. </journal> <volume> 35 (1988), </volume> <pages> 965-984. </pages>
Reference-contexts: Input: A multiset S = S + [ S (disjoint union) of positive and negative 5 examples. Question: Does there exist a concept c 2 C consistent with S, i.e., 9c 2 C : c " S = S + ? The following result is shown in <ref> [17, 23] </ref>: Theorem 2.1 If RP 6= NP and the consistency problem for C is NP-hard, then C is not learnable. Similarly, it turns out that the question whether a hypothesis class H allows robust learning is related to the so-called Minimizing Disagreement problem: * MinDis (H). <p> Such neurons compute Boolean threshold functions; in a sense, they `learn' which variables are essential, and give the same weight to all essential variables. Pitt and Valiant have shown in <ref> [23] </ref> that the consistency problem for Boolean threshold functions is NP-complete. They are therefore not learnable (unless RP=NP). <p> This leads to decisions with very trustworthy `yes' answers, but overcareful `no' answers. A more formal approach to these ideas is based on Valiant's notion of heuris-tical learning (see <ref> [23] </ref>). <p> S = S + [ S (disjoint union of multisets.) Question: Is there a rule h 2 H that is consistent with S (i.e., S h) and covers at least a fraction r of S + (i.e., jS + " hj r jS + j)? The following result is from <ref> [23] </ref>: Theorem 6.1 If RP 6= NP and Rule (C; H; r) is NP-hard, then C is not heuristically learnable by H with accept rate r. One main result of this section is the following. Theorem 6.2 For all rationals 0 &lt; r &lt; 1, Rule (Hyperplanes,Halfspaces,r) is NP-complete. Proof. <p> Notice however that negative results are stronger for the more specific situation, whereas positive results are stronger for the general case. Therefore, we did not introduce the notion of robust learning in full generality. Heuristical learning with constant accept rate has been introduced by Pitt and Valiant in <ref> [23] </ref>. They also applied the combinatorial method to PAC learning and heuristical learning. In [2], this method is used for showing that probabilistic automata are not robustly trainable. There exist some other nonlearnability results concerning neural nets, which we will mention briefly.
Reference: [24] <author> R. L. Rivest, </author> <title> Learning decision lists, </title> <booktitle> Machine Learning 2 (1987), </booktitle> <pages> 229-246. </pages>
Reference-contexts: These are concepts on Boolean vectors. A monomial has the form fx : fl 1 (x 1 ) ^ ^ fl n (x n )g; where each fl i (x) is either "true", "x," or ":x." A 1-DL formula <ref> [24] </ref> is defined by a list of pairs (l 1 ; c 1 ) (l p ; c p )(l p+1 ; c p+1 ), where each c i 2 f0; 1g, l p+1 is "true", and each l i (i p) is a literal (either x j or :x j <p> Follows directly from Corollary 4.10 and Theorem 4.1. 2 The negative results of Corollary 4.11 are rather surprising in light of the fact that there exist polynomial algorithms for learning all of the hypothesis spaces mentioned therein, under the PAC model <ref> [7, 25, 24] </ref>.
Reference: [25] <author> L. G. Valiant, </author> <title> A theory of the learnable, </title> <booktitle> Communications of the ACM 27 (1984), </booktitle> <pages> 1134-1142. </pages>
Reference-contexts: Follows directly from Corollary 4.10 and Theorem 4.1. 2 The negative results of Corollary 4.11 are rather surprising in light of the fact that there exist polynomial algorithms for learning all of the hypothesis spaces mentioned therein, under the PAC model <ref> [7, 25, 24] </ref>.
Reference: [26] <author> K. S. Van Horn, </author> <title> "Learning as Optimization," </title> <type> Ph.D. dissertation, </type> <institution> Computer Science Department, Brigham Young University, </institution> <year> 1994. </year> <month> 26 </month>
Reference-contexts: One complication is that we must express d as a ratio of positive integers d 1 =d 2 , and define * to be 1=(d 2 (jSj + 1)). Details are given in <ref> [26] </ref>. 2 The main result of this section is to demonstrate limits on the approxima-bility of MinDis (Halfspaces) and variants.
References-found: 26

