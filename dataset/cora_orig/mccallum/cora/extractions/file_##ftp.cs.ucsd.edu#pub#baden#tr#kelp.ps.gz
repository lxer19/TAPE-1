URL: file://ftp.cs.ucsd.edu/pub/baden/tr/kelp.ps.gz
Refering-URL: http://www.cs.ucsd.edu/~sfink/pub.html
Root-URL: http://www.cs.ucsd.edu
Phone: 2  
Title: To Appear in IRREGULAR `96 Flexible Communication Mechanisms for Dynamic Structured Applications  
Author: Stephen J. Fink Scott B. Baden and Scott R.Kohn 
Address: Diego, La Jolla, CA 92093-0114  La Jolla, CA 92093-0340  
Affiliation: 1 Department of Computer Science and Engineering, University of California, San  Department of Chemistry and Biochemistry, University of California, San Diego,  
Abstract: Irregular scientific applications are often difficult to paral-lelize due to elaborate dynamic data structures with complicated communication patterns. We describe flexible data orchestration abstractions that enable the programmer to express customized communication patterns arising in an important class of irregular computations|adaptive finite difference methods for partial differential equations. These abstractions are supported by KeLP, a C++ run-time library. KeLP enables the programmer to manage spatial data dependence patterns and express data motion handlers as first-class mutable objects. Using two finite difference applications, we show that KeLP's flexible communication model effectively manages elaborate data motion arising in semi-structured adaptive methods.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> High Performance Fortran Forum, </author> <title> "High performance fortran language specification," </title> <institution> Rice Univeristy, Houston, Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Since these structures give rise to unpredictable communication patterns, parallelization is difficult. To ease the programmer's burden, programming languages and libraries can hide many low-level details of a parallel implementation <ref> [1, 2, 3, 4, 5, 6] </ref>. We present Kernel Lattice Parallelism (KeLP), a C++ class library that provides high-level abstractions to manage data layout and data motion for dynamic irregular block-structured applications 1 .
Reference: 2. <author> S. R. Kohn, </author> <title> A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations. </title> <type> PhD thesis, </type> <institution> University of California at San Diego, </institution> <year> 1995. </year>
Reference-contexts: Since these structures give rise to unpredictable communication patterns, parallelization is difficult. To ease the programmer's burden, programming languages and libraries can hide many low-level details of a parallel implementation <ref> [1, 2, 3, 4, 5, 6] </ref>. We present Kernel Lattice Parallelism (KeLP), a C++ class library that provides high-level abstractions to manage data layout and data motion for dynamic irregular block-structured applications 1 . <p> To Appear in IRREGULAR `96 orchestration model uses two techniques|structural abstraction and the inspector/executor model. Structural abstraction, introduced in the LPARX programming system, separates the description of an elaborate computational structure from the data itself <ref> [2, 7] </ref>. KeLP utilizes structural abstraction to provide intuitive geometric operations for manipulating a high-level description of data dependence patterns. KeLP relies on a generalization of the inspector/executor model employed in Multiblock PARTI [4]. <p> Under structural abstraction, the geometry of a block structured data set is stored separately from the data itself. The decomposition of the blocks exists as a first-class object <ref> [2, 7] </ref>. KeLP extends the notion to communication. That is, communication patterns are first class, mutable objects, along with the handlers that carry out the communication. KeLP inherits three core data decomposition abstractions from LPARX: Region, Grid, and XArray. KeLP extends the model with a fourth core abstraction, the FloorPlan. <p> The To Appear in IRREGULAR `96 second example, which is far less complicated, enables us to compare performance of the KeLP implementation with a carefully hand-coded version. 3 3.1 Adaptive Multigrid Lda3d is a 3D adaptive multigrid eigenvalue solver used in ab initio materials science simulations <ref> [2, 8] </ref>. This application was originally coded in LPARX and then ported to KeLP. The lda3d algorithm demands complex irregular communication to transfer data within and between levels of the mesh hierarchy (see Fig. 1b). MotionPlans and the Region calculus provide a natural paradigm to describe these data motion patterns. <p> LPARX introduced structural abstraction, a set of high-level geometric abstractions used to describe data layout and data motion in irregular dynamic structured applications <ref> [2] </ref>. LPARX uses an asynchronous communication model which is most appropriate for applications where run-time preprocessing of the communication pattern is impossible. However, when communication dependences can be computed beforehand, run-time preprocessing can significantly reduce communication overheads and synchronization. Structural abstraction is employed in other adaptive finite difference solvers.
Reference: 3. <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang, </author> <title> "Communication optimizations for irregular scientific computations on distributed memory architectures," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 22, no. 3, </volume> <pages> pp. 462-479, </pages> <year> 1994. </year>
Reference-contexts: Since these structures give rise to unpredictable communication patterns, parallelization is difficult. To ease the programmer's burden, programming languages and libraries can hide many low-level details of a parallel implementation <ref> [1, 2, 3, 4, 5, 6] </ref>. We present Kernel Lattice Parallelism (KeLP), a C++ class library that provides high-level abstractions to manage data layout and data motion for dynamic irregular block-structured applications 1 . <p> A Grid G is identified by its Region and by the common type of its elements. For example, the Fortran-90 array defined as real A [3:7] corresponds to a one-dimensional Grid A of real with region (A) = <ref> [3; 7] </ref>. A Grid differs from a simple array in that it has a processor assignment and that the run time system provides powerful block copy operations over Grids, described below. For convenience, most KeLP Region calculus operations are defined over Grids. <p> For example, in Fig. 1b, the irregularly-shaped fine level communicates with the irregularly-shaped coarse level over the points falling in the shadow cast by the fine level. KeLP represents a data motion pattern between XArrays using the MotionPlan abstraction. The MotionPlan is a first-class communication schedule <ref> [3, 4] </ref> object representing an atomic block copy between XArrays. The programmer builds and modifies a MotionPlan using Region calculus operations. This functionality gives the user powerful mechanisms to describe highly irregular data motion patterns. <p> We speculate that the optimized KeLP implementation would be at least as efficient as the hand-coded version in such cases. 4 Related work Data orchestration is based on the inspector/executor paradigm, which was employed in the CHAOS <ref> [3] </ref>, CHAOS++ [5] and Multiblock PARTI [4] run time systems. Our work is most similar to Multiblock PARTI, which provides coarse-grain block transfers over structured data decompositions. Multiblock PARTI supports regular block distributions for dynamic arrays, but does not directly support irregular block decompositions as in systems like KeLP.
Reference: 4. <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> "An integrated runtime and compile-time approach for parallelizing structured and block structured applications," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> to appear. </note>
Reference-contexts: Since these structures give rise to unpredictable communication patterns, parallelization is difficult. To ease the programmer's burden, programming languages and libraries can hide many low-level details of a parallel implementation <ref> [1, 2, 3, 4, 5, 6] </ref>. We present Kernel Lattice Parallelism (KeLP), a C++ class library that provides high-level abstractions to manage data layout and data motion for dynamic irregular block-structured applications 1 . <p> KeLP utilizes structural abstraction to provide intuitive geometric operations for manipulating a high-level description of data dependence patterns. KeLP relies on a generalization of the inspector/executor model employed in Multiblock PARTI <ref> [4] </ref>. KeLP encodes data dependence patterns into an object called a MotionPlan, and interprets the corresponding data motion using a handler called a Mover. The programmer may customize the data motion pattern and interpretation according to the needs of the application. <p> For example, in Fig. 1b, the irregularly-shaped fine level communicates with the irregularly-shaped coarse level over the points falling in the shadow cast by the fine level. KeLP represents a data motion pattern between XArrays using the MotionPlan abstraction. The MotionPlan is a first-class communication schedule <ref> [3, 4] </ref> object representing an atomic block copy between XArrays. The programmer builds and modifies a MotionPlan using Region calculus operations. This functionality gives the user powerful mechanisms to describe highly irregular data motion patterns. <p> This prevents some optimizations such as posting for contiguous messages directly into user data structures. KeLP eliminates the need for one-sided communication by disallowing block copy operations from inside a forall loop. Instead, all block copy operations are treated as collective communication. Following the inspector/executor paradigm <ref> [4] </ref>, all processors store a locally relevant piece of the distributed communication pattern (MotionPlan). No costly global synchronization is required to detect termination of data movement. <p> We speculate that the optimized KeLP implementation would be at least as efficient as the hand-coded version in such cases. 4 Related work Data orchestration is based on the inspector/executor paradigm, which was employed in the CHAOS [3], CHAOS++ [5] and Multiblock PARTI <ref> [4] </ref> run time systems. Our work is most similar to Multiblock PARTI, which provides coarse-grain block transfers over structured data decompositions. Multiblock PARTI supports regular block distributions for dynamic arrays, but does not directly support irregular block decompositions as in systems like KeLP.
Reference: 5. <author> C. Chang, A. Sussman, and J. Saltz, </author> <title> "Support for distributed dynamic data structures in C++," </title> <type> Tech. Rep. </type> <institution> CS-TR-3266, University of Maryland, </institution> <year> 1995. </year>
Reference-contexts: Since these structures give rise to unpredictable communication patterns, parallelization is difficult. To ease the programmer's burden, programming languages and libraries can hide many low-level details of a parallel implementation <ref> [1, 2, 3, 4, 5, 6] </ref>. We present Kernel Lattice Parallelism (KeLP), a C++ class library that provides high-level abstractions to manage data layout and data motion for dynamic irregular block-structured applications 1 . <p> We speculate that the optimized KeLP implementation would be at least as efficient as the hand-coded version in such cases. 4 Related work Data orchestration is based on the inspector/executor paradigm, which was employed in the CHAOS [3], CHAOS++ <ref> [5] </ref> and Multiblock PARTI [4] run time systems. Our work is most similar to Multiblock PARTI, which provides coarse-grain block transfers over structured data decompositions. Multiblock PARTI supports regular block distributions for dynamic arrays, but does not directly support irregular block decompositions as in systems like KeLP.
Reference: 6. <author> M. Kaddoura and S. Ranka, </author> <title> "Runtime support for data parallel applications for adaptive and non-uniform computational environments." </title> <note> Preliminary Version, </note> <month> February </month> <year> 1995. </year>
Reference-contexts: Since these structures give rise to unpredictable communication patterns, parallelization is difficult. To ease the programmer's burden, programming languages and libraries can hide many low-level details of a parallel implementation <ref> [1, 2, 3, 4, 5, 6] </ref>. We present Kernel Lattice Parallelism (KeLP), a C++ class library that provides high-level abstractions to manage data layout and data motion for dynamic irregular block-structured applications 1 . <p> Rendleman et al. have developed BOXLIB, a library for implementing Adaptive Mesh Refinement on single processor computers [11]. Parashar and Browne employ it in a novel implementation technique called DAGH [12]. Scheduled communication mechanisms have proven useful in several other contexts. The STANCE system <ref> [6] </ref> uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments. The PASSION [13] and Jovian [14] systems pre-process communication patterns to optimize parallel I/O. Communication schedules have traditionally been considered for message-passing architectures.
Reference: 7. <author> S. R. Kohn and S. B. Baden, </author> <title> "Irregular coarse-grain data parallelism under LPARX," </title> <journal> J. Scientific Programming, </journal> <volume> vol. 5, no. 3, </volume> <year> 1996. </year>
Reference-contexts: To Appear in IRREGULAR `96 orchestration model uses two techniques|structural abstraction and the inspector/executor model. Structural abstraction, introduced in the LPARX programming system, separates the description of an elaborate computational structure from the data itself <ref> [2, 7] </ref>. KeLP utilizes structural abstraction to provide intuitive geometric operations for manipulating a high-level description of data dependence patterns. KeLP relies on a generalization of the inspector/executor model employed in Multiblock PARTI [4]. <p> Under structural abstraction, the geometry of a block structured data set is stored separately from the data itself. The decomposition of the blocks exists as a first-class object <ref> [2, 7] </ref>. KeLP extends the notion to communication. That is, communication patterns are first class, mutable objects, along with the handlers that carry out the communication. KeLP inherits three core data decomposition abstractions from LPARX: Region, Grid, and XArray. KeLP extends the model with a fourth core abstraction, the FloorPlan. <p> A Grid G is identified by its Region and by the common type of its elements. For example, the Fortran-90 array defined as real A [3:7] corresponds to a one-dimensional Grid A of real with region (A) = <ref> [3; 7] </ref>. A Grid differs from a simple array in that it has a processor assignment and that the run time system provides powerful block copy operations over Grids, described below. For convenience, most KeLP Region calculus operations are defined over Grids.
Reference: 8. <author> E. J. Bylaska, S. R. Kohn, S. B. Baden, A. Edelman, R. Kawai, M. E. G. Ong, and J. H. Weare, </author> <title> "Scalable parallel numerical methods and software tools for material To Appear in IRREGULAR `96 design," </title> <booktitle> in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> (San Francisco, CA), </address> <month> February </month> <year> 1995. </year>
Reference-contexts: KeLP's abstractions offer increased flexibility in expressing communication compared to previous block-structured inspector/executor libraries such as Multi-block PARTI. Such flexibility is vital in implementing irregular applications. To understand why, consider an adaptive multigrid method employed in ab-initio materials design <ref> [8] </ref>. In this application dynamic collections of 3D blocks of data are organized into levels. <p> The To Appear in IRREGULAR `96 second example, which is far less complicated, enables us to compare performance of the KeLP implementation with a carefully hand-coded version. 3 3.1 Adaptive Multigrid Lda3d is a 3D adaptive multigrid eigenvalue solver used in ab initio materials science simulations <ref> [2, 8] </ref>. This application was originally coded in LPARX and then ported to KeLP. The lda3d algorithm demands complex irregular communication to transfer data within and between levels of the mesh hierarchy (see Fig. 1b). MotionPlans and the Region calculus provide a natural paradigm to describe these data motion patterns.
Reference: 9. <author> Message-Passing Interface Standard, </author> <title> "MPI: A message-passing interface standard," </title> <institution> University of Tennessee, Knoxville, TN, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: KeLP's data orchestration mechanisms simplify the expres To Appear in IRREGULAR `96 sion these complex block communication patterns. Additionally, KeLP allows the programmer to customize communication pattern interpretation, allowing affiliated computation to occur along with data motion. KeLP runs using message-passing (e.g.. MPI <ref> [9] </ref>) and has been tested on the IBM SP2, Cray T3D, Intel Paragon, clusters of workstations and single processor workstations. We have used KeLP to implement a dynamic irregular multilevel adaptive mesh refinement algorithm and a simple uniform static finite difference calculation.
Reference: 10. <author> G. M. </author> <title> Amdahl, "Validity of the single processor approach to achieving large scale computing capabilities," </title> <booktitle> in Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <address> (Atlantic City, NJ), </address> <pages> pp. 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: On the SP2, lda3d is communication bound and KeLP version reduces overall running time by up to 40%. On the Paragon, communication is not the bottleneck and although communication overheads are substantially reduced, Amdahl's law <ref> [10] </ref> limits overall application improvement to approximately 10%. Clearly the benefits of KeLP would be substantial on workstation clusters that experience high message start times. In Fig. 4, load imbalance is measured as the time a processor must wait for other processors when entering a communication phase.
Reference: 11. <author> C. Rendleman, V. B. J. Bell, B. Crutchfield, L. Howell, and M. </author> <title> Welcome, "Boxlib users guide and manual: A library for managing rectangular domains. edition 1.99," </title> <type> tech. rep., </type> <institution> Center for Computational Science and Engineering, Lawrence Livermore National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: However, when communication dependences can be computed beforehand, run-time preprocessing can significantly reduce communication overheads and synchronization. Structural abstraction is employed in other adaptive finite difference solvers. Rendleman et al. have developed BOXLIB, a library for implementing Adaptive Mesh Refinement on single processor computers <ref> [11] </ref>. Parashar and Browne employ it in a novel implementation technique called DAGH [12]. Scheduled communication mechanisms have proven useful in several other contexts. The STANCE system [6] uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments.
Reference: 12. <author> M. Parashar and J. C. Browne, "Dagh: </author> <title> A data-management infrastructure for parallel adaptive mesh refinement techniques," </title> <type> tech. rep., </type> <institution> Department of Computer Science, University of Texas at Austin, </institution> <year> 1995. </year>
Reference-contexts: Structural abstraction is employed in other adaptive finite difference solvers. Rendleman et al. have developed BOXLIB, a library for implementing Adaptive Mesh Refinement on single processor computers [11]. Parashar and Browne employ it in a novel implementation technique called DAGH <ref> [12] </ref>. Scheduled communication mechanisms have proven useful in several other contexts. The STANCE system [6] uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments. The PASSION [13] and Jovian [14] systems pre-process communication patterns to optimize parallel I/O. Communication schedules have traditionally been considered for message-passing architectures.
Reference: 13. <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krishnaiyer, R. Ponnusamy, T. Singh, and R. Thakur, </author> <title> "PASSION:parallel and scalable software for I/O," </title> <type> Tech. Rep. </type> <institution> SCCS-636, NPAC, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Parashar and Browne employ it in a novel implementation technique called DAGH [12]. Scheduled communication mechanisms have proven useful in several other contexts. The STANCE system [6] uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments. The PASSION <ref> [13] </ref> and Jovian [14] systems pre-process communication patterns to optimize parallel I/O. Communication schedules have traditionally been considered for message-passing architectures. However, recent work [15, 16] has shown that inspector/executor techniques may be necessary to achieve good performance on distributed shared-memory architectures as well.
Reference: 14. <author> R.Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz, </author> <title> "A framework for optimizing parallel I/O," </title> <type> Tech. Rep. </type> <institution> CS-TR-3417, University of Maryland: Department of Computer Science, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Parashar and Browne employ it in a novel implementation technique called DAGH [12]. Scheduled communication mechanisms have proven useful in several other contexts. The STANCE system [6] uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments. The PASSION [13] and Jovian <ref> [14] </ref> systems pre-process communication patterns to optimize parallel I/O. Communication schedules have traditionally been considered for message-passing architectures. However, recent work [15, 16] has shown that inspector/executor techniques may be necessary to achieve good performance on distributed shared-memory architectures as well.
Reference: 15. <author> S. S. Mukherjee, S. D. Sharman, M. D. Hill, J. R. Larus, A. Rogers, and J. Saltz, </author> <title> "Efficient support for irregular applications on distributed memory machines," </title> <booktitle> in PPoPP 95, </booktitle> <year> 1995. </year>
Reference-contexts: The STANCE system [6] uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments. The PASSION [13] and Jovian [14] systems pre-process communication patterns to optimize parallel I/O. Communication schedules have traditionally been considered for message-passing architectures. However, recent work <ref> [15, 16] </ref> has shown that inspector/executor techniques may be necessary to achieve good performance on distributed shared-memory architectures as well. To Appear in IRREGULAR `96 5 Conclusion We have introduced run-time data orchestration, a set of abstractions for managing communication in dynamic block-structured applications.
Reference: 16. <author> B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, J. R. Larus, A. Rogers, and D. A. Wood, </author> <title> "Application-specific protocols for user-level shared memory," </title> <booktitle> in Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The STANCE system [6] uses inspector/executor mechanisms to manage data-parallel applications on adaptive, non-uniform environments. The PASSION [13] and Jovian [14] systems pre-process communication patterns to optimize parallel I/O. Communication schedules have traditionally been considered for message-passing architectures. However, recent work <ref> [15, 16] </ref> has shown that inspector/executor techniques may be necessary to achieve good performance on distributed shared-memory architectures as well. To Appear in IRREGULAR `96 5 Conclusion We have introduced run-time data orchestration, a set of abstractions for managing communication in dynamic block-structured applications.
References-found: 16

