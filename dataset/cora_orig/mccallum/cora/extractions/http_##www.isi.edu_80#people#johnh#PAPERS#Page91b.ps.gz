URL: http://www.isi.edu:80/people/johnh/PAPERS/Page91b.ps.gz
Refering-URL: http://www.isi.edu:80/people/johnh/PAPERS/Page91b.html
Root-URL: http://www.isi.edu
Email: fpage,guy,popek,johnh,waimak,dieterg@cs.ucla.edu  
Title: Management of Replicated Volume Location Data in the Ficus Replicated File System  
Author: Thomas W. Page Jr., Richard G. Guy, John S. Heidemann, Gerald J. Popek Wai Mak, and Dieter Rothmeier 
Address: Los Angeles  
Affiliation: Department of Computer Science University of California  
Date: June 1991, pages 17-29  
Note: Appeared in the Proceedings of the Summer USENIX Conference,  
Abstract: Existing techniques to provide name transparency in distributed file systems have been designed for modest scale systems, and do not readily extend to very large configurations. This paper details the approach which is now operational in the Ficus replicated Unix filing environment, and shows how it differs from other methods currently in use. The Ficus mechanism permits optimistic management of the volume location data by exploiting the existing directory reconciliation algorithms which merge directory updates made during network partition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard G. Guy. Ficus: </author> <title> A Very Large Scale Reliable Distributed File System. </title> <type> Ph.D. dissertation, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <year> 1991. </year> <note> In preparation. </note>
Reference-contexts: We will see how these same algorithms may be leveraged to manage the volume location data. 3 The details of the two-phase algorithms for scavenging logically deleted directory entries and garbage collecting unreferenced files can be found in <ref> [1, 3] </ref>. The issues are somewhat more subtle than a first glance would suggest. Appeared in the Proceedings of the Summer USENIX Conference, June 1991, pages 17-29 2.2 Stackable Layers Ficus is also unique in its support for extensible filing environment features via stackable layered structuring.
Reference: [2] <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier. </author> <title> Implementation of the Ficus replicated file system. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 63-71. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF [12]; larger scale file systems are exemplified by AFS [7], Decorum [6], Coda [14], Sprite [9], and Ficus <ref> [2, 10] </ref>). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point. <p> This architecture permits replication to co-exist with other independently implemented filing features and to remain largely independent of the underlying file system implementation. We briefly introduce the layered architecture here; for more details, see <ref> [4, 10, 2] </ref> The stackable layers architecture provides a mechanism whereby new functionality can be added to a file system transparently to all other modules. This is in contrast to today's Unix file systems in which substantial portions must be rebuilt in order to add a new feature.
Reference: [3] <author> Richard G. Guy and Gerald J. Popek. </author> <title> Algorithms for consistency in optimistically replicated file systems. </title> <type> Technical Report CSD-910006, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> March </month> <year> 1991. </year> <note> Submitted for publication. </note>
Reference-contexts: We argue that serializability is not provided in single machine Unix file systems, and is not required in distributed file systems. This kind of policy seems necessary and appropriate for the scale and failure modes in a nation-wide file system. Details of the reconciliation algorithms may be found in <ref> [3, 11] </ref>, while for more information about the architecture of Ficus see [10, 4]. 2.1 Reconciliation The essential elements of the optimistic replica consistency strategy are the reconciliation algorithms which ensure eventual mutual consistency of replicas. <p> We will see how these same algorithms may be leveraged to manage the volume location data. 3 The details of the two-phase algorithms for scavenging logically deleted directory entries and garbage collecting unreferenced files can be found in <ref> [1, 3] </ref>. The issues are somewhat more subtle than a first glance would suggest. Appeared in the Proceedings of the Summer USENIX Conference, June 1991, pages 17-29 2.2 Stackable Layers Ficus is also unique in its support for extensible filing environment features via stackable layered structuring. <p> Like directory entries, deleted graft point entries may be expunged when all replicas have the entry logically deleted, and all replicas know that all replicas know the entry is logically deleted (hence the two-phase nature of the reconciliation algorithms; see <ref> [3] </ref>). Now consider moving a volume replica. When a replica storage site is changed, the h hostname i field in the graft point entry is updated.
Reference: [4] <author> John S. Heidemann and Gerald J. Popek. </author> <title> A layered approach to file system development. </title> <type> Technical Report CSD-910007, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> March </month> <year> 1991. </year> <note> Submitted for publication. </note>
Reference-contexts: This kind of policy seems necessary and appropriate for the scale and failure modes in a nation-wide file system. Details of the reconciliation algorithms may be found in [3, 11], while for more information about the architecture of Ficus see <ref> [10, 4] </ref>. 2.1 Reconciliation The essential elements of the optimistic replica consistency strategy are the reconciliation algorithms which ensure eventual mutual consistency of replicas. <p> This architecture permits replication to co-exist with other independently implemented filing features and to remain largely independent of the underlying file system implementation. We briefly introduce the layered architecture here; for more details, see <ref> [4, 10, 2] </ref> The stackable layers architecture provides a mechanism whereby new functionality can be added to a file system transparently to all other modules. This is in contrast to today's Unix file systems in which substantial portions must be rebuilt in order to add a new feature.
Reference: [5] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Rob-ert Sidebotham, and Michael West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The difference between the Ficus and AFS methods lies in the nature of Ficus volumes (which are replicated) and the relationship of graft points and volume location databases. In Ficus, like AFS <ref> [5] </ref>, a volume is a collection of files which are managed together and which form a subtree of the name space 2 . Each logical volume in Ficus is represented by a set of volume replicas which form a maximal, but extensible, collection of containers for file replicas.
Reference: [6] <author> Michael L. Kazar, Bruce W. Leverett, Owen T. Anderson, Vasilis Apostolides, Beth A. Bottos, Sailesh Chutani, Craig F. Everhart, W. Anthony Mason, Shu-Tsui Tu, and Edward R. Zayas. </author> <title> Decorum file system architectural overview. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 151-163. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: The traditional Unix volume super-tree connection mechanism has been widely altered or replaced to support both small and large scale distributed file systems. Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF [12]; larger scale file systems are exemplified by AFS [7], Decorum <ref> [6] </ref>, Coda [14], Sprite [9], and Ficus [2, 10]). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point.
Reference: [7] <author> Michael Leon Kazar. </author> <title> Synchronization and caching issues in the Andrew File System. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 31-43. </pages> <publisher> USENIX, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: The traditional Unix volume super-tree connection mechanism has been widely altered or replaced to support both small and large scale distributed file systems. Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF [12]; larger scale file systems are exemplified by AFS <ref> [7] </ref>, Decorum [6], Coda [14], Sprite [9], and Ficus [2, 10]). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point.
Reference: [8] <author> S. R. Kleiman. Vnodes: </author> <title> An architecture for multiple file system types in Sun Unix. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 238-247. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: A volume replica is represented by a vfs structure (see <ref> [8] </ref>).
Reference: [9] <author> John K. Ousterhout, Andrew R. Cherenson, Frederick Douglis, Michael N. Nelson, and Brent B. Welch. </author> <title> The sprite network operating system. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 23-36, </pages> <month> Febru-ary </month> <year> 1988. </year>
Reference-contexts: Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF [12]; larger scale file systems are exemplified by AFS [7], Decorum [6], Coda [14], Sprite <ref> [9] </ref>, and Ficus [2, 10]). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point.
Reference: [10] <author> Thomas W. Page, Jr., Richard G. Guy, Gerald J. Popek, and John S. Heidemann. </author> <title> Architecture of the Ficus scalable replicated file system. </title> <type> Technical Report CSD-910005, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <month> March </month> <year> 1991. </year> <note> Submitted for publication. </note>
Reference-contexts: Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF [12]; larger scale file systems are exemplified by AFS [7], Decorum [6], Coda [14], Sprite [9], and Ficus <ref> [2, 10] </ref>). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point. <p> This kind of policy seems necessary and appropriate for the scale and failure modes in a nation-wide file system. Details of the reconciliation algorithms may be found in [3, 11], while for more information about the architecture of Ficus see <ref> [10, 4] </ref>. 2.1 Reconciliation The essential elements of the optimistic replica consistency strategy are the reconciliation algorithms which ensure eventual mutual consistency of replicas. <p> This architecture permits replication to co-exist with other independently implemented filing features and to remain largely independent of the underlying file system implementation. We briefly introduce the layered architecture here; for more details, see <ref> [4, 10, 2] </ref> The stackable layers architecture provides a mechanism whereby new functionality can be added to a file system transparently to all other modules. This is in contrast to today's Unix file systems in which substantial portions must be rebuilt in order to add a new feature.
Reference: [11] <author> D. Stott Parker, Jr., Gerald Popek, Gerard Rudisin, Allen Stoughton, Bruce J. Walker, Eve-lyn Walton, Johanna M. Chow, David Edwards, Stephen Kiser, and Charles Kline. </author> <title> Detection of mutual inconsistency in distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 9(3) </volume> <pages> 240-247, </pages> <month> May </month> <year> 1983. </year>
Reference-contexts: We argue that serializability is not provided in single machine Unix file systems, and is not required in distributed file systems. This kind of policy seems necessary and appropriate for the scale and failure modes in a nation-wide file system. Details of the reconciliation algorithms may be found in <ref> [3, 11] </ref>, while for more information about the architecture of Ficus see [10, 4]. 2.1 Reconciliation The essential elements of the optimistic replica consistency strategy are the reconciliation algorithms which ensure eventual mutual consistency of replicas. <p> Each time a file is updated, the version vector on the copy receiving the update is incremented. File replicas are reconciled by comparing their version vectors as described in <ref> [11] </ref>. As a part of reconciliation, a replica with a dominant version vector is propagated over the out-of-date replica which also receives the new version vector.
Reference: [12] <author> Gerald J. Popek and Bruce J. Walker. </author> <title> The Locus Distributed System Architecture. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The traditional Unix volume super-tree connection mechanism has been widely altered or replaced to support both small and large scale distributed file systems. Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF <ref> [12] </ref>; larger scale file systems are exemplified by AFS [7], Decorum [6], Coda [14], Sprite [9], and Ficus [2, 10]). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point. <p> Further, as a distributed file system scales across distinct administrative domains, the prospect of maintaining global agreement by convention becomes exceedingly difficult. IBM's TCF, like its predecessor Locus <ref> [12] </ref>, achieves transparency by renegotiating a common view of the mount table among all sites in a partition every time the communications or node topology (partition membership) changes. This design achieves a very high degree of network transparency in limited scale local area networks where topology change is relatively rare.
Reference: [13] <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon. </author> <title> Design and implementation of the Sun Network File System. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 119-130. </pages> <publisher> USENIX, </publisher> <month> June </month> <year> 1985. </year> <booktitle> Appeared in the Proceedings of the Summer USENIX Conference, </booktitle> <month> June </month> <year> 1991, </year> <pages> pages 17-29 </pages>
Reference-contexts: The traditional Unix volume super-tree connection mechanism has been widely altered or replaced to support both small and large scale distributed file systems. Examples of the former are Sun's Network File System (NFS) <ref> [13] </ref> and IBM's TCF [12]; larger scale file systems are exemplified by AFS [7], Decorum [6], Coda [14], Sprite [9], and Ficus [2, 10]). <p> If the hostname is remote, the graft daemon obtains a file handle for the remote volume by contacting the remote graft daemon (similar to an NFS mount; see <ref> [13] </ref>) and completes the graft. A graft point caches the pointer to the root node of a grafted volume so that it does not have to be reinterpreted each time it is traversed.
Reference: [14] <author> Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. Okasaki, Ellen H. Siegel, and David C. Steere. Coda: </author> <title> A highly available file system for a distributed workstation environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 447-459, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Examples of the former are Sun's Network File System (NFS) [13] and IBM's TCF [12]; larger scale file systems are exemplified by AFS [7], Decorum [6], Coda <ref> [14] </ref>, Sprite [9], and Ficus [2, 10]). The problem addressed by this paper is simply stated as follows: in the course of expanding a path name in a distributed file system, the system encounters a graft point.
Reference: [15] <author> Edward R. Zayas and Craig F. Everhart. </author> <title> Design and specification of the cellular Andrew environment. </title> <type> Technical Report CMU-ITC-070, </type> <institution> Carnegie-Mellon University, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Further, in a nationwide environment, the topology is constantly in a state of flux; no algorithm which must renegotiate global agreements upon each partition membership change may be considered. Clearly neither of the above approaches scales beyond a few tens of sites. Cellular AFS <ref> [15] </ref> (like Ficus) is designed for larger scale application. AFS employs a Volume Location Data Base (VLDB) for each cell (local cluster) which is replicated on the cell's backbone servers. The mount point itself contains the cell and volume identifiers.
References-found: 15

