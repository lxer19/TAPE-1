URL: http://www.hcs.eng.fsu.edu/pubs/hcs_sci3.ps
Refering-URL: http://www.hcs.eng.fsu.edu/Pubs.html
Root-URL: 
Title: A Cluster Testbed for SCI-based Parallel Processing  
Author: Alan D. George, Robert W. Todd, and Warren Rosen 
Date: Abstract  
Note: Rosen is with the Naval Air Warfare Center, Aircraft  
Address: Dr.  PA  
Affiliation: High-performance Computing and Simulation (HCS) Research Laboratory Electrical Engineering Department, FAMU-FSU College of Engineering Florida State University and Florida A&M University  Division, Warminster,  
Abstract: A critical factor in the design of any parallel processing computer system is the interconnection network between processors. Latency, throughput and other network factors weigh heavily on the performance of any parallel program. In this paper, the average latency and sustained throughput for a ten-node, SPARC-based SCI cluster is tested and measured based on variations in message size, cluster or ring size, and transfer method. These results are compared with corresponding TCP/IP-based Ethernet values in order to help gauge the potential of COTS SCI clusters for supporting parallel processing environments and applications. 
Abstract-found: 1
Intro-found: 1
Reference: [ALNE93] <author> Alnes, K., </author> <title> Enabling Products for Cluster Computing using SCI, </title> <booktitle> Proceedings of the First International Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <pages> pp. 58-64, </pages> <month> August, </month> <year> 1994. </year>
Reference: [DOLP95] <author> Dolphin Inc., </author> <title> 1 Gbit/sec SBus-SCI Cluster Adapter Card, White Paper, Dolphin Interconnect Solutions, </title> <month> March </month> <year> 1995. </year>
Reference-contexts: This allows the SCI card to take direct advantage of fast 64-byte transfers without the need for breaking apart a message. Sbus commands are mapped to SCI commands and back again using the simple table shown in Table 1. Table 1. Sbus to SCI Mapping <ref> [DOLP95] </ref> Sbus cmd Sbus size SCI Request SCI Response read () 2 0 - 2 3 bytes read_sb () response_16 write () 2 0 - 2 3 bytes write_sb () response_00 3.
Reference: [GUST95] <author> Gustavson, D.B. and Q. Li, </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface, </title> <booktitle> Proceedings of the Second International Workshop on SCI-based High Performance Low-Cost Computing, </booktitle> <pages> pp. </pages> <address> 131--154, </address> <month> March, </month> <year> 1995. </year>
Reference-contexts: By combining these and other technical advantages with the logistic and cost advantages of international standardization and COTS availability, it may be that SCI is at present uniquely qualified as a parallel processing interconnect technology <ref> [GUST95] </ref>. In the next section an overview of the cluster testbed is provided. This is followed by the presentation and discussion of a series of latency and throughput tests conducted on the testbed.
Reference: [KARP93] <author> Karpoff, W. and B. </author> <title> Lake, PARDO - a deterministic, scalable programming paradigm for distributed memory parallel computers and workstation clusters, </title> <booktitle> Proceedings of Supercomputing 1993, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Somewhat similar is the Parallel Applications Management System (PAMS) which provides preprocessor directives for C and Fortran compilers supporting constructs such as parallel DO loops while alleviating the user from involvement with interprocessor communication <ref> [KARP93] </ref>. While there are literally dozens of parallel programming environments like these, ranging from implicit data-parallel languages to explicit message-passing ones, these four tools (i.e. PVM, MPI, HPF, and PAMS) are particularly interesting in the short term as several vendors are developing SCI/Sbus versions.
Reference: [LOVE93] <author> Loveman, D., </author> <title> High Performance Fortran, </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> Vol. 1, No. 1, </volume> <pages> pp. 25-42, </pages> <year> 1993. </year>
Reference-contexts: Unlike PVM and MPI which require explicit parallelism and parallelization by the user, a very different approach to parallel program design is afforded by High Performance Fortran (HPF) <ref> [LOVE93] </ref>. HPF follows a data-parallel model of computation primarily aimed toward algorithms and applications whose domains can be decomposed such that the same operation can be applied to some or all elements.
Reference: [MPIF93] <author> Message Passing Interface Forum, </author> <title> MPI: A Message Passing Interface, </title> <booktitle> Proceedings of Supercomputing 1993, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 878-883, </pages> <year> 1993. </year>
Reference-contexts: Another similar model of parallel computation is the Message Passing Interface (MPI) specification developed by the MPI Forum, and it too is available for a growing number of distributed and shared-memory platforms <ref> [MPIF93] </ref>. Unlike PVM and MPI which require explicit parallelism and parallelization by the user, a very different approach to parallel program design is afforded by High Performance Fortran (HPF) [LOVE93].
Reference: [SCI93] <institution> Scalable Coherent Interface, ANSI/IEEE Standard 1596-1992, IEEE Service Center, </institution> <address> Piscataway, New Jersey, </address> <year> 1993. </year>
Reference-contexts: networks in that it supports cache coherent, shared memory between physically distributed processors and memory units in a fashion scalable to tens of thousands of processors, while supporting a variety of topologies and specifying link data rates of eight gigabits per second and latencies expected to drop below a microsecond <ref> [SCI93] </ref>. By combining these and other technical advantages with the logistic and cost advantages of international standardization and COTS availability, it may be that SCI is at present uniquely qualified as a parallel processing interconnect technology [GUST95]. In the next section an overview of the cluster testbed is provided.
Reference: [SUND90] <author> Sunderam, </author> <title> V.S., PVM: A Framework for Parallel Distributed Computing, </title> <journal> Concurrency: Practice & Experience, </journal> <volume> Vol. 2, No. 4, </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: For instance, one of the most widely used parallel programming tools for workstation clusters is the Parallel Virtual Machine (PVM) software developed and distributed for free by the Oak Ridge National Laboratory <ref> [SUND90] </ref>. This tool provides the programmer with a set of message-passing functions used with a high-level language program and has been ported to many platforms including machines on which these functions are physically implemented via shared-memory access as well as distributed-memory machines such as workstation clusters.
References-found: 8

