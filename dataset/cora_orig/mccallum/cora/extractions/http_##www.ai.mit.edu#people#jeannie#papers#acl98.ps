URL: http://www.ai.mit.edu/people/jeannie/papers/acl98.ps
Refering-URL: http://www.ai.mit.edu/people/jeannie/resume.html
Root-URL: 
Email: walker@research.att.com jeannie@ai.mit.edu shri@research.att.com  
Title: Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email  
Author: Marilyn A. Walker Jeanne C. Fromer Shrikanth Narayanan 
Address: 180 Park Ave. 545 Technology Square 180 Park Ave. Florham Park, NJ 07932 Cambridge, MA, 02139 Florham Park, NJ 07932  
Affiliation: ATT Labs Research MIT AI Lab ATT Labs Research  
Abstract: This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy. While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent's choices when there are multiple ways to realize a communicative intention. Our method is based on a combination of learning algorithms and empirical evaluation techniques. The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning. The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders. 
Abstract-found: 1
Intro-found: 1
Reference: <author> A.G. Barto, S. J. Bradtke, and S. P. Singh. </author> <year> 1995. </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence Journal, 72(1-2):81138. </journal>
Reference: <author> R. E. Bellman. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, N.J. </address>
Reference: <author> A. W. Biermann and Philip M. Long. </author> <year> 1996. </year> <title> The composition of messages in speech-graphics interactive systems. </title> <booktitle> In Proc. of the 1996 International Symposium on Spoken Dialogue, </booktitle> <pages> pp. 97100. </pages>
Reference: <author> J. Chu-Carroll and S. Carberry. </author> <year> 1995. </year> <title> Response generation in collaborative negotiation. </title> <booktitle> In Proc. of the 33rd Annual Meeting of the ACL, </booktitle> <pages> pp. 136143. </pages>
Reference: <author> P. R. Cohen. </author> <year> 1995. </year> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> MIT Press, </publisher> <address> Boston. </address>
Reference: <author> M. Danieli and E. Gerbino. </author> <year> 1995. </year> <title> Metrics for evaluating dialogue strategies in a spoken language system. </title> <booktitle> In Proc. of the 1995 AAAI Spring Symposium on Empirical Methods in Discourse, </booktitle> <pages> pages 3439. </pages>
Reference: <author> J. C. Fromer. </author> <year> 1998. </year> <title> Learning optimal discourse strategies in a spoken dialogue system. </title> <type> Technical Report Forthcoming, </type> <institution> MIT AI Lab M.S. </institution> <type> Thesis. </type>
Reference-contexts: Possible algorithms include Genetic Algorithms, Q-learning, TD-Learning, and Adaptive Dynamic Programming (Russell and Norvig, 1995). Here we use Q-learning to illustrate the method (Watkins, 1989). See <ref> (Fromer, 1998) </ref> for experiments using alternative algorithms. <p> TYPE STRATEGY UTILITY Read Read-First .21 Read-Choice-Prompt .07 Read-Summarize-Only .08 Summarize Summarize-System .162 Summarize-Choice -0.03 Summarize-Both .09 Table 3: Utilities for Presentation Strategy Choices after 124 Training Sessions The SI and MI strategies affect the whole dialogue; the presentation strategies apply locally and 3 See <ref> (Fromer, 1998) </ref> for experiments in which local rewards are nonzero. for 108 ELVIS Dialogues can be actived in different states of the dialogue. <p> Table 3 reports utilities for the use of a strategy after one scenario was completed . The policy implied by the utilities at other phases of the task are the same. See <ref> (Fromer, 1998) </ref> for more detail. The Read-First strategy in D1 has the best performance of the read strategies. This strategy takes the initiative to read a message, which might result in messages being read that the user wasn't interested in. <p> In spoken dialogue systems, the system designers construct the state space and decide what state variables need to be monitored. Our initial results suggest that the state representation that the agent uses to interact with the user may not be the optimal state representation for learning. See <ref> (Fromer, 1998) </ref>. Second, in advance of actually running learning experiments, it is not clear how much experience an agent will need to determine which strategy is better.
Reference: <author> E. H. Hovy. </author> <year> 1993. </year> <title> Automated discourse generation using discourse structure relations. </title> <journal> Artificial Intelligence Journal, 63:341385. </journal>
Reference: <author> C. Kamm, S. Narayanan, D. Dutton, and R. Ritenour. </author> <year> 1997. </year> <title> Evaluating spoken dialog systems for telecommunication services. </title> <booktitle> In EUROSPEECH 97. </booktitle>
Reference-contexts: those for deriving a performance function (Section 3), and for using the derived performance function as feedback to the agent with a learning algorithm (Section 4). 3 Using PARADISE to Derive a Performance Function 3.1 ELVIS Spoken Dialogue System ELVIS is implemented using a general-purpose platform for spoken dialogue agents <ref> (Kamm et al., 1997) </ref>. The platform consists of a speech recognizer that supports barge-in so that the user can interrupt the agent when it is speaking.
Reference: <author> R. Keeney and H. Raiffa. </author> <year> 1976. </year> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> John Wiley and Sons. </publisher>
Reference: <author> E. Levin and R. Pieraccini. </author> <year> 1997. </year> <title> A stochastic model of computer-human interaction for learning dialogue strategies. </title> <booktitle> In EUROSPEECH 97. </booktitle>
Reference: <author> M.T. Maybury. </author> <year> 1991. </year> <title> Planning multi-media explanations using communicative acts. </title> <booktitle> In Proc. of the Ninth National Conf. on Artificial Intelligence, </booktitle> <pages> pages 6166. </pages>
Reference: <author> K. R. McKeown. </author> <year> 1985. </year> <title> Discourse strategies for generating natural language text. </title> <journal> Artificial Intelligence, </journal> <volume> 27(1):142, </volume> <month> September. </month>
Reference: <author> J. D. Moore and C. L. Paris. </author> <year> 1989. </year> <title> Planning text for advisory dialogues. </title> <booktitle> In Proc. 27th Annual Meeting of the ACL. </booktitle>
Reference: <author> S. Russell and P. Norvig. </author> <year> 1995. </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> N.J. </address>
Reference-contexts: Possible algorithms include Genetic Algorithms, Q-learning, TD-Learning, and Adaptive Dynamic Programming <ref> (Russell and Norvig, 1995) </ref>. Here we use Q-learning to illustrate the method (Watkins, 1989). See (Fromer, 1998) for experiments using alternative algorithms.
Reference: <author> R. S. Sutton. </author> <year> 1991. </year> <title> Planning by incremental dynamic programming. </title> <booktitle> In Proc. Ninth Conf. on Machine Learning, </booktitle> <pages> pages 353357. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> M. A. Walker, D. Litman, C. Kamm, and A. Abella. </author> <year> 1997. </year> <title> PARADISE: A general framework for evaluating spoken dialogue agents. </title> <booktitle> In Proc. of the 35th Annual Meeting of the ACL, </booktitle> <pages> pp. 271280. </pages>
Reference-contexts: This paper presents a method based on dynamic programming by which dialogue agents can learn to optimize their choice of dialogue strategies. We draw on the recently proposed PARADISE evaluation framework <ref> (Walker et al., 1997) </ref> to identify the important performance factors and to provide a performance function for calculating the utility of the final state of a dialogue. We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone. <p> results from modeling a corpus of 232 spoken dialogues in which ELVIS conversed with human users to carry out a set of email tasks. 2 Method for Learning to Optimize Dialogue Strategy Selection Our method for learning to optimize dialogue strategy selection combines the application of PARADISE to empirical data <ref> (Walker et al., 1997) </ref>, with algorithms for learning optimal strategy choices. PARADISE provides an empirical method for deriving a performance function that calculates overall agent performance as a linear combination of a number of simpler metrics. <p> Find out the Meeting Time and the Meeting Place. Scenario 1.1 is represented in terms of the attribute value matrix (AVM) in Table 1. Successful completion of a scenario requires that all attribute-values must be exchanged <ref> (Walker et al., 1997) </ref>. <p> The AVM matrix supports calculating Task Success objectively by using the Kappa statistic to compare the information in the AVM that the users filled in with an AVM key such as that in Table 1 <ref> (Walker et al., 1997) </ref>. In order to calculate User Satisfaction, users were asked to evaluate the agent's performance with a user satisfaction survey. The data from the survey resulted in user satisfaction values that range from 0 to 33. <p> The performance function is derived through mul-tivariate linear regression with User Satisfaction as the dependent variable and all the other measures as independent variables <ref> (Walker et al., 1997) </ref>. See Table 2. In the ELVIS data, an initial regression over the measures in Table 2 suggests that Comp, MRS and ET are the only significant contributors to User Satisfaction. <p> However, to our knowledge, these methods have not previously been applied to interactions with real users. The lack of an appropriate performance function has been a critical methodological limitation. We use the PARADISE framework <ref> (Walker et al., 1997) </ref> to derive an empirically motivated performance function, that combines both subjective user preferences and objective system performance measures into a single function. It would have been impossible to predict a priori which dialogue factors influence the usability of a dialogue agent, and to what degree.
Reference: <author> M. Walker, J. Fromer, G. Di Fabbrizio, C. </author> <title> Mestel, </title> <publisher> and D. </publisher>
Reference: <author> Hindle. </author> <year> 1998. </year> <title> What can I say: Evaluating a spoken language interface to email. </title> <booktitle> In Proc. of the Conf. on Computer Human Interaction (CHI 98). </booktitle>
Reference: <author> M. A. Walker. </author> <year> 1993. </year> <title> Informational Redundancy and Resource Bounds in Dialogue. </title> <type> Ph.D. thesis, </type> <institution> University of Pennsylvania. </institution>
Reference: <author> C. J. Watkins. </author> <year> 1989. </year> <title> Models of Delayed Reinforcement Learning. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University. </institution>
Reference-contexts: Possible algorithms include Genetic Algorithms, Q-learning, TD-Learning, and Adaptive Dynamic Programming (Russell and Norvig, 1995). Here we use Q-learning to illustrate the method <ref> (Watkins, 1989) </ref>. See (Fromer, 1998) for experiments using alternative algorithms.
References-found: 21

