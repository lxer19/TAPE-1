URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-029.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: Labeling RAAM  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Alessandro Sperduti 
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-93-029  
Abstract: In this paper we propose an extension of the Recursive Auto-Associative Memory (RAAM) by Pollack. This extension, the Labeling RAAM (LRAAM), is able to encode labeled graphs with cycles by representing pointers explicitly. A theoretical analysis of the constraints imposed on the weights by the learning task under the hypothesis of perfect learning and linear output units is presented. Cycles and confluent pointers result to be particularly effective in imposing constraints on the weights. Some technical problems encountered in the RAAM, such as the termination problem in the learning and decoding processes, are solved more naturally in the LRAAM framework. The representations developed for the pointers seem to be robust to recurrent decoding along a cycle. Data encoded in a LRAAM can be accessed by pointer as well as by content. The direct access by content can be achieved by transforming the encoder network of the LRAAM in a Bidirectional Associative Memory (BAM). Different access procedures can be defined according to the access key. The access procedures are not wholly reliable, however they seem to have a high likelihood of success. A geometric interpretation of the decoding process is given and the representations developed in the pointer space of a two hidden units LRAAM are presented and discussed. In particular, the pointer space results to be partitioned in a fractal-like fashion. Some effects on the representations induced by the Hopfield-like dynamics of the pointer decoding process are discussed and an encoding scheme able to retain the richness of representation devised by the decoding function is outlined. The application of the LRAAM model to the control of the dynamics of recurrent high-order networks is briefly sketched as well. 
Abstract-found: 1
Intro-found: 1
Reference: [AAM93] <author> A. Atiya and Y. S. Abu-Mostafa. </author> <title> An analog feedback associative memory. </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> 4 </volume> <pages> 117-126, </pages> <year> 1993. </year>
Reference-contexts: exist. 7 Or, in order to obtain a more robust encoding, as soon as the input net value is on the correct side and over a given tolerance. 40 can be viewed as an approximate method to build analog BAMs, which actually are analog Hopfield networks with a hidden layer <ref> [AAM93] </ref>. Most probably an LRAAM is something between them. In fact, while extending the representational capabilities of the RAAM model, it doesn't possess the same synthetic capabilities of the RAAM, since it uses explicitly the concept of pointer. <p> In fact, data stored in an LRAAM can be accessed both by pointer and by content. While access by pointer is a reliable procedure, access by content is not so reliable. However, recent developments in analog Hopfield networks with hidden units <ref> [AAM93] </ref> allow to test if an equilibrium state is asymptotically stable. Since the training set of an LRAAM defines, in first approximation, a set of equilibria states for the associated BAM, a prediction of the reliability of the access procedures can be made.
Reference: [BMM92] <author> D.S. Blank, L.A. Medeen, and J.B. Marshall. </author> <title> Exploring the symbolic/subsymbolic continuum: a case study of raam. </title> <editor> In J. Dinsmore, editor, </editor> <booktitle> The Symbolic and Connectionist Paradigms: Closing the Gap, </booktitle> <volume> volume 1, </volume> <pages> pages 113-148. </pages> <publisher> Lawrence Erlbaum, </publisher> <year> 1992. </year>
Reference-contexts: In the last years, different papers have discussed or used the RAAM model with interesting results <ref> [Chr91, BMM92, Rei92, SW92] </ref>. The basic RAAM can encode arbitrary tree structures of variable depth but fixed branching factor (valence). The idea is to map a symbolic tree into a numeric vector and then to reconstruct a very close approximation of the symbolic tree starting from the numeric vector. <p> is the decoding function, i.e., given a structure does it exist a reduced representation from which the recursive application of the decoding function can extract all the components of the structure? The approach we use in performing this exploration is very close to the one used by Blank and al. <ref> [BMM92] </ref>, but adjusted to the LRAAM model and extended in scope.
Reference: [Cha90] <author> D. J. Chalmers. </author> <title> Syntactic transformations on distributed representations. </title> <journal> Connection Science, </journal> <volume> 2 </volume> <pages> 53-62, </pages> <year> 1990. </year>
Reference-contexts: A unit of such a memory is said to be coarsely tuned. 1 inference over trees might be performed by numerical transformation (i.e. neural networks) over their numerical representation, very fast and cheap inference engines would be built (see <ref> [Cha90] </ref>). A more formal characterization of representations of structures in connectionist systems has been developed by Smolensky [Smo90]. He reduces the problem of representing structured objects to three subproblems: decomposing the structures via roles, representing conjunctions, and representing variable/value bindings. The representation of variable/value bindings is obtained through tensor algebra.
Reference: [Chr91] <author> L. Chrisman. </author> <title> Learning recursive distributed representations for holistic computation. </title> <journal> Connection Science, </journal> <volume> 3 </volume> <pages> 345-366, </pages> <year> 1991. </year>
Reference-contexts: In the last years, different papers have discussed or used the RAAM model with interesting results <ref> [Chr91, BMM92, Rei92, SW92] </ref>. The basic RAAM can encode arbitrary tree structures of variable depth but fixed branching factor (valence). The idea is to map a symbolic tree into a numeric vector and then to reconstruct a very close approximation of the symbolic tree starting from the numeric vector.
Reference: [Dye91] <author> M. G. Dyer. </author> <title> Symbolic NeuroEngineering for Natural Language Processing: A Multilevel Research Approach., </title> <booktitle> volume 1 of Advances in Connectionist and Neural Computation Theory, </booktitle> <pages> pages 32-86. </pages> <publisher> Ablex, </publisher> <year> 1991. </year>
Reference-contexts: However, the ability to synthesize reduced descriptors for structures with cycles is what makes the difference between the LRAAM and the RAAM. The only system that we know of which is able to represent labeled graphs is the DUAL system proposed by Dyer <ref> [Dye91] </ref>. It is able to encode small labeled graphs representing relationships among entities. The idea is to have two networks, one responsible for the encoding of the relationships between one particular entity and the others, and one which devises a compressed representation of the weights of the first network.
Reference: [Hin90] <author> G. E. Hinton. </author> <title> Mapping part-whole hierarchies into connectionist networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 47-75, </pages> <year> 1990. </year>
Reference-contexts: Reduced representations of structured objects in connectionist systems are related by Hinton to the problem of mapping part-whole hierarchies into connectionist networks <ref> [Hin90] </ref>. The scheme he proposes considers two quite different methods for performing inference. Simple "intuitive" inferences can be performed by a single settling of a network without changing the way in which the world is mapped into the network. <p> The possibility to change the mapping allows to apply the knowledge of the system to any part of the task. 2 The choice to have distributed representations of variables (i.e. roles) is controversial. Hinton <ref> [Hin90] </ref> believes that in a nonlinear system it is probably easier to make use of the information about the fillers of roles if this information is localized. 2 In this paper, we present an extension of the RAAM, the Labeling RAAM (LRAAM).
Reference: [KL90] <author> J. Kindermann and A. Linden. </author> <title> Inversion of neural networks by gradient descent. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 277-286, </pages> <year> 1990. </year>
Reference-contexts: Another solution to this problem can be to perform a gradient descent on the pointers maintaining the label fixed. This can be performed using the inversion technique proposed by Kindermann and Linden <ref> [KL90] </ref>. Using this method, representations for the pointers which satisfy the generalization test, without the constraint to be an asymptotically stable memory of the BAM, can be found. Obviously, this technique is computationally more expensive since it involves gradient descent.
Reference: [Kos92] <author> B. Kosko. </author> <title> Neural Networks and Fuzzy Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: In an LRAAM it is possible to access a component of the encoded structure in other ways if the Encoder Network is transformed in a Bidirectional Associative Memory (BAM) <ref> [Kos92] </ref>. A BAM consists of two layers of processing elements, name them layer B H and B O , that are fully interconnected between layers with weight matrices M h , from B H to B O , and M o , from B O to B h .
Reference: [Pla91] <author> T. </author> <title> Plate. Holographic reduced representations. </title> <type> Technical Report CRG-TR-91-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: However, this system cannot be considered at the same level of the LRAAM, since it devises a reduced representation of a set of functions relating the components of the graph instead of a reduced representation for the graph. Potentially also Holographic Reduced Representations <ref> [Pla91] </ref> are able to encode cyclic graphs. The LRAAM model can also be viewed as an extension of the Hopfield networks philosophy.
Reference: [Pol89] <author> J. B. Pollack. </author> <title> Implications of Recursive Distributed Representations, </title> <booktitle> pages 527-536. Advances in Neural Information Processing Systems I. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: In particular, an example of how LRAAM can be used to control the dynamics of a recurrent network is briefly presented. Conclusions are stated in Section 7. 2 The RAAM model The RAAM (Recursive Auto-Associative Memory) was introduced by Pollack <ref> [Pol90, Pol89] </ref> to allow traditional symbolic data structures, such as trees with labeled leaves, to be represented subsymbolically as distributed patterns of activation. In the last years, different papers have discussed or used the RAAM model with interesting results [Chr91, BMM92, Rei92, SW92].
Reference: [Pol90] <author> J. B. Pollack. </author> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):77-106, </volume> <year> 1990. </year>
Reference-contexts: Finally, the coarse-coded memory of BoltzCONS needs a huge number of units because it utilizes binary codes. The above problems, together with the observation that BoltzCONS needs a large amount of human effort to design, to compress and to tune the representations, have stimulated Pollack <ref> [Pol90] </ref> to design the Recursive Auto-Associative Memory architecture (RAAM). The RAAM system uses back-propagation to discover compact recursive distributed representations for fixed-valence trees. The compact recursive distributed representations obtained are very interesting because they synthesize the characteristics of an item (categorical features) preserving its individual peculiarity (distinctive features). <p> In particular, an example of how LRAAM can be used to control the dynamics of a recurrent network is briefly presented. Conclusions are stated in Section 7. 2 The RAAM model The RAAM (Recursive Auto-Associative Memory) was introduced by Pollack <ref> [Pol90, Pol89] </ref> to allow traditional symbolic data structures, such as trees with labeled leaves, to be represented subsymbolically as distributed patterns of activation. In the last years, different papers have discussed or used the RAAM model with interesting results [Chr91, BMM92, Rei92, SW92].
Reference: [Rei92] <author> R. Reilly. </author> <title> A connectionist technique for on-line parsing. </title> <journal> Network, </journal> <volume> 3 </volume> <pages> 37-46, </pages> <year> 1992. </year> <month> 44 </month>
Reference-contexts: In the last years, different papers have discussed or used the RAAM model with interesting results <ref> [Chr91, BMM92, Rei92, SW92] </ref>. The basic RAAM can encode arbitrary tree structures of variable depth but fixed branching factor (valence). The idea is to map a symbolic tree into a numeric vector and then to reconstruct a very close approximation of the symbolic tree starting from the numeric vector.
Reference: [RT87] <author> R. Rosenfeld and D. S. Touretzky. </author> <title> Four capacity models for coarse-coded symbol memories. </title> <type> Technical Report CMU-CS-87-182, </type> <institution> Carnegie Mellon, </institution> <year> 1987. </year>
Reference-contexts: It is based on parallel associative retrieval and it differs from other connectionist systems because it constructs and modifies composite symbol structures dynamically, by representing them as activity patterns rather than as weights. It uses distributed representations of linked lists, loaded in coarse-coded memories 1 <ref> [RT87] </ref>, as basic representational elements and LISP's car, cdr, and cons functions as basic operations. Links are implemented by associations.
Reference: [Smo90] <author> P. Smolensky. </author> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 159-216, </pages> <year> 1990. </year>
Reference-contexts: A more formal characterization of representations of structures in connectionist systems has been developed by Smolensky <ref> [Smo90] </ref>. He reduces the problem of representing structured objects to three subproblems: decomposing the structures via roles, representing conjunctions, and representing variable/value bindings. The representation of variable/value bindings is obtained through tensor algebra.
Reference: [SN90] <author> J. A. Sirat and J-P. Nadal. </author> <title> Neural trees: a new tool for classification. </title> <journal> Network, </journal> <volume> 1 </volume> <pages> 423-438, </pages> <year> 1990. </year>
Reference-contexts: An example of neural code implementing a Neural Tree <ref> [SN90] </ref> has been given and different aspects of the neural code discussed in [Spe93, SS93a, SS93b]. Neural Trees (NTs) are decision trees where the decision at each node is taken by a perceptron 8 . Usually, the tree structure is stored and managed using classical symbolic data structures and programming.
Reference: [Spe93] <author> A. Sperduti. </author> <title> Optimization and Functional Reduced Descriptors in Neural Networks. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Pisa, Italy, </institution> <year> 1993. </year> <month> TD-22/93. </month>
Reference-contexts: An example of neural code implementing a Neural Tree [SN90] has been given and different aspects of the neural code discussed in <ref> [Spe93, SS93a, SS93b] </ref>. Neural Trees (NTs) are decision trees where the decision at each node is taken by a perceptron 8 . Usually, the tree structure is stored and managed using classical symbolic data structures and programming.
Reference: [SS93a] <author> A. Sperduti and A. Starita. </author> <title> An example of neural code: Neural trees implemented by LRAAMs. </title> <booktitle> In International Conference on Neural Networks and Genetic Algorithms, </booktitle> <year> 1993. </year> <note> Innsbruck. To appear. </note>
Reference-contexts: An example of neural code implementing a Neural Tree [SN90] has been given and different aspects of the neural code discussed in <ref> [Spe93, SS93a, SS93b] </ref>. Neural Trees (NTs) are decision trees where the decision at each node is taken by a perceptron 8 . Usually, the tree structure is stored and managed using classical symbolic data structures and programming.
Reference: [SS93b] <author> A. Sperduti and A. Starita. </author> <title> Modular neural codes implementing neural trees. </title> <booktitle> In 6th Italian Workshop on Parallel Architectures and Neural Networks, </booktitle> <year> 1993. </year> <note> to appear. </note>
Reference-contexts: An example of neural code implementing a Neural Tree [SN90] has been given and different aspects of the neural code discussed in <ref> [Spe93, SS93a, SS93b] </ref>. Neural Trees (NTs) are decision trees where the decision at each node is taken by a perceptron 8 . Usually, the tree structure is stored and managed using classical symbolic data structures and programming.
Reference: [SW92] <author> A. Stolcke and D. Wu. </author> <title> Tree matching with recursive distributed representations. </title> <type> Technical Report TR-92-025, </type> <institution> International Computer Science Institute, </institution> <year> 1992. </year>
Reference-contexts: In the last years, different papers have discussed or used the RAAM model with interesting results <ref> [Chr91, BMM92, Rei92, SW92] </ref>. The basic RAAM can encode arbitrary tree structures of variable depth but fixed branching factor (valence). The idea is to map a symbolic tree into a numeric vector and then to reconstruct a very close approximation of the symbolic tree starting from the numeric vector. <p> A more robust solution would be to train a classifier to discriminate between terminals and nonterminals, however it would result in a relevant computational overload. A more elegant solution to this problem was developed by Stolcke and Wu <ref> [SW92] </ref>. They used one unit of the hidden layer to represent explicitly the distinction between terminals and nonterminals.
Reference: [Tou90] <author> D. S. Touretzky. Boltzcons: </author> <title> Dynamic symbol structures in a connectionist network. </title> <journal> Artificial Intellicence, </journal> <volume> 46 </volume> <pages> 5-46, </pages> <year> 1990. </year> <month> 45 </month>
Reference-contexts: The goal of these researchers is to provide evidence of the potentiality of the connectionist approach to handle domains of structured tasks. The common background of their ideas is the search for a realization of the distal access ability and consequently of the compositionality one. BoltzCONS <ref> [Tou90] </ref> is an example of how a connectionist system (i.e. Boltzman machine) can handle symbolic structures. It is based on parallel associative retrieval and it differs from other connectionist systems because it constructs and modifies composite symbol structures dynamically, by representing them as activity patterns rather than as weights.
References-found: 20

