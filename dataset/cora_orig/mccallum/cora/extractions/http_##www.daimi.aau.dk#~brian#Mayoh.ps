URL: http://www.daimi.aau.dk/~brian/Mayoh.ps
Refering-URL: http://www.daimi.aau.dk/~brian/
Root-URL: http://www.daimi.aau.dk
Title: Cooperative Mining of Patient Data  
Author: Junping Du, Brian H. Mayoh, Niels Damgaard 
Keyword: Key Words: Machine Leaming, Medical Data, WEKA  
Address: Ny Munkegade, Building 540 DK-8000 Aarhus C Denmark  
Affiliation: Computer Science Department Aarhus University  
Note: Contact author: Prof. Brian Mayoh  
Email: Email: brian@daimi.aau.dk  
Phone: Phone: 45 89423188 Fax: 45 89423255  
Date: Abstract  
Abstract: In this paper, we use machine leaming schemes lR, FOIL, InductH and C5.0 to generate decision trees and rules from the examples in the medical dataset. Furthermore we have used an approach where neural networks generated by genetic algorithms were also used to classify the data. The aim of our study is to infer the patterns that can help doctors to identify, recognize and predict the effect of the risk factors on the long term subjective cure rates of patients who undergo colposuspension. Surprisingly high test classification was sometimes achieved. Most of our best results came when one learning method suggested the preprocessing steps to be used for another method. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. G. R. </author> <title> Uthurusamy Advances in Knowledge discovery and data mining AAAI Press / The MIT Press, </title> <address> Menlo Park, CA. </address> <year> 1996. </year>
Reference-contexts: Knowledge discovery can be defined as the extraction of implicit, previously unknown, and potentially useful information from real world data, and communicating the discovered knowledge to people in an understandable way <ref> [ 1, 2] </ref>. Machine learning is a technique that can discover previously unknown regularities and trends from diverse datasets, in the hope that machines can help in the often tedious and error-prone process of acquiring knowledge from empirical data, and help people to explain and codify their knowledge.
Reference: 2. <author> G. Piatetsky-Shapiro & W. J. </author> <title> Frawley Knowledge discovery in databases. </title> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: Knowledge discovery can be defined as the extraction of implicit, previously unknown, and potentially useful information from real world data, and communicating the discovered knowledge to people in an understandable way <ref> [ 1, 2] </ref>. Machine learning is a technique that can discover previously unknown regularities and trends from diverse datasets, in the hope that machines can help in the often tedious and error-prone process of acquiring knowledge from empirical data, and help people to explain and codify their knowledge.
Reference: 3. <author> D. </author> <title> Michie Methodologies from machine learning in data analysis and software. </title> <journal> ComputerJournal, </journal> <volume> 34(6) </volume> <year> 559-565,1991. </year>
Reference-contexts: It encompasses a wide variety of techniques used for the discovery of rules, patterns and relationships in sets of data and produces a generalization of these relationships that can be used to interpret new, unseen data <ref> [3, 4] </ref>. The output of a learning scheme is some form of structural description of a dataset, acquired from examples of given data. These descriptions encapsulate the knowledge learned by the system and can be represented in different ways.
Reference: 4. <author> M. Pazzani & D. </author> <title> Kibler The utility of knowledge in inductive learning Machine Learning, </title> <booktitle> 9(1): </booktitle> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: It encompasses a wide variety of techniques used for the discovery of rules, patterns and relationships in sets of data and produces a generalization of these relationships that can be used to interpret new, unseen data <ref> [3, 4] </ref>. The output of a learning scheme is some form of structural description of a dataset, acquired from examples of given data. These descriptions encapsulate the knowledge learned by the system and can be represented in different ways.
Reference: 5. <author> W. L. McGuire, A. T. Tandon et al. </author> <title> How to use prognostic factors in auxillary node-negative breast cancer patients. </title> <institution> J Natl Cancer Inst. </institution> <address> 82: 1006-1015,1990. </address>
Reference-contexts: The clinician usually makes decisions based on a simple dichotomization of variables into favorable and unfavorable classifications, and then rather subjectively weights the factors to reach an overall assessment of the patient's outcome <ref> [5] </ref>. Two weaknesses of this approach are: information may be lost in the dichotomization of continuous variables and the subjective weighting of multiple factors does not lead to a quantitative assessment of the outcome.
Reference: 6. <institution> D.R.Cox Regression models and life-tables. JR Stat Soc [B] 34 </institution> <month> 187-220, </month> <year> 1972. </year>
Reference-contexts: Two weaknesses of this approach are: information may be lost in the dichotomization of continuous variables and the subjective weighting of multiple factors does not lead to a quantitative assessment of the outcome. More sophisticated quantitative techniques such as Cox regression modeling <ref> [6] </ref>, recursive partitioning [7], neural network analysis [8] and machine learning methods have the potential to improve this decision making process.
Reference: 7. <author> A. Ciampi, J. F. Lawless, S. M. McKinney and K. </author> <title> Singhal Regression and recursive partition strategies in the analysis of medical survival data. </title> <journal> J. Clin. </journal> <volume> Epidemiol 41 </volume> <pages> 737-748, </pages> <year> 1988. </year>
Reference-contexts: Two weaknesses of this approach are: information may be lost in the dichotomization of continuous variables and the subjective weighting of multiple factors does not lead to a quantitative assessment of the outcome. More sophisticated quantitative techniques such as Cox regression modeling [6], recursive partitioning <ref> [7] </ref>, neural network analysis [8] and machine learning methods have the potential to improve this decision making process. In section 2 we describe our medical data; in section 3 we describe the 5 learning methods used in this paper, and our results are presented and discussed in the remaining sections.
Reference: 8. <author> P. M. Ravdin, G. M. Clark, A. K. Tandon, W. L. </author> <title> McGuire Predicting recurrence in auxillary node-negative breast cancer patients using adaptive artificial intelligence. </title> <institution> Breast Cancer Res Treat 16:190, </institution> <year> 1990. </year>
Reference-contexts: Two weaknesses of this approach are: information may be lost in the dichotomization of continuous variables and the subjective weighting of multiple factors does not lead to a quantitative assessment of the outcome. More sophisticated quantitative techniques such as Cox regression modeling [6], recursive partitioning [7], neural network analysis <ref> [8] </ref> and machine learning methods have the potential to improve this decision making process. In section 2 we describe our medical data; in section 3 we describe the 5 learning methods used in this paper, and our results are presented and discussed in the remaining sections.
Reference: 9. <author> N. Qian & T. J. </author> <title> Sejnowski Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J Mol. </journal> <volume> Biol 202: </volume> <pages> 865-884, </pages> <year> 1988. </year> <editor> l0. H. Bohr, J. Bohr, S. </editor> <title> Brunak et al Protein secondary structure and homology by neural network analysis. </title> <journal> FEBS Lett 241: </journal> <pages> 223-228, </pages> <year> 1988. </year>
Reference-contexts: In this paper we will just leave out "artificial". Neural networks have outperformed analytic techniques for such complex tasks as predicting protein secondary structure <ref> [9, 10] </ref> and synthesizing spoken language [11l. Neural networks learn to distinguish different classes of events by being given information about an event and its eventual class or outcome [12]. GANN is a combined genetic algorithm and neural network approach [24].
Reference: 11. <author> T. J. Sejnowski & C. R. </author> <title> Rosenberg Net talk: A parallel network that learns to read aloud. </title> <type> Technical Report JHU/EECS-86-01, </type> <institution> Johns Hopkins University, </institution> <year> 1986. </year>
Reference: 12. <author> D. E. Rumelhart, G. E. Hinton, R. J. </author> <title> Williams Learning representations by back propagating errors. </title> <booktitle> Nature 323: </booktitle> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: Neural networks have outperformed analytic techniques for such complex tasks as predicting protein secondary structure [9, 10] and synthesizing spoken language [11l. Neural networks learn to distinguish different classes of events by being given information about an event and its eventual class or outcome <ref> [12] </ref>. GANN is a combined genetic algorithm and neural network approach [24]. The system is a black box program that takes training, validation and test data in a simple format as input and gives a neural network that solves the classification problem specified by the training, validation and test sets.
Reference: 13. <author> R. J. McQueen, S. R. Garner, C. G. Nevill-Manning and I. H. </author> <title> Witten Applying machine learning to agricultural data. </title> <journal> Computers and Electronics in Agriculture. </journal> <volume> 12 </volume> <pages> 275-293, </pages> <year> 1994. </year>
Reference: 14. <author> G. Holmes, A. Donkin and I. H. Witten Weka: </author> <title> A machine learning workbench. </title> <booktitle> Proceedings of the 1994 Second Australian and New Zealand Conference on Intelligent Information Systems, </booktitle> <pages> pages 357-361, </pages> <address> Brisbane, Australia, </address> <year> 1994. </year> <editor> l5. R. C. </editor> <title> Holte Very simple classif cation rules perform well on most commonly-used datasets. </title> <journal> MachineLearning. </journal> <volume> 1993; 11 </volume> <pages> 63-91. </pages>
Reference-contexts: One of the most important features of the WEKA workbench is that it allows many different schemes to be run on the same dataset and for the output of each scheme to be evaluated in a consistent fashion <ref> [14] </ref>.
Reference: 16. <author> J. R. </author> <title> Quinlan Learning logical def nitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5: </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference: 17. <author> J. R. </author> <booktitle> Quinlan Determinate Literals in Inductive Logic Programming Proceedings 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 746-750, </pages> <year> 1991. </year>
Reference: 18. <author> J. R. Quinlan & R. M. Cameron-Jones FOIL: </author> <title> a midterm report. </title> <booktitle> Proceeding European Conference on Machine Learning, </booktitle> <address> p3-20, </address> <year> 1993. </year> <note> l9. </note> <author> R. J. McQueen, D. Neal, R. de War, R. and S. R. Garner. </author> <title> Preparing and processing relational data through the WEKA machine learning workbench. </title> <type> Working paper, </type> <institution> Department of Computer Science, University of Waikato, Hamilton, NewZealand. </institution> <year> 1994. </year>
Reference: 20. <author> C5.0: </author> <title> An Informal Tutorial. </title> <journal> Rulequest Research. </journal> <year> 1997. </year>
Reference-contexts: From our experiments we can see rules are often more accurate predictors than decision trees. We applied a new feature of C5.0: 10-trial boosting where ten separate decision trees or rules are combined to make predictions. Boosting can generate several classifiers rather than just one <ref> [20] </ref>. Decision Trees Rules Experiment Training Accuracy (%) Test Accuracy (%) Training Accuracy (%) Test Accuracy (%) 2 99.0 63.2 96.1 68.8 4 92.7 66.0 91.7 66.8 6 99.6 84.4 98.0 87.6 8 100.0 83.6 97.2 85.2 10 99.6 83.6 96.3 89.2 Table 3. <p> Machine learning schemes such as 1R, FOIL, InductH and C5.0 generate more useful descriptions which can be easily interpreted by human users. Schemes such as genetic algorithms <ref> [20] </ref> or neural network mentioned above, generate implicit internal models of the data which are not easily understood by human beings or other machines. There are a number of reasons why some learning schemes in our study are unable to achieve higher accuracy.
Reference: 21. <author> H. Braun, J. </author> <title> Weisbrod Evolving neural feedforward networks. </title> <booktitle> In: Artificial Neural Nets and GenetiAlgorithms. Proceedings of the International Conference. Austria. </booktitle> <pages> 25-32, </pages> <year> 1993. </year>
Reference-contexts: For many practical problems the use of neural networks has led to very satisfactory results. Nevertheless the choice of an appropriate, problem specific network architecture still remains a very poorly understood task <ref> [21] </ref>. Given an actual problem, one can choose a few different architectures, train the chosen architectures a few times and finally select the architecture with the best behavior. There may exist totally different and much more suitable topologies.
Reference: 22. <author> S. A. Harp, T. </author> <title> Samad Optimizing neural networks with genetic algorithms. </title> <booktitle> the American Power Conference. </booktitle> <address> USA. Vol.2.1138-43, </address> <year> 1992. </year>
Reference: 23. <author> D. E. </author> <title> Goldberg Genetic Algorithms in search, </title> <publisher> optimisation and machine learning Addison-Wesley Publishing Company Inc. </publisher> <address> Massachusetts. </address> <year> 1989. </year>
Reference: 24. <editor> P. Andersen, N. Damgaard and U. </editor> <title> Skyt The GANNsystem. Genetic Algorithms generating feedforward neural networks. http://www.daimi.aau.dk/~damgaard/NNSG/ 25 Du Junping,K.L.Rasmussen,J.Aagaard,B.Mayoh, T.Srensen,etc. Applications of Machine Learning: a Medical Follow Up Study. </title> <booktitle> In workshop Intelligent Data Analysis in Medicine and Pharmacology at IJCAI conference, </booktitle> <year> 1997. </year> <title> 26 S. </title> <editor> Fuglsig, J. Aagaard, M. Jnler, S. Olesen, J.P. </editor> <title> Nrgaard Survival after Transurethral Resection of the Prostate: A 10-year Followup The Jornal of Urology 151 (1994) 637-73 27 R.Kohavi, G.H.John Wrappers for feature subset selection Art.Int.97(1997)273-324. 28 A.L.Blum, P.Langley Selection of relevant features and examples in machine learning Art.Int.97(1997)245-271. </title>
Reference-contexts: Neural networks learn to distinguish different classes of events by being given information about an event and its eventual class or outcome [12]. GANN is a combined genetic algorithm and neural network approach <ref> [24] </ref>. The system is a black box program that takes training, validation and test data in a simple format as input and gives a neural network that solves the classification problem specified by the training, validation and test sets.
References-found: 21

