URL: http://vibes.cs.uiuc.edu/Publications/Papers/SC95IO.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Title: Input/Output Characteristics of Scalable Parallel Applications  
Author: Phyllis E. Crandall Ruth A. Aydt Andrew A. Chien Daniel A. Reed 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Rapid increases in computing and communication performance are exacerbating the long-standing problem of performance-limited input/output. Indeed, for many otherwise scalable parallel applications, input/output is emerging as a major performance bottleneck. The design of scalable input/output systems depends critically on the input/output requirements and access patterns for this emerging class of large-scale parallel applications. However, hard data on the behavior of such applications is only now becoming available. In this paper, we describe the input/output requirements of three scalable parallel applications (electron scattering, terrain rendering, and quantum chemistry) on the Intel Paragon XP/S. As part of an ongoing parallel input/output characterization effort, we used instrumented versions of the application codes to capture and analyze input/output volume, request size distributions, and temporal request structure. Because complete traces of individual application input/output requests were captured, in-depth, off-line analyses were possible. In addition, we conducted informal interviews of the application developers to understand the relation between the codes' current and desired input/output structure. The results of our studies show a wide variety of temporal and spatial access patterns, including highly read-intensive and write-intensive phases, extremely large and extremely small request sizes, and both sequential and highly irregular access patterns. We conclude with a discussion of the broad spectrum of access patterns and their profound implications for parallel file caching and prefetching schemes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bordawekar, R., del Rosario, J. M., and Choudhary, A. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93 (1993), </booktitle> <pages> pp. 452-461. </pages>
Reference-contexts: For example, in several cases, input/output is done by a single node sequentially, followed by data broadcast through the interconnection network. Such I/O patterns could be expressed as collective operations <ref> [1, 5, 11] </ref> to allow the filesystem to optimize performance. In general, these observations point out the importance of developing standard parallel file system API's; not only to provide functional portability, but also to provide performance portability.
Reference: [2] <author> Chen, P., Lee, E., Gibson, G., Katz, R., and Patterson, D. </author> <title> RAID: High-Performance, Reliable Secondary Storage. </title> <journal> ACM Computing Surveys 26 (June 1994), </journal> <pages> 145-185. </pages>
Reference-contexts: Moreover, commodity storage technology trends suggest that the disparity between peak processor speeds and disk transfer rates will continue to increase | the commodity disk market favors low cost, low power consumption and high capacity over high data rates. With commodity disks, only disk arrays <ref> [2] </ref> can provide the requisite peak data transfer rates.
Reference: [3] <author> Choudary, A., Bordawekar, R., Harry, M., Krishnaiyer, R., Ponnusamy, R., Singh, T., and Thakur, R. </author> <title> PASSION: Parallel And Scalable Software for Input-Output. </title> <type> Tech. rep., </type> <institution> Department of Electrical and Computer Engineering, Syracuse University, </institution> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Several research groups are now developing parallel file systems and access policies that can exploit an application developer's knowledge of access patterns. PIOUS [18] is a portable input/output system designed for use with PVM. PASSION <ref> [3] </ref> supports out-of-core algorithms in a user-level library, but focuses on a high-level array oriented interface. PPFS [8] provides user control of file cache sizes and policies, as well as data placement.
Reference: [4] <author> Corbett, P. F., Baylor, S. J., and Feitelson, D. G. </author> <title> Overview of the Vesta Parallel File System. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems (1993), </booktitle> <pages> pp. 1-16. 26 </pages>
Reference-contexts: PASSION [3] supports out-of-core algorithms in a user-level library, but focuses on a high-level array oriented interface. PPFS [8] provides user control of file cache sizes and policies, as well as data placement. Similarly, IBM's Vesta parallel file system <ref> [4] </ref> allows applications to define logical partitions, data distributions, and some access information.
Reference: [5] <author> Corbett, P. F., Feitelson, D. G., Prost, J.-P., and Baylor, S. J. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Proceedings of Supercomputing '93 (1993), </booktitle> <pages> pp. 472-481. </pages>
Reference-contexts: For example, in several cases, input/output is done by a single node sequentially, followed by data broadcast through the interconnection network. Such I/O patterns could be expressed as collective operations <ref> [1, 5, 11] </ref> to allow the filesystem to optimize performance. In general, these observations point out the importance of developing standard parallel file system API's; not only to provide functional portability, but also to provide performance portability.
Reference: [6] <author> Foster, I. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1995. </year>
Reference-contexts: 100.00 Read 51499 4,201,634,304 32,263.20 98.36 Write 207 3,849,268 5.88 0.02 Seek 813 3,495,198,798 1.67 0.00 Open 157 - 518.74 1.58 Close 156 - 11.50 0.04 Table 5: Number, size, and duration of I/O operations (HTF) O (N 2 ) one electron and O (N 4 ) two electron integrals <ref> [6] </ref>, the data volume grows dramatically with matrix size and is substantial even for small matrices. This phase is, therefore, write intensive, which shows clearly in Table 6 and Figure 12. The final, self-consistent field calculation phase is quite read intensive, with each node repeatedly reading the integral files.
Reference: [7] <author> French, J. C. </author> <title> Characterizing the Balance of Parallel I/O Systems. </title> <booktitle> In Sixth Annual Distributed Memory Computer Conference (1991), </booktitle> <pages> pp. 724-727. </pages>
Reference-contexts: Similarly, IBM's Vesta parallel file system [4] allows applications to define logical partitions, data distributions, and some access information. In addition to research efforts, several vendors have developed parallel file systems, including the Thinking Machines CM-5 Scalable Parallel File System [16, 13], the Intel Concurrent File System <ref> [7] </ref> for the iPSC/2 and iPSC/860 [19], the Intel Parallel File System for the Pargon XP/S [27], and PIOFS for the IBM SP-2. 10 Conclusions and Future Work Inefficient and immature input/output subsystems have emerged as a major performance bottleneck on scalable parallel systems.
Reference: [8] <author> Huber, Jr., J. V., Elford, C. L., Reed, D. A., Chien, A. A., and Blumen-thal, D. S. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In Proceedings of the International Conference on Supercomputing (July 1995). </booktitle>
Reference-contexts: This data indicates that parallel input/output systems must deliver high performance across broad diversity in application access patterns. Our preliminary experiences with parallel file systems <ref> [8, 9] </ref> suggests that supporting robust performance requires tuning file system policies to specific access patterns. The remainder of this paper is organized as follows. In x2-3, we summarize our approach to input/output performance characterization and its relation to the new Scalable I/O Initiative. <p> Finally, because the input/output in this application is dominated by small writes, read prefetching would benefit little. In contrast, write request aggregation and write behind could dramatically reduce the output cost. To quantify these effects, we ported the ESCAT code to PPFS, our portable parallel file system <ref> [8] </ref>, and configured the file system with write behind and global request aggregation policies. This combination of policies effectively eliminated the behavior seen in Figure 4. <p> The latter approach is promising, and demonstrations of the effectiveness such approaches are appearing <ref> [8] </ref>. Even for our set of only three application codes, no simple characterization of input/output request sizes or access patterns is viable. Further, studies show that the detailed spatial and 20 temporal characteristics of the input/output critically affect input/output performance. <p> Aggregation is feasible; as an example, the ESCAT code employs multiple writers into disjoint locations in a shared file. Individually, these requests would utilize a disk poorly, however they can be combined, significantly increasing disk efficiency <ref> [8] </ref>. This experience suggests that in some cases, two level buffering at compute nodes and input/output nodes can be beneficial. Characterization studies are by their nature inductive, covering only a small sample of the possibilities and attempting to extract more general patterns. <p> PIOUS [18] is a portable input/output system designed for use with PVM. PASSION [3] supports out-of-core algorithms in a user-level library, but focuses on a high-level array oriented interface. PPFS <ref> [8] </ref> provides user control of file cache sizes and policies, as well as data placement. Similarly, IBM's Vesta parallel file system [4] allows applications to define logical partitions, data distributions, and some access information. <p> Inherent in such an adaptive approach is the need to identify access patterns and choose policies based on access pattern characteristics. As we continue to expand our input/output characterization to a larger suite of codes, we are developing a portable parallel file system <ref> [8] </ref> that allows users to advertize expected file access patterns and to choose file distribution, caching, and prefetch policies. To lessen the cognitive burden of access specification, we have begun developing general, adaptive prefetching methods that can learn to hide input/output latency by automatically classifying and predicting access patterns.
Reference: [9] <author> Jensen, D. W. </author> <title> Disk I/O In High-Performance Computing Systems. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana-Champagne, </institution> <year> 1993. </year>
Reference-contexts: This data indicates that parallel input/output systems must deliver high performance across broad diversity in application access patterns. Our preliminary experiences with parallel file systems <ref> [8, 9] </ref> suggests that supporting robust performance requires tuning file system policies to specific access patterns. The remainder of this paper is organized as follows. In x2-3, we summarize our approach to input/output performance characterization and its relation to the new Scalable I/O Initiative.
Reference: [10] <author> Jensen, D. W., and Reed, D. A. </author> <title> File Archive Activity in a Supercomputing Environment. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing (July 1993). </booktitle>
Reference-contexts: Notable examples of this work include Lawrie and Randell's study [14] of automatic file migration algorithms for the CDC Cyber 175, Stritter's analysis [29] of file lifetime distributions, Smith's study [28] of file access behavior on IBM mainframes, and Reed and Jensen's study <ref> [10] </ref> of accesses to NCSA's file archive. More recently Miller and Katz [17] captured detailed traces of application file accesses from a suite of Cray applications from the National Center for Atmospheric Research (NCAR).
Reference: [11] <author> Kotz, D. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation (June 1994). </booktitle>
Reference-contexts: For example, in several cases, input/output is done by a single node sequentially, followed by data broadcast through the interconnection network. Such I/O patterns could be expressed as collective operations <ref> [1, 5, 11] </ref> to allow the filesystem to optimize performance. In general, these observations point out the importance of developing standard parallel file system API's; not only to provide functional portability, but also to provide performance portability.
Reference: [12] <author> Kotz, D., and Nieuwejaar, N. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94 (November 1994). </booktitle> <pages> 27 </pages>
Reference-contexts: Pasquale and Polyzos [21, 22] considered the static and dynamic file access characteristics of production vector workloads at the San Diego Supercomputer Center (SDSC) and concluded that most input/output intensive applications had regular behavior. The work by Kotz et al <ref> [12, 24] </ref> is closest in spirit to our characterization effort. Using instrumentation in the input/output libraries of the Intel iPSC/860 and the Thinking Machines CM-5, they captured traces of individual file operations and analyzed the data to extract access patterns.
Reference: [13] <author> Kwan, T. T., and Reed, D. A. </author> <title> Performance of the CM-5 Scalable File System. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Supercomputing (July 1994). </booktitle>
Reference-contexts: Similarly, IBM's Vesta parallel file system [4] allows applications to define logical partitions, data distributions, and some access information. In addition to research efforts, several vendors have developed parallel file systems, including the Thinking Machines CM-5 Scalable Parallel File System <ref> [16, 13] </ref>, the Intel Concurrent File System [7] for the iPSC/2 and iPSC/860 [19], the Intel Parallel File System for the Pargon XP/S [27], and PIOFS for the IBM SP-2. 10 Conclusions and Future Work Inefficient and immature input/output subsystems have emerged as a major performance bottleneck on scalable parallel systems.
Reference: [14] <author> Lawrie, D. H., Randal, J. M., and Barton, R. R. </author> <title> Experiments with Automatic File Migration. </title> <booktitle> IEEE Computer (July 1982), </booktitle> <pages> 45-55. </pages>
Reference-contexts: Much of the early work considered whole file access characteristics, including file sizes, lifetimes, and reuse intervals. Notable examples of this work include Lawrie and Randell's study <ref> [14] </ref> of automatic file migration algorithms for the CDC Cyber 175, Stritter's analysis [29] of file lifetime distributions, Smith's study [28] of file access behavior on IBM mainframes, and Reed and Jensen's study [10] of accesses to NCSA's file archive.
Reference: [15] <author> Li, P., Curkendall, D., Duquette, W., and Henry, H. </author> <title> Remote interactive visualization and analysis (riva) using parallel supercomputers. </title> <booktitle> In Proceedings of the 1995 Parallel Rendering Symposium (October 1995). </booktitle>
Reference-contexts: By generating these views in real-time, it is possible to conduct a "virtual flyby" of the planetary surface where scientists can interactively examine false color terrains from a variety of positions and orientations, supporting rapid exploration of large data sets. A parallel ray-identification algorithm <ref> [15] </ref> distributes terrain data among processing nodes, decomposing via the natural data parallelism, and exploiting positional derivatives to vary rendering resolution. Together these techniques achieve several frames per second on gigabyte data sets, approaching the ten frames per second needed for real-time animation.
Reference: [16] <author> LoVerso, S. J., Isman, M., Nanopoulos, A., Nesheim, W., Milne, E. D., and Wheeler, R. sfs: </author> <title> A Parallel File System for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference (1993), </booktitle> <pages> pp. 291-305. </pages>
Reference-contexts: Similarly, IBM's Vesta parallel file system [4] allows applications to define logical partitions, data distributions, and some access information. In addition to research efforts, several vendors have developed parallel file systems, including the Thinking Machines CM-5 Scalable Parallel File System <ref> [16, 13] </ref>, the Intel Concurrent File System [7] for the iPSC/2 and iPSC/860 [19], the Intel Parallel File System for the Pargon XP/S [27], and PIOFS for the IBM SP-2. 10 Conclusions and Future Work Inefficient and immature input/output subsystems have emerged as a major performance bottleneck on scalable parallel systems.
Reference: [17] <author> Miller, E. L., and Katz, R. H. </author> <booktitle> Input/Output Behavior of Supercomputing Applications. In Proceedings of Supercomputing '91 (November 1991), </booktitle> <pages> pp. 567-576. </pages>
Reference-contexts: Unfortunately, file system and storage hierarchy designers have little empirical data on parallel input/output access patterns and are often forced to extrapolate from measured access patterns on either traditional vector supercomputers <ref> [17, 21, 22] </ref> or Unix workstations [20]. Neither of these environments reflects the application usage patterns, diversity of configurations, or economic tradeoffs salient in scalable parallel systems. <p> Finally, x9 and x10 describe, respectively, related 2 work on input/output characterization and a brief summary of our experiences and directions for future research. 2 Background Though the reasons for input/output in high-performance applications are varied, they can be broadly classified as compulsory, checkpoint, or out-of-core <ref> [17] </ref>. As the name suggests, compulsory accesses are unavoidable and arise from reading initialization files, generating application output (e.g., scientific data sets or visualizations), or reading input data sets. <p> More recently Miller and Katz <ref> [17] </ref> captured detailed traces of application file accesses from a suite of Cray applications from the National Center for Atmospheric Research (NCAR). They observed that many access patterns were sequential and cyclic and that input/output operations could be classified as compulsory, checkpoint, and data staging.
Reference: [18] <author> Moyer, S. A., and Sundaram, V. S. </author> <title> PIOUS: A Scalable Parallel I/O System for Distributed Computing Environments. </title> <booktitle> In 1994 Scalable High Performance Computing Conference (May 1994), </booktitle> <pages> pp. 71-78. </pages>
Reference-contexts: Given the limitations of current parallel file systems, extrinsic knowledge is critical to understanding if certain access patterns are inherent or file system artifacts. Several research groups are now developing parallel file systems and access policies that can exploit an application developer's knowledge of access patterns. PIOUS <ref> [18] </ref> is a portable input/output system designed for use with PVM. PASSION [3] supports out-of-core algorithms in a user-level library, but focuses on a high-level array oriented interface. PPFS [8] provides user control of file cache sizes and policies, as well as data placement.
Reference: [19] <author> Nitzberg, B. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Tech. Rep. </type> <institution> RND-92-020, NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In addition to research efforts, several vendors have developed parallel file systems, including the Thinking Machines CM-5 Scalable Parallel File System [16, 13], the Intel Concurrent File System [7] for the iPSC/2 and iPSC/860 <ref> [19] </ref>, the Intel Parallel File System for the Pargon XP/S [27], and PIOFS for the IBM SP-2. 10 Conclusions and Future Work Inefficient and immature input/output subsystems have emerged as a major performance bottleneck on scalable parallel systems.
Reference: [20] <author> Ousterhout, J., et al. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the Tenth Symposium on Operating System Principles (Dec. </booktitle> <year> 1985). </year> <month> 28 </month>
Reference-contexts: Unfortunately, file system and storage hierarchy designers have little empirical data on parallel input/output access patterns and are often forced to extrapolate from measured access patterns on either traditional vector supercomputers [17, 21, 22] or Unix workstations <ref> [20] </ref>. Neither of these environments reflects the application usage patterns, diversity of configurations, or economic tradeoffs salient in scalable parallel systems.
Reference: [21] <author> Pasquale, B. K., and Polyzos, G. </author> <title> A Static Analysis of I/O Characteristics of Scientific Applications in a Production Workload. </title> <booktitle> In Proceedings of Supercomputing '93 (Novermber 1993), </booktitle> <pages> pp. 388-397. </pages>
Reference-contexts: Unfortunately, file system and storage hierarchy designers have little empirical data on parallel input/output access patterns and are often forced to extrapolate from measured access patterns on either traditional vector supercomputers <ref> [17, 21, 22] </ref> or Unix workstations [20]. Neither of these environments reflects the application usage patterns, diversity of configurations, or economic tradeoffs salient in scalable parallel systems. <p> They observed that many access patterns were sequential and cyclic and that input/output operations could be classified as compulsory, checkpoint, and data staging. Pasquale and Polyzos <ref> [21, 22] </ref> considered the static and dynamic file access characteristics of production vector workloads at the San Diego Supercomputer Center (SDSC) and concluded that most input/output intensive applications had regular behavior. The work by Kotz et al [12, 24] is closest in spirit to our characterization effort.
Reference: [22] <author> Pasquale, B. K., and Polyzos, G. </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications. </title> <booktitle> In Proceedings of Supercomputing '94 (Novermber 1994), </booktitle> <pages> pp. 660-669. </pages>
Reference-contexts: Unfortunately, file system and storage hierarchy designers have little empirical data on parallel input/output access patterns and are often forced to extrapolate from measured access patterns on either traditional vector supercomputers <ref> [17, 21, 22] </ref> or Unix workstations [20]. Neither of these environments reflects the application usage patterns, diversity of configurations, or economic tradeoffs salient in scalable parallel systems. <p> They observed that many access patterns were sequential and cyclic and that input/output operations could be classified as compulsory, checkpoint, and data staging. Pasquale and Polyzos <ref> [21, 22] </ref> considered the static and dynamic file access characteristics of production vector workloads at the San Diego Supercomputer Center (SDSC) and concluded that most input/output intensive applications had regular behavior. The work by Kotz et al [12, 24] is closest in spirit to our characterization effort.
Reference: [23] <author> Poole, J. T. </author> <title> Preliminary Survey of I/O Intensive Applications. </title> <institution> California Institute of Technology, </institution> <note> Available at http://www.ccsf.caltech.edu/SIO/SIO.html, 1994. </note>
Reference-contexts: These applications represent a snapshot of current input/output practice on scalable parallel machines and reflect the developers' input/output design choices based on perceived and actual limitations of available input/output systems. These initial codes are but a small part of the nascent Scalable Input/Output Initiative's (SIO) code suite <ref> [23] </ref>, and our initial characterization is a first step in a continuing input/output characterization effort. <p> These span a broad range of disciplines, including biology, chemistry, earth sciences, engineering, graphics, and physics <ref> [23] </ref>. Despite large differences in their underlying algorithms, the codes share two features. First, each code runs on one or more scalable parallel systems, permitting cross-machine comparisons of input/output performance. Second, all codes have both high input/output and computational requirements. In short, they typify large-scale scientific and engineering computing.
Reference: [24] <author> Purakayastha, A., Ellis, C. S., Kotz, D., Nieuwejaar, N., and Best, M. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium (April 1995). To appear. </booktitle>
Reference-contexts: Pasquale and Polyzos [21, 22] considered the static and dynamic file access characteristics of production vector workloads at the San Diego Supercomputer Center (SDSC) and concluded that most input/output intensive applications had regular behavior. The work by Kotz et al <ref> [12, 24] </ref> is closest in spirit to our characterization effort. Using instrumentation in the input/output libraries of the Intel iPSC/860 and the Thinking Machines CM-5, they captured traces of individual file operations and analyzed the data to extract access patterns.
Reference: [25] <author> Reed, D. A. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <booktitle> In Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation (May 1994), </booktitle> <pages> pp. 25-51. </pages>
Reference-contexts: As a prelude to a more detailed instrumentation of system software as part of the Scalable I/O Initiative, we have developed a suite of application input/output software instrumentation and characterization tools. This suite, an extension of the Pablo performance environment <ref> [26, 25] </ref>, brackets invocations of input/output routines with instrumentation software that captures the parameters and duration of each invocation. 3.1 Pablo Input/Output Instrumentation The Pablo performance environment consists of (a) an extensible performance data metafor-mat and associated library that separates the structure of performance data records from their semantics, (b) an
Reference: [26] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum, Ed. </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993, </year> <pages> pp. 104-113. </pages>
Reference-contexts: As a prelude to a more detailed instrumentation of system software as part of the Scalable I/O Initiative, we have developed a suite of application input/output software instrumentation and characterization tools. This suite, an extension of the Pablo performance environment <ref> [26, 25] </ref>, brackets invocations of input/output routines with instrumentation software that captures the parameters and duration of each invocation. 3.1 Pablo Input/Output Instrumentation The Pablo performance environment consists of (a) an extensible performance data metafor-mat and associated library that separates the structure of performance data records from their semantics, (b) an
Reference: [27] <author> Rudolf Berrendorf and Heribert C. Burg and Ulrich Detert and R udiger Esser and Michael Gerndt and Renate Knecht. </author> <title> Intel Paragon XP/S Architecture, Software Environment, and Performance. </title> <type> Tech. Rep. </type> <institution> KFA-ZAM-IB-9409, Forschungszentrum Julich GmbH, </institution> <year> 1994. </year>
Reference-contexts: Finally, general input/output statistics computed off-line from event traces provide means, variances, minima, maxima, and distributions of file operation durations and sizes. 3.2 Intel Paragon XP/S Using the Pablo performance instrumentation software, we measured application input/output performance on the Intel Paragon XP/S <ref> [27] </ref> at the Caltech Concurrent Supercomputing Facility (CCSF). At the time our experiments were conducted, the system had 512 computation nodes and 16 I/O nodes, each with a RAID-3 disk array composed of 5 1.2GB disks. <p> In addition to research efforts, several vendors have developed parallel file systems, including the Thinking Machines CM-5 Scalable Parallel File System [16, 13], the Intel Concurrent File System [7] for the iPSC/2 and iPSC/860 [19], the Intel Parallel File System for the Pargon XP/S <ref> [27] </ref>, and PIOFS for the IBM SP-2. 10 Conclusions and Future Work Inefficient and immature input/output subsystems have emerged as a major performance bottleneck on scalable parallel systems.
Reference: [28] <author> Smith, A. J. </author> <title> Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms. </title> <journal> IEEE Transactions on Software Engineering SE-7, </journal> <month> 4 (July </month> <year> 1981), </year> <pages> 403-417. </pages>
Reference-contexts: Much of the early work considered whole file access characteristics, including file sizes, lifetimes, and reuse intervals. Notable examples of this work include Lawrie and Randell's study [14] of automatic file migration algorithms for the CDC Cyber 175, Stritter's analysis [29] of file lifetime distributions, Smith's study <ref> [28] </ref> of file access behavior on IBM mainframes, and Reed and Jensen's study [10] of accesses to NCSA's file archive. More recently Miller and Katz [17] captured detailed traces of application file accesses from a suite of Cray applications from the National Center for Atmospheric Research (NCAR).
Reference: [29] <author> Stritter, T. R. </author> <title> File Migration. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Much of the early work considered whole file access characteristics, including file sizes, lifetimes, and reuse intervals. Notable examples of this work include Lawrie and Randell's study [14] of automatic file migration algorithms for the CDC Cyber 175, Stritter's analysis <ref> [29] </ref> of file lifetime distributions, Smith's study [28] of file access behavior on IBM mainframes, and Reed and Jensen's study [10] of accesses to NCSA's file archive.
Reference: [30] <author> Winstead, C., and McKoy, V. </author> <title> Studies of Electron-Molecule Collisions on Massively Parallel Computers. In Modern Electronic Structure Theory, </title> <editor> D. R. Yarkony, Ed., </editor> <volume> vol. 2. </volume> <publisher> World Scientific, </publisher> <year> 1994. </year> <month> 29 </month>
Reference-contexts: The Schwinger multichannel (SMC) method is an adaptation of Schwinger's variational principle for the scattering amplitude that makes it suitable for calculating low-energy electron-molecule collisions <ref> [30] </ref>. The scattering probabilities are obtained by solving linear systems whose terms include a Green's function which has no analytic form and is evaluated by numerical quadrature.
References-found: 30

