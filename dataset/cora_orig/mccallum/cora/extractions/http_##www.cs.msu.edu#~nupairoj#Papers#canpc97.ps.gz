URL: http://www.cs.msu.edu/~nupairoj/Papers/canpc97.ps.gz
Refering-URL: http://www.cs.msu.edu/~nupairoj/pub.html
Root-URL: http://www.cs.msu.edu
Email: fnupairoj,nig@cps.msu.edu  
Title: Performance Metrics and Measurement Techniques of Collective Communication Services  
Author: Natawut Nupairoj and Lionel M. Ni 
Address: East Lansing, MI 48824-1027  
Affiliation: Department of Computer Science Michigan State University  
Abstract: The performance of collective communication is critical to the overall system performance. In general, the performance of the collective communication is dependent not only on the underlying hardware, but also its implementation. To evaluate the performance of collective communication accurately, identifying the most representative metrics and using correct measurement techniques are two important issues that have to be considered. This paper focuses on the measurement techniques for collective communication services. The proposed techniques can provide an accurate evaluation of the completion time without requiring a global clock and without having to know the detailed implementations of collective communication services. Experimental results conducted on the IBM/SP at Argonne National Laboratory are presented.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. P. I. Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Collective communication is frequently invoked in parallel programs to communicate and coordinate a group of participating processors. The MPI standard <ref> [1] </ref> classifies this communication class into three subclasses: one-to-all (one source, all processors are destinations), all-to-one (all processors are sources, one destination), and all-to-all (all processors are both sources and destinations). Note that all processors in the group could be a subset of processors in the system.
Reference: 2. <author> High Performance Fortran Forum, </author> <title> "High Performance Fortran Language Specification (version 1.0, </title> <type> draft)," </type> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: By dynamically balancing the workload, programmers can perform machine-dependent tuning to allow their programs to achieve the best performance. However, dynamic workload balancing is a very complicated issue, and in some high-level data parallel programming language, such as HPF <ref> [2] </ref>, it is assumed that a vector data is evenly-distributed across all processors (e.g. block and cyclic distributions). Moreover, machine-dependent dynamic load balancing prohibits program portability due to different implementations of collective communication services.
Reference: 3. <author> D. Culler et al., </author> <title> "LogP: Towards a realistic model of parallel computation," </title> <booktitle> in Proc. of the 4th ACM SIGPLAN Sym. on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Finally, the paper is concluded in Section 7. 2 Models 2.1 Point-to-Point Parameterized Communication Model The point-to-point communication model used in this paper is the parameterized communication model which is the extension of the LogP model <ref> [3] </ref>. Basically, the model involves five parameters: sending latency (t send ), receiving latency (t recv ), network latency (t net ), holding latency (t hold ), and end-to-end latency (t end ), as shown in Figure 2. <p> The tree-based structure has been used to implement many-to-one communication services in both hardware [5] and software <ref> [3, 6] </ref>. A tree-based many-to-one communication service can be represented by a directed graph, or digraph, G (V,E) with the node set V (G) and the arc set E (G).
Reference: 4. <author> N. Nupairoj and L. Ni, </author> <title> "Benchmarking of Multicast Communication Services," </title> <type> Tech. Rep. </type> <institution> MSU-CPS-ACS-103, Department of Computer Science, Michigan State University, </institution> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Observe that t hold represents the latency of invoking the send operation, and t end reflects the delay of delivering a message across the network. In <ref> [4] </ref>, we explain how to evaluate both t hold and t end at the user-application level. 2.2 Collective Communication Flow Model In this study, we consider an implementation of the collective communications based on the MPI standard which all participating processes form a group and forward messages among them.
Reference: 5. <author> C. E. Leiserson et al., </author> <title> "The network architecture of the Connection Machine CM-5," </title> <booktitle> in Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> (San Diego, CA.), </address> <pages> pp. 272-285, </pages> <institution> Association for Computing Machinery, </institution> <year> 1992. </year>
Reference-contexts: The tree-based structure has been used to implement many-to-one communication services in both hardware <ref> [5] </ref> and software [3, 6]. A tree-based many-to-one communication service can be represented by a directed graph, or digraph, G (V,E) with the node set V (G) and the arc set E (G).
Reference: 6. <author> C. Huang and P. K. McKinley, </author> <title> "Communication Issues in Parallel Computing across ATM Networks," </title> <type> Tech. Rep. </type> <institution> MSU-CPS-94-34, Michigan State University, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The tree-based structure has been used to implement many-to-one communication services in both hardware [5] and software <ref> [3, 6] </ref>. A tree-based many-to-one communication service can be represented by a directed graph, or digraph, G (V,E) with the node set V (G) and the arc set E (G).
Reference: 7. <author> N. Nupairoj and L. M. Ni, </author> <title> "Performance Metrics and Measurement Techniques of Collective Communication Services," </title> <type> Tech. Rep. </type> <institution> MSU-CPS-ACS-96-12, Michigan State University, Department of Computer Science, East Lansing, Michigan, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: In other words, the root process is always the critical destination. We can also conclude two additional properties of the flow latency of tree-based many-to-one communication services in the following theorems. Due to the limiting spaces, the proof of these theorems can be found in our technical report <ref> [7] </ref>. Theorem 8. If a source i is the last process that calls the tree-based many-to-one communication routine, t f (i; r) t c where r is the root process of the operation. Theorem 9. <p> However, the MPI-F reduce implementation performs better than a pure binomial-based implementation. This is probably because the MPI-F reduce is customized to perform well on the IBM/SP. Due to the limiting spaces, the performance of the gather operation can be found in our technical report <ref> [7] </ref>. Fig. 6. The completion time (t c ) of three scatter algorithms with message size being 1 bytes. 7 Conclusion Evaluating the performance of collective communication services is a challenging problem. Using improper performance metrics and incorrect measurement techniques can produce results which are very misleading.
Reference: 8. <author> H. Franke, P. Hochschild, P. Pattnaik, and M. Snir, </author> <title> "MPI-F: An Efficient Implementation of MPI on IBM-SP1," </title> <booktitle> in Proceedings of the 1994 International Conference on Parallel Processing, vol. III, </booktitle> <address> (St. Charles, IL), </address> <pages> pp. 197-201, </pages> <month> Aug. </month> <year> 1994. </year> <title> Fig. 8. The completion time (t c ) of three reduce algorithms with message size being 512 bytes. </title>
Reference-contexts: Each node has an IBM/RS6000 processor (62.5 MHz) with 128 MB memory and 1 GB local disk. The peak performance is 125 MFlops per node. Both Algorithm 2 and Algorithm 3 are implemented using MPI-F library version 1.41 <ref> [8] </ref>. In order to fully utilize the high-performance switch, the library euilib with option us is used.
Reference: 9. <author> J.-Y. L. Park, H.-A. Choi, N. Nupairoj, and L. M. Ni, </author> <title> "Construction of Optimal Multicast Trees Based on the Parameterized Communication Model," </title> <booktitle> in Proceedings of the 1996 International Conference on Parallel Processing, </booktitle> <pages> pp. 180-187, </pages> <month> Aug. </month> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: However, if the sustained performance is desirable, the average values can be used instead. 6.1 One-to-Many Communication multicast, the sequential multicast, and the optimal multicast <ref> [9] </ref> when the message is 1024 bytes. When the number of processes is small, all multicasts exhibit the same performance. As the number of processes increases, the completion time of all multicasts increases. The sequential multicast shows the worst performance and does not scale.
References-found: 9

