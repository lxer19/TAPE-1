URL: http://robotics.stanford.edu/~ronnyk/disc.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fjfd,ronnyk,sahamig@CS.Stanford.EDU  
Title: Supervised and Unsupervised Discretization of Continuous Features  
Author: James Dougherty Ron Kohavi Mehran Sahami 
Note: In Armand Prieditis Stuart Russell, eds., Machine Learning: Proceedings of the Twelfth International Conference, 1995, Morgan Kaufmann Publishers,  
Address: Stanford, CA. 94305  San Francisco, CA.  
Affiliation: Computer Science Department Stanford University  
Abstract: Many supervised machine learning algorithms require a discrete feature space. In this paper, we review previous work on continuous feature discretization, identify defining characteristics of the methods, and conduct an empirical evaluation of several methods. We compare binning, an unsupervised discretization method, to entropy-based and purity-based methods, which are supervised algorithms. We found that the performance of the Naive-Bayes algorithm significantly improved when features were discretized using an entropy-based method. In fact, over the 16 tested datasets, the discretized version of Naive-Bayes slightly outperformed C4.5 on average. We also show that in some cases, the performance of the C4.5 induction algorithm significantly improved if features were discretized in advance; in our experiments, the performance never significantly degraded, an interesting phenomenon considering the fact that C4.5 is capable of locally discretiz ing features.
Abstract-found: 1
Intro-found: 1
Reference: <author> Catlett, J. </author> <year> (1991a), </year> <title> Megainduction: machine learning on very large databases, </title> <type> PhD thesis, </type> <institution> Univeristy of Sydney. </institution>
Reference: <author> Catlett, J. </author> <year> (1991b), </year> <title> On changing continuous attributes into ordered discrete attributes, </title> <editor> in Y. Kodratoff, ed., </editor> <booktitle> "Proceedings of the European Working Session on Learning", </booktitle> <address> Berlin, Germany: </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 164-178. </pages>
Reference-contexts: The advantages of discretizing during the learning process have not yet been shown. In this paper, we include such a comparison. Other reasons for variable discretization, aside from the algorithmic requirements mentioned above, include increasing the speed of induction algorithms <ref> (Catlett 1991b) </ref> and viewing General Logic Diagrams (Michalski 1978) of the induced classifier. In this paper, we address the effects of discretization on learning accuracy by comparing a range of discretiza-tion methods using C4.5 and a Naive Bayes classifier. <p> We believe that differentiating static and dynamic discretization is also important. Many discretiza-tion methods require some parameter, k, indicating the maximum number of intervals to produce in dis-cretizing a feature. Static methods, such as binning, entropy-based partitioning <ref> (Catlett 1991b, Fayyad & Irani 1993, Pfahringer 1995) </ref>, and the 1R algorithm (Holte 1993), perform one discretization pass of the data for each feature and determine the value of k for each feature independent of the other features.
Reference: <author> Chan, C.-C., Batur, C. & Srinivasan, A. </author> <year> (1991), </year> <title> Determination of quantization intervals in rule based model for dynamic systems, </title> <booktitle> in "Proceedings of the IEEE Conference on Systems, Man, and Cybernetics", Charlottesvile, Virginia, </booktitle> <pages> pp. 1719-1723. </pages>
Reference-contexts: Pfahringer (1995) uses entropy to select a large number of candidate split-points and employs a best-first search with a Minimum Description Length heuristic to determine a good discretization. Adaptive Quantizers <ref> (Chan, Batur & Srinivasan 1991) </ref> is a method combining supervised and unsupervised discretization. One begins with a binary equal width interval partitioning of the continuous feature. A set of classification rules are then induced on the discretized data (using an ID3-like algorithm) and tested for accuracy in predicting discretized outputs.
Reference: <author> Chiu, D. K. Y., Cheung, B. & Wong, A. K. C. </author> <year> (1990), </year> <title> "Information synthesis based on hierarchical entropy discretization", </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2, </journal> <pages> 117-129. </pages>
Reference: <author> Chmielewski, M. R. & Grzymala-Busse, J. W. </author> <year> (1994), </year> <title> Global discretization of continuous attributes as preprocessing for machine learning, </title> <booktitle> in "Third International Workshop on Rough Sets and Soft Computing", </booktitle> <pages> pp. 294-301. </pages>
Reference-contexts: There are three different axes by which discretization methods can be classified: global vs. local, supervised vs. unsupervised, and static vs. dynamic. Local methods, as exemplified by C4.5, produce partitions that are applied to localized regions of the instance space. Global methods <ref> (Chmielewski & Grzymala-Busse 1994) </ref>, such as binning, produce a mesh over the entire n-dimensional continuous instance space, where each feature is partitioned into regions independent of the other attributes. The mesh contains Q n i=1 k i regions, where k i is the number of partitions of the ith feature. <p> In some cases this could make effective classification much more difficult. A variation of equal frequency intervals|maximal marginal entropy| adjusts the boundaries to decrease entropy at each interval <ref> (Chmielewski & Grzymala-Busse 1994, Wong & Chiu 1987) </ref>. Holte (1993) presented a simple example of a supervised discretization method.
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993), </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning, </title> <booktitle> in "Proceedings of the 13th International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1022-1027. </pages>
Reference-contexts: All the methods presented are static discretizers. 3 Methods In our study, we consider three methods of dis-cretization in depth: equal width intervals, 1RD, the method proposed by Holte for the 1R algorithm, and the entropy minimization heuristic <ref> (Fayyad & Irani 1993, Catlett 1991b) </ref>. 3.1 Equal Width Interval Binning Equal width interval binning is perhaps the simplest method to discretize data and has often been applied as a means for producing nominal values from continuous ones.
Reference: <author> Fulton, T., Kasif, S. & Salzberg, S. </author> <year> (1994), </year> <title> An efficient algorithm for finding multi-way splits for decision trees, </title> <note> Unpublished paper. </note>
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> "Very simple classification rules perform well on most commonly used datasets", </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference-contexts: We believe that differentiating static and dynamic discretization is also important. Many discretiza-tion methods require some parameter, k, indicating the maximum number of intervals to produce in dis-cretizing a feature. Static methods, such as binning, entropy-based partitioning (Catlett 1991b, Fayyad & Irani 1993, Pfahringer 1995), and the 1R algorithm <ref> (Holte 1993) </ref>, perform one discretization pass of the data for each feature and determine the value of k for each feature independent of the other features. Dynamic methods conduct a search through the space of possible k values for all features simultaneously, thereby capturing interdependencies in feature dis-cretization.
Reference: <author> Iba, W. & Langley, P. </author> <year> (1992), </year> <title> Induction of one-level decision trees, </title> <booktitle> in "Proceedings of the Ninth International Conference on Machine Learning", </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <pages> pp. 233-240. </pages>
Reference-contexts: The method is applied to each continuous feature independently. It makes no use of instance class information whatsoever and is thus an unsupervised discretization method. 3.2 Holte's 1R Discretizer Holte (1993) describes a simple classifier that induces one-level decision trees, sometimes called decision stumps <ref> (Iba & Langley 1992) </ref>. In order to properly deal with domains that contain continuous valued features, a simple supervised discretization method is given.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/ml94.ps. </note>
Reference-contexts: In this paper, we address the effects of discretization on learning accuracy by comparing a range of discretiza-tion methods using C4.5 and a Naive Bayes classifier. The Naive-Bayes classifier is the one implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref>, which is described in Langley, Iba & Thompson (1992). There are three different axes by which discretization methods can be classified: global vs. local, supervised vs. unsupervised, and static vs. dynamic. <p> None of the methods tested was dynamic, i.e., each feature was discretized independent of other features and of the algorithm's performance. We plan to pursue wrapper methods <ref> (John, Kohavi & Pfleger 1994) </ref> that search through the space of k values, indicating the number of intervals per attribute. Another variant that could be explored is local versus global discretiza-tion based on Fayyad & Irani's method.
Reference: <author> Kerber, R. </author> <year> (1992), </year> <title> Chimerge: Discretization of nu-meric attributes, </title> <booktitle> in "Proceedings of the Tenth National Conference on Artificial Intelligence", </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 123-128. </pages>
Reference-contexts: Since these unsupervised methods do not utilize instance labels in setting partition boundaries, it is likely that classification information will be lost by binning as a result of combining values that are strongly associated with different classes into the same bin <ref> (Kerber 1992) </ref>. In some cases this could make effective classification much more difficult. A variation of equal frequency intervals|maximal marginal entropy| adjusts the boundaries to decrease entropy at each interval (Chmielewski & Grzymala-Busse 1994, Wong & Chiu 1987). Holte (1993) presented a simple example of a supervised discretization method. <p> This method appears to work reasonably well when used in conjunction with the 1R induction algorithm. The ChiMerge system <ref> (Kerber 1992) </ref> provides a statistically justified heuristic method for supervised dis-cretization. This algorithm begins by placing each observed real value into its own interval and proceeds by using the 2 test to determine when adjacent intervals should be merged.
Reference: <author> Kohavi, R. </author> <year> (1994), </year> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations, </title> <booktitle> in "Twelfth National Conference on Artificial Intelligence", </booktitle> <pages> pp. 613-618. </pages> <note> Available by anonymous ftp from Starry.Stanford.EDU:pub/ronnyk/aaai94.ps. </note>
Reference-contexts: In this paper, we address the effects of discretization on learning accuracy by comparing a range of discretiza-tion methods using C4.5 and a Naive Bayes classifier. The Naive-Bayes classifier is the one implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref>, which is described in Langley, Iba & Thompson (1992). There are three different axes by which discretization methods can be classified: global vs. local, supervised vs. unsupervised, and static vs. dynamic. <p> None of the methods tested was dynamic, i.e., each feature was discretized independent of other features and of the algorithm's performance. We plan to pursue wrapper methods <ref> (John, Kohavi & Pfleger 1994) </ref> that search through the space of k values, indicating the number of intervals per attribute. Another variant that could be explored is local versus global discretiza-tion based on Fayyad & Irani's method.
Reference: <author> Kohavi, R. </author> <year> (1995), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <booktitle> in "Proceedings of the 14th International Joint Conference on Artificial Intelligence". </booktitle>
Reference-contexts: We chose sixteen datasets from the U.C. Irvine repository (Murphy & Aha 1994) that each had at least one continuous feature. For the datasets that had more than 3000 test instances, we ran a single train/test experiment and report the theoretical standard deviation estimated using the Binomial model <ref> (Kohavi 1995) </ref>. For the remaining datasets, we ran five-fold cross-validation and report the standard deviation of the cross-validation. Table 2 describes the datasets with the last column showing the accuracy of predicting the majority class on the test set.
Reference: <institution> Available by anonymous ftp from starry.Stanford.EDU:pub/ronnyk/accEst.ps. </institution>
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in "Tools with Artificial Intelligence", </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <note> Available by anonymous ftp from: starry.Stanford.EDU:pub/ronnyk/mlc/ toolsmlc.ps. </note>
Reference-contexts: In this paper, we address the effects of discretization on learning accuracy by comparing a range of discretiza-tion methods using C4.5 and a Naive Bayes classifier. The Naive-Bayes classifier is the one implemented in MLC ++ <ref> (Kohavi, John, Long, Manley & Pfleger 1994) </ref>, which is described in Langley, Iba & Thompson (1992). There are three different axes by which discretization methods can be classified: global vs. local, supervised vs. unsupervised, and static vs. dynamic. <p> None of the methods tested was dynamic, i.e., each feature was discretized independent of other features and of the algorithm's performance. We plan to pursue wrapper methods <ref> (John, Kohavi & Pfleger 1994) </ref> that search through the space of k values, indicating the number of intervals per attribute. Another variant that could be explored is local versus global discretiza-tion based on Fayyad & Irani's method.
Reference: <author> Kohonen, T. </author> <year> (1989), </year> <title> Self-Organization and Associative Memory, </title> <address> Berlin, Germany: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This method has yet to be tested experimentally. Vector Quantization <ref> (Kohonen 1989) </ref> is also related to the notion of discretization. This method attempts to partition an N -dimensional continuous space into a Voronoi Tessellation and then represent the set of points in each region by the region into which it falls.
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of bayesian classifiers, </title> <booktitle> in "Proceedings of the tenth national conference on artificial intelligence", </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference-contexts: The method is applied to each continuous feature independently. It makes no use of instance class information whatsoever and is thus an unsupervised discretization method. 3.2 Holte's 1R Discretizer Holte (1993) describes a simple classifier that induces one-level decision trees, sometimes called decision stumps <ref> (Iba & Langley 1992) </ref>. In order to properly deal with domains that contain continuous valued features, a simple supervised discretization method is given.
Reference: <author> Maass, W. </author> <year> (1994), </year> <title> Efficient agnostic pac-learning with simple hypotheses, </title> <booktitle> in "Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory", </booktitle> <pages> pp. 67-75. </pages>
Reference: <author> Michalski, R. S. </author> <year> (1978), </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions, </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference-contexts: The advantages of discretizing during the learning process have not yet been shown. In this paper, we include such a comparison. Other reasons for variable discretization, aside from the algorithmic requirements mentioned above, include increasing the speed of induction algorithms (Catlett 1991b) and viewing General Logic Diagrams <ref> (Michalski 1978) </ref> of the induced classifier. In this paper, we address the effects of discretization on learning accuracy by comparing a range of discretiza-tion methods using C4.5 and a Naive Bayes classifier.
Reference: <author> Michalski, R. S. & Stepp, R. E. </author> <year> (1983), </year> <title> Learning from observations: Conceptual clustering, </title> <editor> in T. M. </editor> <publisher> M. </publisher>
Reference-contexts: 1 Introduction Many algorithms developed in the machine learning community focus on learning in nominal feature spaces <ref> (Michalski & Stepp 1983, Kohavi 1994) </ref>. However, many real-world classification tasks exist that involve continuous features where such algorithms could not be applied unless the continuous features are first dis-cretized. Continuous variable discretization has received significant attention in the machine learning community only recently.
Reference: <editor> R. S. Michalski, J. G. Carbonell, ed., </editor> <booktitle> "Machine Learning: An Artificial Intelligence Approach", </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto. </address>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <title> UCI repository of machine learning databases, For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: The heuristic was chosen based on examining S-plus's histogram binning algorithm (Spector 1994). We chose sixteen datasets from the U.C. Irvine repository <ref> (Murphy & Aha 1994) </ref> that each had at least one continuous feature. For the datasets that had more than 3000 test instances, we ran a single train/test experiment and report the theoretical standard deviation estimated using the Binomial model (Kohavi 1995).
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990), </year> <title> "Boolean feature discovery in empirical learning", </title> <booktitle> Machine Learning 5, </booktitle> <pages> 71-99. </pages>
Reference-contexts: C4.5's performance was significantly improved on two datasets|cleve and diabetes|using the entropy dis-cretization method and did not significantly degrade on any dataset, although it did decrease slightly on some. The entropy-based discretization is a global method and does not suffer from data fragmentation <ref> (Pagallo & Haussler 1990) </ref>.
Reference: <author> Pfahringer, B. </author> <year> (1995), </year> <title> Compression-based discretiza-tion of continuous attributes, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> "Proceedings of the Twelfth International Conference on Machine Learning", </booktitle> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, Califor-nia. </address>
Reference-contexts: Often, uniform binning of the data is used to produce the necessary data transformations for a learning algorithm, and no careful study of how this discretization affects the learning process is performed (Weiss & Kulikowski 1991). In decision tree methods, such as C4.5 <ref> (Quinlan 1993) </ref>, continuous values are discretized during the learning process. The advantages of discretizing during the learning process have not yet been shown. In this paper, we include such a comparison. <p> For the remaining datasets, we ran five-fold cross-validation and report the standard deviation of the cross-validation. Table 2 describes the datasets with the last column showing the accuracy of predicting the majority class on the test set. Table 3 shows the accuracies of the C4.5 induction algorithm <ref> (Quinlan 1993) </ref> using the different discretization methods. Table 4 shows the accuracies of the Naive-Bayes induction algorithm. Figure 1 shows a line plot of two discretization methods: log `-binning and entropy.
Reference: <author> Richeldi, M. & Rossotto, M. </author> <year> (1995), </year> <title> Class-driven statistical discretization of continuous attributes (extended abstract), </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> "Machine Learning: ECML-95 (Proc. Euro-pean Conf. on Machine Learning, 1995)", Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Ver-lag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 335 - 338. </pages>
Reference: <author> Rissanen, J. </author> <year> (1986), </year> <title> "Stochastic complexity and modeling", </title> <journal> Ann. </journal> <volume> Statist 14, </volume> <pages> 1080-1100. </pages>
Reference: <author> Spector, P. </author> <year> (1994), </year> <title> An Introduction to S and S-PLUS, </title> <publisher> Duxbury Press. </publisher>
Reference-contexts: The number of bins, k, in the equal width interval discretization was set to both k = 10 and k = maxf1; 2 log `g, where l is the number of distinct observed values for each attribute. The heuristic was chosen based on examining S-plus's histogram binning algorithm <ref> (Spector 1994) </ref>. We chose sixteen datasets from the U.C. Irvine repository (Murphy & Aha 1994) that each had at least one continuous feature.
Reference: <author> Ting, K. M. </author> <year> (1994), </year> <title> Discretization of continuous-valued attributes and instance-based learning, </title> <type> Technical Report 491, </type> <institution> University of Sydney. </institution>
Reference-contexts: In the original paper, this method was applied locally at each node during tree generation. The method was found to be quite promising as a global discretization method <ref> (Ting 1994) </ref>, and in this paper the method is used for global discretiza-tion. Pfahringer (1995) uses entropy to select a large number of candidate split-points and employs a best-first search with a Minimum Description Length heuristic to determine a good discretization.
Reference: <author> Van de Merckt, T. </author> <year> (1993), </year> <title> Decision trees in numerical attribute spaces, </title> <booktitle> in "Proceedings of the 13th International Joint Conference on Artificial Intelligence", </booktitle> <pages> pp. 1016-1021. </pages>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA. </address>
Reference-contexts: Continuous variable discretization has received significant attention in the machine learning community only recently. Often, uniform binning of the data is used to produce the necessary data transformations for a learning algorithm, and no careful study of how this discretization affects the learning process is performed <ref> (Weiss & Kulikowski 1991) </ref>. In decision tree methods, such as C4.5 (Quinlan 1993), continuous values are discretized during the learning process. The advantages of discretizing during the learning process have not yet been shown. In this paper, we include such a comparison.
Reference: <author> Weiss, S. M., Galen, R. S. & Tadepalli, P. V. </author> <year> (1990), </year> <title> "Maximizing the predicative value of production rules", </title> <booktitle> Artificial Intelligence 45, </booktitle> <pages> 47-71. </pages>
Reference-contexts: Chmielewski & Grzymala-Busse (1994) have taken a similar approach using a cluster-based method to find candidate interval boundaries and then applying an entropy-based consistency function from the theory of Rough Sets to evaluate these intervals. The Predicative Value Maximization algorithm <ref> (Weiss, Galen & Tadepalli 1990) </ref> makes use of a supervised discretization method by finding partition boundaries with locally maximal predictive values|those most likely to make correct classification decisions. The search for such boundaries begins at a coarse level and is refined over time to find locally optimal partition boundaries.
Reference: <author> Wong, A. K. C. & Chiu, D. K. Y. </author> <year> (1987), </year> <title> Synthesizing statistical knowledge from incomplete mixed-mode data, </title> <journal> in "IEEE Transaction on Pattern Analysis and Machine Intelligence 9", </journal> <pages> pp. 796-805. </pages>
References-found: 33

