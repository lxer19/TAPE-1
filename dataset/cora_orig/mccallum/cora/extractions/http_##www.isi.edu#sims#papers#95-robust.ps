URL: http://www.isi.edu/sims/papers/95-robust.ps
Refering-URL: http://www.isi.edu/~knoblock/
Root-URL: 
Email: fchunnan,knoblockg@isi.edu  
Title: Estimating the Robustness of Discovered Knowledge  
Author: Chun-Nan Hsu and Craig A. Knoblock 
Note: To appear in the Proceedings of the First International Conference on Knowledge Discovery and Data  
Address: 4676 Admiralty Way Marina del Rey, CA 90292  Montreal, Canada, 1995.  
Affiliation: Information Sciences Institute and Department of Computer Science University of Southern California  Mining,  
Abstract: This paper introduces a new measurement, robustness, to measure the quality of machine-discovered knowledge from real-world databases that change over time. A piece of knowledge is robust if it is unlikely to become inconsistent with new database states. Robustness is different from predictive accuracy in that by the latter, the system considers only the consistency of a rule with unseen data, while by the former, the consistency after deletions and updates of existing data is also considered. Combining robustness with other utility measurements, a system can make intelligent decisions in learning and maintenance of knowledge learned from changing databases. This paper defines robustness, then presents an estimation approach for the robustness of Horn-clause rules learned from a relational database. The estimation approach applies the Laplace law of succession, which can be efficiently computed. The estimation is based on database schemas and transaction logs. No domain-specific information is required. However, if it is available, the approach can exploit it. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ambite, J.-L., and Knoblock, C. A. </author> <year> 1995. </year> <title> Reconciling distributed information sources. </title> <booktitle> In Working Notes of the AAAI Spring Symposium on Information Gathering in Distributed Heterogeneous Environments. </booktitle>
Reference-contexts: We are currently working on applying our approach to learning for semantic query optimization, as described earlier in this paper. The approach can also be applied to other database applications, such as view management, intelligent database caching (Arens & Knoblock 1994), and learning for the integration of heterogeneous multidatabases <ref> (Ambite & Knoblock 1995) </ref>. These applications require the system to extract a compressed description (e.g., a view definition) of data, and the consistency of the description with the database is important. Robustness can guide the system to extract robust descriptions so that they can be used with minimal maintenance effort.
Reference: <author> Arens, Y., and Knoblock, C. A. </author> <year> 1994. </year> <title> Intelligent caching: Selecting, representing, and reusing data in an information server. </title> <booktitle> In Proceedings of the Third International Conference on Information and Knowledge Management. </booktitle>
Reference-contexts: We are currently working on applying our approach to learning for semantic query optimization, as described earlier in this paper. The approach can also be applied to other database applications, such as view management, intelligent database caching <ref> (Arens & Knoblock 1994) </ref>, and learning for the integration of heterogeneous multidatabases (Ambite & Knoblock 1995). These applications require the system to extract a compressed description (e.g., a view definition) of data, and the consistency of the description with the database is important.
Reference: <author> Cestnik, B., and Bratko, I. </author> <year> 1991. </year> <title> On estimating probabilities in tree pruning. </title> <booktitle> In Machine Learning - EWSL-91, European Working Session on Learning. </booktitle> <address> Berlin, Germany: </address> <publisher> Springer-Verlag. </publisher> <pages> 138-150. </pages>
Reference-contexts: The Laplace law applies to any repeatable experiments that can be performed as many times as required. An example of a repeatable experiment is tossing a coin. The Laplace law is a special case of a modified estimate called m-Probability <ref> (Cestnik & Bratko 1991) </ref>. A prior probability of outcomes can be brought to bear in this more general estimate. Theorem 2 (m-Probability) Let r, n, and C be as in Theorem 1.
Reference: <author> Helmbold, D. P., and Long, P. M. </author> <year> 1994. </year> <title> Tracking drifting concepts by minimizing disagreement. </title> <booktitle> Machine Learning 14 </booktitle> <pages> 27-45. </pages>
Reference: <author> Howson, C., and Urbach, P. </author> <year> 1988. </year> <title> Scientific Reasoning: The Bayesian Approach. </title> <publisher> Open Court. </publisher>
Reference-contexts: The probability that the outcome of the next experiment will be C can be esti mated as r + 1 . Detailed description and a proof of the Laplace law of succession can be found in <ref> (Howson & Urbach 1988) </ref>. The Laplace law applies to any repeatable experiments that can be performed as many times as required. An example of a repeatable experiment is tossing a coin. The Laplace law is a special case of a modified estimate called m-Probability (Cestnik & Bratko 1991).
Reference: <author> Hsu, C.-N., and Knoblock, C. A. </author> <year> 1993a. </year> <title> Learning database abstractions for query reformulation. </title> <booktitle> In Proceedings of the AAAI Workshop on Knowledge Discovery in Databases. </booktitle>
Reference: <author> Hsu, C.-N., and Knoblock, C. A. </author> <year> 1993b. </year> <title> Reformulating query plans for multidatabase systems. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management. </booktitle> <address> Washing-ton, D.C.: </address> <publisher> ACM. </publisher>
Reference: <author> Hsu, C.-N., and Knoblock, C. A. </author> <year> 1994. </year> <title> Rule induction for semantic query optimization. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference: <author> Hsu, C.-N., and Knoblock, C. A. </author> <year> 1995. </year> <title> Using inductive learning to generate rules for semantic query optimization. </title> <editor> In Piatetsky-Shapiro, G., and Fayyad, U., eds., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press. </publisher> <address> Chapter 17. </address>
Reference: <author> King, J. J. </author> <year> 1981. </year> <title> Query Optimization by Semantic Reasoning. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, Department of Computer Science. </institution>
Reference: <author> Lavrac, N., and Dzeroski, S. </author> <year> 1994. </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference: <author> Lloyd, J. W. </author> <year> 1987. </year> <booktitle> Foundations of Logic Programming. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Knowledge is expressed in Horn-clause rules in this paper. Table 1 shows some Horn-clause rules describing the data. We adopt standard Prolog terminology and semantics as defined in <ref> (Lloyd 1987) </ref> in our discussion of rules. In addition, we refer to literals on database relations as database literals (e.g., seaport ( ,?glc cd,?storage, , , )), and literals on built-in relations as built-in literals (e.g., ?latitude 35.89).
Reference: <author> Piatetsky-Shapiro, G. </author> <year> 1984. </year> <title> A Self-Organizing Database System A Different Approach To Query Optimization. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, New York University. </institution>
Reference: <author> Siegel, M. D. </author> <year> 1988. </year> <title> Automatic rule derivation for semantic query optimization. </title> <editor> In Kerschberg, L., ed., </editor> <booktitle> Proceedings of the Second International Conference on Expert Database Systems. </booktitle> <institution> Fairfax, VA: George Mason Foundation. </institution> <month> 371-385. </month>
Reference: <author> Ullman, J. D. </author> <year> 1988. </year> <title> Principles of Database and Knowledge-base Systems, </title> <booktitle> Volume II. </booktitle> <address> Palo Alto, CA: </address> <publisher> Computer Science Press. </publisher>
Reference: <author> Widmer, G., and Kubat, M. </author> <year> 1993. </year> <title> Effective learning in dynamic environments by explicit context tracking. </title> <booktitle> In Machine Learning: </booktitle> <address> ECML-93. Berlin: </address> <publisher> Springer-Verlag. </publisher> <pages> 6 </pages>
References-found: 16

