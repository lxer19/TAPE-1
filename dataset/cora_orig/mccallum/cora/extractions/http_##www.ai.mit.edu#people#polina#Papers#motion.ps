URL: http://www.ai.mit.edu/people/polina/Papers/motion.ps
Refering-URL: http://www.ai.mit.edu/people/polina/Papers/
Root-URL: 
Title: Motion from Color  
Author: P. Golland and A. M. Bruckstein 
Address: 32000, Israel.  
Affiliation: Center for Intelligent Systems, Computer Science Department, Technion, I.I.T., Haifa  
Abstract: The use of color images for motion estimation is investigated in this work. Beyond the straightforward approach of using the color components as separate images of the same scene, a new method, based on exploiting color invariance under motion, is discussed. Two different sets of color-related, locally computable motion `invariants' are analyzed and tested in this paper, and the results of motion estimation based on them are compared to the direct use of the RGB brightness functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barron, D. Fleet, and S. Beauchemin. </author> <title> Performance of optical flow techniques. </title> <journal> International Journal of Computer Vision, </journal> <volume> 12(2), </volume> <year> 1994. </year> <month> pp.43-77. </month>
Reference-contexts: Both synthetic and real image sequences were used for the testing. The results obtained by methods using color functions were compared with the results provided by using brightness functions for motion estimation. Barron et al. <ref> [1] </ref> implemented a number of existing techniques for optical flow estimation in order to compare their performance. This work provides a lot of useful information about the practical aspects of the optical flow estimation problem. We based our implementation on this work. <p> An object with some color pattern on it was attached to a robot hand, which was moving in various ways while the image sequences were taken. Since it is very difficult to determine the true image flow for real images [15], usually only qualitative testing is performed <ref> [1] </ref>. Figures 9-11 illustrate some of the experiments carried out in order to test the proposed methods. 11 methods are reported: using RGB, normalized rgb and HSV quantities respectively.
Reference: [2] <author> G. Buchsbaum. </author> <title> A spatial processor model for object color perception. </title> <journal> Journal of Franklin Inst., </journal> <volume> 310, </volume> <year> 1980. </year>
Reference-contexts: If all detectors are exposed to the same input spectrum S (), the color perceived by the vision system is determined by three non-negative numbers, denoted by (R; G; B), obtained from the formulae <ref> [2, 6, 19] </ref>: R = S () D r () d; R B = S () D b () d; the integration being over the visible range of wavelength ([400nm; 700nm] for the human vision system). <p> Ideally, we should have used a color constancy algorithm to extract the `true' color characteristics from the image and to use them as invariants for motion estimation. There has been some research done on exact separation of color from brightness <ref> [2, 16, 4] </ref>, but no definite method, that would incorporate the most general reflectance model, has yet been proposed.
Reference: [3] <author> M. Campani and A. Verri. </author> <title> Computing optical flow from an overconstrained system of linear algebraic equations. </title> <booktitle> In Proceedings of the Third International Conference on Computer Vision ICCV'90, </booktitle> <address> Osaka, Japan, </address> <year> 1990. </year> <month> pp.22-26. </month>
Reference-contexts: This approach was used by Lucas and Kanade [9] (constant model), Campani and Verri <ref> [3] </ref> (linear in space and constant in time) and Otte and Nagel [15] (linear both in space and in time).
Reference: [4] <author> G. Finlayson, B. Funt, and K. Barnard. </author> <title> Color constancy under varying illumination. </title> <booktitle> In Proceedings of The 5th International Conference on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <year> 1995. </year> <month> pp.720-725. </month>
Reference-contexts: Ideally, we should have used a color constancy algorithm to extract the `true' color characteristics from the image and to use them as invariants for motion estimation. There has been some research done on exact separation of color from brightness <ref> [2, 16, 4] </ref>, but no definite method, that would incorporate the most general reflectance model, has yet been proposed.
Reference: [5] <author> R. Haralick and J. Lee. </author> <title> The facet approach to optic flow. </title> <booktitle> In Proceedings Image Understanding Workshop. Editor: L.S. Baumann, Science Applications, </booktitle> <address> Arlington, VA, </address> <year> 1983. </year> <month> pp.84-93. </month>
Reference-contexts: This drawback was subsequently corrected by Nagel [11] via the introduction of a weighted optimization process, in which smoothness of the velocity field was required only for the optical flow component normal to the intensity gradient in places where its magnitude was large (edges). Haralick and Lee <ref> [5] </ref> then proposed a method using higher order derivatives of the image intensity. They assumed that not only the intensity function itself, but also its first order derivatives remain unchanged under motion, obtaining three additional equations similar to (3).
Reference: [6] <author> B. Horn. </author> <title> Exact reproduction of colored images. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 26, </volume> <year> 1984. </year> <month> pp.135-167. </month>
Reference-contexts: If all detectors are exposed to the same input spectrum S (), the color perceived by the vision system is determined by three non-negative numbers, denoted by (R; G; B), obtained from the formulae <ref> [2, 6, 19] </ref>: R = S () D r () d; R B = S () D b () d; the integration being over the visible range of wavelength ([400nm; 700nm] for the human vision system).
Reference: [7] <author> B. Horn and B. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17, </volume> <year> 1981. </year> <month> pp.185-203. </month>
Reference-contexts: To do this, however, seems impossible without additional information about the motion field. Instead, we can extract the so-called optical flow, which is a 2D field of velocities associated with the variation of brightness patterns of the image <ref> [7, 17] </ref>. The following two examples are often given to help understand the difference between an image flow and an optical flow. A uniformly painted ball is rotating around its center in some way. <p> Gradient based methods address this problem in various ways. Horn and Schunck <ref> [7] </ref> were the first to use a smoothness constraint on the optical field.
Reference: [8] <author> A. Jennings. </author> <title> Matrix Computation for Engineers and Scientists. </title> <publisher> John Wiley & Sons, Ltd., </publisher> <year> 1980. </year>
Reference-contexts: It is common to use the so-called condition number of the coefficient matrix of a system (in our case A T A) as a measure of confidence in the solution of this system. The formal definition of the condition number k of a matrix B is the following <ref> [14, 8] </ref>: k (B) = kBk kB 1 k; if B is non-singular +1; if B is singular (10) where kBk is the norm of the matrix B equal to the maximal eigenvalue of the matrix B T B.
Reference: [9] <author> B. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proceedings of DARPA Image Understanding Workshop, 1981. pp.121-130. </booktitle> <pages> 22 </pages>
Reference-contexts: This approach was used by Lucas and Kanade <ref> [9] </ref> (constant model), Campani and Verri [3] (linear in space and constant in time) and Otte and Nagel [15] (linear both in space and in time).
Reference: [10] <author> A. Mitiche, Y. Wang, and J. Aggarwal. </author> <title> Experiments in computing optical flow with the gradient-based, multiconstraint method. </title> <journal> Pattern Recognition, </journal> <volume> 20(2), </volume> <year> 1987. </year> <month> pp.173-179. </month>
Reference-contexts: Each one of these methods tries to find and extract more than one function invariant under motion and thereby determine the optical flow components using a number of constraints based on those invariants. This approach was used by Wohn, Davis and Thrift [21], Mitiche et al. <ref> [10] </ref>, Ohta [13], 3 each one of them using different invariants such as brightness averages, medians, curvature or the RGB color channels. The methods discussed in this work use the multiple constraint approach for image flow estimation.
Reference: [11] <author> H.-H. Nagel. </author> <title> Constraints for the estimation of displacement vector fields from image sequences. </title> <booktitle> In Proceedings IJCAI-83, </booktitle> <address> Karlsruhe, Germany, </address> <year> 1983. </year> <month> pp.945-951. </month>
Reference-contexts: Their method provided quite satisfactory results, but failed on edges. This drawback was subsequently corrected by Nagel <ref> [11] </ref> via the introduction of a weighted optimization process, in which smoothness of the velocity field was required only for the optical flow component normal to the intensity gradient in places where its magnitude was large (edges).
Reference: [12] <author> H.-H. Nagel. </author> <title> On the estimation of optical flow: relations between different approaches and some new results. </title> <journal> Artificial Intelligence, </journal> <volume> 33, </volume> <year> 1987. </year> <month> pp.299-324. </month>
Reference-contexts: They assumed that not only the intensity function itself, but also its first order derivatives remain unchanged under motion, obtaining three additional equations similar to (3). A slightly different method also using higher order derivatives was developed by Tretiak and Pastor [18]. In <ref> [12] </ref> Nagel showed that these two methods were particular cases of a more general approach, and developed a generalized scheme for optical flow estimation using higher order derivatives.
Reference: [13] <author> N. Ohta. </author> <title> Optical flow detection by color images. </title> <booktitle> In Proceedings of IEEE International Conference on Image Processing, Pan Pacific,Singapore, </booktitle> <year> 1989. </year> <month> pp.801-805. </month>
Reference-contexts: Each one of these methods tries to find and extract more than one function invariant under motion and thereby determine the optical flow components using a number of constraints based on those invariants. This approach was used by Wohn, Davis and Thrift [21], Mitiche et al. [10], Ohta <ref> [13] </ref>, 3 each one of them using different invariants such as brightness averages, medians, curvature or the RGB color channels. The methods discussed in this work use the multiple constraint approach for image flow estimation. <p> To summarize, both the velocity vector and the condition number of the matrix A T A are computed at each point of the image and then analyzed in order to get a final result. 5 ' fl A similar method was first proposed by Ohta <ref> [13] </ref>. He suggested using pairs of color channels in order to get two equations for the two unknown components of the image flow. However, he did not test this method experimentally, neither proposed ways to combine the results obtained from different pairs of equations.
Reference: [14] <author> J. Ortega. </author> <title> Numerical Analysis: A Second Course. </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, USA, </address> <year> 1990. </year>
Reference-contexts: It is common to use the so-called condition number of the coefficient matrix of a system (in our case A T A) as a measure of confidence in the solution of this system. The formal definition of the condition number k of a matrix B is the following <ref> [14, 8] </ref>: k (B) = kBk kB 1 k; if B is non-singular +1; if B is singular (10) where kBk is the norm of the matrix B equal to the maximal eigenvalue of the matrix B T B.
Reference: [15] <author> M. Otte and H.-H. Nagel. </author> <title> Optical flow estimation: advances and comparisons. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 800, </volume> <year> 1994. </year> <month> pp.51-60. </month>
Reference-contexts: This approach was used by Lucas and Kanade [9] (constant model), Campani and Verri [3] (linear in space and constant in time) and Otte and Nagel <ref> [15] </ref> (linear both in space and in time). The velocity components at a certain point of the chosen neighborhood were expanded into Taylor series relatively to the central point of the neighborhood and then substituted into the major optical flow constraint (3). <p> Real images were obtained using a color camera. An object with some color pattern on it was attached to a robot hand, which was moving in various ways while the image sequences were taken. Since it is very difficult to determine the true image flow for real images <ref> [15] </ref>, usually only qualitative testing is performed [1]. Figures 9-11 illustrate some of the experiments carried out in order to test the proposed methods. 11 methods are reported: using RGB, normalized rgb and HSV quantities respectively.
Reference: [16] <author> S. Shafer. </author> <title> Using color to separate reflection components. Color: Research and Application, </title> <type> 10(4), </type> <year> 1985. </year> <month> pp.210-218. </month>
Reference-contexts: Ideally, we should have used a color constancy algorithm to extract the `true' color characteristics from the image and to use them as invariants for motion estimation. There has been some research done on exact separation of color from brightness <ref> [2, 16, 4] </ref>, but no definite method, that would incorporate the most general reflectance model, has yet been proposed.
Reference: [17] <author> A. Singh. </author> <title> Optic Flow Computation: A Unified Perspective. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, USA, </address> <year> 1991. </year>
Reference-contexts: To do this, however, seems impossible without additional information about the motion field. Instead, we can extract the so-called optical flow, which is a 2D field of velocities associated with the variation of brightness patterns of the image <ref> [7, 17] </ref>. The following two examples are often given to help understand the difference between an image flow and an optical flow. A uniformly painted ball is rotating around its center in some way. <p> It is immediately recognized that a problem arises due to the difference between these two fields. A number of authors <ref> [17, 20] </ref> investigated the connection between them, introducing constraints on motions and surface properties in order to either satisfy assumptions that make the image flow and the optical flow identical, or propose methods to obtain the true image flow. <p> A similar observation was made by Singh <ref> [17] </ref>, who showed theoretically that the optical flow is equal to the translation component of the image flow for objects with lambertian surface reflectivity.
Reference: [18] <author> O. Tretiak and L. </author> <title> Pastor. Velocity estimations from image sequences with second order differential operators. </title> <booktitle> In Proceedings International Conference on Pattern Recognition. </booktitle> <address> Montreal, Canada, </address> <year> 1984. </year> <month> pp.16-19. </month>
Reference-contexts: They assumed that not only the intensity function itself, but also its first order derivatives remain unchanged under motion, obtaining three additional equations similar to (3). A slightly different method also using higher order derivatives was developed by Tretiak and Pastor <ref> [18] </ref>. In [12] Nagel showed that these two methods were particular cases of a more general approach, and developed a generalized scheme for optical flow estimation using higher order derivatives.
Reference: [19] <author> H. Trussel. </author> <title> DSP solutions run the gamut for color systems. </title> <journal> IEEE Signal Processing Magazine, </journal> <month> April, </month> <year> 1993. </year> <month> pp.8-23. </month>
Reference-contexts: If all detectors are exposed to the same input spectrum S (), the color perceived by the vision system is determined by three non-negative numbers, denoted by (R; G; B), obtained from the formulae <ref> [2, 6, 19] </ref>: R = S () D r () d; R B = S () D b () d; the integration being over the visible range of wavelength ([400nm; 700nm] for the human vision system).
Reference: [20] <author> A. Verri and T. Poggio. </author> <title> Against quantitative optical flow. </title> <booktitle> In Proceedings of First International Conference on Computer Vision, </booktitle> <address> London, </address> <year> 1987. </year> <month> pp.171-180. </month>
Reference-contexts: It is immediately recognized that a problem arises due to the difference between these two fields. A number of authors <ref> [17, 20] </ref> investigated the connection between them, introducing constraints on motions and surface properties in order to either satisfy assumptions that make the image flow and the optical flow identical, or propose methods to obtain the true image flow.
Reference: [21] <author> K. Wohn, L. Davis, and P. Thrift. </author> <title> Motion estimation based on multiple local constraints and nonlinear smoothing. </title> <journal> Pattern Recognition, </journal> <volume> 16(6), </volume> <year> 1983. </year> <month> pp.563-570. </month>
Reference-contexts: Each one of these methods tries to find and extract more than one function invariant under motion and thereby determine the optical flow components using a number of constraints based on those invariants. This approach was used by Wohn, Davis and Thrift <ref> [21] </ref>, Mitiche et al. [10], Ohta [13], 3 each one of them using different invariants such as brightness averages, medians, curvature or the RGB color channels. The methods discussed in this work use the multiple constraint approach for image flow estimation.
References-found: 21

