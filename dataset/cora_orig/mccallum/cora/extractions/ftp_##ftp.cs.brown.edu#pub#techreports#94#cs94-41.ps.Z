URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-41.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-41.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Adams, E. Balas, and D. Zawack. </author> <title> The shifting bottleneck procedure for job shop scheduling. </title> <journal> Management Science, </journal> <volume> 34(3) </volume> <pages> 391-401, </pages> <year> 1988. </year>
Reference-contexts: Instead, we generate policies in two phases. We first employ Monte Carlo simulation and constrained dispatch scheduling using a stochastic model of the environment to sample the large state/action space and provide default reflexes to handle the remaining states. We then apply bottleneck-centered scheduling heuristics <ref> [1, 29] </ref> to improve initial policies. Deliberation scheduling is used to allocate computation time across these two phases. This work is described in detail in [21, 20]. <p> The second phase analyzes the results of the first phase in order to detect areas for improvement (bottlenecks) within the policy and adds additional constraints on the action space in order to mitigate the bottlenecks and improve subsequent policies. Our technique for propagating constraints is based on bottleneck-centered heuristics <ref> [1, 29] </ref>, a form of opportunistic scheduling in which critical points of resource contention are located and used to identify and remove potential for negative interactions between activities. Bottleneck-centered heuristics have demonstrated their effectiveness in deployed scheduling solutions [29]. We employ bottleneck-centered heuristics through two main steps. <p> Our work on large action spaces also borrows from and extends the operations research literature on bottleneck-centered heuristics developed by Adams et al. <ref> [1] </ref> and analyzed by Muscettola [29]. 4 Application Areas Our technology is directly applicable to problems in military and civilian air traffic control and fleet maintenance. We have also applied our techniques to problems of mobile robotics.
Reference: [2] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm [23] or the value iteration algorithm <ref> [2] </ref>. However, for large state spaces, even a polynomial-time algorithm such as policy iteration becomes too inefficient. To compensate we consider a highly-restricted subset of the entire state space in our planning.
Reference: [3] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1987. </year>
Reference: [4] <author> Mark Boddy. </author> <title> Anytime problem solving using dynamic programming. </title> <booktitle> In Proceedings AAAI-91, </booktitle> <pages> pages 738-743. </pages> <publisher> AAAI, </publisher> <year> 1991. </year>
Reference: [5] <author> Mark Boddy and Thomas Dean. </author> <title> Solving time-dependent planning problems. </title> <booktitle> In Proceedings IJCAI 11, </booktitle> <pages> pages 979-984. </pages> <address> IJCAII, </address> <year> 1989. </year>
Reference: [6] <author> Mark Boddy and Thomas Dean. </author> <title> Decision-theoretic deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: While this approach is satisfactory in some planning domains, in domains such as scheduling, the ability to find a reasonable envelope and discount factor is less certain; furthermore, as noted in [10] and <ref> [6] </ref>, the interdependence of problem partitioning and allocation of on-line deliberation time must be carefully considered. We have developed a planning and execution architecture that explicitly considers the problem of dynamically shifting deliberation focus.
Reference: [7] <author> Thomas Jr. Davis, H. Erzberger, S. M. Green, and W. Nedell. </author> <title> Design and evaluation of an air traffic control final approach spacing tool. </title> <journal> AIAA Journal of Guidance, Control, and Dynamics, </journal> <volume> 14 </volume> <pages> 848-854, </pages> <year> 1991. </year>
Reference-contexts: Neither Lansky nor Ow et al.'s systems deal with uncertainty nor provide options for user input and Lansky's system cannot handle concurrent planning and execution. There is ongoing work at NASA in collaboration with the FAA on building systems for the automated management and control of air traffic <ref> [7] </ref>. We see our work as extending the basic capabilities of such systems to handle uncertainty and make the best use of the time available for planning in time-critical situations.
Reference: [8] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 49-54. </pages> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference: [9] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Deliberation scheduling for time-critical sequential decision making. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in AI, </booktitle> <pages> pages 309-316, </pages> <year> 1993. </year>
Reference: [10] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <note> submitted to Artificial Intelligence, </note> <year> 1993. </year>
Reference-contexts: The language of reward functions is quite rich, allowing us to specify much more complex goals, including the maintenance of properties of the world and prioritized combinations of primitive goals; this is explored in <ref> [10] </ref>. Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm [23] or the value iteration algorithm [2]. However, for large state spaces, even a polynomial-time algorithm such as policy iteration becomes too inefficient. <p> The basic algorithm is employed under various models of planning and execution. Two that have been considered are precursor deliberation and recurrent deliberation; they vary in the amount of concurrency between planning and execution. The details of the algorithms and models are given in <ref> [10] </ref>. We compared the performance of our planning algorithm with policy iteration optimizing the policy for the whole domain. Our results show that our planning algorithm supplies a good policy early, and typically converges to a policy that is close to optimal before the whole-domain policy iteration method does. <p> While this approach is satisfactory in some planning domains, in domains such as scheduling, the ability to find a reasonable envelope and discount factor is less certain; furthermore, as noted in <ref> [10] </ref> and [6], the interdependence of problem partitioning and allocation of on-line deliberation time must be carefully considered. We have developed a planning and execution architecture that explicitly considers the problem of dynamically shifting deliberation focus.
Reference: [11] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <pages> pages 574-579. </pages> <publisher> AAAI, </publisher> <year> 1993. </year>
Reference: [12] <author> Thomas Dean and Keiji Kanazawa. </author> <title> Probabilistic temporal reasoning. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 524-528. </pages> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference: [13] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference: [14] <author> Thomas Dean and Keiji Kanazawa. </author> <title> Persistence and probabilistic inference. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(3) </volume> <pages> 574-585, </pages> <year> 1989. </year>
Reference: [15] <author> Thomas Dean and Michael Wellman. </author> <title> On the value of goals. </title> <editor> In Josh Tenenberg, Jay Weber, and James Allen, editors, </editor> <booktitle> Proceedings from the Rochester Planning Workshop: From Formal Systems to Practical Systems, </booktitle> <pages> pages 129-140, </pages> <year> 1989. </year> <month> 16 </month>
Reference: [16] <author> Thomas Dean and Michael Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference: [17] <author> Mark Drummond and John Bresina. </author> <title> Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 138-144. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: In order to guarantee global optimality without explicit exhaustive search over the action space we may need to retract as well as add constraints. This is an extension to our approach. 3 Related Research Drummond and Bresina's anytime synthetic projection algorithm <ref> [17] </ref> incrementally constructs conditional plans for stochastic domains. Their work provided the initial motivation for our research into state-space reduction. Instead of building on Markov decision theory, Drummond and Bresina's work involves search in the space of situations given a set of operators that map situations to situations.
Reference: [18] <author> Michael P. Georgeff and Amy L. Lansky. </author> <title> Reactive reasoning and planning. </title> <booktitle> In Proceedings AAAI-87, </booktitle> <pages> pages 677-682. </pages> <publisher> AAAI, </publisher> <year> 1987. </year>
Reference-contexts: Probabilistic logic provides a more expressive representation than that offered by Markov process theory, but with the increased expressiveness comes increased computational overhead. There have been a variety of other planning systems that are related to the problem. Georgeff's procedural reasoning system <ref> [18] </ref> was designed for on-line use in evolving situations, but it simply executes user-supplied procedures rather than constructing plans of action on its own.
Reference: [19] <author> Lloyd Greenwald and Thomas Dean. </author> <title> Anticipating computational demands when solving time-critical decision-making problems. </title> <booktitle> In Workshop on the Algorithmic Foundations of Robotics, </booktitle> <year> 1994. </year>
Reference-contexts: A stochastic model of the environment is used both during online deliberation to predict future states and during strategy table construction to discern patterns of shifting dynamics. Detailed presentations of this architecture may be found in <ref> [19, 22] </ref>. A typical instantiation of the planning and execution architecture proceeds as follows.
Reference: [20] <author> Lloyd Greenwald and Thomas Dean. </author> <title> Deliberation scheduling for time-critical scheduling in stochastic domains. </title> <type> Technical Report CS-94-35, </type> <institution> Brown University Department of Computer Science, Providence, RI, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: We then apply bottleneck-centered scheduling heuristics [1, 29] to improve initial policies. Deliberation scheduling is used to allocate computation time across these two phases. This work is described in detail in <ref> [21, 20] </ref>. In scheduling problems, each state consists of a number of available and unavailable resources and a number of activities that need to be scheduled on these resources. Actions consists of differing assignments of activities to resources.
Reference: [21] <author> Lloyd Greenwald and Thomas Dean. </author> <title> Monte carlo simulation and bottleneck-centered heuristics for time-critical scheduling in stochastic domains. </title> <booktitle> In ARPI Planning Initiative Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: We then apply bottleneck-centered scheduling heuristics [1, 29] to improve initial policies. Deliberation scheduling is used to allocate computation time across these two phases. This work is described in detail in <ref> [21, 20] </ref>. In scheduling problems, each state consists of a number of available and unavailable resources and a number of activities that need to be scheduled on these resources. Actions consists of differing assignments of activities to resources.
Reference: [22] <author> Lloyd Greenwald and Thomas Dean. </author> <title> Solving time-critical decision-making problems with predictable computational demands. </title> <booktitle> In Second International Conference on AI Planning Systems, </booktitle> <year> 1994. </year>
Reference-contexts: A stochastic model of the environment is used both during online deliberation to predict future states and during strategy table construction to discern patterns of shifting dynamics. Detailed presentations of this architecture may be found in <ref> [19, 22] </ref>. A typical instantiation of the planning and execution architecture proceeds as follows.
Reference: [23] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm <ref> [23] </ref> or the value iteration algorithm [2]. However, for large state spaces, even a polynomial-time algorithm such as policy iteration becomes too inefficient. To compensate we consider a highly-restricted subset of the entire state space in our planning.
Reference: [24] <author> F. Kabanza. </author> <title> Synthesis of reactive plans for multi-path environments. </title> <booktitle> In Proceedings AAAI-90, </booktitle> <pages> pages 164-169. </pages> <publisher> AAAI, </publisher> <year> 1990. </year>
Reference-contexts: We improve on the work of Drummond and Bresina by providing (i) coherent semantics for goals in stochastic domains, (ii) theoretically sound probabilistic foundations, (iii) and decision-theoretic methods 10 for controlling inference. Kabanza <ref> [24] </ref> describes a method for planning in nondeterministic environments that relies on exploring a small portion of the set of all possible action sequences and representing the resulting restricted automaton using a propositional branching time logic.
Reference: [25] <author> J. G. Kemeny and J. L. Snell. </author> <title> Finite Markov Chains. </title> <address> D. </address> <publisher> Van Nostrand, </publisher> <address> New York, </address> <year> 1960. </year>
Reference-contexts: A policy is a mapping from S to A, specifying an action to be taken in each situation. An environment combined with a policy for choosing actions in that environment yields a Markov chain <ref> [25] </ref>. A reward function is a mapping from S to &lt;, specifying the instantaneous reward that the system derives from being in each state.
Reference: [26] <author> Jak Kirman, Ann Nicholson, Moises Lejter, Thomas Dean, and Eugene Santos Jr. </author> <title> Using goals to find plans with high expected utility. </title> <booktitle> In Proceedings of the Second European Workshop on Planning, </booktitle> <year> 1993. </year>
Reference: [27] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An Algorithm for Probabilistic Planning. </title> <type> Technical Report 93-06-03, </type> <institution> University of Washington Department of Computer Science and Engineering, </institution> <month> June </month> <year> 1993. </year> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: Georgeff's procedural reasoning system [18] was designed for on-line use in evolving situations, but it simply executes user-supplied procedures rather than constructing plans of action on its own. Systems for synthesizing plans in stochastic domains, such as those by Kushmerick, Hanks and Weld <ref> [27] </ref>, do not directly address the problem of generating plans given time and quality constraints. Lansky [28] has developed planning systems for deterministic domains that exploit structural properties of the state space to expedite planning. Ow et al. [31] describe some initial efforts at building systems that modify plans incrementally.
Reference: [28] <author> Amy L. Lansky. </author> <title> Localized event-based reasoning for multiagent domains. </title> <journal> Computational Intelligence, </journal> <volume> 4(4), </volume> <year> 1988. </year>
Reference-contexts: Systems for synthesizing plans in stochastic domains, such as those by Kushmerick, Hanks and Weld [27], do not directly address the problem of generating plans given time and quality constraints. Lansky <ref> [28] </ref> has developed planning systems for deterministic domains that exploit structural properties of the state space to expedite planning. Ow et al. [31] describe some initial efforts at building systems that modify plans incrementally.
Reference: [29] <author> Nicola Muscettola. </author> <title> An experimental analysis of bottleneck-centered opportunistic scheduling. </title> <booktitle> In Proceedings of the Second European Workshop on Planning, </booktitle> <year> 1993. </year>
Reference-contexts: Instead, we generate policies in two phases. We first employ Monte Carlo simulation and constrained dispatch scheduling using a stochastic model of the environment to sample the large state/action space and provide default reflexes to handle the remaining states. We then apply bottleneck-centered scheduling heuristics <ref> [1, 29] </ref> to improve initial policies. Deliberation scheduling is used to allocate computation time across these two phases. This work is described in detail in [21, 20]. <p> The second phase analyzes the results of the first phase in order to detect areas for improvement (bottlenecks) within the policy and adds additional constraints on the action space in order to mitigate the bottlenecks and improve subsequent policies. Our technique for propagating constraints is based on bottleneck-centered heuristics <ref> [1, 29] </ref>, a form of opportunistic scheduling in which critical points of resource contention are located and used to identify and remove potential for negative interactions between activities. Bottleneck-centered heuristics have demonstrated their effectiveness in deployed scheduling solutions [29]. We employ bottleneck-centered heuristics through two main steps. <p> Our technique for propagating constraints is based on bottleneck-centered heuristics [1, 29], a form of opportunistic scheduling in which critical points of resource contention are located and used to identify and remove potential for negative interactions between activities. Bottleneck-centered heuristics have demonstrated their effectiveness in deployed scheduling solutions <ref> [29] </ref>. We employ bottleneck-centered heuristics through two main steps. First, find a critical interaction (bottleneck) among activities; namely one which contributes directly to a policy with high expected cost. In practice [29] critical interactions occur during time intervals in which there is a high demand/supply ratio for resources. <p> Bottleneck-centered heuristics have demonstrated their effectiveness in deployed scheduling solutions <ref> [29] </ref>. We employ bottleneck-centered heuristics through two main steps. First, find a critical interaction (bottleneck) among activities; namely one which contributes directly to a policy with high expected cost. In practice [29] critical interactions occur during time intervals in which there is a high demand/supply ratio for resources. <p> Our work on large action spaces also borrows from and extends the operations research literature on bottleneck-centered heuristics developed by Adams et al. [1] and analyzed by Muscettola <ref> [29] </ref>. 4 Application Areas Our technology is directly applicable to problems in military and civilian air traffic control and fleet maintenance. We have also applied our techniques to problems of mobile robotics.
Reference: [30] <author> Nils Nilsson. </author> <title> Probabilistic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 71-88, </pages> <year> 1986. </year> <month> 17 </month>
Reference-contexts: Kabanza's approach does not make use of any probabilistic information regarding state transitions and makes no attempt to construct even an approximately optimal plan for any measure of performance. Thiebaux et al. [33] attempt to extend our work and that of Drummond and Bresina to use probabilistic logic <ref> [30] </ref> for planning under uncertainty. Probabilistic logic provides a more expressive representation than that offered by Markov process theory, but with the increased expressiveness comes increased computational overhead. There have been a variety of other planning systems that are related to the problem.
Reference: [31] <author> P. S. Ow, S. F. Smith, and A. Thiriez. </author> <title> Reactive plan revision. </title> <booktitle> In Proceedings AAAI-88. AAAI, </booktitle> <year> 1988. </year>
Reference-contexts: Lansky [28] has developed planning systems for deterministic domains that exploit structural properties of the state space to expedite planning. Ow et al. <ref> [31] </ref> describe some initial efforts at building systems that modify plans incrementally. Neither Lansky nor Ow et al.'s systems deal with uncertainty nor provide options for user input and Lansky's system cannot handle concurrent planning and execution.
Reference: [32] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: Thus, the color of walls need only be mentioned when describing change that occurs as a result of the paint operator. A simplified version of the Bayesian network formalism <ref> [32] </ref> is well suited to specifying stochastic state transition and reward models. For each action, we use a two-slice network, in which nodes in the first slice represent values of state variables at time t and nodes in the second slice represent values of state variables at time t+1.
Reference: [33] <author> Sylvie Thiebaux, Joachim Hertzberg, William Shoaf, and Moti Schneider. </author> <title> A stochastic model of actions and plans for for anytime planning under uncertainty. </title> <editor> In E. Sandewall and C. Backstrom, editors, </editor> <booktitle> Current Trends in AI Planning. </booktitle> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1994. </year>
Reference-contexts: Kabanza's approach does not make use of any probabilistic information regarding state transitions and makes no attempt to construct even an approximately optimal plan for any measure of performance. Thiebaux et al. <ref> [33] </ref> attempt to extend our work and that of Drummond and Bresina to use probabilistic logic [30] for planning under uncertainty. Probabilistic logic provides a more expressive representation than that offered by Markov process theory, but with the increased expressiveness comes increased computational overhead.
References-found: 33

