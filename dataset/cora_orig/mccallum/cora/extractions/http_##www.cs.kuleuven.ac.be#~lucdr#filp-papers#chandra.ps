URL: http://www.cs.kuleuven.ac.be/~lucdr/filp-papers/chandra.ps
Refering-URL: http://www.cs.kuleuven.ac.be/~lucdr/filp.html
Root-URL: 
Email: freddyc,tadepallig@cs.orst.edu  
Phone: Phone: +1 541 737 4967; Fax: +1 541 737 3014  
Title: Inductive Logic Programming for Speedup Learning  
Author: Chandra Reddy Prasad Tadepalli 
Address: Corvallis, OR-97331. USA  
Affiliation: Dearborn 303, Department of Computer Science Oregon State University,  
Abstract: In this paper, we discuss how inductive-logic-programming (ILP) techniques facilitate learning control knowledge (or speedup learning) in the form of goal-decomposition rules (d-rules). The specific setting we used for speedup learning is the learning-from-exercises approach. We describe an ILP algorithm to learn function-free Horn definitions, and show how this is used to learn d-rules in the exercises approach. We briefly discuss the theoretical basis for the algorithm, and mention a learnability result for function-free Horn definitions.
Abstract-found: 1
Intro-found: 1
Reference: [ Ackerman and Kanfer, 1993 ] <author> P.L. Ackerman and R. Kanfer. </author> <title> Kanfer-Ackerman Air Traffic Control Task c fl CD-ROM Database, Data-Collection Program, and Playback Program. </title> <institution> Dept. of Psychology, Univ. of Minn., Minneapolis, MN, </institution> <year> 1993. </year>
Reference-contexts: By Horn program we mean any set of Horn clauses. demonstrated it in two domains [ Reddy and Tadepalli, 1997a ] : (1) A variant of STRIPS world, and (2) Kanfer-Ackerman air-traffic control (ATC) task <ref> [ Ackerman and Kanfer, 1993 ] </ref> . In the following, we briefly describe the d-rule representation in Section 2, and outline the learning-from-exercises setting and the methods used in Section 3. We show, in Section 4, how learning d-rules is similar to learning function-free Horn programs.
Reference: [ Angluin, 1988 ] <author> D. Angluin. </author> <title> Queries and concept learn-ing. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference: [ Bergadano and Gunetti, 1996 ] <author> F. Bergadano and D. Gunetti. </author> <title> Inductive Logic Programming: From Machine Learning to Software Engineering. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: [ Cohen, 1995 ] <author> W.W. Cohen. </author> <title> Pac-learning non-recursive prolog clauses. </title> <journal> Artificial Intelligence, </journal> <volume> 79(1) </volume> <pages> 1-38, </pages> <year> 1995. </year>
Reference-contexts: This algorithm has been shown to exactly learn from equivalence and membership queries, and, hence, PAC-learn with membership queries the class of first-order function-free non-recursive Horn definitions [ Reddy and Tadepalli, 1997b ] . This class is different from the classes studied in the learnability results by <ref> [ Dzeroski et al., 1992; Cohen, 1995 ] </ref> . 6 Exercises and ILP In the previous sections, we have discussed how d-rules, which are equivalent to Horn programs, could be learned with the help of exercises using an algorithm that can learn only Horn definitions.
Reference: [ Dzeroski et al., 1992 ] <author> S. Dzeroski, S. Muggleton, and S. Russell. </author> <title> Pac-learnability of determinate logic programs. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 128-135, </pages> <year> 1992. </year>
Reference-contexts: This algorithm has been shown to exactly learn from equivalence and membership queries, and, hence, PAC-learn with membership queries the class of first-order function-free non-recursive Horn definitions [ Reddy and Tadepalli, 1997b ] . This class is different from the classes studied in the learnability results by <ref> [ Dzeroski et al., 1992; Cohen, 1995 ] </ref> . 6 Exercises and ILP In the previous sections, we have discussed how d-rules, which are equivalent to Horn programs, could be learned with the help of exercises using an algorithm that can learn only Horn definitions.
Reference: [ Erol et al., 1994 ] <author> K. Erol, J. Hendler, and D.S. Nau. </author> <title> HTN planning: complexity and expressivity. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94). </booktitle> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: All literals in a d-rule are first-order function-free positive literals. A requirement on the subgoals component is that each variable in sg occurs also either in g or in c. D-rules are similar to hierarchical transition networks of <ref> [ Erol et al., 1994 ] </ref> , except that in d-rules the subgoals are totally ordered.
Reference: [ Estlin and Mooney, 1996 ] <author> T.A. Estlin and R.J. Mooney. </author> <title> Multi-strategy learning of search control for partial-order planning. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 843-848. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The previous applications of ILP to speedup learning include DOLPHIN [ Zelle and Mooney, 1993 ] , Grasshopper [ Leckie and Zukerman, 1993 ] and SCOPE <ref> [ Estlin and Mooney, 1996 ] </ref> . All these systems use the ILP system FOIL for learning control rules.
Reference: [ Frazier and Pitt, 1996 ] <author> M. Frazier and L. Pitt. </author> <title> CLASSIC learning. </title> <journal> Machine Learning, </journal> <volume> 26 </volume> <pages> 151-194, </pages> <year> 1996. </year>
Reference-contexts: Even so, the difference persists: in our case, since the antecedent reflects a state in planning, the antecedent changes from an example to another, whereas, in ILP, the background knowledge is unchanging. Our notion of examples is similar to that of Haussler [ 1989 ] and CLASSIC learning <ref> [ Frazier and Pitt, 1996 ] </ref> . The sketch of the algorithm (Generalize) we use for learning function-free Horn definitions is given in Figure 1. First, Generalize tries to include the example (eg) in one of the existing clauses (hypClauses). <p> Therefore, in our case, as in the case of <ref> [ Frazier and Pitt, 1996 ] </ref> , the difference between subset and membership queries is nominal. goal of the hypothesis d-rule into the subgoal sequence suggested by the hypothesis d-rule. If the d-rule interpreter can solve the problems using the subgoal sequence suggested, then Test is considered successful.
Reference: [ Haussler, 1989 ] <author> D. Haussler. </author> <title> Learning conjunctive concepts in structural domains. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 7-40, </pages> <year> 1989. </year>
Reference: [ Kietz and Lubbe, 1994 ] <author> J.-U. Kietz and M. Lubbe. </author> <title> An efficient subsumption algorithm for inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 130-138, </pages> <year> 1994. </year>
Reference-contexts: If the d-rule interpreter can solve the problems using the subgoal sequence suggested, then Test is considered successful. If not, Test fails. To be able to generate problems efficiently, certain assumptions on the clauses, such as determinacy and constant locality, which make subsumption efficient <ref> [ Kietz and Lubbe, 1994 ] </ref> are necessary. In our case, we use domain-specific heuristics, such as ordering the literals and dividing the condition into independent components. The use of lgg, however, introduces a complication.
Reference: [ Korf, 1987 ] <author> R.E. Korf. </author> <title> Planning as search: A quantitative approach. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 65-88, </pages> <year> 1987. </year>
Reference-contexts: The learner follows two main steps: For each exercise, it (1) first solves the problem using the previously learned knowledge, and then (2) incorporates the knowledge gained in solving the exercise into the existing knowledge. In solving an exercise, the learner uses iterative-deepening depth-first search (IDS) <ref> [ Korf, 1987 ] </ref> . Starting from a depth limit, IDS does depth-first search up to that depth limit. If the goal is not reached within the depth limit, the process is repeated after increasing the depth limit.
Reference: [ Lavrac and Dzeroski, 1994 ] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis-Horwood, </publisher> <year> 1994. </year>
Reference: [ Leckie and Zukerman, 1993 ] <author> C. Leckie and I. Zuker-man. </author> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pages 1100-1105, </pages> <year> 1993. </year>
Reference-contexts: The previous applications of ILP to speedup learning include DOLPHIN [ Zelle and Mooney, 1993 ] , Grasshopper <ref> [ Leckie and Zukerman, 1993 ] </ref> and SCOPE [ Estlin and Mooney, 1996 ] . All these systems use the ILP system FOIL for learning control rules.
Reference: [ Minton, 1988 ] <author> S.J. Minton. </author> <title> Learning Search Control Knowledge. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: A d-rule specifies the decomposition of a goal into subgoals, and the conditions under which the particular decomposition is appropriate for efficient planning. The previous methods that have been used for representing control knowledge include macro operators and control rules <ref> [ Minton, 1988 ] </ref> . Explanation-based learning (EBL) has been a popular method for speedup learning [ Minton, 1988; Ruby and Kibler, 1991 ] . EBL, because of its dependence on a single example to generate a rule, leads to over-specific rules. <p> The previous methods that have been used for representing control knowledge include macro operators and control rules [ Minton, 1988 ] . Explanation-based learning (EBL) has been a popular method for speedup learning <ref> [ Minton, 1988; Ruby and Kibler, 1991 ] </ref> . EBL, because of its dependence on a single example to generate a rule, leads to over-specific rules. This results in proliferation of expensive-to-match rules, a problem described as "utility problem" [ Minton, 1988 ] . <p> EBL, because of its dependence on a single example to generate a rule, leads to over-specific rules. This results in proliferation of expensive-to-match rules, a problem described as "utility problem" <ref> [ Minton, 1988 ] </ref> . Inductive logic programming (ILP) techniques, on the other hand, can easily handle multiple examples to learn a rule, and thus can avoid the utility problem. EBL's explanation step, however, is a strength: it explains away irrelevant relations (literals).
Reference: [ Muggleton and De Raedt, 1994 ] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19 </volume> <pages> 629-679, </pages> <year> 1994. </year>
Reference: [ Muggleton and Feng, 1990 ] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <pages> pages 368-381. </pages> <address> Ohmsha/Springer-Verlag, </address> <year> 1990. </year>
Reference-contexts: All these systems use the ILP system FOIL for learning control rules. FOIL is a top-down batch-learning system that uses information-gain criterion to do hill-climbing to select literals among the possible literals to be included in the clause being learned [ Quinlan, 1990 ] . GOLEM <ref> [ Muggleton and Feng, 1990 ] </ref> , like FOIL, is also a batch-learning system. GOLEM is a bottom-up method that uses lgg for generalization. Both GOLEM and FOIL use negative examples.
Reference: [ Natarajan, 1989 ] <author> B.K. Natarajan. </author> <title> On learning from exercises. </title> <booktitle> In Proceedings of the Second Workshop on Computational Learning Theory, </booktitle> <pages> pages 72-87. </pages> <publisher> Mor-gan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: EBL's explanation step, however, is a strength: it explains away irrelevant relations (literals). We use this strength of EBL, to initially prune irrelevant literals, in combination with the main ILP method to learn d-rules. We studied learning d-rules from the setting of learning from exercises <ref> [ Natarajan, 1989; Reddy and Tade-palli, 1997a ] </ref> . In this setting, the learner is input an exercise sequence|that is, planning problems, without their solutions, ordered according to their levels of difficulty.
Reference: [ Plotkin, 1970 ] <author> G.D. Plotkin. </author> <title> A note on inductive generalization. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 5, </volume> <pages> pages 153-163. </pages> <publisher> Elsevier North-Holland, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: The inclusion of an example in an existing clause is done employing Plotkin's least general generalization (lgg) <ref> [ Plotkin, 1970; Mug-gleton and Feng, 1990 ] </ref> . The use of lgg guarantees that a clause of the learner and an example are subsumed by a target clause if and only if their lgg is also subsumed by the same target clause.
Reference: [ Quinlan, 1990 ] <author> J.R. Quinlan. </author> <title> Learning logical definitions of from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: All these systems use the ILP system FOIL for learning control rules. FOIL is a top-down batch-learning system that uses information-gain criterion to do hill-climbing to select literals among the possible literals to be included in the clause being learned <ref> [ Quinlan, 1990 ] </ref> . GOLEM [ Muggleton and Feng, 1990 ] , like FOIL, is also a batch-learning system. GOLEM is a bottom-up method that uses lgg for generalization. Both GOLEM and FOIL use negative examples.
Reference: [ Reddy and Tadepalli, 1997a ] <author> C. Reddy and P. Tade-palli. </author> <title> Learning goal-decomposition rules using exercises. </title> <booktitle> In Proceedings of the 14th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: We have implemented an efficient version of this scheme, replaced the membership queries by self-testing, and 1 By Horn definition we mean a set of Horn clauses all with the same consequent literal. By Horn program we mean any set of Horn clauses. demonstrated it in two domains <ref> [ Reddy and Tadepalli, 1997a ] </ref> : (1) A variant of STRIPS world, and (2) Kanfer-Ackerman air-traffic control (ATC) task [ Ackerman and Kanfer, 1993 ] .
Reference: [ Reddy and Tadepalli, 1997b ] <author> C. Reddy and P. Tade-palli. </author> <title> Learning Horn definitions using equivalence and membership queries. </title> <type> Technical report, </type> <institution> Dept. of Comp. Sci., Oregon State Univ., </institution> <year> 1997. </year> <note> Submitted to ILP-97. </note>
Reference-contexts: This algorithm has been shown to exactly learn from equivalence and membership queries, and, hence, PAC-learn with membership queries the class of first-order function-free non-recursive Horn definitions <ref> [ Reddy and Tadepalli, 1997b ] </ref> .
Reference: [ Reddy et al., 1996 ] <author> C. Reddy, P. Tadepalli, and S. Roncagliolo. </author> <title> Theory-guided empirical speedup learning of goal-decomposition rules. </title> <booktitle> In Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 409-417. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: When we used the learning-from-examples approach to learn d-rules, since the training was not ordered in anyway in this approach, self-testing was not feasible. The learner in that approach needed to depend on membership queries <ref> [ Reddy et al., 1996 ] </ref> . Similar testing methods could be used in ILP when learning multiple concepts, using exercises-like ordering of training. 7 Discussion In this section, we discuss the work related to ours in using ILP for speedup learning, and in using lgg for for generalization.
Reference: [ Ruby and Kibler, 1991 ] <author> D. Ruby and D. Kibler. </author> <title> Learning subgoal sequences for planning. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: The previous methods that have been used for representing control knowledge include macro operators and control rules [ Minton, 1988 ] . Explanation-based learning (EBL) has been a popular method for speedup learning <ref> [ Minton, 1988; Ruby and Kibler, 1991 ] </ref> . EBL, because of its dependence on a single example to generate a rule, leads to over-specific rules. This results in proliferation of expensive-to-match rules, a problem described as "utility problem" [ Minton, 1988 ] .
Reference: [ Zelle and Mooney, 1993 ] <author> J.M. Zelle and R.J. Mooney. </author> <title> Combining FOIL and EBG to speedup logic programs. </title> <booktitle> In Proceedings of the 13th IJCAI, </booktitle> <pages> pages 1106-1111, </pages> <year> 1993. </year>
Reference-contexts: The previous applications of ILP to speedup learning include DOLPHIN <ref> [ Zelle and Mooney, 1993 ] </ref> , Grasshopper [ Leckie and Zukerman, 1993 ] and SCOPE [ Estlin and Mooney, 1996 ] . All these systems use the ILP system FOIL for learning control rules.
References-found: 24

