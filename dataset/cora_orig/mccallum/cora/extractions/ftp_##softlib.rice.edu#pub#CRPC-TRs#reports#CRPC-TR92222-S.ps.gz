URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92222-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Memory-Hierarchy Management  
Author: by Steve Carr Keith D. Cooper Danny C. Sorensen 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Ken Kennedy, Chairman Noah Harding Professor of Computer Science  Associate Professor of Computer Science  Professor  
Date: February, 1993  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  of Computational and Applied Math  
Abstract-found: 0
Intro-found: 1
Reference: [AC72] <author> F.E. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <booktitle> In Design and Optimization of Compilers, </booktitle> <pages> pages 1-30. </pages> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: This transformation is called scalar replacement and in Chapter 2, we show how to apply it to loops automatically. 1.2.2 Unroll-and-Jam Unroll-and-jam is another transformation that can be used to improve the performance of memory-bound loops <ref> [AC72, AN87, CCK88] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more computation into an innermost loop body without a proportional increase in memory references.
Reference: [AK87] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: With the likelihood of cache misses on accesses to B, we can interchange the I- and J-loops to make the distance between successive accesses small, as shown below <ref> [AK87, Wol86a] </ref>. DO 10 J = 1, N 10 A = A + B (I,J) Now, we will only have a cache miss on accesses to B once every cache line, resulting in better memory performance. <p> Dependence direction reversal makes unroll-and-jam unsafe. Additionally, when non-DO statements appear at levels other than the innermost level, loop distribution must be safe in order for unroll-and-jam to be safe. Essentially, no recurrence involving data or control dependences can be violated <ref> [AK87] </ref>. 3.2 Dependence Copying To allow scalar replacement to take advantage of the new opportunities for reuse created by unroll-and-jam, we must update the dependence graph to reflect these changes. <p> To handle safety, if any loop that must be distributed cannot be distributed, we set the unroll vector values in each vector to the smallest unroll amount in all of the vectors for each level. Distribution is unsafe if it breaks a recurrence <ref> [AK87] </ref>. The complete algorithm for distributing loops for unroll-and-jam is given in Figure 3.8. 3.5 Experiment Using the experimental system described in Chapter 2, we have implemented unroll-and-jam as presented in this chapter. In addition to controlling floating-point register pressure, we found that address-register pressure needed to be controlled. <p> Beginning, with the outermost position in the loop, we select the loop with the highest memory cost that can safely go in the current loop position. A loop positioning is safe if no resulting dependence has a negative threshold <ref> [AK87] </ref>. After selecting the outermost loop, we proceed by iteratively selecting the loop with the next highest cost for the next outermost position until a complete ordering is obtained. The algorithm for computing loop order for perfectly nested loops is shown in Figure 4.2. An Example. <p> Instead, we can use a combination of IF-conversion and sparse-matrix techniques that we call IF-inspection to allow us to keep the guard out of the innermost loop and still allow blocking <ref> [AK87] </ref>. The idea is to inspect at run-time the values of an outer-loop induction variable for which the guard is true and the inner loop is executed. Then, we execute 5.3. SOLVING SYSTEMS OF LINEAR EQUATIONS 67 the loop nest for only those values.
Reference: [AK88] <author> J.R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <type> Technical Report TR86-45, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1988. </year>
Reference-contexts: Allen and Kennedy show how data dependence information can be used to recognize reuse of vector data and how that information can be applied to perform vector register allocation <ref> [AK88] </ref>. They also present two transformations, loop interchange and loop fusion, as methods to improve vector register allocation opportunities. Both of these transformations were originally designed to enhance loop parallelism.
Reference: [AN87] <author> A. Aiken and A. Nicolau. </author> <title> Loop quantization: An analysis and algorithm. </title> <type> Technical Report 87-821, </type> <institution> Cornell University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: This transformation is called scalar replacement and in Chapter 2, we show how to apply it to loops automatically. 1.2.2 Unroll-and-Jam Unroll-and-jam is another transformation that can be used to improve the performance of memory-bound loops <ref> [AC72, AN87, CCK88] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more computation into an innermost loop body without a proportional increase in memory references. <p> Although the mechanics of unroll-and-jam are described in detail, there is no discussion of how or when to apply the transformation to a loop nest. Aiken and Nicolau present a transformation identical to unroll-and-jam, called loop quantization <ref> [AN87] </ref>. However, they do not use this transformation to increase data locality, but rather to improve inner-loop parallelism. They perform a strict quantization, where the unrolled iterations of the original loop in the body of the unrolled loop are data-independent.
Reference: [AS78] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois, </institution> <year> 1978. </year>
Reference-contexts: RELATED WORK 7 optimize for any memory hierarchy and relieve the programmer from the tedious task of memory-hierarchy management. 1.3.3 Memory Management The first major work in the area of memory management by a compiler is that of Abu-Sufah on improving virtual memory performance <ref> [AS78] </ref>. In his thesis, he describes the use of a restructuring compiler to improve the virtual memory behavior of a program.
Reference: [ASM86] <author> W. Abu-Sufah and A. Malony. </author> <title> Vector processing on the alliant FX/8 multiprocessors. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 559-566, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Abu-Sufah and Malony showed that the performance of the LANL BMK8A1 benchmark fell by a factor of as much as 2.26 on an Alliant FX/8 when vector sizes were too large to be maintained in cache <ref> [ASM86, GS84] </ref>. Similarly, Liu and Strother found that vector performance on the IBM 3090 fell by a factor of 1.40 when vector lengths exceeded the cache capacity [LS88].
Reference: [BCHT90] <author> P. Briggs, K.D. Cooper, M.W. Hall, and L. Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-102, </type> <institution> Rice University, CRPC, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: We would expect more dramatic improvements over a less sophisticated compiler. These methods should also produce larger improvements on machines where the load penalties are greater. z Our version of Matrix300 is after procedure cloning and inlining to create context for unroll-and-jam <ref> [BCHT90] </ref>. 55 Chapter 4 Loop Interchange The previous two chapters have addressed the problem of improving the performance of memory-bound loops under the assumption that good cache locality already exists in program loops.
Reference: [BCKT89] <author> P. Briggs, K.D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Coloring heuristics for register allocation. </title> <booktitle> In Proceedings of the ACM SIGPLAN 89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: T = A (1) T = T + B (I) Since global register allocation will most likely put scalar quantities in registers, we have removed the load of A (I-1), resulting in a reduction in the balance of the loop from 3 to 2 <ref> [CAC + 81, CH84, BCKT89] </ref>. This transformation is called scalar replacement and in Chapter 2, we show how to apply it to loops automatically. 1.2.2 Unroll-and-Jam Unroll-and-jam is another transformation that can be used to improve the performance of memory-bound loops [AC72, AN87, CCK88].
Reference: [BS88] <author> M. Berry and A. Sameh. </author> <title> Multiprocessor schemes for solving block tridiagonal linear systems. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(3) </volume> <pages> 37-57, </pages> <month> Fall </month> <year> 1988. </year>
Reference-contexts: The BLAS2 version performs a rank 1 update of the matrix while the best BLAS3 version performs a blocked rank 96 update. Also on the Alliant FX/8, Berry and Sameh have achieved speedups of as large as 9 over the standard LINPACK versions for solving tridiagonal linear systems <ref> [BS88, DBMS79] </ref> and on the Cray-2, Calahan showed that blocking LU decomposition improved the performance by a factor of nearly 6 [Cal86]. All of these studies involved tedious hand optimization to attain maximal performance.
Reference: [CAC + 81] <author> G.J. Chaitin, M.A. Auslander, A.K. Chandra, J. Cocke, M.E. Hopkins, and P.W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 45-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: We assume that the target machine has a typical optimizing compiler | one that performs scalar optimizations only. In particular, we assume that it performs strength reduction, allocates registers globally (via some coloring scheme) and schedules the arithmetic pipelines <ref> [CK77, CAC + 81, GM86] </ref>. This makes it possible for our transformation system to restructure the loop nests while leaving the details of optimizing the loop code to the compiler. <p> T = A (1) T = T + B (I) Since global register allocation will most likely put scalar quantities in registers, we have removed the load of A (I-1), resulting in a reduction in the balance of the loop from 3 to 2 <ref> [CAC + 81, CH84, BCKT89] </ref>. This transformation is called scalar replacement and in Chapter 2, we show how to apply it to loops automatically. 1.2.2 Unroll-and-Jam Unroll-and-jam is another transformation that can be used to improve the performance of memory-bound loops [AC72, AN87, CCK88]. <p> In this section, we will review the previous work, noting the deficiencies that we intend to address. 6 CHAPTER 1. INTRODUCTION 1.3.1 Register Allocation The most significant work in the area of register allocation has been done by Chaitin, et al., and Chow and Hennessy <ref> [CAC + 81, CH84] </ref>. Both groups cast the problem of register allocation into that of graph coloring on an interference graph, where the nodes represent scalar memory locations and an edge between two nodes prevents them from occupying the same register. <p> By encoding the reuse of array elements in scalar temporaries, we can give a coloring register allocator the opportunity to allocate values held in arrays to registers <ref> [CAC + 81] </ref>. Although previous algorithms for scalar replacement have been shown to be effective, they have only handled loops without conditional-control flow [CCK90]. The principle reason for past deficiencies is the reliance solely upon dependence information. A dependence contains little information concerning control flow between its source and sink.
Reference: [Cal86] <author> D.A. Calahan. </author> <title> Block-oriented, local-memory-based linear equation solution on the Cray-2: Uniprocessor algorithm. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <year> 1986. </year>
Reference-contexts: Also on the Alliant FX/8, Berry and Sameh have achieved speedups of as large as 9 over the standard LINPACK versions for solving tridiagonal linear systems [BS88, DBMS79] and on the Cray-2, Calahan showed that blocking LU decomposition improved the performance by a factor of nearly 6 <ref> [Cal86] </ref>. All of these studies involved tedious hand optimization to attain maximal performance. The BLAS primitives are noteworthy examples of this methodology, in which each primitive must be recoded in assembly language to get performance on each separate architecture [LHKK79].
Reference: [CCK88] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5, </volume> <year> 1988. </year>
Reference-contexts: Given these assumptions for the target machine and compiler, we will describe the notion of balance defined by Callahan, et. al, to measure the performance of program loops in relation to memory <ref> [CCK88] </ref>. This model will serve as the force behind the application of our transformations described throughout this thesis. Machine Balance A computer is balanced when it can operate in a steady state manner with both memory accesses and floating-point operations being performed at peak speed. <p> If all distance vector entries are zero, the dependence is loop independent. In determining which dependences can be used for memory analysis, we consider only those that have a consistent threshold | that is, those dependences for which the threshold is constant throughout the execution of the loop <ref> [GJG87, CCK88] </ref>. For a dependence to have a consistent threshold, it must be the case that the location accessed by the dependence source on iteration i is accessed by the sink on iteration i + c, where c does not vary with i. <p> Theorem 1.1 A dependence has a consistent threshold iff a i = b i for each 1 i n and there exists an integer t such that b n t = a 0 b 0 . Proof See Callahan, et. al <ref> [CCK88] </ref>. Some carried dependences will have multiple distance vector values associated with one entry. Consider the following loop. DO 10 I = 1, N The true dependence between the references to A (K) has the distances 1; 2; 3; : : :; N1 for the entry associated with the I-loop. <p> This transformation is called scalar replacement and in Chapter 2, we show how to apply it to loops automatically. 1.2.2 Unroll-and-Jam Unroll-and-jam is another transformation that can be used to improve the performance of memory-bound loops <ref> [AC72, AN87, CCK88] </ref>. The transformation unrolls an outer loop and then jams the resulting inner loops back together. Using unroll-and-jam we can introduce more computation into an innermost loop body without a proportional increase in memory references. <p> Because dependences represent the flow of values in an array, Allen and Kennedy suggest that this information could be used to recognize reuse in arrays used in scalar computations. Callahan, Cocke and Kennedy have expanded these ideas to develop scalar replacement as shown in Section 1.2.1 <ref> [CCK88] </ref>. Their method is only applicable to loops that have no inner-loop conditional control flow; therefore, it has limited applicability. Also, their algorithm does not consider register pressure and may expose more reuse than can be handled by a particular machine's register file. <p> Although Abu-Sufah's transformation system shows the potential for improving a program's memory behavior, his use of loop distribution can increase the amount of pipeline interlock and cause performance degradation <ref> [CCK88] </ref>. Additionally, the fact that virtual memory is fully associative rather than set associative prevents the application of his model to cache management. Fabri's work in automatic storage optimization concentrates on extended graph-coloring techniques to manage storage overlays in memory [Fab79]. <p> Scalar replacement is a transformation that uses dependence information to find reuse of array values and expose it by replacing the references with scalar temporaries as was done in the above example <ref> [CCK88, CCK90] </ref>. By encoding the reuse of array elements in scalar temporaries, we can give a coloring register allocator the opportunity to allocate values held in arrays to registers [CAC + 81]. <p> Then the loop at level k can be unrolled at most d k 1 time before a dependence is generated that prevents fusion of the inner n k loops. Proof See Callahan, et. al <ref> [CCK88] </ref>. Essentially, unrolling more than d k 1 will introduce a dependence with a negative entry in the outermost position after fusion. Since negative thresholds are not defined, the dependence direction must be reversed. Dependence direction reversal makes unroll-and-jam unsafe. <p> Below is a method to compute the updated dependence graph after loop unrolling for consistent dependences that contain only one loop induction variable in each subscript position and are not invariant with respect to the unrolled loop. <ref> [CCK88] </ref>. If we unroll the mth loop in a nest L by a factor of k, then the updated dependence graph G 0 0 0 for L can be computed from the dependence graph G = (V; E) for L by the following rules: 1. <p> To remove these cycles, we use the technique of Callahan, et. al, to estimate the amount of interlock and then unroll one loop to create more parallelism by introducing enough copies of the inner loop recurrence <ref> [CCK88] </ref>.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Scalar replacement is a transformation that uses dependence information to find reuse of array values and expose it by replacing the references with scalar temporaries as was done in the above example <ref> [CCK88, CCK90] </ref>. By encoding the reuse of array elements in scalar temporaries, we can give a coloring register allocator the opportunity to allocate values held in arrays to registers [CAC + 81]. <p> Although previous algorithms for scalar replacement have been shown to be effective, they have only handled loops without conditional-control flow <ref> [CCK90] </ref>. The principle reason for past deficiencies is the reliance solely upon dependence information. A dependence contains little information concerning control flow between its source and sink. It only reveals that both statements may be executed. <p> an invariant potential generator to be a partition generator, it must be a definition or the first load of a value along a path through the loop. 2.2.8 Register-Pressure Moderation Unfortunately, experiments have shown that exposing all of the reuse possible with scalar replacement may result in a performance degradation <ref> [CCK90] </ref>. Register spilling can completely counteract any savings from scalar replacement. The problem is that specialized information is necessary to recover the original code to prevent excessive spilling. <p> Although unroll-and-jam has been studied extensively, it has not been shown how to tailor unroll-and-jam to specific loops run on specific architectures. In the past, unroll amounts have been determined experimentally and specified with a compile-time parameter <ref> [CCK90] </ref>. However, the best choice for unroll amounts varies between loops and architectures. Therefore, in this chapter, we derive a method to chose unroll amounts automatically in order to balance program loops with respect to a specific target architecture.
Reference: [CH84] <author> F. Chow and J. Hennessy. </author> <title> Register allocation by priority-based coloring. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, SIGPLAN Notices Vol. </booktitle> <volume> 19, No. 6, </volume> <month> June </month> <year> 1984. </year> <note> 92 BIBLIOGRAPHY </note>
Reference-contexts: T = A (1) T = T + B (I) Since global register allocation will most likely put scalar quantities in registers, we have removed the load of A (I-1), resulting in a reduction in the balance of the loop from 3 to 2 <ref> [CAC + 81, CH84, BCKT89] </ref>. This transformation is called scalar replacement and in Chapter 2, we show how to apply it to loops automatically. 1.2.2 Unroll-and-Jam Unroll-and-jam is another transformation that can be used to improve the performance of memory-bound loops [AC72, AN87, CCK88]. <p> In this section, we will review the previous work, noting the deficiencies that we intend to address. 6 CHAPTER 1. INTRODUCTION 1.3.1 Register Allocation The most significant work in the area of register allocation has been done by Chaitin, et al., and Chow and Hennessy <ref> [CAC + 81, CH84] </ref>. Both groups cast the problem of register allocation into that of graph coloring on an interference graph, where the nodes represent scalar memory locations and an edge between two nodes prevents them from occupying the same register.
Reference: [CK77] <author> John Cocke and Ken Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> <month> 20(11), November </month> <year> 1977. </year>
Reference-contexts: We assume that the target machine has a typical optimizing compiler | one that performs scalar optimizations only. In particular, we assume that it performs strength reduction, allocates registers globally (via some coloring scheme) and schedules the arithmetic pipelines <ref> [CK77, CAC + 81, GM86] </ref>. This makes it possible for our transformation system to restructure the loop nests while leaving the details of optimizing the loop code to the compiler.
Reference: [CK87] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <year> 1987. </year>
Reference-contexts: However, if we analyze the sections of the arrays that are accessed at the source and sink of the offending dependence using array summary information, the potential to apply blocking is revealed <ref> [CK87, HK91] </ref>. Consider Figure 5.4. The region of the array A read by the reference to A (II) goes from I to I+IS-1 and the region written by A (K) goes from I to N. Therefore, the recurrence does not exist for the region from I+IS to N.
Reference: [CKP91] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architecural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: We assume that references to array variables are actually references to memory, while references to scalar variables involve only registers. Memory references are assigned a uniform cost under the assumption that loop interchange, software prefetching or tiling will attain cache locality <ref> [WL91, KM92, CKP91] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture. If fi L = fi M , the loop is balanced for the machine and will run well on that particular machine. <p> Previous work in this area has ignored the effects of cache-line length on the redundancy of prefetching in the presence of limited issue slots <ref> [CKP91] </ref>. Can we take advantage of our loop interchange algorithm to attain stride-one accesses and use cache-line size to derive an efficient and effective method for using software prefetching? Future efforts should be directed toward the development of an effective software prefetching algorithm.
Reference: [CP90] <author> D. Callahan and A. Porterfield. </author> <title> Data cache performance of supercomputer applications. </title> <booktitle> In Supercomputing '90, </booktitle> <year> 1990. </year>
Reference-contexts: The reason the reuse opportunity is only potential is cache interference | where two data items need to occupy the same location in cache at the same time. Previous studies have shown that interference is hard to predict and often prevents outer-loop reuse <ref> [CP90, LRW91] </ref>. However, inner-loop locality is likely to be captured by cache because of short distances between reuse points. Given these factors, our model of cache will assume that all outer-loop reuse will be prevented by cache interference and that all inner-loop reuse will be captured by cache.
Reference: [DBMS79] <author> J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: The BLAS2 version performs a rank 1 update of the matrix while the best BLAS3 version performs a blocked rank 96 update. Also on the Alliant FX/8, Berry and Sameh have achieved speedups of as large as 9 over the standard LINPACK versions for solving tridiagonal linear systems <ref> [BS88, DBMS79] </ref> and on the Cray-2, Calahan showed that blocking LU decomposition improved the performance by a factor of nearly 6 [Cal86]. All of these studies involved tedious hand optimization to attain maximal performance.
Reference: [DDDH90] <author> J.J. Dongarra, J. DuCroz, I. Duff, and S. Hammerling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: A number of other studies have shown the effectiveness of blocking loops for cache performance. Gallivan, et al., show that on the Alliant FX/8, the blocked version of LU decomposition is nearly 8 times faster than the unblocked version, using BLAS3 and BLAS2 respectively <ref> [GJMS88, DDHH88, DDDH90] </ref>. The BLAS2 version performs a rank 1 update of the matrix while the best BLAS3 version performs a blocked rank 96 update.
Reference: [DDHH88] <author> J.J. Dongarra, J. DuCroz, S. Hammerling, and R. Hanson. </author> <title> An extendend set of fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: A number of other studies have shown the effectiveness of blocking loops for cache performance. Gallivan, et al., show that on the Alliant FX/8, the blocked version of LU decomposition is nearly 8 times faster than the unblocked version, using BLAS3 and BLAS2 respectively <ref> [GJMS88, DDHH88, DDDH90] </ref>. The BLAS2 version performs a rank 1 update of the matrix while the best BLAS3 version performs a blocked rank 96 update. <p> To allow for an imperfect scalar optimizer, the effective register-set size for both floating-point and address registers was chosen to be 26. This value was determined experimentally. DMXPY. For our first experiment, we choose DMXPY from the Level 2 BLAS library <ref> [DDHH88] </ref>. This version of DMXPY is a hand-optimized vector-matrix multiply that is machine-specific and difficult to understand. We applied our transformations to the unoptimized version and compared the performance with the BLAS2 version.
Reference: [DDSvdV91] <author> J.J. Dongarra, I.S. Duff, D.C. Sorensen, and H.A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: To improve its cache performance, scientists have developed a block algorithm that essentially groups a number of updates to the matrix A and applies them together to a block portion of the array <ref> [DDSvdV91] </ref>. To attain the best blocking, strip-mine-and-interchange is performed on the outer K-loop for only a portion of the inner loop nest, requiring the technique described in Section 5.1.3 for automatic derivation of the block algorithm. Consider the strip-mined version of LU decomposition below. <p> We applied our algorithm by hand to LU decomposition and compared its performance with the original program and a hand coded version of the right-looking algorithm <ref> [DDSvdV91] </ref>. In the table below, "Block 1" refers to the right-looking version and "Block 2" refers to our algorithm in Figure 5.7. <p> However, a block algorithm, that essentially ignores the preventing recurrence and is similar to the non-pivoting case, can still be mathematically derived using the following result from linear algebra (see Figure 5.9) <ref> [DDSvdV91, Ste73] </ref>. If we have M 1 = 1 0 0 ^ P 2 fl The dependence from A (I,J) to A (KK,J) in statement 10 needed to be deleted in order for our system to work. <p> P i only depends upon the first i columns of A, allowing the computation of k P i 's and ^ M i 's, where k is the blocking factor, and then the block application of the ^ M i 's as is done in Figure 5.9 <ref> [DDSvdV91] </ref>. To install the above result into the compiler, we examine its implications from a data dependence viewpoint. In the point version, each row interchange is followed by a whole-column update in which each row element is updated independently. <p> Although pivoting is not necessary for QR decomposition, the best block algorithm is not an aggregation of the original algorithm. The block application of a number of elementary reflectors involves both computation and storage that does not exist in the original algorithm <ref> [DDSvdV91] </ref>.
Reference: [DS88] <author> K.H. </author> <title> Drechsler and M.P. Stadel. A solution to a problem with morel and renvoise's "global optimization by suppression of partial redudancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: This chapter addresses scalar replacement in the presence of forward conditional control flow. We show how to map partial redundancy elimination to scalar replacement in the presence of conditional control flow to ensure that memory costs will not increase along any execution path <ref> [MR79, DS88] </ref>. This chapter begins with an overview of partial redundancy elimination. Then, a detailed derivation of our algorithm for scalar replacement is given. <p> Because there may be no basic block in which new computations can be inserted on a particular path, insertion is done on flow-graph edges and new basic blocks are created when necessary <ref> [DS88] </ref>. The essential property of this transformation is that it is guaranteed not to increase the number of computations performed along any path [MR79]. In mapping partial redundancy elimination to scalar replacement, references to array expression can be seen as the computations. <p> The place for insertion of loads for partially available generators can be determined using Drechsler and Stadel's formulation for partial redundancy elimination, as shown below <ref> [DS88] </ref>. ppin (b) = antin (b) T pavin (b) T (antloc (b) S (transp (b) T ppout (b)) ppout (b) = FALSE if b is the loop exit T s2succ (b) ppin (s) insert (b) = ppout (b) T :avout (b) T (:ppin (b) S :transp (b)) insert (a,b) = ppin
Reference: [Fab79] <author> Janet Fabri. </author> <title> Automatic storage optimization. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Compiler Construction, </booktitle> <address> Denver, CO, </address> <year> 1979. </year>
Reference-contexts: Additionally, the fact that virtual memory is fully associative rather than set associative prevents the application of his model to cache management. Fabri's work in automatic storage optimization concentrates on extended graph-coloring techniques to manage storage overlays in memory <ref> [Fab79] </ref>. She presents the notion of array renaming (analogous to live-range splitting) to minimize the memory requirements of a program. However, the problem of storage overlays does not map to cache management since the compiler does not have explicit control over the cache.
Reference: [GJ79] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman and Co., </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: However, with the greedy method, we have a running time that is independent of machine architecture, making it more practical for use in a general tool. The greedy approximation of the knapsack problem is also provably no more than two times worse than the optimal solution <ref> [GJ79] </ref>. However, our experiments suggest that in practice the greedy algorithm performs as well as the knapsack algorithm. After determining which generators will be fully scalar replaced, there may still be a few registers available.
Reference: [GJG87] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <year> 1987. </year>
Reference-contexts: If all distance vector entries are zero, the dependence is loop independent. In determining which dependences can be used for memory analysis, we consider only those that have a consistent threshold | that is, those dependences for which the threshold is constant throughout the execution of the loop <ref> [GJG87, CCK88] </ref>. For a dependence to have a consistent threshold, it must be the case that the location accessed by the dependence source on iteration i is accessed by the sink on iteration i + c, where c does not vary with i. <p> Gannon, et al., present a technique to describe the amount of data that must be in the cache for reuse to be possible <ref> [GJG87] </ref>. They call this the reference window. The window for a dependence describes all of the data that is brought into the cache for the two references from the time that one datum is accessed at the source until it is used again at the sink.
Reference: [GJMS88] <author> K. Gallivan, W. Jalby, U. Meier, and A.H. Sameh. </author> <title> Impact of hierarchical memory systems on linear algebra design. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(1) </volume> <pages> 12-48, </pages> <month> Spring </month> <year> 1988. </year>
Reference-contexts: A number of other studies have shown the effectiveness of blocking loops for cache performance. Gallivan, et al., show that on the Alliant FX/8, the blocked version of LU decomposition is nearly 8 times faster than the unblocked version, using BLAS3 and BLAS2 respectively <ref> [GJMS88, DDHH88, DDDH90] </ref>. The BLAS2 version performs a rank 1 update of the matrix while the best BLAS3 version performs a blocked rank 96 update.
Reference: [GKT91] <author> G. Goff, K. Kennedy, and C.W. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Unfortunately, this method does not work on subscripts containing multiple induction variables, called MIV subscripts, that have incoming consistent dependences <ref> [GKT91] </ref>. Consider the following loop. DO 10 I = 1,N 10 A (I) = A (I) + B (I-J) The load of B (I-J) has a loop-independent incoming consistent dependence from itself carried by the I-loop. <p> To update the dependence graph for V MIV , we restrict ourselves to the innermost loop to satisfy the needs of scalar replacement only. Essentially, we will apply the strong SIV test for the innermost-loop induction variable on each MIV reference pair <ref> [GKT91] </ref>.
Reference: [GM86] <author> P.B. Gibbons and S.S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <year> 1986. </year>
Reference-contexts: We assume that the target machine has a typical optimizing compiler | one that performs scalar optimizations only. In particular, we assume that it performs strength reduction, allocates registers globally (via some coloring scheme) and schedules the arithmetic pipelines <ref> [CK77, CAC + 81, GM86] </ref>. This makes it possible for our transformation system to restructure the loop nests while leaving the details of optimizing the loop code to the compiler.
Reference: [GS84] <author> J.H. </author> <title> Griffin and M.L. </title> <type> Simmons. </type> <institution> Los Alamos National Laboratory Computer Benchmarking 1983. </institution> <type> Technical Report LA-10051-MS, </type> <institution> Los Alamos National Laboratory, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: Abu-Sufah and Malony showed that the performance of the LANL BMK8A1 benchmark fell by a factor of as much as 2.26 on an Alliant FX/8 when vector sizes were too large to be maintained in cache <ref> [ASM86, GS84] </ref>. Similarly, Liu and Strother found that vector performance on the IBM 3090 fell by a factor of 1.40 when vector lengths exceeded the cache capacity [LS88].
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: However, if we analyze the sections of the arrays that are accessed at the source and sink of the offending dependence using array summary information, the potential to apply blocking is revealed <ref> [CK87, HK91] </ref>. Consider Figure 5.4. The region of the array A read by the reference to A (II) goes from I to I+IS-1 and the region written by A (K) goes from I to N. Therefore, the recurrence does not exist for the region from I+IS to N.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the Fifteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 319-328, </pages> <month> January </month> <year> 1988. </year> <note> BIBLIOGRAPHY 93 </note>
Reference-contexts: Instead, he illustrates the transformation by a few examples. Irigoin and Triolet describe a new dependence abstraction, called a dependence cone, that can be used to block code for two levels of parallelism and two levels of memory <ref> [IT88] </ref>. The dependence cone gives more information than a distance vector by maintaining a system of linear inequalities that can be used to derive all dependences.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eight ACM Symposium on the Principles of Programming Languages, </booktitle> <year> 1981. </year>
Reference-contexts: Examining the regular sections for these references reveals that the recurrence only exists for the element A (L,L), allowing index-set splitting of the K-loop at L, IF-inspection of the J-loop, distribution (with scalar expansion) and interchange as shown Figure 5.11 <ref> [KKP + 81] </ref>. Below is a table of the results of the performance of Givens QR using DOUBLE-PRECISION REALS run on an IBM RS/6000 model 540.
Reference: [KM92] <author> K. Kennedy and K. McKinley. </author> <title> Optimizing for parallelism and memory hierarchy. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: We assume that references to array variables are actually references to memory, while references to scalar variables involve only registers. Memory references are assigned a uniform cost under the assumption that loop interchange, software prefetching or tiling will attain cache locality <ref> [WL91, KM92, CKP91] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture. If fi L = fi M , the loop is balanced for the machine and will run well on that particular machine. <p> They suggest the use of copy optimizations to remove the effects of set associativity and allow the use of larger portions of the cache. Kennedy and McKinley present a simplified model of cache performance that only considers inner-loop reuse <ref> [KM92] </ref>. Any reuse that occurs across an outer-loop iteration is considered to be prevented by cache interference. <p> This will allow determination of which edges can be made innermost to capture reuse. After computing the memory cost for each loop and computing the order for the loops, we must ensure that no violation of dependences will occur. Safety is ensured using the technique of McKinley, et al. <ref> [KM92] </ref>. Beginning, with the outermost position in the loop, we select the loop with the highest memory cost that can safely go in the current loop position. A loop positioning is safe if no resulting dependence has a negative threshold [AK87].
Reference: [Kuc78] <author> D. Kuck. </author> <title> The Structure of Computers and Computations Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: We say that a dependence exists between two references if there exists a control-flow path from the first reference to the second and both references access the same memory location <ref> [Kuc78] </ref>.
Reference: [LHKK79] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> Basic linear algebra subprograms for fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 308-329, </pages> <year> 1979. </year>
Reference-contexts: All of these studies involved tedious hand optimization to attain maximal performance. The BLAS primitives are noteworthy examples of this methodology, in which each primitive must be recoded in assembly language to get performance on each separate architecture <ref> [LHKK79] </ref>. Hand optimization is less than ideal because it takes months to code the BLAS primitives by hand, although recoding the whole program is a worse alternative. Therefore, significant motivation exists for the development of a restructuring compiler that can 1.3.
Reference: [LRW91] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architecural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Additionally, they do not necessarily derive the best block algorithm with their technique, leaving the possibility that suboptimal performance is still possible. Lam, Rothberg and Wolf present a method to determine block sizes for block algorithms automatically <ref> [LRW91] </ref>. Their results show that typical effective block sizes use less than 10% of the cache. They suggest the use of copy optimizations to remove the effects of set associativity and allow the use of larger portions of the cache. <p> The reason the reuse opportunity is only potential is cache interference | where two data items need to occupy the same location in cache at the same time. Previous studies have shown that interference is hard to predict and often prevents outer-loop reuse <ref> [CP90, LRW91] </ref>. However, inner-loop locality is likely to be captured by cache because of short distances between reuse points. Given these factors, our model of cache will assume that all outer-loop reuse will be prevented by cache interference and that all inner-loop reuse will be captured by cache. <p> BLOCKABILITY Now, we can capture the temporal reuse of JS values of B out of cache for every iteration of the J-loop if JS is less that the size of the cache and no cache interference occurs, and we can capture the temporal reuse of A in registers <ref> [LRW91] </ref>. As stated earlier, both strip-mine-and-interchange and unroll-and-jam make up the transformation technique known as iteration-space blocking. Essentially, unroll-and-jam is strip-mine-and-interchange with the innermost loop unrolled. <p> The optimal block size for a loop is dependent upon the behavior of the set associativity of the cache and has been shown to be difficult to determine <ref> [LRW91] </ref>. Can the compiler predict these effects at compile time or is it hopeless to perform automatic blocking with today's cache architectures? Additionally, an implementation of the techniques developed in Chapter 5 needs to be done to show its viability. 6.3.
Reference: [LS88] <author> B. Liu and N. Strother. </author> <title> Programming in VS FORTRAN on the IBM 3090 for maximum vector performance. </title> <journal> Computer, </journal> <volume> 21(6), </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: Similarly, Liu and Strother found that vector performance on the IBM 3090 fell by a factor of 1.40 when vector lengths exceeded the cache capacity <ref> [LS88] </ref>. In this second study, it was also shown that if the vector code were blocked into smaller sections that fit into cache, the optimal performance was regained.
Reference: [MR79] <author> E. Morel and C. Revoise. </author> <title> Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2), </volume> <month> February </month> <year> 1979. </year>
Reference-contexts: This chapter addresses scalar replacement in the presence of forward conditional control flow. We show how to map partial redundancy elimination to scalar replacement in the presence of conditional control flow to ensure that memory costs will not increase along any execution path <ref> [MR79, DS88] </ref>. This chapter begins with an overview of partial redundancy elimination. Then, a detailed derivation of our algorithm for scalar replacement is given. <p> The essential property of this transformation is that it is guaranteed not to increase the number of computations performed along any path <ref> [MR79] </ref>. In mapping partial redundancy elimination to scalar replacement, references to array expression can be seen as the computations. A load or a store followed by another load from the same location represents a redundant load that can be removed. <p> However, we can guarantee that there will not be an increase in the number of memory loads by only inserting load instructions if they are guaranteed to have a corresponding load removal on any execution path <ref> [MR79] </ref>. Without this guarantee, we may increase the number of memory accesses at execution time, resulting in a performance degradation. The best choice for a partially available generator would be one that is loop-independent.
Reference: [Por89] <author> A.K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: In these cases, the iteration space of a loop can be blocked into sections whose reuse can be captured by the cache. Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, Por89] </ref>. The effect is to shorten the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. <p> Porterfield reported that on computers with large memory latencies, many large scientific programs spent half of their execution time waiting for data to be delivered to cache <ref> [Por89] </ref>. A number of other studies have shown the effectiveness of blocking loops for cache performance. Gallivan, et al., show that on the Alliant FX/8, the blocked version of LU decomposition is nearly 8 times faster than the unblocked version, using BLAS3 and BLAS2 respectively [GJMS88, DDHH88, DDDH90]. <p> If the cache is too small, blocking transformations such as strip-mine-and-interchange can be used to decrease the size of the reference windows. Porterfield proposes a method to determine when the data referenced in a loop does not fit entirely in cache <ref> [Por89] </ref>. He develops the idea of an overflow iteration, which is that iteration of a loop that brings in the data item which will cause the cache to overflow.
Reference: [Sew90] <author> G Sewell. </author> <title> Computational Methods of Linear Algebra. </title> <publisher> Ellis Horwood, </publisher> <address> England, </address> <year> 1990. </year>
Reference-contexts: In Section 5.4, we will address this issue. 5.3.4 QR Decomposition with Givens Rotations Another form of orthogonal matrix that can be used in QR decomposition is the Givens rotation matrix <ref> [Sew90] </ref>. We currently know of no best block algorithm to derive, so instead we show that the index-set splitting technique described in Section 5.1.3 and IF-inspection have wider applicability. Consider the Fortran code for Givens QR shown in Figure 5.10 (note that this algorithm does not check for overflow) [Sew90]. <p> matrix <ref> [Sew90] </ref>. We currently know of no best block algorithm to derive, so instead we show that the index-set splitting technique described in Section 5.1.3 and IF-inspection have wider applicability. Consider the Fortran code for Givens QR shown in Figure 5.10 (note that this algorithm does not check for overflow) [Sew90]. The references to A in the inner K-loop have a long stride between successive accesses, resulting in poor cache performance. Our algorithm from Chapter 4 would recommend interchanging the J-loop to the innermost position, giving stride-one access to the references to A (J,K) and making the references 5.4.
Reference: [Ste73] <author> G.W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: This decomposition can be obtained by multiplying the matrix A by a series of elementary lower triangular matrices, M k : : : M 1 , as follows <ref> [Ste73] </ref>. A = LU 1 : : : M 1 U = M k : : : M 1 A (5.1) Using Equation 5.1, an algorithm for LU decomposition without pivoting can be derived. <p> Using the following matrix formulation U = M n1 P n1 M 3 P 3 M 2 P 2 M 1 P 1 A, the point version that includes partial pivoting can be derived (see Figure 5.8) <ref> [Ste73] </ref>. fl While we can apply index-set splitting to the algorithm in Figure 5.8 after strip mining to break the recurrence carried by the new KK-loop involving statement 10 and statement 40 as in the previous section, we cannot break the recurrence involving statements 10 and 25 using this technique. <p> However, a block algorithm, that essentially ignores the preventing recurrence and is similar to the non-pivoting case, can still be mathematically derived using the following result from linear algebra (see Figure 5.9) <ref> [DDSvdV91, Ste73] </ref>. If we have M 1 = 1 0 0 ^ P 2 fl The dependence from A (I,J) to A (KK,J) in statement 10 needed to be deleted in order for our system to work. <p> Any class of matrices that have this 74 CHAPTER 5. BLOCKABILITY property can be used to solve a system of linear equations. One such class, having orthonormal columns, is used in QR decomposition <ref> [Ste73] </ref>. If A has linearly independent columns, then A can be written uniquely in the form A = QR, where Q has orthonormal columns, QQ T = I and R is upper triangular with positive diagonal elements. <p> Each V k eliminates the values below the diagonal in the kth column. For a more detailed discussion of the QR algorithm and the computation of V k , see Stewart <ref> [Ste73] </ref>. Although pivoting is not necessary for QR decomposition, the best block algorithm is not an aggregation of the original algorithm. The block application of a number of elementary reflectors involves both computation and storage that does not exist in the original algorithm [DDSvdV91].
Reference: [SU70] <author> R. Sethi and J.D. Ullman. </author> <title> The generation of optimal code for arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 17(4) </volume> <pages> 715-728, </pages> <month> October </month> <year> 1970. </year>
Reference-contexts: To compute the number of registers required for those memory references not scalar replaced and for those temporaries needed during the evaluation of expressions, we use the tree labeling technique of Sethi and Ullman <ref> [SU70] </ref>. Using this technique, the number of registers required for each expression in the original loop body is computed and the maximum over all expressions is taken as the value for R ; .
Reference: [Tha81] <author> Khalid O. Thabit. </author> <title> Cache Managemant by the Compiler. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> November </month> <year> 1981. </year>
Reference-contexts: However, the problem of storage overlays does not map to cache management since the compiler does not have explicit control over the cache. Thabit has examined software methods to improve cache performance through the use of packing, prefetching and loop transformations <ref> [Tha81] </ref>. He shows that optimal packing of scalars for cache-line reuse is an NP-complete problem and proposes some heuristics. However, scalars are not considered to be a significant problem because of register-allocation technology.
Reference: [WL91] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: We assume that references to array variables are actually references to memory, while references to scalar variables involve only registers. Memory references are assigned a uniform cost under the assumption that loop interchange, software prefetching or tiling will attain cache locality <ref> [WL91, KM92, CKP91] </ref>. Comparing fi M to fi L can give us a measure of the performance of a loop running on a particular architecture. If fi L = fi M , the loop is balanced for the machine and will run well on that particular machine. <p> He does not consider increasing the "intelligence" of the compiler to improve its effectiveness. Lam and Wolf present a framework for determining memory usage within loop nests and use that framework to apply loop interchange, loop skewing, loop reversal, tiling and unroll-and-jam <ref> [WL91, Wol86b] </ref>. Their method does not work on non-perfectly nested loops and does not encompass a technique to determine unroll-and-jam amounts automatically. Additionally, they do not necessarily derive the best block algorithm with their technique, leaving the possibility that suboptimal performance is still possible. <p> Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, WL91] </ref>. The effect is to shorten the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest.
Reference: [Wol82] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: Unfortunately, there is an recurrence between the definition of A (K) and the load from A (II) carried by the II-loop. Using only standard dependence abstractions, such as distance and direction vectors, we would be prevented from blocking the loop <ref> [Wol82] </ref>. However, if we analyze the sections of the arrays that are accessed at the source and sink of the offending dependence using array summary information, the potential to apply blocking is revealed [CK87, HK91]. Consider Figure 5.4.
Reference: [Wol86a] <author> M. Wolfe. </author> <title> Advanced loop interchange. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: With the likelihood of cache misses on accesses to B, we can interchange the I- and J-loops to make the distance between successive accesses small, as shown below <ref> [AK87, Wol86a] </ref>. DO 10 J = 1, N 10 A = A + B (I,J) Now, we will only have a cache miss on accesses to B once every cache line, resulting in better memory performance. <p> He shows how tiling (or iteration-space blocking) can be used to improve the memory performance of program loops. Wolfe also shows that his techniques for advanced loop interchange can be used to tile loops with non-rectangular iteration spaces and loops that are not perfectly nested <ref> [Wol86a] </ref>. In particular, he discusses blocking for triangular- and trapezoidal-shaped iteration spaces, but he does not present an algorithm. Instead, he illustrates the transformation by a few examples. <p> If interchange across a non-perfectly nested portion of the loop nest is required or different loop orderings of common loops are requested, then the interchanged loop is distributed, when safe, and the interchange is performed as desired <ref> [Wol86a] </ref>. Distribution safety can be incorporated into LoopOrder at the point where interchange safety is tested. To be conservative, any loop containing a distribution-preventing recurrence and any loop nested outside of that loop cannot be interchanged inward. <p> The problem is that when performing interchange of loops that iterate over a triangular region, the loop bounds must be modified to preserve the semantics of the loop <ref> [Wol86a, Wol87] </ref>. Below, we will derive the formula for determining loop bounds when blocking is performed on triangular iteration spaces. We begin with the derivation for strip-mine-and-interchange and then extend it to unroll-and-jam.
Reference: [Wol86b] <author> M. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> Journal of Parallel Programming, </journal> <year> 1986. </year>
Reference-contexts: He does not consider increasing the "intelligence" of the compiler to improve its effectiveness. Lam and Wolf present a framework for determining memory usage within loop nests and use that framework to apply loop interchange, loop skewing, loop reversal, tiling and unroll-and-jam <ref> [WL91, Wol86b] </ref>. Their method does not work on non-perfectly nested loops and does not encompass a technique to determine unroll-and-jam amounts automatically. Additionally, they do not necessarily derive the best block algorithm with their technique, leaving the possibility that suboptimal performance is still possible.
Reference: [Wol87] <author> M. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December </month> <year> 1987. </year>
Reference-contexts: In these cases, the iteration space of a loop can be blocked into sections whose reuse can be captured by the cache. Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, Por89] </ref>. The effect is to shorten the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. <p> In particular, he does not address the problem of an array interfering with itself. Finally, Thabit establishes the safety conditions of loop distribution and strip-mine-and-interchange. Wolfe's memory performance work has concentrated on developing transformations to reshape loops to improve their cache performance <ref> [Wol87] </ref>. He shows how tiling (or iteration-space blocking) can be used to improve the memory performance of program loops. Wolfe also shows that his techniques for advanced loop interchange can be used to tile loops with non-rectangular iteration spaces and loops that are not perfectly nested [Wol86a]. <p> Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, WL91] </ref>. The effect is to shorten the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest. <p> The problem is that when performing interchange of loops that iterate over a triangular region, the loop bounds must be modified to preserve the semantics of the loop <ref> [Wol86a, Wol87] </ref>. Below, we will derive the formula for determining loop bounds when blocking is performed on triangular iteration spaces. We begin with the derivation for strip-mine-and-interchange and then extend it to unroll-and-jam.
Reference: [Wol89] <author> M. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of the Supercomputing '89 Conference, </booktitle> <year> 1989. </year>
Reference-contexts: Unfortunately, this technique does not work on imperfectly nested loops nor does it handle partially blockable loops, both of which occur in linear algebra codes. Wolfe presents work that is very similar to Irigoin and Triolet's <ref> [Wol89] </ref>. He does not use the dependence cone as the dependence abstraction, but instead he uses the standard distance vector. Using loop skewing, loop interchange and strip mining, he can tile an iteration space into blocks which have both data locality and parallelism.
References-found: 50

