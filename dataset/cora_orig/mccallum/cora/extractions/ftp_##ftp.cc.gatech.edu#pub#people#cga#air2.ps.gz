URL: ftp://ftp.cc.gatech.edu/pub/people/cga/air2.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Chris.Atkeson/publications.html
Root-URL: 
Title: Locally Weighted Learning for Control  
Author: Christopher G. Atkeson Andrew W. Moore and Stefan Schaal 
Keyword: locally weighted regression, LOESS, LWR, lazy learning, memory-based learning, least commitment learning, forward models, inverse models, linear quadratic regulation (LQR), shifting setpoint algorithm, dynamic programming.  
Address: 801 Atlantic Drive, Atlanta, GA 30332-0280  5000 Forbes Ave, Pittsburgh, PA 15213  2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan  
Affiliation: College of Computing, Georgia Institute of Technology  Carnegie Mellon University  ATR Human Information Processing Research Laboratories  
Email: cga@cc.gatech.edu, sschaal@cc.gatech.edu  awm@cs.cmu.edu  
Phone: 404-894-1076, fax: 404-853-9378  
Web: http://www.cc.gatech.edu/fac/Chris.Atkeson http://www.cc.gatech.edu/fac/Stefan.Schaal  http://www.cs.cmu.edu/~awm/hp.html  
Date: June 13, 1996 at 9:05  
Abstract: Lazy learning methods provide useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of complex systems. This paper surveys ways in which locally weighted learning, a type of lazy learning, has been applied by us to control tasks. We explain various forms that control tasks can take, and how this affects the choice of learning paradigm. The discussion section explores the interesting impact that explicitly remembering all previous experiences has on the problem of learning to control. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. and Salzberg, S. L. </author> <year> (1993). </year> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 363-368, </pages> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Albus, J. S. </author> <year> (1981). </year> <title> Brains, Behaviour and Robotics. </title> <publisher> BYTE Books, McGraw-Hill. </publisher>
Reference-contexts: Happily, it is actually quite difficult to think of useful tasks that require the system to have an accurate model over the entire input space <ref> (Albus, 1981) </ref>. Indeed, for a robot of more than, say, eight degrees of freedom, it will not be possible for it to get into every significantly different configuration even once in its entire lifetime.
Reference: <author> Atkeson, C. G. </author> <year> (1990). </year> <title> Using local models to control movement. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 316-323. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Atkeson, C. G. </author> <year> (1994). </year> <title> Using local trajectory optimizers to speed up global optimization in dynamic programming. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 663-670. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. </author> <year> (1995). </year> <title> Locally weighted learning. </title> <note> Submitted to the Artificial Intelligence Review special issue on Lazy Learning. </note>
Reference-contexts: The tasks may change over time, or multiple tasks may need to be performed. Lazy learning methods provide an approach to learning models of complex phenomena, dealing with large amounts of data, training quickly, and avoiding interference between multiple tasks during control of complex systems <ref> (Atkeson et al., 1995) </ref>. This paper describes five ways in which lazy learning techniques have been applied by us to control tasks. <p> It is difficult to evaluate a representational tool independently of the paradigm in which it is used, and vice versa. A successful robot learning algorithm typically is composed of sophisticated representational tools and learning paradigms. We will describe using the same representational tool, locally weighted learning <ref> (Atkeson et al., 1995) </ref>, in different tasks with different learning paradigms and with different results. In defining paradigms for learning to control complex systems it is useful to identify three separate components of an indirect (model-based) adaptive control system: modeling, exploration, and policy design. <p> as the modeling component, they should exploit the capabilities of lazy modeling, and make a lazy modeler's job easier. 1.1 Why Focus on Lazy Learning For Learning to Control? We will not review lazy learning here, but expect that our reader has already read the companion paper in this collection <ref> (Atkeson et al., 1995) </ref>, from which we will borrow both terminology and notation. <p> Locally weighted learning easily learns in real time from the continuous stream of training data. It also avoids the negative interference exhibited by other modeling approaches, because locally weighted learning retains all the training data, as do many lazy learning methods <ref> (Atkeson et al., 1995) </ref>. Our approach to modeling the complex functions found in typical task or process dynamics is to use a collection of simple local models. One benefit of local modeling is that it avoids the difficult problem of finding an appropriate structure for a global model. <p> A new local model is created to answer each query. This leads to another benefit of lazy modeling for control: we can delay the choice of local model structure and structural parameters until a query must be answered, and we can make different choices for subsequent queries <ref> (Atkeson et al., 1995) </ref>. Locally weighted learning can represent nonlinear functions, yet has simple training rules with a single global optimum for building a local model in response to a query. This allows complex nonlinear models to be identified (trained) quickly. Currently we are using polynomials as the local models. <p> It is true that lazy learning transfers the computational load onto the lookup process, but our experience is that the linear parameter estimation process during lookup in locally weighted learning is still fast enough for real time robot learning <ref> (Atkeson et al., 1995) </ref>. We use cross validation to choose an appropriate distance metric and weighting function, and to help find irrelevant input variables and terms in the local model. <p> learning <ref> (Atkeson et al., 1995) </ref>. We use cross validation to choose an appropriate distance metric and weighting function, and to help find irrelevant input variables and terms in the local model. In fact, performing one cross validation evaluation in lazy learning is no more expensive than processing a single query (Atkeson et al., 1995). Cheap cross validation makes search for model parameters routine, and we have explored procedures that take advantage of this (Atkeson et al., 1995; Maron and Moore, 1994; Moore et al., 1992; Moore and Lee, 1994). <p> Another attractive feature of locally weighted learning is flexibility. There are explicit parameters to control smoothing, outlier rejection, forgetting, and other processes. The modeling process is easy to understand, and therefore easy to adjust or control <ref> (Atkeson 3 et al., 1995) </ref>. We will see how the explicit representation of specific memories can speed up convergence and improve the robustness and autonomy of optimization and control algorithms (Atkeson et al., 1995; Moore and Schneider, 1995). <p> closest outcome to y d . * First Order Gradient Search: Perform a steepest-ascent search from an initial candidate action toward an action that will give the desired output (Press et al., 1988). 7 Finding the local gradient of the empirical model is easy if locally weighted regression is used <ref> (Atkeson et al., 1995) </ref>. Part of the computation of the locally weighted regression model forms the local linear map, so it is already available. <p> In this implementation the representation used for both forward and inverse models was locally weighted regression using outlier removal and cross validation for choosing the kernel width <ref> (Atkeson et al., 1995) </ref>. Inverse and forward models were used together; the forward model was searched with steepest ascent. Early shots (when no success was predicted) were uncertainty-based (Moore, 1991a). After 100 shots, control choice running on a Sun-4 was taking 0.8 seconds. This implementation demonstrates several important points. <p> By means of these controllers, the amount of data around the setpoints could quickly be increased until the quality of the local models exceeded a statistical threshold (Figure 9b) <ref> (Atkeson et al., 1995) </ref>. 3. At this point, the setpoints were gradually shifted towards the goal setpoints until the statistical confidence in the predictions made by the local model again fell below a threshold (Figure 9c). 4. <p> Memory costs increase linearly with the amount of data, and are not generally a problem. Any algorithm that avoids storing redundant data would greatly reduce the amount of memory needed, and one can also discard data, perhaps selected according to predictive usefulness, redundancy, or age <ref> (Atkeson et al., 1995) </ref>. Computational costs are more serious. For a fixed amount of computation, a single processor can process a limited number of training data points. There are several solutions to this problem (Atkeson et al., 1995): The database can be structured so that the most relevant data points are <p> needed, and one can also discard data, perhaps selected according to predictive usefulness, redundancy, or age <ref> (Atkeson et al., 1995) </ref>. Computational costs are more serious. For a fixed amount of computation, a single processor can process a limited number of training data points. There are several solutions to this problem (Atkeson et al., 1995): The database can be structured so that the most relevant data points are accessed first, or so that close approximations to the output predicted by locally weighted regression can be obtained without explicitly visiting every point in the database. <p> Good representational choices (i.e., choices of the elements of the state and control vectors, etc.) can dramatically speed up learning or make learning possible at all. Feature selection and scaling algorithms are a crude form of choosing new representations <ref> (Atkeson et al., 1995) </ref>.
Reference: <author> Baird, L. C. and Klopf, A. H. </author> <year> (1993). </year> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Technical Report WL-TR-93-1147, </type> <institution> Wright Laboratory, Wright-Patterson Air Force Base Ohio. </institution> <address> http://kirk.usafa.af.mil/ baird/papers/index.html. </address>
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and Sequential Decision Making. </title> <editor> In Gabriel, M. and Moore, J. W., editors, </editor> <booktitle> Learning and Computational Neuroscience, </booktitle> <pages> pages 539-602. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Cannon, R. H. </author> <year> (1967). </year> <title> Dynamics of Physical Systems. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: One step deadbeat control will fail on some non-minimum phase systems, of which pole balancing is one example <ref> (Cannon, 1967) </ref>. In these systems, one must move away from the goal to approach it later. In the case of the cart-pole system the cart must initially move away from the target position so that the pole leans in the direction of future cart motion towards the target.
Reference: <author> Cohn, D. A., Ghahramani, Z., and Jordan, M. I. </author> <year> (1995). </year> <title> Active learning with statistical models. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Connell, M. E. and Utgoff, P. E. </author> <year> (1987). </year> <title> Learning to control a dynamic physical system. </title> <booktitle> In Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 456-460, </pages> <address> Seattle, WA. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Conte, S. D. and De Boor, C. </author> <year> (1980). </year> <title> Elementary Numerical Analysis. </title> <publisher> McGraw Hill. </publisher>
Reference-contexts: Given a monotonic relationship between u and y, the sequence of actions that are chosen are closely related to the Secant method <ref> (Conte and De Boor, 1980) </ref> for numerically finding the zero of a function. See (Ortega and Rheinboldt, 1970) for a good discussion of the multidimensional generalization of the Secant method.
Reference: <author> Deng, K. and Moore, A. W. </author> <year> (1995). </year> <title> Multiresolution Instance-based Learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Farmer, J. D. and Sidorowich, J. J. </author> <year> (1987). </year> <title> Predicting chaotic time series. </title> <journal> Physical Review Letters, </journal> <volume> 59(8) </volume> <pages> 845-848. </pages>
Reference: <author> Farmer, J. D. and Sidorowich, J. J. </author> <year> (1988a). </year> <title> Exploiting chaos to predict the future and reduce noise. </title> <editor> In Lee, Y. C., editor, </editor> <title> Evolution, Learning, and Cognition, </title> <publisher> pages 277-??? World Scientific Press, </publisher> <address> NJ. </address> <note> also available as Technical Report LA-UR-88-901, </note> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, New Mexico. </address>
Reference: <author> Farmer, J. D. and Sidorowich, J. J. </author> <year> (1988b). </year> <title> Predicting chaotic dynamics. </title> <editor> In Kelso, J. A. S., Mandell, A. J., and Schlesinger, M. F., editors, </editor> <booktitle> Dynamic Patterns in Complex Systems, </booktitle> <pages> pages 265-292. </pages> <publisher> World Scientific, </publisher> <address> NJ. </address>
Reference: <author> Friedman, J. H. and Stuetzle, W. </author> <year> (1981). </year> <title> Projection Pursuit Regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76(376) </volume> <pages> 817-823. </pages>
Reference-contexts: if necessary to emulate global models, and can become global or local in particular directions to emulate projection pursuit models (e.g., the distance function can be set to choose a projection direction, for example, but for multiple projection directions multiple distance functions must be used in additive locally weighted fits) <ref> (Friedman and Stuetzle, 1981) </ref>. We expect locally weighted learning to degrade gracefully as the problem dimensionality increases. * Lazy learning depends on having good representations already selected.
Reference: <author> Gorinevsky, D. and Connolly, T. H. </author> <year> (1994). </year> <title> Comparison of some neural network and scattered data approximations: The inverse manipulator kinematics example. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 521-542. </pages>
Reference: <author> Grosse, E. </author> <year> (1989). </year> <title> LOESS: Multivariate Smoothing by Moving Least Squares. </title> <editor> In C. K. Chul, L. L. S. and Ward, J. D., editors, </editor> <title> Approximation Theory VI. </title> <publisher> Academic Press. </publisher>
Reference: <author> Hastie, T. and Loader, C. </author> <year> (1993). </year> <title> Local regression: Automatic kernel carpentry. </title> <journal> Statistical Science, </journal> <volume> 8(2) </volume> <pages> 120-143. </pages>
Reference-contexts: Additionally, if the input data distribution is not too non-uniform, it can be shown that the linearizations returned by locally weighted learning accomplish a low-bias estimate of the true gradient with fewer data points than required for a low-bias prediction of a query <ref> (Hastie and Loader, 1993) </ref>. * Automatic confidence estimations. Locally weighted regression can also be modified to return a confidence interval along with its prediction.
Reference: <author> Huang, P. S. </author> <year> (1996). </year> <title> Planning For Dynamic Motions Using A Search Tree. </title> <type> MS thesis, </type> <institution> University of Toronto, Graduate Department of Computer Science. </institution> <note> http://www.dgp.utoronto.ca/people/psh/home.html. </note>
Reference: <author> Jordan, M. I. and Jacobs, R. A. </author> <year> (1990). </year> <title> Learning to control an unstable system with forward modeling. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 324-331. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Jordan, M. I. and Rumelhart, D. E. </author> <year> (1992). </year> <title> Forward Models: Supervised Learning with a Distal Teacher. </title> <journal> Cognitive Science, </journal> <volume> 16 </volume> <pages> 307-354. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Kuperstein, M. </author> <year> (1988). </year> <title> Neural Model of Adaptive Hand-Eye Coordination for Single Postures. </title> <journal> Science, </journal> <volume> 239 </volume> <pages> 1308-3111. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> Bayesian Model Comparison and Backprop Nets. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 839-846. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Mahadevan, S. </author> <year> (1992). </year> <title> Enhancing Transfer in Reinforcement Learning by Building Stochastic Models of Robot Actions. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Maron, O. and Moore, A. </author> <year> (1994). </year> <title> Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 59-66. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Prieditis and Russell (1995), </booktitle> <pages> pages 387-395. </pages>
Reference: <author> Mel, B. W. </author> <year> (1989). </year> <title> MURPHY: A Connectionist Approach to Vision-Based Robot Motion Planning. </title> <type> Technical Report CCSR-89-17A, </type> <institution> University of Illinois at Urbana-Champaign. </institution>
Reference: <author> Miller, W. T. </author> <year> (1989). </year> <title> Real-Time Application of Neural Networks for Sensor-Based Control of Robots with Vision. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 19(4) </volume> <pages> 825-831. </pages>
Reference: <author> Moore, A. W. </author> <year> (1990). </year> <title> Acquisition of Dynamic Control Knowledge for a Robotic Manipulator. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. </author> <year> (1991a). </year> <title> Knowledge of Knowledge and Intelligent Experimentation for Learning Control. </title> <booktitle> In Proceedings of the 1991 Seattle International Joint Conference on Neural Networks. </booktitle>
Reference-contexts: Inverse and forward models were used together; the forward model was searched with steepest ascent. Early shots (when no success was predicted) were uncertainty-based <ref> (Moore, 1991a) </ref>. After 100 shots, control choice running on a Sun-4 was taking 0.8 seconds. This implementation demonstrates several important points. The first is the precision required of the modeling component. The cue-action must be extremely precise for success. Locally weighted regression provided the needed precision. <p> Locally weighted regression can also be modified to return a confidence interval along with its prediction. This can be done heuristically with the local density of the data providing an uncertainty estimate <ref> (Moore, 1991a) </ref> or by making sensible statistical assumptions (Schaal and Atkeson, 1994b; Cohn et al., 1995). In either case, this has been shown empirically to dramatically reduce the amount of exploration needed when the uncertainty estimates guide the experiment design.
Reference: <author> Moore, A. W. </author> <year> (1991b). </year> <title> Variable Resolution Dynamic Programming: Efficiently Learning Action Maps in Multivariate Real-valued State-spaces. </title> <editor> In Birnbaum, L. and Collins, G., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop. </booktitle> <publisher> Morgan Kaufmann. 31 Moore, </publisher> <editor> A. W. </editor> <year> (1992). </year> <title> Fast, Robust Adaptive Control by Learning only Forward Models. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 571-578. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130. </pages>
Reference: <author> Moore, A. W., Hill, D. J., and Johnson, M. P. </author> <year> (1992). </year> <title> An Empirical Investigation of Brute Force to Choose Features, Smoothers and Function Approximators. </title> <editor> In Hanson, S., Judd, S., and Petsche, T., editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3. </volume> <publisher> MIT Press. </publisher>
Reference: <author> Moore, A. W. and Lee, M. S. </author> <year> (1994). </year> <title> Efficient Algorithms for Minimizing Cross Validation Error. </title> <editor> In Cohen, W. W. and Hirsh, H., editors, </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. and Schneider, J. </author> <year> (1995). </year> <title> Memory-Based Stochastic Optimization. </title> <booktitle> In Proceedings of Neural Information Processing Systems Conference. </booktitle>
Reference-contexts: Lazy learning can be used to represent the cost function directly and to speed the search for maxima or minima <ref> (Moore and Schneider, 1995) </ref>. A linear local model can be used to estimate the first derivatives (gradient) and a quadratic local model can be used to estimate the second derivatives (Hessian) of the cost function at the current point in the optimization procedure.
Reference: <author> Omohundro, S. M. </author> <year> (1987). </year> <title> Efficient Algorithms with Neural Network Behaviour. </title> <journal> Journal of Complex Systems, </journal> <volume> 1(2) </volume> <pages> 273-347. </pages>
Reference: <author> Omohundro, S. M. </author> <year> (1991). </year> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 693-699. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Ortega, J. M. and Rheinboldt, W. C. </author> <year> (1970). </year> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Given a monotonic relationship between u and y, the sequence of actions that are chosen are closely related to the Secant method (Conte and De Boor, 1980) for numerically finding the zero of a function. See <ref> (Ortega and Rheinboldt, 1970) </ref> for a good discussion of the multidimensional generalization of the Secant method. An inverse model, represented using locally weighted regression and trained initially with a feedback learner, has been used by Atkeson (1990).
Reference: <author> Peng, J. </author> <year> (1995). </year> <title> Efficient memory-based dynamic programming. </title> <booktitle> In Prieditis and Russell (1995), </booktitle> <pages> pages 438-446. </pages>
Reference: <author> Peng, J. and Williams, R. J. </author> <year> (1993). </year> <title> Efficient Learning and Planning Within the Dyna Framework. </title> <booktitle> In Proceedings of the Second International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Pomerleau, D. </author> <year> (1994). </year> <title> Reliability estimation for neural network based autonomous driving. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <pages> 12. </pages>
Reference: <author> Preparata, F. P. and Shamos, M. </author> <year> (1985). </year> <title> Computational Geometry. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1988). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY. </address>
Reference-contexts: to y d . * Random Search: Generate random actions, and again use the action which is predicted to produce the closest outcome to y d . * First Order Gradient Search: Perform a steepest-ascent search from an initial candidate action toward an action that will give the desired output <ref> (Press et al., 1988) </ref>. 7 Finding the local gradient of the empirical model is easy if locally weighted regression is used (Atkeson et al., 1995). Part of the computation of the locally weighted regression model forms the local linear map, so it is already available. <p> This approach may become stuck in local minima, so an initial grid search or random search may provide a set of good starting points for gradient searches. * Second Order Gradient Search: Use Newton's method to iterate towards an action with the desired output <ref> (Press et al., 1988) </ref>. If u k is an approximate solution, Newton's method gives u k+1 as a better solution where u k+1 = u k + B 1 (y d c) (7) with B and c as defined in Equation 5. <p> If the partial derivative matrix B is singular, or the action space and state space differ in dimensionality, then robust matrix techniques based on the pseudo-inverse can be applied to invert B <ref> (Press et al., 1988) </ref>.
Reference: <author> Prieditis, A. and Russell, S., </author> <title> editors (1995). </title> <booktitle> Twelfth International Conference on Machine Learning, </booktitle> <address> Tahoe City, CA. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> Combining Instance-Based and Model-Based Learning. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference. </booktitle>
Reference: <editor> Saitta, L., editor (1996). </editor> <booktitle> Thirteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Schaal, S. and Atkeson, C. </author> <year> (1994a). </year> <title> Robot Juggling: An Implementation of Memory-based Learning. </title> <journal> Control Systems Magazine, </journal> <volume> 14(1) </volume> <pages> 57-71. </pages>
Reference-contexts: next state is always attainable in one step, the action may be chosen without paying attention to future states, decisions, or performance. 3.1.1 An Implementation of Deadbeat Control: Devil Sticking I Deadbeat control using lazy learning models was explored by implementing it for a juggling task known as devil sticking <ref> (Schaal and Atkeson, 1994a,b) </ref>. A center stick is batted back and forth between two handsticks. Figure 8 shows a sketch of our devil sticking robot. The juggling robot uses its top two joints to perform planar devil sticking. Hand sticks are mounted on the robot with springs and dampers. <p> However, the setpoint of the task can be manipulated during learning to improve exploration. This is done by the shifting setpoint algorithm (SSA) <ref> (Schaal and Atkeson, 1994a) </ref>. 16 SSA attempts to decompose the control problem into two separate control tasks on dif-ferent time scales. At the fast time scale, it acts as a dynamic regulator by trying to keep the controlled system at a chosen setpoint. <p> Subsequently, the learned model can be used for more sophisticated control algorithms, for planning, or for further exploration. 3.5 Dynamic Regulation With An Unspecified Setpoint: Devil Sticking III The SSA method was tested on the devil sticking juggling task <ref> (Schaal and Atkeson, 1994a,b) </ref>. In this case it had the following steps. 1.
Reference: <author> Schaal, S. and Atkeson, C. G. </author> <year> (1994b). </year> <title> Assessing the Quality of Local Linear Models. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 160-167. </pages> <publisher> Morgan Kaufmann. 32 Stanfill, </publisher> <editor> C. and Waltz, D. </editor> <year> (1986). </year> <title> Towards Memory-Based Reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228. </pages>
Reference: <author> Stengel, R. F. </author> <year> (1986). </year> <title> Stochastic Optimal Control. </title> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: One-step deadbeat control chooses actions to (in expectation) cause the immediate next state to be the desired next state <ref> (Stengel, 1986) </ref>. <p> This idea is posed precisely in the language of linear quadratic regulation (LQR), in which a long term quadratic cost criterion C is minimized that penalizes both state-errors and action magnitudes <ref> (Stengel, 1986) </ref>: 1 X T 1 X ffix T (t)Qffix (t) + u T (t)Ru (t) where Q and R are matrices whose elements set the tradeoff between the size of the action components and the error components. <p> 0), and we have the 15 following linear dynamics: ffix (t + 1) = Affix (t) + Bu (t) (16) The optimal action with respect to the criteria in Equation 14 and linear dynamics in Equation 16 can be obtained by solution of a matrix equation called the Ricatti equation <ref> (Stengel, 1986) </ref>. <p> We recommend <ref> (Stengel, 1986) </ref>. We also provide a very simplified self-contained derivation in Appendix A. The long term cost starting from state x d + ffix turns out to be ffix T Pffix. <p> Note that u is a linear function of the state x in Equation 17: u = Kffix (19) Linear quadratic regulation has useful robustness when compared to deadbeat controllers even if the underlying linear models are imprecise <ref> (Stengel, 1986) </ref>. 3.3 Implementation of Dynamic Regulation: Devil Sticking II Linear quadratic regulation controller design permitted successful devil sticking. It did require manual generation of training data to estimate the matrices of the local linear model: A and B.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to Predict by the Methods of Temporal Differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated Architecture for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. </title> <booktitle> In Proceedings of the 7th International Conference on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: All transitions between cells experienced by the system were remembered in a discrete state transition model. A learning algorithm similar to Dyna <ref> (Sutton, 1990) </ref> was used with full value iteration carried out on the discrete model every time-step. Exploration was achieved by assuming any unvisited state had a future cost of zero.
Reference: <author> Tadepalli, P. and Ok, D. </author> <year> (1996). </year> <title> Scaling up average reward reinforcement learning by approximating the domain models and the value function. </title> <note> In Saitta (1996). http://www.cs.orst.edu:80/~tadepall/research/publications.html. </note>
Reference: <author> Thrun, S. </author> <year> (1996). </year> <title> Is learning the n-th thing any easier than learning the first? In Advances in Neural Information Processing Systems (NIPS) 8. </title> <address> http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/publications.html. </address>
Reference: <author> Thrun, S. and O'Sullivan, J. </author> <year> (1996). </year> <title> Discovering structure in multiple learning tasks: The TC algorithm. </title> <editor> In Saitta (1996). http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/publications.html. van der Smagt, P., Groen, F., and van het Groenewoud, F. </editor> <year> (1994). </year> <title> The locally linear nested network for robot manipulation. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pages 2787-2792. </pages> <month> ftp://ftp.fwi.uva.nl/pub/computer-systems/aut-sys/reports/SmaGroGro94b.ps.gz. </month>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD. Thesis, </type> <institution> King's College, University of Cambridge. </institution>
Reference: <author> Zografski, Z. </author> <year> (1989). </year> <title> Neuromorphic, Algorithmic, and Logical Models for the Automatic Synthesis of Robot Action. </title> <type> PhD dissertation, </type> <institution> University of Ljubljana, Ljubljana, Slovenia, </institution> <address> Yugoslavia. </address>
Reference: <author> Zografski, Z. </author> <year> (1991). </year> <title> New methods of machine learning for the construction of integrated neuromorphic and associative-memory knowledge bases. </title> <editor> In Zajc, B. and Solina, F., editors, </editor> <booktitle> Proceedings, 6th Mediterranean Electrotechnical Conference, </booktitle> <volume> volume II, </volume> <pages> pages 1150-1153, </pages> <institution> Ljubljana, Slovenia, Yugoslavia. IEEE catalog number 91CH2964-5. </institution>
Reference: <author> Zografski, Z. </author> <year> (1992). </year> <title> Geometric and neuromorphic learning for nonlinear modeling, control and forecasting. </title> <booktitle> In Proceedings of the 1992 IEEE International Symposium on Intelligent Control, </booktitle> <pages> pages 158-163, </pages> <address> Glasgow, Scotland. </address> <publisher> IEEE catalog number 92CH3110-4. </publisher>
Reference: <author> Zografski, Z. and Durrani, T. </author> <year> (1995). </year> <title> Comparing predictions from neural networks and memory-based learning. </title> <booktitle> In Proceedings, ICANN '95/NEURONIMES '95: International Conference on Artificial Neural Networks, </booktitle> <pages> pages 221-226, </pages> <address> Paris, France. </address> <month> 33 </month>
References-found: 64

