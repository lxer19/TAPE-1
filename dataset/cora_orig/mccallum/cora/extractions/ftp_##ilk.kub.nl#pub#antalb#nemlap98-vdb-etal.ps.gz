URL: ftp://ilk.kub.nl/pub/antalb/nemlap98-vdb-etal.ps.gz
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Email: fantalb,walterg@kub.nl A.J.M.M.Weijters@tm.tue.nl  
Title: Modularity in Inductively-Learned Word Pronunciation Systems  
Author: Antal van den Bosch Ton Weijters Walter Daelemans 
Address: P.O. Box 90153 P.O. Box 513 NL-5000 LE Tilburg NL-5600 MB Eindhoven The Netherlands The Netherlands  
Affiliation: 1 ILK Computational Linguistics 2 Department of Information Technology Tilburg University Eindhoven University of Technology  
Abstract: In leading morpho-phonological theories and state-of-the-art text-to-speech systems it is assumed that word pronunciation cannot be learned or performed without in-between analyses at several abstraction levels (e.g., morphological, graphemic, phonemic, syllabic, and stress levels). We challenge this assumption for the case of English word pronunciation. Using igtree, an inductive-learning decision-tree algorithms, we train and test three word-pronunciation systems in which the number of abstraction levels (implemented as sequenced modules) is reduced from five, via three, to one. The latter system, classifying letter strings directly as mapping to phonemes with stress markers, yields significantly better generalisation accuracies than the two multi-module systems. Analyses of empirical results indicate that positive utility effects of sequencing modules are outweighed by cascading er rors passed on between modules.
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J., S. Hunnicutt, and D. Klatt. </author> <year> 1987. </year> <title> From text to speech: The mitalk system. </title> <address> Cambridge, UK: </address> <publisher> Cam-bidge University Press. </publisher>
Reference-contexts: In the following subsections, each system is introduced, an outline is given of the experiments performed on the system, and the results are briefly discussed. 3.1 M-A-G-Y-S The architecture of the m-a-g-y-s system is inspired by sound1 (Hunnicutt, 1976; Hunnicutt, 1980), the word-pronunciation subsystem of the mitalk text-to-speech system <ref> (Allen, Hunnicutt, and Klatt, 1987) </ref>. When the mitalk system is faced with an unknown word, sound1 produces on the basis of that word a phonemic transcription with stress markers (Allen, Hunnicutt, and Klatt, 1987). <p> architecture of the m-a-g-y-s system is inspired by sound1 (Hunnicutt, 1976; Hunnicutt, 1980), the word-pronunciation subsystem of the mitalk text-to-speech system <ref> (Allen, Hunnicutt, and Klatt, 1987) </ref>. When the mitalk system is faced with an unknown word, sound1 produces on the basis of that word a phonemic transcription with stress markers (Allen, Hunnicutt, and Klatt, 1987). This word-pronunciation process is divided into the following five processing components: 1. morphological segmentation, which we imple ment as the module referred to as m; 2. graphemic parsing, module a; 3. grapheme-phoneme conversion, module g; 4. syllabification, module y; 5. stress assignment, module s.
Reference: <author> Bloomfield, L. </author> <year> 1933. </year> <title> Language. </title> <address> New York: </address> <publisher> Holt, Rine-hard and Winston. </publisher>
Reference-contexts: Influential pre-Chomskyan linguistic theories have been pointing at the analogy principle as the underlying principle for language learning (De Saussure, 1916), and at induction as the reasoning method for generalising from learned instances of language tasks to new instances through analogy <ref> (Bloomfield, 1933) </ref>. However, methods and resources (e.g., computer technology) were not available then to demonstrate how induction through analogy could be employed to learn and model language tasks.
Reference: <author> Breiman, L., J. Friedman, R. Ohlsen, and C. Stone. </author> <year> 1984. </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Burnage, G., </author> <year> 1990. </year> <title> celex: A guide for users. Centre for Lexical Information, </title> <address> Nijmegen. </address>
Reference-contexts: In case (ii), we use the most probable classification on the last non-terminal node most recently visited instead. 2.2 Data Acquisition and Preprocessing The resource of word-pronunciation instances used in our experiments is the celex lexical data base of English <ref> (Burnage, 1990) </ref>. All items in the celex data bases contain hyphenated spelling, syllabified and stressed phonemic transcriptions, and detailed morphological analyses.
Reference: <author> Chomsky, N. and M. Halle. </author> <year> 1968. </year> <title> The sound pattern of English. </title> <address> New York, NY: </address> <publisher> Harper and Row. </publisher>
Reference-contexts: Applied to morpho-phonology, the argument states that generic learning methods are not able to discover morphology, graphematics, and stress patterns autonomously when learning word pronunciation, although this knowledge appears essential. Phonological and morphological theories, influenced by Chom-skyan theory across the board since the publication of spe <ref> (Chomsky and Halle, 1968) </ref>, have generally adopted the idea of abstraction levels in various guises (e.g., levels, tapes, tiers, grids) (Goldsmith, 1976; Liberman and Prince, 1977; Kosken-niemi, 1984; Mohanan, 1986).
Reference: <author> Daelemans, W. </author> <year> 1988. </year> <title> Grafon: A grapheme-to-phoneme system for Dutch. </title> <booktitle> In Proceedings Twelfth International Conference on Computational Linguistics (COLING-88), Budapest, </booktitle> <pages> pages 133-138. </pages>
Reference: <author> Daelemans, W. </author> <year> 1996. </year> <title> Experience-driven language acquisition and processing. </title> <editor> In M. Van der Avoird and C. Corsius, editors, </editor> <booktitle> Proceedings of the CLS Opening Academic Year 1996-1997. Tilburg: CLS, </booktitle> <pages> pages 83-95. </pages>
Reference: <author> Daelemans, W., S. Gillis, and G. Durieux. </author> <year> 1994. </year> <title> The acquisition of stress: a data-oriented approach. </title> <journal> Computational Linguistics, </journal> <volume> 20(3) </volume> <pages> 421-451. </pages>
Reference: <author> Daelemans, W. and A. Van den Bosch. </author> <year> 1992. </year> <title> Generali-sation performance of backpropagation learning on a syllabification task. </title> <editor> In M. F. J. Drossaers and A. Ni-jholt, editors, TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing, </booktitle> <pages> pages 27-37, </pages> <institution> Enschede. Twente University. </institution>
Reference: <author> Daelemans, W. and A. Van den Bosch. </author> <year> 1997. </year> <title> Language-independent data-oriented grapheme-to-phoneme conversion. </title> <editor> In J. P. H. Van Santen, R. W. Sproat, J. </editor> <address> P. </address>
Reference-contexts: We do this by applying an inductive-learning algorithm from machine learning to word pronunciation. From a wealth of existing algorithms in machine learning (Mitchell, 1997), we choose igtree <ref> (Daelemans, Van den Bosch, and Weijters, 1997) </ref>, an inductive-learning decision-tree learning algorithm. igtree is a fast algorithm which has been demonstrated to be applicable to language tasks (Van den Bosch and Daelemans, 1993; Van den Bosch, Daelemans, and Weijters, 1996; Daelemans, Van den Bosch, and Weijters, 1997). <p> In Section 4 we compare the three systems and analyse the consequences of modularisation. Section 5 briefly mentions related work on inductive learning of word pronunciation. Section 6 summarises the results obtained and lists some points of discussion. 2 Algorithm, Data, Methodology 2.1 Algorithm: IGTREE igtree <ref> (Daelemans, Van den Bosch, and Weij-ters, 1997) </ref> is a top-down induction of decision trees (tdidt) algorithm (Breiman et al., 1984; Quinlan, 1993). tdidt is a widely-used method in supervised machine learning (Mitchell, 1997). igtree is designed as an optimised approximation of the instance-based learning algorithm ib1-ig (Daele-mans and Van den Bosch, <p> We used an automatic alignment algorithm <ref> (Daelemans and Van den Bosch, 1997) </ref> to determine which letters are the first or only letters of a grapheme. letter-window instances phoneme-window instances instance left right classifications left right classif. number context focus context m a g s gs context focus context y s 1 b o o k 1 1
Reference: <editor> Olive, and J. Hirschberg, editors, </editor> <booktitle> Progress in Speech Processing. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> pages 77-89. </pages>
Reference: <author> Daelemans, W., A. Van den Bosch, and A. Weijters. </author> <year> 1997. </year> <title> igtree: using trees for classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 407-423. </pages>
Reference-contexts: We do this by applying an inductive-learning algorithm from machine learning to word pronunciation. From a wealth of existing algorithms in machine learning (Mitchell, 1997), we choose igtree <ref> (Daelemans, Van den Bosch, and Weijters, 1997) </ref>, an inductive-learning decision-tree learning algorithm. igtree is a fast algorithm which has been demonstrated to be applicable to language tasks (Van den Bosch and Daelemans, 1993; Van den Bosch, Daelemans, and Weijters, 1996; Daelemans, Van den Bosch, and Weijters, 1997). <p> In Section 4 we compare the three systems and analyse the consequences of modularisation. Section 5 briefly mentions related work on inductive learning of word pronunciation. Section 6 summarises the results obtained and lists some points of discussion. 2 Algorithm, Data, Methodology 2.1 Algorithm: IGTREE igtree <ref> (Daelemans, Van den Bosch, and Weij-ters, 1997) </ref> is a top-down induction of decision trees (tdidt) algorithm (Breiman et al., 1984; Quinlan, 1993). tdidt is a widely-used method in supervised machine learning (Mitchell, 1997). igtree is designed as an optimised approximation of the instance-based learning algorithm ib1-ig (Daele-mans and Van den Bosch, <p> We used an automatic alignment algorithm <ref> (Daelemans and Van den Bosch, 1997) </ref> to determine which letters are the first or only letters of a grapheme. letter-window instances phoneme-window instances instance left right classifications left right classif. number context focus context m a g s gs context focus context y s 1 b o o k 1 1
Reference: <author> De Saussure, F. </author> <year> 1916. </year> <note> Course de linguistique generale. </note>
Reference-contexts: Inconsistency. Much of the analogy in English word pronunciation is disrupted by productive and complex word morphology, word stress, and graphematics. Influential pre-Chomskyan linguistic theories have been pointing at the analogy principle as the underlying principle for language learning <ref> (De Saussure, 1916) </ref>, and at induction as the reasoning method for generalising from learned instances of language tasks to new instances through analogy (Bloomfield, 1933).
Reference: <author> Paris: Payot. edited posthumously by C. Bally and A. </author> <month> Riedlinger. </month>
Reference: <author> Dietterich, T. G., H. Hild, and G. Bakiri. </author> <year> 1995. </year> <title> A comparison of id3 and backpropagation for English text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 5-28. </pages>
Reference-contexts: In terms of incorrectly processed test instances, (Shavlik, Mooney, and Towell, 1991) obtain better performance with the back-propagation algorithm trained on distributed output (27.7% errors) than with the id3 (Quinlan, 1986) decision-tree algorithm (34.7% errors), both trained and tested on small non-overlapping sets of about 1,000 instances. <ref> (Dietterich, Hild, and Bakiri, 1995) </ref> reports similar errors on similarly-sized training and test sets (29.1% for bp and 34.4% for id3); with a larger training set of 19,003 words from the nettalk data and an input encoding fifteen letters, previous phoneme and stress classifications, some domain-specific features, and error-correcting output codes
Reference: <author> Goldsmith, J. </author> <year> 1976. </year> <title> An overview of autosegmental phonology. </title> <journal> Linguistic Analysis, </journal> <volume> 2 </volume> <pages> 23-68. </pages>
Reference: <author> Hunnicutt, S. </author> <year> 1976. </year> <title> Phonological rules for a text-to-speech system. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> Microfiche 57 </volume> <pages> 1-72. </pages>
Reference: <author> Hunnicutt, S. </author> <year> 1980. </year> <title> Grapheme-phoneme rules: a review. </title> <type> Technical Report STL QPSR 2-3, </type> <institution> Speech Transmission Laboratory, KTH, Sweden. </institution>
Reference: <author> Koskenniemi, K. </author> <year> 1984. </year> <title> A general computational model for wordform recognition and production. </title> <booktitle> In Proceedings of the Tenth International Conference on Computational Linguistics / 22nd Annual Conference of the ACL, </booktitle> <pages> pages 178-181. </pages>
Reference: <author> Liberman, M. and A. Prince. </author> <year> 1977. </year> <title> On stress and linguistic rhythm. </title> <journal> Linguistic Inquiry, </journal> (8):249-336. 
Reference: <author> Mitchell, T. </author> <year> 1997. </year> <title> Machine learning. </title> <address> New York, NY: </address> <publisher> McGraw Hill. </publisher>
Reference-contexts: In this paper we challenge the assumption that levels of abstraction must be made explicit in learning and performing the word-pronunciation task. We do this by applying an inductive-learning algorithm from machine learning to word pronunciation. From a wealth of existing algorithms in machine learning <ref> (Mitchell, 1997) </ref>, we choose igtree (Daelemans, Van den Bosch, and Weijters, 1997), an inductive-learning decision-tree learning algorithm. igtree is a fast algorithm which has been demonstrated to be applicable to language tasks (Van den Bosch and Daelemans, 1993; Van den Bosch, Daelemans, and Weijters, 1996; Daelemans, Van den Bosch, and Weijters, <p> 6 summarises the results obtained and lists some points of discussion. 2 Algorithm, Data, Methodology 2.1 Algorithm: IGTREE igtree (Daelemans, Van den Bosch, and Weij-ters, 1997) is a top-down induction of decision trees (tdidt) algorithm (Breiman et al., 1984; Quinlan, 1993). tdidt is a widely-used method in supervised machine learning <ref> (Mitchell, 1997) </ref>. igtree is designed as an optimised approximation of the instance-based learning algorithm ib1-ig (Daele-mans and Van den Bosch, 1992; Daelemans, Van den Bosch, and Weijters, 1997).
Reference: <author> Mohanan, K. P. </author> <year> 1986. </year> <title> The theory of lexical phonology. </title>
Reference: <editor> Dordrecht: D. </editor> <publisher> Reidel. </publisher>
Reference: <editor> Piatelli-Palmarini, M., editor. </editor> <year> 1980. </year> <title> Language learning: The debate between Jean Piaget and Noam Chomsky. </title> <address> Cambridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-206. </pages>
Reference-contexts: Nodes are connected via arcs denoting feature values. Information gain is used in igtree to determine the order in which feature values are added as arcs to the tree. Information gain is a function from information theory, and is used similarly in id3 <ref> (Quinlan, 1986) </ref> and c4.5 (Quinlan, 1993). <p> To our knowledge, only (Shavlik, Mooney, and Towell, 1991) and (Di-etterich, Hild, and Bakiri, 1995) provides such reports. In terms of incorrectly processed test instances, (Shavlik, Mooney, and Towell, 1991) obtain better performance with the back-propagation algorithm trained on distributed output (27.7% errors) than with the id3 <ref> (Quinlan, 1986) </ref> decision-tree algorithm (34.7% errors), both trained and tested on small non-overlapping sets of about 1,000 instances. (Dietterich, Hild, and Bakiri, 1995) reports similar errors on similarly-sized training and test sets (29.1% for bp and 34.4% for id3); with a larger training set of 19,003 words from the nettalk data
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> c4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Nodes are connected via arcs denoting feature values. Information gain is used in igtree to determine the order in which feature values are added as arcs to the tree. Information gain is a function from information theory, and is used similarly in id3 (Quinlan, 1986) and c4.5 <ref> (Quinlan, 1993) </ref>. <p> The expression D [f i =v j ] 1 igtree can function with any feature weighting method, such as gain ratio <ref> (Quinlan, 1993) </ref>; for all experiments reported here, information gain was used. refers to those patterns in the data base that have value v j for feature f i , j is the number of possible values of f i , and V is the set of possible values for feature f
Reference: <author> Sejnowski, T. J. and C. S. Rosenberg. </author> <year> 1987. </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168. </pages>
Reference-contexts: For use in experiments with learning algorithms, the data is preprocessed to derive fixed-size instances. In the experiments reported in this paper different morpho-phonological (sub)tasks are investigated; for each (sub)task, an instance base (training set) is constructed containing instances produced by windowing <ref> (Sejnowski and Rosenberg, 1987) </ref> and attaching to each instance the classification appropriate for the (sub)task under investigation. Table 1 displays example instances derived from the sample word booking. With this method, for each (sub) task an instance base of 675,745 instances is built. <p> Table 1). This task integration is also used in the nettalk model <ref> (Sejnowski and Rosenberg, 1987) </ref>. A similar argument can be made for integrating the syllabification and stress assignment modules into a single stress-assignment module. Stress markers, in our definition of the stress-assignment subtask, are placed solely on the positions which are also marked as syllable boundaries (i.e., on syllable-initial phonemes). <p> this positive utility effect does not lead to optimal accuracy on the s subtask; in the gs system, stress assignment is performed with letters as input, yielding the best accuracy on stress assignment in our investigations, viz. 3.97% incorrectly classified test instances. 5 Related work The classical nettalk paper by <ref> (Sejnowski and Rosenberg, 1987) </ref> can be seen as a primary source of inspiration for the present study; it has been so for a considerable amount of related work.
Reference: <author> Shavlik, J. W., R. J. Mooney, and G. G. Towell. </author> <year> 1991. </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143. </pages>
Reference-contexts: However, few reports have been made on the joint accuracies on stress markers and phonemes in work on the nettalk data. To our knowledge, only <ref> (Shavlik, Mooney, and Towell, 1991) </ref> and (Di-etterich, Hild, and Bakiri, 1995) provides such reports. In terms of incorrectly processed test instances, (Shavlik, Mooney, and Towell, 1991) obtain better performance with the back-propagation algorithm trained on distributed output (27.7% errors) than with the id3 (Quinlan, 1986) decision-tree algorithm (34.7% errors), both trained <p> However, few reports have been made on the joint accuracies on stress markers and phonemes in work on the nettalk data. To our knowledge, only <ref> (Shavlik, Mooney, and Towell, 1991) </ref> and (Di-etterich, Hild, and Bakiri, 1995) provides such reports. In terms of incorrectly processed test instances, (Shavlik, Mooney, and Towell, 1991) obtain better performance with the back-propagation algorithm trained on distributed output (27.7% errors) than with the id3 (Quinlan, 1986) decision-tree algorithm (34.7% errors), both trained and tested on small non-overlapping sets of about 1,000 instances. (Dietterich, Hild, and Bakiri, 1995) reports similar errors on similarly-sized training
Reference: <author> Stanfill, C. and D. Waltz. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the acm, </journal> <volume> 29(12) </volume> <pages> 1213-1228. </pages>
Reference: <author> Van den Bosch, A. </author> <year> 1997. </year> <title> Learning to pronounce written words, a study in inductive language learning. </title> <type> Ph.D. thesis, </type> <institution> Universiteit Maastricht. </institution>
Reference-contexts: We do this by applying an inductive-learning algorithm from machine learning to word pronunciation. From a wealth of existing algorithms in machine learning (Mitchell, 1997), we choose igtree <ref> (Daelemans, Van den Bosch, and Weijters, 1997) </ref>, an inductive-learning decision-tree learning algorithm. igtree is a fast algorithm which has been demonstrated to be applicable to language tasks (Van den Bosch and Daelemans, 1993; Van den Bosch, Daelemans, and Weijters, 1996; Daelemans, Van den Bosch, and Weijters, 1997). <p> In Section 4 we compare the three systems and analyse the consequences of modularisation. Section 5 briefly mentions related work on inductive learning of word pronunciation. Section 6 summarises the results obtained and lists some points of discussion. 2 Algorithm, Data, Methodology 2.1 Algorithm: IGTREE igtree <ref> (Daelemans, Van den Bosch, and Weij-ters, 1997) </ref> is a top-down induction of decision trees (tdidt) algorithm (Breiman et al., 1984; Quinlan, 1993). tdidt is a widely-used method in supervised machine learning (Mitchell, 1997). igtree is designed as an optimised approximation of the instance-based learning algorithm ib1-ig (Daele-mans and Van den Bosch, <p> We used an automatic alignment algorithm <ref> (Daelemans and Van den Bosch, 1997) </ref> to determine which letters are the first or only letters of a grapheme. letter-window instances phoneme-window instances instance left right classifications left right classif. number context focus context m a g s gs context focus context y s 1 b o o k 1 1 <p> Earlier experiments performed on the tasks investigated in this paper have shown that classification errors on test instances are indeed consistently and significantly decreased when modules are trained on the output of previous modules rather than on data extracted directly from celex <ref> (Van den Bosch, 1997) </ref>. Therefore, we train the m-a-g-y-s system, with igtree, by training the modules of the system on the output of predecessing modules. <p> Although irregular errors are an inherent problem for modular systems, other learning algorithms may be able to handle such errors differently. Experiments with back-propagation learning applied to the same modular systems show siginficantly worse performance than that of igtree <ref> (Van den Bosch, 1997) </ref>. <p> Although such systems trained with ib1-ig would be compu-tationally rather inefficient <ref> (Van den Bosch, 1997) </ref>, employing ib1-ig in learning modular subtasks may lead to other differences in accuracy between modular systems.
Reference: <author> Van den Bosch, A. and W. Daelemans. </author> <year> 1993. </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> pages 45-53. </pages>
Reference: <author> Van den Bosch, A., W. Daelemans, and A. Weij-ters. </author> <year> 1996. </year> <title> Morphological analysis as classification: an inductive-learning approach. </title> <editor> In K. Oflazer and H. Somers, editors, </editor> <booktitle> Proceedings of NeMLaP-2, Ankara, Turkey, </booktitle> <pages> pages 79-89. </pages>
Reference: <author> Weijters, A. </author> <year> 1991. </year> <title> A simple look-up procedure superior to NETtalk? In Proceedings of icann-91, </title> <address> Espoo, Finland. </address>
Reference: <author> Weiss, S. and C. Kulikowski. </author> <year> 1991. </year> <title> Computer systems that learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: combination of these classes (159 combined phonemes and stress markers, gs). 2.3 Methodology Our empirical study focuses on measuring the ability of the igtree learning algorithm to use the knowledge accumulated during learning for the classification of new, unseen instances of the same (sub)task, i.e., we measure their generalisation accuracy. <ref> (Weiss and Kulikowski, 1991) </ref> describe n-fold cross validation (n-fold cv) as a procedure for mea 2 Graphemic parsing is not represented in the celex data. <p> (iv) The tree is tested by letting it classify all instances in the test set, which results in a percentage of incorrectly classified test instances. (v) When each of the n folds has produced an error percentage on test material, a mean generalisation error of the learned model is computed. <ref> (Weiss and Kulikowski, 1991) </ref> argue that by using n-fold cv, preferably with n 10, one can retrieve a good estimate of the true generalisation error of a learning algorithm given an instance base. Mean results can be employed further in significance tests.
Reference: <author> Wolpert, D. H. </author> <year> 1990. </year> <title> Constructing a generalizer superior to NETtalk via a mathematical theory of generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 445-452. </pages>
Reference: <author> Yvon, F. </author> <year> 1996. </year> <title> Prononcer par analogie: motivation, formalisation et evaluation. </title> <type> Ph.D. thesis, </type> <institution> Ecole Na-tionale Superieure des Telecommunication, Paris. </institution>
Reference-contexts: An interesting counterargument against the representation of the word-pronunciation task using fixed-size windows, put forward by Yvon <ref> (Yvon, 1996) </ref>, is that an inductive-learning approach to grapheme-phoneme conversion should be based on associating variable-length chunks of letters to variable-length chunks of phonemes. <p> The chunk-based approach is shown to be applicable, with adequate accuracy, to several corpora, including corpora of French word pronunciations and, as mentioned above, the nettalk data <ref> (Yvon, 1996) </ref>.
References-found: 36

