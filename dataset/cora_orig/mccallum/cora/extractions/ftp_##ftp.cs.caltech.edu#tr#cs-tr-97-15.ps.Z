URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-97-15.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Email: (eric@cs.caltech.edu).  
Title: Improved Uniform Test Error Bounds  
Author: Eric Bax 
Keyword: Key words machine learning, learning theory, generalization, Vapnik-Chervonenkis.  
Address: ifornia, 91125  
Affiliation: Computer Science Department, California Institute of Technology 256-80, Pasadena, Cal  
Date: August 18, 1997  
Abstract: We derive distribution-free uniform test error bounds that improve on VC-type bounds for validation. We show how to use knowledge of test inputs to improve the bounds. The bounds are sharp, but they require intense computation. We introduce a method to trade sharpness for speed of computation. Also, we compute the bounds for several test cases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abu-Mostafa, Y.S. </author> <year> (1996). </year> <title> What you need to know about the VC inequality. Class notes from CS156, </title> <institution> California Institute of Technology. </institution>
Reference-contexts: Since the trained classifiers are chosen without reference to the validation data, the trained classifiers play the role of a class and the validation data plays the role of training data in a simplified version of VC analysis <ref> [1] </ref>. If there are sufficiently few trained classifiers and enough validation data, then the error rates on validation data are uniformly good estimates of the error rates on out-of-sample data with high probability.
Reference: [2] <author> Abu-Mostafa, Y.S. </author> <year> (1989). </year> <title> The Vapnik-Chervonenkis dimension: information versus complexity in learning. </title> <journal> Neural Computation, </journal> <volume> 1 (3), </volume> <pages> 312-317. </pages>
Reference-contexts: Bounds that are uniform over a set of classifiers have applications to model selection and validation. Model selection is the choice, without reference to the training data, of a class of classifiers from which to select a classifier through training. VC analysis <ref> [2, 3, 7, 12, 11] </ref> shows that if the class is sufficiently restricted and there are enough training data, then the error rates on training data are uniformly good estimates of the error rates on out-of-sample data with high probability.
Reference: [3] <author> Baum, E. B., and Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <type> 1 (1), </type> <pages> 151-160. </pages>
Reference-contexts: Bounds that are uniform over a set of classifiers have applications to model selection and validation. Model selection is the choice, without reference to the training data, of a class of classifiers from which to select a classifier through training. VC analysis <ref> [2, 3, 7, 12, 11] </ref> shows that if the class is sufficiently restricted and there are enough training data, then the error rates on training data are uniformly good estimates of the error rates on out-of-sample data with high probability.
Reference: [4] <author> Bax, E., Cataltepe, Z., and Sill, J. </author> <year> (1997). </year> <title> Alternative error bounds for the classifier chosen by early stopping. </title> <publisher> CalTech-CS-TR-97-08. </publisher>
Reference-contexts: Hence, the classifier or classifiers selected because they have low validation error are likely to have low out-of-sample error as well. In this paper, we develop improved uniform bounds. These bounds apply directly to validation schemes with few classifiers, including early stopping with central classifiers <ref> [4] </ref> and a method to validate voting committees and other stacked classifiers [5]. In these schemes, uniform error bounds are required over only a few classifiers, and error bounds for the remaining classifiers are derived by inference. <p> These tests apply to validation of the classifier chosen by early stopping using the method of central classifiers <ref> [4] </ref>. In this scheme, a snapshot of the classifier is recorded after each epoch of training. These snapshots are sampled at intervals, forming a set of central classifiers. Uniform error bounds are computed for the central classifiers.
Reference: [5] <author> Bax, E. </author> <year> (1997). </year> <title> Validation of voting committees. </title> <publisher> CalTech-CS-TR-97-13. </publisher>
Reference-contexts: In this paper, we develop improved uniform bounds. These bounds apply directly to validation schemes with few classifiers, including early stopping with central classifiers [4] and a method to validate voting committees and other stacked classifiers <ref> [5] </ref>. In these schemes, uniform error bounds are required over only a few classifiers, and error bounds for the remaining classifiers are derived by inference. <p> These uniform bounds can be used to select a classifier or to bound the test error of a voting committee or some other stacked classifier [8, 10, 13] by reference <ref> [5] </ref>. In each of 10 tests, the 666 examples were randomly partitioned into 444 training examples, d = 111 validation examples, and d 0 = 111 test examples. In each test, 10 classifiers were trained using early stopping.
Reference: [6] <author> Bax, E. </author> <year> (1997). </year> <title> Similar classifiers and VC error bounds. </title> <publisher> CalTech-CS-TR-97-14. </publisher>
Reference-contexts: To derive distribution-free bounds, use the bound for the worst-case arrangement of training and test inputs (as in the definition of the growth function [12].) For details, see <ref> [12, 6] </ref>. For any but the smallest problems, direct use of the improved bounds is computationally infeasible. However, the mixed bounds prove useful. In [6], the representative classifiers are partitioned into small sets of classifiers with few disagreements. The mixed bounds over these partitions improve VC bounds. <p> For any but the smallest problems, direct use of the improved bounds is computationally infeasible. However, the mixed bounds prove useful. In <ref> [6] </ref>, the representative classifiers are partitioned into small sets of classifiers with few disagreements. The mixed bounds over these partitions improve VC bounds. The computation required for the improved bounds restricts their utility. There are several analytic and algorithmic approaches that could yield computational reductions.
Reference: [7] <author> Blumer, A., Ehrenfeucht, A., and Haussler, D. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36 (4), </volume> <pages> 929-965. </pages>
Reference-contexts: Bounds that are uniform over a set of classifiers have applications to model selection and validation. Model selection is the choice, without reference to the training data, of a class of classifiers from which to select a classifier through training. VC analysis <ref> [2, 3, 7, 12, 11] </ref> shows that if the class is sufficiently restricted and there are enough training data, then the error rates on training data are uniformly good estimates of the error rates on out-of-sample data with high probability.
Reference: [8] <author> Breiman, L. </author> <year> (1992). </year> <title> Stacked regressions. </title> <type> Tech. Rep. No. 367, </type> <institution> Statistics Dept., Univ. of California at Berkeley. </institution>
Reference-contexts: These uniform bounds can be used to select a classifier or to bound the test error of a voting committee or some other stacked classifier <ref> [8, 10, 13] </ref> by reference [5]. In each of 10 tests, the 666 examples were randomly partitioned into 444 training examples, d = 111 validation examples, and d 0 = 111 test examples. In each test, 10 classifiers were trained using early stopping.
Reference: [9] <author> Sill, J. and Abu-Mostafa, Y. </author> <year> (1997). </year> <note> Monotonicity hints. to appear in Advances in Neural Information Processing Systems, 9.. </note>
Reference-contexts: The discrete-valued traits were removed, leaving the six continous-valued traits. Of the 690 examples in the original database, 24 examples had at least one trait missing. These examples were removed, leaving 666 examples. The data were cleaned by Joseph Sill. For further information, see <ref> [9] </ref>. The classifiers are artificial neural networks with six input units, six hidden units, and one output unit. The hidden and output units have tanh activation functions. The initial weights were selected independently and uniformly at random from [0:1; 0:1].
Reference: [10] <author> Sridhar, D. V., Seagrave, R. C., and Bartlett, E. B. </author> <year> (1996). </year> <title> Process modeling using stacked neural networks. </title> <journal> AIChE Journal, </journal> <pages> 42(9) 2529-2539. </pages>
Reference-contexts: These uniform bounds can be used to select a classifier or to bound the test error of a voting committee or some other stacked classifier <ref> [8, 10, 13] </ref> by reference [5]. In each of 10 tests, the 666 examples were randomly partitioned into 444 training examples, d = 111 validation examples, and d 0 = 111 test examples. In each test, 10 classifiers were trained using early stopping.
Reference: [11] <author> Vapnik, V. N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data p.31. </title> <publisher> Springer-Verlag New York, Inc. </publisher>
Reference-contexts: Bounds that are uniform over a set of classifiers have applications to model selection and validation. Model selection is the choice, without reference to the training data, of a class of classifiers from which to select a classifier through training. VC analysis <ref> [2, 3, 7, 12, 11] </ref> shows that if the class is sufficiently restricted and there are enough training data, then the error rates on training data are uniformly good estimates of the error rates on out-of-sample data with high probability.
Reference: [12] <author> Vapnik, V. N. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory Prob. Appl.. </journal> <volume> 16: </volume> <pages> 264-280. </pages>
Reference-contexts: Bounds that are uniform over a set of classifiers have applications to model selection and validation. Model selection is the choice, without reference to the training data, of a class of classifiers from which to select a classifier through training. VC analysis <ref> [2, 3, 7, 12, 11] </ref> shows that if the class is sufficiently restricted and there are enough training data, then the error rates on training data are uniformly good estimates of the error rates on out-of-sample data with high probability. <p> To derive distribution-free bounds, use the bound for the worst-case arrangement of training and test inputs (as in the definition of the growth function <ref> [12] </ref>.) For details, see [12, 6]. For any but the smallest problems, direct use of the improved bounds is computationally infeasible. However, the mixed bounds prove useful. In [6], the representative classifiers are partitioned into small sets of classifiers with few disagreements. <p> To derive distribution-free bounds, use the bound for the worst-case arrangement of training and test inputs (as in the definition of the growth function [12].) For details, see <ref> [12, 6] </ref>. For any but the smallest problems, direct use of the improved bounds is computationally infeasible. However, the mixed bounds prove useful. In [6], the representative classifiers are partitioned into small sets of classifiers with few disagreements. The mixed bounds over these partitions improve VC bounds. <p> For example, for large enough * and validation and test sets of equal size, it is known that the worst-case w for a single classifier is w ; = n 2 <ref> [12] </ref>. It would also be useful to identify the worst-case distributions for the bound (17) with restrictions imposed by knowledge of the test inputs.
Reference: [13] <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks. </booktitle> <volume> 5: </volume> <pages> 241-259. </pages>
Reference-contexts: These uniform bounds can be used to select a classifier or to bound the test error of a voting committee or some other stacked classifier <ref> [8, 10, 13] </ref> by reference [5]. In each of 10 tests, the 666 examples were randomly partitioned into 444 training examples, d = 111 validation examples, and d 0 = 111 test examples. In each test, 10 classifiers were trained using early stopping.
References-found: 13

