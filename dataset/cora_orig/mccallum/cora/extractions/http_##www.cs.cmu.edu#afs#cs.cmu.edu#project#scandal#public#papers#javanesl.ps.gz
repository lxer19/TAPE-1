URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/javanesl.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/javanesl97.html
Root-URL: 
Email: jch@cs.cmu.edu  girija@cs.cmu.edu  sipelste@susq.com  
Title: Interactive Simulations on the Web: Compiling Nesl into Java  
Author: Jonathan C. Hardwick Girija Narlikar Jay Sipelstein 
Abstract: We motivate and describe the design and implementation of a system for compiling the high-level programming language Nesl into Java. As well as increasing the portability of Nesl, this system has enabled us to make existing simulations and algorithm animations available in applet form on the web. We present performance results showing that current Java virtual machines running the generated code achieve about half the performance of a native implementation of Nesl. We conclude that the use of Java as an intermediate language is a viable way to improve the portability of existing high-level programming languages for scientific simulation and computation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ken Arnold and James Gosling. </author> <title> The Java Programming Language. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Java <ref> [1] </ref> has several attractions as a language for scientific computation and simulation. Its implementation as a secure virtual machine with a portable windowing model enables graphical simulations to run safely on client computers. Java's object orientation, strong typing, garbage collection, and predefined libraries all ease the implementation of new applications.
Reference: [2] <author> Aart J.C. Bik and Dennis Gannon. </author> <title> Automatically exploiting implicit parallelism in Java. </title> <journal> Concurrency: Practice and Experience, </journal> <month> June </month> <year> 1997. </year> <note> See also http://www.extreme.indiana.edu/~ajcbik/JAVAR/. </note>
Reference-contexts: Note that we do not currently exploit Nesl's parallelism in the generated Java code, although it should be straightforward to extend our system to use Java threads on an SMP <ref> [2] </ref>, or to use one of the parallel Java approaches discussed in [9]. In this paper, we discuss the design decisions and tradeoffs made in the development of the system, and we present a series of benchmarks to evaluate its performance. <p> However, by compiling into Java source code instead of bytecode, we can take advantage of source-level tools such as the JAVAR restructuring compiler for automatic parallelization <ref> [2] </ref>, and optimizing Java-to-C compilers for generating stand-alone executables [14, 17]. A recent study of interpreters (including one for Java) concluded that performance is related to the expressiveness of the language, the use of native libraries, and the way memory is accessed [18].
Reference: [3] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: for (int i = 0; i &lt; a.length; i++) - // loop over the elements... dst [i] = a [i] * b [i]; // ...multiplying them together - push (dst); // push the result onto the stack - Vcode implements Nesl's nesting of data structures efficiently by using segmented vectors <ref> [3] </ref>. Segmented vectors use two kinds of vectors to represent arbitrary sequence nesting: a normal non-nested vector to hold the data, and a series of specialized vectors (called segment descriptors) to describe how the data is subdivided.
Reference: [4] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 3.1). </title> <type> Technical Report CMU-CS-95-170, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: Finally, Section 7 describes related projects, and Section 8 summarizes the work and our conclusions. 2 The Nesl System Nesl <ref> [4] </ref> is a portable, high-level, functional, nested data-parallel language. The primary data structure in Nesl is the sequence, each element of which can itself be a sequence. Nesl code closely resembles high-level pseudocode, and places more responsibility on the compiler and runtime system for achieving good efficiency [7].
Reference: [5] <author> Guy E. Blelloch and Siddhartha Chatterjee. </author> <title> VCODE: A data-parallel intermediate language. </title> <booktitle> In Symposium on The Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 471-480, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The standard Nesl system consists of three layers, as shown in Figure 1 (see [7] for full details). The interactive front end of the system compiles Nesl programs into a machine-independent intermediate language called Vcode <ref> [5] </ref>. The front end then invokes a Vcode interpreter, which in turn calls the low-level Cvl vector library [6].
Reference: [6] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Margaret Reid-Miller, Jay Sipelstein, and Marco Zagha. CVL: </author> <title> A C vector library. </title> <type> Technical Report CMU-CS-93-114, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: The interactive front end of the system compiles Nesl programs into a machine-independent intermediate language called Vcode [5]. The front end then invokes a Vcode interpreter, which in turn calls the low-level Cvl vector library <ref> [6] </ref>. The primary advantage of using an interpreted intermediate language for Nesl is that it allows users to switch transparently between running their programs on workstations (for development) and supercomputers (for performance). translation, and dashed lines represent linkage to C libraries (rounded boxes). <p> The interpreter's main tasks are to manage the stack and vector memory, and to implement the vector operations via calls to Cvl (C Vector Library), a machine-specific library that implements an abstract vector machine <ref> [6] </ref>. 2 3 Compiling Nesl into Java We considered three different ways to improve Nesl's portability using Java: write a Vcode interpreter in Java, rewrite the Nesl compiler so that it generates Java, or write a translator from Vcode into Java.
Reference: [7] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: Moreover, certain application areas (for example, rapid prototyping of parallel algorithms) are better suited to special-purpose high-level languages than to Java's more general and low-level model. In this paper we describe a system to translate programs written in Nesl <ref> [7] </ref>, a high-level parallel programming language, into Java. Using this system we have translated several algorithm simulations|which previously ran only on X11-based Unix systems|into applets that are now available to anyone with a Java-capable browser. <p> The primary data structure in Nesl is the sequence, each element of which can itself be a sequence. Nesl code closely resembles high-level pseudocode, and places more responsibility on the compiler and runtime system for achieving good efficiency <ref> [7] </ref>. Therefore, the language is well-suited for the concise and simple expression of parallel algorithms. For example, quicksort can be written in 10 lines of Nesl, compared to 1700 lines of C with MPI (which includes code for explicit load-balancing). <p> Although Nesl was designed to explore support for nested data-parallelism, those issues are orthogonal to the point of this paper and are not discussed here. The standard Nesl system consists of three layers, as shown in Figure 1 (see <ref> [7] </ref> for full details). The interactive front end of the system compiles Nesl programs into a machine-independent intermediate language called Vcode [5]. The front end then invokes a Vcode interpreter, which in turn calls the low-level Cvl vector library [6]. <p> It uses a nested data-parallel algorithm. We give the source code and test data for the benchmarks in Appendix A. All three benchmarks have asymptotic running times that are linear in the size of the problem. Timings for the benchmarks have previously been reported in <ref> [7, 12] </ref>. 5.1 Methodology To minimize performance effects due to machine architecture, we used two different machines for benchmark-ing: a Sun SPARCstation 5/85 running Solaris 2.5, and a DX4-120 PC running Windows 95. Compilation was done using Nesl 3.1, gcc v2.7.0, and JDK 1.1.1, with full optimization.
Reference: [8] <author> Per Bothner and R. Alexander Milowsk. </author> <title> The Kawa Scheme interpreter project. </title> <address> http://www.copsol.com/kawa/. </address>
Reference-contexts: The interpreters are designed mainly for demonstration purposes rather than computationally-intensive simulations, because the extra level of interpretation significantly reduces their performance. Of the compilers, some generate Java bytecode directly for reasons of functionality (for example, the Kawa Scheme compiler <ref> [8] </ref> uses the GOTO bytecode instruction to perform tail-recursion elimination). However, by compiling into Java source code instead of bytecode, we can take advantage of source-level tools such as the JAVAR restructuring compiler for automatic parallelization [2], and optimizing Java-to-C compilers for generating stand-alone executables [14, 17].
Reference: [9] <author> Bryan Carpenter, Yuh-Jye Chang, Geoffrey Fox, Donald Leskiw, and Xiaoming Li. </author> <title> Experiments with HPJava. In Concurrency: </title> <journal> Practice and Experience, </journal> <month> June </month> <year> 1997. </year> <note> See also http://www.npac.syr.edu/users/dbc/HPJava/experiments/. </note>
Reference-contexts: Note that we do not currently exploit Nesl's parallelism in the generated Java code, although it should be straightforward to extend our system to use Java threads on an SMP [2], or to use one of the parallel Java approaches discussed in <ref> [9] </ref>. In this paper, we discuss the design decisions and tradeoffs made in the development of the system, and we present a series of benchmarks to evaluate its performance.
Reference: [10] <author> James Gosling. </author> <title> Java intermediate bytecodes. Proceedings of Workshop on Intermediate Representations, </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 30(3), </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: A Vcode program manipulates a stack of strongly typed vectors. The Vcode language provides a large number of stack-based vector operations, as well as instructions for stack manipulation, program control, memory management, and input/output. Note that Vcode shares several properties with Java bytecode <ref> [10] </ref>, including portability, strong typing, a stack-based execution model, and a design that allows easy interpretation.
Reference: [11] <author> Jonathan C. Hardwick. </author> <title> Java optimization. </title> <address> http://www.cs.cmu.edu/~jch/java/optimization.html. </address>
Reference-contexts: Currently, these optimizations must be performed by the programmer. Additionally, there is a lack of performance data available for use in making informed design and optimization decisions. We performed a series of micro-benchmarks to establish the comparative cost of various Java operations <ref> [11] </ref>. These benchmarks show that creating objects is a dominant cost for small problem sizes and becomes relatively more expensive in just-in-time compilers.
Reference: [12] <author> Jonathan C. Hardwick. </author> <title> Porting a vector library: a comparison of MPI, Paris, CMMD and PVM. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 68-77, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It uses a nested data-parallel algorithm. We give the source code and test data for the benchmarks in Appendix A. All three benchmarks have asymptotic running times that are linear in the size of the problem. Timings for the benchmarks have previously been reported in <ref> [7, 12] </ref>. 5.1 Methodology To minimize performance effects due to machine architecture, we used two different machines for benchmark-ing: a Sun SPARCstation 5/85 running Solaris 2.5, and a DX4-120 PC running Windows 95. Compilation was done using Nesl 3.1, gcc v2.7.0, and JDK 1.1.1, with full optimization.
Reference: [13] <author> Jonathan C. Hardwick and Jay Sipelstein. </author> <title> Java as an intermediate language. </title> <type> Technical Report CMU-CS-96-161, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: We use three standard Nesl benchmarks: * Least-squares line-fitting: Finds the best fit to a sequence of points using the least-squares method. It is simple straight-line code with no conditionals or loops, and hence can be used to measure interpretive overhead <ref> [13] </ref>. * Selection (generalized median-finding): Finds the element in a vector that would be at a specified position if the vector were sorted. <p> For the system described in this paper, the expressiveness and advantages of Vcode remain the same, while native libraries can be supplied either dynamically (compiled from VcodeEmulation by a JIT compiler) or statically (by linkage to native code <ref> [13] </ref>).
Reference: [14] <author> G. Muller, B. Moura, F. Bellard, and C. Consel. Harissa: </author> <title> a flexible and efficient Java environment mixing bytecode and compiled code. </title> <booktitle> In Proceedings of the 3rd Usenix Conference on Object-Oriented Technologies and Systems, </booktitle> <month> June </month> <year> 1997. </year> <note> See also http://www.irisa.fr/compose/harissa/harissa.html. </note>
Reference-contexts: However, by compiling into Java source code instead of bytecode, we can take advantage of source-level tools such as the JAVAR restructuring compiler for automatic parallelization [2], and optimizing Java-to-C compilers for generating stand-alone executables <ref> [14, 17] </ref>. A recent study of interpreters (including one for Java) concluded that performance is related to the expressiveness of the language, the use of native libraries, and the way memory is accessed [18].
Reference: [15] <author> Girija Narlikar. </author> <title> Scientific simulations and algorithm visualizations using Nesl and Java. </title> <address> http://www.cs.cmu.edu/~scandal/applets/. </address>
Reference-contexts: Figure 6 shows screen shots of the applets running in a web browser; other applications that we have converted include a geometric graph separator and a two-dimensional convex hull <ref> [15] </ref>. 1. Airflow simulation. This is a simulation of airflow over an airfoil using a parallel implementation of the finite volume method. <p> This system has enabled us to convert existing simulations and algorithm animations written in Nesl into Java applets <ref> [15] </ref>. By combining the portability of Java with the power of a high-level programming language we can quickly make available demonstrations of new parallel algorithms developed by our project. Java was easy to use, and has enough functionality to allow a clean implementation of our system.
Reference: [16] <author> Martin Odersky and Philip Wadler. </author> <title> Pizza into Java: Translating theory into practice. </title> <booktitle> In Proceedings of the 24th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1997. </year> <note> See also http://www.dcs.gla.ac.uk/~wadler/topics/pizza.html. </note>
Reference-contexts: We used the m4 preprocessor to generate multiple specialized methods from a single macro body. An alternative would be to add parametric polymorphism to Java using a system such as Pizza <ref> [16] </ref>. Second, Java compilers and virtual machines are still in their infancy. The Sun JDK compiler does not perform standard optimizations such as common subexpression elimination or loop-invariant code motion (for example, the reference to a.length in the loop in Figure 3 will be evaluated on each loop iteration).
Reference: [17] <author> Todd A. Proebsting, Gregg Townsend, Patrick Bridges, John H. Hartman, Tim Newsham, and Scott A. Watterson. Toba: </author> <title> Java for applications. A way ahead of time (WAT) compiler. </title> <type> Technical Report TR97-01, </type> <institution> Department of Computer Science, University of Arizona, </institution> <year> 1997. </year> <note> See also http://www.cs.arizona.edu/sumatra/toba/. </note>
Reference-contexts: However, by compiling into Java source code instead of bytecode, we can take advantage of source-level tools such as the JAVAR restructuring compiler for automatic parallelization [2], and optimizing Java-to-C compilers for generating stand-alone executables <ref> [14, 17] </ref>. A recent study of interpreters (including one for Java) concluded that performance is related to the expressiveness of the language, the use of native libraries, and the way memory is accessed [18].
Reference: [18] <author> Theodore H. Romer, Dennis Lee, Geoffrey M. Voelker, Alec Wolman, Wayne A. Wong, Jean-Loup Baer, Brian N. Bershad, and Henry M. Levy. </author> <title> The structure and performance of interpreters. </title> <booktitle> In Proceedings of the Seventh ACM Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year> <note> See also http://cag-www.lcs.mit.edu/asplos7/program/. </note>
Reference-contexts: A recent study of interpreters (including one for Java) concluded that performance is related to the expressiveness of the language, the use of native libraries, and the way memory is accessed <ref> [18] </ref>.
Reference: [19] <author> Robert Tolksdorf. </author> <title> Programming languages for the Java virtual machine. </title> <note> http://grunge.cs.tu-berlin.de/~tolk/vmlanguages.html. All CMU papers can also be found at http://www.cs.cmu.edu/~scandal/papers.html. </note>
Reference-contexts: We were therefore careful in our design to avoid creating unnecessary objects (for example, we never create objects in an inner loop). 7 Related Work There are currently more than 25 language implementations that use Java in some way <ref> [19] </ref>. They include both academic and commercial projects, and range from visual programming systems, through interpreters for Prolog and Basic, to compilers for Ada, Scheme, Lisp, and Rexx. The interpreters are designed mainly for demonstration purposes rather than computationally-intensive simulations, because the extra level of interpretation significantly reduces their performance.
References-found: 19

