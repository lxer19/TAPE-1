URL: http://www.cs.wisc.edu/~thain/projects/file-paper.ps
Refering-URL: http://www.cs.wisc.edu/~thain/
Root-URL: 
Title: File Access Patterns and Login Preloading  
Author: Douglas Thain 
Date: 5 May 1998  
Affiliation: University of Wisconsin  
Abstract: I demonstrate a user-level file system tracing technique for Solaris and AFS. The results generally confirm previous studies. A notable exception is that long-lived files are more common than previously measured. I also show how to generate a list of files necessary at login time, and use it to improve workstation login speed by 24 percent.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Ousterhout, et al., </author> <title> "A Trace-Driven Analysis of the UNIX 4.2 BSD File System", </title> <booktitle> Proceedings of the 10th ACM Symposium on Operating System Principles, ACM, </booktitle> <month> December </month> <year> 1995, </year> <pages> pp 15-24. </pages>
Reference-contexts: The trace is performed on individual workstations without disrupting the entire community or requiring special privilege. This trace can be used to verify the access patterns assumed by most file systems, often derived from an early analysis of a BSD UNIX file system. <ref> [1] </ref> Automatic measurements of a user's actions can be used to improve performance in repeatable situations. <p> The earliest is a study of access to files on several large 4.2 BSD systems in 1985 <ref> [1] </ref> . From these patterns, the authors proposed bandwidth requirements for diskless workstations and simulated the performance gains for large disk block caches. This work will be referred to as the BSD paper. <p> This is certainly sustainable by a 10 Mb network, however, the peak throughput at login time of 20 MB in a matter of seconds, is not. In this experiment, 3 MB of the initial burst was stored on a local filesystem. Open Time BSD <ref> [1] </ref> UW &lt; 1ms NA 40% &lt; 1000ms 80% 95% 2 It has been pointed out that eight hours corresponds to the end of one workday. I have verified that the subjects under test were, indeed, working for more than eight hours during this sample.
Reference: [2] <author> M. G. Baker, et al., </author> <title> "Measurements of a Distributed File System", </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principle, ACM, </booktitle> <month> October </month> <year> 1991, </year> <pages> pp 198-211. </pages>
Reference-contexts: From these patterns, the authors proposed bandwidth requirements for diskless workstations and simulated the performance gains for large disk block caches. This work will be referred to as the BSD paper. A follow up paper in 1991 compared a trace of the Sprite network file system <ref> [2] </ref> to the results in the BSD study. This paper concluded that the same general patterns were in effect, although file sizes had increased, and the bandwidth required for each client had increased by a factor of twenty. This paper will be referred to as the Sprite paper.
Reference: [3] <author> J. J. Kistlen and M. Satyanrayanan, </author> <title> "Disconnected Operation in the Coda File System", </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating System Principles Operating Systems Review 23, </booktitle> <month> December </month> <year> 1989, </year> <pages> pp 213-225. </pages>
Reference-contexts: The AFS filesystem [5] is in use at the University of Wisconsin, and has a substantially different caching scheme than Sprite. In general, a complete file is copied from a server to a workstation at first reference. Files larger than 64k are copied incrementally. The Coda filesystem <ref> [3] </ref> is an extension of AFS, and allows a workstation to continue to service users, even in the face of disconnection. Coda requests that a user specify which files are critical for disconnected operation. <p> I have no hypothesis to explain this measurement, however, I believe a more careful examination of the applications in use would be of value. The primary work of the users in this study was compiling and editing class and research projects. Coda <ref> [3] </ref> UW Time to Plateau 8 hours 8-15 hours Storage Needed 30 MB 40-100 MB workstation experiences bursts of use. After a certain amount of operation, a user's storage needs begin to plateau. These results are qualitatively similar to those presented int [3]. <p> Coda <ref> [3] </ref> UW Time to Plateau 8 hours 8-15 hours Storage Needed 30 MB 40-100 MB workstation experiences bursts of use. After a certain amount of operation, a user's storage needs begin to plateau. These results are qualitatively similar to those presented int [3]. A workstation initially requires 10-20 MB of storage immediately at login time to begin operation, and occasional jumps occur as the user moves to a new application or project.
Reference: [4] <author> M. K. McKusick, et. al., </author> <title> "A Fast File System for UNIX", </title> <publisher> ACM, </publisher> <month> August </month> <year> 1984, </year> <pages> pp 181-197. </pages>
Reference: [5] <author> J. H. Howard, et. al., </author> <title> "Scale and Performance in a Distributed File System", </title> <publisher> ACM, </publisher> <month> February </month> <year> 1998, </year> <pages> pp 51-81. </pages>
Reference-contexts: This paper concluded that the same general patterns were in effect, although file sizes had increased, and the bandwidth required for each client had increased by a factor of twenty. This paper will be referred to as the Sprite paper. The AFS filesystem <ref> [5] </ref> is in use at the University of Wisconsin, and has a substantially different caching scheme than Sprite. In general, a complete file is copied from a server to a workstation at first reference. Files larger than 64k are copied incrementally.
References-found: 5

