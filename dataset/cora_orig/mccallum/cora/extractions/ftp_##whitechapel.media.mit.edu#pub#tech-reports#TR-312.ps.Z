URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-312.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: trevor,sandy@media.mit.edu  
Title: Attention-driven Expression and Gesture Analysis in an Interactive Environment  
Author: Trevor Darrell and Alex P. Pentland 
Date: July 1995)  
Note: (Appeared, Intl. Workshop on Automatic Face and Gesture Recognition, Zurich,  
Abstract: M.I.T. Media Laboratory Vision and Modeling Group Technical Report No. 312 Abstract To provide natural user interfaces to interactive environments, accurate and fast recognition of gestures and expressions is needed. We adopt a view-based gesture recognition strategy that runs in an unconstrained interactive environment, which uses active vision methods to determine context cuess for the view-based method. Using vision routines already implemented for an interactive environment, we determine the spatial location of salient body parts and guide an active camera to obtain foveated images of gestures or expressions. Face recognition routines used to obtain an estimate of the identity of the user, and provide an index into the best set of view templates to use. The resulting system combines low-resolution, user-independent processing with high-resolution, user-specific models, all of which are computed in real time as part of an interactive environment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Darrell and A. P. Pentland, </author> <title> Space-Time Gestures. </title> <booktitle> Proc. IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 335-340, </pages> <publisher> IEEE Comp. Soc. Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993 </year>
Reference-contexts: 1 Introduction Gesture and expression are important interface modalities for interactive environments. Previously, we developed a method for view-based recognition of spatio-temporal hand gestures <ref> [1] </ref> and a similar mechanism for the analysis/real-time tracking of facial expressions [3]. These methods offered real-time performance and a relatively high level of accuracy, but required user-dependent training of model views, and a high-resolution image of the hand or face. <p> Full details of the clustering procedure are in <ref> [1] </ref>; in essence we use a "leader" algorithm, in which the first image is used to create a view model, and subsequent images also used to create new view models when the largest correlation score in the current set of view models drops below some threshold. <p> Adapted from <ref> [1] </ref>. input, four view models were found to be sufficient to track the hand through the two waving gestures. <p> Figure 1 (c) shows the scores for the two waving sequences, displayed as a surface plot. 1 sequence which contained three instances of each gesture. One can see that in this representation, recognition is straightforward. In <ref> [1] </ref> we reported recognition accuracy of over 90% in tests conducted on multiple users, given assumed conditions of high-resolution (foveated) hand images, little global rotation, and user-dependent templates. We have also applied the view based approach to the task of tracking facial expressions [3].
Reference: [2] <author> T. Darrell, P. Maes, B. Blumberg, A. P. Pent-land, </author> <title> A Novel Environment for Situated Vision and Behavior, </title> <booktitle> Proc. IEEE Workshop for Visual Behaviors, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <address> Los Alami-tos, CA, </address> <year> 1994 </year>
Reference-contexts: Our ALIVE system, which provides a video-based environment for interacting with artificial life creatures, implemented vision routines to find a user in a room and locate certain salient body parts <ref> [2] </ref>. The ALIVE system assumed little prior knowledge about the user, and operated on coarse-scale images of the user. <p> The resolution of hand images from the single camera in previous versions of ALIVE was insufficient to apply the view-based recognition techniques; hand gestures were modeled only using the motion of the hand location. Adapted from <ref> [2] </ref>. in unrestricted interactive domains, we need to find mechanisms which can both find the face and provide cues for detecting who the user is. <p> This allows a completely non-invasive (no wires, gloves, goggles) mode of virtual interaction. A wide field-of-view video camera acquires an image of the user which is combined with computer graphics images and projected on a large screen in front of the user (Figure 3 (a,b)). As described in <ref> [2] </ref>, computer vision routines in ALIVE analyze the image of the user, compute figure/ground segmentation, and analyze the contour/silhouette of the user to determine the location of head, hands, and other salient body features.
Reference: [3] <author> T. Darrell, I. Essa, and A. P. Pentland, </author> <title> Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis, </title> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> Morgan Kaufman 1995 </publisher>
Reference-contexts: 1 Introduction Gesture and expression are important interface modalities for interactive environments. Previously, we developed a method for view-based recognition of spatio-temporal hand gestures [1] and a similar mechanism for the analysis/real-time tracking of facial expressions <ref> [3] </ref>. These methods offered real-time performance and a relatively high level of accuracy, but required user-dependent training of model views, and a high-resolution image of the hand or face. <p> In [1] we reported recognition accuracy of over 90% in tests conducted on multiple users, given assumed conditions of high-resolution (foveated) hand images, little global rotation, and user-dependent templates. We have also applied the view based approach to the task of tracking facial expressions <ref> [3] </ref>. Figure 2 (a) shows a sequence of faces performing a surprise expression. Figure 2 (b) shows the view models learned for this sequence, and Figure 2 (c) shows the normalized correlation score for these models. <p> Adapted from <ref> [3] </ref>. paradigm to map vision input to motor control output dimensions, using the facial model presented in [4]. Interpolation requires a set of control points or exemplars from which to derive the desired mapping. <p> To produce the interpolated surprise measures shown in these figures, we mapped the vision scores to a one-dimensional motor value, labeled "surprise". Equivalently we could have mapped the vision scores directly to the motor controls corresponding to a "surprise" face, as was done in <ref> [3] </ref>. With this formulation, the distinction between expression recognition and expression tracking becomes blurred; the surprise measure can be used directly for animation, or peaks can be found and used for recognition/detection.
Reference: [4] <author> I. A. Essa. </author> <title> Analysis, Interpretation, and Synthesis of Facial Expressions, </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, MIT Media Laboratory, </institution> <address> Cambridge, MA 02139, USA, </address> <year> 1994. </year>
Reference-contexts: Adapted from [3]. paradigm to map vision input to motor control output dimensions, using the facial model presented in <ref> [4] </ref>. Interpolation requires a set of control points or exemplars from which to derive the desired mapping.
Reference: [5] <author> Turk, M., and Pentland, A., </author> <title> Eigenfaces for Recognition, </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <year> 1991. </year>
Reference-contexts: The head location is translated into gaze angles for the active camera's motor system, and a foveated image of the body part is acquired. Face recognition routines <ref> [5, 6] </ref> are then run on the foveated image to estimate the identity of the user and select the appropriate set of view-based templates to use when multiple users are present. Figure 4 (a) shows the overall architecture of the active imaging system we have implemented in ALIVE. <p> Preliminary experiments have indicated that these cues can be used to separate between a population of known users, when the number of users is not too large. In addition, more sophisticated face recognition routines may be run on the foveated image <ref> [5] </ref>. Face recognition and biometric cues can then be combined into a single estimate of identity and used to perform model selection on the view models.
Reference: [6] <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Computer Vision and Pattern Recognition Conference, </booktitle> <pages> pages 84-91. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: The head location is translated into gaze angles for the active camera's motor system, and a foveated image of the body part is acquired. Face recognition routines <ref> [5, 6] </ref> are then run on the foveated image to estimate the identity of the user and select the appropriate set of view-based templates to use when multiple users are present. Figure 4 (a) shows the overall architecture of the active imaging system we have implemented in ALIVE.
Reference: [7] <author> T. Poggio and F. Girosi, </author> <title> A theory of networks for approximation and learning. </title> <institution> MIT AI Lab TR-1140, </institution> <year> 1989. </year>
Reference-contexts: In this example pairs of real faces and model faces for different expressions are presented to the interpolation method during a training phase, by generating a 3-D model face and asking the user to match it. We use the Radial Basis Function (RBF) method presented in <ref> [7] </ref>, and define the interpolated motor controls to be a weighted sum of radial functions centered at each example: Y = i=1 where Y are the model face parameters, X are the observed view-model scores, X i are the example scores, G is an RBF (and in our case was simply
References-found: 7

