URL: http://www.cs.iastate.edu/~rlutz/publications/isre93.ps
Refering-URL: http://www.cs.iastate.edu/~rlutz/homepage.html
Root-URL: http://www.cs.iastate.edu
Title: Analyzing Software Requirements Errors in Safety-Critical, Embedded Systems  
Author: Robyn R. Lutz 
Address: Pasadena, CA 91109  
Affiliation: Jet Propulsion Laboratory California Institute of Technology  
Abstract: This paper analyzes the root causes of safety-related software errors in safety-critical, embedded systems. The results show that software errors identified as potentially hazardous to the system tend to be produced by different error mechanisms than non-safety-related software errors. Safety-related software errors are shown to arise most commonly from (1) discrepancies between the documented requirements specifications and the requirements needed for correct functioning of the system and (2) misunderstandings of the software's interface with the rest of the system. The paper uses these results to identify methods by which requirements errors can be prevented. The goal is to reduce safety-related software errors and to enhance the safety of complex, embedded systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. A. Addy, </author> <title> "A Case Study on Isolation of Safety-Critical Software," </title> <booktitle> in Proc 6th Annual Conf on Computer Assurance. </booktitle> <address> NIST/IEEE, </address> <year> 1991, </year> <pages> pp. 75-83. </pages>
Reference-contexts: Addy, looking at the types of errors that caused safety problems in a large, real-time control system, concluded that the design complexity inherent in such a system requires hidden interfaces which allow errors in non-critical software to affect safety-critical software <ref> [1] </ref>. This is consistent with Selby and Basili's results when they analyzed 770 software errors during the updating of a library tool [22].
Reference: [2] <author> A. M. Davis, </author> <title> Software Requirements, Analysis and Specification. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be considered in chooosing a formal method <ref> [2, 4, 5, 15, 20, 23] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [3] <author> D. E. Eckhardt, et al., </author> <title> "An Experimental Evaluation of Software Redundancy as a Strategy for Improving Reliability," </title> <journal> IEEE Trans Software Eng, </journal> <volume> 17, 7, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 692-702. </pages>
Reference-contexts: Eckhardt et al., in a study of software redundancy, analyzed the errors in twenty independent versions of a software component of an inertial navigation system <ref> [3] </ref>. He found that inadequate understanding of the specifications or the underlying coordinate system was a major contributor to the program faults causing coincident failures.
Reference: [4] <author> A. Endres, </author> <title> "An Analysis of Errors and Their Causes in Systems Programs," </title> <journal> IEEE Trans Software Eng, SE-1, </journal> <volume> 2, </volume> <month> June </month> <year> 1975, </year> <pages> pp. 140-149. </pages>
Reference-contexts: The work done by Endres is a direct forerunner of Nakajo and Kume's in that Endres backtracked from the error type to the technical and organizational causes which led to each type of error <ref> [4] </ref>. Moreover, because he studied the system testing of an operating system, the software's interaction with the hardware was a source of concern. Endres noted the difficulty of precisely specifying functional demands on the systems before the programmer had seen their effect on the dynamic behavior of the system. <p> The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be considered in chooosing a formal method <ref> [2, 4, 5, 15, 20, 23] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [5] <author> E. M. Gray and R. H. Thayer, </author> <title> "Requirements," in Aerospace Software Engineering, A Collection of Concepts. </title> <editor> Ed. C. Anderson and M. Dorfman. Wash-ington: AIAA, </editor> <year> 1991, </year> <pages> pp. 89-121. </pages>
Reference-contexts: Similarly, specifying the interfaces-especially the timing and dependency relationships-between the software outputs (e.g., star identification) and system outputs (e.g., closing the shutter on the star scanner) is necessary. <ref> [5, 10] </ref> System-development issues such as timing (real-time activities, interrupt handling, frequency of sensor data), hardware capabilities and limitations (storage capacity, power transients, noise characteristics), communication links (buffer and interface formats), and the expected operating environment (temperature, pressure, radiation) need to be reflected in the software requirements specifications because they are <p> The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be considered in chooosing a formal method <ref> [2, 4, 5, 15, 20, 23] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [6] <institution> ANSI/IEEE Standard Glossary of Software Engineering Terminology. </institution> <address> New York: </address> <publisher> IEEE, </publisher> <year> 1983. </year>
Reference: [7] <author> M. S. Jaffe et al., </author> <title> "Software Requirements Analysis for Real-Time Process-Control Systems," </title> <journal> IEEE Trans Software Eng, </journal> <volume> 17, 3, </volume> <month> March </month> <year> 1991, </year> <pages> pp. 241-258. </pages>
Reference-contexts: Many of the safety-related software errors reported in Sect. III involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in <ref> [7, 11] </ref>. Early detection of these safety-critical objects and increased attention to software operations involving them might forestall safety-related software errors involving them. 3. Use formal specification techniques in addition to natural-language software requirements specifications.
Reference: [8] <author> P. Jalote, </author> <title> An Integrated Approach to Software Engineering. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Similarly, standard measures of the internal complexity of modules have limited usefulness in anticipating software errors during system testing It is not the internal complexity of a module but the complexity of the module's connection to its environment that yields the persistent, safety-related errors seen in the embedded systems here <ref> [8] </ref>. V. Conclusion A. Recommendations The results in Sect. III indicate that safety-related software errors tend to be produced by different error mechanisms than non-safety-related software errors. This means that system safety can be directly enhanced by targeting the causes of safety-related errors.
Reference: [9] <author> J. C. Knight, </author> <title> "Testing," in Aerospace Software Engineering, A Collection of Concepts. </title> <editor> Ed. C. Ander-son and M. Dorfman. </editor> <address> Washington: AIAA, </address> <year> 1991, </year> <pages> pp. 135-159. </pages>
Reference-contexts: Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative validation and testing methods such as those described in <ref> [9, 11] </ref> offer greater coverage. 6. Include requirements for "defensive design" [17]. Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft.
Reference: [10] <author> N. G. Leveson, </author> <title> "Safety," in Aerospace Software Engineering, A Collection of Concepts. </title> <editor> Ed. C. Ander-son and M. Dorfman. </editor> <address> Washington: AIAA, </address> <year> 1991, </year> <pages> pp. 319-336. </pages>
Reference-contexts: Similarly, specifying the interfaces-especially the timing and dependency relationships-between the software outputs (e.g., star identification) and system outputs (e.g., closing the shutter on the star scanner) is necessary. <ref> [5, 10] </ref> System-development issues such as timing (real-time activities, interrupt handling, frequency of sensor data), hardware capabilities and limitations (storage capacity, power transients, noise characteristics), communication links (buffer and interface formats), and the expected operating environment (temperature, pressure, radiation) need to be reflected in the software requirements specifications because they are <p> Identify safety-critical hazards early in the requirements analysis. These hazards are constraints on the possible designs and factors in any contemplated tradeoffs between safety (which tends to encourage software simplicity) and increased functionality (which tends to encourage software complexity) <ref> [10, 22] </ref>. Many of the safety-related software errors reported in Sect. III involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in [7, 11]. <p> CASE tools offer a possible solution to the difficulty of promulgating change without increasing paperwork. The prevalence of safety-related software errors involving misunderstood or missing requirements points up the inadequacy of consistency checks of requirements and code as a means of demonstrating system correctness <ref> [10] </ref>. Code that implements incorrect requirements is incorrect if it fails to provide needed system behavior. Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems.
Reference: [11] <author> N. G. Leveson, </author> <title> "Software Safety in Embedded Computer Systems," </title> <journal> Commun ACM , Vol. </journal> <volume> 34, No. 2, </volume> <month> Feb </month> <year> 1991, </year> <pages> pp. 35-46. </pages>
Reference-contexts: Leveson listed a set of common assumptions that are often false for control systems, resulting in software errors <ref> [11] </ref>. Among these assumptions are that the software specification is correct, that it is possible to predict realistically the software's execution environment (e.g., the existence of transients), and that it is possible to anticipate and specify correctly the software's behavior under all possible circumstances. <p> Many of the safety-related software errors reported in Sect. III involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in <ref> [7, 11] </ref>. Early detection of these safety-critical objects and increased attention to software operations involving them might forestall safety-related software errors involving them. 3. Use formal specification techniques in addition to natural-language software requirements specifications. <p> Similarly, generating test cases from misunderstood or missing requirements will not test system correctness. Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative validation and testing methods such as those described in <ref> [9, 11] </ref> offer greater coverage. 6. Include requirements for "defensive design" [17]. Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft. <p> Run-time safety checks on the validity of input data, watchdog timers, delay timers, software filters, software-imposed initialization conditions, additional exception handling, and assertion checking can be used to combat the many safety-critical software errors involving conditional and omission faults <ref> [11] </ref>. Requirements for error-handling, overflow protection, signal saturation limits, heartbeat and pulse frequency, maximum event duration, and system behavior under unexpected conditions can be added and traced into the design. Many safety-related functional faults involve error-recovery routines being invoked inappropriately because of erroneous limit values or bad data.
Reference: [12] <author> N. G. Leveson and P. R. Harvey, </author> <title> "Analyzing Software Safety," </title> <journal> IEEE Transactions on Software Engineering, SE-9, </journal> <volume> 5, </volume> <month> Sept </month> <year> 1983, </year> <pages> pp. 569-579. </pages>
Reference-contexts: Many safety-related functional faults involve error-recovery routines being invoked inappropriately because of erroneous limit values or bad data. Backward analysis from critical failures to possible causes offers one check of how defensive the requirements and design are <ref> [12] </ref>. Requirements specifications that account for worst-case scenarios, models that can predict the range of possible (rather than allowable) values, and simulations that can discover unexpected interactions before system testing contribute to the system's defense against hazards. B.
Reference: [13] <author> Karan L'Heureux, </author> <title> "Software Systems Safety Program RTOP, Phase A Report," Internal Document, </title> <institution> Jet Propulsion Laboratory, </institution> <month> April 19, </month> <year> 1991. </year>
Reference: [14] <author> R. Lutz, </author> <title> "Analyzing Software Requirements Errors in Safety-Critical, Embedded Systems," </title> <institution> TR92-27, Dept. of Comp. Sci, Iowa State University. </institution>
Reference: [15] <author> R. Lutz and J. S. K. Wong, </author> <title> "Detecting Unsafe Error Recovery Schedules," </title> <journal> IEEE Trans Software Eng, </journal> <volume> 18, 8, </volume> <month> Aug, </month> <year> 1992, </year> <pages> pp. 749-760. </pages>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be considered in chooosing a formal method <ref> [2, 4, 5, 15, 20, 23] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [16] <author> T. Nakajo and H. Kume, </author> <title> "A Case History Analysis of Software Error Cause-Effect Relationships," </title> <journal> IEEE Trans Software Eng 17, </journal> <volume> 8, </volume> <month> Aug </month> <year> 1991, </year> <pages> pp. 830-838. </pages>
Reference-contexts: Nakajo and Kume categorized 670 errors found during the software development of two firmware products for controlling measuring instruments and two software products for instrument measurement programs <ref> [16] </ref>. Over 90% of the errors were either interface or functional faults, similar to the results reported here. While the key human error on the spacecraft involved communication between teams, the key human error in their study involved communication within a development team.
Reference: [17] <author> P. G. Neumann, </author> <title> "The Computer-Related Risk of the Year: Weak Links and Correlated Events," </title> <booktitle> in Proc 6th Annual Conf on Computer Assurance. </booktitle> <address> NIST/IEEE, </address> <year> 1991, </year> <pages> pp. 5-8. </pages>
Reference-contexts: Traceability of requirements and automatic test generation from specifications offers only partial validation of complex, embedded systems. Alternative validation and testing methods such as those described in [9, 11] offer greater coverage. 6. Include requirements for "defensive design" <ref> [17] </ref>. Many of the safety-related software errors involve inadequate software responses to extreme conditions or extreme values. Anomalous hardware behavior, unanticipated states, events out of order, and obsolete data are all causes of safety-related software errors on the spacecraft.
Reference: [18] <author> A. P. Nikora, </author> <title> "Error Discovery Rate by Severity Category and Time to Repair Software Failures for Three JPL Flight Projects," Internal Document, </title> <institution> Jet Propulsion Laboratory, </institution> <year> 1991. </year>
Reference-contexts: Errors discovered in the testing phase take longer to correct (because they tend to be more complicated and difficult to isolate). This is consistent with the results in <ref> [18] </ref> indicating that more severe errors take longer to discover than less severe errors during system-level testing. Furthermore, this effect was found to be more pronounced in more complex (as measured by lines of code) software.
Reference: [19] <author> T. J. Ostrand and E. J. Weyuker, </author> <title> "Collecting and Categorizing Software Error Data in an Industrial Environment," </title> <journal> The Journal of Systems and Software, </journal> <volume> 4, </volume> <year> 1984, </year> <pages> pp. 289-300. </pages>
Reference-contexts: In the safety-critical, embedded software on the spacecraft, the flaw was more often a failure to identify or to understand the requirements. Ostrand and Weyuker categorized 173 errors found during the development and testing of an editor system <ref> [19] </ref>. Only 2% of the errors were found during system testing, reflecting the simplicity and stability of the interfaces and requirements. Most of the errors (61%) were found instead during function testing. <p> The most frequent class of errors, other than coding and clerical, was design errors. All three of the most common design errors-extreme conditions neglected, forgotten cases or steps, and loop control errors- are also common functional faults on the spacecraft. Both the findings presented in <ref> [19, 21] </ref> and in this paper confirm the common experience that early insertion and late discovery of software errors maximizes the time and effort that the correction takes.
Reference: [20] <editor> Proc Berkeley Workshop on Temporal and Real-Time Specification. Eds. P. B. Ladkin and F. H. Vogt. </editor> <address> Berkeley, CA: </address> <institution> International Computer Science Institute, </institution> <year> 1990, </year> <month> TR-90-060. </month>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be considered in chooosing a formal method <ref> [2, 4, 5, 15, 20, 23] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
Reference: [21] <author> N. F. Schneidewind and H.-M. Hoffmann, </author> <title> "An Experiment in Software Error Data Collection and Analysis," </title> <journal> IEEE Trans Software Eng, </journal> <volume> SE-5, 3, </volume> <month> May </month> <year> 1979, </year> <pages> pp. 276-286. </pages>
Reference-contexts: Most of the errors (61%) were found instead during function testing. Over half of these errors were caused by omissions, confirming the findings of the present study that omissions are a major cause of software errors. Schneidewind and Hoffmann <ref> [21] </ref> categorized 173 errors found during the development of four small programs by a single programmer. Again, there were no significant interfaces with hardware and little system testing. The most frequent class of errors, other than coding and clerical, was design errors. <p> The most frequent class of errors, other than coding and clerical, was design errors. All three of the most common design errors-extreme conditions neglected, forgotten cases or steps, and loop control errors- are also common functional faults on the spacecraft. Both the findings presented in <ref> [19, 21] </ref> and in this paper confirm the common experience that early insertion and late discovery of software errors maximizes the time and effort that the correction takes.
Reference: [22] <author> R. W. Selby and V. R. Basili, </author> <title> "Analyzing Error-Prone System Structure," </title> <journal> IEEE Trans Software Eng 17, </journal> <volume> 2, </volume> <month> Febr </month> <year> 1991, </year> <pages> pp. 141-152. </pages>
Reference-contexts: This is consistent with Selby and Basili's results when they analyzed 770 software errors during the updating of a library tool <ref> [22] </ref>. Of the 46 errors documented in trouble reports, 70% were categorized as "wrong" and 28% as "missing." They found that subsystems that were highly interactive with other subsystems had proportionately more errors than less interactive subsystems. <p> Identify safety-critical hazards early in the requirements analysis. These hazards are constraints on the possible designs and factors in any contemplated tradeoffs between safety (which tends to encourage software simplicity) and increased functionality (which tends to encourage software complexity) <ref> [10, 22] </ref>. Many of the safety-related software errors reported in Sect. III involve data objects or processes that would be targeted for special attention using hazard-detection techniques such as those described in [7, 11].
Reference: [23] <author> J. M. Wing, </author> <title> "A Specifier's Introduction to Formal Methods," </title> <journal> Computer , Vol. </journal> <volume> 23, </volume> <month> Sept </month> <year> 1990, </year> <pages> pp. 8-26. </pages>
Reference-contexts: The capability to describe dynamic events, the timing of process interactions in distinct computers, decentralized supervisory functions, etc., should be considered in chooosing a formal method <ref> [2, 4, 5, 15, 20, 23] </ref>. 4. Promote informal communication among teams. Many safety-related software errors resulted from one individual or team misunderstanding a requirement or not knowing a fact about the system that member (s) of another development team knew.
References-found: 23

