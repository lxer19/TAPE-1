URL: http://www.cs.utexas.edu/users/plaxton/ps/1992/ensl_lip_38.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Title: Sorting-Based Selection Algorithms for Hypercubic Networks  
Author: P. Berthome A. Ferreira ; B. M. Maggs S. Perennes C. G. Plaxton 
Keyword: Parallel algorithms, hypercube, selection. Resume Mots-cles Algorithmes paralleles, hypercube, selection.  
Abstract: This paper presents several deterministic algorithms for selecting the kth largest record from a set of n records on any n-node hypercubic network. All of the algorithms are based on the selection algorithm of Cole and Yap, as well as on various sorting algorithms for hypercubic networks. Our fastest algorithm runs in O(lg n lg fl n) time, very nearly matching the trivial (lg n) lower bound. Dans cet article, nous presentons plusieurs algorithmes deterministes permettant de trouver le k-ieme element dans un ensemble ayant n enregistrements sur un reseau hypercubique quelconque. Tous ces algorithmes sont bases sur l'algorithme de selection de Cole et Yap, ainsi que sur divers algorithmes de tri sur reseaux hypercubiques. Notre meilleur algorithme fonctionne en temps O(lg n lg fl n), ce qui nous rapproche de la borne inferieure immediate (lg n). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Komlos, W. L. Steiger, and E. Szemeredi. </author> <title> Deterministic selection in O(log log n) parallel time. </title> <booktitle> In Proceedings of the 18th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 188-195, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Cole and Yap [4] then described 1 an O ((lg lg n) 2 ) selection algorithm for this model. The running time was later improved to O (lg lg n) by Ajtai, Komlos, Steiger, and Szemeredi <ref> [1] </ref>. The comparisons performed by the latter algorithm are specified by an expander graph, however, making it unlikely that this algorithm can be efficiently implemented on a hypercubic network. A different set of upper and lower bounds hold in the PRAM models.
Reference: [2] <author> P. Beame and J. H-astad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 83-93, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The comparisons performed by the latter algorithm are specified by an expander graph, however, making it unlikely that this algorithm can be efficiently implemented on a hypercubic network. A different set of upper and lower bounds hold in the PRAM models. Beame and Hastad <ref> [2] </ref> proved an (lg n= lg lg n) lower bound on the time for selection in the CRCW comparison PRAM using a polynomial number of processors. Vishkin [10] discovered an O (lg n lg lg n) time PRAM algorithm that uses O (n= lg n lg lg n) processors.
Reference: [3] <author> R. Cole. </author> <title> An optimally efficient parallel selection algorithm. </title> <journal> IPL, </journal> <volume> 26 </volume> <pages> 295-299, </pages> <year> 1988. </year> <month> 12 </month>
Reference-contexts: The algorithm is work-efficient (i.e., exhibits optimal speedup) because the processor time product is equal to the time, O (n), of the fastest sequential algorithm for this problem. Cole <ref> [3] </ref> later found an O (lg n lg fl n) time work-efficient PRAM algorithm. For any p-processor hypercubic network, Plaxton [8] showed that selection from a set of n records requires ((n=p) lg lg p + lg p) time in the worst case.
Reference: [4] <author> R. Cole and C. K. Yap. </author> <title> A parallel median algorithm. </title> <journal> IPL, </journal> <volume> 20 </volume> <pages> 137-139, </pages> <year> 1985. </year>
Reference-contexts: The lower bound implies a lower bound on the time to select the kth largest record as well. Valiant also showed how to find the largest record in O (lg lg n) time. Cole and Yap <ref> [4] </ref> then described 1 an O ((lg lg n) 2 ) selection algorithm for this model. The running time was later improved to O (lg lg n) by Ajtai, Komlos, Steiger, and Szemeredi [1]. <p> our best asymptotic result is a uniform selection algorithm with a running time of O (lg n lg fl n). 2 Selection by successive approximation 2.1 Approximate selection In this section, we develop an efficient subroutine for approximate selection based on the parallel comparison model algorithm of Cole and Yap <ref> [4] </ref>. There are two major differences.
Reference: [5] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 501-548, </pages> <year> 1993. </year>
Reference-contexts: An O (lg n lg fl n) algorithm is presented is Section 2.5. This improvement in the running time is obtained at the expense of using a non-uniform variant of the Sharesort algorithm <ref> [5] </ref> that requires a certain amount of preprocessing. Finally, in Section 2.6 we show how to avoid the non-uniformity introduced in Section 2.5. <p> O (ffi) and hence T (d; 0) = O (d lg lg d) = O (lg n lg (3) n). 2.4 An O (lg n lg (4) n) algorithm We can improve the time bound achieved in Section 2.3 by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [5] </ref>. Several variants of that algorithm exist; in particular, detailed descriptions of two versions of Sharesort may be found in [5]. Both of these variants are designed to sort n records on an n-processor hypercubic network. <p> 2.4 An O (lg n lg (4) n) algorithm We can improve the time bound achieved in Section 2.3 by making use of the Sharesort algorithm of Cypher and Plaxton <ref> [5] </ref>. Several variants of that algorithm exist; in particular, detailed descriptions of two versions of Sharesort may be found in [5]. Both of these variants are designed to sort n records on an n-processor hypercubic network. The first algorithm runs in O (lg n (lg lg n) 3 ) time and the second algorithm, which is somewhat more complicated, runs in O (lg n (lg lg n) 2 ) time. <p> There exists a non-uniform deterministic algorithm for sorting these records in time O (lg n (lg lg n lg lg p Proof: As indicated in Section 2.4, there are a number of variants of the Sharesort algorithm of Cypher and Plaxton <ref> [5] </ref>. These algorithms differ solely in the way that the so-called shared key sorting subroutine is implemented. <p> Perhaps the simplest variant of Sharesort runs in O (lg n lg lg n) time and relies upon an optimal logarithmic time shared key sorting subroutine. This particular result is mentioned in the original Sharesort paper <ref> [5] </ref> and more fully described by Leighton [6, Section 3.5.3]. Although it is the fastest of the Sharesort variants, this sorting algorithm suffers from the disadvantage that it is non-uniform. <p> Let M (x; y) denote the task of merging x sorted lists of length y. One possible recurrence for performing the merge is (minor technical details related to integrality constraints are dealt with in <ref> [5] </ref> and will not be addressed here) M (n 1=5 ; n 4=5 ) M (n 4=45 ; n 16=45 ) + M (n 1=9 ; n 4=9 ) + O (lg n) + SKS (n); (3) where SKS (n) denotes the time required to solve the shared key sorting problem. <p> The fastest known uniform version of the shared key sorting subroutine takes O (lg n lg lg n) time, which leads to an O (lg n (lg lg n) 2 ) running time for the corresponding uniform variant of Sharesort <ref> [5] </ref>. In the following, we will show how to adapt this uniform version of Sharesort to obtain a uniform version of a "sparse" Sharesort, that is, an algorithm for sorting n records on p n processors. <p> By a similar analysis as that provided in Section 2.5, the theorem will follow if we can prove that SSKS (n; p) = lg n (lg lg n lg lg p n ): (5) We now sketch certain minor modifications to the SharedKeySort () algorithm of <ref> [5] </ref> that will yield a "SparseSharedKeySort ()" routine with the desired properties. The SharedKeySort () procedure consists of essentially two subroutine calls; these calls are made to the subroutines PlanRoute () and DoRoute (), respectively.
Reference: [6] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, and Hypercubes. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: All of the algorithms described in this paper use the edges of the hypercube in a very restricted way. At each time step, only the edges associated with a single dimension are used, and consecutive dimensions are used on consecutive steps. Such algorithms are called normal <ref> [6, Section 3.1.4] </ref>. The bounded-degree variants of the hypercube, including the butterfly, cube-connected cycles, and shu*e-exchange graph, can all simulate any normal hypercube algorithm with constant slowdown. <p> Perhaps the simplest variant of Sharesort runs in O (lg n lg lg n) time and relies upon an optimal logarithmic time shared key sorting subroutine. This particular result is mentioned in the original Sharesort paper [5] and more fully described by Leighton <ref> [6, Section 3.5.3] </ref>. Although it is the fastest of the Sharesort variants, this sorting algorithm suffers from the disadvantage that it is non-uniform.
Reference: [7] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new gen eralized connection network. </title> <journal> JACM, </journal> <volume> 29 </volume> <pages> 642-667, </pages> <year> 1982. </year>
Reference-contexts: There are two major differences. First, we use Nassimi and Sahni's sparse enumeration sort <ref> [7] </ref> instead of a constant time sort (as is possible in the parallel comparison model), and second we obtain a total running time that is proportional to the running time of the largest call to sparse enumeration sort, whereas in the Cole and Yap algorithm, the running time is proportional to
Reference: [8] <author> C. G. Plaxton. </author> <title> Efficient Computation on Sparse Interconnection Networks. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: The algorithm is work-efficient (i.e., exhibits optimal speedup) because the processor time product is equal to the time, O (n), of the fastest sequential algorithm for this problem. Cole [3] later found an O (lg n lg fl n) time work-efficient PRAM algorithm. For any p-processor hypercubic network, Plaxton <ref> [8] </ref> showed that selection from a set of n records requires ((n=p) lg lg p + lg p) time in the worst case. The bound implies that a work-efficient algorithm is not possible. <p> Hence T (d; 0) = O (d lg d) = O (lg n lg lg n). This algorithm is essentially equivalent to that described by Plaxton in <ref> [8] </ref>.
Reference: [9] <author> L. G. Valiant. </author> <title> Parallelism in comparison problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 4 </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: The bounded-degree variants of the hypercube, including the butterfly, cube-connected cycles, and shu*e-exchange graph, can all simulate any normal hypercube algorithm with constant slowdown. For simplicity, we will describe all of the algorithms in terms of the hypercube. 1.2 Previous work In <ref> [9] </ref>, Valiant proved an (lg lg n) lower bound on the time to find the largest record in a set of n records using n processors in the parallel comparison model. The lower bound implies a lower bound on the time to select the kth largest record as well.

References-found: 9

