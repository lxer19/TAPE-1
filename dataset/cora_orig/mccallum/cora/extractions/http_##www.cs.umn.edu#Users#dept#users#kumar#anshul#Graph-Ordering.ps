URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/Graph-Ordering.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/anshul/
Root-URL: http://www.cs.umn.edu
Title: Fast and Effective Algorithms for Graph Partitioning and Sparse Matrix Ordering  
Author: Anshul Gupta P. O. Box 
Keyword: LIMITED DISTRIBUTION NOTICE  
Affiliation: Computer Science/Mathematics IBM Research  IBM Research Division T.J. Watson Research Center  Yorktown  IBM Research Division Almaden Austin China Haifa Tokyo Watson Zurich  
Address: 20496 (90799)  Heights, New York 10598  
Note: RC  
Pubnum: Report  
Email: anshul@watson.ibm.com  
Date: July 10, 1996  
Abstract: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g., payment of royalties). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-complete. </title> <journal> SIAM Journal on Algebraic Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year>
Reference-contexts: An important application of graph-partitioning is in computing fill-reducing orderings of sparse matrices for solving large sparse systems of linear equations. Finding an optimal ordering is an NP-complete problem <ref> [1] </ref> and heuristics must be used to obtain an acceptable non-optimal solution. Improving the run time and quality of ordering heuristics has been a subject of research for almost three decades.
Reference: [2] <author> Alan George and Joseph W.-H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: GP-based heuristics regard the symmetric sparse matrix as the adjacency matrix of a graph and follow a divide-and-conquer strategy to label the nodes of the graph by partitioning it into smaller subgraphs. The initial success of MD-based heuristics prompted intense research <ref> [2] </ref> to improve their run time and quality, and they have been the methods of choice among practitioners. <p> The initial success of MD-based heuristics prompted intense research [2] to improve their run time and quality, and they have been the methods of choice among practitioners. The multiple minimum degree (MMD) algorithm by George and Liu <ref> [3, 2] </ref> and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff [4] represent the state of the art in MD-based heuristics.
Reference: [3] <author> Alan George and Joseph W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: The initial success of MD-based heuristics prompted intense research [2] to improve their run time and quality, and they have been the methods of choice among practitioners. The multiple minimum degree (MMD) algorithm by George and Liu <ref> [3, 2] </ref> and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff [4] represent the state of the art in MD-based heuristics. <p> Intuitively, smaller node-separators mean less fill and work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [25, 26, 24, 3] </ref>. Such bounds cannot be proved for arbitrary matrices without a well-defined structure, such as the sparse matrices arising in LP computations.
Reference: [4] <author> Timothy A. Davis, Patrick Amestoy, and Iain S. Duff. </author> <title> An approximate minimum degree ordering algorithm. </title> <type> Technical Report TR-94-039, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1994. </year>
Reference-contexts: The multiple minimum degree (MMD) algorithm by George and Liu [3, 2] and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff <ref> [4] </ref> represent the state of the art in MD-based heuristics. <p> In our implementation, the size of the terminal subgraphs ranges from a hundred nodes to a few hundred nodes depending on the size of the original graph. We use the AMD variation (due to Davis, Amestoy, and Duff <ref> [4] </ref>) of the minimum-degree heuristic for ordering these small subgraphs. After the nodes of the two subgraphs at any level of recursion have been labeled, the nodes of the separator at that level are labeled and the algorithm returns to the previous level, if any. <p> In Section 5.1, we compare WGPP with METIS. Currently the best minimum-degree-based code available for computing fill-reducing ordering for sparse matrices is that of approximate minimum degree (AMD) <ref> [4] </ref>. METIS is one of the well known graph-partitioning-based sparse-matrix ordering softwares. Recent work by Ashcraft and Liu [8] and by Hendrickson and Rothberg [7] reports graph-partitioning-based ordering heuristics that are better than METIS, but the corresponding software is not available for direct comparison with WGPP.
Reference: [5] <author> Anshul Gupta. WGPP: </author> <title> Watson graph partitioning (and sparse matrix ordering) package: Users manual. </title> <type> Technical Report RC 20453 (90427), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May 6, </month> <year> 1996. </year>
Reference-contexts: The multiple minimum degree (MMD) algorithm by George and Liu [3, 2] and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff [4] represent the state of the art in MD-based heuristics. Recent work by the author <ref> [5, 6] </ref>, Hendrickson and Rothberg [7], Ashcraft and Liu [8], and Karypis and Kumar [9, 10] suggests that GP-based heuristics are capable of producing better-quality orderings than MD-based heuristics for finite-element problems while staying within a small constant factor of the run time of MD-based heuristics. <p> Our ordering often turned out to be several hundred times faster than AMD and MMD on our set of LP test matrices, while producing better-quality orderings on average. We have developed a graph-partitioning and sparse-matrix-ordering package (WGPP) <ref> [5] </ref> 1 based 1 The package is available to users in the form of a linkable module. 2 on the heuristics described in this paper.
Reference: [6] <author> Anshul Gupta. </author> <title> Graph partitioning based sparse matrix ordering algorithms for finite-element and optimization problems. </title> <booktitle> In Proceedings of the Second SIAM Conference on Sparse Matrices, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The multiple minimum degree (MMD) algorithm by George and Liu [3, 2] and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff [4] represent the state of the art in MD-based heuristics. Recent work by the author <ref> [5, 6] </ref>, Hendrickson and Rothberg [7], Ashcraft and Liu [8], and Karypis and Kumar [9, 10] suggests that GP-based heuristics are capable of producing better-quality orderings than MD-based heuristics for finite-element problems while staying within a small constant factor of the run time of MD-based heuristics.
Reference: [7] <author> Bruce Hendrickson and Edward Rothberg. </author> <title> Improving the runtime and quality of nested dissection ordering. </title> <type> Technical Report SAND96-0868J, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <year> 1996. </year>
Reference-contexts: The multiple minimum degree (MMD) algorithm by George and Liu [3, 2] and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff [4] represent the state of the art in MD-based heuristics. Recent work by the author [5, 6], Hendrickson and Rothberg <ref> [7] </ref>, Ashcraft and Liu [8], and Karypis and Kumar [9, 10] suggests that GP-based heuristics are capable of producing better-quality orderings than MD-based heuristics for finite-element problems while staying within a small constant factor of the run time of MD-based heuristics. <p> However, for most finite-element and finite-difference matrices, it is possible to relabel these separator nodes to reduce the fill-in and the operation count. This technique, which has been used in <ref> [8, 7] </ref>, involves creating an elimination graph consisting of the separator nodes and using a minimum-degree heuristic to order this graph. <p> Currently the best minimum-degree-based code available for computing fill-reducing ordering for sparse matrices is that of approximate minimum degree (AMD) [4]. METIS is one of the well known graph-partitioning-based sparse-matrix ordering softwares. Recent work by Ashcraft and Liu [8] and by Hendrickson and Rothberg <ref> [7] </ref> reports graph-partitioning-based ordering heuristics that are better than METIS, but the corresponding software is not available for direct comparison with WGPP. Therefore, in Section 5.2, we compare WGPP's sparse-matrix orderings with those of AMD and METIS. Results for the traditional multiple minimum-degree (MMD) algorithm are also included for reference.
Reference: [8] <author> Cleve Ashcraft and Joseph W.-H. Liu. </author> <title> Robust ordering of sparse matrices using multisec-tion. </title> <type> Technical Report CS 96-01, </type> <institution> Department of Computer Science, York University, </institution> <address> Ontario, Canada, </address> <year> 1996. </year>
Reference-contexts: The multiple minimum degree (MMD) algorithm by George and Liu [3, 2] and the approximate minimum degree (AMD) algorithm by Davis, Amestoy, and Duff [4] represent the state of the art in MD-based heuristics. Recent work by the author [5, 6], Hendrickson and Rothberg [7], Ashcraft and Liu <ref> [8] </ref>, and Karypis and Kumar [9, 10] suggests that GP-based heuristics are capable of producing better-quality orderings than MD-based heuristics for finite-element problems while staying within a small constant factor of the run time of MD-based heuristics. An important area where sparse-matrix orderings are used is that of linear programming. <p> On the other hand, node-separators of size two (i.e., a; b or k; l) are available with almost the same degree of imbalance between the two partitions. Ashcraft and Liu <ref> [8] </ref> find a node-separator within the coarse graph and refine it into a node-separator of the original graph in order to overcome the drawback of the edge-separator approach. <p> However, for most finite-element and finite-difference matrices, it is possible to relabel these separator nodes to reduce the fill-in and the operation count. This technique, which has been used in <ref> [8, 7] </ref>, involves creating an elimination graph consisting of the separator nodes and using a minimum-degree heuristic to order this graph. <p> In Section 5.1, we compare WGPP with METIS. Currently the best minimum-degree-based code available for computing fill-reducing ordering for sparse matrices is that of approximate minimum degree (AMD) [4]. METIS is one of the well known graph-partitioning-based sparse-matrix ordering softwares. Recent work by Ashcraft and Liu <ref> [8] </ref> and by Hendrickson and Rothberg [7] reports graph-partitioning-based ordering heuristics that are better than METIS, but the corresponding software is not available for direct comparison with WGPP. Therefore, in Section 5.2, we compare WGPP's sparse-matrix orderings with those of AMD and METIS.
Reference: [9] <author> George Karypis and Vipin Kumar. METIS: </author> <title> Unstructured graph partitioning and sparse matrix ordering system. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: Recent work by the author [5, 6], Hendrickson and Rothberg [7], Ashcraft and Liu [8], and Karypis and Kumar <ref> [9, 10] </ref> suggests that GP-based heuristics are capable of producing better-quality orderings than MD-based heuristics for finite-element problems while staying within a small constant factor of the run time of MD-based heuristics. An important area where sparse-matrix orderings are used is that of linear programming. <p> However, while computing a fill-reducing ordering of a sparse matrix, it is the size of the node-separator that must be minimized. Current graph-partitioning-based ordering algorithms follow two different approaches to finding a small node-separator. Karypis and Kumar <ref> [9] </ref> refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then they use an algorithm for finding a minimum cover [29, 30] to compute a node-separator from the edge-separator. <p> To the best of our knowledge, the METIS <ref> [9] </ref> software package represents the state of the art in graph-partitioning|both in terms of partitioning time and quality. In Section 5.1, we compare WGPP with METIS. Currently the best minimum-degree-based code available for computing fill-reducing ordering for sparse matrices is that of approximate minimum degree (AMD) [4].
Reference: [10] <author> George Karypis and Vipin Kumar. </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs. </title> <type> Technical Report TR 95-035, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: Recent work by the author [5, 6], Hendrickson and Rothberg [7], Ashcraft and Liu [8], and Karypis and Kumar <ref> [9, 10] </ref> suggests that GP-based heuristics are capable of producing better-quality orderings than MD-based heuristics for finite-element problems while staying within a small constant factor of the run time of MD-based heuristics. An important area where sparse-matrix orderings are used is that of linear programming. <p> In both parallel factorization and triangular solutions, a part of the parallelism would be lost if an MD-based heuristic is used to preorder the sparse matrix. 3 Multilevel graph partitioning Recent research <ref> [18, 19, 10] </ref> has shown multilevel algorithms to be fast and effective in computing graph-partitions. A typical multilevel graph-partitioning algorithm has four components: coarsening, initial partitioning, uncoarsening, and refining. <p> Given a weighted graph after any stage of coarsening, there are several choices of matchings for the next coarsening step. A simple matching scheme [19] known as random matching (RM) randomly chooses pairs of connected unmatched nodes to include in the matching. In <ref> [10] </ref>, Karypis and Kumar describe a heuristic known as heavy-edge matching (HEM) to aid in the selection of a matching that not only reduces the run time of the refinement component of graph-partitioning, but also tends to generate partitions with small separators. <p> In WGPP, we have completely eliminated the initial partitioning phase, thereby simplifying and speeding up the overall partitioning process. One of the effective heuristics for initial partitioning is graph growing <ref> [10] </ref>. Graph growing computes an initial partition by recursively bisecting the graph into two subgraphs of appropriate weight. For example, a three-way initial partition is produced by bisecting the coarse graph into two parts with a weight ratio of 2:1; the larger subgraph is further bisected into two equal parts. <p> Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic [20, 21, 19, 22] or a greedy refinement scheme <ref> [10, 23] </ref> for refining the edge-separators. The linear time Fiduccia-Mattheyses variation [21] of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large.
Reference: [11] <author> Edward Rothberg and Bruce Hendrickson. </author> <title> Sparse matrix ordering methods for interior point linear programming. </title> <type> Technical Report SAND96-0475J, </type> <institution> Sandia National Laboratories, </institution> <address> Albu-querque, NM, </address> <year> 1996. </year>
Reference-contexts: An important area where sparse-matrix orderings are used is that of linear programming. Until now, with the exception of Rothberg and Hendrickson <ref> [11] </ref>, most researchers have focused on ordering sparse matrices arising in finite-element applications, and these applications have guided the development of the ordering heuristics. The use of the interior-point method for solving LP problems is relatively recent.
Reference: [12] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Partitioning the graph of a sparse-matrix to minimize the edge-cut and distributing different partitions to different processors minimizes the communication overhead in parallel sparse matrix-vector multiplication <ref> [12] </ref>. Sparse matrix-vector multiplication is an integral part of all iterative schemes for solving sparse linear systems. GP-based ordering methods are more suitable for solving sparse systems using direct methods on distributed-memory parallel computers than MD-based methods in two respects.
Reference: [13] <author> George Karypis and Vipin Kumar. </author> <title> Parallel multilevel graph partitioning. </title> <type> Technical Report TR 95-036, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: GP-based ordering methods are more suitable for solving sparse systems using direct methods on distributed-memory parallel computers than MD-based methods in two respects. There is strong theoretical and experimental evidence that the process of graph-partitioning and sparse-matrix ordering based on it can be parallelized effectively <ref> [13] </ref>. On the other hand, the only attempt to perform a minimum-degree ordering in parallel that we are aware of [14] was not successful in reducing the ordering time over a serial implementation.
Reference: [14] <author> Madhurima Ghose and Edward Rothberg. </author> <title> A parallel implementation of the multiple minimum degree ordering heuristic. </title> <type> Technical report, </type> <institution> Old Dominion University, </institution> <address> Norfolk, VA, </address> <year> 1994. </year>
Reference-contexts: There is strong theoretical and experimental evidence that the process of graph-partitioning and sparse-matrix ordering based on it can be parallelized effectively [13]. On the other hand, the only attempt to perform a minimum-degree ordering in parallel that we are aware of <ref> [14] </ref> was not successful in reducing the ordering time over a serial implementation. In addition to being parallelizable itself, a GP-based ordering also aids the parallelization of the factorization and triangular solution phases of a direct solver.
Reference: [15] <author> Anshul Gupta, George Karypis, and Vipin Kumar. </author> <title> Highly scalable parallel algorithms for sparse matrix factorization. </title> <type> Technical Report 94-63, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems, 1997. Postscript file available via anonymous FTP from the site ftp://ftp.cs.umn.edu/users/kumar. </note>
Reference-contexts: In addition to being parallelizable itself, a GP-based ordering also aids the parallelization of the factorization and triangular solution phases of a direct solver. Gupta, Karypis, and Kumar <ref> [15, 16] </ref> have proposed a highly scalable parallel formulation of sparse Cholesky factorization. This algorithm derives a significant part of its parallelism from the underlying partitioning of the graph of the sparse matrix. <p> In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel. This is the basis of many efficient parallel algorithms for sparse-matrix factorization <ref> [15, 16, 28] </ref>. 4.1 Graph bisection A key step in our ordering algorithm is finding a small node bisector of a graph. This can be accomplished by the heuristics described in Section 3 with some modifications to the coarsening and refinement strategies.
Reference: [16] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> Parallel formulation of interior point algorithms. </title> <type> Technical Report 94-20, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <month> April </month> <year> 1994. </year> <booktitle> Shorter versions appear in Supercomputing '94 Proceedings and Proceedings of the 1995 DIMACS Workshop on Parallel Processing of Discrete Optimization Problems. </booktitle>
Reference-contexts: In addition to being parallelizable itself, a GP-based ordering also aids the parallelization of the factorization and triangular solution phases of a direct solver. Gupta, Karypis, and Kumar <ref> [15, 16] </ref> have proposed a highly scalable parallel formulation of sparse Cholesky factorization. This algorithm derives a significant part of its parallelism from the underlying partitioning of the graph of the sparse matrix. <p> In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel. This is the basis of many efficient parallel algorithms for sparse-matrix factorization <ref> [15, 16, 28] </ref>. 4.1 Graph bisection A key step in our ordering algorithm is finding a small node bisector of a graph. This can be accomplished by the heuristics described in Section 3 with some modifications to the coarsening and refinement strategies.
Reference: [17] <author> Anshul Gupta and Vipin Kumar. </author> <title> Parallel algorithms for forward and back substitution in direct solution of sparse linear systems. </title> <booktitle> In Supercomputing '95 Proceedings, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Gupta, Karypis, and Kumar [15, 16] have proposed a highly scalable parallel formulation of sparse Cholesky factorization. This algorithm derives a significant part of its parallelism from the underlying partitioning of the graph of the sparse matrix. In <ref> [17] </ref>, Gupta and Kumar present efficient parallel algorithms for solving lower- and upper-triangular systems resulting from sparse factorization.
Reference: [18] <author> T. Bui and C. Jones. </author> <title> A heuristic for reducing fill in sparse matrix factorization. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 445-452, </pages> <year> 1993. </year>
Reference-contexts: In both parallel factorization and triangular solutions, a part of the parallelism would be lost if an MD-based heuristic is used to preorder the sparse matrix. 3 Multilevel graph partitioning Recent research <ref> [18, 19, 10] </ref> has shown multilevel algorithms to be fast and effective in computing graph-partitions. A typical multilevel graph-partitioning algorithm has four components: coarsening, initial partitioning, uncoarsening, and refining.
Reference: [19] <author> Bruce Hendrickson and Robert Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <booktitle> In Supercomputing '95 Proceedings, </booktitle> <year> 1995. </year> <note> Also available as Technical Report SAND93-1301, </note> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM. </address>
Reference-contexts: In both parallel factorization and triangular solutions, a part of the parallelism would be lost if an MD-based heuristic is used to preorder the sparse matrix. 3 Multilevel graph partitioning Recent research <ref> [18, 19, 10] </ref> has shown multilevel algorithms to be fast and effective in computing graph-partitions. A typical multilevel graph-partitioning algorithm has four components: coarsening, initial partitioning, uncoarsening, and refining. <p> Given a weighted graph after any stage of coarsening, there are several choices of matchings for the next coarsening step. A simple matching scheme <ref> [19] </ref> known as random matching (RM) randomly chooses pairs of connected unmatched nodes to include in the matching. <p> Therefore, multiple cycles of coarsening and uncoarsening on the smaller, coarser q-node graph have little impact on the overall run time of the entire algorithm. Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic <ref> [20, 21, 19, 22] </ref> or a greedy refinement scheme [10, 23] for refining the edge-separators. The linear time Fiduccia-Mattheyses variation [21] of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large. <p> The Kernighan-Lin and Fiduccia-Mattheyses heuristics were originally developed to refine two partitions, but can be adapted for refining multiple partitions <ref> [19, 22] </ref> by using multiple priority queues. 4 Ordering sparse matrices using multiple multilevel recursive bi sections In this section, we describe how graph bisection (partition into two parts) by the process described in Section 3 is used to compute a fill-reducing ordering of a symmetric sparse-matrix.
Reference: [20] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <year> 1970. </year>
Reference-contexts: Therefore, multiple cycles of coarsening and uncoarsening on the smaller, coarser q-node graph have little impact on the overall run time of the entire algorithm. Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic <ref> [20, 21, 19, 22] </ref> or a greedy refinement scheme [10, 23] for refining the edge-separators. The linear time Fiduccia-Mattheyses variation [21] of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large.
Reference: [21] <author> C. M. Fiduccia and R. M. Mattheyses. </author> <title> A linear time heuristic for improving network partitions. </title> <booktitle> In Proceedings of the 19th IEEE Design Automation Conference, </booktitle> <pages> pages 175-181, </pages> <year> 1982. </year>
Reference-contexts: Therefore, multiple cycles of coarsening and uncoarsening on the smaller, coarser q-node graph have little impact on the overall run time of the entire algorithm. Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic <ref> [20, 21, 19, 22] </ref> or a greedy refinement scheme [10, 23] for refining the edge-separators. The linear time Fiduccia-Mattheyses variation [21] of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large. <p> Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic [20, 21, 19, 22] or a greedy refinement scheme [10, 23] for refining the edge-separators. The linear time Fiduccia-Mattheyses variation <ref> [21] </ref> of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large. <p> After each uncoarsening step, we refine the node-separator; i.e., in the final steps of uncoarsening, the subject of refinement is changed from the edge-separator to 10 a node-separator. For refining the node-separator, we use Ashcraft and Liu's modification [31] of the Fiduccia-Mattheyses algorithm <ref> [21] </ref>. 4.2 Recursive bisection and ordering Once a node-separator of the original graph is found, it is removed and the entire procedure is repeated recursively on the two disconnected subgraphs.
Reference: [22] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Communications of the ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Therefore, multiple cycles of coarsening and uncoarsening on the smaller, coarser q-node graph have little impact on the overall run time of the entire algorithm. Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic <ref> [20, 21, 19, 22] </ref> or a greedy refinement scheme [10, 23] for refining the edge-separators. The linear time Fiduccia-Mattheyses variation [21] of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large. <p> The Kernighan-Lin and Fiduccia-Mattheyses heuristics were originally developed to refine two partitions, but can be adapted for refining multiple partitions <ref> [19, 22] </ref> by using multiple priority queues. 4 Ordering sparse matrices using multiple multilevel recursive bi sections In this section, we describe how graph bisection (partition into two parts) by the process described in Section 3 is used to compute a fill-reducing ordering of a symmetric sparse-matrix.
Reference: [23] <author> George Karypis and Vipin Kumar. </author> <title> Multilevel k-way partitioning scheme for irregular graphs. </title> <type> Technical Report TR 95-064, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: Depending on the number of partitions k, we either use a variation of the popular Kernighan-Lin heuristic [20, 21, 19, 22] or a greedy refinement scheme <ref> [10, 23] </ref> for refining the edge-separators. The linear time Fiduccia-Mattheyses variation [21] of the Kernighan-Lin heuristic is used for a small number of partitions and the greedy algorithm is used if the number of partitions required is large.
Reference: [24] <author> Alan George. </author> <title> Nested dissection of a regular finite-element mesh. </title> <journal> SIAM Journal on Numerical Ananlysis, </journal> <volume> 10 </volume> <pages> 345-363, </pages> <year> 1973. </year>
Reference-contexts: These labels specify the sequence in which the matrix columns corresponding to the nodes are eliminated during numerical factorization. The overall approach of our ordering algorithm follows the fundamental technique of generalized nested dissection <ref> [24] </ref>. <p> Intuitively, smaller node-separators mean less fill and work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [25, 26, 24, 3] </ref>. Such bounds cannot be proved for arbitrary matrices without a well-defined structure, such as the sparse matrices arising in LP computations.
Reference: [25] <author> R. J. Lipton and R. E. Tarjan. </author> <title> A separator theorem for planar graphs. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 36 </volume> <pages> 177-189, </pages> <year> 1979. </year> <month> 21 </month>
Reference-contexts: Intuitively, smaller node-separators mean less fill and work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [25, 26, 24, 3] </ref>. Such bounds cannot be proved for arbitrary matrices without a well-defined structure, such as the sparse matrices arising in LP computations.
Reference: [26] <author> R. J. Lipton, D. J. Rose, and R. E. Tarjan. </author> <title> Generalized nested dissection. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 </volume> <pages> 346-358, </pages> <year> 1979. </year>
Reference-contexts: Intuitively, smaller node-separators mean less fill and work during factorization. For finite-element graphs with certain nice properties, it can be proved that this technique yields orderings within a constant factor of the optimal <ref> [25, 26, 24, 3] </ref>. Such bounds cannot be proved for arbitrary matrices without a well-defined structure, such as the sparse matrices arising in LP computations.
Reference: [27] <author> Anshul Gupta. </author> <title> Graph partitioning based sparse matrix ordering algorithms for interior-point methods. </title> <type> Technical Report RC 20467 (90480), </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> May 21, </month> <year> 1996. </year> <note> Available on the WWW at the IBM Research CyberJournal site at http://www.research.ibm.com:8080/. </note>
Reference-contexts: Such bounds cannot be proved for arbitrary matrices without a well-defined structure, such as the sparse matrices arising in LP computations. However, as we show in <ref> [27] </ref> and Section 5, with 7 with a star-shaped structure is coarsened to a two-node graph. suitable modifications, the strategy works quite well in practice even for matrices with arbitrary sparsity patterns. In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel.
Reference: [28] <author> Alan George, Joseph W.-H. Liu, and Esmond G.-Y. Ng. </author> <title> Communication reduction in parallel sparse Cholesky factorization on a hypercube. </title> <editor> In M. T. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1987, </booktitle> <pages> pages 576-586. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: In addition, graph bisection yields two submatrices of the original matrix that can be factored independently in parallel. This is the basis of many efficient parallel algorithms for sparse-matrix factorization <ref> [15, 16, 28] </ref>. 4.1 Graph bisection A key step in our ordering algorithm is finding a small node bisector of a graph. This can be accomplished by the heuristics described in Section 3 with some modifications to the coarsening and refinement strategies.
Reference: [29] <author> Iain S. Duff and Torbjorn Wiberg. </author> <title> Remarks on implementations of O(n 1=2 o ) assignment algorithms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14 </volume> <pages> 267-287, </pages> <year> 1988. </year>
Reference-contexts: Karypis and Kumar [9] refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then they use an algorithm for finding a minimum cover <ref> [29, 30] </ref> to compute a node-separator from the edge-separator. This approach relies heavily on the assumption that the size of a node-separator is proportional to the size of the edge-separator containing it. This assumption is often incorrect, especially for the highly unstructured LP matrices.
Reference: [30] <author> Alex Pothen and C.-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference-contexts: Karypis and Kumar [9] refine the edge-separator between the two subgraphs after each step of uncoarsening so that few edges connect nodes of different subgraphs in the final partitioning of the original graph. Then they use an algorithm for finding a minimum cover <ref> [29, 30] </ref> to compute a node-separator from the edge-separator. This approach relies heavily on the assumption that the size of a node-separator is proportional to the size of the edge-separator containing it. This assumption is often incorrect, especially for the highly unstructured LP matrices.
Reference: [31] <author> Cleve Ashcraft and Joseph W.-H. Liu. </author> <title> Generalized nested dissection: Some recent progress. </title> <booktitle> In Proceedings of Fifth SIAM Conference on Applied Linear Algebra, </booktitle> <address> Snowbird, Utah, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: After each uncoarsening step, we refine the node-separator; i.e., in the final steps of uncoarsening, the subject of refinement is changed from the edge-separator to 10 a node-separator. For refining the node-separator, we use Ashcraft and Liu's modification <ref> [31] </ref> of the Fiduccia-Mattheyses algorithm [21]. 4.2 Recursive bisection and ordering Once a node-separator of the original graph is found, it is removed and the entire procedure is repeated recursively on the two disconnected subgraphs.

References-found: 31

