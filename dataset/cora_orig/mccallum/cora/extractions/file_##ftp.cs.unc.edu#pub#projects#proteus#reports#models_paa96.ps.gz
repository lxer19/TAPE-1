URL: file://ftp.cs.unc.edu/pub/projects/proteus/reports/models_paa96.ps.gz
Refering-URL: http://www.cs.unc.edu/~phm/papers.html
Root-URL: http://www.cs.unc.edu
Title: Models and Resource Metrics for Parallel and Distributed Computation  
Author: Zhiyong Li, Peter H. Mills, John H. Reif 
Address: Durham, N.C. 27708-0129  
Affiliation: Department of Computer Science, Duke University,  
Abstract: This paper presents a framework of using resource metrics to characterize the various models of parallel computation. Our framework reflects the approach of recent models to abstract architectural details into several generic parameters, which we call resource metrics. We examine the different resource metrics chosen by different parallel models, categorizing the models into four classes: the basic synchronous models, and extensions of the basic models which more accurately reflect practical machines by incorporating notions of asynchrony, communication cost and memory hierarchy. We then present a new parallel computation model, the LogP-HMM model, as an illustration of design principles based on the framework of resource metrics. The LogP-HMM model extends an existing parameterized network model (LogP) with a sequential hierarchical memory model (HMM) characterizing each processor. The result captures both network communication costs and the effects of multileveled memory such as local cache and I/O. More generally, the LogP-HMM is representative of a class of models formed by combining a network model with any of several existing hierarchical memory models. Along these lines we introduce a variant of the LogP-HMM model, the LogP-UMH, which combines the LogP with the Universal Memory Hierarchy (UMH) model. We examine the potential utility of both our models in the design of several near optimal FFT and sorting algorithms. We also examine the potential of the LogP-UMH to more accurately reflect parallel machines by matching the model to the CM-5 and IBM SP2.
Abstract-found: 1
Intro-found: 1
Reference: [AACS87] <author> A. Aggarwal, B. Alpern, A. Chandra, and M. Snir, </author> <title> "A model for hierarchical memory," </title> <booktitle> in Proc. 19th ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 305-314, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: For example, a sequential computer is suitably characterized by the resources of sequential computation time and space usage. It is commonly accepted that the sequential computational models, such as RAM and its hierarchical memory extensions HMM, BT f and UMH <ref> [AACS87, ACS87, ACF94] </ref>, reflect these resources quite well and therefore provide a common 4 base for sequential computation. Resource metrics for a practical parallel machine are far more complicated than those of sequential machines. <p> In this case, the sorting lower bound for N=P elements in each processor is also the lower bound for the whole sorting procedure. By the result in <ref> [AACS87] </ref>, the sorting lower bound for N=P elements in HMM model with memory access cost f (x)=log x is exactly ((N=P ) log (N=P ) log log (N=P )): A similar argument can be used for FFT computation. Therefore we prove the theorem. 2 FFT Algorithm 1. <p> After each processor accepts the N=P N=P 2 messages, it begins 17 Step II which comprises the computation of the non-input nodes of N=P 2 disjoint P -input but-terflies. By the result of <ref> [AACS87] </ref>, Step I computation needs O ((N=P ) log (N=P ) log log (N=P )) time. <p> Sorting. A near optimal sorting algorithm can be obtained by using columnsort [Lei85] and the modified median-sort algorithm proposed in <ref> [AACS87] </ref> for a local view of memory layout. We will denote this modified median-sort algorithm as HMM-sorting in the rest of the paper and we will use the result that the HMM-sorting can sort N elements in O (N log N log log N ) time.
Reference: [AC94] <author> B. Alpern and L. Carter, </author> <title> "Towards a model for portable parallel performance: exposing the memory hierarchy," </title> <booktitle> in Portability and Performance for Parallel Processing, </booktitle> <pages> pp. 21-41, </pages> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH <ref> [AC94] </ref>, and P-UMH [NV91]). The approach followed by these models is that of a parameterized (or generic) model, which abstracts the architectural details into several generic parameters which we call resource met-rics. <p> Rigorous analysis of algorithms is generally difficult to accomplish within this model due to the richness of the resource metrics and their qualitative properties. 4.3 Hierarchical models The Parallel Memory Hierarchy model (PMH) <ref> [AC94] </ref> and Parallel Hierarchical Memory model (P-HMM) [VS94] discussed in this section address the concerns of memory hierarchy in a parallel setting. The P-HMM primarily originates from considering in a parallel network the existence of secondary or disk memory. <p> PMH on the other hand uses "memory hierarchy" as a more general technique to model not only the hierarchy within a processor but also the communication characteristics of a parallel machine. 4.3.1 Parallel Memory Hierarchy model The PMH is a so-called generic model which defines a class of specific models <ref> [AC94] </ref>. In the PMH model, a parallel computer is modeled as a tree and each node of the tree is called a module. All of the leaf modules are used to denote the processors and the internal modules hold the data. <p> Even though this model is termed a parallel memory hierarchy, the internal modules need not necessarily correspond to actual memory modules of the real machine; when modeling the CM-5 for example, many of the modules are used to capture the interprocessor communication capabilities <ref> [AC94] </ref>. The PMH model derives from the developers' experience in tuning code for memory hierarchy. Many sequential algorithms have been developed for the antecedent sequential UMH model.
Reference: [ACF94] <author> B. Alpern, L. Carter, and E. Feig, </author> <title> "The uniform memory model of computation," </title> <journal> Algorithmica, </journal> <volume> vol. 12, </volume> <pages> pp. 72-109, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: For example, a sequential computer is suitably characterized by the resources of sequential computation time and space usage. It is commonly accepted that the sequential computational models, such as RAM and its hierarchical memory extensions HMM, BT f and UMH <ref> [AACS87, ACS87, ACF94] </ref>, reflect these resources quite well and therefore provide a common 4 base for sequential computation. Resource metrics for a practical parallel machine are far more complicated than those of sequential machines. <p> And there fore it is optimal. 6 LogP-UMH Model The principal difference between the LogP-UMH and the LogP-HMM model is that in the former the memory of each processor is organized in a fashion following the Uniform Memory Hierarchy (UMH) <ref> [ACF94] </ref>. The UMH is an alternative model for multilevel memories, and is an instance of the more general MH model also described in [ACF94]. The memory hierarchy in the MH model consists of several levels of memory modules, where each level is characterized by three parameters. <p> the LogP-UMH and the LogP-HMM model is that in the former the memory of each processor is organized in a fashion following the Uniform Memory Hierarchy (UMH) <ref> [ACF94] </ref>. The UMH is an alternative model for multilevel memories, and is an instance of the more general MH model also described in [ACF94]. The memory hierarchy in the MH model consists of several levels of memory modules, where each level is characterized by three parameters. <p> After transposing this submatrix, the elements that need to stay and those that need to be communicated will be put into different memory subblocks each of size . The matrix transposition can be done in O (N=P ) time by using the algorithm in <ref> [ACF94] </ref>. 3. The transposed submatrix is stored at memory level dlog 2 (N=(ffP ))e. We can now pipeline the blocks that need to be sent down the memory hierarchy and then sent through the network (we can schedule it so that the message passing through the network forms a ring). <p> However, here the local computation will be conducted in a memory hierarchy characterized by UMH parameters. Since the communication pattern is the same as the algorithm presented for the LogP-HMM model, we need only analyze the local FFT computation. We use a three step algorithm <ref> [ACF94, Loa92] </ref> for the local m-input FFT computation, where m = N=P , as follows: 1. Regard the m inputs as a p p m matrix stored in column major order at level dlog 2 (m=ff)e. Compute p p m-input FFTs along the rows. 2. Transpose the matrix. 3. <p> By lemma 2, this will cost O (m) time (since p p m) = O (m)). Step 2 of matrix transposition takes O (m) time using the algorithms described in <ref> [ACF94] </ref>. Step 3 takes the same amount of time as Step 1. <p> ( N P + L+o P N The same result can be obtained for the UMH machine with M (`) = h; 2; `i (which has a smaller bandwidth than that considered here) if we use the hybrid method for the global computation and use the two step algorithm in <ref> [ACF94] </ref> for the local computation. 6.2 Matching LogP-UMH to practical parallel machines The LogP-UMH, in comparison to the LogP-HMM, can be used to more accurately model many distributed memory machines. The interconnection network of the machine will be captured by the four parameters L, o, g, and P . <p> The multilevel memory for each node which typically consists of a register file, a cache, and an internal memory (plus a disk storage for each node 3 There is an omission in <ref> [ACF94] </ref> for the similar analysis. 22 CM-5 node processor LogP-UMH memory level n ` s ` 1=f (`) n ` s ` 1=f (`) 0 264 4 4 256 1 4 2 8K 4K 256K 1K Table 2: Comparison of CM-5 node memory hierarchy with a LogP-UMH memory hierarchy derived from
Reference: [ACS87] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "Hierarchical memory with block transfer," </title> <booktitle> in Proc. 28th Symp. on Foundations of Computer Science, </booktitle> <pages> pp. 204-216, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: For example, a sequential computer is suitably characterized by the resources of sequential computation time and space usage. It is commonly accepted that the sequential computational models, such as RAM and its hierarchical memory extensions HMM, BT f and UMH <ref> [AACS87, ACS87, ACF94] </ref>, reflect these resources quite well and therefore provide a common 4 base for sequential computation. Resource metrics for a practical parallel machine are far more complicated than those of sequential machines.
Reference: [ACS89] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "On communication latency in PRAM computation," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 11-21, </pages> <year> 1989. </year>
Reference-contexts: a communication step, in which it can write and then read a word from the global memory, or a computation computation step, which is an operation that accesses at most two words from its local memory. 4.2.2 BPRAM An extension of the LPRAM, the Block PRAM (BPRAM), is described in <ref> [ACS89] </ref>. The BPRAM takes into account the reduced cost for transferring a contiguous block of data. The BPRAM model is defined with two parameters l (latency or startup time) and P (number of processors). The cost of accessing local memory is unit time. <p> The cost of accessing local memory is unit time. However the cost of transmitting a block of size b of contiguous locations from global memory is l + b . We present the following FFT algorithm for the BPRAM model from <ref> [ACS89] </ref>. We follow the EREW assumption and the input and the output are both stored in global memory. The pair [time, work] is used to measure the complexity of the LPRAM algorithms. Let T M denote the minimal running time and w M denote the work. <p> Therefore all stages can be completed in O (((log N )=k)(l + k2 k )) time. The permutation is needed to rearrange the data after each stage and this permutation can be done in time O ( log (lP=N) ) by <ref> [ACS89] </ref>. In total, (log N )=k permutations are needed.
Reference: [ACS90] <author> A. Aggarwal, A. K. Chandra, and M. Snir, </author> <title> "Communication complexity of PRAMs," </title> <journal> J. Theoretical Computer Science, </journal> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM <ref> [ACS90] </ref>, Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> synchronous latency models such as the LPRAM and BPRAM which add the notion of latency into the PRAM model, and models such as the BSP and LogP which not only incorporate asynchrony and latency but also address the issue of bandwidth limitation. 8 4.2.1 LPRAM The Local-Memory PRAM (LPRAM) model <ref> [ACS90] </ref> consists of a shared global memory and a set of processors with unbounded local memory executing in lock-step. The access protocol to global memory is CREW.
Reference: [AV88] <author> A. Aggarwal and J. Vitter, </author> <title> "The input/output complexity of sorting and related problems," </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, </volume> <pages> pp. 1116-1127, </pages> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: It originates from the consideration that data must often reside in secondary storage rather than main memory; in a parallel setting this may involve the parallel access of multiple disks. Therefore it is necessary to design parallel algorithms which consider the possible data movement between main and secondary memory <ref> [AV88] </ref>, and more generally which consider multiple levels of memory including register and cache.
Reference: [Bel92] <author> G. Bell, </author> <title> "Ultracomputers : A teraflop before its time," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, no. 8, </volume> <pages> pp. 26-47, </pages> <year> 1992. </year>
Reference: [Ble90] <author> G. E. Blelloch, </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The most recent variation is QRQW [GMR94], which assumes that simultaneous access to the same memory block will be inserted into a request queue and served in a FIFO manner. 3.2 VRAM Another extension of the serial RAM model is the Vector Random Access Machine (VRAM) <ref> [Ble90] </ref>. The VRAM is a serial random access machine with the addition of a vector memory, 6 a vector processor, and vector input and output ports. Typical vector instructions include elementwise operations, data movement operations, scans, and packs.
Reference: [BNK92] <author> A. Bar-Noy and S. Kipnis, </author> <title> "Designing broadcasting algorithms in the Postal model for message-passing systems," </title> <booktitle> in Proc. 4th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 11-22, </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model <ref> [BNK92] </ref>, BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> In total, (log N )=k permutations are needed. If we take k = log (max ( l P )) then it yields that the work is O (N log N + lP + log l log lP ) 4.2.3 Postal model The Postal model <ref> [BNK92] </ref> is a distributed memory model with the constraint that the point-to-point communication has latency . It can be regarded as a model described by two parameters: &lt; P; &gt;, where P stands for the number of processors.
Reference: [Cel94] <author> J. Celuch, </author> <title> "The 9076 SP1 high-performance communication network," </title> <type> Technical Report, </type> <institution> KGNVMC, Kingston, </institution> <year> 1994. </year>
Reference-contexts: The IBM SP2 provides a more interesting example since each node of the SP2 not only has a cache and an internal memory, but also has a large disk storage. The interconnection network of the SP2 is a high-performance switch <ref> [Cel94, SDG + 94] </ref>. Using the LogP-UMH model, we can abstract the communication characteristics of this switch by four parameters L, o, g, and P .
Reference: [CKP + 93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramo-nian, and T. von Eicken, </author> <title> "LogP: Towards a realistic model of parallel computation," </title> <booktitle> in Proc. 4th ACM Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 1-12, </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP [Val90], and LogP <ref> [CKP + 93] </ref>), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> The communication overhead is the time that the processor engages in sending and receiving a message. In most cases, the value is dependent on the communication protocol implemented in a practical machine. For example, in the CM-5 it could be a linear function of the message size <ref> [CKP + 93] </ref>. Block transfer capability. In most architectures, a significant cost (latency) is incurred to access the first of a contiguous block of words, but after that, successive words can be accessed in unit time. Memory hierarchy. <p> The LogP model uses the param 10 eters L (an upper bound of latency for transmitting a single message), o (the computation overhead of handling a message), g (a lower bound of time interval between consecutive message transmissions at a processor) and P (the number of processors) <ref> [CKP + 93] </ref>. In contrast to the BSP model, it removes the barrier synchronization requirement (h-relation in BSP) and allows the processors to run asynchronously. <p> FFT for LogP. The data layout and communication scheduling are two key aspects to achieving an efficient algorithm for the FFT problem under the LogP model. Three methods of data layout are discussed in <ref> [CKP + 93] </ref>. The cyclic layout assigns the ith row of the butterfly to the ith processor. The block layout places the first N=P rows on the first processor, the next N=P rows on the second processor, and so on. <p> For example, the CM-5 can be modeled as follows. The use of L, o, g and P to model the CM-5 has been discussed in <ref> [CKP + 93] </ref>, where it is determined that the values of L = 6 s, o = 2 s and g = 4 s are fairly appropriate.
Reference: [CZ89] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating asynchrony into the PRAM model," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 169-178, </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: This problem has spurred the development of several extensions of the PRAM which attempt to make the model more practical while still preserving much of its simplicity. The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM <ref> [CZ89] </ref>), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH <p> The total time, including the synchronization cost after each stage, is O (B log N= log B). 4.1.2 APRAM The Asynchronous PRAM (APRAM) is a "fully" asynchronous model <ref> [CZ89, CZ90] </ref>. The APRAM model consists of a global shared memory and a set of processes with their own local memories. The basic operations executed by the APRAM process are called events. An APRAM computation is denoted as the set of possible serializations of events executed by the processes. <p> For an algorithm the round complexity is defined as the maximum round complexity over all of the possible computations. Summation for APRAM. As an example of APRAM algorithm design we present an example of summation from <ref> [CZ89] </ref>. As in the PRAM model, a binary tree is used to sum the N input values, which initially reside at the N leaves. <p> N processes are used and with each process p are also associated three variables V (p), L (p) and R (p), which denote the value of p, the left child of p, and the right child of p respectively. The algorithm for process i, as described in <ref> [CZ89] </ref>, consists of two steps: (1) wait until L (i) and R (i) are valids; (2) set V (i) := L (i) + R (i) and valid (i) := true. The algorithm terminates when the valid bit at the root of the tree is true.
Reference: [CZ90] <author> R. Cole and O. Zajicek, </author> <title> "The expected advantage of asynchrony," </title> <booktitle> in Proc. 2nd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 85-94, </pages> <publisher> ACM, </publisher> <year> 1990. </year> <month> 25 </month>
Reference-contexts: The total time, including the synchronization cost after each stage, is O (B log N= log B). 4.1.2 APRAM The Asynchronous PRAM (APRAM) is a "fully" asynchronous model <ref> [CZ89, CZ90] </ref>. The APRAM model consists of a global shared memory and a set of processes with their own local memories. The basic operations executed by the APRAM process are called events. An APRAM computation is denoted as the set of possible serializations of events executed by the processes.
Reference: [FW78] <author> S. Fortune and J. Wyllie, </author> <title> "Parallelism in random access machines," </title> <booktitle> in Proc.10th ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 114-118, </pages> <year> 1978. </year>
Reference-contexts: Historically, the Parallel Random Access Machine (PRAM) is the most widely used parallel model <ref> [FW78] </ref>. The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free. The PRAM model could be regarded as one extreme which makes a large number of assumptions in order to simplify algorithm design [J 92, Rei93].
Reference: [Gib89] <author> P. B. Gibbons, </author> <title> "A more practical PRAM model," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: This problem has spurred the development of several extensions of the PRAM which attempt to make the model more practical while still preserving much of its simplicity. The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM <ref> [Gib89] </ref> and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the <p> be viewed as the addition of more resource metrics to the PRAM model in order to gain improved performance measures. 4.1 Asynchronous models Among the first extensions to the PRAM were the Phase PRAM and APRAM models, which incorporate some notion of asynchronous execution. 4.1.1 Phase PRAM The Phase PRAM <ref> [Gib89] </ref> extends the PRAM model with semi-asynchrony. A Phase PRAM machine consists of a shared global memory, a set of P sequential processors, and a private local memory for each processor. <p> The cost of global read, global write and local operations are the same constant. The cost of a synchronization step, B (P ), is dependent on the number of processors P . As an example of Phase PRAM algorithm design, below we present an algorithm for FFT computation from <ref> [Gib89] </ref>. It is worth noting that a variant of the Phase PRAM, the Phase LPRAM model, accounts as well for the cost of communication latency. FFT for Phase PRAM. The algorithm presented in [Gib89] for the EREW Phase PRAM divides the columns of an N -input FFT graph into log N= <p> As an example of Phase PRAM algorithm design, below we present an algorithm for FFT computation from <ref> [Gib89] </ref>. It is worth noting that a variant of the Phase PRAM, the Phase LPRAM model, accounts as well for the cost of communication latency. FFT for Phase PRAM. The algorithm presented in [Gib89] for the EREW Phase PRAM divides the columns of an N -input FFT graph into log N= log B stages, each consisting of log B columns. It also divides the rows into N=B sets, each consisting of B rows.
Reference: [GMR94] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran, </author> <title> "The QRQW PRAM : Accounting for contention in parallel algorithms," </title> <booktitle> in Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pp. 638-647, </pages> <year> 1994. </year>
Reference-contexts: Protocols include EREW (exclusive read exclusive write), CREW (concurrent read exclusive write), and CRCW (concurrent read concurrent write). The latter protocol can be further divided into several classes by the semantics of the concurrent write. The most recent variation is QRQW <ref> [GMR94] </ref>, which assumes that simultaneous access to the same memory block will be inserted into a request queue and served in a FIFO manner. 3.2 VRAM Another extension of the serial RAM model is the Vector Random Access Machine (VRAM) [Ble90].
Reference: [Goo93] <author> M. Goodrich, </author> <title> "Parallel algorithms column I: Models of computation," </title> <journal> SIGACT News, </journal> <volume> vol. 24, </volume> <pages> pp. 16-21, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The challenge then becomes to design a general model of parallel computation in such a way that it balances being sufficiently detailed to reflect realistic aspects impacting performance while still remaining abstract enough to be machine-independent and amenable to analysis <ref> [Ski91, Goo93] </ref>. Historically, the Parallel Random Access Machine (PRAM) is the most widely used parallel model [FW78]. The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free.
Reference: [J 92] <author> J. JaJa, </author> <title> An introduction to parallel algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free. The PRAM model could be regarded as one extreme which makes a large number of assumptions in order to simplify algorithm design <ref> [J 92, Rei93] </ref>. However, for many current parallel machines, the PRAM is often inaccurate in predicting the actual running time and resource utilization of algorithms since it hides details which impact performance such as the time required for network communication as well as issues of asynchrony and memory hierarchy.
Reference: [KSSS93] <author> R. Karp, A. Sahay, E. Santos, and K. Schauser, </author> <title> "Optimal broadcast and summation in the LogP model," </title> <booktitle> in Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 142-153, </pages> <year> 1993. </year>
Reference-contexts: It can be regarded as a model described by two parameters: &lt; P; &gt;, where P stands for the number of processors. Several elegant optimal broadcast and summation algorithms have been designed based on this model, which were then extended for the LogP model <ref> [KSSS93] </ref>. Algorithms other than broadcast and summation have largely not been presented for this model. 4.2.4 Bulk-Synchronous Parallel model The BSP is a distributed memory model [Val90]. <p> Moreover, it is often reasonable to ignore the parameter of o in a practical machine, such as in a machine with low bandwidth (high g value). Examples using this strategy can be found in the FFT algorithm discussed below and also in <ref> [KSSS93] </ref>. FFT for LogP. The data layout and communication scheduling are two key aspects to achieving an efficient algorithm for the FFT problem under the LogP model. Three methods of data layout are discussed in [CKP + 93].
Reference: [Lei85] <author> T. Leighton, </author> <title> "Tight bounds on the complexity of parallel sorting," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 34, no. 3, </volume> <pages> pp. 344-354, </pages> <year> 1985. </year>
Reference-contexts: Sorting. A near optimal sorting algorithm can be obtained by using columnsort <ref> [Lei85] </ref> and the modified median-sort algorithm proposed in [AACS87] for a local view of memory layout.
Reference: [Loa92] <author> C. V. Loan, </author> <title> Computational frameworks for the fast Fourier transform. </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: However, here the local computation will be conducted in a memory hierarchy characterized by UMH parameters. Since the communication pattern is the same as the algorithm presented for the LogP-HMM model, we need only analyze the local FFT computation. We use a three step algorithm <ref> [ACF94, Loa92] </ref> for the local m-input FFT computation, where m = N=P , as follows: 1. Regard the m inputs as a p p m matrix stored in column major order at level dlog 2 (m=ff)e. Compute p p m-input FFTs along the rows. 2. Transpose the matrix. 3.
Reference: [NV91] <author> M. Nodine and J. Vitter, </author> <title> "Large-scale sorting in parallel memories," </title> <booktitle> in Proc. 3rd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 29-39, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH <ref> [NV91] </ref>). The approach followed by these models is that of a parameterized (or generic) model, which abstracts the architectural details into several generic parameters which we call resource met-rics. <p> The model can be extended to allow block transfer; the resulting model is called the P-BT model [VS94]. Several other extensions which use the UMH memory model and PRAM interconnection respectively are discussed in <ref> [NV91] </ref>. Two factors are critical for developing efficient P-HMM algorithms: data placement and movement between the levels of memory hierarchy, and data movement among the processors. FFT for P-HMM. We present the following P-HMM algorithm from [VS94], computing the N -input FFT when N P 2 : 1.
Reference: [PV81] <author> F. Preparata and J. Vuillemin, </author> <title> "The cube-connection cycles: A versatile network for parallel computation," </title> <journal> Communications of the ACM, </journal> <volume> vol. 24, </volume> <pages> pp. 300-309, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Our approach of using the LogP-HMM model (and its variant the LogP-UMH) in the design of FFT algorithms has broader applicability to the more general class of ascend-and-descend algorithms <ref> [PV81] </ref> (whose data movement follows an FFT graph), used for solving such problems as sorting, polynomial multiplication and convolution. The remainder of this paper is organized as follows. Section 2 identifies resources and resource metrics for parallel computation. Section 3 discusses basic synchronous models.
Reference: [Rei93] <author> J. Reif, </author> <title> Synthesis of parallel algorithms. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free. The PRAM model could be regarded as one extreme which makes a large number of assumptions in order to simplify algorithm design <ref> [J 92, Rei93] </ref>. However, for many current parallel machines, the PRAM is often inaccurate in predicting the actual running time and resource utilization of algorithms since it hides details which impact performance such as the time required for network communication as well as issues of asynchrony and memory hierarchy.
Reference: [Sah92] <author> A. Sahay, </author> <title> "Hiding communication costs in bandwidth-limited parallel FFT computation," </title> <type> Technical Report UCB/CSD 92/722, </type> <institution> UC Berkeley, </institution> <year> 1992. </year>
Reference-contexts: The naive communication schedule stalls on the first send. The technique of overlapping computation and communication can be used to eliminate this stall for the large problem instances. The hybrid method introduced in <ref> [Sah92] </ref> staggers the different starting rows for different processors: processor i starts with its iN=P 2 -th row, proceeds to the last row, and wraps around. <p> Since the algorithm will operate on the local data set and use message passing to redistribute the data, we will refer to this algorithm as a local view algorithm. The idea of the hybrid method has been discussed in Section 4.2.5. A more detailed discussion can be found in <ref> [Sah92] </ref>. Two steps are used to compute the N -input FFT. In Step I, each processor computes an N=P -input butterfly; after each processor finishes its computation, it sends N=P 2 messages to each of the other processors.
Reference: [SDG + 94] <author> C. Stunkel, D. Dhea, D. Grice, P. Hochschild, and M. Tsao, </author> <title> "The SP1 high-performance switch," </title> <booktitle> in Proc. of the Scalable High Performance Computing Conference, </booktitle> <address> (Knoxville, TN), </address> <pages> pp. 150-157, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The IBM SP2 provides a more interesting example since each node of the SP2 not only has a cache and an internal memory, but also has a large disk storage. The interconnection network of the SP2 is a high-performance switch <ref> [Cel94, SDG + 94] </ref>. Using the LogP-UMH model, we can abstract the communication characteristics of this switch by four parameters L, o, g, and P .
Reference: [Ski91] <author> D. Skillicorn, </author> <title> "Models for practical parallel computation," </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol. 20, no. 2, </volume> <pages> pp. 133-158, </pages> <year> 1991. </year>
Reference-contexts: The challenge then becomes to design a general model of parallel computation in such a way that it balances being sufficiently detailed to reflect realistic aspects impacting performance while still remaining abstract enough to be machine-independent and amenable to analysis <ref> [Ski91, Goo93] </ref>. Historically, the Parallel Random Access Machine (PRAM) is the most widely used parallel model [FW78]. The PRAM model assumes that all processors work synchronously and that interprocessor communication is essentially free.
Reference: [Sny86] <author> L. Snyder, </author> <title> "Type architecture, shared memory, and the corollary of modest potential," </title> <booktitle> Annual Review of Computer Science, </booktitle> <pages> pp. 289-317, </pages> <year> 1986. </year> <month> 26 </month>
Reference-contexts: This leads to an optimal algorithm for large problem instances and reasonable g value. 4.2.6 Candidate Type Architecture The Candidate Type Architecture (CTA) <ref> [Sny86] </ref> is an earlier precursor to parameterized latency models which presents a more direct abstraction of a practical parallel machine. The CTA consists of a front-end and a back-end. The front-end works as a control processor; the back-end is a MIMD machine.
Reference: [Thi92] <institution> Thinking Machine Corporation, </institution> <note> "DPEAC reference manual," </note> <year> 1992. </year>
Reference-contexts: The obvious omission of memory hierarchy in this characterization can be alleviated by adding the parameters of ff, and bandwidth 1=f (`) for the memory levels. To match the values suggested in CM-5 technical specification documents <ref> [Thi92] </ref>, we take ff=256, =32 and 1=f (`)=4 to approximate the memory hierarchy.
Reference: [Val90] <author> L. Valiant, </author> <title> "A bridging model for parallel computation," </title> <journal> Communications of the ACM, </journal> <volume> vol. 33, </volume> <pages> pp. 103-111, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: The variations extend the PRAM to incorporate realistic aspects such as asynchrony of processes (e.g., the Phase PRAM [Gib89] and APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP <ref> [Val90] </ref>, and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM [VS94], PMH [AC94], and P-UMH [NV91]). <p> Several elegant optimal broadcast and summation algorithms have been designed based on this model, which were then extended for the LogP model [KSSS93]. Algorithms other than broadcast and summation have largely not been presented for this model. 4.2.4 Bulk-Synchronous Parallel model The BSP is a distributed memory model <ref> [Val90] </ref>. Like the Phase PRAM, the BSP is also a semi-asynchronous model because it requires synchronization after each "superstep", within which the processes can run asynchronously. <p> If the length of a superstep is L, then L local operations and a bL=gc relation message pattern can be realized. The parameters of the machine are therefore L, g and P (the number of processors). Below we present an FFT algorithm for the BSP as described in <ref> [Val90] </ref>. FFT for BSP. The algorithm divides the FFT graph into successive layers where each layer is a superstep. In each layer, the FFT graph is separated into a set of small input size FFTs. Each of them is computed by a different processor.
Reference: [VS90] <author> J. Vitter and E. Shriver, </author> <title> "Optimal disk I/O with parallel block transfer," </title> <booktitle> in Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 159-169, </pages> <year> 1990. </year>
Reference-contexts: However, perhaps because of the complexity and generality of the model, not too many algorithms have been developed for the PMH parallel model. 4.3.2 Parallel Hierarchical Memory model The P-HMM model is also called the parallel I/O model <ref> [VS90, VS94] </ref>. It originates from the consideration that data must often reside in secondary storage rather than main memory; in a parallel setting this may involve the parallel access of multiple disks.
Reference: [VS94] <author> J. S. Vitter and E. A. M. Shriver, </author> <title> "Algorithms for parallel memory II: Hierarchical multilevel memories," </title> <journal> Algorithmica, </journal> <pages> pp. 148-169, </pages> <year> 1994. </year> <month> 27 </month>
Reference-contexts: APRAM [CZ89]), communication costs such as network latency and bandwidth (e.g., the LPRAM [ACS90], Postal Model [BNK92], BSP [Val90], and LogP [CKP + 93]), and memory hierarchy, reflecting the effects of multileveled memory such as differing access times for registers, local cache, main memory, and disk I/O (e.g., the P-HMM <ref> [VS94] </ref>, PMH [AC94], and P-UMH [NV91]). The approach followed by these models is that of a parameterized (or generic) model, which abstracts the architectural details into several generic parameters which we call resource met-rics. <p> Rigorous analysis of algorithms is generally difficult to accomplish within this model due to the richness of the resource metrics and their qualitative properties. 4.3 Hierarchical models The Parallel Memory Hierarchy model (PMH) [AC94] and Parallel Hierarchical Memory model (P-HMM) <ref> [VS94] </ref> discussed in this section address the concerns of memory hierarchy in a parallel setting. The P-HMM primarily originates from considering in a parallel network the existence of secondary or disk memory. <p> However, perhaps because of the complexity and generality of the model, not too many algorithms have been developed for the PMH parallel model. 4.3.2 Parallel Hierarchical Memory model The P-HMM model is also called the parallel I/O model <ref> [VS90, VS94] </ref>. It originates from the consideration that data must often reside in secondary storage rather than main memory; in a parallel setting this may involve the parallel access of multiple disks. <p> The model can be extended to allow block transfer; the resulting model is called the P-BT model <ref> [VS94] </ref>. Several other extensions which use the UMH memory model and PRAM interconnection respectively are discussed in [NV91]. Two factors are critical for developing efficient P-HMM algorithms: data placement and movement between the levels of memory hierarchy, and data movement among the processors. FFT for P-HMM. <p> Two factors are critical for developing efficient P-HMM algorithms: data placement and movement between the levels of memory hierarchy, and data movement among the processors. FFT for P-HMM. We present the following P-HMM algorithm from <ref> [VS94] </ref>, computing the N -input FFT when N P 2 : 1. Compute p p N -input FFTs. Assume that the ith FFT is on the ith contiguous group of N =P tracks (or memory levels). 2. <p> Therefore, we have following recurrence T (N; P ) = 2 N T ( N ; P ) + O ( N which gives the result O ((N=P ) log N log (log N= log P )): This matches the FFT lower bound in <ref> [VS94] </ref> and so is optimal. 14 5 LogP-HMM model The resource metrics chosen by different parallel models discussed so far are summarized in Table 1. <p> For both models we will examine their potential utility in the design of several near optimal FFT and sorting algorithms using two different memory layout schemes (further described in Section 5.2). 5.1 Definition of the model The LogP-HMM model is defined much like the parallel hierarchy memory model <ref> [VS94] </ref>. A LogP-HMM machine consists of a set of asynchronously executing processors, each with an unlimited local memory. The local memory is organized as a sequence of layers with increasing size, where the size of layer i is 2 i . <p> Therefore we prove the theorem. 2 FFT Algorithm 1. This algorithm is designed for the global (or track-oriented) view of the memory system. Using the technique in <ref> [VS94] </ref> and the algorithm given in section 4.3.2, we can compute the FFT on a LogP-HMM machine. The time needed by this algorithm is O ( L+o P log N P ): Steps 1 and 4 take p p N ; P ) + (N=P ) log (N=P ) time. <p> Again the second term is for transferring the data in the memory hierarchy. Summing all of them together, we get the following result: O ( N P log log N which matches the lower bound O ((N=P ) log N log (log N= log P )) proven in <ref> [VS94] </ref>. And there fore it is optimal. 6 LogP-UMH Model The principal difference between the LogP-UMH and the LogP-HMM model is that in the former the memory of each processor is organized in a fashion following the Uniform Memory Hierarchy (UMH) [ACF94].
References-found: 33

