URL: http://www.coli.uni-sb.de/~christer/acl94.ps
Refering-URL: http://www.coli.uni-sb.de/~christer/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Internet: christer@sics.se  
Title: GRAMMAR SPECIALIZATION THROUGH ENTROPY THRESHOLDS create a specialized grammar that retains a high coverage but
Author: Christer Samuelsson 
Keyword: BACKGROUND  
Note: The idea behind this is to  This has turned out to be possible  
Address: Box 1263 S-164 28 Kista, Sweden  
Affiliation: Swedish Institute of Computer Science  
Abstract: Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values. Previous work by Manny Rayner and the author, see [Samuelsson & Rayner 1991] attempts to tailor an existing natural-language system to a specific application domain by extracting a specialized grammar from the original one using a large set of training examples. The training set is a treebank consisting of implicit parse trees that each specify a verified analysis of an input sentence. The parse trees are implicit in the sense that each node in the tree is the (mnemonic) name of the grammar rule resolved on at that point, rather than the syntactic category of the LHS of the grammar rule as is the case in an ordinary parse tree. Figure 1 shows five examples of implicit parse trees. The analyses are verified in the sense that each analysis has been judged to be the preferred one for that input sentence by a human evaluator using a semi-automatic evaluation method. A new grammar is created by cutting up each implicit parse tree in the treebank at appropriate points, creating a set of new rules that consist of chunks of original grammar rules. The LHS of each new rule will be the LHS phrase of the original grammar rule at the root of the tree chunk and the RHS will be the RHS phrases of the rules in the leaves of the tree chunk. For example, cutting up the first parse tree of Figure 1 at the NP of the rule vp_v_np yields rules 2 and 3 of Figure 3. speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a]. 1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner et al 1993] and large corpora of real user data collected using Wizard-of-Oz simulation. The resulting specialized grammar was compiled into LR parsing tables, and a special LR parser exploited their special properties, see [Samuelsson 1994b]. The technical vehicle previously used to extract the specialized grammar is explanation-based generalization (EBG), see e.g. [Mitchell et al 1986]. Very briefly, this consists of redoing the derivation of each training example top-down by letting the implicit parse tree drive a rule expansion process, and aborting the expansion of the specialized rule currently being extracted if the current node of the implicit parse tree meets a set of tree-cutting criteria 2 . In this case the extraction process is invoked recursively to extract subrules rooted in the current node. The tree-cutting criteria can be local ("The LHS of the original grammar rule is an NP ,") or dependent on the rest of the parse tree ("that doesn't dominate the empty string only,") and previous choices of nodes to cut at ("and there is no cut above the current node that is also labelled NP."). A problem not fully explored yet is how to arrive at an optimal choice of tree-cutting criteria. In the previous scheme, these must be specified manually, and the choice is left to the designer's intuitions. This article addresses the problem of automating this process and presents a method where the nodes to cut at are selected automatically using the information-theoretical concept of entropy. Entropy is well-known from physics, but the concept of perplexity is perhaps better known in the 
Abstract-found: 1
Intro-found: 1
Reference: [Alshawi ed. 1992] <author> Hiyan Alshawi, </author> <title> editor. The Core Language Engine, </title> <publisher> MIT Press 1992. </publisher>
Reference: [Jelinek 1990] <author> Fred Jelinek. </author> <title> "Self-Organizing Language Models for Speech Recognition", </title> <booktitle> in Readings in Speech Recognition, </booktitle> <pages> pp. 450-506, </pages> <publisher> Morgan Kauf-mann 1990. </publisher>
Reference-contexts: Perplexity Perplexity is related to entropy as follows. The observed perplexity P o of a language model with respect to an (imaginary) infinite test sequence w 1 ; w 2 ; ::: is defined through the formula (see <ref> [Jelinek 1990] </ref>) ln P o = lim n Here p (w 1 ; :::; w n ) denotes the probability of the word string w 1 ; :::; w n .
Reference: [Mitchell et al 1986] <author> Tom M. Mitchell, Richard M. Kel-ler and Smadar T. Kedar-Cabelli. </author> <title> "Explanation-Based Generalization: A Unifying View", </title> <booktitle> in Machine Learning 1 , No. </booktitle> <volume> 1, </volume> <pages> pp. 47-80, </pages> <year> 1986. </year>
Reference: [Quinlan 1986] <author> J. Ross Quinlan. </author> <title> "Induction of Decision Trees", </title> <booktitle> in Machine Learning 1 , No. </booktitle> <volume> 1, </volume> <pages> pp. 81-107, </pages> <year> 1986. </year>
Reference-contexts: It is interesting to note that a textbook method for constructing decision trees for classification from attribute-value pairs is to minimize the (weighted average of the) remaining entropy 5 over all possible choices of root attribute, see <ref> [Quinlan 1986] </ref>. DETAILED SCHEME First, the treebank is partitioned into a training set and a test set. The training set will be indexed in an and-or tree and used to extract the specialized rules. The test set will be used to check the coverage of the set of extracted rules. <p> This is basically the entropy used in <ref> [Quinlan 1986] </ref>. Unfortunately, this tends to promote daughters of cut-nodes to in turn become cutnodes, and also results in a problem with instability, especially in conjunction with the additional constraints discussed in a later section, since the entropy of each node is now dependent on the choice of cutnodes.
Reference: [Rayner et al 1993] <author> M. Rayner, H. Alshawi, I. Bretan, D. Carter, V. Digalakis, B. Gamback, J. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman and C. Samuelsson. </author> <title> "A Speech to Speech Translation System Built From Standard Components", </title> <booktitle> in Procs. ARPA Workshop on Human Language Technology, </booktitle> <address> Princeton, NJ 1993. </address>
Reference: [Samuelsson 1994a] <author> Christer Samuelsson. </author> <title> Fast Natural-Language Parsing Using Explanation-Based Learning, </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, </institution> <address> Stockholm, Sweden 1994. </address>
Reference-contexts: It has proved reasonable to assume that the coverage is monotone on both sides of some maximum, which simplifies this task considerably. EXPERIMENTAL RESULTS A module realizing this scheme has been implemented and applied to the very setup used for the previous experiments with the hand-coded tree-cutting criteria, see <ref> [Samuelsson 1994a] </ref>. 2100 of the verified parse trees constituted the training set, while 230 of them were used for the test set. The table below summarizes the results for some grammars of different coverage extracted using: 1. Hand-coded tree-cutting criteria. 2.
Reference: [Samuelsson 1994b] <author> Christer Samuelsson. </author> <title> "Notes on LR Parser Design" to appear in Procs. </title> <booktitle> 15th International Conference on Computational Linguistics, </booktitle> <address> Kyoto, Japan 1994. </address>
Reference: [Samuelsson & Rayner 1991] <author> Christer Samuelsson and Manny Rayner. </author> <title> "Quantitative Evaluation of Explanation-Based Learning as an Optimization Tool for a Large-Scale Natural Language System", </title> <booktitle> in Procs. 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 609-615, </pages> <address> Sydney, Australia 1991. </address>
References-found: 8

