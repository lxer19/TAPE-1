URL: ftp://ftp.cse.cuhk.edu.hk/pub/techreports/95/tr-95-3.ps.gz
Refering-URL: ftp://ftp.cs.cuhk.hk/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email ho052@cs.cuhk.hk and lwchan@cs.cuhk.hk  
Title: Confluent Preorder Parsing CS-TR-95-03  
Author: HO, Kei Shiu Edward and CHAN, Lai Wan 
Keyword: Neural Networks, Connectionist Syntactic Parsing, RAAM, Holistic Transformation, Confluent Preorder Parser, Linearization of a Hierarchical Parse Tree, Parsing Erroneous Sentences, Syntactic Disambiguation  
Address: Shatin, N.T., Hong Kong  
Affiliation: Department of Computer Science The Chinese University of Hong Kong  
Abstract: In this paper, syntactic parsing is discussed in the context of connectionism. A new model - the Confluent Preorder Parser (CPP), is proposed which exemplifies the holistic parsing paradigm. Holistic parsing has the advantage that little assumption has to be made concerning the detailed parsing algorithm, which is often unknown or debatable, especially when human language understanding is concerned. In the CPP, syntactic parsing is achieved by transforming in a oneshot manner, from the connectionist representation of the sentence to the connectionist representation of the preorder traversal of its parse tree, instead of the parse tree itself. As revealed by the simulation experiments, generalization performance is excellent (as high as 90%). Besides, the CPP is also capable of parsing erroneous sentences and resolving syntactic ambiguities. A systematic study is conducted to explore the range of factors which can affect the effectiveness of it. This error-recovery capability is especially useful in natural language processing when incomplete or even ungrammatical sentences are to be dealt with. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Allen, J. </editor> <booktitle> (1995) Natural Language Understanding. </booktitle> <publisher> The Benjamin/Cummings Publishing Company, Inc. </publisher>
Reference-contexts: character strings, linked-lists, etc.) and the parsing problem is treated as a rational inference process in which the internal structure of the sentence is broken down and analyzed step-by-step (which corresponds to, say, reading the sentence from left-to-right), with the parse tree being built up incrementally at the same time <ref> (see Allen, 1995) </ref>. <p> More importantly, there is a strong linguistic assumption behind these language processing algorithms, for example, the Chomskyan generative grammar, the case grammar, etc. Almost all symbolic AI parsing systems <ref> (e.g. chart parser, see Allen, 1995) </ref> are inherently incremental. Disadvantages of the incremental approaches are evident. Their parsing algorithms may not be psychologically plausible in the sense that they are incapable of reflecting the real processing mechanism underlying actual language usage.
Reference: <author> Blank, D. S., Meeden, L. A. & Marshall, J. B. </author> <title> (1992) Exploring the Symbolic/Subsymbolic Continuum: A Case Study of RAAM. </title> <editor> In J. Dinsmore (Ed.), </editor> <title> The Symbolic and Connectionist Paradigms: Closing the Gap, </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates, pp.113-148. </publisher>
Reference-contexts: In this paper, syntactic parsing is discussed in the setting of connectionism, sentences and parse trees are represented subsymbolically as vectors of real-valued units (subsymbols) and syntactic parsing is achieved via holistic inference <ref> (Blank et. al., 1992) </ref> - a process in which the connectionist representation of a sentence is transformed to the connectionist representation of its parse tree directly without first decoding the subsymbolic representation back to the sentence it encodes nor building the parse tree incrementally (see Figure 1).
Reference: <author> Chalmers, D. J. </author> <title> (1992) Syntactic Transformations of Distributed Representations. </title> <editor> In N. Sharkey (Ed.), </editor> <title> Connectionist Natural Language Processing , Intellect Books, </title> <publisher> pp.46-55. </publisher>
Reference: <author> Chrisman, L. </author> <title> (1991) Learning recursive distributed representations for holistic computation. </title> <booktitle> Connection Science , 3, </booktitle> <pages> 345-366. </pages>
Reference-contexts: Secondly, to further enhance the performance of the CPP, the idea of confluent inference <ref> (Chrisman, 1991) </ref> is applied. The representation of a complete sentence and that of the preorder traversal of its corresponding parse tree will no longer be developed independently as in the BPN. Instead, they are forced to be approximately equal (or so called confluent) during training. <p> Confluent Preorder Parser (CPP) In view of this, a new model - the Confluent Preorder Parser (CPP), is proposed (see Figure 7). Basically, it is a dual-ported SRAAM <ref> (Chrisman, 1991) </ref>, with one single hidden layer (shaded in Figure 7) apparently shared between two SRAAM networks - the SRAAM Sentence Encoder and the SRAAM Preorder Traversal Encoder. <p> As a result, the error incurred by the Parsing Network can be avoided. To achieve this, identical representation will have to be developed for each sentence and the preorder traversal of its parse tree during training. Through confluent inference <ref> (Chrisman, 1991) </ref>, the two types of representations will no longer evolve independently as in the BPN (see Section 4.2). encoder sub-net decoder sub-net prefix &lt;1..i&gt; 1 ... n prefix &lt;1..i-1&gt; 1 ... m i-th terminal 1 ... n prefix &lt;1..i-1&gt; 1 ... m i-th terminal 1 ... n prefix &lt;1..i-1&gt; 1 <p> This idea conforms with the original spirit of confluent inference as advocated by Chrisman <ref> (Chrisman, 1991) </ref> - making close the representations for the problem space, which in our case are the SRAAM codes for the sentences, and the representations for the solution space, which are the SRAAM codes for the preorder traversals of the parse trees. Lets illustrate this idea with an example.
Reference: <author> Elman, J. L. </author> <title> (1991) Incremental learning, or The importance of Starting Small. </title> <type> CRL Technical Report 9101, </type> <institution> Center for Research in Language, University of California, </institution> <address> San Diego. </address>
Reference-contexts: Only after the CPP has managed to process this subset of shorter and simpler sentences (say when training is 100% successful) can learning proceed to include the more complex sentences also. This suggests the use of the incremental learning technique <ref> (Elman, 1991) </ref>. Finally, although we have emphasized that parsing by the CPP is implemented as a holistic transformation between SRAAM representations, it can also be analyzed as a step-by-step rational inference process by perceiving its operations as governed by a finite automata.
Reference: <author> Ho, K. S. Edward & Chan, L. W. </author> <title> (1994) Representing Sentence Structures in Neural Networks. </title> <booktitle> Proceedings of the International Conference in Neural Information Processing Systems , Vol. </booktitle> <pages> 3, </pages> <note> pp.1462-1467. 22 Jain, </note> <author> A. N. & Waibel, A. H. </author> <title> (1990) Incremental Parsing by Modular Recurrent Connectionist Networks. </title> <editor> In D. </editor> <publisher> S. </publisher>
Reference-contexts: So in Section 2, the basic principles of RAAM and SRAAM will be covered first. Then we will briefly describe in Section 3 the Backpropagation Parsing Network (BPN) - a holistic connectionist parser first proposed in <ref> (Ho & Chan, 1994) </ref>, and discuss its limitations. Rectifying these limitations, a new model is introduced in Section 4. This new model - the Confluent Preorder Parser (CPP), differs from the BPN in two major aspects. Firstly, the equivalence between a parse tree and its preorder traversal is exploited. <p> In subsequent discussions, we will attempt the parsing problem using RAAM as the representation tool. 3. Backpropagation Parsing Network (BPN) 3.1 Syntactic Parsing via SRAAM-to-RAAM Transformation Ho and Chan realize the holistic syntactic transformation in Figure 1 by using a fully-connected 3-layered feedforward neural net <ref> (Ho & Chan, 1994) </ref>. A set of sentences and their respective parse trees are first encoded 6 by training a SRAAM and a (structured) RAAM respectively and independently. <p> Generalization capability of the parser is then measured by evaluating its performance on the testing set. 5.1 Experiment 1 As a preliminary evaluation, the performance of the two syntactic parsers, BPN and CPP, are compared using the example quoted in <ref> (Ho & Chan, 1994) </ref>. There are totally 34 training examples and 5 testing cases. The simulation parameters and results have been summarized in Table III. <p> D N&gt; corresponding to the parse tree (ii) is closer (or more similar) to S than the sentence &lt;D N V D A N&gt; corresponding to the parse tree (i) : (i) ((D N) (V (D (A N)))) The error-recovery capability of the BPN has only been briefly investigated in <ref> (Ho & Chan, 1994) </ref>. Here, a systematic study will be performed to evaluate the effectiveness of the CPP in tackling with erroneous 15 sentences. <p> The rate and momentum in each case are fixed during training. This is also true for experiment 2. 5. The data for the BPN as quoted in Table III are adopted from the original paper <ref> (Ho & Chan, 1994) </ref>. 6. For S_WRONG_EL EMENT, it means that the p-th element is wrong and for S_MISSING_ELEMENT, it implies that the p-th element is deleted. Concerning S_SWAP_ELEMENT, it implies that the p-th element and the (p+1)-th element of the original sentence S are swapped where 1pLength (S)-1.
Reference: <editor> Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2 , San Mateo, </booktitle> <address> CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 364-371. </pages>
Reference: <author> Kwasny, S. C. & Faisal, K. A. </author> <title> (1992) Symbolic Parsing via Subsymbolic Rules. </title> <editor> In J. Dinsmore (Ed.), </editor> <title> The Symbolic and Connectionist Paradigms: Closing the Gap , Hillsdale, </title> <address> NJ: </address> <publisher> Lawrence Erlbaum Associates, pp.209-236. </publisher>
Reference-contexts: The linguistic assumptions which the incremental approaches are based on are originally postulated for explaining the competence of natural language only, but not to account for its performance. Consequently, they may not be suitable and efficient for implementation as a von Neumann processing system <ref> (see Kwasny & Faisal, 1992, for example) </ref>. In addition, many of the incremental systems treat the different dimensions or modalities of language separately and independently - syntax processing is done first, then semantics, and finally pragmatics. <p> Some of these hybrid approaches apply connectionist techniques to implement some or all of the components of the incremental parser. Examples include the PARSEC model (Jain & Waibel, 1990), the SPEC model 21 (Miikkulainen, 1993), and the connectionist deterministic parser proposed by Kwasny and Faisal <ref> (Kwasny & Faisal, 1992) </ref>. Simulation results have shown that the CPP can generalize well to novel sentences. Moreover, it exhibits some error-recovery capability when presented with noisy inputs and we have tailored an experiment to show how this capability can be further extended to achieve syntactic disambiguation.
Reference: <author> Kwasny, S. C. & Kalman, B. L. </author> <title> (1993) Tail-Recursive Distributed Representations and Simple Recurrent Networks . Technical Report WUCS-93-52, </title> <institution> Department of Computer Science, Washington University. </institution>
Reference-contexts: are not distinguished */ for each t i of the k children of t do preorder_traversal (t i ) - Kwasny and Kalman first applied this type of 1-to-1 mapping between a tree and its preorder traversal to facilitate the encoding of a set of trees using a SRAAM network <ref> (Kwasny & Kalman, 1993) </ref>. The major difference between their approach and that of the CPP is that in theirs, null pointers are explicitly represented for the leaves of the trees and those internal nodes with number of branches less than the valence of the tree (intuitively, they are not full).
Reference: <author> Maskara, A. & Noetzel, A. </author> <title> (1993) Forced Simple Recurrent Neural Networks and Grammatical Inference. </title> <booktitle> Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates, pp.420-425. </publisher>
Reference: <author> Miikkulainen, R. </author> <title> (1993) Subsymbolic Case-Role Analysis of Sentences with Embedded Clauses . Technical Report AI93-202, </title> <institution> Department of Computer Sciences, the University of Texas at Austin. </institution>
Reference-contexts: Between these two extremes, there is much room to accommodate different design alternatives. Some of these hybrid approaches apply connectionist techniques to implement some or all of the components of the incremental parser. Examples include the PARSEC model (Jain & Waibel, 1990), the SPEC model 21 <ref> (Miikkulainen, 1993) </ref>, and the connectionist deterministic parser proposed by Kwasny and Faisal (Kwasny & Faisal, 1992). Simulation results have shown that the CPP can generalize well to novel sentences.
Reference: <author> Pollack, J. B. </author> <title> (1990) Recursive Distributed Representations. </title> <booktitle> Artificial Intelligence , 46, </booktitle> <pages> 77-105. </pages>
Reference-contexts: Finally, a conclusion will be given in Section 8. 2. Recursive Auto-Associative Memory (RAAM) How can symbolic data structures be represented in a neural network? Pollack has offered an efficient scheme - the Recursive Auto-Associative Memory (hereafter referred as RAAM or structured RAAM) <ref> (Pollack, 1990) </ref>. Stated in its original form, a RAAM is basically a km-m-km feedforward encoder network (where k 2 can be considered as the degree of compression) capable of deriving distributed representations for recursive symbolic data structures, e.g. trees, lists, etc. <p> V D N P D N&gt; (which corresponds to the parse tree in Figure 3) can be perceived as being represented by the left-branching tree (((((((# D) N) V) D) N) P) D) N) shown in Figure 4 (where # is an extra terminal used to denote an empty sentence/sequence) <ref> (Pollack, 1990) </ref>. <p> To exploit these advantages, the original RAAM is adapted accordingly, giving rise to the Sequential RAAM (SRAAM) architecture in Figure 5 <ref> (Pollack, 1990) </ref>. As shown, the choice of the value of n can now be made independently of the value of m. As an illustration, to encode the sentence &lt;D N V D N P D N&gt;, the set of training patterns in Table II will be used. <p> Training sentences and testing sentences are generated using the context-free grammar below which is being adopted from <ref> (Pollack, 1990) </ref> : Sentence Noun Phrase Verb Phrase Prepositional Phrase Adjectival Phrase s fi np vp np fi D ap np fi np pp vp fi V pp ap fi A N Each syntactically well-formed sentence is generated using s as the root.
Reference: <author> Pollack, J. & Waltz, D. </author> <title> (1985) Massively parallel parsing: A strongly interactive model of natural language interpretation. </title> <booktitle> Cognitive Science , 9, </booktitle> <pages> 51-74. </pages>
Reference-contexts: In general, these different interpretations correspond to different possible senses (meanings) of W. For example, the word star in the sentence the astronomer married the star can be perceived in two ways : an astronomical body, or a popular, famous person (e.g. singer, actress, etc.) in the show business <ref> (Pollack & Waltz, 1985) </ref>. Occasionally, each of the possible senses may correspond to a different syntactic interpretation of the ambiguous word W. For example, the syntactic category of the word duck is different for its two occurrences in they eat a duck and they duck their heads.
Reference: <author> Rumelhart, D.E., Hinton, G. E., & Williams, R. J. </author> <title> (1986) Learning internal representations through error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, </editor> & <booktitle> The PDP Research Group (Eds.), Parallel distributed processing: Experiments in the microstructure of cognition , Vol. 1, </booktitle> <address> Cambridge: </address> <publisher> MIT Press, pp.318-362. </publisher>
Reference-contexts: Stated in its original form, a RAAM is basically a km-m-km feedforward encoder network (where k 2 can be considered as the degree of compression) capable of deriving distributed representations for recursive symbolic data structures, e.g. trees, lists, etc. Upon successful training, say, by using the backpropagation algorithm <ref> (Rumelhart, Hinton & Williams, 1986) </ref>, the hidden layer activation is the RAAM representation of the composite structure with its k components appearing in the input layer.
Reference: <author> Stolcke, A. & Wu, D. </author> <title> Tree matching with recursive distributed representations . Technical Report TR-92-025, </title> <institution> International Computer Science Institute, Berkeley. </institution>
References-found: 15

