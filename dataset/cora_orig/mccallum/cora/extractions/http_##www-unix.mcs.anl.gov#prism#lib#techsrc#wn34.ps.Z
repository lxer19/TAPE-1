URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn34.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: Strassen's Algorithm for Matrix Multiplication: Modeling, Analysis, and Implementation  
Author: Steven Huss-Lederman Elaine M. Jacobson J. R. Johnson Anna Tsao Thomas Turnbull 
Date: November 15, 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen. </author> <title> LAPACK: A portable linear algebra library for high-performance computers. </title> <booktitle> In Proceedings, Supercomputing `90, </booktitle> <pages> pages 2-11, </pages> <address> Los Alamitos, CA, 1990. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK Users' Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Its central role is evidenced by its inclusion as a key primitive operation in portable libraries, such as the Level 3 BLAS [10], where it can then be used as a building block in the implementation of many other routines, as done in LAPACK ([1], <ref> [2] </ref>). Thus, any speedup in matrix multiplication can improve the performance of a wide variety of numerical algorithms.
Reference: [3] <author> D. H. Bailey. </author> <title> Extra high speed matrix multiplication on the Cray-2. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 9 </volume> <pages> 603-607, </pages> <year> 1988. </year>
Reference-contexts: Both of these myths have been questioned in recent work. By stopping the Strassen recursions early and performing the bottom-level multiplications using the traditional algorithm, competitive performance is seen for matrix sizes in the hundreds in Bailey's FORTRAN implementation on the CRAY-2 <ref> [3] </ref>, Douglas, et al.'s [11] C implementation of the Winograd variant of Strassen's algorithm on various machines, and IBM's ESSL library routine [18]. <p> In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent [5] and Spei [27]. Recent work emphasizes either theory [14, 19] or practice <ref> [3, 4, 11] </ref>, but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation.
Reference: [4] <author> D. H. Bailey, K. Lee, and H. D. Simon. </author> <title> Using Strassen's Algorithm to Accelerate the Solution of Linear Systems. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(5) </volume> <pages> 357-371, </pages> <year> 1990. </year>
Reference-contexts: In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Therefore, at least theoretically, when considering rectangular matrices, the cutoff criterion (9) should be used instead of a simpler condition derived from the square cutoff, (m 12 or k 12 or n 12), which has been used by others <ref> [4, 11] </ref>. Cutoffs are addressed by almost all work on Strassen's algorithm. <p> Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent [5] and Spei [27]. Recent work emphasizes either theory [14, 19] or practice <ref> [3, 4, 11] </ref>, but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation. <p> Further reduction is possible if all of the stages are allowed to overlap. Using Strassen's original algorithm, Bailey, et al. <ref> [4] </ref> devised a straightforward scheme which reduces the total memory requirements to (mk + kn + mn)=3: Since Winograd's variant nests the additions/subtractions in stage (4) it is not clear that a similar reduction is possible. Below we present two computation schemes, both of which are used in our implementation. <p> Letting matrix dimensions be given by m fi k and k fi n, these are: m o or k o or n o; (16) mkn o (nk + mn + mk)=3: (17) Condition (16), used in <ref> [4, 11] </ref>, prevents Strassen's algorithm from being applied in certain situations where it would be beneficial. One situation where this can occur, as was illustrated in Section 3, is when one of the matrix dimensions is below the square cutoff and one of the other dimensions is large.
Reference: [5] <author> R. P. Brent. </author> <title> Algorithms for matrix multiplication. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1970. </year> <type> Technical Report CS 157, </type> <note> (Available from National Technical Information Service, # AD705509). </note>
Reference-contexts: In addition, the stability analyses of Brent <ref> [5] </ref> and then Higham [14, 15] show that Strassen's algorithm is stable enough to be studied further and considered seriously in the development of high-performance codes for matrix multiplication. A useful implementation of Strassen's algorithm must first efficiently handle matrices of arbitrary size. <p> In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent <ref> [5] </ref> and Spei [27]. Recent work emphasizes either theory [14, 19] or practice [3, 4, 11], but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation. <p> Temporary Allocation and Memory Usage Intermediate results required by any variant of Strassen's algorithm need to be stored in temporary locations. Early works addressing the issue of minimizing the amount of auxiliary memory needed includes <ref> [5, 6, 20, 27] </ref>. All recent implementations have tried to reduce memory usage. In this section we obtain lower bounds on the auxiliary memory required for the Winograd variant 25 of Strassen's algorithm and develop a computation schedule for the general case that achieves this lower bound. <p> One situation where this can occur, as was illustrated in Section 3, is when one of the matrix dimensions is below the square cutoff and one of the other dimensions is large. Condition (17), suggested by Brent <ref> [5] </ref> and then Higham [14], scales the theoretical condition (9) by o =12, so that it reduces to the square condition (15) when m = k = n.
Reference: [6] <author> J. Cohen and M. Roth. </author> <title> On the implementation of Strassen's fast multiplication algorithm. </title> <journal> Acta Informatica, </journal> <volume> 6 </volume> <pages> 341-355, </pages> <year> 1976. </year>
Reference-contexts: In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> More recently Higham [14] and Knight [19] explore minimization over full operation count formulas rather than solution of any local inequality such as (8). Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. Cohen et al. <ref> [6] </ref> address only the square case, and timing results are only given for the square case in both Brent [5] and Spei [27]. Recent work emphasizes either theory [14, 19] or practice [3, 4, 11], but not both. <p> Temporary Allocation and Memory Usage Intermediate results required by any variant of Strassen's algorithm need to be stored in temporary locations. Early works addressing the issue of minimizing the amount of auxiliary memory needed includes <ref> [5, 6, 20, 27] </ref>. All recent implementations have tried to reduce memory usage. In this section we obtain lower bounds on the auxiliary memory required for the Winograd variant 25 of Strassen's algorithm and develop a computation schedule for the general case that achieves this lower bound.
Reference: [7] <author> D. Coppersmith and S. Winograd. </author> <title> Matrix multiplication via arithmetic progressions. </title> <journal> Symbolic Computation, </journal> <volume> 9(3) </volume> <pages> 251-280, </pages> <year> 1990. </year> <title> Shorter version appears in Proceedings, </title> <booktitle> 19th Annual ACM Symposium on the Theory of Computing (STOC), 1987, </booktitle> <address> New York, NY, </address> <pages> pp. 1-6. </pages>
Reference: [8] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Letting T (m) be the arithmetic operation count required to compute C by Strassen's method with full recursion as described above, we have T (m) = 7T (m=2) + 18 ((m=2) 2 ): This is easily solved <ref> [8] </ref>, giving T (m) = fi (m lg (7) ) = fi (m 2:807 ). Strassen in fact observed that T (m) was less than 4:7m lg (7) , showing that the constant is not prohibitive.
Reference: [9] <institution> Cray Research, Inc. </institution> <note> Volume 3: (UNICOS) Math and Scientific Library Reference Manual (SR-2081), Release 5.0. </note> <institution> Mendota Heights, MN, </institution> <year> 1989. </year>
Reference-contexts: The total memory required in this scenerio is bounded by (mk + kn + 7mn)=3 If the temporaries used in stages (1) and (2) are reused in stages (3) and (4) this can be reduced to which is the memory requirement for CRAY's implementation of Strassen's algorithm <ref> [9] </ref>. Further reduction is possible if all of the stages are allowed to overlap.
Reference: [10] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Its central role is evidenced by its inclusion as a key primitive operation in portable libraries, such as the Level 3 BLAS <ref> [10] </ref>, where it can then be used as a building block in the implementation of many other routines, as done in LAPACK ([1], [2]). Thus, any speedup in matrix multiplication can improve the performance of a wide variety of numerical algorithms.
Reference: [11] <author> C. Douglas, M. Heroux, G. Slishman, and R. M. Smith. GEMMW: </author> <title> A portable level 3 BLAS Winograd variant of Strassen's matrix-matrix multiply algorithm. </title> <journal> Journal of Computational Physics, </journal> <volume> 110 </volume> <pages> 1-10, </pages> <year> 1994. </year>
Reference-contexts: Both of these myths have been questioned in recent work. By stopping the Strassen recursions early and performing the bottom-level multiplications using the traditional algorithm, competitive performance is seen for matrix sizes in the hundreds in Bailey's FORTRAN implementation on the CRAY-2 [3], Douglas, et al.'s <ref> [11] </ref> C implementation of the Winograd variant of Strassen's algorithm on various machines, and IBM's ESSL library routine [18]. <p> In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> Basing the modeling solely on full operation counts suggests that one technique for dealing with odd-sized matrices, known as dynamic peeling, could in fact be a viable alternative even though it had in the past been dismissed as undesirable <ref> [11] </ref>. Using this approach we develop a general, efficient, and portable implementation of Strassen's algorithm which is usable in any program in place of calls to DGEMM, the Level 3 BLAS 1 matrix multiplication routine. In our design, careful consideration has been given to all of the issues mentioned above. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Therefore, at least theoretically, when considering rectangular matrices, the cutoff criterion (9) should be used instead of a simpler condition derived from the square cutoff, (m 12 or k 12 or n 12), which has been used by others <ref> [4, 11] </ref>. Cutoffs are addressed by almost all work on Strassen's algorithm. <p> Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent [5] and Spei [27]. Recent work emphasizes either theory [14, 19] or practice <ref> [3, 4, 11] </ref>, but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation. <p> Typically the extra rows and columns contain zeros and are inserted so that A and B are in the top-left or bottom-right block (see <ref> [11] </ref> for an exception). The second approach to dealing with odd-sized matrices, called peeling, applies Strassen's algorithm to smaller matrices obtained by deleting rows and columns. <p> The first will demonstrate that an even lower memory requirement is attainable for the case where the input parameter has fi = 0 . This scheme is similar to the one used in the implementation of Winograd's variant reported in Douglas, et al., <ref> [11] </ref>, where additional storage requirements when fi = 0 are slightly more than (m max (k; n) + kn)=3. Our technique requires (m max (k; n) + kn)=3 in this case. <p> Similarly if k is odd the contribution from the last column of A and the last row of B must be included, and if n is odd the contribution from the last column 32 of B must be included. This approach has been dismissed by others <ref> [11] </ref> because of the inefficiency of some of these required fixup operations. Our implementation partially deals with this concern by combining the required operations and computing them with BLAS routines. <p> Letting matrix dimensions be given by m fi k and k fi n, these are: m o or k o or n o; (16) mkn o (nk + mn + mk)=3: (17) Condition (16), used in <ref> [4, 11] </ref>, prevents Strassen's algorithm from being applied in certain situations where it would be beneficial. One situation where this can occur, as was illustrated in Section 3, is when one of the matrix dimensions is below the square cutoff and one of the other dimensions is large. <p> We have observed that one can optimize the primitives and methods to typically get a several percent gain. To keep our code general however, we have not included these machine-specific techniques in our code. Finally, we compare to a public domain implementation from Douglas, et al. <ref> [11] </ref>, DGEMMW. We see in Figure 24 that, for general ff and fi on square matrices, there are matrix sizes where each code does better. The average ratio is 0.991, which shows that we are slightly better.
Reference: [12] <author> P. C. Fischer. </author> <title> Further schemes for combining matrix algorithms, </title> <booktitle> in Automata, Languages and Programming, volume 14 of Lecture Notes in Computer Science, </booktitle> <pages> pages 428-436. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year>
Reference-contexts: In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Other early theoretical work on the square case includes <ref> [12] </ref> and [13], where this constant is reduced further. Spei [27] seems to be the first to give an operation count theory of optimal cutoffs on even inputs for both square and rectangular matrices, in an equation similar to our (8).
Reference: [13] <author> P. C. Fischer and R. L. Probert. </author> <title> Efficient procedures for using matrix algorithms, </title> <booktitle> in Automata, Languages and Programming, volume 14 of Lecture Notes in Computer Science, </booktitle> <pages> pages 413-427. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1974. </year> <month> 66 </month>
Reference-contexts: In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Winograd's variant of Strassen's algorithm (credited to M. Paterson) uses the required 7 multiplications and this minimal number of 15 additions/subtractions <ref> [13] </ref>. <p> Other early theoretical work on the square case includes [12] and <ref> [13] </ref>, where this constant is reduced further. Spei [27] seems to be the first to give an operation count theory of optimal cutoffs on even inputs for both square and rectangular matrices, in an equation similar to our (8). <p> Strassen's original paper [28] used this flexibility to obtain an algorithm whose operation count is less than 4:7m lg (7) for all m fi m matrices. Using Winograd's variant of Strassen's algorithm and a more careful application of this strategy, Fischer and Probert <ref> [13] </ref> improved this bound to 4:537m lg (7) . Even with this improvement the computing time with static padding suffers from large jumps. The corresponding situation for static peeling is even worse.
Reference: [14] <author> N. J. Higham. </author> <title> Exploiting fast matrix multiplication within the level 3 BLAS. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16 </volume> <pages> 352-368, </pages> <year> 1990. </year>
Reference-contexts: In addition, the stability analyses of Brent [5] and then Higham <ref> [14, 15] </ref> show that Strassen's algorithm is stable enough to be studied further and considered seriously in the development of high-performance codes for matrix multiplication. A useful implementation of Strassen's algorithm must first efficiently handle matrices of arbitrary size. <p> In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> Thus, his development also expresses that strictly local analysis suffices to theoretically determine optimal cutoff for even inputs. More recently Higham <ref> [14] </ref> and Knight [19] explore minimization over full operation count formulas rather than solution of any local inequality such as (8). Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. <p> Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent [5] and Spei [27]. Recent work emphasizes either theory <ref> [14, 19] </ref> or practice [3, 4, 11], but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation. <p> One situation where this can occur, as was illustrated in Section 3, is when one of the matrix dimensions is below the square cutoff and one of the other dimensions is large. Condition (17), suggested by Brent [5] and then Higham <ref> [14] </ref>, scales the theoretical condition (9) by o =12, so that it reduces to the square condition (15) when m = k = n.
Reference: [15] <author> N. J. Higham. </author> <title> Stability of block algorithms with fast level 3 BLAS. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 </volume> <pages> 274-291, </pages> <year> 1992. </year>
Reference-contexts: In addition, the stability analyses of Brent [5] and then Higham <ref> [14, 15] </ref> show that Strassen's algorithm is stable enough to be studied further and considered seriously in the development of high-performance codes for matrix multiplication. A useful implementation of Strassen's algorithm must first efficiently handle matrices of arbitrary size.
Reference: [16] <author> J. E. Hopcroft and L. R. Kerr. </author> <title> Some techniques for proving certain simple programs optimal. </title> <booktitle> In Proceedings, Tenth Annual Symposium on Switching and Automata Theory, </booktitle> <pages> pages 36-45, </pages> <year> 1969. </year>
Reference-contexts: Thus, compared to the standard method, Strassen's technique provides an asymptotically faster algorithm for multiplying matrices. One can ask if it is possible to improve upon Strassen's construction to obtain further savings. Hopcroft and Kerr <ref> [16] </ref> showed that at least seven multiplications are required by any algorithm for multiplying 2fi2 matrices that does not depend on commutativity. Thus a recursive algorithm based on 2 fi 2 matrix multiplication cannot be asymptotically faster than Strassen's algorithm.
Reference: [17] <author> S. Huss-Lederman, A. Tsao, and T. Turnbull. </author> <title> A parallelizable eigensolver for real diagonal-izable matrices with real eigenvalues. </title> <journal> SIAM J. Sci. Computing, </journal> <volume> 18(2), </volume> <year> 1997. </year> <note> (to appear). </note>
Reference-contexts: Sample Application: Eigensolver In order to measure the efficiency of our implementation of Strassen's algorithm, we have incorporated it into a divide-and-conquer-based symmetric eigensolver, whose kernel operation is matrix multiplication. This eigensolver is based on the Invariant Subspace Decomposition Algorithm (ISDA) <ref> [17] </ref> and is part of the PRISM project. The ISDA uses matrix multiplication to apply a polynomial function to a matrix until a certain convergence criterion is met.
Reference: [18] <institution> IBM Engineering and Scientific Subroutine Library Guide and Reference, </institution> <year> 1992. </year> <title> Order No. </title> <publisher> SC23-0526. </publisher>
Reference-contexts: recursions early and performing the bottom-level multiplications using the traditional algorithm, competitive performance is seen for matrix sizes in the hundreds in Bailey's FORTRAN implementation on the CRAY-2 [3], Douglas, et al.'s [11] C implementation of the Winograd variant of Strassen's algorithm on various machines, and IBM's ESSL library routine <ref> [18] </ref>. In addition, the stability analyses of Brent [5] and then Higham [14, 15] show that Strassen's algorithm is stable enough to be studied further and considered seriously in the development of high-performance codes for matrix multiplication.
Reference: [19] <author> P. A. Knight. </author> <title> Fast rectangular matrix multiplication and QR decomposition. </title> <journal> Linear Algebra Appl., </journal> <volume> 221 </volume> <pages> 69-81, </pages> <year> 1995. </year>
Reference-contexts: In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> While not explicitly computed in his paper, his equations (4.3) and (4.6) could lead to our equation (1). He explores a slightly different operation count model than ours, but again could have obtained (4) from his basic formulas. In more recent work, Knight <ref> [19] </ref> obtains the equivalent of (4) for Strassen's original algorithm; this also could be obtained from Spei's results. Here, in (4), and the corresponding result for Strassen's original algorithm, we have obtained closed form expressions for operation count costs of both the original and the Winograd variant of Strassen's algorithm. <p> Thus, his development also expresses that strictly local analysis suffices to theoretically determine optimal cutoff for even inputs. More recently Higham [14] and Knight <ref> [19] </ref> explore minimization over full operation count formulas rather than solution of any local inequality such as (8). Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. <p> Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent [5] and Spei [27]. Recent work emphasizes either theory <ref> [14, 19] </ref> or practice [3, 4, 11], but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation.
Reference: [20] <author> A. Kreczmar. </author> <title> On memory requirements of Strassen algorithms. </title> <editor> In A. Mazurkiewicz, editor, </editor> <booktitle> Mathematical Foundations of Computer Science 1976, volume 45 of Lecture Notes in Computer Science, </booktitle> <pages> pages 404-407. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year>
Reference-contexts: Temporary Allocation and Memory Usage Intermediate results required by any variant of Strassen's algorithm need to be stored in temporary locations. Early works addressing the issue of minimizing the amount of auxiliary memory needed includes <ref> [5, 6, 20, 27] </ref>. All recent implementations have tried to reduce memory usage. In this section we obtain lower bounds on the auxiliary memory required for the Winograd variant 25 of Strassen's algorithm and develop a computation schedule for the general case that achieves this lower bound.
Reference: [21] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings, ASPLOS IV, </booktitle> <pages> pages 63-74, </pages> <address> New York, NY, 1991. </address> <publisher> ACM. </publisher>
Reference-contexts: These points are true anomalies and can be readily reproduced. Though we have not investigated the reason for their existence in detail, we feel they are due to cache effects as has been studied previously <ref> [21] </ref>.
Reference: [22] <author> V. Pan. </author> <title> Strassen algorithm is not optimal. trilinear technique of aggregating, uniting, and canceling for constructing fast algorithms for matrix multiplication. </title> <booktitle> In Proceedings, 19th Annual Symposium on the Foundations of Computer Science (FOCS), </booktitle> <pages> pages 166-176, </pages> <address> Ann Arbor, MI, </address> <year> 1978. </year>
Reference-contexts: Other methods have been developed which reduce the complexity even closer to the lower bound of (m 2 ) ([7], <ref> [22] </ref>), but so far none of these have lent themselves to practical implementations for typical-size matrices, say of order 10,000 or less. Strassen's algorithm has long suffered from the erroneous assumptions that it is not efficient for matrix sizes that are seen in practice, and that it is unstable.
Reference: [23] <author> N. Pippenger. Pebbling. </author> <type> Unpublished manuscript, </type> <month> May </month> <year> 1980. </year>
Reference-contexts: on, until the final results, subblocks of C, are produced at the bottom level. 26 B11 B12 B21 B22A11 A12 A21 A22 S4 U2 T4 T1 T3 27 A schedule for completing these computations and the temporary variables needed for in-termediate results can be established by playing the pebble game <ref> [23] </ref> on this directed graph. A pebble represents a temporary memory location. Placing a pebble on a vertex corresponds to performing the operation that computes the variable labeling that vertex and storing the result in the memory location represented by the pebble.
Reference: [24] <author> W. H. Press, S. A. Teukolsy, W. T. Vetterling, and B. P. Flannery. </author> <title> Numerical Recipes in FORTRAN: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: It is likely that this is one of the reasons that for many years Strassen's algorithm was thought to be of merely theoretical interest <ref> [24, 26] </ref>. The key to surmounting this difficulty is to not continue the Strassen recursions past the point where it is more efficient to use regular matrix multiplication.
Reference: [25] <author> R. L. Probert. </author> <title> On the additive complexity of matrix multiplication. </title> <journal> SIAM Journal of Computing, </journal> <volume> 5(6) </volume> <pages> 187-203, </pages> <year> 1976. </year>
Reference-contexts: Even though Strassen's algorithm leads to an asymptotic improvement there is substantial overhead due to the extra additions and subtractions. While reducing the number of additions does not lead to an asymptotic improvement it may provide a practical improvement. Probert <ref> [25] </ref> proved that the minimal number of additions and subtractions used by any bilinear algorithm for multiplying 2fi2 matrices with seven multiplications is 15.
Reference: [26] <author> R. Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: It is likely that this is one of the reasons that for many years Strassen's algorithm was thought to be of merely theoretical interest <ref> [24, 26] </ref>. The key to surmounting this difficulty is to not continue the Strassen recursions past the point where it is more efficient to use regular matrix multiplication.
Reference: [27] <author> J. Spei. </author> <title> Untersuchungen des zeitgewinns durch neue algorithmen zur matrix-multiplikation. </title> <journal> Computing, </journal> <volume> 17 </volume> <pages> 23-36, </pages> <year> 1976. </year>
Reference-contexts: While most consider only the square case, Spei <ref> [27] </ref> addresses the rectangular case and comes closest to our development here. While not explicitly computed in his paper, his equations (4.3) and (4.6) could lead to our equation (1). He explores a slightly different operation count model than ours, but again could have obtained (4) from his basic formulas. <p> Other early theoretical work on the square case includes [12] and [13], where this constant is reduced further. Spei <ref> [27] </ref> seems to be the first to give an operation count theory of optimal cutoffs on even inputs for both square and rectangular matrices, in an equation similar to our (8). Thus, his development also expresses that strictly local analysis suffices to theoretically determine optimal cutoff for even inputs. <p> Going beyond operation count theory, implementations supporting early work are generally research codes, with limited reporting of results. Cohen et al. [6] address only the square case, and timing results are only given for the square case in both Brent [5] and Spei <ref> [27] </ref>. Recent work emphasizes either theory [14, 19] or practice [3, 4, 11], but not both. In this work we extend the theory to other models and tie this to the development and examination of a practical implementation. <p> Temporary Allocation and Memory Usage Intermediate results required by any variant of Strassen's algorithm need to be stored in temporary locations. Early works addressing the issue of minimizing the amount of auxiliary memory needed includes <ref> [5, 6, 20, 27] </ref>. All recent implementations have tried to reduce memory usage. In this section we obtain lower bounds on the auxiliary memory required for the Winograd variant 25 of Strassen's algorithm and develop a computation schedule for the general case that achieves this lower bound.
Reference: [28] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math., </journal> <volume> 13 </volume> <pages> 354-356, </pages> <year> 1969. </year> <month> 67 </month>
Reference-contexts: Much less effort has been given towards the investigation of alternative algorithms whose asymptotic complexity is less than the fi (m 3 ) operations required by the conventional algorithm to multiply m fi m matrices. One such algorithm is Strassen's algorithm, introduced in 1969 <ref> [28] </ref>, which has complexity fi (m lg (7) ), where lg (x) denotes the base 2 logarithm of x and lg (7) 2:807. <p> In addition, establishing an appropriate cutoff criterion for stopping the recursions early is crucial to obtaining competitive performance on matrices of practical size. Finally, excessive amounts of memory should not be required to store temporary results. Previous work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>. We note that two periods of work on Strassen's algorithm are seen here; an early period from 1969 to 1976, and a recent one from 1988 to the present. <p> The algorithm can be applied in a straightforward fashion to square matrices with order a power of 2, but issues arise for all other matrices. Earlier work addressing these issues can be found in <ref> [3, 5, 6, 4, 11, 12, 13, 14, 19, 28] </ref>, but to our knowledge our development in this paper is the first to address the issues fully through both theoretical modeling and practical implementation. <p> The standard algorithm for multiplying two mfim matrices requires m 3 scalar multiplications and m 3 m 2 scalar additions for a total arithmetic operation count of 2m 3 m 2 . In Volker Strassen's now famous 1969 paper <ref> [28] </ref>, he introduced an algorithm, stated there for square matrices, which is based on a clever way of multiplying 2 fi 2 matrices using 7 multiplications and 18 additions/subtractions. His construction does not depend on the commutativity of the component multiplications and hence can be applied to block matrices. <p> Cutoffs are addressed by almost all work on Strassen's algorithm. Strassen, in <ref> [28] </ref>, used the flexibility of cutoff to reduce the cost of his algorithm for square matrices whose dimensions are not all powers of 2, thereby obtaining the constant of 4.7 on his upper bound for complexity in the square case. <p> In general, matrices of dimension mfim can be embedded in matrices of dimension 2 d q, where m 2 d q and c=2 &lt; q c, where c is set to the optimal cutoff for square matrices implied by (9). Strassen's original paper <ref> [28] </ref> used this flexibility to obtain an algorithm whose operation count is less than 4:7m lg (7) for all m fi m matrices. Using Winograd's variant of Strassen's algorithm and a more careful application of this strategy, Fischer and Probert [13] improved this bound to 4:537m lg (7) .
References-found: 28

