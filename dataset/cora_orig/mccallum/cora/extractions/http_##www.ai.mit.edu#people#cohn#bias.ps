URL: http://www.ai.mit.edu/people/cohn/bias.ps
Refering-URL: http://www.ai.mit.edu/people/cohn/papers.html
Root-URL: 
Email: cohn@harlequin.com  
Title: Minimizing Statistical Bias with Queries  
Author: David A. Cohn 
Address: One Cambridge Center Cambridge, MA 02142  
Affiliation: Adaptive Systems Group Harlequin, Inc.  
Date: 1997  
Note: Appears in M. Mozer, M. Jordan and T. Petsche, eds., Advances In Neural Infor- mation Processing Systems 9, MIT Press,  
Abstract: I describe a querying criterion that attempts to minimize the error of a learner by minimizing its estimated squared bias. I describe experiments with locally-weighted regression on two simple problems, and observe that this "bias-only" approach outperforms the more common "variance-only" exploration approach, even in the presence of noise.
Abstract-found: 1
Intro-found: 1
Reference: <author> Box, G., & Draper, N. </author> <year> (1987). </year> <title> Empirical model-building and response surfaces, </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Cleveland, W., Devlin, S., & Grosse, E. </author> <year> (1988). </year> <title> Regression by local fitting. </title> <journal> Journal of Econometrics, </journal> <volume> 37, </volume> <pages> 87-114. </pages>
Reference: <author> Cohn, D. </author> <title> (1996) Neural network exploration using optimal experiment design. </title> <booktitle> Neural Networks, </booktitle> <volume> 9(6) </volume> <pages> 1071-1083. </pages> <note> Earlier version available as AI Lab Memo 1491, </note> <institution> MIT. ftp://psyche.mit.edu/pub/cohn/AIM-1491.ps.Z. </institution>
Reference: <author> Cohn, D., Ghahramani, Z., & Jordan, M. </author> <year> (1996). </year> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research 4 </journal> <pages> 129-145. </pages>
Reference: <author> Connor, J. </author> <year> (1993). </year> <title> Bootstrap Methods in Neural Network Time Series Prediction. </title>
Reference: <editor> In J. Alspector et al., eds., </editor> <booktitle> Proc. of the Int. Workshop on Applications of Neural Networks to Telecommunications, </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, N.J. </address>
Reference: <author> Dietterich, T., & Kong, E. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. </title> <editor> In S. Prieditis and S. Russell, eds., </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning. </booktitle>
Reference: <author> Efron, B. </author> <title> (1983) Estimating the error rate of a prediction rule: some improvements on cross-validation. </title> <journal> J. Amer. Statist. Assoc. </journal> <volume> 78 </volume> <pages> 316-331. </pages>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993). </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference: <author> Fedorov, V. </author> <year> (1972). </year> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 1-58. </pages>
Reference-contexts: We can then write the expected error of the learner as Z E (^y (x; D) y (x)) jx P (x)dx; (1) where E [] denotes the expectation over P and over training sets D. The expectation inside the integral may be decomposed as follows <ref> (Geman et al., 1992) </ref>: E (^y (x; D) y (x)) jx = E (y (x) E [yjx]) i + (E D [y (x; D)] E [yjx]) +E D (^y (x; D) E D [^y (x; D)]) i where E D [] denotes the expectation over training sets.
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> Information-based objective functions for active data selection, </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 590-604. </pages>
Reference: <author> Paass, G., and Kindermann, J. </author> <year> (1994). </year> <title> Bayesian Query Construction for Neural Network Models. </title> <editor> In G. Tesauro et al., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Plutowski, M., & White, H. </author> <year> (1993). </year> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4, </volume> <pages> 305-318. </pages>
Reference: <author> Schaal, S. & Atkeson, C. </author> <year> (1994). </year> <title> Robot Juggling: An Implementation of Memory-based Learning. </title> <booktitle> Control Systems 14, </booktitle> <pages> 57-71. </pages>
References-found: 15

