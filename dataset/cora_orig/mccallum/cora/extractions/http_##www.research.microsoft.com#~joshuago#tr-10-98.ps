URL: http://www.research.microsoft.com/~joshuago/tr-10-98.ps
Refering-URL: http://www.research.microsoft.com/~joshuago/
Root-URL: http://www.research.microsoft.com
Title: An Empirical Study of Smoothing Techniques for Language Modeling  
Author: Stanley F. Chen and Joshua Goodman 
Address: Cambridge, Massachusetts  
Affiliation: Computer Science Group Harvard University  
Date: August 1998  
Pubnum: TR-10-98  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bahl, Lalit R., Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. </author> <year> 1989. </year> <title> A tree-based statistical language model for natural language speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 37 </volume> <pages> 1001-1008, </pages> <month> July. </month>
Reference: <author> Bahl, Lalit R., Frederick Jelinek, and Robert L. Mercer. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March. </month>
Reference: <author> Baum, L.E. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8. </pages> <note> 60 Bell, </note> <author> Timothy C., John G. Cleary, and Ian H. Witten. </author> <year> 1990. </year> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: maximum likelihood distribution, or we can take the smoothed 0th-order model to be the uniform distribution p unif (w i ) = 1 Given fixed p ML , it is possible to search efficiently for the w i1 in+1 that maximize the probability of some data using the Baum-Welch algorithm <ref> (Baum, 1972) </ref>.
Reference: <author> Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. </author> <year> 1990. </year> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <month> June. </month>
Reference: <author> Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, Jennifer C. Lai, </author> <title> and Robert L. </title>
Reference: <author> Mercer. </author> <year> 1992a. </year> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <month> March. </month>
Reference: <author> Brown, Peter F., Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1992b. </year> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <month> December. </month>
Reference: <author> Chen, Stanley F. </author> <year> 1996. </year> <title> Building Probabilistic Models for Natural Language. </title> <type> Ph.D. thesis, </type> <institution> Harvard University, </institution> <month> June. </month>
Reference-contexts: More specifically, Bahl et al. suggest partitioning the range of possible total count values and taking all w i1 in+1 associated with the same partition to be in the same bucket. In previous work <ref> (Chen, 1996) </ref>, we show that bucketing according to the average number of counts per nonzero element in a distribution P c (w i jw i :c (w i in+1 )&gt;0j yields better performance than using the value P in+1 ). 2.4 Katz Smoothing Katz smoothing (1987) extends the intuitions of the <p> While extensive empirical analysis is reported, they present only a single entropy result, comparing the above smoothing technique with another smoothing method introduced in their paper, extended deleted estimation. In our previous work <ref> (Chen, 1996) </ref>, we present further results, indicating that this smoothing works well for bigram language models. When extending this method to trigram models, there are two options for implementation. <p> Typically, we set parameter values to optimize the perplexity of held-out data; for more details, refer to Section 4.2. More details about our complete implementation, including techniques for limiting memory usage for large data sets, are given elsewhere <ref> (Chen, 1996) </ref>. One observation that we take advantage of is that for some algorithms, when optimizing the values of parameters on a held-out set, it is sufficient to only consider a small portion of the entire n-gram model. <p> does well on larger data sets is that on larger training sets, data that is deleted from the middle of a training set is sufficiently different from the remainder of the data that it is similar in nature to 14 These results differ slightly from those reported in previous work <ref> (Chen, 1996) </ref>; in that work we reported that held-out estimation is superior.
Reference: <author> Chen, Stanley F., Douglas Beeferman, and Ronald Rosenfeld. </author> <year> 1998. </year> <title> Evaluation metrics for language models. In DARPA Broadcast News Transcription and Understanding Workshop. </title>
Reference-contexts: It has been shown previously that there is some linear correlation between the word-error rate produced using a language model and the cross-entropy of the model on the corresponding text <ref> (Chen, Beeferman, and Rosenfeld, 1998) </ref>. However, the strength of the correlation depends on the nature of the models being compared. For these experiments, we used Broadcast News speech data (DARPA, 1998). <p> Cross-entropy does not always correlate well with word-error 58 rate, especially when the models compared are created using very different techniques <ref> (Chen, Beeferman, and Rosenfeld, 1998) </ref>. However, in our experiments we found that when the only difference between models is smoothing, the correlation between the two measures is quite strong.
Reference: <author> Chen, Stanley F. and Joshua T. Goodman. </author> <year> 1996. </year> <title> An empirical study of smoothing techniques for language modeling. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 310-318, </pages> <address> Santa Cruz, California, </address> <month> June. </month>
Reference-contexts: More specifically, Bahl et al. suggest partitioning the range of possible total count values and taking all w i1 in+1 associated with the same partition to be in the same bucket. In previous work <ref> (Chen, 1996) </ref>, we show that bucketing according to the average number of counts per nonzero element in a distribution P c (w i jw i :c (w i in+1 )&gt;0j yields better performance than using the value P in+1 ). 2.4 Katz Smoothing Katz smoothing (1987) extends the intuitions of the <p> While extensive empirical analysis is reported, they present only a single entropy result, comparing the above smoothing technique with another smoothing method introduced in their paper, extended deleted estimation. In our previous work <ref> (Chen, 1996) </ref>, we present further results, indicating that this smoothing works well for bigram language models. When extending this method to trigram models, there are two options for implementation. <p> Typically, we set parameter values to optimize the perplexity of held-out data; for more details, refer to Section 4.2. More details about our complete implementation, including techniques for limiting memory usage for large data sets, are given elsewhere <ref> (Chen, 1996) </ref>. One observation that we take advantage of is that for some algorithms, when optimizing the values of parameters on a held-out set, it is sufficient to only consider a small portion of the entire n-gram model. <p> does well on larger data sets is that on larger training sets, data that is deleted from the middle of a training set is sufficiently different from the remainder of the data that it is similar in nature to 14 These results differ slightly from those reported in previous work <ref> (Chen, 1996) </ref>; in that work we reported that held-out estimation is superior.
Reference: <author> Church, Kenneth. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages>
Reference-contexts: Hence, the choice of smoothing algorithm can make a significant difference in speech recognition performance. 6 Discussion Smoothing is a fundamental technique for statistical modeling, important not only for language modeling but for many other applications as well, e.g., prepositional phrase attachment (Collins and Brooks, 1995), part-of-speech tagging <ref> (Church, 1988) </ref>, and stochastic parsing (Magerman, 1994; Goodman, 1997). Whenever data sparsity is an issue, smoothing can help performance, and data sparsity is almost always an issue in statistical modeling.
Reference: <author> Church, Kenneth W. and William A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> Clarkson, P. and R. Rosenfeld. </author> <year> 1997. </year> <title> Statistical language modeling using the CMU-Cambridge toolkit. </title> <booktitle> In Proceedings of Eurospeech '97. </booktitle>
Reference: <author> Collins, Michael and James Brooks. </author> <year> 1995. </year> <title> Prepositional phrase attachment through a backed-off model. </title> <editor> In David Yarowsky and Kenneth Church, editors, </editor> <booktitle> Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <pages> pages 27-38, </pages> <address> Cambridge, MA, </address> <month> June. </month>
Reference-contexts: Hence, the choice of smoothing algorithm can make a significant difference in speech recognition performance. 6 Discussion Smoothing is a fundamental technique for statistical modeling, important not only for language modeling but for many other applications as well, e.g., prepositional phrase attachment <ref> (Collins and Brooks, 1995) </ref>, part-of-speech tagging (Church, 1988), and stochastic parsing (Magerman, 1994; Goodman, 1997). Whenever data sparsity is an issue, smoothing can help performance, and data sparsity is almost always an issue in statistical modeling.
Reference: <author> Cover, Thomas M. and Joy A. Thomas. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> John Wiley. DARPA. </publisher> <year> 1998. </year> <title> DARPA Broadcast News Transcription and Understanding Workshop. </title>
Reference: <author> Gale, William A. and Kenneth W. Church. </author> <year> 1990. </year> <title> Estimation procedures for language context: poor estimates are worse than none. </title> <booktitle> In COMPSTAT, Proceedings in Computational Statistics, 9th Symposium, </booktitle> <pages> pages 69-74, </pages> <address> Dubrovnik, Yugoslavia, </address> <month> September. </month>
Reference: <author> Gale, William A. and Kenneth W. Church. </author> <year> 1994. </year> <title> What's wrong with adding one? In N. </title> <editor> Oostdijk and P. de Haan, editors, </editor> <booktitle> Corpus-Based Research into Language. </booktitle> <address> Rodolpi, Amsterdam. </address>
Reference: <author> Gale, William A. and Geoffrey Sampson. </author> <year> 1995. </year> <title> Good-Turing frequency estimation without tears. </title> <journal> Journal of Quantitative Linguistics, </journal> <note> 2(3). To appear. 61 Godfrey, J.J., E.C. </note> <author> Holliman, and J. McDaniel. </author> <year> 1992. </year> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings of ICASSP-92, </booktitle> <volume> volume I, </volume> <pages> pages 517-520. </pages>
Reference: <author> Good, I.J. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3 and </volume> 4):237-264. 
Reference-contexts: Gale and Church (1990; 1994) have argued that this method generally performs poorly. 2.2 Good-Turing Estimate The Good-Turing estimate <ref> (Good, 1953) </ref> is central to many smoothing techniques.
Reference: <author> Goodman, Joshua. </author> <year> 1997. </year> <title> Probabilistic feature grammars. </title> <booktitle> In Proceedings of the International Workshop on Parsing Technologies 1997. </booktitle>
Reference: <author> Hull, Jonathon. </author> <year> 1992. </year> <title> Combining syntactic knowledge and visual text recognition: A hidden Markov model for part of speech tagging in a word recognition algorithm. </title> <booktitle> In AAAI Symposium: Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 77-83. </pages>
Reference: <author> Jeffreys, H. </author> <year> 1948. </year> <title> Theory of Probability. </title> <publisher> Clarendon Press, </publisher> <address> Oxford, </address> <note> second edition. </note>
Reference: <author> Jelinek, Frederick and Robert L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, The Netherlands: </address> <publisher> North-Holland, </publisher> <month> May. </month>
Reference: <author> Johnson, W.E. </author> <year> 1932. </year> <title> Probability: deductive and inductive problems. </title> <journal> Mind, </journal> <volume> 41 </volume> <pages> 421-423. </pages>
Reference: <author> Katz, Slava M. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March. </month>
Reference: <author> Kernighan, M.D., K.W. Church, and W.A. Gale. </author> <year> 1990. </year> <title> A spelling correction program based on a noisy channel model. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics, </booktitle> <pages> pages 205-210. </pages>
Reference: <author> Kneser, Reinhard and Hermann Ney. </author> <year> 1995. </year> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 181-184. </pages>
Reference: <author> Kucera, H. and W.N. Francis. </author> <year> 1967. </year> <title> Computational Analysis of Present-Day American English. </title>
Reference-contexts: thus search over in each later experiment); we also list the insignificant parameters and the value we set them to. 4.3 Data We used data from the Brown corpus, the North American Business news corpus, the Switchboard corpus, and the Broadcast News corpus. 12 The text of the Brown corpus <ref> (Kucera and Francis, 1967) </ref> was extracted from the tagged text in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz, 1993) and amounted to about one million words.
Reference: <institution> Brown University Press, Providence R.I. </institution>
Reference: <author> Lidstone, G.J. </author> <year> 1920. </year> <title> Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities. </title> <journal> Transactions of the Faculty of Actuaries, </journal> <volume> 8 </volume> <pages> 182-192. </pages>
Reference: <author> MacKay, David J. C. and Linda C. Peto. </author> <year> 1995. </year> <title> A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering, </booktitle> <volume> 1(3) </volume> <pages> 1-19. </pages>
Reference: <author> Magerman, David M. </author> <year> 1994. </year> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University, </institution> <month> February. </month>
Reference: <author> Marcus, M., B. Santorini, and M. Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of English: </title> <journal> the Penn Treeback. Computational Linguistics, </journal> <volume> 19(2). </volume>
Reference-contexts: the value we set them to. 4.3 Data We used data from the Brown corpus, the North American Business news corpus, the Switchboard corpus, and the Broadcast News corpus. 12 The text of the Brown corpus (Kucera and Francis, 1967) was extracted from the tagged text in the Penn Treebank <ref> (Marcus, Santorini, and Marcinkiewicz, 1993) </ref> and amounted to about one million words. The vocabulary we used with the Brown corpus experiments is the set of all 53,850 11 We found that the larger the value of each k n , the better the performance.
Reference: <author> Markov, A.A. </author> <year> 1913. </year> <title> An example of statistical investigation in the text of `Eugene Onyegin' illustrating coupling of tests in chains. </title> <booktitle> Proceedings of the Academy of Science, St. Petersburg, </booktitle> <volume> 7 </volume> <pages> 153-162. </pages>
Reference: <author> Nadas, Arthur. </author> <year> 1984. </year> <title> Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-32(4):859-861, </volume> <month> August. </month>
Reference: <author> Ney, Hermann, Ute Essen, and Reinhard Kneser. </author> <year> 1994. </year> <title> On structuring probabilistic dependences in stochastic language modeling. </title> <booktitle> Computer, Speech, and Language, </booktitle> <volume> 8 </volume> <pages> 1-38. </pages>
Reference-contexts: Text compression applications have requirements, such as the ability to build models very efficiently and incrementally, that we do not consider in this work. 2.6 Absolute Discounting Absolute discounting <ref> (Ney, Essen, and Kneser, 1994) </ref>, like Jelinek-Mercer smoothing, involves the interpolation of higher- and lower-order models. However, instead of multiplying the higher-order maximum-likelihood distribution by a factor w i1 in+1 , the higher-order distribution is created by subtracting a fixed discount D 1 from each nonzero count.
Reference: <author> Placeway, P., S. Chen, M. Eskenazi, U. Jain, V. Parikh, B. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore, M. Siegler, R. Stern, and E. Thayer. </author> <year> 1997. </year> <booktitle> The 1996 Hub-4 Sphinx-3 system. In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <month> February. </month>
Reference-contexts: The data consists of 130 million words of transcriptions of television and radio news shows. For these experiments, we used the 50,000 word vocabulary developed by the Sphinx speech recognition group <ref> (Placeway et al., 1997) </ref> for the evaluation. The average sentence length is about 15 words. For each experiment, we selected three segments of held-out data along with the segment of training data. <p> However, the strength of the correlation depends on the nature of the models being compared. For these experiments, we used Broadcast News speech data (DARPA, 1998). We generated narrow-beam lattices with the Sphinx-III speech recognition system <ref> (Placeway et al., 1997) </ref> using a Katz-smoothed trigram model trained on 130 million words of Broadcast News text; trigrams occurring only once in the training data were excluded from the model.
Reference: <author> Press, W.H., B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. </author> <year> 1988. </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Thus, it is important to optimize parameter values to meaningfully compare smoothing techniques, and this optimization should be specific to the given training set. In each of our experiments, optimal values for the parameters of each method were searched for using Powell's search algorithm <ref> (Press et al., 1988) </ref>. Parameters were chosen to optimize the cross-entropy of a held-out set associated with each training set. More specifically, as described in Section 4.3 there are three held-out sets associated with each training set, and parameter optimization was performed using the first of the three.
Reference: <author> Ries, Klaus. </author> <year> 1997. </year> <type> personal communication. </type>
Reference-contexts: Kneser (1994) have developed an estimate for the optimal D for absolute discounting and Kneser-Ney smoothing as a function of training data counts (as given in equation (20)), it is possible to create analogous equations to estimate the optimal values for D 1 , D 2 , and D 3 <ref> (Ries, 1997) </ref>.
Reference: <author> Rogina, Ivica and Alex Waibel. </author> <year> 1995. </year> <title> The Janus speech recognizer. </title> <booktitle> In ARPA SLT Workshop. </booktitle>
Reference-contexts: The Switchboard data is three million words of telephone conversation transcriptions (Godfrey, Holliman, and McDaniel, 1992). The version of the data we used was processed by the Janus speech recognition group <ref> (Rogina and Waibel, 1995) </ref>, and in our experiments we used their 9,800 word vocabulary. The average sentence length is about 16 words. The Broadcast News text was taken from the language modeling data distributed for the 1996 DARPA Hub 4 continuous speech recognition evaluation (Rudnicky, 1996).
Reference: <author> Rosenfeld, Ronald. </author> <year> 1995. </year> <title> The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation. </title> <booktitle> In Proceedings of the Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 47-50, </pages> <address> Austin, Texas, </address> <month> January. </month>
Reference: <author> Rudnicky, A.I. </author> <year> 1996. </year> <title> Hub 4: Business Broadcast News. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 8-11. </pages>
Reference-contexts: The average sentence length is about 16 words. The Broadcast News text was taken from the language modeling data distributed for the 1996 DARPA Hub 4 continuous speech recognition evaluation <ref> (Rudnicky, 1996) </ref>. The data consists of 130 million words of transcriptions of television and radio news shows. For these experiments, we used the 50,000 word vocabulary developed by the Sphinx speech recognition group (Placeway et al., 1997) for the evaluation. The average sentence length is about 15 words.
Reference: <author> Seymore, K., S. Chen, M. Eskenazi, and R. Rosenfeld. </author> <year> 1997. </year> <title> Language and pronunciation modeling in the CMU 1996 Hub 4 evaluation. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> Washington, D.C., </address> <month> February. </month>
Reference: <author> Srihari, Rohini and Charlotte Baltus. </author> <year> 1992. </year> <title> Combining statistical and syntactic methods in recognizing handwritten sentences. </title> <booktitle> In AAAI Symposium: Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 121-127. </pages>
Reference: <author> Stern, Richard M. </author> <year> 1996. </year> <title> Specification of the 1995 ARPA hub 3 evaluation: Unlimited vocabulary NAB news baseline. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 5-7. </pages>
Reference-contexts: The average sentence length is about 21 words. The North American Business news text was taken from the language modeling data distributed for the 1995 ARPA continuous speech recognition evaluation <ref> (Stern, 1996) </ref>. The data included 110 million words of Associated Press (AP) text, 98 million words of Wall Street Journal (WSJ) text, and 35 million words of San Jose Mercury News (SJM) text. For these experiments, we used the 20,000 word vocabulary supplied for the evaluation.
Reference: <author> Weng, Fuliang, Andreas Stolcke, and Ananth Sankar. </author> <year> 1997. </year> <title> Hub4 language modeling using domain interpolation and data clustering. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <address> Washington, D.C., </address> <month> February. </month>
Reference: <author> Witten, Ian H. and Timothy C. Bell. </author> <year> 1991. </year> <title> The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <month> July. 63 </month>
Reference-contexts: Extensive comparisons between Witten-Bell smoothing and other smoothing techniques for text compression are presented in (Bell, Cleary, and Witten, 1990) and <ref> (Witten and Bell, 1991) </ref>; however, comparisons with smoothing techniques used in language modeling are not reported.
References-found: 47

