URL: http://www.ai.mit.edu/people/cgdemarc/postscript/icgi94.ps
Refering-URL: http://www.ai.mit.edu/people/cgdemarc/cgdemarc.html
Root-URL: 
Email: cgdemarc@ai.mit.edu  
Title: The Acquisition of a Lexicon from Paired Phoneme Sequences and Semantic Representations  
Author: Carl de Marcken 
Note: Childes database of mother-child interactions are presented.  
Address: 545 Technology Square Cambridge, MA 02139, USA  
Affiliation: MIT Artificial Intelligence Laboratory  
Abstract: We present an algorithm that acquires words (pairings of phonological forms and semantic representations) from larger utterances of unsegmented phoneme sequences and semantic representations. The algorithm maintains from utterance to utterance only a single coherent dictionary, and learns in the presence of homonymy, synonymy, and noise. Test results over a corpus of utterances generated from the 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bachenko and E. Fitzpatrick. </author> <year> 1990. </year> <title> "A Computational Grammar of Discourse-Neutral Prosodic Phrasing in English," </title> <journal> Computational linguistics 16, </journal> <volume> 3, </volume> <pages> 155-170. </pages>
Reference-contexts: But, as data in <ref> [1] </ref> suggests, short utterances often are pauseless, and many sentences children hear are quite short (5.6 words on average in our test database, with little embedding). 2 The particular phonemes used in the paper are the output of a public domain text-to-phoneme converter, which is frequently inaccurate (witness "OK" ! /k/).
Reference: [2] <author> P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer, and P. Roosin. </author> <year> 1990. </year> <title> "A Statistical Approach to Machine Translation," </title> <journal> Computational Linguistics 16, </journal> <volume> 2, </volume> <pages> 79-85. 11 </pages>
Reference-contexts: Cartwright and Brent's is a batch description-length formulation [10] that uses the size of the dictionary as a prior. Their algorithms perform with minimal adequacy, unable to distinguish correlations due to the dictionary from correlations due to syntax and semantics. Brown et al <ref> [2] </ref> present a statistical machine translation algorithm that makes use of estimated correspondances between words in English and French. From an aligned multilingual database they estimate for every English word the distribution of French words it might translate to, including the number of words it will translate to.
Reference: [3] <author> S. Carey. </author> <year> 1978. </year> <title> The Child as a Word Learner. </title> <editor> In M. Halle, J. Bresnan and G. Miller (eds), </editor> <title> Linguistic Theory and Psychological Reality. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Mas-sachussets. </address>
Reference-contexts: The total error is quite low (it would be zero if the gradient descent search had produced correct activations of exactly 0 or 1), 8 This heuristic prevents the algorithm from learning words that only occur once or twice, a problem given Carey's <ref> [3] </ref> evidence that children can (and need to) acquire some words from a very small number of exposures. One solution would be to speed the cooling process as the majority of the dictionary becomes stable.
Reference: [4] <author> T. Cartwright and M. Brent. </author> <year> 1994. </year> <title> Segmenting Speech Without a Lexicon: Evidence for a Bootstrapping Model of Lexical Acquisition, </title> <booktitle> in Proceedings of the 16th Conference of the Cognitive Science Society, </booktitle> <year> 1994. </year>
Reference-contexts: But some methods for increasing the ambiguity of semantic interpretation for an utterance are discussed in the next section and in section 6. 2 Previous Work Olivier [9] and Cartwright and Brent <ref> [4] </ref> present simple algorithms that segment text and phoneme sequences by learning from statistical irregularities. In particular, they place phoneme sequences in a dictionary if those phonemes occur consecutively more often than they would if phonemes were selected by a memoryless random process with 2 identical aggregate distribution. <p> The right 12 were selected randomly from the 1100 entries. extent by Cartwright and Brent <ref> [4] </ref> and Church [5]. Alternatively, as a more immediate workaround, we could provide the test corpus with an explicit semantic clue, such as the following association: /i8/ f PROGRESSIVE g.
Reference: [5] <author> K. </author> <title> Church 1987. "Phonological parsing and lexical retrieval," </title> <journal> Cognition, </journal> <volume> 25, </volume> <pages> 53-69. </pages>
Reference-contexts: The right 12 were selected randomly from the 1100 entries. extent by Cartwright and Brent [4] and Church <ref> [5] </ref>. Alternatively, as a more immediate workaround, we could provide the test corpus with an explicit semantic clue, such as the following association: /i8/ f PROGRESSIVE g.
Reference: [6] <author> A. P. Dempster and N. M. Laird and D. B. Rubin. </author> <year> 1977. </year> <title> "Maximum Likelihood from Incomplete Data via the EM Algorithm" Journal of the Royal Statistical Society, </title> <booktitle> B 39, </booktitle> <pages> 1-38. </pages>
Reference-contexts: word sequence from the probability of the word sequence given the meaning, which can be very effective if the proper definitions of some of the words in the utterance are known. 3 When the algorithm is presented with an utterance, it performs a local variation of the expectation-maximization (EM) procedure <ref> [6] </ref>: it attempts to parse the utterance using the words in its dictionary, resulting in values for hidden word-activation variables. By parse we mean that the algorithm attempts to find a set of words that collectively cover all the phonemes and sememes of the utterance, without overlap or mismatched elements.
Reference: [7] <author> R. Jackendoff. </author> <year> 1990. </year> <title> Semantic Structures. </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, Massachussets. </address>
Reference-contexts: Our model of our problem is based on his work. His work differs from ours in two principle ways: first, Siskind learns more complex semantic representations (Jackendoff-style semantic formulae <ref> [7] </ref> rather than simple symbol sets 3 ), in an environment where his algorithms are presented with many ambiguous semantic representations (one of which is correct); and second, Siskind's work assumes pre-segmented tokens as input. So, his algorithm receives ok here is the big ball rather than our /khirzbigb=l/.
Reference: [8] <author> B. MacWhinney and C. Snow. </author> <year> 1985. </year> <title> "The Child Language Data Exchange System," </title> <journal> Journal of Child Language 12, </journal> <pages> 271-296. </pages>
Reference: [9] <author> D. Olivier. </author> <year> 1968. </year> <title> Stochastic Grammars and Language Acquisition Mechanisms. </title> <type> PhD thesis, </type> <institution> Harvard University, Cambridge, Massachusetts. </institution>
Reference-contexts: But some methods for increasing the ambiguity of semantic interpretation for an utterance are discussed in the next section and in section 6. 2 Previous Work Olivier <ref> [9] </ref> and Cartwright and Brent [4] present simple algorithms that segment text and phoneme sequences by learning from statistical irregularities.
Reference: [10] <author> J. Rissanen. </author> <year> 1978. </year> <title> "Modeling by shortest data description," </title> <type> Automatica 14, </type> <pages> 456-471. </pages>
Reference-contexts: Olivier's algorithm is on-line, extremely efficient, and incorporates no priors. Cartwright and Brent's is a batch description-length formulation <ref> [10] </ref> that uses the size of the dictionary as a prior. Their algorithms perform with minimal adequacy, unable to distinguish correlations due to the dictionary from correlations due to syntax and semantics.
Reference: [11] <author> J. Siskind. </author> <year> 1992. </year> <title> Naive Physics, Event Perception, Lexical Semantics, and Language Acquisition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Cambridge, Massachusetts. </institution>
Reference-contexts: Brown et al of course assume segmentation of the source and target language, and make multiple passes over the data. They also make use of generally linear correspondances between utterances in the two languages. Siskind has presented a series of algorithms <ref> [11] </ref>, [12], [13] that learn word-meaning associations when presented with paired sequences of tokens and semantic representations. Our model of our problem is based on his work.
Reference: [12] <author> J. Siskind. </author> <year> 1993. </year> <title> Lexical Acquisition as Constraint Satisfaction. </title> <type> Technical Report IRCS-93-41, </type> <institution> University of Pennsylvania Institute for Research in Cognitive Science. </institution>
Reference-contexts: Brown et al of course assume segmentation of the source and target language, and make multiple passes over the data. They also make use of generally linear correspondances between utterances in the two languages. Siskind has presented a series of algorithms [11], <ref> [12] </ref>, [13] that learn word-meaning associations when presented with paired sequences of tokens and semantic representations. Our model of our problem is based on his work.
Reference: [13] <author> J. Siskind. </author> <year> 1994. </year> <title> Lexical Acquisition in the Presence of Noise and Homonymy. </title> <booktitle> In Proceedings of the 12th National Conferance on Artificial Intelligence (AAAI-94). </booktitle>
Reference-contexts: Brown et al of course assume segmentation of the source and target language, and make multiple passes over the data. They also make use of generally linear correspondances between utterances in the two languages. Siskind has presented a series of algorithms [11], [12], <ref> [13] </ref> that learn word-meaning associations when presented with paired sequences of tokens and semantic representations. Our model of our problem is based on his work. <p> It creates new words, using a variety of methods that have proven successful but are not in any way the only ones that might work. Some of the methods used in this process are similar to those used by Siskind <ref> [13] </ref>. We can divide the methods into two parts: fixing words that participated in the parse and creating wholly new words. Fixes include deleting and adding phonemes and sememes from a definition. Words participate in fixes with some probability.
Reference: [14] <author> P. Suppes. </author> <year> 1973. </year> <title> "The semantics of children's language," </title> <journal> American Psychologist. </journal> <volume> 12 </volume>
References-found: 14

