URL: http://www.ics.uci.edu/~kibler/mlc95.ps
Refering-URL: http://www.ics.uci.edu/~kibler/
Root-URL: 
Email: fpdatta, kiblerg@ics.uci.edu  
Title: Learning Prototypical Concept Descriptions  
Author: Piew Datta and Dennis Kibler 
Address: Irvine, CA 92717  
Affiliation: Department of Information and Computer Science University of California  
Abstract: We describe a new representation for learning concepts that differs from the traditional de cision tree and rule approach. This represen tation, called prototypical concept descrip tions, can represent several prototypes for a concept. We also describe PL, our algorithm for learning these prototypes, and demon strate that prototypical concept descriptions can, in some situations, classify more accu rately than standard Machine Learning algo rithms. More importantly, we show that they yield more stable descriptions when applied in noisy and dynamic situations.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler, D. and Albert M. </author> <year> (1991). </year> <title> Instance based learning algorithms. </title> <journal> Machine learning, </journal> <volume> volume 6, </volume> <pages> pp 37-66. </pages> <address> Boston, MA: </address> <publisher> Kluwer Publishers. </publisher>
Reference-contexts: By choosing only those features that discrim inate classes as part of the learned theory and pre ferring simpler concepts, these algorithms follow the MDL bias. Bayesian classifiers (Duda & Hart 1973), neural networks (Rosenblatt 1958; Rumelhart, Hinton, & Williams 1986), and instance-based methods such as IB1 <ref> (Aha, Kibler, & Albert 1991) </ref> and PEBLS (Cost & Salzberg 1993), on the other hand, use all of the relevant features to represent concept theories, even those that are redundant, but informative.
Reference: <author> Aydin, A. and Pearce, J. </author> <year> (1994). </year> <title> Prototype effects in categorization by pigeons. </title> <journal> Journal of Experimen tal Psychology: Animal Behavior Processes, </journal> <volume> volume 20 (n3) pp.264-277. </volume>

Reference: <author> Duda, R., and Hart P. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: By choosing only those features that discrim inate classes as part of the learned theory and pre ferring simpler concepts, these algorithms follow the MDL bias. Bayesian classifiers <ref> (Duda & Hart 1973) </ref>, neural networks (Rosenblatt 1958; Rumelhart, Hinton, & Williams 1986), and instance-based methods such as IB1 (Aha, Kibler, & Albert 1991) and PEBLS (Cost & Salzberg 1993), on the other hand, use all of the relevant features to represent concept theories, even those that are redundant, but informative.
Reference: <author> Huttenlocher, J., Hedges, L. and Duncan, S. </author> <year> (1991). </year> <title> Categories and particulars: prototype effects in esti mating spatial location. </title> <journal> Psychological Review, </journal> <volume> volume 98 (n3) pp.352-376. </volume>
Reference: <author> John, G., Kohavi, R., and Pfleger, K. </author> <year> (1994). </year> <title> Irrel evant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Medin, D., Altom, M. and Murphy, T. </author> <year> (1984). </year> <title> Given versus induced category representations: Use of proto type and exemplar information in classification. Jour nal of Experimental Psychology: Learning, Memory, </title> & <journal> Cognition, </journal> <volume> volume 10 (n3) pp. </volume> <pages> 333-352. </pages>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1994). </year> <title> UCI repository of ma chine learning databases [machine readable data repos itory]. </title> <type> Tech. Rep., </type> <institution> University of California, Irvine. </institution>
Reference-contexts: In section 2, we de scribe our algorithm to learn prototypical concept de scriptions and we use the Exclusive Or domain as an illustration. Section 3 details the experiments using data from the UCI data repository <ref> (Murphy & Aha 1994) </ref>. Lastly, Section 4 summarizes our contributions and discusses future work. 2 LEARNING PROTOTYPICAL DESCRIPTIONS This section defines PL's method for creating charac teristic prototypes. <p> These domains were chosen because they provide a mixture of artificial and natu ral data and they only contain nominal features. The domains and their characteristics are listed in Table 4. All of these databases are located in the UCI Machine Learning Data repository <ref> (Murphy & Aha 1994) </ref>. The averaged accuracy results are listed in Table 4. An asterisk beside the an algorithm's accuracy indicates that the algorithm and PL are significantly different to a .98% confidence level using a two-sided t-test.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Ma chine Learning, volume 1. </booktitle> <address> Boston, MA: </address> <publisher> Kluwer Pub lishers. </publisher>
Reference-contexts: 1 INTRODUCTION Many machine learning algorithms use the minimum description length (MDL) (Rissanen 1976) bias to choose between two or more theories that classify equally well. Examples include most decision tree methods such as ID3 <ref> (Quinlan 1986) </ref> and other rule based algorithms, such as CN2 (Clark & Boswell 1991). By choosing only those features that discrim inate classes as part of the learned theory and pre ferring simpler concepts, these algorithms follow the MDL bias.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: A similar process will occur for learn ing prototypes for the negative examples. PL will also learn two prototypes for the negative examples. 3 EVALUATION To evaluate the usefulness of prototypical concept de scriptions, we will test PL on classification accuracy and compare it against ID3 and C4.5 <ref> (Quinlan 1993) </ref>. To better understand the circumstances under which PL performs well, we describe experiments on two very different artificial domains.
Reference: <author> Richardson, K. and Bhavnani, K. </author> <year> (1984). </year> <title> How a con cept is formed: Prototype or contingency abstraction? British Journal of Psychology, </title> <journal> volume 75 (n4) pp. </journal> <volume> 507 519. </volume>
Reference-contexts: On the other hand, some researchers contend that other mental processes occur which give the illusion of prototypical models <ref> (Richardson & Bhavnani 1984) </ref>. Despite the current conflict in psychology, we contend that for machine learning, prototypes provide some benefits for learn ing. Learning prototypes to represent concepts has advan tages over learning discriminating descriptions.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data de scription. </title> <journal> Automatica, </journal> <volume> volume 14 pp. </volume> <pages> 465-471. </pages>
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The perception: a probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> volume 65 (n6) pp.386 408. </volume>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, </author> <note> R. </note>
Reference: <author> J. </author> <year> (1986). </year> <title> Leaning internal representations by error propagation. </title> <booktitle> Parallel Distributed Processing Explo rations in the Microstructure of Cognition chapter 8 pp. </booktitle> <pages> 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing al gorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In addition, prototypical concept descriptions have the advantage of being more comprehensible than other methods representing redundant features. Al though some research has focused on finding proto typical examples by using typicality measures (Zhang 1992), Monte Carlo sampling and random mutation hill climbing <ref> (Skalak 1994) </ref>, our research centers on creating a prototype not necessarily any member of the training examples. We follow the standard psy chological terminology where a prototype is an ideal example.
Reference: <author> Wogulis, J. and Pazzani, M. </author> <year> (1993). </year> <title> A Methodology for Evaluating Theory Revision Systems: Results with Audrey II. </title> <booktitle> Proceeding of International Joint Confer ence for Artificial Intelligence, </booktitle> <pages> pp. 1128-1134. </pages>
Reference-contexts: Syntactic stability measures the amount of change among the theories generated by different training sets. The syntactic stability is measured by noting the av erage number of modifications (edit distance) needed to change one theory into another <ref> (Wogulis & Pazzani 1993) </ref>. The syntactic stability is computed by using the theories generated from each fold of the cross vali dation experiment. We compare each of the theories to each other and report the average number of modifica tions necessary to change one theory syntacticly into another.
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In em Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In addition, prototypical concept descriptions have the advantage of being more comprehensible than other methods representing redundant features. Al though some research has focused on finding proto typical examples by using typicality measures <ref> (Zhang 1992) </ref>, Monte Carlo sampling and random mutation hill climbing (Skalak 1994), our research centers on creating a prototype not necessarily any member of the training examples. We follow the standard psy chological terminology where a prototype is an ideal example.
References-found: 17

