URL: file://ftp.cc.gatech.edu/pub/groups/architecture/Beehive/jpdc95.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Architecture/Beehive/index.html
Root-URL: 
Title: Cache-based Synchronization in Shared Memory Multiprocessors  
Author: Umakishore Ramachandran Joonwon Lee 
Note: To appear in The Journal of Parallel and Distributed Computing. This work is supported in part by an NSF PYI Award MIP-9058430.  
Address: Atlanta, Georgia 30332 USA  KAIST, Seoul Campus Seoul, Korea 130-650  
Affiliation: College of Computing Georgia Institute of Technology  Dept. of Information and Communication  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under MACH. </title> <booktitle> In ACM Sigmetrics Conference on Measurement & Modeling of Computer Systems, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The ratio of shared accesses under the control of a lock to all shared accesses varies from 50% to 70% in some applications <ref> [1, 34] </ref> and below 10% in other applications [18]. In the results reported in this section this ratio (lock-ratio in Table 2) is set at 50%.
Reference: [2] <author> Anant Agarwal. </author> <title> Limits on interconnection network performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: While the use of direct networks in shared memory multiprocessors can reduce the average latency for memory accesses if the applications are properly structured to exploit communication locality <ref> [2] </ref>, the inefficiency due to synchronization effects still persist. Usually, access to shared data is acquired via synchronization methods such as locks, semaphores, and barriers. Thus there is additional delay in accessing the synchronization variables and then acquiring the actual data.
Reference: [3] <author> Anant Agarwal, R. L. Sites, and M. Horowitz. ATUM: </author> <title> A new technique for capturing address traces using microcode. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 186-195, </pages> <year> 1986. </year>
Reference-contexts: Trace-driven, execution-driven, and probabilistic are the three methods of generating the workload for the simulation. Trace-driven simulation has some validity concerns as observed by several researchers <ref> [3, 19, 20, 7] </ref> due to the distortions that may be introduced due to the instrumentation code that is inserted for collecting the traces. <p> Further the traces obtained from one machine may not represent true interactions in another machine. Lastly, the traces usually do not capture OS related activities (such as interrupts, context switches, and I/O) unless hardware instrumentation is available <ref> [3] </ref>. Execution-driven simulation is closest to capturing the true interactions between the program and the underlying architecture being simulated and in this sense may be the best technique if simulation is to be used. But in general this technique is more difficult since it requires collecting/developing several real programs.
Reference: [4] <author> T. E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Recently, there have been several software proposals for reducing the the network traffic in the implementation of locks and barriers in cache coherent shared memory multiprocessors. Anderson <ref> [4] </ref> and Graunke and Thakkar [24] have independently proposed algorithms that use software queues for ordering the lock requests from processors. These algorithms eliminate the bursty network traffic that is possible in a straightforward implementation of mutual exclusion locks under high lock contention. <p> In the next two subsections, we present implementations of CBL for snoopy caches and for directory-based caches. 1 The number of machine instructions needed to implement Anderson's ticket lock <ref> [4] </ref> is roughly 50 for lock acquisition, and about 15 for lock release on the Sequent Symmetry and the KSR-1 [42]. 5 ' $ P1:read-lock P2:read-lock ~ P3:write-lock ' $ P4:read-lock P5:read-lock ~ P6:write-lock - state next-node count 3.2 Snoopy Cache Implementation In this subsection, we present an overview of a <p> In this scheme, each processor doubles its mean delay after each failure to acquire the lock. There needs to be a maximum bound on the mean delay to eliminate an unnecessary long delay when there remain only a few processors in a lock competition <ref> [4] </ref>. upper three lines are for executing the work-queue workload model. The WBI protocol is slightly better then the backoff scheme when there is little lock contention (for less than 16 processors).
Reference: [5] <author> J. Archibald and J. Baer. </author> <title> Cache coherence protocols: evaluation using a multiprocessor model. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 278-298, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: It is interesting to note that even with private caches with fairly high hit ratios, the scalability of bus-based machines is limited to at most tens of processors <ref> [5, 18] </ref>. This limitation may be directly attributed to the network traffic that is generated as a result of the cache coherence and synchronization activities leading to a saturation of the bus. <p> Vernon et al. [50] report on the performance of snoopy cache protocols using the MVA (mean value analysis) technique. The results 14 from this analysis are surprisingly close to the simulation results of Archibald and Baer <ref> [5] </ref> while the required computation time is significantly smaller than simulation. <p> The sync model is a probabilistic model similar to the one developed by Archibald and Baer <ref> [5] </ref>. The main enhancements of the sync model, over that of Archibald and Baer, are the separation of synchronization accesses from normal read/write accesses and the use of overall completion time as the evaluation metric. <p> Many parameters are fixed not only because their effects are well studied in <ref> [5] </ref> but also our primary concern was to measure the effect of various synchronization mechanisms on protocol performance. The values of the parameters used in the simulation are summarized in Table 2. <p> Real traces [18, 51] show that the read ratio ranges from 0.6 to 0.9 depending on the application of traced programs. In <ref> [5] </ref> simulation was done varying the read ratio from 0.7 to 0.85. In our simulation the read ratio is set to 0.85. Shared accesses are secured by lock primitives with a probability lock-ratio. <p> The bus as well as the links in the MIN are assumed to be 32 bits wide. 4.4 Simulation Results Though processor utilization is often measured in other studies <ref> [5] </ref>, we measured the completion time (in processor cycles) for executing a given workload because synchronization sometimes makes processors busy even when they are not doing any useful computation. The CSIM simulation tool generates various statistics including average queue length and server utilization. <p> The CSIM simulation tool generates various statistics including average queue length and server utilization. We present those numbers only when it is necessary to justify the completion times. Since the performance studies of different cache configurations are available in <ref> [5, 52, 51] </ref>, we restrict our study to the understanding of the effects of different synchronization scenarios. 4.4.1 Snoopy Cache Protocols Snooping cache schemes evaluated are the Berkeley protocol [29] as an invalidation scheme, the Dragon [36] protocol as a write update scheme, Bitar and Despain's method [8], and our lock-based
Reference: [6] <author> B. Beck, B. Kasten, and S. Thakkar. </author> <title> VLSI assist for a multiprocessor. </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 10-20, </pages> <year> 1987. </year>
Reference-contexts: While the programmer is responsible for increasing the concurrency and consequently reducing the waiting times at synchronization points, the computer architect's task is to reduce the synchronization overhead. Hardware support for synchronization comes in various forms such as a special-purpose coprocessor (e.g. Sequent SLIC <ref> [6] </ref>), a combining network [23], and a special bus for interprocessor communication [53]. As we observed earlier, Bitar and Despain [8] were the first to propose exclusive locks (with no queuing) as a cache primitive.
Reference: [7] <author> P. Bitar. </author> <title> A critique of trace-driven simulation for shared-memory multiprocessors. ISCA'89 Workshop: Cache and Interconnect Architectures in Multiprocessors, </title> <year> 1989. </year>
Reference-contexts: Trace-driven, execution-driven, and probabilistic are the three methods of generating the workload for the simulation. Trace-driven simulation has some validity concerns as observed by several researchers <ref> [3, 19, 20, 7] </ref> due to the distortions that may be introduced due to the instrumentation code that is inserted for collecting the traces.
Reference: [8] <author> P. Bitar and A. M. Despain. </author> <title> Multiprocessor cache synchronization : Issues, innovations, evolution. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 424-433, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: In write-invalidate schemes [21, 29, 39], a write to a cached line results in invalidating copies of this line present in other caches. In write update schemes [36, 49], a write to a cached line results in updating copies of this line present in other caches. Bitar and Despain <ref> [8] </ref> propose a scheme in which the cache accepts lock and unlock commands from the processor in addition to the traditional read and write requests. This lock scheme combines lock-based synchronization with the line transfer, thus performing locking in zero time. <p> Hardware support for synchronization comes in various forms such as a special-purpose coprocessor (e.g. Sequent SLIC [6]), a combining network [23], and a special bus for interprocessor communication [53]. As we observed earlier, Bitar and Despain <ref> [8] </ref> were the first to propose exclusive locks (with no queuing) as a cache primitive. Goodman et al. [22] and ourselves [41] were the first to independently propose queue-based locks in hardware as a cache primitive. <p> are available in [5, 52, 51], we restrict our study to the understanding of the effects of different synchronization scenarios. 4.4.1 Snoopy Cache Protocols Snooping cache schemes evaluated are the Berkeley protocol [29] as an invalidation scheme, the Dragon [36] protocol as a write update scheme, Bitar and Despain's method <ref> [8] </ref>, and our lock-based protocol. The problem size is 1024 tasks, and the results are the average of 5 runs. size 100 (fine to medium grain) without the barrier synchronization. BD denotes the Bitar and Despain's scheme, and CBL is our cache-based locking scheme.
Reference: [9] <author> P. </author> <title> Borrill. </title> <journal> IEEE 896.1: the Futurebus. Electron, </journal> <volume> 33(10) </volume> <pages> 628-631, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: 21 Output Lines 23 23 26 Product Terms 47 78 768 Total Size 35 fi 47 37 fi 78 47 fi 768 Table 1: Size of PLAs for cache controllers For snoopy caches, we have shown [31] that CBL can be implemented easily on state-of-the-art bus systems such as Futurebus <ref> [9] </ref>. For directory-based caches, our protocol assumes no specific capability in the interconnection network. In fact, the implementation can be optimized if some specific interconnect is assumed [17].
Reference: [10] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-based cache coherence in large-scale multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 49-58, </pages> <month> June </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: For this reason, coherence protocols based on a directory associated with the shared memory for maintaining the state information are usually preferred for large-scale shared memory multiprocessors <ref> [33, 10] </ref>. Invalidation-based protocols have been found to be preferable for large-scale multiprocessors relying entirely on hardware to provide cache coherence. There are several ways to organize the state information in the central directory [10, 48]. <p> Invalidation-based protocols have been found to be preferable for large-scale multiprocessors relying entirely on hardware to provide cache coherence. There are several ways to organize the state information in the central directory <ref> [10, 48] </ref>. One possibility is to associate a bit map with each memory block, where each bit represents the presence or absence of that memory block in a particular processor. A variant of this basic scheme is to use a limited set of pointers instead of a bit map.
Reference: [11] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A scalable cache coherence scheme. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <year> 1991. </year>
Reference-contexts: A variant of this basic scheme is to use a limited set of pointers instead of a bit map. When the level of active sharing for a memory block exceeds the set of available pointers, the system may either emulate full map in software <ref> [11] </ref>, or invalidate one of the cached copies to make room for the new request, or resort to broadcast. Another variant is to make the pointer allocation dynamic [35, 46].
Reference: [12] <author> H. Cheong and A. V. Veidenbaum. </author> <title> Stale data detection and coherence enforcement using flow analysis. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 138-145, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: The reader is referred to [48] for a more complete survey of the pros and cons of different directory based schemes. Software Enforcement of Coherence. In software-controlled cache schemes, cache lines are invalidated or updated according to the compiler's static analysis of parallel programs. There have been several proposals <ref> [12, 16] </ref> to determine the cacheability of shared data through compile-time flow analysis for specific language constructs such as FORTRAN doall. Min and Baer [38] suggest a time-stamp based cache protocol that allows run-time determination of whether or not a cached 3 data item is valid.
Reference: [13] <author> David R. Cheriton, Hendrik A. Goosen, and Patrick D. Boyle. </author> <title> Paradigm: A highly scalable shared-memory multicomputer architecture. </title> <journal> IEEE Computer, </journal> <volume> 24(2) </volume> <pages> 33-48, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: This lock scheme combines lock-based synchronization with the line transfer, thus performing locking in zero time. It does not distinguish between read lock and write lock requests, and also does not order the processors waiting for the lock. KSR-1 [43] a commercial shared memory multiprocessor, as well as Paradigm <ref> [13] </ref> - a research prototype from Stanford, provide primitives very similar to the one proposed by Bitar and Despain. In [22], Goodman et al. suggest a synchronization primitive, QOSB, which can be used by the programmer for constructing high level synchronization operations.
Reference: [14] <author> D. W. Clark. </author> <title> Cache performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <year> 1983. </year>
Reference-contexts: While hardware measurement provides an accurate estimate of multiprocessor performance for the measured benchmarks, the feasible tests are limited to a small number of case studies. Therefore, hardware measurement is more often used with other techniques such as analytical modeling and simulation <ref> [14, 28, 26] </ref>. 4.1 Analytical Models Analytical modeling provides initial estimates of performance but has limitations owing to the assumptions that are often made to make the model tractable. Vernon et al. [50] report on the performance of snoopy cache protocols using the MVA (mean value analysis) technique.
Reference: [15] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: But in general this technique is more difficult since it requires collecting/developing several real programs. Also it involves simulating all aspects (such as instruction-sets) of the underlying architecture and not just the aspects one is interested in studying. Augmentation <ref> [15] </ref> is an attractive approach to supplement execution-driven simulation so that only interesting aspects of the target machine are actually simulated and others are executed on the host machine.
Reference: [16] <author> R. Cytron, S. Marlovsky, and K. P. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 229-238, </pages> <year> 1988. </year>
Reference-contexts: The reader is referred to [48] for a more complete survey of the pros and cons of different directory based schemes. Software Enforcement of Coherence. In software-controlled cache schemes, cache lines are invalidated or updated according to the compiler's static analysis of parallel programs. There have been several proposals <ref> [12, 16] </ref> to determine the cacheability of shared data through compile-time flow analysis for specific language constructs such as FORTRAN doall. Min and Baer [38] suggest a time-stamp based cache protocol that allows run-time determination of whether or not a cached 3 data item is valid.
Reference: [17] <author> Martin H. Davis, Jr. </author> <title> Using Optical Waveguides in General Purpose Parallel Computers. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: For directory-based caches, our protocol assumes no specific capability in the interconnection network. In fact, the implementation can be optimized if some specific interconnect is assumed <ref> [17] </ref>. Though the CBL scheme enables efficient handling of synchronization, the hardware complexity of the scheme would be significant, especially for general interconnection networks that incur non-deterministic network delays. We have presented detailed algorithms for the CBL scheme (see Appendix B) and their correctness have been tested by extensive simulations.
Reference: [18] <author> S. J. Eggers and R. H. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 373-382, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: It is interesting to note that even with private caches with fairly high hit ratios, the scalability of bus-based machines is limited to at most tens of processors <ref> [5, 18] </ref>. This limitation may be directly attributed to the network traffic that is generated as a result of the cache coherence and synchronization activities leading to a saturation of the bus. <p> Examples for this class of applications include FFT (fast fourier transformation), linear equation solver, and numerous CAD programs <ref> [18] </ref>. There are other classes of applications that do not need such a barrier. For example, the traveling salesman problem can be implemented as a tree search problem. Executing a node in the search tree may generate several children nodes. <p> With respect to our two-level workload model (see Figure 7), we set the ratio of shared accesses to total accesses to be 0.03 during the "execute" phase of a task, and 0.5 during insertions of items into and/or deletions of items from from the queue. Real traces <ref> [18, 51] </ref> show that the read ratio ranges from 0.6 to 0.9 depending on the application of traced programs. In [5] simulation was done varying the read ratio from 0.7 to 0.85. In our simulation the read ratio is set to 0.85. <p> The ratio of shared accesses under the control of a lock to all shared accesses varies from 50% to 70% in some applications [1, 34] and below 10% in other applications <ref> [18] </ref>. In the results reported in this section this ratio (lock-ratio in Table 2) is set at 50%. Varying the lock-ratio produces only a small difference in performance because the degree of sharing is quite low (0.03, see Table 2) during task execution.
Reference: [19] <author> Richard M. Fujimoto and William C. Hare. </author> <title> On the accuracy of multiprocessor tracing techniques. </title> <type> Technical Report GIT-CC-92/53, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Trace-driven, execution-driven, and probabilistic are the three methods of generating the workload for the simulation. Trace-driven simulation has some validity concerns as observed by several researchers <ref> [3, 19, 20, 7] </ref> due to the distortions that may be introduced due to the instrumentation code that is inserted for collecting the traces.
Reference: [20] <author> Stephen R. Goldschmidt and John L. Hennessy. </author> <title> The accuracy of trace-driven simulations of multiprocessors. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Trace-driven, execution-driven, and probabilistic are the three methods of generating the workload for the simulation. Trace-driven simulation has some validity concerns as observed by several researchers <ref> [3, 19, 20, 7] </ref> due to the distortions that may be introduced due to the instrumentation code that is inserted for collecting the traces. <p> Since the execution path of a parallel program depends on the ordering of the events in the program, both these distortions have the potential of completely changing the execution path unless timing dependencies are carefully eliminated from the traces <ref> [20] </ref>. Program startup effects may also distort the results especially if the trace length is not long. Further the traces obtained from one machine may not represent true interactions in another machine.
Reference: [21] <author> J. R. Goodman. </author> <title> Using cache memory to reduce processor-memory traffic. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Protocol. There are two broad classifications of protocol families based on the semantics for dealing with processor writes: write-invalidate and write update. In write-invalidate schemes <ref> [21, 29, 39] </ref>, a write to a cached line results in invalidating copies of this line present in other caches. In write update schemes [36, 49], a write to a cached line results in updating copies of this line present in other caches.
Reference: [22] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: KSR-1 [43] a commercial shared memory multiprocessor, as well as Paradigm [13] - a research prototype from Stanford, provide primitives very similar to the one proposed by Bitar and Despain. In <ref> [22] </ref>, Goodman et al. suggest a synchronization primitive, QOSB, which can be used by the programmer for constructing high level synchronization operations. Similar to the lock primitive proposed by Bitar and Despain, QOSB is an implicit exclusive lock that can be associated with any main memory block. <p> Sequent SLIC [6]), a combining network [23], and a special bus for interprocessor communication [53]. As we observed earlier, Bitar and Despain [8] were the first to propose exclusive locks (with no queuing) as a cache primitive. Goodman et al. <ref> [22] </ref> and ourselves [41] were the first to independently propose queue-based locks in hardware as a cache primitive. While our primitives support both exclusive (write) and shared (read-shared) locks, Goodman et al. support only exclusive locks. <p> Like the queueing mechanism used in QOSB <ref> [22] </ref>, a distributed FCFS queue is constructed using participating cache lines. The processor and the cache together form a node of the shared memory multiprocessor. Each node is assigned a unique id which we refer to as node-id.
Reference: [23] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU ultracomputer designing an MIMD shared memory parallel computer. </title> <journal> IEEE Trans. Computers, </journal> <volume> C35(2):175-189, </volume> <year> 1983. </year> <month> 24 </month>
Reference-contexts: While the programmer is responsible for increasing the concurrency and consequently reducing the waiting times at synchronization points, the computer architect's task is to reduce the synchronization overhead. Hardware support for synchronization comes in various forms such as a special-purpose coprocessor (e.g. Sequent SLIC [6]), a combining network <ref> [23] </ref>, and a special bus for interprocessor communication [53]. As we observed earlier, Bitar and Despain [8] were the first to propose exclusive locks (with no queuing) as a cache primitive.
Reference: [24] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization algorithms for shared-memory multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23 </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Recently, there have been several software proposals for reducing the the network traffic in the implementation of locks and barriers in cache coherent shared memory multiprocessors. Anderson [4] and Graunke and Thakkar <ref> [24] </ref> have independently proposed algorithms that use software queues for ordering the lock requests from processors. These algorithms eliminate the bursty network traffic that is possible in a straightforward implementation of mutual exclusion locks under high lock contention.
Reference: [25] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Anderson [4] and Graunke and Thakkar [24] have independently proposed algorithms that use software queues for ordering the lock requests from processors. These algorithms eliminate the bursty network traffic that is possible in a straightforward implementation of mutual exclusion locks under high lock contention. Hensgen et al. <ref> [25] </ref> and Mellor-Crummey and Scott [37] have proposed efficient algorithms that bring down the amount of network traffic generated during barrier synchronization operations from O (n 2 ) (using the straightforward counter based algorithm) to O (nlogn).
Reference: [26] <author> Mark D. Hill and Alan J. Smith. </author> <title> Experimental evaluation of on-chip microprocessor cache memories. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 158-166, </pages> <year> 1984. </year>
Reference-contexts: While hardware measurement provides an accurate estimate of multiprocessor performance for the measured benchmarks, the feasible tests are limited to a small number of case studies. Therefore, hardware measurement is more often used with other techniques such as analytical modeling and simulation <ref> [14, 28, 26] </ref>. 4.1 Analytical Models Analytical modeling provides initial estimates of performance but has limitations owing to the assumptions that are often made to make the model tractable. Vernon et al. [50] report on the performance of snoopy cache protocols using the MVA (mean value analysis) technique.
Reference: [27] <institution> IEEE P1596 - SCI Coherence Protocols. Scalable Coherent Interface, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Yet another variant is to maintain the state information in the form of a linked list with the memory block as the head and the set of participating cache lines forming the active chain of processors sharing that memory block <ref> [27, 32] </ref>. There are space/time trade-offs, and scalability implications depending on the particular choice of organizing the state information. The reader is referred to [48] for a more complete survey of the pros and cons of different directory based schemes. Software Enforcement of Coherence.
Reference: [28] <author> Rajeev Jog, Phillip L. Vitale, and James R. Callister. </author> <title> Performance evaluation of a commercial cache-coherent share memory multiprocessor. </title> <booktitle> In ACM Sigmetrics Conference on Measurement & Modeling of Computer Systems, </booktitle> <pages> pages 173-182, </pages> <year> 1990. </year>
Reference-contexts: While hardware measurement provides an accurate estimate of multiprocessor performance for the measured benchmarks, the feasible tests are limited to a small number of case studies. Therefore, hardware measurement is more often used with other techniques such as analytical modeling and simulation <ref> [14, 28, 26] </ref>. 4.1 Analytical Models Analytical modeling provides initial estimates of performance but has limitations owing to the assumptions that are often made to make the model tractable. Vernon et al. [50] report on the performance of snoopy cache protocols using the MVA (mean value analysis) technique.
Reference: [29] <author> R. H. Katz, S. J. Eggers, D. A. Wood, C. L. Perkins, and R. G. Sheldon. </author> <title> Implementing a cache consistency protocol. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Protocol. There are two broad classifications of protocol families based on the semantics for dealing with processor writes: write-invalidate and write update. In write-invalidate schemes <ref> [21, 29, 39] </ref>, a write to a cached line results in invalidating copies of this line present in other caches. In write update schemes [36, 49], a write to a cached line results in updating copies of this line present in other caches. <p> Since the performance studies of different cache configurations are available in [5, 52, 51], we restrict our study to the understanding of the effects of different synchronization scenarios. 4.4.1 Snoopy Cache Protocols Snooping cache schemes evaluated are the Berkeley protocol <ref> [29] </ref> as an invalidation scheme, the Dragon [36] protocol as a write update scheme, Bitar and Despain's method [8], and our lock-based protocol. The problem size is 1024 tasks, and the results are the average of 5 runs. size 100 (fine to medium grain) without the barrier synchronization.
Reference: [30] <author> J. Lee and U. Ramachandran. </author> <title> Synchronization with multiprocessor caches. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 27-37, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Further our primitives allow combining data transfer with the granting of the lock (similar to Bitar and Despain) thus providing synchronization for free. Subsequently, similar locking primitives have made their way into commercial machines (such as KSR-1 [43]), as well as university prototypes [33]. It is easy to show <ref> [30, 32] </ref> that a straightforward implementation of mutual exclusion locks (using test&test-and-set) and barriers (decrementing a shared counter on arrival and spinning locally on a shared flag for completion) could result in O (n 2 ) network traffic (where n is the number of processors) irrespective of whether invalidation or write-update
Reference: [31] <author> Joonwon Lee. </author> <title> Architectural Features for Scalable Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: To implement a queue, each directory entry of a cache line has a next-node field containing the node-id of the next waiting cache if any. The protocol described in this subsection assumes a single process per processor. Snoopy CBL that allows multiple processes per processor is presented in <ref> [31] </ref>. In the discussion to follow, we use lock and line interchangeably since lock acquisition is merged with the cache line transfer. Each cache line has a directory entry with the fields as shown in example P1 in Figure 1). <p> In <ref> [31] </ref>, we quantified the hardware complexity of the state machine for CBL (through VLSI implementation with a PLA) to be roughly an order of magnitude greater than those of either the Berkeley or Dragon protocols (see Table 1). <p> it offers (see Section 4). 13 Berkeley Dragon Snooping CBL Input Lines 12 14 21 Output Lines 23 23 26 Product Terms 47 78 768 Total Size 35 fi 47 37 fi 78 47 fi 768 Table 1: Size of PLAs for cache controllers For snoopy caches, we have shown <ref> [31] </ref> that CBL can be implemented easily on state-of-the-art bus systems such as Futurebus [9]. For directory-based caches, our protocol assumes no specific capability in the interconnection network. In fact, the implementation can be optimized if some specific interconnect is assumed [17].
Reference: [32] <author> Joonwon Lee and Umakishore Ramachandran. </author> <title> Architectural primitives for a scalable shared memory multiprocessor. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1991. </year>
Reference-contexts: Yet another variant is to maintain the state information in the form of a linked list with the memory block as the head and the set of participating cache lines forming the active chain of processors sharing that memory block <ref> [27, 32] </ref>. There are space/time trade-offs, and scalability implications depending on the particular choice of organizing the state information. The reader is referred to [48] for a more complete survey of the pros and cons of different directory based schemes. Software Enforcement of Coherence. <p> Further our primitives allow combining data transfer with the granting of the lock (similar to Bitar and Despain) thus providing synchronization for free. Subsequently, similar locking primitives have made their way into commercial machines (such as KSR-1 [43]), as well as university prototypes [33]. It is easy to show <ref> [30, 32] </ref> that a straightforward implementation of mutual exclusion locks (using test&test-and-set) and barriers (decrementing a shared counter on arrival and spinning locally on a shared flag for completion) could result in O (n 2 ) network traffic (where n is the number of processors) irrespective of whether invalidation or write-update <p> To simplify the discussion of the locking primitives we do not discuss simple read/write operations in the rest of the paper. However, it should be clear that such locking primitives can be incorporated as part of the processor-cache interface coexisting with the read/write cache primitives (see <ref> [32] </ref> for an example). In Section 4, where we discuss performance benefits of our cache-based locks, we assume a Berkeley-style invalidation-based protocol for private accesses and shared accesses made without an explicit lock association. 3.1 Rationale for Read-lock Read-sharing is common in most shared memory parallel applications.
Reference: [33] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For this reason, coherence protocols based on a directory associated with the shared memory for maintaining the state information are usually preferred for large-scale shared memory multiprocessors <ref> [33, 10] </ref>. Invalidation-based protocols have been found to be preferable for large-scale multiprocessors relying entirely on hardware to provide cache coherence. There are several ways to organize the state information in the central directory [10, 48]. <p> Further our primitives allow combining data transfer with the granting of the lock (similar to Bitar and Despain) thus providing synchronization for free. Subsequently, similar locking primitives have made their way into commercial machines (such as KSR-1 [43]), as well as university prototypes <ref> [33] </ref>.
Reference: [34] <author> Z. Li and W. Abu-sufah. </author> <title> A technique for reducing synchronization overhead in large scale multiprocessor. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 284-291, </pages> <year> 1985. </year>
Reference-contexts: The ratio of shared accesses under the control of a lock to all shared accesses varies from 50% to 70% in some applications <ref> [1, 34] </ref> and below 10% in other applications [18]. In the results reported in this section this ratio (lock-ratio in Table 2) is set at 50%.
Reference: [35] <author> David J. Lilja and Pen-Chung Yew. </author> <title> Combining hardware and software cache coherence strategies. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <pages> pages 274-283, </pages> <year> 1991. </year>
Reference-contexts: Another variant is to make the pointer allocation dynamic <ref> [35, 46] </ref>. Yet another variant is to maintain the state information in the form of a linked list with the memory block as the head and the set of participating cache lines forming the active chain of processors sharing that memory block [27, 32].
Reference: [36] <author> E. McCreight. </author> <title> The Dragon Computer System: An early overview. </title> <institution> Xerox Corp., </institution> <month> Sept. </month> <year> 1984. </year>
Reference-contexts: Protocol. There are two broad classifications of protocol families based on the semantics for dealing with processor writes: write-invalidate and write update. In write-invalidate schemes [21, 29, 39], a write to a cached line results in invalidating copies of this line present in other caches. In write update schemes <ref> [36, 49] </ref>, a write to a cached line results in updating copies of this line present in other caches. Bitar and Despain [8] propose a scheme in which the cache accepts lock and unlock commands from the processor in addition to the traditional read and write requests. <p> Since the performance studies of different cache configurations are available in [5, 52, 51], we restrict our study to the understanding of the effects of different synchronization scenarios. 4.4.1 Snoopy Cache Protocols Snooping cache schemes evaluated are the Berkeley protocol [29] as an invalidation scheme, the Dragon <ref> [36] </ref> protocol as a write update scheme, Bitar and Despain's method [8], and our lock-based protocol. The problem size is 1024 tasks, and the results are the average of 5 runs. size 100 (fine to medium grain) without the barrier synchronization.
Reference: [37] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: These algorithms eliminate the bursty network traffic that is possible in a straightforward implementation of mutual exclusion locks under high lock contention. Hensgen et al. [25] and Mellor-Crummey and Scott <ref> [37] </ref> have proposed efficient algorithms that bring down the amount of network traffic generated during barrier synchronization operations from O (n 2 ) (using the straightforward counter based algorithm) to O (nlogn). <p> However, the directory-based cache protocols are scalable to a large number of processors with such a workload (bottom two lines in Figures 13 and 14) due to the multiple communication paths available in the MIN. More efficient software synchronization algorithms <ref> [37] </ref> are not compared in this simulation study. First of all, it is shown in [37] that for snoopy caches a central counter based barrier does the best in terms of measured performance for a modest number of processors. <p> More efficient software synchronization algorithms <ref> [37] </ref> are not compared in this simulation study. First of all, it is shown in [37] that for snoopy caches a central counter based barrier does the best in terms of measured performance for a modest number of processors. <p> In a sense, the performance of CBL gives an upper bound for the performance of synchronization algorithms which use efficient queue maintenance in software a la the MCS algorithm in <ref> [37] </ref>. The scalability of such software synchronization algorithms is presented in [37]. Our work puts the results reported by [37] in context by considering a workload that contains shared and private accesses in addition to the synchronization accesses. <p> In a sense, the performance of CBL gives an upper bound for the performance of synchronization algorithms which use efficient queue maintenance in software a la the MCS algorithm in <ref> [37] </ref>. The scalability of such software synchronization algorithms is presented in [37]. Our work puts the results reported by [37] in context by considering a workload that contains shared and private accesses in addition to the synchronization accesses. <p> In a sense, the performance of CBL gives an upper bound for the performance of synchronization algorithms which use efficient queue maintenance in software a la the MCS algorithm in <ref> [37] </ref>. The scalability of such software synchronization algorithms is presented in [37]. Our work puts the results reported by [37] in context by considering a workload that contains shared and private accesses in addition to the synchronization accesses. As we mentioned in Section 3, software synchronization algorithms incur an overhead for queue maintenance in software (despite reducing the network traffic) that is not present in CBL.
Reference: [38] <author> S. L. Min and J. Baer. </author> <title> A timestamp-based cache coherence scheme. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages I:23-32, </pages> <year> 1989. </year> <month> 25 </month>
Reference-contexts: In software-controlled cache schemes, cache lines are invalidated or updated according to the compiler's static analysis of parallel programs. There have been several proposals [12, 16] to determine the cacheability of shared data through compile-time flow analysis for specific language constructs such as FORTRAN doall. Min and Baer <ref> [38] </ref> suggest a time-stamp based cache protocol that allows run-time determination of whether or not a cached 3 data item is valid. In Beehive [45], synchronization information is used in developing marking algorithms that determine when global operations have to be performed to ensure coherence for shared data.
Reference: [39] <author> M. Papamarcos and J. Patel. </author> <title> A low overhead solution for multiprocessors with private cache memories. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Protocol. There are two broad classifications of protocol families based on the semantics for dealing with processor writes: write-invalidate and write update. In write-invalidate schemes <ref> [21, 29, 39] </ref>, a write to a cached line results in invalidating copies of this line present in other caches. In write update schemes [36, 49], a write to a cached line results in updating copies of this line present in other caches.
Reference: [40] <author> C. D. Polychronopoulos. </author> <booktitle> Parallel Programming and Compilers, </booktitle> <pages> pages 113-158. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: This model represents a dynamic scheduling paradigm believed to be the kernel of several parallel programs <ref> [40] </ref>. Data parallel applications consist of several phases of computation, and each phase uses the results generated by the previous phase.
Reference: [41] <author> U. Ramachandran and J. Lee. </author> <title> Processor initiated sharing in multiprocessor caches. </title> <type> Technical Report GIT-ICS-88/43, </type> <institution> Georgia Institute of Technology, </institution> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Sequent SLIC [6]), a combining network [23], and a special bus for interprocessor communication [53]. As we observed earlier, Bitar and Despain [8] were the first to propose exclusive locks (with no queuing) as a cache primitive. Goodman et al. [22] and ourselves <ref> [41] </ref> were the first to independently propose queue-based locks in hardware as a cache primitive. While our primitives support both exclusive (write) and shared (read-shared) locks, Goodman et al. support only exclusive locks.
Reference: [42] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In the next two subsections, we present implementations of CBL for snoopy caches and for directory-based caches. 1 The number of machine instructions needed to implement Anderson's ticket lock [4] is roughly 50 for lock acquisition, and about 15 for lock release on the Sequent Symmetry and the KSR-1 <ref> [42] </ref>. 5 ' $ P1:read-lock P2:read-lock ~ P3:write-lock ' $ P4:read-lock P5:read-lock ~ P6:write-lock - state next-node count 3.2 Snoopy Cache Implementation In this subsection, we present an overview of a lock-based snoopy cache protocol supporting exclusive and non-exclusive locks.
Reference: [43] <institution> Kendall Square Research. Technical summary, </institution> <year> 1992. </year>
Reference-contexts: This lock scheme combines lock-based synchronization with the line transfer, thus performing locking in zero time. It does not distinguish between read lock and write lock requests, and also does not order the processors waiting for the lock. KSR-1 <ref> [43] </ref> a commercial shared memory multiprocessor, as well as Paradigm [13] - a research prototype from Stanford, provide primitives very similar to the one proposed by Bitar and Despain. <p> Further our primitives allow combining data transfer with the granting of the lock (similar to Bitar and Despain) thus providing synchronization for free. Subsequently, similar locking primitives have made their way into commercial machines (such as KSR-1 <ref> [43] </ref>), as well as university prototypes [33].
Reference: [44] <author> Herb Schwetman. </author> <title> CSIM Reference Manual. </title> <institution> MCC Corp., </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: In the case of CBL, we assume a Berkeley-style invalidation-based protocol for shared accesses made without the protection of a lock. The simulator is written in CSIM <ref> [44] </ref>. A simple FIFO queue is used to simulate shared bus service for snoopy CBL. For the directory based CBL, a packet switched network is assumed as the interconnect (owing to its generality) with infinite buffering in the switches which are 2 X 2 crossbar.
Reference: [45] <author> Gautam Shah and Umakishore Ramachandran. </author> <title> Towards exploiting the architectural features of beehive. </title> <type> Technical Report GIT-CC-91/51, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Min and Baer [38] suggest a time-stamp based cache protocol that allows run-time determination of whether or not a cached 3 data item is valid. In Beehive <ref> [45] </ref>, synchronization information is used in developing marking algorithms that determine when global operations have to be performed to ensure coherence for shared data.
Reference: [46] <author> R. Simoni and M. Horowitz. </author> <title> Dynamic pointer allocation for scalable cache coherence directories. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 72-81, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Another variant is to make the pointer allocation dynamic <ref> [35, 46] </ref>. Yet another variant is to maintain the state information in the form of a linked list with the memory block as the head and the set of participating cache lines forming the active chain of processors sharing that memory block [27, 32].
Reference: [47] <author> Anand Sivasubramaniam, Gautam Shah, Joonwon Lee, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> Experimental evaluation of algorithmic performance on two shared memory multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 13-24, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: When n is 8 the performance gap between CBL and BD is 14303 cycles, which is more than three times the gap when the grain size is fine to medium. The experimental results (on the Sequent Symmetry) reported in Reference <ref> [47] </ref> corroborates our simulation results. The effect of barrier synchronization is shown in Figures 10 and 12. Irrespective of the specific cache protocol, the net effect of the barrier is to synchronize the queue access of all the processors thus aggrevating the contention for this shared resource (see Figure 7).
Reference: [48] <author> Per Stenstrom. </author> <title> A survey of cache coherence schemes for multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 12-24, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Invalidation-based protocols have been found to be preferable for large-scale multiprocessors relying entirely on hardware to provide cache coherence. There are several ways to organize the state information in the central directory <ref> [10, 48] </ref>. One possibility is to associate a bit map with each memory block, where each bit represents the presence or absence of that memory block in a particular processor. A variant of this basic scheme is to use a limited set of pointers instead of a bit map. <p> There are space/time trade-offs, and scalability implications depending on the particular choice of organizing the state information. The reader is referred to <ref> [48] </ref> for a more complete survey of the pros and cons of different directory based schemes. Software Enforcement of Coherence. In software-controlled cache schemes, cache lines are invalidated or updated according to the compiler's static analysis of parallel programs.
Reference: [49] <author> C. P. Thacker and L. C. Stewart. Firefly: </author> <title> A multiprocessor workstation. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-172, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Protocol. There are two broad classifications of protocol families based on the semantics for dealing with processor writes: write-invalidate and write update. In write-invalidate schemes [21, 29, 39], a write to a cached line results in invalidating copies of this line present in other caches. In write update schemes <ref> [36, 49] </ref>, a write to a cached line results in updating copies of this line present in other caches. Bitar and Despain [8] propose a scheme in which the cache accepts lock and unlock commands from the processor in addition to the traditional read and write requests.
Reference: [50] <author> Mary Vernon, Edward D. Lazowska, and John Zahorjan. </author> <title> An accurate and efficient performance analysis technique for multiprocessor snooping cache-consistency protocols. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <year> 1988. </year>
Reference-contexts: Therefore, hardware measurement is more often used with other techniques such as analytical modeling and simulation [14, 28, 26]. 4.1 Analytical Models Analytical modeling provides initial estimates of performance but has limitations owing to the assumptions that are often made to make the model tractable. Vernon et al. <ref> [50] </ref> report on the performance of snoopy cache protocols using the MVA (mean value analysis) technique. The results 14 from this analysis are surprisingly close to the simulation results of Archibald and Baer [5] while the required computation time is significantly smaller than simulation.
Reference: [51] <author> W-D Weber and A. Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <year> 1989. </year>
Reference-contexts: With respect to our two-level workload model (see Figure 7), we set the ratio of shared accesses to total accesses to be 0.03 during the "execute" phase of a task, and 0.5 during insertions of items into and/or deletions of items from from the queue. Real traces <ref> [18, 51] </ref> show that the read ratio ranges from 0.6 to 0.9 depending on the application of traced programs. In [5] simulation was done varying the read ratio from 0.7 to 0.85. In our simulation the read ratio is set to 0.85. <p> The CSIM simulation tool generates various statistics including average queue length and server utilization. We present those numbers only when it is necessary to justify the completion times. Since the performance studies of different cache configurations are available in <ref> [5, 52, 51] </ref>, we restrict our study to the understanding of the effects of different synchronization scenarios. 4.4.1 Snoopy Cache Protocols Snooping cache schemes evaluated are the Berkeley protocol [29] as an invalidation scheme, the Dragon [36] protocol as a write update scheme, Bitar and Despain's method [8], and our lock-based
Reference: [52] <author> W-D Weber and A. Gupta. </author> <title> Reducing memory and traffic requirements for scalable directory-based cache coherence schemes. </title> <booktitle> In Workshop on Scalable Shared Memory Architecture, </booktitle> <address> May 1990. Seattle, WA USA. </address> <month> 26 </month>
Reference-contexts: The CSIM simulation tool generates various statistics including average queue length and server utilization. We present those numbers only when it is necessary to justify the completion times. Since the performance studies of different cache configurations are available in <ref> [5, 52, 51] </ref>, we restrict our study to the understanding of the effects of different synchronization scenarios. 4.4.1 Snoopy Cache Protocols Snooping cache schemes evaluated are the Berkeley protocol [29] as an invalidation scheme, the Dragon [36] protocol as a write update scheme, Bitar and Despain's method [8], and our lock-based
Reference: [53] <author> W. A. Wulf and C. G. Bell. </author> <title> C.mmp amulti-mini processor. </title> <booktitle> In Proceedings of the Fall Joint Computer Conference, </booktitle> <pages> pages 765-777, </pages> <month> Dec. </month> <year> 1972. </year> <month> 27 </month>
Reference-contexts: Hardware support for synchronization comes in various forms such as a special-purpose coprocessor (e.g. Sequent SLIC [6]), a combining network [23], and a special bus for interprocessor communication <ref> [53] </ref>. As we observed earlier, Bitar and Despain [8] were the first to propose exclusive locks (with no queuing) as a cache primitive. Goodman et al. [22] and ourselves [41] were the first to independently propose queue-based locks in hardware as a cache primitive.
References-found: 53

