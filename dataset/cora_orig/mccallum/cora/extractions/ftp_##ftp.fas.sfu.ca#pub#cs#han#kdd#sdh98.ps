URL: ftp://ftp.fas.sfu.ca/pub/cs/han/kdd/sdh98.ps
Refering-URL: http://fas.sfu.ca/cs/research/groups/DB/sections/publication/kdd/kdd.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: nstefanog@cs.sfu.ca  
Phone: ph. (604) 291-4411 fax (604) 291-3045  
Title: An Efficient Two-Step Method for Classification of Spatial Data  
Author: Krzysztof Koperski Jiawei Han Nebojsa Stefanovic fkoperski, han, 
Keyword: Spatial Data Mining, Spatial Decision Support Systems, Decision Trees.  
Address: Burnaby, B.C., Canada V5A 1S6  
Affiliation: School of Computing Science Simon Fraser University  
Abstract: Spatial data mining, i.e., discovery of interesting, implicit knowledge in spatial databases, is a highly demanding field because very large amounts of spatial data have been collected in various applications, ranging from remote sensing, to geographical information systems (GIS), computer cartography, environmental assessment and planning, etc. In this paper, an efficient method for building decision trees for the classification of objects stored in geographic information databases is proposed and studied. Our approach to spatial classification is based on both (1) non-spatial properties of the classified objects and (2) attributes, predicates and functions describing spatial relations between classified objects and other features located in the spatial proximity of the classified objects. Several optimization techniques are explored, including a two-step spatial computation technique, use of spatial-join indices, etc. We implemented the algorithm and conducted experiments that showed the effectiveness of the proposed method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. G. Aref and H. Samet. </author> <title> Extending DBMS with Spatial Operations. </title> <booktitle> Proc. 2nd Symp. SSD'91, </booktitle> <pages> pp. 299-318, </pages> <address> Zurich, Switzerland, </address> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Most of the classification methods consider only relational data. Geographic data consists of spatial objects and non-spatial descriptions of these objects. Non-spatial descriptions of spatial objects can be stored in a relational database where one attribute is a pointer to the spatial description of the object <ref> [1] </ref>. In the process of spatial classification one wants to find rules that partition a set of classified objects into a number of classes using not only nonspatial properties of the classified objects, but also spatial relations of the classified objects to other objects in the database.
Reference: [2] <author> T. Brinkhoff, H. P. Kriegel, R. Schneider, B. Seege. </author> <title> Multistep Processing of Spatial Joins. </title> <booktitle> In Proc. 1994 ACM-SIGMOD Conf. Management of Data, </booktitle> <address> Minneapolis, </address> <publisher> Minnesota, </publisher> <pages> pp. 197-208, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: To accelerate this process we use two-step approach in which some rough computations are performed first and then fine computations are done only for the promising patterns. Such process is similar to the two-step approach for mining spatial association rules [13], or to multi-step spatial join approach <ref> [2] </ref>.
Reference: [3] <author> W. W. Cohen. </author> <title> Learning Trees and Rules with Set-valued Features. </title> <booktitle> Proc. of the 13th National Conference on Artificial Intelligence (AAAI), </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: Other measures for building decision trees may also be used [8]. Recently some decision tree algorithms that take into account a large number of features describing objects have been proposed <ref> [3] </ref>. The algorithm [3] can be modified to analyze spatial descriptions of the data. For every classified object a set of generalized predicates that are satisfied by this object is stored. An example of such description is presented in Table 3. <p> Other measures for building decision trees may also be used [8]. Recently some decision tree algorithms that take into account a large number of features describing objects have been proposed <ref> [3] </ref>. The algorithm [3] can be modified to analyze spatial descriptions of the data. For every classified object a set of generalized predicates that are satisfied by this object is stored. An example of such description is presented in Table 3.
Reference: [4] <author> M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. </author> <title> A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases. </title> <booktitle> In Proc. of the Second International Conference on Data Mining KDD-96, </booktitle> <pages> pp. 226-231, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: It combines research in statistics, machine learning, spatial reasoning and spatial databases. In the last few years algorithms for spatial associations [13], clustering <ref> [4] </ref>, generalized spatial descriptions [15], characteristics of spatial clusters [18] and others were analyzed. One of the areas which has not brought much attention is the classification of spatial objects.
Reference: [5] <author> M. Ester, H.-P. Kriegel, and J. Sander. </author> <title> Spatial data mining: A database approach. </title> <booktitle> Proc. Int. Symp. on Large Spatial Databases (SSD'97), </booktitle> <address> Berlin, Germany, </address> <pages> pp. 47-66, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: The proposed method deals with image databases and is tailored for the astronomical application. Thus, it is not suitable for the analysis of vector data format, often used in Geographic Information Systems. A method for classification of spatial objects was proposed by Ester et. al. <ref> [5] </ref>. The proposed algorithm is based on ID3 algorithm [20] and it uses the concept of neighborhood graphs. It considers not only non-spatial properties of the classified objects, but also nonspatial properties of neighboring objects. <p> Step 6 generates binary decision tree using ID3 algorithm [20] modified to allow processing of description in the form of sets of predicates as described in the previous section. The algorithm presented above introduces a number of optimization techniques in the comparison with algorithm suggested in <ref> [5] </ref>, namely: 1. It uses progressive refinement approach (sampling and coarse predicates followed by fine computations for promising pat terns) to accelerate the computation. 2. It may use aggregate information. 3. It may use distance-based join index [16] to accelerate query processing. Neighborhood index [5] may not be the best for <p> the comparison with algorithm suggested in <ref> [5] </ref>, namely: 1. It uses progressive refinement approach (sampling and coarse predicates followed by fine computations for promising pat terns) to accelerate the computation. 2. It may use aggregate information. 3. It may use distance-based join index [16] to accelerate query processing. Neighborhood index [5] may not be the best for that purpose. 4. It may use concept hierarchies which result in simpler decision trees and faster computations [11]. 5. It uses relevance analysis process to eliminate predicates and attributes that do not contribute to the quality of classifi cation. Complexity analysis. <p> As one can observe the quality of the classification process increases drastically when relevance analysis is performed. If we choose to do classification based on the algorithm proposed in <ref> [5] </ref> we get a poor quality because this algorithm performs no relevance analysis. After applying relevance analysis execution time decreased 3 to 4 times depending on the value of the threshold. The algorithm [5] is based on neighborhood indices that avoid process of spatial join which is necessary to extract spatial <p> If we choose to do classification based on the algorithm proposed in <ref> [5] </ref> we get a poor quality because this algorithm performs no relevance analysis. After applying relevance analysis execution time decreased 3 to 4 times depending on the value of the threshold. The algorithm [5] is based on neighborhood indices that avoid process of spatial join which is necessary to extract spatial predicates. When no neighborhood indices exist the algorithm [5] has to produce them which results in the classification time similar to the time in our experiment for threshold equal to 1 when all <p> After applying relevance analysis execution time decreased 3 to 4 times depending on the value of the threshold. The algorithm <ref> [5] </ref> is based on neighborhood indices that avoid process of spatial join which is necessary to extract spatial predicates. When no neighborhood indices exist the algorithm [5] has to produce them which results in the classification time similar to the time in our experiment for threshold equal to 1 when all predicates are used for decision tree construction. The process of relevance analysis took about 17 seconds where 16 seconds were spent on spatial operations. <p> Then, refined computations are done only for the set of promising patterns producing smaller and more accurate decision trees. Conducted experiments showed the importance of relevance analysis which resulted in better quality of decision trees in comparison to the trees produced by algorithm proposed in <ref> [5] </ref>. We also observed that the time to build decision trees was shorter when relevance analysis is performed on a sample of the objects and only relevant attributes are computed for all objects in comparison to the time when all predicates were used for the construction of the decision tree.
Reference: [6] <author> U. M. Fayyad, S. G. Djorgovski, and N. Weir. </author> <title> Automating the Analysis and Cataloging of Sky Surveys. </title> <booktitle> In [7] </booktitle>
Reference-contexts: Algorithm for building decision trees from spatial data is shown in Section 4. Experimental analysis is presented in Section 5 and the paper ends with the discussion of future work in Section 6. 2 Relevant Work Classification of spatial data has been analyzed by some researchers. Fayyad et. al. <ref> [6] </ref> used decision tree methods to classify images of stellar objects to detect stars and galaxies. About 3 TB of sky images were analyzed. Data images were preprocessed by low-level image processing system FOCAS, which selected objects and produced basic attributes like: magnitudes, areas, intensity, image moments, ellipticity, orientation, etc.
Reference: [7] <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, Eds. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI/MIT Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: Each object in a database (in relational databases tuples are treated as objects) is assumed to belong to a predefined class, as it is determined by one of the attributes, called the class label attribute. A number of classification methods were proposed by statistics and machine learning researchers <ref> [7, 20, 21] </ref>. The most common classification method constructs decision trees. Such method employs a top-down, divide-and-conquer strategy that partitions the set of given objects into smaller subsets where the leaf nodes are associated mostly with a single class. Most of the classification methods consider only relational data.
Reference: [8] <author> U. M. Fayyad, K. B. Irani. </author> <title> The Attribute Selection Problem in Decision Tree Generation. </title> <booktitle> Proc. of the Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <pages> pp. 104-110, </pages> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference-contexts: Other measures for building decision trees may also be used <ref> [8] </ref>. Recently some decision tree algorithms that take into account a large number of features describing objects have been proposed [3]. The algorithm [3] can be modified to analyze spatial descriptions of the data. <p> In the similar manner for other attributes instead of branching on all attribute values the algorithm branches on one attribute value. For example, we may have separate branches for sum (population) = M EDI U M and for sum (population) &lt;&gt; M EDIU M . As claimed in <ref> [8] </ref> such binary decision trees are usually more compact and more accurate than trees produced by regular ID3 algorithm which branch on all values of the predicates.
Reference: [9] <author> J. Han, and Y. Fu. </author> <title> Exploration of the Power of Attribute-Oriented Induction in Data Mining. </title> <booktitle> In [7]. </booktitle>
Reference-contexts: Step 4 is spatial query processing. Step 5 has been verified in <ref> [9] </ref>. Step 6 generates binary decision tree using ID3 algorithm [20] modified to allow processing of description in the form of sets of predicates as described in the previous section. The algorithm presented above introduces a number of optimization techniques in the comparison with algorithm suggested in [5], namely: 1.
Reference: [10] <author> J. Han, K. Koperski, and N. Ste--fanovic. GeoMiner: </author> <title> A system prototype for spatial data mining. </title> <booktitle> In Proc. 1997 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <pages> pp. 553-556, </pages> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: The experiments analyzing the impact of different levels of concepts used for the classification on classification accuracy should be constructed. Finally, we would like to integrate this method with our spatial data mining prototype Ge-oMiner <ref> [10] </ref>.
Reference: [11] <author> M. Kamber, L. Winstone, W. Gong, S. Cheng, and J. Han. </author> <title> Generalization and Decision Tree Induction: Efficient Classification in Data Mining. </title> <booktitle> Proc. of 1997 Int. Workshop Research Issues on Data Engineering (RIDE'97)(April, </booktitle> <address> Birming-ham, England), </address> <year> 1997, </year> <pages> pp. 111-120. </pages>
Reference-contexts: It may use aggregate information. 3. It may use distance-based join index [16] to accelerate query processing. Neighborhood index [5] may not be the best for that purpose. 4. It may use concept hierarchies which result in simpler decision trees and faster computations <ref> [11] </ref>. 5. It uses relevance analysis process to eliminate predicates and attributes that do not contribute to the quality of classifi cation. Complexity analysis. Execution time of the algorithm presented above can be estimated using equations presented below.
Reference: [12] <author> K. Kira, and L. A. Rendell. </author> <title> The Feature Selection Problem: Traditional Methods and a New Algorithm. </title> <booktitle> Proc. of the Tenth National Conference on Artificial Intelligence AAAI-92 pp. </booktitle> <pages> 129-134, </pages> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference-contexts: For example, one can use Minimum Bounding Rectangles (MBRs) to find coarse g close to predicates which imply that MBRs of two objects are within specific distance threshold. Then, some machine learning methods may be used for the extraction of the relevant predicates or functions <ref> [12, 23] </ref>. In our experiments we used RELIEF algorithm [12] whose algorithmic description is presented in Table 4. The RELIEF algorithm uses nearest neighbor approach to find relevant predicates. <p> Then, some machine learning methods may be used for the extraction of the relevant predicates or functions [12, 23]. In our experiments we used RELIEF algorithm <ref> [12] </ref> whose algorithmic description is presented in Table 4. The RELIEF algorithm uses nearest neighbor approach to find relevant predicates. <p> One can observe that weights for relevant predicates have positive values, while expected weights for non-relevant predicates are zero. Finally, only predicates with weights larger than the predefined threshold are used for classification. The value for the threshold can be set based on statistical theory <ref> [12] </ref>. In our case only the relevant predicates are computed in detail for all classified objects. Besides improving the efficiency of the algo rithm accuracy of the classification and size of the decision tree may improve as well because there will be no branching on irrelevant predicates.
Reference: [13] <author> K. Koperski, and J. Han. </author> <title> Discovery of Spatial Association Rules in Geographic Information Databases. </title> <booktitle> In Advances in Spatial Databases, Proceedings of 4th Symposium, </booktitle> <address> SSD'95. </address> <month> (Aug. </month> <pages> 6-9, </pages> <address> Portland, </address> <publisher> Maine). Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995, </year> <pages> pp. 47-66. </pages>
Reference-contexts: This increasing amount of data and demand for decision support systems has created a new area of research, spatial data mining, which is a subfield of data mining that deals with the extraction of implicit knowledge, spatial relationships, or other interesting patterns not explicitly stored in spatial databases <ref> [13] </ref>. It combines research in statistics, machine learning, spatial reasoning and spatial databases. In the last few years algorithms for spatial associations [13], clustering [4], generalized spatial descriptions [15], characteristics of spatial clusters [18] and others were analyzed. <p> mining, which is a subfield of data mining that deals with the extraction of implicit knowledge, spatial relationships, or other interesting patterns not explicitly stored in spatial databases <ref> [13] </ref>. It combines research in statistics, machine learning, spatial reasoning and spatial databases. In the last few years algorithms for spatial associations [13], clustering [4], generalized spatial descriptions [15], characteristics of spatial clusters [18] and others were analyzed. One of the areas which has not brought much attention is the classification of spatial objects. <p> To accelerate this process we use two-step approach in which some rough computations are performed first and then fine computations are done only for the promising patterns. Such process is similar to the two-step approach for mining spatial association rules <ref> [13] </ref>, or to multi-step spatial join approach [2].
Reference: [14] <author> K. Koperski, J. Han, and J. Adhikary. </author> <title> Mining Knowledge in Geographical Data. </title> <journal> In Communications of ACM, </journal> <note> 1998. (to appear). </note>
Reference-contexts: 1 Introduction The study and the development of data mining algorithms for spatial databases <ref> [14] </ref> is motivated by the large amount of data collected through remote sensing, medical equipment, and other methods. Moreover, the geocod-ing of consumer addresses in combination with large amount of recorded sales transactions creates very large spatially related databases.
Reference: [15] <author> W. Lu, J. Han, and B. C. Ooi. </author> <title> Discovery of General Knowledge in Large Spatial Databases. </title> <booktitle> In Proceedings of Far East Workshop on Geographic Information Systems (June 21-22, </booktitle> <address> Singapore). </address> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 275-289. </pages>
Reference-contexts: It combines research in statistics, machine learning, spatial reasoning and spatial databases. In the last few years algorithms for spatial associations [13], clustering [4], generalized spatial descriptions <ref> [15] </ref>, characteristics of spatial clusters [18] and others were analyzed. One of the areas which has not brought much attention is the classification of spatial objects.
Reference: [16] <author> W. Lu and J. Han. </author> <title> Distance-Associated Join Indeces for Spatial Range Search. </title> <booktitle> Proceedings of the Interantional Conference on Data Engineering pp. </booktitle> <pages> 284-292, </pages> <year> 1992. </year>
Reference-contexts: It is done by finding for all relevant nonspatial aggregate attributes the size of the buffer X max where the information gain for the aggregated attribute is maximum. Such buffer would be used to compute aggregates for all relevant attributes of thematic maps. Distance-based join index <ref> [16] </ref> may be used to accelerate the computation. 4. Build sets of predicates describing all objects using relevant fine predicates, func tions, and attributes. 5. Perform generalization of the sets of pred icates based on concept hierarchies. 6. Generate binary decision tree. Rationale of the Algorithm. <p> It uses progressive refinement approach (sampling and coarse predicates followed by fine computations for promising pat terns) to accelerate the computation. 2. It may use aggregate information. 3. It may use distance-based join index <ref> [16] </ref> to accelerate query processing. Neighborhood index [5] may not be the best for that purpose. 4. It may use concept hierarchies which result in simpler decision trees and faster computations [11]. 5.
Reference: [17] <institution> MapInfo Corporation. MapInfo Professional Home Page. </institution> <note> www.mapinfo.com/events/mipro/mipro.html </note>
Reference-contexts: TIGER data contains over 87,000 polygons, lines and points presenting roads, parks, rivers, lakes and other objects in Wash-ington state. MapInfo Professional 4.1 2 Geographic Information System <ref> [17] </ref> was used as spatial database for the experiments. We have tested quality of classification and time of the execution of the algorithm. Relevance analysis was done based on 100 objects from each class label set.
Reference: [18] <author> R. T. Ng and Y. Yu. </author> <title> Discovering Strong, Common and Discriminating Characteristics of Clusters from Thematic Maps. </title> <booktitle> Proceedings of the 11th Annual Symposium on Geographic Information Systems pp. </booktitle> <pages> 392-394, </pages> <year> 1997. </year>
Reference-contexts: It combines research in statistics, machine learning, spatial reasoning and spatial databases. In the last few years algorithms for spatial associations [13], clustering [4], generalized spatial descriptions [15], characteristics of spatial clusters <ref> [18] </ref> and others were analyzed. One of the areas which has not brought much attention is the classification of spatial objects. <p> The algorithm also does not perform relevance analysis and thus, it may produce overspecialized, poor quality trees. Finally, the algorithm does not take into account concept hierarchies that may exist for the non-spatial and spatial attribute values. Ng and Yu <ref> [18] </ref> described a method for the extraction of strong, common and discriminating characteristics of clusters based on thematic maps. The authors proposed measures for interest/utility values of the characteristics of clusters.
Reference: [19] <author> K. Peterson. </author> <title> A Trade Area Primer. </title> <journal> Business Geographics, </journal> <volume> Vol. 5, No. 9, </volume> <year> 1997, </year> <pages> pp. 18-21. </pages>
Reference-contexts: For example, in the case of stores and shopping malls a buffer may represent the area where the customers live or work. In business geographics the spatial extent of a store's customers is called trade area <ref> [19] </ref>. <p> In addition to that, we may use different criteria to determine shape of the buffers. The buffers may be based on rings, customer penetration polygons, Voronoi diagrams, drive-time polygons, etc. Please see <ref> [19] </ref> for a detailed discussion. The rings have some advantages like: (1) ease of use, (2) no need to determine trade area based on customer data, (3) easy comparison between sites. One study showed high degree of similarity between rings and customer penetration polygons [19]. <p> Please see <ref> [19] </ref> for a detailed discussion. The rings have some advantages like: (1) ease of use, (2) no need to determine trade area based on customer data, (3) easy comparison between sites. One study showed high degree of similarity between rings and customer penetration polygons [19]. <p> close to (x,park) Table 6: Generalized descriptions of classified objects presented as sets of predicates determine better neighborhood for the classification task. 3.3 Aggregate Information Aggregate values for areas close to spatial objects play an important role in the analysis of many business objects like stores, restaurants, gas stations, etc. <ref> [19] </ref>. To handle aggregate values of non-spatial attributes in thematic maps one can calculate aggregate values for the block groups intersecting buffers. For example, one can notice that only a small part of block C on the map presented in Figure 1 is intersected by buffer for OID1.
Reference: [20] <author> J. R. Quinlan. </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> No. 1, </volume> <year> 1986, </year> <pages> pp. 81-106. </pages>
Reference-contexts: Each object in a database (in relational databases tuples are treated as objects) is assumed to belong to a predefined class, as it is determined by one of the attributes, called the class label attribute. A number of classification methods were proposed by statistics and machine learning researchers <ref> [7, 20, 21] </ref>. The most common classification method constructs decision trees. Such method employs a top-down, divide-and-conquer strategy that partitions the set of given objects into smaller subsets where the leaf nodes are associated mostly with a single class. Most of the classification methods consider only relational data. <p> Thus, it is not suitable for the analysis of vector data format, often used in Geographic Information Systems. A method for classification of spatial objects was proposed by Ester et. al. [5]. The proposed algorithm is based on ID3 algorithm <ref> [20] </ref> and it uses the concept of neighborhood graphs. It considers not only non-spatial properties of the classified objects, but also nonspatial properties of neighboring objects. <p> Besides improving the efficiency of the algo rithm accuracy of the classification and size of the decision tree may improve as well because there will be no branching on irrelevant predicates. In the construction of the decision tree we used information gain utilized in the ID3 algorithm <ref> [20] </ref> to find attributes/predicates that partition datasets into classes. Information gain is computed in the following way. <p> Step 4 is spatial query processing. Step 5 has been verified in [9]. Step 6 generates binary decision tree using ID3 algorithm <ref> [20] </ref> modified to allow processing of description in the form of sets of predicates as described in the previous section. The algorithm presented above introduces a number of optimization techniques in the comparison with algorithm suggested in [5], namely: 1.
Reference: [21] <author> S. R. Safavian and D. Landgrebe. </author> <title> A Survey of Decision Tree Classifier Technology. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> Vol. 21, No. 3, </volume> <year> 1991, </year> <pages> pp. 660-674. </pages>
Reference-contexts: Each object in a database (in relational databases tuples are treated as objects) is assumed to belong to a predefined class, as it is determined by one of the attributes, called the class label attribute. A number of classification methods were proposed by statistics and machine learning researchers <ref> [7, 20, 21] </ref>. The most common classification method constructs decision trees. Such method employs a top-down, divide-and-conquer strategy that partitions the set of given objects into smaller subsets where the leaf nodes are associated mostly with a single class. Most of the classification methods consider only relational data.
Reference: [22] <institution> U.S. Census Bureau. </institution> <note> Tiger/Line Documentation. www.census.gov/geo/www/tiger/tl95doc.html </note>
Reference-contexts: Relations of the data to thematic maps were not taken into account. In order to evaluate classification quality and execution time of the algorithm we created a synthetic dataset which was merged with the TIGER 1 data <ref> [22] </ref> for Washington state. The synthetic data contained 1800 polygons with 5 to 55 edges. The polygons belonged to 4 sets: two class label sets (T00 and T01) and two predictive sets (S00 and S01).
Reference: [23] <author> D. Wetteschereck, D. W. Aha, and T. Mohri. </author> <title> A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> No. 10, </volume> <year> 1997, </year> <pages> pp. 1-37. </pages>
Reference-contexts: For example, one can use Minimum Bounding Rectangles (MBRs) to find coarse g close to predicates which imply that MBRs of two objects are within specific distance threshold. Then, some machine learning methods may be used for the extraction of the relevant predicates or functions <ref> [12, 23] </ref>. In our experiments we used RELIEF algorithm [12] whose algorithmic description is presented in Table 4. The RELIEF algorithm uses nearest neighbor approach to find relevant predicates.
References-found: 23

