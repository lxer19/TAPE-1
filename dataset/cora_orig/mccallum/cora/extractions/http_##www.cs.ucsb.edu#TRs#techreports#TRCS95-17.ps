URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS95-17.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fdandrese, tyang, veho, ibarrag@cs.ucsb.edu  
Title: SWEB: Towards a Scalable World Wide Web Server on Multicomputers  
Author: Daniel Andresen Tao Yang Vegard Holmedahl Oscar H. Ibarra 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: We investigate the issues involved in developing a scalable World Wide Web (WWW) server on a cluster of workstations and parallel machines, using the Hypertext Transport Protocol (HTTP). The main objective is to strengthen the processing capabilities of such a server by utilizing the power of multicomputers to match huge demands in simultaneous access requests from the Internet. We have implemented a system called SWEB on a distributed memory machine, the Meiko CS-2, and networked SUN and DEC workstations. The scheduling component of the system actively monitors the usages of CPU, I/O channels and the interconnection network to effectively distribute HTTP requests across processing units to exploit task and I/O parallelism. We present the experimental results on the performance of this system. Our results indicate that the system delivers good performance on multi-computers and obtains significant improvements over other approaches.
Abstract-found: 1
Intro-found: 1
Reference: [A95] <author> D.Andresen, L.Carver, R.Dolin, C.Fischer, J.Frew, M.Goodchild, O.Ibarra, R.Kothuri, M.Larsgaard, B.Manjunath, D.Nebert, J.Simpson, T.Smith, T.Yang, Q.Zheng, </author> <title> "The WWW Prototype of the Alexandria Digital Library", </title> <booktitle> Proceedings of ISDL'95: International Symposium on Digital Libraries, </booktitle> <address> Japan August 22 - 25, </address> <year> 1995. </year>
Reference-contexts: 1 Motivation The Scalable Web server (SWEB) project grew out of the needs of the Alexandria Digital Library (ADL) project at UCSB <ref> [A95] </ref>. Digital library systems, which provide the on-line retrieval and processing of digitized documents through Internet, have increasingly turned into a topic of national importance. The Alexandria Project is focused on the design, implementation, and deployment of a distributed digital library for spatially-indexed information. <p> A rapid prototype system for the Alexandria digital library has already been developed which allows users to browse spatially-indexed maps and images <ref> [A95] </ref>. To expose the system to 1 a broad user community, the next version of the system (available at the end of 1995) will be connected to the World Wide Web (WWW) using the Hypertext Transport Protocol (HTTP). <p> For WWW-based network information systems such as digital libraries, the servers involve much more intensive I/O and heterogeneous CPU activities. This is because such systems process a large amount of digitized images. And content-based searching and multi-resolution image browsing <ref> [A95] </ref> have large demands in computation speed, disk storage and network bandwidth in the server. To meet such demands, the SWEB project makes use of existing networked computers and inexpensive disk resources to strengthen the processing and storage capabilities of WWW servers. <p> We expect that recycling the idle cycles of those processing units and retrieving files in parallel from inexpensive disks can significantly improve the scalability of the server in response to a large amount of simultaneous HTTP requests. We have developed the preliminary version of a scalable WWW server (SWEB) <ref> [A95] </ref> running on a Meiko CS-2 distributed memory machine and a network of workstations (NOW) such as SUN and DEC machines. Each processing unit is capable of handling a user request following the HTTP protocol.
Reference: [BCS95] <author> J. Baumgartner, D. J. Cook, and B. Shirazi, </author> <title> Genetic solutions to the load balancing problem. </title> <booktitle> Proc of ICPP 95 Workshop, </booktitle> <pages> pp 72-78. </pages>
Reference-contexts: Thirdly, we assume lossless document delivery, keeping consistency between the source library holding and what is sent to the client. Our dynamic scheduling scheme is closely related to the previous work on load balancing on distributed systems <ref> [ELZ86, BCS95, WR93] </ref>. In these studies, tasks arrivals may temporarily be uneven among processors and the goal of load balancing is to adjust the imbalance between processors by appropriately transferring tasks from overloaded processors to underloaded processors. The most well known techniques are called receiver-initiated and sender-initiated strategies.
Reference: [BD94] <author> C. M. Bowman, P. B. Danzig, D. Hardy, U. Manber and M Schwartz, </author> <title> The Harvest Information Discovery and Access System. </title> <booktitle> P roceedings of the Second International World Wide Web Conference, </booktitle> <pages> pp. 763-771, </pages> <address> Chicago, Illinois, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Other projects have focused on improving the performance of a single-workstation server. The Apache project has programmed a set of changes to the early versions of NCSA httpd to make the server more efficient. The CERN server has achieved significant improvements. The Harvest project <ref> [BD94] </ref> has introduced a HTTP cache which can be a major accelerator for pages without a dynamically created component. All these efforts are concentrating on the single workstation server level and can be embodied in our system, while our work focuses on a collaborating multi-workstation server.
Reference: [APA95] <institution> Apache HTTP Server Project, </institution> <address> http : ==www:apache:org=index:html, June 26, </address> <year> 1995. </year>
Reference: [BC95] <author> T. Bemers-Lee and D. Connolly, </author> <title> Hypertext Markup Language - 2.0, </title> <address> http : ==www:w3:org=hypertext=W W W=M arkU p=html spec=html spec toc:html, </address> <month> June 16, </month> <year> 1995. </year>
Reference-contexts: The URL defines which resource the user wishes to access, the HTML language allows the information to be presented in a platform-independent but still well-formatted manner, and the HTTP protocol is the application-level mechanism for achieving the transfer of information <ref> [HT95, BC95, BL95] </ref>. * HTML is derived from the Standard Generalized Markup Language (SGML) optimized for presenting hypertext information. It operates on the basis of tags, where each piece of the text is marked to indicate the format to be used.
Reference: [BL95] <author> T. Bemers-Lee, </author> <title> Uniform Resource Locators, </title> <address> http : ==www:w3:org=hypertext=W W W=Addressing=U RL=, </address> <year> 1995. </year>
Reference-contexts: The URL defines which resource the user wishes to access, the HTML language allows the information to be presented in a platform-independent but still well-formatted manner, and the HTTP protocol is the application-level mechanism for achieving the transfer of information <ref> [HT95, BC95, BL95] </ref>. * HTML is derived from the Standard Generalized Markup Language (SGML) optimized for presenting hypertext information. It operates on the basis of tags, where each piece of the text is marked to indicate the format to be used.
Reference: [ELZ86] <author> D. L. Eager, E. D. Lazowska, and J. Zahojan, </author> <title> Adaptive load sharing in homogeneous distributed systems, </title> <journal> IEEE Trans on Software Engineering, </journal> <volume> 12(5) </volume> <pages> 662-675. </pages> <year> 1986. </year>
Reference-contexts: Thirdly, we assume lossless document delivery, keeping consistency between the source library holding and what is sent to the client. Our dynamic scheduling scheme is closely related to the previous work on load balancing on distributed systems <ref> [ELZ86, BCS95, WR93] </ref>. In these studies, tasks arrivals may temporarily be uneven among processors and the goal of load balancing is to adjust the imbalance between processors by appropriately transferring tasks from overloaded processors to underloaded processors. The most well known techniques are called receiver-initiated and sender-initiated strategies. <p> In a single-faceted 11 scheduling system, a processor can be classified as lightly loaded and heavily loaded based on one parameter, e.g. CPU load. One purpose of such a classification is to update load information only when a classification changes to reduce unnecessary overhead, e.g. <ref> [ELZ86, WR93] </ref>. In our problem context, it is hard to classify a processor as heavily or lighted loaded since there are several load parameters. A processor could have a light CPU load but its local disk may receive many access requests from the network file system.
Reference: [FKC95+] <author> C. Fu, M. Kim, D. C., O. H. Ibarra, T. Yang, </author> <title> Performance Evaluation of Parallel I/O on the Meiko CS-2. </title> <type> Tech Report, </type> <month> UCSB </month> <year> 1995. </year>
Reference-contexts: It is tested in <ref> [FKC95+] </ref> that the local disk read bandwidth is about b 1 = 5M Bytes=s for a set of files with size 1.5 MB and the remote disk read bandwidth is about b 2 = 4:5M B=s. Our experiments for Meiko show that d is about 0:10.
Reference: [GM95] <author> J. Gosling and H. McGilton, </author> <title> The Java Language Environment: A White Paper, </title> <address> http : ==java:sun:com=, </address> <year> 1995. </year>
Reference: [HL95] <author> J. Hsieh, M. Lin, J. Liu, T. Ruwart, and D. H.C. Du, </author> <title> Performance of A Mass Storage System for Video-On-Demand , Submitted to J ournal of Parallel and Distributed Computing, </title> <journal> Special issue on Multimedia Processing and Technology. </journal> <volume> 24 </volume>
Reference-contexts: It should be noted that WWW applications only represent a special class of Internet information 3 systems. There are other information servers dealing with huge file sizes and large numbers of users, for example multi-media servers <ref> [HL95] </ref>. Our situation has several differences. First, our current system has no real-time processing constraints for displaying digital movies or audio clips. Secondly, many of our users tend to browse different text and images information, rather than focusing one particular document such as one film beginning to end.
Reference: [HT95] <author> Hypertext Transfer Protocol(HTTP): </author> <title> A protocol for networked information, </title> <institution> http : ==www:w3:org=hypertext=W W W=P rotocols=HT T P=HT T P 2:html, </institution> <month> June 26, </month> <year> 1995. </year>
Reference-contexts: The URL defines which resource the user wishes to access, the HTML language allows the information to be presented in a platform-independent but still well-formatted manner, and the HTTP protocol is the application-level mechanism for achieving the transfer of information <ref> [HT95, BC95, BL95] </ref>. * HTML is derived from the Standard Generalized Markup Language (SGML) optimized for presenting hypertext information. It operates on the basis of tags, where each piece of the text is marked to indicate the format to be used. <p> A number of WWW browsers and servers use such protocols, for example, NetCom's NetScape browser, the NCSA and CERN HTTP servers <ref> [HT95] </ref>. A simple HTTP request would typically activate a sequence of events from initiation to completion as shown in figure 1. First, the client determines the host name from the URL, and uses the local Domain Name System (DNS) server to determine its IP address.
Reference: [LYC95] <author> Lycos Usage: </author> <title> Accesses per Day, </title> <address> http : ==lycos:cs:cmu:edu=usage day:html, </address> <month> June 17, </month> <year> 1995. </year>
Reference-contexts: Our work is motivated by the fact that the Alexandria digital library WWW server has a potential to become the bottleneck in delivering digitized documents over high-speed Internet. Popular WWW sites such as the Lycos and Y ahoo <ref> [LYC95] </ref> receive over one million accesses a day. For example, in 1993 alone, the weekly access rate for NCSA's HTTP server at UIUC increased from 91K to 1.5M.
Reference: [KB94] <author> E.D. Katz, M. Butler, R. McGrath, </author> <title> A Scalable HTTP Server: the NCSA Prototype, </title> <journal> Computer Networks and ISDN Systems. </journal> <volume> vol. 27, </volume> <year> 1994, </year> <pages> pp. 155-164. </pages>
Reference-contexts: For example, in 1993 alone, the weekly access rate for NCSA's HTTP server at UIUC increased from 91K to 1.5M. The peak request arrival rate at NCSA exceeds more than 20 requests/second, but a single, high-end workstation can handle only a limited number of requests per second <ref> [KB94] </ref>. The requests to the NCSA server typically involve only simple file retrieval operations. For WWW-based network information systems such as digital libraries, the servers involve much more intensive I/O and heterogeneous CPU activities. This is because such systems process a large amount of digitized images. <p> Section 6 provides analytic results on a performance bound of SWEB and verifies it with the actual experiment results. Section 7 discusses conclusions and future work. 2 Related Work Numerous other initiatives to create high-performance HTTP servers have been reported. NCSA <ref> [KB94] </ref> has built a multi-workstation HTTP server based on round-robin domain name resolution (DNS) to assign requests to workstations. The round-robin technique is effective when HTTP requests access HTML information of relatively-uniform size chunks and the load and computing powers of workstations are relatively comparable. <p> The NCSA has performed a number of tests using high-end workstations, and discovered in their working environment approximately 5-10 rps could be dealt with using the NCSA httpd server <ref> [KB94] </ref>, which cannot match the current and future loads (e.g. a digital library server). Thus multiple servers are needed for achieving scalable performance. We assume that the architecture of our system contains a set of networked workstations. <p> Internet. The Internet speed significantly affects the performance in information delivery. Currently, Internet bandwidth is increasing rapidly with the advent of ATM, BISDN, and other WAN technologies. However, the NCSA work <ref> [KB94] </ref> and our experiment show that the Internet is not necessarily the bottleneck in a high traffic system, and the server is another bottleneck for document delivery. Our goal is to get the requested information out of the server as fast as possible. <p> The current version of SWEB uses a distributed scheduler. The user requests are first evenly routed to SWEB processors via the DNS scheme as shown in Fig. 3. The use of DNS rotation to provide 8 the initial assignment of HTTP requests is used in the NCSA multi-workstation server <ref> [KB94] </ref>. In this scheme, multiple real machines are mapped to the same IP name. <p> The rotation on available workstation network IDs is in a round-robin fashion. This functionality is available in current DNS systems. The major advantages of this technique are simplicity, ease of implementation, and reliability <ref> [KB94] </ref>. round-robin fashion from a list of servers. The DNS assigns the requests without consulting dynamically-changing system load information. Thus the SWEB conducts a further re-direction of requests. Each processor in SWEB contains a scheduler and those processors collaborate with each others to exchange system load information. <p> The first experiment was run to determine how many requests per second SWEB could process. This depends on the average file sizes requested and the number of nodes. In <ref> [KB94] </ref>, it is reported that a high-end workstation running NCSA httpd could fulfill approximately 5 requests per second (rps). We examine how a one-node NCSA 16 httpd 1.3 server performs, and compare it with the 6-node SWEB on Meiko CS-2 and the 4-node SWEB on NOW.
Reference: [MC95] <author> R. McGrath, </author> <title> Performance of Several HTTP Demons on an HP 735 Workstation, </title> <address> http : ==hoohoo:ncsa:uiuc:edu=Inf ormationServers=P erf ormance=V 1:4=report:html, </address> <month> April 25, </month> <year> 1995. </year>
Reference: [ME94] <author> Meiko Corp., </author> <title> Computing Surface CS-2 Product Description, Meiko, </title> <address> Waltham, MA, </address> <year> 1994 </year>
Reference-contexts: Each node has a scalar processing unit (a 40Mhz SuperSparc chip) with 32MB of RAM running Solaris 2.3 <ref> [ME94] </ref>. We mainly use six CS-2 nodes, each of which is connected to a dedicated 1GB hard drive on which the test files reside. Disk service is available to all other nodes via NSF mounts These nodes are connected via a modified fat-tree network with a peak bandwidth of 40MB/s.
Reference: [NC95] <author> NCSA development team, </author> <title> The Common Gateway Interface, </title> <address> http : ==hoohoo:ncsa:uiuc:edu=cgi=, </address> <month> June, </month> <year> 1995. </year>
Reference: [NCH95] <institution> NCSA development team, The NCSA HTTPd Home Page, </institution> <address> http : ==hoohoo:ncsa:uiuc:edu=, </address> <month> June 13, </month> <year> 1995. </year>
Reference-contexts: Recently NCSA has come out with version 1.4 of their httpd software, which in many cases doubles the performance of previous versions by utilizing a pre-forking strategy <ref> [NCH95] </ref>. This technique can be incorporated in our system and will be beneficial. However we have not done so at this stage and in this paper we focus on alternative means of achieving the scalability through dynamic scheduling and request redirection. <p> As part of this issue, many of the Alexandria-specific functions need to be characterized in terms of cpu, disk, and network demands. We are also incorporating the pre-forking strategy <ref> [NCH95] </ref> into our scheme to further reduce the server overhead. Acknowledgments This work was supported in part by funding from NSF, ARPA, and NASA under NSF IRI94-11330 and NSF CCR-9409695, and a startup fund from University of California at Santa Barbara.
Reference: [SS95] <author> M.G. Siriram and Mukesh Singhai, </author> <title> Measures of the Potential for Load Sharing in Distributed Computing Systems, </title> <journal> IEEE Trans. on Software Engineering vol. </journal> <volume> 21, no. 5, </volume> <month> May, </month> <year> 1995, </year> <pages> pp. 468-475. </pages>
Reference: [ST90] <author> W. R. Stevens, </author> <title> UNIX Network Programming, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference: [SU95] <author> W. Shu and M. Y. Wu, </author> <title> An incremental parallel scheduling approach to solving dynamic and irregular problems. </title> <booktitle> Proceedings of Inter. Conf. on Parallel Processing, 1995. II. </booktitle> <pages> 143-150. </pages>
Reference: [WR93] <author> M. Willebeek-LeMair and A. Reeves, </author> <title> Strategies for Dynamic Load Balancing on Highly Parallel Computers, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 9, </volume> <month> September, </month> <year> 1993, </year> <pages> pp. 979-993. </pages>
Reference-contexts: Thirdly, we assume lossless document delivery, keeping consistency between the source library holding and what is sent to the client. Our dynamic scheduling scheme is closely related to the previous work on load balancing on distributed systems <ref> [ELZ86, BCS95, WR93] </ref>. In these studies, tasks arrivals may temporarily be uneven among processors and the goal of load balancing is to adjust the imbalance between processors by appropriately transferring tasks from overloaded processors to underloaded processors. The most well known techniques are called receiver-initiated and sender-initiated strategies. <p> In a single-faceted 11 scheduling system, a processor can be classified as lightly loaded and heavily loaded based on one parameter, e.g. CPU load. One purpose of such a classification is to update load information only when a classification changes to reduce unnecessary overhead, e.g. <ref> [ELZ86, WR93] </ref>. In our problem context, it is hard to classify a processor as heavily or lighted loaded since there are several load parameters. A processor could have a light CPU load but its local disk may receive many access requests from the network file system. <p> To avoid this unsynchronized overloading, we conservatively increase the CPU load of p x by . This strategy is found to be effective in <ref> [WR93] </ref>. We use = 30% in our system. * t net = # bytes required net bandwidth This term is used to estimate the time necessary to return the results back to the client over the network.
Reference: [JX93] <author> J. Ju, G. Xu, and K. Yang, </author> <title> An Intelligent Dynamic Load Balancer for Workstation Clusters IEEE Trans. </title> <journal> on Software Engineering vol. </journal> <volume> 21, no. 5, </volume> <month> May, </month> <year> 1995. </year>
Reference: [MZ95] <author> F.J. Muniz and E.J. </author> <title> Zaluska Parallel Load-balancing: An extension to the gradient model, </title> <journal> Parallel Computing,, </journal> <volume> vol 21, </volume> <year> 1995, </year> <pages> pp. 287-301. </pages>
Reference: [WS94] <author> G. Wright and W. R. </author> <title> Stevens, </title> <journal> TCP/IP Illustrated, </journal> <volume> Volume 2, </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994. </year> <month> 25 </month>
References-found: 24

