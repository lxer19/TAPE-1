URL: http://www.cis.ohio-state.edu/~gcao/paper/TPDS98.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~gcao/publications.html
Root-URL: 
Email: E-mail: fgcao, singhalg@cis.ohio-state.edu  
Title: On Coordinated Checkpointing in Distributed Systems  
Author: Guohong Cao and Mukesh Singhal 
Keyword: Index Terms: Distributed system, coordinated checkpointing, causal dependence, non blocking, consistent checkpoints.  
Address: Columbus, OH 43210  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: Coordinated checkpointing simplifies failure recovery and eliminates domino effects in case of failures by preserving a consistent global checkpoint on stable storage. However, the approach suffers from high overhead associated with the checkpointing process. Two approaches are used to reduce the overhead: first is to minimize the number of synchronization messages and the number of checkpoints; the other is to make the checkpointing process non-blocking. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. In other words, the Prakash-Singhal algorithm forces only a minimum number of processes to take checkpoints and it does not block the underlying computation. However, we found two problems in this algorithm. In this paper, we identify these problems and prove a more general result: there does not exist a non-blocking algorithm that forces only a minimum number of processes to take their checkpoints. Based on this general result, we propose an efficient algorithm that neither forces all processes to take checkpoints nor blocks the underlying computation during checkpointing. Also, we point out future research directions in designing coordinated checkpointing algorithms for distributed computing systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arup Acharya and B.R. Badrinath. </author> <title> "Checkpointing Distributed Applications on Mobil Computers". </title> <booktitle> the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: do f irst j := f irst j [ CP [j][i]:f irst; clear the forced checkpoint list. 19 4.4 An Example The basic idea of the algorithm can be better understood by the example presented in messages to P 1 , P 3 and P 4 , since R 2 <ref> [1] </ref> = 1; R 2 [3] = 1, and R 2 [4] = 1. When P 2 's request reaches P 4 , P 4 takes a checkpoint. Then, P 4 sends a computation message m3 to P 3 . <p> For example, in mobile computing systems, due to vulnerability of Mobile Hosts (M Hs) to catastrophic failures, e.g., loss, theft, or physical damage, the disk storage on an M H cannot be considered stable storage. A reasonable solution <ref> [1] </ref> is to utilize the stable storages at Mobile Support Stations (M SSs) to store 27 checkpoints of M Hs. Thus, to take a checkpoint, an M H has to transfer a large amount of data to its local M SS over the wireless network.
Reference: [2] <author> G. Barigazzi and L. Strigini. </author> <title> "Application-Transparent Setting of Recovery Points". </title> <booktitle> Digest of Papers FTCS-13, </booktitle> <pages> pages 48-55, </pages> <year> 1983. </year>
Reference-contexts: Similar to Case 2, we have a contradiction. 2 Theorem 3 The checkpointing algorithm terminates within a finite time. Proof. The proof is similar to [18]. 2 6 Related Work The first coordinated checkpointing algorithm was presented in <ref> [2] </ref>. However, it assumes that all communications between processes are atomic, which is too restrictive. The Koo-Toueg algorithm [10] relaxes this assumption. In this algorithm, only those processes that have communicated with the checkpoint initiator either directly or indirectly since the last checkpoint need to take new checkpoints. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [3] <author> B. Bhargava, S.R. Lian, and P.J. Leu. </author> <title> "Experimental Evaluation of Concurrent Check-pointing and Rollback-Recovery Algorithms". </title> <booktitle> Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pages 182-189, </pages> <year> 1990. </year>
Reference-contexts: Checkpointing includes the time to trace the dependence tree and to save the states of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically degrade system performance <ref> [3, 7] </ref>. Recently, nonblocking algorithms [7, 20] have received considerable attention. In these algorithms, processes need not block during checkpointing by using a checkpointing sequence number to identify orphan messages. However, these algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. <p> f irst j [ CP [j][i]:f irst; clear the forced checkpoint list. 19 4.4 An Example The basic idea of the algorithm can be better understood by the example presented in messages to P 1 , P 3 and P 4 , since R 2 [1] = 1; R 2 <ref> [3] </ref> = 1, and R 2 [4] = 1. When P 2 's request reaches P 4 , P 4 takes a checkpoint. Then, P 4 sends a computation message m3 to P 3 . <p> Moreover, based on the empirical study of <ref> [3, 7] </ref>, blocking the underlying computation may dramatically reduce the system performance. Thus, our non-blocking algorithm is much more efficient than blocking algorithms for a network of workstations. Coordinated checkpointing algorithms suffer from the synchronization overhead associ 20 ated with the checkpointing process. <p> Thus, our non-blocking algorithm is much more efficient than blocking algorithms for a network of workstations. Coordinated checkpointing algorithms suffer from the synchronization overhead associ 20 ated with the checkpointing process. During the last decade, the communication overhead far exceeded the overhead of accessing stable storage <ref> [3] </ref>. Furthermore, the memory available to run processes tended to be small. These tradeoffs naturally favored uncoordinated check-pointing schemes over coordinated checkpointing schemes. In modern systems, the overhead of coordinating checkpoints is negligible compared to the overhead of saving the states [7, 15]. <p> Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance <ref> [3, 7] </ref>. The Chandy-Lamport algorithm [4] is the earliest non-blocking algorithm for coordinated checkpointing. However, in their algorithm, system messages (markers) are sent along all channels in the network during checkpointing. This leads to a message complexity of O (n 2 ).
Reference: [4] <author> K.M. Chandy and L. Lamport. </author> <title> "Distributed Snapshots: Determining Global States of Distributed Systems". </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1985. </year>
Reference-contexts: When P 2 's request reaches P 4 , P 4 takes a checkpoint. Then, P 4 sends message m3 to P 3 . When m3 arrives at P 3 , P 3 takes a checkpoint before processing it due to m3:csn &gt; csn 3 <ref> [4] </ref>. For the same reason, P 1 takes a checkpoint before processing m2. P 0 has not communicated with other processes before it takes a local checkpoint. Later, it sends a computation message m1 to P 1 . <p> irst; clear the forced checkpoint list. 19 4.4 An Example The basic idea of the algorithm can be better understood by the example presented in messages to P 1 , P 3 and P 4 , since R 2 [1] = 1; R 2 [3] = 1, and R 2 <ref> [4] </ref> = 1. When P 2 's request reaches P 4 , P 4 takes a checkpoint. Then, P 4 sends a computation message m3 to P 3 . When m3 arrives at P 3 , P 3 takes a forced checkpoint before processing it because m3:csn &gt; csn 3 [4] <p> <ref> [4] </ref> = 1. When P 2 's request reaches P 4 , P 4 takes a checkpoint. Then, P 4 sends a computation message m3 to P 3 . When m3 arrives at P 3 , P 3 takes a forced checkpoint before processing it because m3:csn &gt; csn 3 [4] and P 3 has sent a message during the current checkpoint interval. For the same reason, P 1 takes a forced checkpoint before processing m2. P 0 has not communicated with other processes before it takes a local checkpoint. <p> Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. The Chandy-Lamport algorithm <ref> [4] </ref> is the earliest non-blocking algorithm for coordinated checkpointing. However, in their algorithm, system messages (markers) are sent along all channels in the network during checkpointing. This leads to a message complexity of O (n 2 ). <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints [2, 5, 6, 9, 10, 13, 19]; the other is to make check-pointing non-blocking <ref> [4, 7, 11, 20] </ref>. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies. Therefore, our algorithm is the first correct algorithm to combine these two approaches.
Reference: [5] <author> F. Cristian and F. Jahanian. </author> <title> "A timestamp-based Checkpointing Protocol for long-lived Distributed Computations". </title> <booktitle> Proc. of IEEE Symp. Reliable Distributed Syst., </booktitle> <pages> pages 12-20, </pages> <year> 1991. </year>
Reference-contexts: Kim and Park [9] proposed an improved scheme that allows the new checkpoints in some subtrees to be committed while the others are aborted. To further reduce the system messages needed to synchronize the checkpointing, loosely synchronous clocks <ref> [5, 19] </ref> are used. More specifically, loosely-synchronized checkpoint clocks can trigger the local checkpointing actions of all participating processes at approximately the same time without the need of broadcasting the checkpoint request by an initiator. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [6] <author> Y. Deng and E.K. Park. </author> <title> "Checkpointing and Rollback-Recovery Algorithms in Dis--tributed Systems". </title> <journal> Journal of Systems and Software, </journal> <pages> pages 59-71, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: When a process takes a checkpoint, it asks (by sending checkpoint requests to) all relevant processes to take checkpoints. Therefore, coordinated checkpointing suffers from high overhead associated with the checkpointing process. Much of the previous work <ref> [6, 9, 10, 13] </ref> in coordinated checkpointing has focused on minimizing the number of synchronization messages and the number of checkpoints during checkpointing. However, these algorithms (called blocking algorithms) force all relevant processes in the system to block their computations during checkpointing. <p> As a result, only P 1 , P 2 , and P 3 take new checkpoints. P 4 and P 5 continues their computation without taking new checkpoints. 2.3 Basic Ideas Behind Non-blocking Algorithms Most of the existing coordinated checkpointing algorithms <ref> [6, 10, 13] </ref> rely on the two-phase protocol and save two kinds of checkpoints on stable storage: tentative and permanent. In the first phase, the initiator takes a tentative checkpoint and forces all relevant processes to take tentative checkpoints. <p> However, these two algorithms [10, 13] assume a complex scheme (such as slide window) to deal with the message loss problem and 24 do not consider lost messages in checkpointing and recovery. Deng and Park <ref> [6] </ref> proposed an algorithm to address both orphan message and lost messages. In Koo and Toueg's algorithm [10], if any of the involved processes is not able to or not willing to take a checkpoint, the entire checkpointing process is aborted. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [7] <author> E.N. Elnozahy, D.B. Johnson, and W. Zwaenepoel. </author> <title> "The Performance of Consistent Checkpointing". </title> <booktitle> Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 86-95, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Checkpointing includes the time to trace the dependence tree and to save the states of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically degrade system performance <ref> [3, 7] </ref>. Recently, nonblocking algorithms [7, 20] have received considerable attention. In these algorithms, processes need not block during checkpointing by using a checkpointing sequence number to identify orphan messages. However, these algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. <p> Checkpointing includes the time to trace the dependence tree and to save the states of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically degrade system performance [3, 7]. Recently, nonblocking algorithms <ref> [7, 20] </ref> have received considerable attention. In these algorithms, processes need not block during checkpointing by using a checkpointing sequence number to identify orphan messages. However, these algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. <p> Therefore, blocking algorithms may dramatically degrade system performance [3, 7]. Recently, nonblocking algorithms <ref> [7, 20] </ref> have received considerable attention. In these algorithms, processes need not block during checkpointing by using a checkpointing sequence number to identify orphan messages. However, these algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. Therefore, they suffer from the disadvantages of centralized algorithms, such as poor reliability, bottleneck, etc. Moreover, 2 these algorithms [7, 20] require all processes in the system to take checkpoints during check--pointing, even though many of them may <p> However, these algorithms <ref> [7, 20] </ref> assume that a distinguished initiator decides when to take a checkpoint. Therefore, they suffer from the disadvantages of centralized algorithms, such as poor reliability, bottleneck, etc. Moreover, 2 these algorithms [7, 20] require all processes in the system to take checkpoints during check--pointing, even though many of them may not be necessary. <p> Suppose P 3 processes m1 before it receives the checkpoint request from P 2 . When P 3 receives the checkpoint request from P 2 , it takes a checkpoint (see Figure 2). In this case, m1 becomes an orphan. Most non-blocking algorithms <ref> [7, 20] </ref> use a Checkpoint Sequence Number (csn) to avoid inconsistencies. In these algorithms, a process is forced to take a checkpoint if it receives a computation message whose csn is greater than its local csn. <p> Moreover, based on the empirical study of <ref> [3, 7] </ref>, blocking the underlying computation may dramatically reduce the system performance. Thus, our non-blocking algorithm is much more efficient than blocking algorithms for a network of workstations. Coordinated checkpointing algorithms suffer from the synchronization overhead associ 20 ated with the checkpointing process. <p> Furthermore, the memory available to run processes tended to be small. These tradeoffs naturally favored uncoordinated check-pointing schemes over coordinated checkpointing schemes. In modern systems, the overhead of coordinating checkpoints is negligible compared to the overhead of saving the states <ref> [7, 15] </ref>. Using concurrent and incremental checkpointing [7], the overhead of either coordinated or uncoordinated checkpointing is essentially the same. Therefore, uncoordinated checkpoint-ing is not likely to be an attractive technique in practice given the negligible performance gains. <p> Furthermore, the memory available to run processes tended to be small. These tradeoffs naturally favored uncoordinated check-pointing schemes over coordinated checkpointing schemes. In modern systems, the overhead of coordinating checkpoints is negligible compared to the overhead of saving the states [7, 15]. Using concurrent and incremental checkpointing <ref> [7] </ref>, the overhead of either coordinated or uncoordinated checkpointing is essentially the same. Therefore, uncoordinated checkpoint-ing is not likely to be an attractive technique in practice given the negligible performance gains. <p> Thus, our coordinated checkpointing algorithm has many advantages over uncoordinated checkpointing algorithms. Due to technological advancements, the synchronization cost is negligible compared to the overhead of saving the states on stable storage <ref> [7, 15] </ref>. Stable storage for checkpoints is provided by a highly available network file server. Checkpoints cannot be saved on a local disk or in local volatile or nonvolatile memory since that will make them inaccessible during an extended outage of the local machine. <p> Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance <ref> [3, 7] </ref>. The Chandy-Lamport algorithm [4] is the earliest non-blocking algorithm for coordinated checkpointing. However, in their algorithm, system messages (markers) are sent along all channels in the network during checkpointing. This leads to a message complexity of O (n 2 ). <p> However, this scheme requires each process to log each message sent, which may introduce some performance degradation and require the system to be deterministic. The Elnozahy-Johnson-Zwaenepoel algorithm <ref> [7] </ref> uses a checkpoint sequence number to identify orphan messages, thus avoiding the need for processes to be blocked during check-pointing. However, this approach requires all processes to take checkpoints during check-pointing. The algorithm proposed by Silva and Silva [20] uses a similar idea as [7] except that the processes which <p> The Elnozahy-Johnson-Zwaenepoel algorithm <ref> [7] </ref> uses a checkpoint sequence number to identify orphan messages, thus avoiding the need for processes to be blocked during check-pointing. However, this approach requires all processes to take checkpoints during check-pointing. The algorithm proposed by Silva and Silva [20] uses a similar idea as [7] except that the processes which did not communicate with others during the previous checkpoint interval do not need to take new checkpoints. Both algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. <p> However, this approach requires all processes to take checkpoints during check-pointing. The algorithm proposed by Silva and Silva [20] uses a similar idea as [7] except that the processes which did not communicate with others during the previous checkpoint interval do not need to take new checkpoints. Both algorithms <ref> [7, 20] </ref> assume that a distinguished initiator decides when to take a checkpoint. Therefore, they suffer from the disadvantages of centralized algorithms such as one-site failure, traffic bottle-neck, etc. Moreover, their algorithms require almost all processes to take checkpoints, even though many of them are unnecessary. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints [2, 5, 6, 9, 10, 13, 19]; the other is to make check-pointing non-blocking <ref> [4, 7, 11, 20] </ref>. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies. Therefore, our algorithm is the first correct algorithm to combine these two approaches.
Reference: [8] <author> S.T. Huang. </author> <title> "Detecting Termination of Distributed Computations by External Agents". </title> <booktitle> Proceedings of the 9th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 79-84, </pages> <year> 1989. </year>
Reference-contexts: Note that csn i [i] is the checkpoint sequence number of P i . weight: a non-negative variable of type real with maximum value of 1. It is used to detect the termination of the checkpointing algorithm as in <ref> [8] </ref>. f irst i : a boolean array of size n maintained by each process P i . The array is initialized to all zeroes each time a checkpoint at that process is taken.
Reference: [9] <author> J.L. Kim and T. Park. </author> <title> "An Efficient Protocol For Checkpointing Recovery in Distributed Systems". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 955-960, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: When a process takes a checkpoint, it asks (by sending checkpoint requests to) all relevant processes to take checkpoints. Therefore, coordinated checkpointing suffers from high overhead associated with the checkpointing process. Much of the previous work <ref> [6, 9, 10, 13] </ref> in coordinated checkpointing has focused on minimizing the number of synchronization messages and the number of checkpoints during checkpointing. However, these algorithms (called blocking algorithms) force all relevant processes in the system to block their computations during checkpointing. <p> Deng and Park [6] proposed an algorithm to address both orphan message and lost messages. In Koo and Toueg's algorithm [10], if any of the involved processes is not able to or not willing to take a checkpoint, the entire checkpointing process is aborted. Kim and Park <ref> [9] </ref> proposed an improved scheme that allows the new checkpoints in some subtrees to be committed while the others are aborted. To further reduce the system messages needed to synchronize the checkpointing, loosely synchronous clocks [5, 19] are used. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [10] <author> R. Koo and S. Toueg. </author> <title> "Checkpointing and Rollback-Recovery for Distributed Systems". </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 23-31, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: A system state is said to be consistent if it contains no orphan message; i.e., a message whose receive event is recorded in the state of the destination process, but its send event is lost <ref> [10, 22] </ref>. In order to record a consistent global checkpoint on stable storage, processes must synchronize their checkpointing activities. When a process takes a checkpoint, it asks (by sending checkpoint requests to) all relevant processes to take checkpoints. Therefore, coordinated checkpointing suffers from high overhead associated with the checkpointing process. <p> When a process takes a checkpoint, it asks (by sending checkpoint requests to) all relevant processes to take checkpoints. Therefore, coordinated checkpointing suffers from high overhead associated with the checkpointing process. Much of the previous work <ref> [6, 9, 10, 13] </ref> in coordinated checkpointing has focused on minimizing the number of synchronization messages and the number of checkpoints during checkpointing. However, these algorithms (called blocking algorithms) force all relevant processes in the system to block their computations during checkpointing. <p> As a result, only P 1 , P 2 , and P 3 take new checkpoints. P 4 and P 5 continues their computation without taking new checkpoints. 2.3 Basic Ideas Behind Non-blocking Algorithms Most of the existing coordinated checkpointing algorithms <ref> [6, 10, 13] </ref> rely on the two-phase protocol and save two kinds of checkpoints on stable storage: tentative and permanent. In the first phase, the initiator takes a tentative checkpoint and forces all relevant processes to take tentative checkpoints. <p> For any process, after it takes a checkpoint, it recursively forces all dependent processes to take checkpoints. The Koo-Toueg algorithm <ref> [10] </ref> uses this scheme, and it has been proved [10] that this algorithm forces only a minimum number of processes to take checkpoints. In the following, we prove that the Koo-Toueg algorithm is a min-process algorithm and a min-process algorithm forces only a minimum number of processes to take checkpoints. <p> For any process, after it takes a checkpoint, it recursively forces all dependent processes to take checkpoints. The Koo-Toueg algorithm <ref> [10] </ref> uses this scheme, and it has been proved [10] that this algorithm forces only a minimum number of processes to take checkpoints. In the following, we prove that the Koo-Toueg algorithm is a min-process algorithm and a min-process algorithm forces only a minimum number of processes to take checkpoints. <p> Proposition 2 P p ` j fl j P p i P q =) P p ` i P q Lemma 1 An algorithm forces only a minimum number of processes to take checkpoints if and only if it is a min-process algorithm. Proof. It has been proved <ref> [10] </ref> that the Koo-Toueg algorithm forces only a minimum number of processes to take checkpoints; thus, we only need to prove the following: in [10], when a process P p initiates a new checkpointing process and takes a checkpoint C p;i , a process P q takes a checkpoint C q;j <p> Proof. It has been proved <ref> [10] </ref> that the Koo-Toueg algorithm forces only a minimum number of processes to take checkpoints; thus, we only need to prove the following: in [10], when a process P p initiates a new checkpointing process and takes a checkpoint C p;i , a process P q takes a checkpoint C q;j associated with C p;i if and only if P q fl j1 Necessity: In [10], when a process P p initiates a new checkpoint <p> thus, we only need to prove the following: in <ref> [10] </ref>, when a process P p initiates a new checkpointing process and takes a checkpoint C p;i , a process P q takes a checkpoint C q;j associated with C p;i if and only if P q fl j1 Necessity: In [10], when a process P p initiates a new checkpoint C p;i , it recursively asks all dependent processes to take checkpoints. For example, P p asks P k m to take a checkpoint, P k m asks P k m1 to take a checkpoint, and so on. <p> Proof. The proof is similar to [18]. 2 6 Related Work The first coordinated checkpointing algorithm was presented in [2]. However, it assumes that all communications between processes are atomic, which is too restrictive. The Koo-Toueg algorithm <ref> [10] </ref> relaxes this assumption. In this algorithm, only those processes that have communicated with the checkpoint initiator either directly or indirectly since the last checkpoint need to take new checkpoints. Thus, it reduces the number of synchronization messages and the number of checkpoints. <p> Thus, it reduces the number of synchronization messages and the number of checkpoints. Later, Leu and Bhargava [13] presented an algorithm, which is resilient to multiple process failures. Also, this algorithm does not assume that the channel is F IF O, which is necessary in <ref> [10] </ref>. However, these two algorithms [10, 13] assume a complex scheme (such as slide window) to deal with the message loss problem and 24 do not consider lost messages in checkpointing and recovery. Deng and Park [6] proposed an algorithm to address both orphan message and lost messages. <p> Later, Leu and Bhargava [13] presented an algorithm, which is resilient to multiple process failures. Also, this algorithm does not assume that the channel is F IF O, which is necessary in [10]. However, these two algorithms <ref> [10, 13] </ref> assume a complex scheme (such as slide window) to deal with the message loss problem and 24 do not consider lost messages in checkpointing and recovery. Deng and Park [6] proposed an algorithm to address both orphan message and lost messages. <p> Deng and Park [6] proposed an algorithm to address both orphan message and lost messages. In Koo and Toueg's algorithm <ref> [10] </ref>, if any of the involved processes is not able to or not willing to take a checkpoint, the entire checkpointing process is aborted. Kim and Park [9] proposed an improved scheme that allows the new checkpoints in some subtrees to be committed while the others are aborted. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [11] <author> T.H. Lai and T.H. Yang. </author> <title> "On Distributed Snapshots". </title> <journal> Information Processing Letters, </journal> <pages> pages 153-158, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: This leads to a message complexity of O (n 2 ). Moreover, it requires all processes to take checkpoints and the channel must be F IF O. To relax the F IF O assumption, Lai and Yang <ref> [11] </ref> proposed another algorithm. In their algorithm, when a process takes a checkpoint, it piggybacks a checkpoint request (a f lag) to the messages it sends out from each channel. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints [2, 5, 6, 9, 10, 13, 19]; the other is to make check-pointing non-blocking <ref> [4, 7, 11, 20] </ref>. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies. Therefore, our algorithm is the first correct algorithm to combine these two approaches.
Reference: [12] <author> L. Lamport. </author> <title> "Time, Clocks and Ordering of Events in Distributed Systems". </title> <journal> Communication of the ACM, </journal> <month> July </month> <year> 1978. </year>
Reference-contexts: Thus, the sending of m is recorded at P j . 1 ! is the "happened before" relation described in <ref> [12] </ref> 23 Case 2: P j 's last checkpoint is taken due to a request from a process P k , k 6= i. According to the assumption, P j sends m after taking its local checkpoint which is triggered by P k .
Reference: [13] <author> P.Y. Leu and B. Bhargava. </author> <title> "Concurrent Robust Checkpointing and Recovery in Distributed Systems". Pro. </title> <booktitle> 4th IEEE Int. Conf. on Data Eng., </booktitle> <pages> pages 154-163, </pages> <year> 1988. </year>
Reference-contexts: When a process takes a checkpoint, it asks (by sending checkpoint requests to) all relevant processes to take checkpoints. Therefore, coordinated checkpointing suffers from high overhead associated with the checkpointing process. Much of the previous work <ref> [6, 9, 10, 13] </ref> in coordinated checkpointing has focused on minimizing the number of synchronization messages and the number of checkpoints during checkpointing. However, these algorithms (called blocking algorithms) force all relevant processes in the system to block their computations during checkpointing. <p> As a result, only P 1 , P 2 , and P 3 take new checkpoints. P 4 and P 5 continues their computation without taking new checkpoints. 2.3 Basic Ideas Behind Non-blocking Algorithms Most of the existing coordinated checkpointing algorithms <ref> [6, 10, 13] </ref> rely on the two-phase protocol and save two kinds of checkpoints on stable storage: tentative and permanent. In the first phase, the initiator takes a tentative checkpoint and forces all relevant processes to take tentative checkpoints. <p> In this algorithm, only those processes that have communicated with the checkpoint initiator either directly or indirectly since the last checkpoint need to take new checkpoints. Thus, it reduces the number of synchronization messages and the number of checkpoints. Later, Leu and Bhargava <ref> [13] </ref> presented an algorithm, which is resilient to multiple process failures. Also, this algorithm does not assume that the channel is F IF O, which is necessary in [10]. <p> Later, Leu and Bhargava [13] presented an algorithm, which is resilient to multiple process failures. Also, this algorithm does not assume that the channel is F IF O, which is necessary in [10]. However, these two algorithms <ref> [10, 13] </ref> assume a complex scheme (such as slide window) to deal with the message loss problem and 24 do not consider lost messages in checkpointing and recovery. Deng and Park [6] proposed an algorithm to address both orphan message and lost messages. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [14] <author> D. Manivannan, R. Netzer, and Mukesh Singhal. </author> <title> "Finding Consistent Global Checkpoints in a Distributed Computation". </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 623-627, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In his work, a graph called rollback dependence graph (or "R-graph") can be used to quickly find zigzag paths. Based on R-graph, Wang developed efficient algorithms to calculate the maximum and the minimum consistent global checkpoints for both general nondeterministic executions and piecewise deterministic executions. Manivannan et al. <ref> [14] </ref> generalized this problem. They proposed an algorithm to enumerate all such consistent global checkpoints that contain a target set of checkpoints and showed how the minimum and the maximum checkpoints are 11 special cases. Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag <p> Manivannan et al. <ref> [14] </ref> generalized this problem. They proposed an algorithm to enumerate all such consistent global checkpoints that contain a target set of checkpoints and showed how the minimum and the maximum checkpoints are 11 special cases. Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works [14, 23, 24] are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works <ref> [14, 23, 24] </ref> are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works <ref> [14, 23, 24] </ref> are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing.
Reference: [15] <author> G. Muller, M. Hue, and N. Peyrouz. </author> <title> "Performance of Consistent Checkpointing in a modular Operating System: Results of the FTM Experiment". </title> <booktitle> Lecture Notes in Computer Science: Dependable Computing-EDCC-1, </booktitle> <pages> pages 491-508, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Furthermore, the memory available to run processes tended to be small. These tradeoffs naturally favored uncoordinated check-pointing schemes over coordinated checkpointing schemes. In modern systems, the overhead of coordinating checkpoints is negligible compared to the overhead of saving the states <ref> [7, 15] </ref>. Using concurrent and incremental checkpointing [7], the overhead of either coordinated or uncoordinated checkpointing is essentially the same. Therefore, uncoordinated checkpoint-ing is not likely to be an attractive technique in practice given the negligible performance gains. <p> Thus, our coordinated checkpointing algorithm has many advantages over uncoordinated checkpointing algorithms. Due to technological advancements, the synchronization cost is negligible compared to the overhead of saving the states on stable storage <ref> [7, 15] </ref>. Stable storage for checkpoints is provided by a highly available network file server. Checkpoints cannot be saved on a local disk or in local volatile or nonvolatile memory since that will make them inaccessible during an extended outage of the local machine.
Reference: [16] <author> R.H.B. Netzer and Jian Xu. </author> <title> "Necessary and Sufficient Conditions for Consistent Global Snapshots". </title> <journal> IEEE Transactions on Parallel and Distributed System, </journal> <pages> pages 165-169, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: We also assume that each process P p takes an initial checkpoint C p;0 immediately before execution begins and ends with a virtual checkpoint that represents the last state attained before termination <ref> [16] </ref>. <p> Therefore, no non-blocking min-process algorithm exists. Corollary 1 there does not exist a non-blocking algorithm that forces only a minimum number of processes to take their checkpoints. Proof. The proof directly follows from Lemma 1 and Theorem 1. Remarks Netzer and Xu <ref> [16] </ref> introduced the concept of "zigzag" paths to define the necessary and sufficient conditions for a set of local checkpoints to lie on a consistent global checkpoint. Our definition of "z-dependence" captures the essence of zigzag paths. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works [14, 23, 24] are based on zigzag paths. Similar to <ref> [16] </ref>, these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing.
Reference: [17] <author> Ravi Prakash and Mukesh Singhal. </author> <title> "Maximal Global Snapshot with Concurrent Initiators". </title> <booktitle> Proceedings of the Sixth IEEE symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 344-351, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Techniques to handle concurrent initiations of checkpointing by multiple processes can be found in <ref> [17] </ref>. As multiple concurrent initiations of checkpointing is orthogonal to our discussion, we only briefly mention the main features of [17]. When a process receives the first request for checkpointing initiated by another process, it takes a local checkpoint and propagates the request. <p> Techniques to handle concurrent initiations of checkpointing by multiple processes can be found in <ref> [17] </ref>. As multiple concurrent initiations of checkpointing is orthogonal to our discussion, we only briefly mention the main features of [17]. When a process receives the first request for checkpointing initiated by another process, it takes a local checkpoint and propagates the request. All local checkpoints taken by the participating processes for a checkpoint initiation collectively form a global checkpoint. The state information collected by each independent checkpointing is combined.
Reference: [18] <author> Ravi Prakash and Mukesh Singhal. </author> <title> "Low-Cost Checkpointing and Failure Recovery in Mobile Computing Systems". </title> <journal> IEEE Transactions on Parallel and Distributed System, </journal> <pages> pages 1035-1048, </pages> <month> October </month> <year> 1996. </year> <month> 29 </month>
Reference-contexts: If each process can initiate a checkpointing process, the network would be flooded with control messages and processes might waste their time taking unnecessary checkpoints. Prakash-Singhal algorithm <ref> [18] </ref> was the first algorithm to combine these two approaches. More specifically, it forces only a minimum number of processes to take checkpoints and does not block the underlying computation during checkpointing. However, we found two problems in this algorithm. <p> denotes all the computation performed between its i th and (i + 1) th checkpoint, including the i th checkpoint but not the (i + 1) th checkpoint. 2.2 Minimizing Dependence Information In order to record the dependence relationship among processes, we use a similar approach as the Prakash-Singhal algorithm <ref> [18] </ref>, where each process P i maintains a boolean vector R i , which has n bits. R i [j] = 1 represents that P i depends on P j . At P i , the vector is initialized to 0 except P i [i]. <p> When P 3 receives m1, it takes a checkpoint before processing m1 since the csn appended to m1 is larger than its local csn. This scheme works only when every process in the computation can receive each checkpoint request and then increases its own csn. Since the Prakash-Singhal algorithm <ref> [18] </ref> forces only a subset of processes to take checkpoints, the csn of some processes may be out-of-date, and may not be able to avoid inconsistencies. <p> Similar to Case 2, we have a contradiction. 2 Theorem 3 The checkpointing algorithm terminates within a finite time. Proof. The proof is similar to <ref> [18] </ref>. 2 6 Related Work The first coordinated checkpointing algorithm was presented in [2]. However, it assumes that all communications between processes are atomic, which is too restrictive. The Koo-Toueg algorithm [10] relaxes this assumption. <p> These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm <ref> [18] </ref> combined them. However, their algorithm has some problems and may result in inconsistencies. Therefore, our algorithm is the first correct algorithm to combine these two approaches. <p> Based on this innovative idea, our non-blocking algorithm avoids the avalanche effect and significantly reduces the number of checkpoints. 3. We identified two problems in the Prakash-Singhal algorithm <ref> [18] </ref>. From Theorem 1, no non-blocking min-process algorithm exists. This implies that there are three directions in designing efficient coordinated checkpointing algorithms.
Reference: [19] <author> P. Ramanathan and K.G. Shin. </author> <title> "Use of Common Time Base for Checkpointing and Rollback Recovery in a Distributed System". </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 571-583, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Kim and Park [9] proposed an improved scheme that allows the new checkpoints in some subtrees to be committed while the others are aborted. To further reduce the system messages needed to synchronize the checkpointing, loosely synchronous clocks <ref> [5, 19] </ref> are used. More specifically, loosely-synchronized checkpoint clocks can trigger the local checkpointing actions of all participating processes at approximately the same time without the need of broadcasting the checkpoint request by an initiator. <p> However, a process taking a checkpoint needs to wait for a period that equals the sum of the maximum deviation between clocks and the maximum time to detect a failure in another process in the system. All the above coordinated checkpointing algorithms <ref> [2, 5, 6, 9, 10, 13, 19] </ref> require processes to be blocked during checkpointing. Checkpointing includes the time to trace the dependence tree and to save the state of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically reduce system performance [3, 7]. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints <ref> [2, 5, 6, 9, 10, 13, 19] </ref>; the other is to make check-pointing non-blocking [4, 7, 11, 20]. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies.
Reference: [20] <author> L.M. Silva and J.G. Silva. </author> <title> "Global Checkpointing for Distributed Programs". </title> <booktitle> Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 155-162, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Checkpointing includes the time to trace the dependence tree and to save the states of processes on stable storage, which may be long. Therefore, blocking algorithms may dramatically degrade system performance [3, 7]. Recently, nonblocking algorithms <ref> [7, 20] </ref> have received considerable attention. In these algorithms, processes need not block during checkpointing by using a checkpointing sequence number to identify orphan messages. However, these algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. <p> Therefore, blocking algorithms may dramatically degrade system performance [3, 7]. Recently, nonblocking algorithms <ref> [7, 20] </ref> have received considerable attention. In these algorithms, processes need not block during checkpointing by using a checkpointing sequence number to identify orphan messages. However, these algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. Therefore, they suffer from the disadvantages of centralized algorithms, such as poor reliability, bottleneck, etc. Moreover, 2 these algorithms [7, 20] require all processes in the system to take checkpoints during check--pointing, even though many of them may <p> However, these algorithms <ref> [7, 20] </ref> assume that a distinguished initiator decides when to take a checkpoint. Therefore, they suffer from the disadvantages of centralized algorithms, such as poor reliability, bottleneck, etc. Moreover, 2 these algorithms [7, 20] require all processes in the system to take checkpoints during check--pointing, even though many of them may not be necessary. <p> Suppose P 3 processes m1 before it receives the checkpoint request from P 2 . When P 3 receives the checkpoint request from P 2 , it takes a checkpoint (see Figure 2). In this case, m1 becomes an orphan. Most non-blocking algorithms <ref> [7, 20] </ref> use a Checkpoint Sequence Number (csn) to avoid inconsistencies. In these algorithms, a process is forced to take a checkpoint if it receives a computation message whose csn is greater than its local csn. <p> The Elnozahy-Johnson-Zwaenepoel algorithm [7] uses a checkpoint sequence number to identify orphan messages, thus avoiding the need for processes to be blocked during check-pointing. However, this approach requires all processes to take checkpoints during check-pointing. The algorithm proposed by Silva and Silva <ref> [20] </ref> uses a similar idea as [7] except that the processes which did not communicate with others during the previous checkpoint interval do not need to take new checkpoints. Both algorithms [7, 20] assume that a distinguished initiator decides when to take a checkpoint. <p> However, this approach requires all processes to take checkpoints during check-pointing. The algorithm proposed by Silva and Silva [20] uses a similar idea as [7] except that the processes which did not communicate with others during the previous checkpoint interval do not need to take new checkpoints. Both algorithms <ref> [7, 20] </ref> assume that a distinguished initiator decides when to take a checkpoint. Therefore, they suffer from the disadvantages of centralized algorithms such as one-site failure, traffic bottle-neck, etc. Moreover, their algorithms require almost all processes to take checkpoints, even though many of them are unnecessary. <p> All the above algorithms follow two approaches to reduce the overhead associated with coordinated checkpointing algorithms: one is to minimize the number of synchronization messages and the number of checkpoints [2, 5, 6, 9, 10, 13, 19]; the other is to make check-pointing non-blocking <ref> [4, 7, 11, 20] </ref>. These two approaches were orthogonal in previous years until the Prakash-Singhal algorithm [18] combined them. However, their algorithm has some problems and may result in inconsistencies. Therefore, our algorithm is the first correct algorithm to combine these two approaches.
Reference: [21] <author> M. Spezialetti and P. Kearns. </author> <title> "Efficient Distributed Snapshots". </title> <booktitle> Proceedings of the 6 th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 382-388, </pages> <year> 1986. </year>
Reference-contexts: The combination is driven by the fact that the union of consistent global checkpoints is also a consistent global checkpoint. The checkpoints thus generated are more recent than each of the checkpoints collected independently, and also more recent than that collected by <ref> [21] </ref>. Therefore, the amount of computation lost during rollback, after process failures, is minimized. Next, we present our non-blocking algorithm. Checkpointing initiation: Any process can initiate a checkpointing process.
Reference: [22] <author> R.E. Strom and S.A. Yemini. </author> <title> "Optimistic Recovery In Distributed Systems". </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: A system state is said to be consistent if it contains no orphan message; i.e., a message whose receive event is recorded in the state of the destination process, but its send event is lost <ref> [10, 22] </ref>. In order to record a consistent global checkpoint on stable storage, processes must synchronize their checkpointing activities. When a process takes a checkpoint, it asks (by sending checkpoint requests to) all relevant processes to take checkpoints. Therefore, coordinated checkpointing suffers from high overhead associated with the checkpointing process.
Reference: [23] <author> Y. Wang. </author> <title> "Maximum and Minimum consistent Global Checkpoints and Their Application". </title> <booktitle> The 14 th IEEE Symp. on Reliable Distributed Systems, </booktitle> <pages> pages 86-95, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Based on z-dependence, we found and proved the impossibility result. It is impossible to prove the result only based on zigzag paths. Wang <ref> [23, 24] </ref> considered the problem of constructing the maximum and the minimum consistent global checkpoints that contain a target set of checkpoints. In his work, a graph called rollback dependence graph (or "R-graph") can be used to quickly find zigzag paths. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works <ref> [14, 23, 24] </ref> are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works <ref> [14, 23, 24] </ref> are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing.
Reference: [24] <author> Y. Wang. </author> <title> "Consistent Global Checkpoints that Contain a Given Set of Local Checkpoints". </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 456-468, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Based on z-dependence, we found and proved the impossibility result. It is impossible to prove the result only based on zigzag paths. Wang <ref> [23, 24] </ref> considered the problem of constructing the maximum and the minimum consistent global checkpoints that contain a target set of checkpoints. In his work, a graph called rollback dependence graph (or "R-graph") can be used to quickly find zigzag paths. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works <ref> [14, 23, 24] </ref> are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing. <p> Their work [14] is based on a concept called "Z-cone", which is a generalization of zigzag paths. All these works <ref> [14, 23, 24] </ref> are based on zigzag paths. Similar to [16], these works [14, 23, 24] are not useful in coordinated checkpointing since checkpoints are guaranteed to be consistent in coordinated checkpointing.

References-found: 24

