URL: ftp://cse.ogi.edu/pub/chcc/oviatt/icslp94/icslp94paper.ps
Refering-URL: http://www.cse.ogi.edu/CHCC/Publications/text.html
Root-URL: http://www.cse.ogi.edu
Title: INTEGRATION THEMES IN MULTIMODAL HUMAN-COMPUTER INTERACTION  
Author: Sharon Oviatt and Erik Olsen 
Note: U. S. A.  
Address: 333 Ravenswood Avenue, Menlo Park, CA.,  94025  
Affiliation: Computer Dialogue Laboratory Artificial Intelligence Center SRI International,  
Abstract: This research examines how people integrate spoken and written input during multimodal human-computer interaction. Three studies used a semi-automatic simulation technique to collect data on people's free use of spoken and written input. Within-subject repeated-measures studies were designed, with data analyzed from 44 subjects and 240 tasks. The primary factors that govern people's selection to write versus speak at given points during a human-computer exchange were evaluated. Analyses revealed that people write digits more often than textual content, and proper names more often than other text. A form-based presentation, in comparison with an unconstrained format, also increased the likelihood of writing. However, the most influential factor in patterning people's integrated use of speech and writing is contrastive functionality, or the use of spoken and written input in a contrastive way to designate a shift in content or functionality, such as original versus corrected input, data versus command, and digits versus text. Different patterns of contrastive mode use accounted for approximately 57% of the integrated pen/voice use observed in these studies. Information also is summarized on preferential mode use, and simultaneity of pen/voice input. One long-term goal of this research is the development of quantitative predictive models of natural modality integration, which could provide guidance on the strategic design of robust multimodal systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Johnson, S. Feiner, J. Marks, M. Maybury, and J. Moore (eds.) </author> <title> Intelligent Multimedia Multimodal Systems. </title> <publisher> AAAI: Stanford University, </publisher> <month> March </month> <year> 1994. </year> <note> Working notes from Spring Symposium Series. </note>
Reference-contexts: To design multimodal systems with performance advantages over unimodal ones, research will be needed on how people select and integrate different modalities in the context of different types of human-computer interaction. Although there is considerable current interest in developing multimodal and multimedia systems <ref> [1, 2] </ref>, proactive empirical research aimed at designing well-integrated systems capable of supporting rather than fragmenting user behavior has been lacking. The goal of the present research is to begin specifying how people integrate their use of spoken and written input during multimodal human-computer exchanges.
Reference: [2] <author> M. T. Maybury (ed.) </author> <title> Intelligent Multimedia Interfaces. </title> <publisher> AAAI Press/MIT Press: </publisher> <address> Menlo Park, California, </address> <year> 1993. </year>
Reference-contexts: To design multimodal systems with performance advantages over unimodal ones, research will be needed on how people select and integrate different modalities in the context of different types of human-computer interaction. Although there is considerable current interest in developing multimodal and multimedia systems <ref> [1, 2] </ref>, proactive empirical research aimed at designing well-integrated systems capable of supporting rather than fragmenting user behavior has been lacking. The goal of the present research is to begin specifying how people integrate their use of spoken and written input during multimodal human-computer exchanges.
Reference: [3] <author> S. L. Oviatt, P. R. Cohen, M. W. Fong, and M. P. Frank. </author> <title> A rapid semi-automatic simulation technique for investigating interactive speech and handwriting. </title> <editor> In J. Ohala (ed.), </editor> <booktitle> Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <volume> vol. </volume> <pages> 2, </pages> <institution> University of Alberta, </institution> <month> October </month> <year> 1992, </year> <pages> 1351-1354. </pages>
Reference-contexts: This semi-automation contributed to the fast pace of the simulation, and to a low rate of technical errors. Details of the simulation technique and its capabilities have been presented elsewhere <ref> [3, 4] </ref>. Research Design and Data Capture Three studies were completed in which the research design was a completely crossed factorial with repeated measures. In two studies, the main factors of interest included: (1) communication modality - speech-only, pen-only, combined pen/voice, and (2) presentation format - structured, unconstrained.
Reference: [4] <author> S. L. Oviatt, P. R. Cohen, M. Wang, and J. Gaston. </author> <title> A simulation-based research strategy for designing complex NL systems. </title> <booktitle> In ARPA Human Language Technology Workshop, </booktitle> <publisher> Morgan Kaufmann: </publisher> <address> San Mateo, Cali-fornia, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: This semi-automation contributed to the fast pace of the simulation, and to a low rate of technical errors. Details of the simulation technique and its capabilities have been presented elsewhere <ref> [3, 4] </ref>. Research Design and Data Capture Three studies were completed in which the research design was a completely crossed factorial with repeated measures. In two studies, the main factors of interest included: (1) communication modality - speech-only, pen-only, combined pen/voice, and (2) presentation format - structured, unconstrained.
Reference: [5] <author> S. L. Oviatt, P. R. Cohen, and M. Q. Wang. </author> <title> Toward interface design for human language technology: Modality and structure as determinants of linguistic complexity. Speech Communication, </title> <month> December, </month> <year> 1994, </year> <note> in press. </note>
Reference-contexts: Likewise, the percentage of written textual input graduated from 8 to 21 letters was 14% to 13%, respectively, which again was not significantly different. That is, although writing took considerably longer than speaking, and although people frequently abbreviated written input with standard and nonstandard abbreviations <ref> [5] </ref>, they nonetheless did not selectively write briefer content. Finally, perceived importance of content did not significantly influence the likelihood that people would choose to write something.
References-found: 5

