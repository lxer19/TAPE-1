URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1997/97-47.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1997.html
Root-URL: http://www.cs.rutgers.edu
Email: mundhenk@ti.uni-trier.de  goldsmit@cs.engr.uky.edu  lusena@cs.engr.uky.edu  allender@cs.rutgers.edu  
Author: Martin Mundhenk Judy Goldsmith Christopher Lusena Eric Allender ; 
Keyword: Finite-Horizon Markov Decision Process Problems 1  
Address: D-54286 Trier, Germany  Lexington KY 40506-0046  Lexington KY 40506-0046  Piscataway, NJ 08855, USA  
Affiliation: Universitat Trier FB IV Informatik  Dept. of Computer Science University of Kentucky  Dept. of Computer Science University of Kentucky  Department of Computer Science Rutgers University  
Note: by  DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs, Bell-core, and Bell Labs. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 97-47 September 1997 Encyclopaedia of Complexity Results for 1 A preliminary version of some of this work appeared as [27]. 2 Supported in part by the Office of the Vice Chancellor for Research and Graduate Studies at the University of Kentucky, and by the Deutsche Forschungsgemeinschaft (DFG), grant Mu 1226/2-1. 3 Supported in part by NSF grant CCR-9315354 and CCR-9610348. 4 Supported in part by NSF grant CCR-9315354 and CCR-9610348. 5 Permanent Member 6 Supported in part by NSF grant CCR-9509603. Portions of this work were performed while at the Institute of Mathematical Sciences, Chennai (Madras), India, and at the Wilhelm-Schickard Institut fur Informatik, Universitat Tubingen (supported by DFG grant TU 7/117-1). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Allender and M. Ogihara. </author> <title> Relationships among PL, #L, and the determinant. </title> <journal> Theoretical Informatics and Applications, </journal> <volume> 30(1) </volume> <pages> 1-21, </pages> <year> 1996. </year>
Reference-contexts: It is denoted as a tuple M = (S; s 0 ; A; O; t; o; r), where * S, A and O are finite sets of states, actions and observations, * s 0 2 S is the initial state, * t : S fi A fi S ! <ref> [0; 1] </ref> is the state transition function, where t (s; a; s 0 ) is the probability that state s 0 is reached from state s on action a (where s 0 2S t (s; a; s 0 ) 2 f0; 1g for every s 2 S; a 2 A), * <p> GapL is the class of functions representable as differences of #L functions, GapL = #L #L, and PL is the class of sets A, for which there is a GapL function f such that for every x, x 2 A iff f (x) &gt; 0 (see <ref> [1] </ref>). We use these results to prove the following. Lemma 4.2 The stationary policy evaluation problem for partially-observable MDPs is in PL. <p> From Lemma 4.1 we get that (T k1 ) (i;j) 2 #L. The reward function is part of the input too, thus r is in GapL (note that rewards may be negative integers). Because GapL is closed under multiplication and polynomial summation (see <ref> [1] </ref>), it follows that p 2 GapL. 2 Since unobservable and fully observable MDPs are a special case of POMDPs, we get the following corollary. Corollary 4.3 The stationary policy evaluation problems for unobservable and fully observable MDPs are in PL. <p> Thus the policy-existence problem for stationary unobservable MDPs logspace disjunctively reduces to the stationary policy evaluation problem for unobservable MDPs. From Lemma 4.2 and the closure of PL under logspace disjunctive reductions (see <ref> [1] </ref>), it follows that the policy existence problem is in PL. In order to show PL-hardness, note that for MDPs with only one action there is no difference between the complexity of the policy evaluation problem and the policy existence problem. <p> The functions r and t are part of the input, and GapL is closed under all operations used on the right hand side of the equation <ref> [1] </ref>. Therefore it follows that av (i; m) jAj2 jSjm is computable in GapL. Thus the problem whether av (i; m) jAj2 jSjm is greater than 0 is decidable in PL. (This follows using a characterization of PL by GapL from [1]). <p> used on the right hand side of the equation <ref> [1] </ref>. Therefore it follows that av (i; m) jAj2 jSjm is computable in GapL. Thus the problem whether av (i; m) jAj2 jSjm is greater than 0 is decidable in PL. (This follows using a characterization of PL by GapL from [1]).
Reference: [2] <author> C. Alvarez and B. Jenner. </author> <title> A very hard log-space counting class. </title> <journal> Theoretical Computer Science, </journal> <volume> 107 </volume> <pages> 3-30, </pages> <year> 1993. </year>
Reference-contexts: In the interest of completeness, in this section we give a short description of the probabilistic and counting complexity classes we use in this work. The class #L <ref> [2] </ref> is the class of functions f such that, for some nondeterministic logarithmically space-bounded machine N, the number of accepting paths of N on x equals f (x).
Reference: [3] <author> C. Backstrom. </author> <title> Expressive Equivalence of Planning Formalisms. </title> <journal> Artificial Intelligence, </journal> <volume> 76 </volume> <pages> 17-34, </pages> <year> 1995. </year>
Reference-contexts: Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially <ref> [3, 13, 14, 16, 18] </ref>, consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]).
Reference: [4] <author> J.L. Balcazar. </author> <title> The complexity of searching implicit graphs. </title> <journal> Artificial Intelligence, </journal> <volume> 86 </volume> <pages> 171-188, </pages> <year> 1996. </year>
Reference: [5] <author> J.L. Balcazar, A. Lozano, and J. Toran. </author> <title> The complexity of algorithmic problems on succinct instances. </title> <editor> In R. Baeza-Yates and U. Manber, editors, </editor> <booktitle> Computer Science, </booktitle> <pages> pages 351-377. </pages> <publisher> Plenum Press, </publisher> <year> 1992. </year>
Reference-contexts: It is easy to see that in going from straightforward to succinct encodings, the complexity of the corresponding decision problems increases at most exponentially. More importantly (and less obviously), there are many problems for which this exponential increase in complexity is inherent (see <ref> [5] </ref> for conditions). For most of the problems considered here, the completeness proofs for the flat POMDPs translate upward to the respective succinctly represented POMDPs.
Reference: [6] <author> D. Beauquier, D. Burago, and A. Slissenko. </author> <title> On the complexity of finite memory policies for Markov decision processes. </title> <booktitle> In Mathematical Foundations of Computer Science, </booktitle> <pages> pages 191-200. </pages> <booktitle> Lecture Notes in Computer Science Vol. </booktitle> <volume> 969, </volume> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: For example, our decision problem for POMDPs with nonnegative rewards under history-dependent policy is NL-complete. (Cf. Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. Beauquier et al. <ref> [6] </ref> considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially [3, 13, 14, 16, 18], consider computationally simpler problems without probabilistic transitions. <p> the size of the circuit, for an MDP with n states, encoded by a circuit of size c log n, the transition probabilities must be written using roughly log n bits (in their standard representation).) The issue of bit-counts in transition probabilities has arisen before; it occurs, for instance, in <ref> [6, 35] </ref>. It is also important to note that our probabilities are specified by single bit-strings, rather than as rationals specified by two bit-strings.
Reference: [7] <author> R. Beigel, N. Reingold and D. Spielman, </author> <title> PP is closed under intersection. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50 </volume> <pages> 191-202, </pages> <year> 1995. </year> - <note> 29 -[8] A. </note> <author> Borodin, S. Cook, and N. Pippenger. </author> <title> Parallel computation for well-endowed rings and space-bounded probabilistic machines. </title> <journal> Information and Control, </journal> 58:113-136, 1983. 
Reference-contexts: Finally, PP is closed under polynomial-time disjunctive reducibility <ref> [7] </ref>, which completes the proof. 2 Omitting the restriction on the number of actions, the complexity of the problem rises to NP PP . Theorem 6.18 The stationary policy existence problem for unobservable compressed MDP with log n horizon is p m -complete for NP PP .
Reference: [9] <author> C. Boutilier, T. Dean, and S. Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Second European Workshop on Planning, </booktitle> <year> 1995. </year>
Reference-contexts: Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]). Many of the results are for succinctly represented systems, described by 2-Phase Temporal Bayes Nets <ref> [9, 10] </ref>, sequential-effects-tree representation (ST) [25], probabilistic state-space operators (PSOs) [22], or other related representations.
Reference: [10] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In 14th International Conference on AI, </booktitle> <year> 1995. </year>
Reference-contexts: We prove what many practitioners already suspected. This gives strength to the call for approximations, and the search for special cases that are in fact computationally simpler than the general case. For instance, there is interest in the AI community in so-called "structured," or succinctly represented MDPs <ref> [10, 13, 25] </ref>. These representations arise when one can make use of structures in the state space to provide small descriptions of very large systems [10]. <p> For instance, there is interest in the AI community in so-called "structured," or succinctly represented MDPs [10, 13, 25]. These representations arise when one can make use of structures in the state space to provide small descriptions of very large systems <ref> [10] </ref>. Whereas those systems are not tractable by classical methods, there is some hope expressed in different algorithmic approaches that many special cases of these structured POMDPs can be solved more efficiently. Boutillier, et al., in [10] conjecture that finding optimal policies for structured POMDPs is infeasible. <p> of structures in the state space to provide small descriptions of very large systems <ref> [10] </ref>. Whereas those systems are not tractable by classical methods, there is some hope expressed in different algorithmic approaches that many special cases of these structured POMDPs can be solved more efficiently. Boutillier, et al., in [10] conjecture that finding optimal policies for structured POMDPs is infeasible. We prove this conjecture by showing that in many cases the complexity of our decision problems increases exponentially if succinct descriptions [19, 37] for POMDPs are considered. <p> Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]). Many of the results are for succinctly represented systems, described by 2-Phase Temporal Bayes Nets <ref> [9, 10] </ref>, sequential-effects-tree representation (ST) [25], probabilistic state-space operators (PSOs) [22], or other related representations.
Reference: [11] <author> C. Boutilier and D. Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations, </title> <year> 1994. </year>
Reference: [12] <author> D. Burago, M. de Rougemont, and A. Slissenko. </author> <title> On the complexity of partially observed Markov decision processes. </title> <journal> Theoretical Computer Science, </journal> <volume> 157(2) </volume> <pages> 161-183, </pages> <year> 1996. </year>
Reference-contexts: For example, our decision problem for POMDPs with nonnegative rewards under history-dependent policy is NL-complete. (Cf. Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in <ref> [12] </ref>. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially [3, 13, 14, 16, 18], consider computationally simpler problems without probabilistic transitions.
Reference: [13] <author> T. Bylander. </author> <title> The computational complexity of propositional STRIPS planning. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 165-204, </pages> <year> 1994. </year>
Reference-contexts: We prove what many practitioners already suspected. This gives strength to the call for approximations, and the search for special cases that are in fact computationally simpler than the general case. For instance, there is interest in the AI community in so-called "structured," or succinctly represented MDPs <ref> [10, 13, 25] </ref>. These representations arise when one can make use of structures in the state space to provide small descriptions of very large systems [10]. <p> Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially <ref> [3, 13, 14, 16, 18] </ref>, consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]). <p> Most of the related AI papers, especially [3, 13, 14, 16, 18], consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in <ref> [13] </ref>). Many of the results are for succinctly represented systems, described by 2-Phase Temporal Bayes Nets [9, 10], sequential-effects-tree representation (ST) [25], probabilistic state-space operators (PSOs) [22], or other related representations. <p> For instance, Bylander <ref> [13] </ref> showed that the time-dependent policy existence problem for unobservable, succinct MDPs with nonnegative rewards is PSPACE-complete, even if the transitions are restricted to be deterministic. <p> This is true, for instance, when a system is modeled as a Bayes belief net, or when actions are given in the STRIPS model <ref> [13, 20, 24] </ref>. Changing the way in which MDPs (and policies) are represented may change the complexities of the considered problems too. We focus on the concept of succinct representations, which was introduced independently in [19, 37].
Reference: [14] <author> D. Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 333-379, </pages> <year> 1987. </year>
Reference-contexts: Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially <ref> [3, 13, 14, 16, 18] </ref>, consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]).
Reference: [15] <author> F. D'Epenoux. </author> <title> A probabilistic production and inventory problem. </title> <journal> AManagement Science, </journal> <volume> 10 </volume> <pages> 98-108, </pages> <year> 1963. </year>
Reference-contexts: Linear programming has been used to decide these two problems for certain forms of MDPs and certain types of policies since 1960 <ref> [15] </ref>; in 1987, Papadimitriou and Tsitsik-lis [30] showed that in fact certain policy existence problems in these cases are P-complete. The complexity of these problems is quite sensitive to details in the specification.
Reference: [16] <author> K. Erol, J. Hendler, and D. Nau. </author> <title> Complexity results for hierarchical task-network planning. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <year> 1996. </year>
Reference-contexts: Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially <ref> [3, 13, 14, 16, 18] </ref>, consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]).
Reference: [17] <author> S. Fenner, L. Fortnow, and S. Kurtz. </author> <title> Gap-definable counting classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(1) </volume> <pages> 116-148, </pages> <year> 1994. </year>
Reference-contexts: We make use of the fact that A 2 PP if and only if there exists a GapP function f such that for every x, x 2 A if and only if f (x) &gt; 0 (see <ref> [17] </ref>). One can show that the function p from the proof of Lemma 4.2, is in GapP under these circumstances, because the respective matrix powering is in GapP (see the proofs of Lemmas 4.9 and 4.1), and GapP is closed under multiplication and summation.
Reference: [18] <author> K. Erol, D. Nau, and V. S. Subrahmanian. </author> <title> Complexity, decidability and undecidability results for domain-independent planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76 </volume> <pages> 75-88, </pages> <year> 1995. </year>
Reference-contexts: Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially <ref> [3, 13, 14, 16, 18] </ref>, consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]).
Reference: [19] <author> H. Galperin and A. Wigderson. </author> <title> Succinct representation of graphs. </title> <journal> Information and Control, </journal> <volume> 56 </volume> <pages> 183-198, </pages> <year> 1983. </year>
Reference-contexts: Boutillier, et al., in [10] conjecture that finding optimal policies for structured POMDPs is infeasible. We prove this conjecture by showing that in many cases the complexity of our decision problems increases exponentially if succinct descriptions <ref> [19, 37] </ref> for POMDPs are considered. For example, policy existence problems for POMDPs with nonnegative rewards are NL-complete under straightforward descriptions. Using succinct descriptions, the completeness increases to PSPACE. We also consider a new intermediate notion of compressed representations and get intermediate complexity results, e.g. NP-completeness for the above example. <p> Changing the way in which MDPs (and policies) are represented may change the complexities of the considered problems too. We focus on the concept of succinct representations, which was introduced independently in <ref> [19, 37] </ref>. POMDPs having very regular structure arise in many practical and theoretical areas of computer science, and often it is most natural to represent these processes very compactly.
Reference: [20] <author> J. Goldsmith, M. Littman, and M. Mundhenk. </author> <title> The complexity of plan existence and evaluation in probabilistic domains. </title> <booktitle> Proc. 13th Annual Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 182-189, </pages> <year> 1997. </year>
Reference-contexts: Most of the related AI papers, especially [3, 13, 14, 16, 18], consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except <ref> [20, 24] </ref> and one theorem in [13]). Many of the results are for succinctly represented systems, described by 2-Phase Temporal Bayes Nets [9, 10], sequential-effects-tree representation (ST) [25], probabilistic state-space operators (PSOs) [22], or other related representations. <p> number of states | leading to different complexities, in certain cases. (For instance, Littman showed that the general plan existence problem is EXP-complete, but if plans are limited to size polynomial in the size of the planning domain, then the problem is PSPACE-complete [25]; the PSPACE-completeness was proved independently in <ref> [20] </ref>.) Sometimes the stochasticity of the system does not add computational complexity. For instance, Bylander [13] showed that the time-dependent policy existence problem for unobservable, succinct MDPs with nonnegative rewards is PSPACE-complete, even if the transitions are restricted to be deterministic. <p> This is true, for instance, when a system is modeled as a Bayes belief net, or when actions are given in the STRIPS model <ref> [13, 20, 24] </ref>. Changing the way in which MDPs (and policies) are represented may change the complexities of the considered problems too. We focus on the concept of succinct representations, which was introduced independently in [19, 37]. <p> In particular, there are no known heuristics for NP PP -complete problems; surprisingly, these problems arise frequently, at least in the AI/planning literature. (See <ref> [20] </ref> for more examples.) Acknowledgements We would like to thank Anne Condon, Andy Klapper, Matthew Levy, and especially Michael Littman for discussions and suggestions on this material.
Reference: [21] <author> H. Jung. </author> <title> On probabilistic time and space. </title> <booktitle> In Proceedings 12th ICALP, </booktitle> <pages> pages 281-291. </pages> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 194, </volume> <publisher> Springer-Verlag, </publisher> <year> 1985. </year> - <note> 30 -[22] N. </note> <author> Kushmerick, S. Hanks, and D.S. Weld. </author> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76 </volume> <pages> 239-286, </pages> <year> 1995. </year>
Reference-contexts: In apparent contrast to P-complete sets, sets in PL are decidable using very fast parallel computations <ref> [21] </ref>. Probabilistic polynomial time, PP, is defined analogously. <p> Lemma 4.4 The stationary policy evaluation problem for unobservable MDPs is PL-hard. Proof Consider A 2 PL. Then there exists a probabilistic logspace machine N accepting A, and a polynomial p such that each computation of N on x uses at most p (jxj) random decisions <ref> [21] </ref>. Now, fix some input x. We construct an unobservable MDP M (x) with only one action, which models the behavior of N on x.
Reference: [23] <author> R. Ladner. </author> <title> Polynomial space counting problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18 </volume> <pages> 1087-1097, </pages> <year> 1989. </year>
Reference-contexts: Probabilistic polynomial time, PP, is defined analogously. A classic PP-complete problem is Majsat: given a Boolean formula, do the majority of assignments satisfy it? For polynomial-space-bounded computations, PSPACE equals probabilistic PSPACE, and #PSPACE (defined analogously to #L and #P) is the same as the class of polynomial-space-computable functions <ref> [23] </ref>. (Note that some functions in #PSPACE produce output of exponential length.) For any complexity classes C and C 0 the class C C 0 consists of those sets that are C-Turing reducible to sets in C 0 , i.e., sets that can be accepted with resource bounds specified by C, <p> Proof In order to show that the problem is in PSPACE, we can use the same technique as in the proof of Lemma 4.2 yielding here that the problem is in PPSPACE (=probabilistic PSPACE). Ladner <ref> [23] </ref> showed that FPSPACE = #PSPACE, from which it follows that PPSPACE = PSPACE. Showing PSPACE-hardness is even easier than showing PL-hardness in the proof of Theorem 4.4, because here we deal with a deterministic class.
Reference: [24] <author> M.L. Littman. </author> <title> Probabilistic STRIPS planning is EXPTIME-complete. </title> <type> Technical Report CS-1996-18, </type> <institution> Duke University Department of Computer Science, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: Most of the related AI papers, especially [3, 13, 14, 16, 18], consider computationally simpler problems without probabilistic transitions. Thus, the complexity of their problems is generally lower than of ours (except <ref> [20, 24] </ref> and one theorem in [13]). Many of the results are for succinctly represented systems, described by 2-Phase Temporal Bayes Nets [9, 10], sequential-effects-tree representation (ST) [25], probabilistic state-space operators (PSOs) [22], or other related representations. <p> This is true, for instance, when a system is modeled as a Bayes belief net, or when actions are given in the STRIPS model <ref> [13, 20, 24] </ref>. Changing the way in which MDPs (and policies) are represented may change the complexities of the considered problems too. We focus on the concept of succinct representations, which was introduced independently in [19, 37].
Reference: [25] <author> M.L. Littman. </author> <title> Probabilistic propositional planning: Representations and complexity. </title> <booktitle> Proc. 14th National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: We prove what many practitioners already suspected. This gives strength to the call for approximations, and the search for special cases that are in fact computationally simpler than the general case. For instance, there is interest in the AI community in so-called "structured," or succinctly represented MDPs <ref> [10, 13, 25] </ref>. These representations arise when one can make use of structures in the state space to provide small descriptions of very large systems [10]. <p> Thus, the complexity of their problems is generally lower than of ours (except [20, 24] and one theorem in [13]). Many of the results are for succinctly represented systems, described by 2-Phase Temporal Bayes Nets [9, 10], sequential-effects-tree representation (ST) <ref> [25] </ref>, probabilistic state-space operators (PSOs) [22], or other related representations. <p> the size of the representation, or the number of states | leading to different complexities, in certain cases. (For instance, Littman showed that the general plan existence problem is EXP-complete, but if plans are limited to size polynomial in the size of the planning domain, then the problem is PSPACE-complete <ref> [25] </ref>; the PSPACE-completeness was proved independently in [20].) Sometimes the stochasticity of the system does not add computational complexity. For instance, Bylander [13] showed that the time-dependent policy existence problem for unobservable, succinct MDPs with nonnegative rewards is PSPACE-complete, even if the transitions are restricted to be deterministic. <p> The same proof applies here too. 2 As a consequence we obtain that the policy existence problem for flat MDPs with exponential horizon is in PSPACE. This was originally proved in <ref> [25] </ref>. The complexity gap between the stationary and the time-dependent policy existence problems for compressed MDP [log n]s is as big as that for flat MDPs, but the difference no longer depends on the number of actions.
Reference: [26] <author> W.S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: There have certainly been algorithms presented and analyzed, but we will not generally survey them here. For that, we recommend <ref> [26, 32] </ref> as a starting place. The complexity of partially-observable MDPs was first considered by Papadimitriou and Tsitsiklis [29, 30].
Reference: [27] <author> M. Mundhenk, J. Goldsmith, and E. Allender. </author> <title> The complexity of policy evaluation for finite-horizon partially-observable Markov decision processes. </title> <booktitle> Proc. MFCS '97, </booktitle> <year> 1997. </year>
Reference: [28] <author> C.H. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: a given MDP have a policy with expected reward exactly r? Section 8 considers the complexity of testing the median and average policies of an MDP. 2 Definitions and Preliminaries 2.1 Complexity Theory: Counting Classes For definitions of complexity classes, reductions, and standard results from complexity theory we refer to <ref> [28] </ref>. In the interest of completeness, in this section we give a short description of the probabilistic and counting complexity classes we use in this work. <p> For any class C PSPACE, it is the case that NP C PSPACE, and therefore NP PSPACE =PSPACE; see Papadimitriou's textbook <ref> [28] </ref>. <p> This follows from Corollary 5.2, or alternatively from Theorem 5.5. - 17 Theorem 5.10 The stationary policy existence problem for succinct POMDPs with non--negative rewards is NEXP-complete. Proof One can use a succinct 3Sat input like that in <ref> [28, pp 493-5] </ref> to construct a succinct version of the POMDP used in the reduction for Theorem 5.4. 2 Theorem 5.11 The time- or history-dependent policy existence problems for succinct POMDPs with nonnegative rewards are PSPACE-complete.
Reference: [29] <author> C.H. Papadimitriou and J.N. Tsitsiklis. </author> <title> Intractable problems in control theory. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 24, </volume> <pages> pages 639-654, </pages> <year> 1986. </year>
Reference-contexts: There have certainly been algorithms presented and analyzed, but we will not generally survey them here. For that, we recommend [26, 32] as a starting place. The complexity of partially-observable MDPs was first considered by Papadimitriou and Tsitsiklis <ref> [29, 30] </ref>. <p> They showed an NP-completeness result for the stationary, finite-horizon case <ref> [29, p. 645] </ref>. For time-dependent policies they showed that the policy existence problem for fully-observable MDPs is P-complete (for finite or infinite horizon). For history-dependent policies they proved the policy existence problem for POMDPs to be PSPACE-complete, and that for unobservable MDPs to be NP-complete [30].
Reference: [30] <author> C.H. Papadimitriou and J.N. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: Linear programming has been used to decide these two problems for certain forms of MDPs and certain types of policies since 1960 [15]; in 1987, Papadimitriou and Tsitsik-lis <ref> [30] </ref> showed that in fact certain policy existence problems in these cases are P-complete. The complexity of these problems is quite sensitive to details in the specification. <p> There have certainly been algorithms presented and analyzed, but we will not generally survey them here. For that, we recommend [26, 32] as a starting place. The complexity of partially-observable MDPs was first considered by Papadimitriou and Tsitsiklis <ref> [29, 30] </ref>. <p> For time-dependent policies they showed that the policy existence problem for fully-observable MDPs is P-complete (for finite or infinite horizon). For history-dependent policies they proved the policy existence problem for POMDPs to be PSPACE-complete, and that for unobservable MDPs to be NP-complete <ref> [30] </ref>. <p> Because of the subtle differences between the decision problems, our complexity results on similar problems differ greatly from the results in <ref> [30] </ref>. For example, our decision problem for POMDPs with nonnegative rewards under history-dependent policy is NL-complete. (Cf. Corollary 5.3, compare to PSPACE-completeness in [30, Theorem 6].) Unobservable MDPs were also considered in [12]. <p> Because of the subtle differences between the decision problems, our complexity results on similar problems differ greatly from the results in [30]. For example, our decision problem for POMDPs with nonnegative rewards under history-dependent policy is NL-complete. (Cf. Corollary 5.3, compare to PSPACE-completeness in <ref> [30, Theorem 6] </ref>.) Unobservable MDPs were also considered in [12]. Beauquier et al. [6] considered different optimality criteria, and thus their results are not directly comparable with ours. Most of the related AI papers, especially [3, 13, 14, 16, 18], consider computationally simpler problems without probabilistic transitions. <p> The policy existence problems are expressed similarly. If f or g is the identity function, we omit it in the problem description. 4 Policy Evaluation Problems 4.1 Flat MDPs The standard polynomial time algorithm for evaluating a given policy for a given POMDP uses dynamic programming <ref> [30, 32] </ref>. We show that for POMDPs this evaluation can be - 10 performed quickly in parallel. Eventually this yields the policy evaluation problem being complete for PL. <p> Moreover, the transition probabilities and rewards do not need to be calculated exactly. Note that earlier work, for instance Papadimitriou and Tsitsiklis' <ref> [30] </ref> considered a different problem than we do here, with strikingly different complexity. They considered MDPs with nonpositive rewards, and asked whether there was a policy with expected reward 0. <p> Dynamic programming can be used for the finite horizon time-dependent policy case. Theorem 6.1 The time-dependent and history-dependent policy existence problems for fully-observable MDPs are P-complete. Because the proof is a straightforward modification of a proof in <ref> [30] </ref>, we omit it here. The exact complexity of the policy existence problem for stationary, fully-observable MDPs with finite horizon is not known. From [30] it follows that it is P-hard, and it is easily seen to be in NP. But completeness results are not known. <p> Theorem 6.1 The time-dependent and history-dependent policy existence problems for fully-observable MDPs are P-complete. Because the proof is a straightforward modification of a proof in <ref> [30] </ref>, we omit it here. The exact complexity of the policy existence problem for stationary, fully-observable MDPs with finite horizon is not known. From [30] it follows that it is P-hard, and it is easily seen to be in NP. But completeness results are not known. <p> This proves PL-hardness of the policy existence problem. 2 - 19 The time-dependent policy existence problem for unobservable MDPs turns out to be NP-complete. Theorem 6.3 The time-dependent policy existence problem for unobservable MDPs is NP-complete. Papadimitriou and Tsitsiklis proved a similar theorem <ref> [30] </ref>. Their MDPs had only non-positive rewards, and their formulation of the decision problem was whether there is a policy with reward 0. However, our result can be proven by a proof very similar to theirs showing a reduction from 3Sat. <p> Theorem 6.6 The history-dependent policy existence problem for POMDPs is PSPACE-complete. The proof of Theorem 6.6 is a straightforward modification of the proof by Pa-padimitriou and Tsitsiklis <ref> [30, Theorem 6] </ref>, where the rewards for reaching the satisfying resp. unsatisfying final state are changed appropriately. 6.2 Succinct MDPs Note that we still take the size of the state space as our finite horizon, which may be exponential in the size of the representation of the MDP. <p> For instance, we have been unable to determine the complexity of the finite horizon stationary policy existence problem for fully-observable MDPs. (The infinite horizon case is known to be P-complete <ref> [30] </ref>; as noted in the introduction, the finite horizon problem is only known to be P-hard and in NP.) However, we can determine the complexity of the exact policy existence problem, as follows. Theorem 7.1 The exact stationary policy existence problem for fully-observable (flat) MDPs is NP-complete.
Reference: [31] <author> C.H. Papadimitriou and M. Yannakakis. </author> <title> A note on succinct representations of graphs. </title> <journal> Information and Control, </journal> <volume> 71 </volume> <pages> 181-185, </pages> <year> 1986. </year>
Reference-contexts: Proof Membership in NEXP follows from Theorem 6.8. To show NEXP-hardness, we sketch a reduction from the succinct version of 3Sat, shown to be NEXP-complete in <ref> [31] </ref>, to the stationary case. The reduction is similar to that of Theorem 6.3. where a formula was transformed into a POMDP which "checks" all clauses of the formula in parallel, works here.
Reference: [32] <author> M.L. Puterman. </author> <title> Markov decision processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Markov decision processes are used in the natural and social sciences and engineering to model a vast array of systems. (Puterman's book <ref> [32] </ref> has excellent discussions of applications.) They are used in a variety of applied computer science fields, including control theory, robotics, expert systems, learning theory, and data mining to model assembly lines, robot location problems, medical diagnosis and treatment experts, and many other systems. <p> There have certainly been algorithms presented and analyzed, but we will not generally survey them here. For that, we recommend <ref> [26, 32] </ref> as a starting place. The complexity of partially-observable MDPs was first considered by Papadimitriou and Tsitsiklis [29, 30]. <p> The policy existence problems are expressed similarly. If f or g is the identity function, we omit it in the problem description. 4 Policy Evaluation Problems 4.1 Flat MDPs The standard polynomial time algorithm for evaluating a given policy for a given POMDP uses dynamic programming <ref> [30, 32] </ref>. We show that for POMDPs this evaluation can be - 10 performed quickly in parallel. Eventually this yields the policy evaluation problem being complete for PL.
Reference: [33] <author> S. </author> <title> Toda. PP is as hard as the polynomial-time hierarchy. </title> <journal> SIAM Journal on Computing, </journal> <volume> 20 </volume> <pages> 865-877, </pages> <year> 1991. </year>
Reference-contexts: Thus the policy existence problem is complete for NP PP . 2 From Toda's result that PH P PP <ref> [33] </ref> and Theorem 6.18 we have the following. Corollary 6.19 The stationary policy existence problem for unobservable sMDPs and log n horizon is p m -hard for PH.
Reference: [34] <author> J. Toran. </author> <title> Complexity classes defined by counting quantifiers. </title> <journal> Journal of the ACM, </journal> <volume> 38(3) </volume> <pages> 753-774, </pages> <year> 1991. </year>
Reference-contexts: For any class C PSPACE, it is the case that NP C PSPACE, and therefore NP PSPACE =PSPACE; see Papadimitriou's textbook [28]. Another useful result is that the class NP PP equals the " NP m " closure of PP <ref> [34] </ref>, which can be seen as the closure of PP under polynomial-time disjunctive reducibility - 6 with an exponential number of queries (each of the queries computable in polynomial time from its index in the list of queries). 2.2 Markov Decision Processes A Markov decision process (MDP) describes a controlled stochastic <p> For the existence question, one can guess a (polynomial-sized) policy, and verify that it has expected reward &gt; 0 by consulting a PP oracle. To show NP PP -hardness, one needs that NP PP equals the np m closure of PP <ref> [34] </ref>, which can be seen as the closure of PP under polynomial-time disjunctive reducibility with an exponential number of queries (each of the queries computable in polynomial time from its index in the list of queries). Let A 2 NP PP .
Reference: [35] <author> P. Tseng. </author> <title> Solving h-horizon, stationary Markov decision problems in time proportional to log h. </title> <journal> Operations Research Letters, </journal> <volume> 9, </volume> <pages> pages 287-297, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: the size of the circuit, for an MDP with n states, encoded by a circuit of size c log n, the transition probabilities must be written using roughly log n bits (in their standard representation).) The issue of bit-counts in transition probabilities has arisen before; it occurs, for instance, in <ref> [6, 35] </ref>. It is also important to note that our probabilities are specified by single bit-strings, rather than as rationals specified by two bit-strings.
Reference: [36] <author> V. Vinay. </author> <title> Counting auxiliary pushdown automata and semi-unbounded arithmetic circuits. </title> <booktitle> In Proc. 6th Structure in Complexity Theory Conference, </booktitle> <pages> pages 270-284. </pages> <note> IEEE, 1991. </note> - <author> 31 -[37] K. W. Wagner. </author> <title> The complexity of combinatorial problems with succinct input representation. </title> <journal> Acta Informatica, </journal> <volume> 23 </volume> <pages> 325-356, </pages> <year> 1986. </year>
Reference-contexts: Lemma 4.1 (cf. <ref> [36] </ref>) Let T be an n fi n matrix of nonnegative binary integers, each of length n, and let 1 i; j n, 0 m n. The function mapping (T; m; i; j) to (T m ) (i;j) is in #L. <p> We argue that av (i; m) is in GapL (defined in <ref> [36] </ref>), i.e. the class of integer functions f for which exist logspace nondeterministic Turing machines N f such that the number of accepting paths minus the number of rejectin paths N f on input x equals f (x).
Reference: [38] <author> R. </author> <title> Washington. Incremental Markov-model planning. </title> <note> http://www.cis.upenn.edu/~rwash/papers/inc-markov.html (submitted for review). </note>
Reference-contexts: Is the system fully specified, or is it represented by some sort of function (here, circuits) that computes the probabilities as needed? Similarly for the controller itself. For instance, partially observable MDPs (POMDPs) model everything from robot location problems to medical treatment <ref> [38] </ref>. Unfortunately, the optimal policies for POMDPs are generally expressible as functions from initial segments of the history of a process (a series of observations, giving partial or full information about the state at previous and current times, and | implicitly | of actions taken).
References-found: 35

