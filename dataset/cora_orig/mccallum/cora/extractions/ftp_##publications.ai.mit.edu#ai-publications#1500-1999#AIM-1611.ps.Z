URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1611.ps.Z
Refering-URL: http://www.ai.mit.edu/people/mmp/mmp.html
Root-URL: 
Title: Estimating Dependency Structure as a Hidden Variable  
Author: Marina Meila Michael I. Jordan Quaid Morris 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1996  
Date: 1611 June, 1997  151  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors. This report describes research done at the Dept. of Electrical Engineering and Computer Science, the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense and by the Office of Naval Research. Michael I. Jordan is a NSF Presidential Young Investigator. The authors can be reached at M.I.T., Center for Biological and Computational Learning, 45 Carleton St., Cambridge MA 02142, USA. E-mail: mmp@ai.mit.edu, jordan@psyche.mit.edu, quaid@ai.mit.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. K. Chow and C. N. Liu. </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> "IEEE Transactions on Information Theory", </journal> <volume> IT-14(3):462-467, </volume> <month> MAy </month> <year> 1968. </year>
Reference-contexts: Mixtures of factorial distributions, a subclass of tree distributions, have been investigated recently by [8]. Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu <ref> [1] </ref> and was extended to polytrees by Pearl [10] and to mixtures of trees with observed structure variable by Geiger [5] and Friedman [4]. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable. <p> N X P k (x i ) log T k (x i ) (9) This problem can be solved exactly as shown in <ref> [1] </ref>. Here we will give a brief description of the procedure.
Reference: [2] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: a belief network, but most of the results here owe to the belief network perspective. 3 THE BASIC ALGORITHM: ML FITTING OF MIXTURES OF TREES This section will show how a mixture of trees can be fit to an observed dataset in the Maximum Likelihood paradigm via the EM algorithm <ref> [2] </ref>. The observations are denoted by fx 1 ; x 2 ; : : : ; x N g; the corresponding values of the structure variable are fz i ; i = 1; : : : N g.
Reference: [3] <author> Brendan J. Frey, Geoffrey E. Hinton, and Peter Dayan. </author> <title> Does the wake-sleep algorithm produce good density estimators? In D. </title> <editor> Touretsky, M. Mozer, and M. Has-selmo, editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> number 8, </volume> <pages> pages 661-667. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: One dataset contained images of single digits in 64 dimensions, the second contained 128 dimensional vectors representing randomly paired digit images. The training, validation and test set contained 6000, 2000, and 5000 exemplars respectively. The data sets, the training conditions and the algorithms we compared with are described in <ref> [3] </ref>. We tried mixtures of 16, 32, 64 and 128 trees, fitted by the basic algorithm. The training set was used to fit the model parameters and the validation set to determine when EM has converged. <p> These results are shown in table 3. The other algorithms mentioned in the table are the mixture of factorial distributions (MF), the completely factorized model (which assumes that every variable is independent of all the others) called "Base rate", the Helmholtz Machine trained by the wake-sleep algorithm <ref> [3] </ref> (Helmholtz Machine), the same Helmholtz Machine where a mean field approximation was used for training (Mean Field) and a fully visible and fully connected sigmoid belief network.
Reference: [4] <author> Nir Friedman and Moses Goldszmidt. </author> <title> Building classifiers using Bayesian networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI 96), </booktitle> <pages> pages 1277-1284, </pages> <address> Menlo Park, CA, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu [1] and was extended to polytrees by Pearl [10] and to mixtures of trees with observed structure variable by Geiger [5] and Friedman <ref> [4] </ref>. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable. The following section introduces the model; then, section 3 develops the basic algorithm for its estimation from data in the ML framework.
Reference: [5] <author> Dan Geiger. </author> <title> An entropy-based learning algorithm of bayesian conditional trees. </title> <booktitle> In Proceedings of the 8th Conference on Uncertainty in AI, </booktitle> <pages> pages 92-97. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu [1] and was extended to polytrees by Pearl [10] and to mixtures of trees with observed structure variable by Geiger <ref> [5] </ref> and Friedman [4]. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable. The following section introduces the model; then, section 3 develops the basic algorithm for its estimation from data in the ML framework.
Reference: [6] <author> B. </author> <title> German. Glass identification database. U.C. Irvine Machine Learning Repository. </title>
Reference-contexts: In the testing phase, a new instance was classified by picking the most likely value of the class variable given the other variables settings. The first task used the Glass database <ref> [6] </ref>. The data set has 214 instances of 9-dimensional continuous valued vectors. The class variable has 6 values. The continuous variables were discretized in 4 uniform bins each. We tested mixtures with different values of m, variable degrees of smoothing with the marginals and values of fi for edge pruning.
Reference: [7] <author> David Heckerman, Dan Geiger, and David M. Chick-ering. </author> <title> Learning Bayesian networks: the combination of knowledge and statistical data. </title> <journal> Machine Learining, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: A Dirichlet prior over a tree can be repre sented as a table of fictitious marginal probabilities P 0k uv for each pair u; v of variables plus an equivalent sample size N 0 that gives the strength of the prior <ref> [7] </ref>.
Reference: [8] <author> Petri Kontkanen, Petri Myllymaki, and Henry Tirri. </author> <title> Constructing bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> Univeristy of Helsinky, Department of Computer Science, </institution> <year> 1996. </year> <month> 5 </month>
Reference-contexts: Of relevance to the present work are the mixtures of Gaussians, whose distribution space, in the case of continuous variables overlaps with the space of mixtures of trees. Mixtures of factorial distributions, a subclass of tree distributions, have been investigated recently by <ref> [8] </ref>. Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu [1] and was extended to polytrees by Pearl [10] and to mixtures of trees with observed structure variable by Geiger [5] and Friedman [4]. <p> For comparison we tried also mixtures of factorial distributions of different sizes. One seventh of the data, picked randomly at each trial, was used for testing and the rest for training. We replicate for comparison results obtained and cited in <ref> [8] </ref> on training/test sets of the same size. Table 2 shows a selection of the results we obtained. Smoothing with marginals proved to be bad for classification; therefore those results are not shown. <p> MST is mixtures of spanning trees, MF is a mixture of factorial distributions. Algorithm Classification #runs Algorithm Classification #runs performance performance MST m=30 fi =0 *.82 10 MF m=35 .73 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [8] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [8] .66 MST m=40 fi =.1 .73 3 C4 from [8] .66 MST m=25 fi =0 .77 14 1Rw from [8] .66 Table 3: Compression rates (bits per digit) for the single digit (Digit) and double digit (Pairs) datasets. <p> Algorithm Classification #runs Algorithm Classification #runs performance performance MST m=30 fi =0 *.82 10 MF m=35 .73 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [8] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [8] .66 MST m=40 fi =.1 .73 3 C4 from [8] .66 MST m=25 fi =0 .77 14 1Rw from [8] .66 Table 3: Compression rates (bits per digit) for the single digit (Digit) and double digit (Pairs) datasets. <p> performance MST m=30 fi =0 *.82 10 MF m=35 .73 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [8] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [8] .66 MST m=40 fi =.1 .73 3 C4 from [8] .66 MST m=25 fi =0 .77 14 1Rw from [8] .66 Table 3: Compression rates (bits per digit) for the single digit (Digit) and double digit (Pairs) datasets. MST is mixtures of spanning trees, MF is a mixture of factorial distributions. <p> 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [8] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [8] .66 MST m=40 fi =.1 .73 3 C4 from [8] .66 MST m=25 fi =0 .77 14 1Rw from [8] .66 Table 3: Compression rates (bits per digit) for the single digit (Digit) and double digit (Pairs) datasets. MST is mixtures of spanning trees, MF is a mixture of factorial distributions. A * marks the best performance on each dataset.
Reference: [9] <author> Hermann Ney, Ute Essen, and Reinhard Kneser. </author> <title> On structuring probabilistic dependences in stochastic language modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38, </pages> <year> 1994. </year>
Reference-contexts: k uv by uv = (1 ff)P k uv + ffP total uv ; 0 &lt; ff &lt; 1 (16) 2 Note that to use P [m] together with edge pruning on has to compute the normalization constant in (11). 3 This method and several variations thereof are discussed in <ref> [9] </ref>. Its effect is to give a small probability weight to unseen instances and to draw the components closer to each other, thereby reducing the effective value of m.
Reference: [10] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kauf-man Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Mixtures of factorial distributions, a subclass of tree distributions, have been investigated recently by [8]. Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu [1] and was extended to polytrees by Pearl <ref> [10] </ref> and to mixtures of trees with observed structure variable by Geiger [5] and Friedman [4]. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable.
Reference: [11] <author> Jeff Schlimmer. </author> <title> Mushroom database. U.C. Irvine Machine Learning Repository. </title> <type> 6 </type>
Reference-contexts: Smoothing with marginals proved to be bad for classification; therefore those results are not shown. The effect of edge pruning seems not to be significant on classification although, as expected, it increases the test set likelihood. The second data set used was the Mushroom database <ref> [11] </ref>. This data set has 8124 instances of 23 discrete attributes (including the class variable, which is treated like any other attribute for the purpose of model learning). The training set comprised 6000 randomly chosen examples, and the test set was formed by the remaining 2124.
References-found: 11

