URL: http://www.cs.ucsb.edu/~schmittm/scizzl98.ps
Refering-URL: http://www.cs.ucsb.edu/~schmittm/res.html
Root-URL: http://www.cs.ucsb.edu
Email: fibel,schauser,chriss,schmittmg@cs.ucsb.edu  
Title: Scintilla: Cluster Computing with SCI  
Author: Max Ibel, Klaus E. Schauser, Chris J. Scheiman, and Michael Schmitt 
Web: http://www.cs.ucsb.edu/research/scintilla  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: The Scintilla project at UCSB studies SCI-based cluster computing. The Scalable Coherent Interface (SCI) is a recent communication standard for cluster interconnects. We focus on non-coherent SCI, using our cluster setup of SBus-based and PCI-based workstations connected via Dolphin SCI adapters. Our motivation for choosing SCI as network fabric is the very low latency and high bandwidth. We study how to map a variety of programming models efficiently onto the SCI hardware, focusing on message passing and global address space support, implementing Active Messages and Split-C. We present implementation trade-offs, present performance measurements and compare the PCI and SBus adapters. We found that the user-level load/store programming interface of SCI is very convenient to use, achieves low latencies, and is fully virtualized, simultaneously supporting multiple parallel programs and communication channels. On the other hand, neither of the programming models studied maps directly to SCI. Issues such as notification, atomic operations, and virtual address space limitations represent major implementation challenges, which we address with a combination of compiler and run-time support. Overall, we found the SCI network a good substrate for high-performance cluster computing. 
Abstract-found: 1
Intro-found: 1
Reference: [ACP95] <author> T. E. Anderson, D. E. Culler, and D. Patterson. </author> <title> A case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: This work was supported by the National Science Foundation NSF CAREER Award CCR-9502661, NSF Postdoctoral Award ASC-9504291, Sun Microsystems, Dolphin Interconnects. Microsoft, Intel, and the California Micro Program Grant #97-155. Computational resources were provided by the NSF Instrumentation Grant CDA-9529418 and NSF Infrastructure Grant CDA-9216202. MPP systems <ref> [ACP95] </ref>. The recent explosive growth of the Internet, as well as advances and interest in multimedia, data mining and data warehousing have created additional communication-intensive parallel applications that require cluster networks with low latency and high bandwidth.
Reference: [AHKL96] <author> G. Acher, H. Hellwagner, W. Karl, and M. Leberecht. </author> <title> A PCI-SCI Bridge for Building a PC Cluster with Distributed Shared Memory. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: Besides Dolphin Interconnects [Dol95a], Interconnect Systems Solution [Kib97] and Vitesse are also developing commodity SCI products to connect clusters of workstations. Several other groups even build their own SCI network cards or bridges, for example the SMILE group at the University of Munich <ref> [AHKL96] </ref>, or the RD24 project at CERN. SCI has been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sun Starfire [Mic96], Data General [CA96], and Sequent NUMA-Q [LCS96]).
Reference: [BCF + 95] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Ku-lawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A Gigabit Per Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: However, the demand for low-latency, high-performance clusters has led to the development of more efficient approaches in the commodity market, as witnessed by the popularity of Tandem Server workstations. The newer PCI-based cluster consists of 4 Ultra30 workstations. Net [Hor95], Myrinet <ref> [BCF + 95] </ref>, HAL [Lar98], and the recently defined VIA interface [DR97, CM97].
Reference: [BCL + 95] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: This is especially true for Active Messages, where each message handler is executed on the receiver. The receiving processor must be notified of new message arrival and synchronization requirements must be obeyed. Remote queues is an abstraction addressing the notification and synchronization problems associated with message delivery <ref> [SS95, BCL + 95] </ref>. Sending processors just enqueue their messages on the remote queue. The receiver is notified of the message either using interrupts or polling. Since interrupts are very expensive, our implementation relies on polling.
Reference: [BLA + 94] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, and E. W. Felten. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proc. of the 21st Int'l Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: In the last two products, the nodes are connected via cache-coherent SCI on the memory bus. There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory. We discus the SHRIMP multicomputer <ref> [BLA + 94] </ref> and the DEC Memory Channel [GK96]. Other similar networks are Tandem ServerNet [Hor95] and the PCI-based Synfinity network from HAL/Fujitsu [Lar98]. The DEC Memory Channel is very similar to SCI.
Reference: [CA96] <author> R. Clark and K. Alnes. </author> <title> An SCI Interconnect Chipset and Adapter. </title> <booktitle> In Proc. of Hot Interconnects IV, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: SCI has been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sun Starfire [Mic96], Data General <ref> [CA96] </ref>, and Sequent NUMA-Q [LCS96]). In the last two products, the nodes are connected via cache-coherent SCI on the memory bus. There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory.
Reference: [CDG + 93] <author> D. E. Culler, A. Dusseau, S. C. Golstein, A. Krish-namurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. of Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Request handlers may answer by sending a single reply message, while reply handlers are prohibited from additional communication. Active Messages achieve an order of magnitude performance improvement over more traditional communication mechanisms. Split-C is a simple SPMD parallel extension of the C programming language <ref> [CDG + 93] </ref>. Both the parallelism and data layout are explicit and are specified by the programmer. Split-C provides a global address space in the form of distributed arrays and global pointers, and supports several efficient split-phase operations to access remote data.
Reference: [CM97] <author> Intel Compaq and Microsoft. </author> <title> Virtual Interface Specification, </title> <month> Dec </month> <year> 1997. </year>
Reference-contexts: The newer PCI-based cluster consists of 4 Ultra30 workstations. Net [Hor95], Myrinet [BCF + 95], HAL [Lar98], and the recently defined VIA interface <ref> [DR97, CM97] </ref>. A common approach to achieving low latency is to bypass the operating system for normal communication, but still ensure protection by relying on the virtual memory mechanism and using the operating system only to set up communication segments.
Reference: [DBDF96] <author> S. N. Damianakis, A. Bilas, C. Dubnicky, and E. W. Felten. </author> <title> Client-Server Computing on the SHRIMP Multicomputer. </title> <booktitle> In Hot Interconnects IV Symposium on High Performance Interconnects, </booktitle> <month> August, 96. </month>
Reference-contexts: The SHRIMP hardware is connected to the memory bus, not the I/O bus. In this way, the network interface can snoop all writes, and automatically forward them to a remote node if they access a page that has been exported. The work presented in <ref> [DBDF96] </ref> shows how a number of message passing libraries, in particular RPC and socket interfaces, can be implemented efficiently on SHRIMP.
Reference: [Dol95a] <author> Dolphin. </author> <title> Multiprocessor Systems Design With SCI and Dolphin Technology. Dolphin Interconnect Solutions, </title> <year> 1995. </year>
Reference-contexts: SCI switches allow for multiply connected rings. The SCI standard specifies four basic types of transactions: read, write, move, and lock. All transactions are split-phase, separating requests from responses. Read and write transactions send replies back, whereas move transactions are one-way. Several vendors have added or removed transactions <ref> [LCS96, Dol95a] </ref>. Our SBus-based Dolphin network adapters do not support lock transactions, and for PCI, the mapping of bus operations to SCI transactions is implementation specific. <p> George et al. report their experiments with a multi-threaded runtime system and their version of Active Messages. Their implementation has a 68s latency, but is thread safe. The NOW group at Berkeley is specifically examining clusters of SMP nodes [LC98, LMC97]. Besides Dolphin Interconnects <ref> [Dol95a] </ref>, Interconnect Systems Solution [Kib97] and Vitesse are also developing commodity SCI products to connect clusters of workstations. Several other groups even build their own SCI network cards or bridges, for example the SMILE group at the University of Munich [AHKL96], or the RD24 project at CERN.
Reference: [Dol95b] <author> Dolphin. </author> <note> SBus-to-SCI Adapter User's Guide, DIS303 SBus-2. Dolphin Interconnect Solutions A.S., </note> <year> 1995. </year>
Reference-contexts: We examine two important programming models: message passing and global memory. Our testbed consists of two clusters: The first cluster consists of 8 UltraSparcs connected by Dolphin's SCI SBus-2 adapters <ref> [Dol95b] </ref>, as shown in Figure 1. Four Ultra-30 Workstation comprise the second cluster, also shown in Figure 1. We also experiment with several dual-Pentium-II PC's, running Linux, NT and Solaris. The SCI adapter cards provide a simple user-level load/store programming interface for accessing remote memory.
Reference: [DR97] <author> D. Dunning and G. Regnier. </author> <title> The Virtual Interface Architecture. In Hot Interconnects V Symposium on High Performance Interconnects, </title> <month> August 97. </month>
Reference-contexts: The newer PCI-based cluster consists of 4 Ultra30 workstations. Net [Hor95], Myrinet [BCF + 95], HAL [Lar98], and the recently defined VIA interface <ref> [DR97, CM97] </ref>. A common approach to achieving low latency is to bypass the operating system for normal communication, but still ensure protection by relying on the virtual memory mechanism and using the operating system only to set up communication segments.
Reference: [Gil96] <author> R. Gillett. </author> <title> Memory Channel Network for PCI. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Essentially, part of the network interface is mapped into the application's address space, such that messages can be composed and received at user-level. DEC Memory Channel <ref> [Gil96] </ref> and SCI take this approach a step further by having individual load or store instructions directly turn into remote memory accesses. In this paper, we study how to map parallel programming models efficiently onto the SCI hardware.
Reference: [GK96] <author> R. Gillet and R. Kaufmann. </author> <title> Experience using the First-Generation Memory Channel for PCI Network. </title> <booktitle> In Hot Interconnects IV Symposium on High Performance Interconnects, </booktitle> <month> August, 96. </month>
Reference-contexts: There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory. We discus the SHRIMP multicomputer [BLA + 94] and the DEC Memory Channel <ref> [GK96] </ref>. Other similar networks are Tandem ServerNet [Hor95] and the PCI-based Synfinity network from HAL/Fujitsu [Lar98]. The DEC Memory Channel is very similar to SCI. It too provides a limited global memory to a cluster of workstations via network interfaces on the I/O bus. <p> The size of the global memory is limited by hardware design (e.g., 128MB total). The network has a raw latency of 2:9s for 32 byte messages, enabling higher level protocols such as MPI, PVM, and HPF to have communication latency of less than 10s <ref> [GK96] </ref>. SHRIMP is based on a similar shared memory principle as SCI, but only supports remote writes. Like our SCI interface, SHRIMP uses an import/export mapping to keep track of global memory. The SHRIMP hardware is connected to the memory bus, not the I/O bus.
Reference: [GL95] <author> D. B. Gustavson and Q. Li. </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface. </title> <type> Technical report, </type> <institution> SCIzzl, Santa Clara University, Department of Computer Engineering, </institution> <address> Santa Clara, California, </address> <year> 1995. </year>
Reference-contexts: Gustavson and Li were the first to promote SCI in the LAMP project <ref> [GL95] </ref>, and point out that SCI is well suited for networks of workstations. Probably the most comprehensive experimental performance study of SBus-2 based SCI adapters is from Omang and Parady [Oma95, OP96].
Reference: [GPT + 96] <author> A. George, W. Phipps, R. Todd, D. Zirpoli, K. Justice, M. Giacoboni, and M. Sarwar. </author> <title> SCI and the Scalable Cluster Architecture Latency-Hiding (SCALE) Project. </title> <booktitle> In Proceedings of the Sixth International Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <month> Septem-ber </month> <year> 1996. </year>
Reference-contexts: Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms [RGL96, RMG97, Rya97]. Their device driver work focuses on the support of DMA transfers and interrupts. The SCALE cluster project <ref> [GPT + 96, GPTR97] </ref> strives to investigate techniques for efficient programming of workstation clusters for high performance computing. George et al. report their experiments with a multi-threaded runtime system and their version of Active Messages. Their implementation has a 68s latency, but is thread safe.
Reference: [GPTR97] <author> A. George, W. Phipps, R. Todd, and W. Rossen. </author> <title> Multithreading and Lightweight Communication Protocol Enhancements for SCI-based SCALE Systems. </title> <booktitle> In Proceedings of the Seventh International Workshop on SCI-based High-Perforance Low-Cost Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms [RGL96, RMG97, Rya97]. Their device driver work focuses on the support of DMA transfers and interrupts. The SCALE cluster project <ref> [GPT + 96, GPTR97] </ref> strives to investigate techniques for efficient programming of workstation clusters for high performance computing. George et al. report their experiments with a multi-threaded runtime system and their version of Active Messages. Their implementation has a 68s latency, but is thread safe.
Reference: [Hor95] <author> R. W. Horst. TNet: </author> <title> A Reliable System Area Network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: However, the demand for low-latency, high-performance clusters has led to the development of more efficient approaches in the commodity market, as witnessed by the popularity of Tandem Server workstations. The newer PCI-based cluster consists of 4 Ultra30 workstations. Net <ref> [Hor95] </ref>, Myrinet [BCF + 95], HAL [Lar98], and the recently defined VIA interface [DR97, CM97]. <p> There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory. We discus the SHRIMP multicomputer [BLA + 94] and the DEC Memory Channel [GK96]. Other similar networks are Tandem ServerNet <ref> [Hor95] </ref> and the PCI-based Synfinity network from HAL/Fujitsu [Lar98]. The DEC Memory Channel is very similar to SCI. It too provides a limited global memory to a cluster of workstations via network interfaces on the I/O bus.
Reference: [IEE93] <editor> IEEE, </editor> <address> 345 East 47th Street, New York. </address> <institution> IEEE Standard for Scalable Coherent Interface (SCI), </institution> <year> 1993. </year>
Reference-contexts: Section 5 presents related work. In Section 6 we conclude with an outlook on future work. 2 SCI Hardware and Raw Performance Data SCI grew out of the desire to develop a scalable bus architecture which could support a large number of processors. SCI is an IEEE standard <ref> [IEE93] </ref> which describes a scalable coherent interface that allows a large number of nodes to be connected by point-to-point links, arranged in rings or attached to switches.
Reference: [ISSW96] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> Implementing Active Messages and Split-C for SCI Clusters and Some Architectural Implications. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: For example, a remote enqueue transaction that takes a message and enqueues it in the destination memory, atomically updating a queue pointer, could help solving the notification problem and reducing the polling overhead <ref> [ISSW96] </ref>. A larger address translation table would facilitate the use of SCI for global address space applications and make them more efficient. Furthermore, the current SBus implementation limits the most efficient 64 byte stores to aligned data. Applications which access unaligned data have to use less efficient store primitives.
Reference: [ISSW97] <author> M. Ibel, K. E. Schauser, C. J. Scheiman, and M. Weis. </author> <title> High-Performance Cluster Computing Using Scalable Coherent Interface. </title> <booktitle> In Seventh International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: In polling, the receiving processor checks whether a message has been enqueued, and if so, removes it from the queue and processes it. We have to emulate a remote queue under SCI. We now briefly discuss some implementation alternatives and sketch possible optimizations. More details can be found in <ref> [ISSW97] </ref>. queue. 3.2.1 Remote Queue on Shared Memory queue. On shared-memory architectures, the straightforward way to implement the queue-like functionality is to build a queue in user memory using hardware synchronization primitives.
Reference: [Jam96] <author> D. V. James. </author> <title> Combinable locks: NullSwap efficiently delays cache-line transfers. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: Since many processes can concurrently enqueue data, we need some sort of atomic read/write operation, such as test-and-set, to prevent write conflicts [Oma97]. The SCI standard provides support for lock transactions <ref> [Jam96] </ref>. 1 Given such a lock transaction, our scheme for sending a message could be: lock the remote queue, read the destination write pointer, increment the pointer by the message size, check for overflow, write 1 Unfortunately, lock transactions are currently not supported by our SBus-2 network adapters. the pointer back
Reference: [Kib97] <author> K. Kibria. </author> <title> Prospects for new SCI interface chips. </title> <booktitle> In Proceedings of the Seventh International Workshop on SCI-based High-Perforance Low-Cost Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: George et al. report their experiments with a multi-threaded runtime system and their version of Active Messages. Their implementation has a 68s latency, but is thread safe. The NOW group at Berkeley is specifically examining clusters of SMP nodes [LC98, LMC97]. Besides Dolphin Interconnects [Dol95a], Interconnect Systems Solution <ref> [Kib97] </ref> and Vitesse are also developing commodity SCI products to connect clusters of workstations. Several other groups even build their own SCI network cards or bridges, for example the SMILE group at the University of Munich [AHKL96], or the RD24 project at CERN.
Reference: [Lar98] <author> J. Larson. </author> <title> The HAL interconnect PCI card. </title> <booktitle> In Workshop on Communication, Architecture, and Applications for Network-based Parallel Computing, </booktitle> <month> Jan </month> <year> 1998. </year>
Reference-contexts: However, the demand for low-latency, high-performance clusters has led to the development of more efficient approaches in the commodity market, as witnessed by the popularity of Tandem Server workstations. The newer PCI-based cluster consists of 4 Ultra30 workstations. Net [Hor95], Myrinet [BCF + 95], HAL <ref> [Lar98] </ref>, and the recently defined VIA interface [DR97, CM97]. A common approach to achieving low latency is to bypass the operating system for normal communication, but still ensure protection by relying on the virtual memory mechanism and using the operating system only to set up communication segments. <p> There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory. We discus the SHRIMP multicomputer [BLA + 94] and the DEC Memory Channel [GK96]. Other similar networks are Tandem ServerNet [Hor95] and the PCI-based Synfinity network from HAL/Fujitsu <ref> [Lar98] </ref>. The DEC Memory Channel is very similar to SCI. It too provides a limited global memory to a cluster of workstations via network interfaces on the I/O bus. Unlike in SCI, pages are replicated if several nodes import the same memory for reading.
Reference: [LC98] <author> S. S. Lumetta and D. E. Culler. </author> <title> Managing Concurrent Access for Shared Memory Active Messages. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <address> Orlando, Florida, </address> <month> April </month> <year> 1998. </year>
Reference-contexts: George et al. report their experiments with a multi-threaded runtime system and their version of Active Messages. Their implementation has a 68s latency, but is thread safe. The NOW group at Berkeley is specifically examining clusters of SMP nodes <ref> [LC98, LMC97] </ref>. Besides Dolphin Interconnects [Dol95a], Interconnect Systems Solution [Kib97] and Vitesse are also developing commodity SCI products to connect clusters of workstations.
Reference: [LCS96] <author> T. D. Lovett, R. M. Clapp, and R. J. Safranek. NUMA-Q: </author> <title> An SCI based Enterprise Server. </title> <booktitle> In Sixth International Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> Septem-ber </month> <year> 1996. </year>
Reference-contexts: SCI switches allow for multiply connected rings. The SCI standard specifies four basic types of transactions: read, write, move, and lock. All transactions are split-phase, separating requests from responses. Read and write transactions send replies back, whereas move transactions are one-way. Several vendors have added or removed transactions <ref> [LCS96, Dol95a] </ref>. Our SBus-based Dolphin network adapters do not support lock transactions, and for PCI, the mapping of bus operations to SCI transactions is implementation specific. <p> SCI has been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sun Starfire [Mic96], Data General [CA96], and Sequent NUMA-Q <ref> [LCS96] </ref>). In the last two products, the nodes are connected via cache-coherent SCI on the memory bus. There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory.
Reference: [LMC97] <author> S. S. Lumetta, A. M. Mainwaring, and D. E. Culler. </author> <title> Multi-Protocol Active Messages on a Cluster of SMP's. </title> <booktitle> In Proceedings of SC97: High Performance Networking and Computing, </booktitle> <address> San Jose, California, </address> <month> November </month> <year> 1997. </year>
Reference-contexts: George et al. report their experiments with a multi-threaded runtime system and their version of Active Messages. Their implementation has a 68s latency, but is thread safe. The NOW group at Berkeley is specifically examining clusters of SMP nodes <ref> [LC98, LMC97] </ref>. Besides Dolphin Interconnects [Dol95a], Interconnect Systems Solution [Kib97] and Vitesse are also developing commodity SCI products to connect clusters of workstations.
Reference: [Mic96] <author> Sun Microsystem. </author> <title> Sun Cluster Channel. </title> <address> http://www.sun.com/products-n-solutions/hw/ servers/hpc/tech/interconnect.html, </address> <year> 1996. </year>
Reference-contexts: SCI has been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sun Starfire <ref> [Mic96] </ref>, Data General [CA96], and Sequent NUMA-Q [LCS96]). In the last two products, the nodes are connected via cache-coherent SCI on the memory bus. There are other network architectures that are very similar to SCI, in that they provide a load/store interface to remote memory.
Reference: [Oma95] <author> K. Omang. </author> <title> Performance results from SALMON, a cluster of Workstations Connected by SCI. </title> <type> Technical report, </type> <institution> Department of Informatics, University of Oslo, Norway, </institution> <year> 1995. </year>
Reference-contexts: Gustavson and Li were the first to promote SCI in the LAMP project [GL95], and point out that SCI is well suited for networks of workstations. Probably the most comprehensive experimental performance study of SBus-2 based SCI adapters is from Omang and Parady <ref> [Oma95, OP96] </ref>. In this study, important hardware parameters like bandwidth and latency in various configurations of two nodes, connected with one or two SCI rings, are explored, and bottlenecks like the DMA engine are highlighted.
Reference: [Oma97] <author> K. Omang. </author> <title> Synchronization Support in I/O Adapter Based SCI Clusters. </title> <booktitle> In Proceedings of Workshop on Communication and Architectural Support for Network-based Parallel Computing, </booktitle> <address> CANPC'97, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: On shared-memory architectures, the straightforward way to implement the queue-like functionality is to build a queue in user memory using hardware synchronization primitives. Since many processes can concurrently enqueue data, we need some sort of atomic read/write operation, such as test-and-set, to prevent write conflicts <ref> [Oma97] </ref>. <p> Omang also studied the support of atomic read/write operations, such as test-and-set on the IO adapter cards <ref> [Oma97, Oma98] </ref>. Omang's work is part of the SCILLA project at the University of Oslo, which studies the use of SCI in workstation clusters. Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms [RGL96, RMG97, Rya97].
Reference: [Oma98] <author> K. Omang. </author> <title> Performance of a cluster of pci based ultrasparc workstations interconnected with sci. </title> <booktitle> In Proceedings of Network-Based Parallel Computing, Communication, Architecture, and Applications, </booktitle> <address> Las Vegas, </address> <month> Jan </month> <year> 1998. </year>
Reference-contexts: Omang also studied the support of atomic read/write operations, such as test-and-set on the IO adapter cards <ref> [Oma97, Oma98] </ref>. Omang's work is part of the SCILLA project at the University of Oslo, which studies the use of SCI in workstation clusters. Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms [RGL96, RMG97, Rya97].
Reference: [OP96] <author> K. Omang and B. Parady. </author> <title> Performance of Low-Cost UltraSparc Multiprocessors connected by SCI. </title> <type> Technical report, </type> <institution> Department of Informatics, University of Oslo, Norway and Sun Microsystems Inc., </institution> <year> 1996. </year>
Reference-contexts: Using DMA, we observed a peak bandwidth of 36 MB/s. Omang and Parady report experience with a multi-threaded DMA driver that can hide some of the DMA setup cost by pipelining DMA setup and DMA transfer <ref> [OP96] </ref>. They also report results for multiple SCI adapters in one node. In particular, the bandwidth of 2 adapters per node sending and receiving bidirectionally reaches 57 MB/s. The PCI adapters show a much higher performance than the SBus cards. <p> Gustavson and Li were the first to promote SCI in the LAMP project [GL95], and point out that SCI is well suited for networks of workstations. Probably the most comprehensive experimental performance study of SBus-2 based SCI adapters is from Omang and Parady <ref> [Oma95, OP96] </ref>. In this study, important hardware parameters like bandwidth and latency in various configurations of two nodes, connected with one or two SCI rings, are explored, and bottlenecks like the DMA engine are highlighted.
Reference: [RGL96] <author> S. J. Ryan, S. Gjessing, and M. Liaaen. </author> <title> Cluster Communication Using a PCI to SCI Interface. </title> <booktitle> In IASTED 8th Int. Conf. on Parallel and Distributed Computing and Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Omang's work is part of the SCILLA project at the University of Oslo, which studies the use of SCI in workstation clusters. Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms <ref> [RGL96, RMG97, Rya97] </ref>. Their device driver work focuses on the support of DMA transfers and interrupts. The SCALE cluster project [GPT + 96, GPTR97] strives to investigate techniques for efficient programming of workstation clusters for high performance computing.
Reference: [RMG97] <author> S. J. Ryan, A. Maus, and S. Gjessing. </author> <title> An operating system independent driver for an I/O based SCI interface. </title> <booktitle> In Proceedings of the Seventh International Workshop on SCI-based High-Perforance Low-Cost Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: Omang's work is part of the SCILLA project at the University of Oslo, which studies the use of SCI in workstation clusters. Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms <ref> [RGL96, RMG97, Rya97] </ref>. Their device driver work focuses on the support of DMA transfers and interrupts. The SCALE cluster project [GPT + 96, GPTR97] strives to investigate techniques for efficient programming of workstation clusters for high performance computing.
Reference: [Rya97] <author> S. Ryan. </author> <title> The design and implementation of a portable driver for shared memory cluster adapters. </title> <type> Technical Report 255, </type> <institution> University of Oslo, </institution> <year> 1997. </year>
Reference-contexts: Omang's work is part of the SCILLA project at the University of Oslo, which studies the use of SCI in workstation clusters. Another SCILLA project is to create a portable device driver for Dolphin PCI adapters on a wide range of platforms <ref> [RGL96, RMG97, Rya97] </ref>. Their device driver work focuses on the support of DMA transfers and interrupts. The SCALE cluster project [GPT + 96, GPTR97] strives to investigate techniques for efficient programming of workstation clusters for high performance computing.
Reference: [SS95] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: This is especially true for Active Messages, where each message handler is executed on the receiver. The receiving processor must be notified of new message arrival and synchronization requirements must be obeyed. Remote queues is an abstraction addressing the notification and synchronization problems associated with message delivery <ref> [SS95, BCL + 95] </ref>. Sending processors just enqueue their messages on the remote queue. The receiver is notified of the message either using interrupts or polling. Since interrupts are very expensive, our implementation relies on polling.
Reference: [vECGS92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In this paper, we study how to map parallel programming models efficiently onto the SCI hardware. We focus on two common programming models: message passing and global address space. For message passing, we implement Active Messages <ref> [vECGS92] </ref>; but our results apply to any message passing scheme, since Active Messages is a low-overhead messaging layer which can, and often is, used to implement other protocols.
Reference: [WBvE96] <author> M. Welsh, A. Basu, and T. von Eicken. </author> <title> Low-Latency Communication over Fast Ethernet. </title> <booktitle> In Proceedings of Euro-Par '96, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Note that the round-trip write times are for two writes, ping ponged between two nodes. The read time are for a single read, since it is inherently a round-trip communication. These results are better than the fastest measurements reported for comparable commodity networks, for example fast Ethernet (30s) <ref> [WBvE96] </ref> or Myrinet (10s for 4-byte transfers as reported by Myricom). Our results for PCI are even better: We measured one-way latencies between 3:1s and 5:4s for 4-byte to 64-byte stores.
References-found: 38

